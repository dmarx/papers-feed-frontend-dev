# 

## Abstract

## CONTENTS

understanding of the algorithms made available to her in multiple machine learning packages and software, and that she will be able to assess their prerequisites and limitations, and to extend them and develop new algorithms. Note that, while adopting a presentation with a strong mathematical flavor, we will still make explicit the details of many important machine learning algorithms.

Unsurprisingly, the book will be more accessible to a reader with some background in mathematics and statistics. It assumes familiarity with basic concepts in linear algebra and matrix analysis, in multivariate calculus and in probability and statistics. We tried to place a limit at the use of measure theoretic tools, that are avoided up to a few exceptions, which are be localized and be accompanied with alternative interpretations allowing for a reading at a more elementary level.

The book starts with an introductory chapter that describes notation used throughout the book and serve at a reminder of basic concepts in calculus, linear algebra and probability. It also introduces some measure theoretic terminology, and can be used as a reading guide for the sections that use these tools. This chapter is followed by two chapters offering background material on matrix analysis and optimization. The latter chapter, which is relatively long, provides necessary references to many algorithms that are used in the book, including stochastic gradient descent, proximal methods, etc.

Chapter 4, which is also introductory, illustrates the bias-variance dilemma in machine learning through the angle of density estimation and motivates chapter 5 in which basic concepts for statistical prediction are provided. Chapter 6 provides an introduction to reproducing kernel theory and Hilbert space techniques that are used in many places, before tackling, with chapters 7 to 11, the description of various algorithms for supervised statistical learning, including linear methods, support vector machines, decision trees, boosting, or neural networks.

Chapter 12, which presents sampling methods and an introduction to the theory of Markov chains, starts a series of chapters on generative models, and associated learning algorithms. Graphical models and described in chapters 13 to 15. Chapter 16 introduces variational methods for models with latent variables, with applications to graphical models in chapter 17. Generative techniques using deep learning are presented in chapter 18.

Chapters 19 to 21 focus on unsupervised learning methods, for clustering, factor analysis and manifold learning. The final chapter of the book is theory-oriented and discusses concentration inequalities and generalization bounds.

Subtracting F(x) to both sides (which is allowed since F(x) < ∞) and dividing by µ yields f (µ) ≤ f (λ) .

If F is strictly convex, then, either

as soon as 0 < µ < λ, yielding f (µ) < f (λ) .

Now consider the converse statement. By comparing the expression in (3.3) to that obtained with λ = 1, we find, for all x, y ∈ Ω

which is (3.2). Since F satisfies (3.2) in its domain, it is convex. If the function in (3.3) is increasing, then the inequality is strict for 0 < λ < 1 as soon as the lower bound is finite, and F is strictly convex.

Proof If x is a local minimum of F, then, obviously, x ∈ dom(F), and for any y ∈ R d and small enough µ > 0, F(x) ≤ F((1µ)x + µy). Using the function in (3.3) for λ = µ and for λ = 1, we get

so that x is a global minimum.

■

## Relative interior

If Ω is convex, then Ω and Ω (its topological interior and closure) are convex too (the easy proof is left to the reader). However, topological interiors of interesting convex sets are often empty, and a more adapted notion of relative interior is preferable.

Define the affine hull of a set Ω, denoted aff(Ω), as the smallest affine subset of R d that contains Ω. The vector space parallel to aff(Ω) (generated by all differences xy, x, y ∈ Ω) will be denoted --→ aff (Ω). Their dimension k, is the largest integer such that there exist x 0 , x 1 , . . . , x k ∈ Ω such that x 1x 0 , . . . , x kx 0 are linearly independent. Moreover, given these points, elements of the affine hull are defined through barycentric coordinates, yielding aff(Ω) = {x = λ (0) x 0 + • • • + λ (k) x k :, λ (0) + • • • + λ (k) = 1} .

## Contents

Preface 1 General Notation and Background Material 1.1 Linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4 Probability theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 A Few Results in Matrix Analysis 2.1 Notation and basic facts . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 The trace inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Some matrix norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Introduction to Optimization 3.1 Basic Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Unconstrained Optimization Problems . . . . . . . . . . . . . . . . . . 3.2.1 Conditions for optimality (general case) . . . . . . . . . . . . . 3.2.2 Convex sets and functions . . . . . . . . . . . . . . . . . . . . . 3.2.3 Relative interior . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.4 Derivatives of convex functions and optimality conditions . . . 3.2.5 Direction of descent and steepest descent . . . . . . . . . . . . 3.2.6 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.7 Line search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Stochastic gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Stochastic approximation methods . . . . . . . . . . . . . . . . 3.3.2 Deterministic approximation and convergence study . . . . . . 3.3.3 The ADAM algorithm . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Constrained optimization problems . . . . . . . . . . . . . . . . . . . . 3.4.1 Lagrange multipliers . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 Convex constraints . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.4 Projected gradient descent . . . . . . . . . . . . . . . . . . . . . 3.5 General convex problems . . . . . . . . . . . . . . . . . . . . . . . . . .

## Preface

Machine learning addresses the issue of analyzing, reproducing and predicting various mechanisms and processes observable through experiments and data acquisition. With the impetus of large technological companies in need of leveraging information included in the gigantic datasets that they produced or obtained through user data, with the development of new data acquisition techniques in biology, physics or astronomy, with the improvement of storage capacity and high-performance computing, this field has experienced an explosive growth over the past decades, in terms of scientific production and technological impact.

While it is being recognized in some places as a scientific discipline in itself, machine learning (which has received a few almost synonymic denominations across time, including artificial intelligence, machine intelligence or statistical learning), can also be seen as an interdisciplinary field interfacing techniques from traditional domains such as computer science, applied mathematics, and statistics. From statistics, and more specially nonparametric statistics, it borrows its main formalism, asymptotic results and generalization bounds. It also builds on many classical methods that have been developed for estimation and prediction. From computer science, it involves the construction and implementation of efficient algorithms, programming design and architecture. Finally, machine learning leverages classical methods from linear algebra and functional analysis, as well as from convex and nonlinear optimization, fields within which it had also provided new problems and discoveries. It forms a significant part of the larger field commonly called "data science," which includes methods for storing, sharing and managing data, the development powerful computer architectures for increasingly demanding algorithms, and, importantly, the definition of ethical limits and processes through which data should be used in the modern world. This book, which originates from lecture notes of a series of graduate course taught in the Department of Applied Mathematics and Statistics at Johns Hopkins University, adopts a viewpoint (or bias) mainly focused on the mathematical and statistical aspects of the subject. Its goal is to introduce the mathematical foundations and techniques that lead to the development and analysis of many of the algorithms that are used today. It is written with the hope to provide the reader with a deeper [Chapter 1](#) General Notation and Background Material 1.1 Linear algebra 1. The set of all subsets of a given set A is denoted P (A). If A and B are two sets, the notation B A refers to the set of all functions f : A → B. In particular, R A is the space of real-valued functions, and forms a vector space. When A is finite, this space is finite dimensional and can be identified with R |A| , where |A| denotes the cardinality (number of elements) of A.

The indicator function of a subset C of A will be denoted 1 C : A → {0, 1}, with 1 C (x) = 1 if x ∈ C and 0 otherwise. We will sometimes write 1 x∈C for 1 C (x).

## Elements of the d-dimensional

Euclidean space R d will be denoted with letters such as x, y, z, and their coordinates will be indexed as parenthesized exponents, so that

$x =          $x (1)  . . .

$x (d)          $(we will always identify element of R d with column vectors). We will not distinguish in the notation between "points" in R d , seen as an affine space, and "vectors" in R d , seen as a vector space. The vectors 0 d and 1 d will denote the d-dimensional vectors with all coordinates equal to 0 and 1, respectively. The identity matrix in R d will be denoted Id R d . The canonical basis of R d , provided by the columns of Id R d will be denoted e 1 , . . . , e d .

## 3.

The Euclidean norm of a vector x ∈ R d is denoted |x| with |x| = (x (1) ) 2 + • • • + (x (d) ) 2 1/2 .

It will sometimes be denoted |x| 2 , identifying it as a member of the family of ℓ p norms |x| p = (x (1) ) p + • • • + (x (d) ) p 1/p (1.1) CHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL for p ≥ 1. One can also define |x| p for 0 < p < 1, using (1.1), but in this case one does not get a norm because the triangle inequality |x + y| p ≤ |x| p + |y| p is not true in general. The family is interesting, however, because it approximates, in the limit p → 0, the number of non-zero components of x, denoted |x| 0 , which is a measure of sparsity. Note that we also use the notation |A| to denote the cardinality (number of elements) of a set A, hopefully without risk of confusion.

While we use single bars (|x|) to represent norms of finite-dimensional vectors, we will use double bars (∥h∥) for infinite-dimensional objects. Entry (i, j) in a matrix A ∈ M m,d (R) will either be denoted A(i, j) or A (i) j . The rows of A will be denoted A (1) , . . . , A (m) and the columns A 1 , . . . , A m .

The operator norm of a matrix A ∈ M m,d is defined by 

$|A| op = max{|Ax| : x ∈ R d , |x| = 1}.$
## 6.

A k-multilinear mapping is a function a : (x 1 , . . . , x k ) → a(x 1 , . . . , x k ) defined on (R d ) k with values in R q which is linear in each of its variables. The mapping is symmetric if its value is unchanged after any permutation of the variables. If k = 2 and q = 1, one also says that a is a bilinear form. The norm of a k-multilinear mapping is defined as A symmetric bilinear form a is called positive semidefinite if a(x, x) ≥ 0 for all x ∈ R d , and positive definite if it is positive semi-definite and a(x, x) = 0 if and only if x = 0.

## TOPOLOGY

Symmetric bilinear forms can always be expressed in the form a(x, y) = x T Ay for some symmetric matrix A, and a is positive (semi-)definite if and only A is also. Analogous statements hold for negative (semi-)definite forms and matrices. We will use the notation A ≻ 0 (resp. ⪰ 0) to indicate that A is positive definite (resp. positive semidefinite). Note that, if a(x, y) = x T Ay for A ∈ S d , then |a| = |A| op .

## Topology

1. The open balls in R d will be denoted B(x, r) = {y ∈ R d : |y -x| < r}, with x ∈ R d and r > 0. The closed balls are denoted B(x, r) and contain all y's such that |y -x| ≤ r. A set U ⊂ R d is open if and only if for any x ∈ U , there exists r > 0 such that B(x, r) ⊂ U . A set Γ ⊂ R d is closed if its complement, denoted

$Γ c = {x ∈ R d : x Γ }$is open. The topological interior of a set A ⊂ R d is the largest open set included in A. It will be denoted either by Å or int(A). A point x belongs to Å if and only if B(x, r) ⊂ A for some r > 0.

2. The closure of A is the smallest closed set that contains A and will be denoted either Ā or cl(A). A point x belongs to Ā if and only if B(x, r) ∩ A ∅ for all r > 0. Alternatively, x belongs to Ā if and only if there exists a sequence (x k ) that converges to x with x k ∈ A for all k.

## 3.

A compact set in R d is a set Γ such that any sequence of points in Γ contains a subsequence that converges to some point in Γ . An alternate definition is that, whenever Γ is covered by a collection of open sets, there exists a finite subcollection that still covers Γ .

One can show that compact subsets of R d are exactly its bounded and closed subsets.

## 4.

A metric space is a space B equipped with a distance, i.e., a function ρ : B × B → [0, +∞) that satisfies the following three properties. subsets in metric spaces is the same as above, with ρ(x, y) replacing |x -y|, and one says that (x n ) converges to x if and only if ρ(x n , x) → 0.

Compact subsets are also defined in the same way, but are not necessarily characterized as bounded and closed.

## Calculus

1. If x, y ∈ R d , we will denote by [x, y] the closed segment delimited by x and y, i.e., the set of all points (1t)x + ty for 0 ≤ t ≤ 1. One denotes by [x, y), (x, y] and (x, y)

the semi-open or open segments, with appropriate strict inequality for t. (Similarly to the notation for open intervals, whether (x, y) denotes an open segment or a pair of points will always be clear from the context.)

2. The derivative of a differentiable function f : t → f (t) from an interval I ⊂ R to R will be denoted by ∂f , or ∂ t f if the variable t is well identified. Its value at t 0 ∈ I is denoted either as ∂f (t 0 ) or ∂f | t=t 0 . Higher derivatives are denoted as ∂ k f , k ≥ 0, with the usual convention ∂ 0 f = f . Note that notation such as f ′ , f ′′ , f (3) will never refer to derivatives.

In the following, U is an open subset of R d . If f is a function from U to R m , we let f (i) denote the i th component of f , so that

$f (x) =           f (1) (x) . . . f (m) (x)           for x ∈ U . If d = 1$, and f is differentiable, the derivative of f at x is the column vector of the derivatives of its components,

$∂f (x) =          $∂f (1) (x) . . . ∂f (m) (x)

$         $For d ≥ 1 and j ∈ {1, . . . , d}, the j th partial derivative of f at x is

$∂ j f (x) = ∂(t → f (x + te j ))| t=0 ∈ R m ,$where e 1 , . . . , e d form the canonical basis of R d . If the notation for the variables on which f depends is well understood from the context, we will alternatively use ∂ x j f . (For example, if f : (α, β) → f (α, β), we will prefer ∂ α f to ∂ 1 f .) The differential of f at x is the linear mapping from R d to R m represented by the matrix

$df (x) = [∂ 1 f (x), . . . , ∂ d f (x)].$It is defined so that, for all

$h ∈ R d df (x)h = ∂(t → f (x + th))| t=0$where the right-hand side is the directional derivative of f at x in the direction h. Note that, if f : R d → R (i.e., m = 1), df (x) is a row vector. If f is differentiable on U and df (x) is continuous as a function of x, one says that f is continuously differentiable, or C 1 .

Differentials obey the product rule and the chain rule. If f , g : U → R, then d(f g)(x) = f (x)dg(x) + g(x)df (x).

$If f : U → R m , g : Ũ ⊂ R k → U , then d(f • g)(x) = df (g(x))dg(x).$If d = m (so that df (x) is a square matrix), we let ∇•f (x) = trace(df (x)), the divergence of f .

The Euclidean gradient of a differentiable function f :

$U → R is ∇f (x) = df (x) T .$More generally, one defines the gradient of f with respect to a tensor field x → A(x) taking values in S ++ d , as the vector ∇ A f (x) that satisfies the relation

$df (x)h = ∇ A f (x) T A(x)h$for all h ∈ R d , so that

$∇ A f (x) = A(x) -1 df (x) T . (1.3)$In particular, the Euclidean gradient is associated with A(x) = Id R d for all x. With some abuse of notation, we will denote ∇ A f = A -1 ∇f when A is a fixed matrix, therefore identified with the constant tensor field x → A.

3. We here compute, as an illustration and because they will be useful later, the differential of the determinant and the inversion in matrix spaces.

Recall that, if A = [a 1 , . . . , a d ] ∈ M d is a d by d matrix,, with a 1 , . . . , a d ∈ R d , det(A) is a d-linear form δ(a 1 , . . . , a d ) which vanishes when two columns coincide and such that δ(e 1 , . . . , e d ) = 1. In particular δ changes signs when two of its columns are inverted. It follows from this that ∂ a ij det(A) = δ(a 1 , . . . , a i-1 , e j , a j+1 , . . . , a d ) = (-1) i-1 δ(e j , a 1 , . . . , a i-1 , . . . , a d ) = (-1) i+j det A (ij) ,

where A (ij) is the matrix A with row i and column j removed. We therefore find that the differential of A → det(A) is the mapping

$H → trace(cof(A) T H) (1.4)$where cof(A) is the matrix composed of co-factors (-1) i+j det A (ij) . As a consequence, if A is invertible, then the differential of log | det(A)| is the mapping 

$H → trace(det(A) -1 cof(A) T H) = trace(A -1 H) (1.$$∂ i k • • • ∂ i 1 f (x) = ∂ i k (∂ i k-1 • • • ∂ i 1 f )(x)$If all order k partial derivatives of f exist and are continuous, one says that f is k-times continuously differentiable, or C k and, when true, the order in which the derivatives are taken does not matter. In this case, one typically groups derivatives with the same order using a power notation, writing, for example

$∂ 1 ∂ 2 ∂ 1 f = ∂ 2 1 ∂ 2 f$for a C 3 function.

If f is C k , its k th differential at x is a symmetric k-multilinear map that can also be iteratively defined by (for h 1 , . . . , h k ∈ R d )

$d k f (x)(h 1 , . . . , h k ) = d(d k-1 f (x)(h 1 , . . . , h k-1 ))h k ∈ R m .$It is related to partial derivatives through the relation:

$d k f (x)(h 1 , . . . , h k ) = d i 1 ,...,i k =1 h (i 1 ) 1 • • • h (i k ) k ∂ i k • • • ∂ i 1 f (x).$When m = 1 and k = 2, one denotes by ∇ 2 f (x) = (∂ i ∂ j f (x), i, j = 1, . . . , n) the symmetric matrix formed by partial derivatives of order 2 of f at x. It is called the Hessian of f at x and satisfies

$h T 1 ∇ 2 f (x)h 2 = d 2 f (x)(h 1 , h 2 ).$The Laplacian of f is the trace of ∇ 2 f and denoted ∆f .

## 5.

Taylor's theorem, in its integral form, generalizes the fundamental theorem of calculus to higher derivatives. It expresses the fact that, if f is C k on U and x, y ∈ U are such that the closed segment [x, y] is included in U , then, letting h = yx:

$f (x + h) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 (k -1)! d k-1 f (x)(h, . . . , h) + 1 (k -1)! 1 0 (1 -t) k-1 d k f (x + th)(h, . . . , h) dt (1.7)$The last term (remainder) can also be written as

$1 k! 1 0 (1 -t) k-1 d k f (x + th)(h, . . . , h) dt 1 0 (1 -t) k-1 dt .$If f takes scalar values, then d k f (x + th)(h, . . . , h) is real and the intermediate value theorem implies that there exists some z in [x, y] such that

$f (x + h) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 (k -1)! d k-1 f (x)(h, . . . , h) + 1 k! d k f (z)(h, . . . , h). (1.8)$This is not true if f takes vector values. However, for any M such that

$|d k f (z)| ≤ M for z ∈ [x, y] (such M's always exist because f is C k ), one has 1 (k -1)! 1 0 (1 -t) k-1 d k f (x + th)(h, . . . , h) dt ≤ M k! |h| k .$Equation (1.7) can be written as

$f (x + h) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 k! d k f (x)(h, . . . , h) + 1 (k -1)! 1 0 (1 -t) k-1 (d k f (x + th)(h, . . . , h) -d k f (x)(h, • • • , h)) dt . (1.9) Let ϵ x (r) = max |d k f (x + h) -d k f (x)| : |h| ≤ r .$Since d k f is continuous, ϵ x (r) tends to 0 when r → 0 and we have

$1 0 (1 -t) k-1 |d k f (x + th)(h, . . . , h) -d k f (x)(h, • • • , h)| dt ≤ |h| k k ϵ x (|h|).$This shows that (1.7) implies that

$f (x + h) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 k! d k f (x)(h, . . . , h) + |h| k k! ϵ x (|h|) (1.10) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 k! d k f (x)(h, . . . , h) + o(|h| k ) (1.11)$1.4 Probability theory 1. When discussing probabilistic concepts, we will make the convenient assumption that all random variables are defined on a fixed probability space (Ω, P). This means that Ω is large enough to include enough randomness to generate all required variables (and implicitly enlarged when needed).

We assume that the reader is familiar with concepts related to discrete random variables or continuous variables (with values in R d for some d) and their probability density functions, or p.d.f.'s. In particular, X : Ω → R d is a random variable with p.d.f. f if and only if the expectation of ϕ(X) is given by

$E(ϕ(X)) = R d ϕ(x)f (x)dx$for all bounded and continuous functions ϕ : R d → [0, +∞).

2. With a few exceptions, we will use capital letters for random variables and small letters for scalars and vectors that represent realizations of these variables. One of these exceptions will be our notation for training data, defined as an independent and identically distributed (i.i.d.) sample of a given random variable. A realization of such a sample will always be denoted T = (x 1 , . . . , x N ), which is therefore a series of observations. We will use the notation T = (X 1 , . . . , X N ) for the collection of i.i.d. random variables that generate the training set, so that T = (X 1 (ω), . . . , X N (ω)) = T (ω)

for some ω ∈ Ω. Another exception will apply to variables denoted using Greek letters, for which we will use boldface fonts (such as α, β, . . .).

For a random variable X, the notation [X = x], or [X ∈ A] refers to subsets of Ω, for example, [X = x] = {ω ∈ Ω : X(ω) = x} .

## 3.

As much as possible-but not always-we will avoid making explicit reference to measure theory, leaving to readers familiar with this theory the task to complete the notation and sometimes assumption gaps in order to make some of our statements fully rigorous.

However, there will be situations in which the flexibility of the measure-theoretic formalism is needed for the exposition. The following notions may help the reader navigate through these situations (basic references in measure theory are Rudin [[171]](#b189), Dudley [[66]](#b84), Billingsley [[32]](#b50)).

A measurable space is a pair (S, S ) where S is a set and S ⊂ P (S) contains S, is stable by complementation (if A ∈ S , then A c = S \ A ∈ S ), by countable unions and intersections. Such an S is called a σ -algebra and elements of S form the measurable subsets of S (relative to the σ -algebra).

A (positive) measure µ on (S, S ) in a mapping from S → [0, +∞) that associates to A ∈ S its measure µ(A), such that the measure of a countable union of disjoint sets is the countable sum of their measures. A function f : Ω → R d is called measurable if the inverse images by f of open subsets of R d are mesurable. A measurable set A (or event) is negligible (for P) if P(A) = 0 and events are said to happen almost surely if their complements are negligible, i.e., P(A c ) = 0. 4. The integral of a function f : Ω → R d with respect to a measure (such as P) is denoted S f (x)µ(dx). This integral is defined, using a limit argument, as a function which is linear in f and such that

$A µ(dx) = S 1 A (x)µ(dx) = µ(A).$The Lebesgue measure, L d , on R d provides an important example. For this measure S is the σ -algebra generated by open subsets, R d f (x)L d (dx) extends the Riemann integral and is denoted R d f (x)dx. Another important example, when S is finite or countable, is the counting measure, denoted card, that return the number of elements of a set, so that card(A) = |A|. In this case, S = P (S) and the integral is simply the sum:

$S f (x)card(dx) = x∈F f (x).$5. If µ and ν are measures on (S, S ), one says that ν is absolutely continuous with respect to µ and write ν ≪ µ if, ∀A ∈ S : µ(A) = 0 ⇒ ν(A) = 0.

(1.12)

The Radon-Nikodym theorem states that ν ≪ µ if and only if ν has a density with respect to µ, i.e., there exists a measurable function ϕ : S → [0, +∞) such that S f (x)ν(dx) = S f (x)ϕ(x)µ(dx) for all measurable f : S → [0, +∞).

## 6.

If µ 1 is a measure on (S 1 , S 1 ) and µ 2 a measure on (S 2 , S 2 ), their tensor product is denoted µ 1 ⊗ µ 2 . It is a measure on S 1 × S 2 defined by

$µ 1 ⊗ µ 2 (A 1 × A 2 ) = µ 1 (A 1 )µ 2 (A 2 )$for A 1 ∈ S 1 and A 2 ∈ S 2 (the σ -algebra on S 1 × S 2 is the smallest one that contains all sets

$A 1 × A 2 , A 1 ∈ S 1 , A 2 ∈ S 2 ).$The integral, with respect to the product measure, of a function f :

$S 1 × S 2 → R d is denoted S 1 ×S 2 f (x 1 , x 2 )µ 1 (dx 1 )µ 2 (dx 2 ) = S 1 ×S 2 f (x 1 , x 2 )µ 1 ⊗ µ 2 (dx 1 , dx 2 ).$The tensor product between more that two measures is defined similarly, with notation

$µ 1 ⊗ • • • ⊗ µ n = n k=1 µ k .$7. When using measure-theoretic probability, we will therefore assume that the pair (Ω, P) is completed to a triple (Ω, A, P) where A is a σ -algebra and P a probability measure, that is a positive measure on (Ω, A) such that P(Ω) = 1. This triple is called a probability space.

A random variable X must then also take values in a measurable space, say (S, S ), and must be such that, for all C ∈ S , the set [X ∈ C] belongs to A. This justify the computation of P(X ∈ C), which will also be denoted P X (C).

A random variable X taking values in R d has a p.d.f. if and only if P X ≪ L d and the p.d.f. is the density provided by the Radon-Nikodym theorem. For a discrete random variable (i.e., taking values in a finite or countable set), the p.m.f. of X is also the density of P X with respect to the counting measure card.

If X is a random variable with values in R d , the integral of X with respect to P is the expectation of X, denoted E(X). More generally, if (S, S , P ) is a probability space, we will use the notation E P (f ) = S f (x)P (dx).

If P = P X for some random variable X : Ω → S, we will use E X rather than E P X .

8. One more technical consideration. Whenever we will consider measurable spaces, and sometimes without additional mention, we will assume that these spaces are complete metric spaces that have a dense countable subset (i.e., that are separable).

If not specified otherwise, their σ -algebras are given by the smallest ones containing all open sets (the Borel σ -algebra).

Chapter 2

## A Few Results in Matrix Analysis

This chapter collects a few results in linear algebra that will be useful in the rest of this book.

## Notation and basic facts

We denote by M n,d (R) the space of all n × d matrices with real coefficients [1](#foot_0) . For a matrix A ∈ M n,d (R) and integer k ≤ n and l ≤ d, we let A ⌈kl⌉ ∈ M k,l (R) denote the matrix A restricted to its first k rows and first l columns. The i, j entry of A will be denoted A(i, j) or A (ij) .

We assume that the reader is familiar with elementary matrix analysis, including, in particular the fact that symmetric matrices are diagonalizable in an orthonormal basis, i.e., if A ∈ M d,d (R) is a symmetric matrix (whose space is denoted S d ), there exists an orthogonal matrix U ∈ O d (i.e., satisfying U T U = U U T = Id R d ) and a diagonal matrix D ∈ M d,d (R) such that

$A = U DU T .$The identity AU = U D then implies that the columns of U form an orthonormal basis of eigenvectors of A.

$If A ∈ S +$d is positive semi-definite (i.e., u T Au ≥ 0 for all u ∈ R d ), the entries of D in the decomposition A = U DU T are non-negative, and one can define the matrix square root of A as S = U D ⊙1/2 U T where D ⊙1/2 is the diagonal matrix formed taking the square roots of all coefficients of D. We will use the notation S = A 1/2 . Note that D 1/2 = D ⊙1/2 if D is diagonal and positive semi-definite.

If A ∈ S ++ d is positive definite (i.e., A is positive semi-definite and u T Au = 0 implies u = 0) and B is positive semi-definite, both being d ×d matrices, the generalized CHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS eigenvalue problem associated with A and B consists in finding a diagonal matrix D and a matrix U such that BU = AU D and U T AU = Id R d . Letting Ũ = A 1/2 U , the problem is equivalent to solving A -1/2 BA -1/2 Ũ = Ũ D with Ũ T Ũ = Id R d , i.e., finding the eigenvalue decomposition of the symmetric positive-definite matrix A -1/2 BA -1/2 .

If A ∈ M n,d (R), it can be decomposed as T   where U ∈ O n (R) and V ∈ O d (R)) are orthogonal matrices and D ∈ M n,d (R) is diagonal (i.e., such that D(i, j) = 0 whenever i j) with non-negative diagonal coefficients. These coefficients are called the singular values of A, and the procedure is called a singular-value decomposition (SVD) of A. An equivalent formulation is that there exist orthonormal bases u 1 , . . . , u n of R n and v 1 , . . . , v d of R d (forming the columns of U and V ) such that Av i = λ i u i for i ≤ min(n, d), where λ 1 , . . . , λ min(n,d) are the singular values. Of course, if A is square and symmetric positive semi-definite, an eigenvalue decomposition of A is also a singular value decomposition (and the singular values coincide with the eigenvalues). More generally, if A = U DV T , then AA T = U DD T U T and A T A = V D T DV T are eigenvalue decompositions of AA T and A T A. Singular values are uniquely defined, up to reordering. However, the matrices U and V are not unique up to column reordering in general.

$A = U DV$If m = min(n, d), then, forming the matrices Ũ = U ⌈n,m⌉ (resp. Ṽ = V ⌈d,m⌉ ) by removing from U (resp. V ) its last nm (resp. dm) columns , and D = D ⌈m,m⌉ by removing from D its nm rows and dm columns, one has A = Ũ D Ṽ T with Ũ , D and Ṽ having respectively size n×m, m×m and m×d, Ũ T Ũ = Ṽ T Ṽ = Id R m and D diagonal with non-negative coefficients. This representation provides a reduced SVD of A and one can create a full SVD from a reduced one by completing the missing rows of Ũ and Ṽ to form orthogonal matrices, and by adding the required number of zeros to D.

## The trace inequality

We now descibe Von Neumann's trace theorem. Its justification follows the proof given in Mirsky [[137]](#b155). (2.1)

Moreover, if trace(A T B) = m i=1 λ i µ i , then there exist n × n and d × d orthogonal matrices U and V such that U T AV and U T BV are both diagonal, i.e., one can find SVDs of A and B in the same bases of R n and R d .

Proof We can assume without loss of generality that d ≤ n because, if the result holds for A and B, it also holds for A T and B T . Let A = U 1 ΛV T 1 and B = U 2 MV T 2 be the singular values decompositions of A and B (both Λ and M are n × d matrices). Then trace(

$A T B) = trace(V 1 Λ T U T 1 U 2 MV 2 ) = trace(Λ T U MV T ) with U = U T 1 U 2 and V = V T 1 V 2 .$Let u(i, j), 1 ≤ i, j ≤ n and v(i, j), 1 ≤ i, j ≤ d be the coefficients of the orthogonal matrices U and V . Then

$trace(Λ T U MV T ) = d i,j=1 u(i, j)v(i, j)λ i µ j ≤ 1 2 d i,j=1 λ i µ j u(i, j) 2 + 1 2 d i,j=1$λ i µ j v(i, j) 2  λ i µ j u(i, j

$) 2 = d i,j=1 d i ′ =i ξ i ′ d j ′ =j η j ′ u(i, j) 2 = d i ′ ,j ′ =1 ξ i ′ η j ′ i ′ i=1 j ′ j=1 u(i, j) 2 ≤ d i ′ ,j ′ =1 ξ i ′ η j ′ min(i ′ , j ′ ) (2.3)$where we used the fact that U is orthogonal, which implies that j ′ j=1 u(i, j) 2 and i ′ i=1 u(i, j) 2 are both less than 1. Notice also that, when u(i, j) = δ ij (i.e., u(i, j) = 1 if i = j and zero otherwise), then i ′ i=1 j ′ j=1 u(i, j) 2 = min(i ′ , j ′ ), so that the last inequality is an identity, and the chain of equalities leading to [(2.3)](#)

$implies d i ′ ,j ′ =1 ξ i ′ η j ′ min(i ′ , j ′ ) = d i=1 λ i µ j .$We therefore obtain (for any U ), the fact that

$d i,j=1 λ i µ j u(i, j) 2 ≤ d i=1 λ i µ j .$The same identity obviously holds with v in place of u, and combining the two yields (2.1).

We now consider conditions for equality. Clearly, if one can find SVD decompositions of A and B with U 1 = U 2 and V 1 = V 2 , then U = Id R n , V = Id R d and (2.1) is an identity. We want to prove the converse statement.

For (2.1) to be an equality, we first need (2.2) to be an identity, which requires that u(i, j) = v(i, j) as soon as λ i µ j > 0. We also need an equality in (2.3), which requires i ′ i=1 j ′ j=1 u(i, j) 2 = min(i ′ , j ′ ) as soon as λ i ′ > λ i ′ +1 and µ j ′ > µ j ′ +1 . The same identity must be true with v(i, j) replacing u(i, j)

In view of this, denote by i 1 < • • • < i p (resp. j 1 < • • • < j q ) the indexes at which the singular values of A (resp. B) differ form their successors, with the convention λ d+1 = µ d+1 = 0. Let, for k = 1, . . . , p and l = 1, . . . , q

$C(k, l) = i k i=1 j l j=1 u(i, j) 2 .$Then, we must have C(k, l) = min(i k , j l ) for all k, l and u(i, j) = v(i, j) for i = 1, . . . , i p and j = 1, . . . , j q .

If, for all i, j ≤ d, we let U ⌈ij⌉ be the matrix formed by the first i rows and j columns of U , the condition C kl = min(i k , j l ) requires that U ⌈i k j l ⌉ U T ⌈i k j l ⌉ = Id R i k if i k ≤ j l and U T ⌈i k j l ⌉ U ⌈i k j l ⌉ = Id R j l if j l ≤ i k . This shows that, if i k ≤ j l , the rows of U ⌈i k j l ⌉ form an orthonormal family, and necessarily, all elements u(i, j) for i ≤ i k and j > j l vanish. The symmetric situation holds if j l ≤ i k .

Let r k = i ki k-1 and s l = j lj l-1 (with i 0 = j 0 = 0). We now consider possible changes in the SVDs of A and B. With our notation, the matrix Λ takes the form

$Λ =                              λ i 1 Id R r 1 0 0 • • • 0 0 . . . 0 0 λ i 2 Id R r 2 0 • • • 0 0 . .$. 0 . . . . . . . . . . . . . . . 0 0 . . . λ i p Id R r p 0 0 . . . 0 0 . . . 0 0 . . . 0 . . . . . . . . . . . . 0 . . . 0 0 . . . 0

$                            $Let W , W be n × n and d × d orthogonal matrices taking the form

$W =                    W 1 0 0 • • • 0 0 W 2 0 • • • 0 . . . . . . . . . 0 0 . . . W p 0 0 . . . W p+1                    , W =                    W 1 0 0 • • • 0 0 W 2 0 • • • 0 . . . . . . . . . 0 0 . . . W p 0 0 . . . Wp+1                   $where W 1 , . . . , W p are orthogonal with respective sizes r 1 , . . . , r p , W p+1 is orthogonal with size ni p and Wp+1 is orthogonal with size di p . Then we have

$W D W = D$proving that U 1 can be replaced by U 1 W provided that V 1 is replaced by V 1 W . Similar transformations can be made on U 1 and V 2 , with U 2 replaced by U 2 Z and V 2 by

$V 2 Z with Z =                    Z 1 0 0 • • • 0 0 Z 2 0 • • • 0 . . . . . . . . . 0 0 . . . Z q 0 0 . . . Z q+1                    , Z =                    Z 1 0 0 • • • 0 0 Z 2 0 • • • 0 . . . . . . . . . 0 0 . . . Z q 0 0 . . . Zq+1                   $with a structure similar to W and W , replacing r 1 , . . . , r p by s 1 , . . . , s q . As a consequence, U = U T 1 U 2 can be replaced by W T U Z and V by W T V Z. To complete the proof, we need to show that, when (2.1) is an equality, these matrices can be chosen so that W T U Z = Id R n and W T V Z = Id R d .

Let us consider a first step in this direction, assuming that i 1 ≤ j 1 so that

$U [i 1 j 1 ] U T ⌈i 1 j 1 ⌉ = Id R i 1 .$Complete U T ⌈i 1 j 1 ⌉ into a orthogonal matrix Z 1 = [U T ⌈i 1 j 1 ⌉ , Ũ ]. Build a matrix Z as above by taking Z 2 , . . . , Z q+1 equal to the identity. Then U Z has a first i 1 × i 1 block equal to Id R i 1 , which implies that all coefficients on the right and below this block are zeros. If j 1 ≤ i 1 , a similar construction can be made on the other side, letting W 1 = [U ⌈i 1 j 1 ⌉ Ũ ] with the first j 1 × j 1 block of the new matrix U equal to the identity. Note that, since V ⌈i p j q ⌉ = U ⌈i p j q ⌉ , the same result is obtained on V at the same time.

Pursuing this way (and skipping the formal induction argument, which is a bit tedious), we can progressively introduce identity blocks into U and V and transform them into new matrices (that we still denote by U and V ) taking the form (letting k = min(i p , j q ))

$U = Id R k 0 0 Ū and V = Id R k 0 0 V$If k = i p (resp. k = j q ), the final reduction can be obtained by choosing W p+1 = Ū and Wp+1 = V (resp. Z p+1 = Ū T and Zp+1 = V T ), leading to SVDs for A and B with identical matrices

$U 1 = U 2 and V 1 = V 2 .$■ Remark 2.2 Note that, since the singular values of -A and of A coincide, theorem 2.1 implies trace(A T B) ≤ m i=1 λ i µ i . [(2.4)](#) for all matrices A and B, with equality if either A and B or -A and B have an SVD using the same bases. ♦

## Applications

Let p and d be integers with p ≤ d. Let A ∈ S d (R), B ∈ S p (R) be symmetric matrices. We consider the following optimization problem: maximize, over matrices U ∈ M d,p (R) such that U T U = Id R p , the function

$F(U ) = trace(U T AU B) = trace(AU BU T ) .$We first note that the singular values of U BU T , which is d × d, are the same as the eigenvalues of B completed with zeros. Letting λ 1 ≥ • • • ≥ λ d be the eigenvalues of A and µ 1 ≥ • • • ≥ µ p those of B, we therefore have, from theorem 2.1,

$F(U ) ≤ p i=1$λ i µ i .

## APPLICATIONS

Introduce the eigenvalue decompositions of A and B in the form A = V ΛV T and B = W MW T . For F(U ) to be equal to its upper-bound, we know that we must arrange U BU T to take the form

$U BU T = V M 0 0 0 V T .$Use, as before, the notation V ⌈dp⌉ to denote the matrix formed with the p first columns of V . Take U = V ⌈dp⌉ W T , which satisfies U T U = Id R p . We then have

$V ⌈dp⌉ W T BW V T ⌈dp⌉ = V ⌈dp⌉ MV T ⌈dp⌉ = V M 0 0 0. V T ,$which shows that U is optimal. We summarize this discussion in the next theorem.

Theorem 2.3 Let A ∈ S d (R) and B ∈ S p (R) be symmetric matrices, with p ≤ d. Let eigenvalue decompositions of A and B be given by A = V ΛV T and B = W MW T , where the diagonal elements of Λ (resp. M) are

$λ 1 ≥ • • • ≥ λ d (resp. µ 1 ≥ • • • ≥ µ p ).$Define F(U ) = trace(AU BU T ), for U ∈ M d,p (R). Then, max F(U ) :

$U T U = Id R p = p i=1 λ i µ i .$This maximum is attained at U = V ⌈d,p⌉ W T .

The following corollary applies theorem 2.3 with B = diag(µ 1 , . . . , µ p ). µ i e T i Ae i .

Then, the maximum of F over all orthonormal families e 1 , . . . , e p in R d is p i=1 λ i µ i and is attained when e 1 , . . . , e p are eigenvectors of A with eigenvalues λ 1 , . . . , λ p .

The minimum of F over all orthonormal families e 1 , . . . , e p in R d is p i=1 λ d-i+1 µ i and is attained when e 1 , . . . , e p are eigenvectors of A with eigenvalues λ d , . . . , λ d-p+1 .

Proof The statement about the maximum is just a special case of theorem 2.3, with B = diag(µ 1 , . . . , µ p ), noting that the ith diagonal element of U T AU is e T i Ae i where e i is the ith column of U .

The statement about the minimum is deduced by replacing A by -A.

## ■

Applying this corollary with p = 1, we retrieve the elementary result that λ 1 = max{u T Au : |u| = 1} and λ d = min{u T Au : |u| = 1}.

To complete this chapter, we quickly state and prove Rayleigh's theorem. 

$λ k = max V :dim(V )=k min{u T Au, u ∈ V , |u| = 1} = min V :dim(V )=d-k+1 max{u T Au, u ∈ V , |u| = 1}$where the min and max are taken over linear subspaces of R d .

Proof Let e 1 , . . . , e d be an orthonormal basis of eigenvectors of A associated with λ 1 , . . . , λ d . Let, for k ≤ l, W k,l = span(e k , . . . , e l ). Let V be a subspace of dimension k.

Then V ∩ W k,d ∅ (because the sum of the dimensions of these two spaces is d + 1). Taking u 0 with norm 1 in this intersection, we have

$min{u T Au, u ∈ V , |u| = 1} ≤ u T 0 Au 0 ≤ max{u T Au, u ∈ W k,d , |u| = 1} = λ k ,$where the last identity follows by considering the eigenvalues of A restricted to W k,d . So, the maximum of the right-hand side is indeed less than λ k , and it is attained for V = W 1,k . This proves the first identity, and the second one can be obtained by applying the first one to -A.

■

## Some matrix norms

The operator norm of a matrix A ∈ M n,d (R), is defined as

$|A| op = max{|Ax| : x ∈ R d , |x| = 1}.$It is equal to the square root of the largest eigenvalue of A T A, i.e., to the largest singular value of A.

The Frobenius norm of A is

$|A| F = trace(A T A) = d i,j=1$A(i, j) 2 , so that

$|A| F =        m k=1 σ 2 k        1/2$where σ 1 , . . . , σ m are the singular values of A (and m = min(n, d)).

The nuclear norm of A is defined by

$|A| * = d k=1 σ k .$One can prove that this is a norm using an equivalent definition, provided by the following proposition. Proof The fact that trace(U AV T ) ≤ |A| * for any U and V is a consequence of the trace inequality applied with B = [Id, 0] or its transpose depending on whether n ≤ d or not. The upper-bound being attained when U and V are the matrices forming the singular value decomposition of A, the proof is complete.

## ■

The fact that |A| * is a norm, for which the only non-trivial fact was the triangular inequality, now is an easy consequence of this proposition, because the maximum of the sum of two functions is always less than the sum of their maximums. More precisely, we have |A + B| * = max{trace(U AV T ) + trace(U BV T ) :

$U T U = Id, V T V = Id} ≤ max{trace(U AV T ) : U T U = Id, V T V = Id} + max{trace(U BV T ) : U T U = Id, V T V = Id} = |A| * + |B| *$The nuclear norms is also called the Ky Fan norm of order d. Ky Fan norms of order k (for 1 ≤ k ≤ d) associate to a matrix A the quantity

$|A| (k) = λ 1 + • • • + λ k ,$i.e., the sum of its k largest singular values. One has the following proposition.

## Proposition 2.7 The Ky Fan norms satisfy the triangular inequality.

Proof We prove this following the argument suggested in Bhatia [[28]](#b46). For A ∈ M d,d , and k = 1, . . . , d, let trace (k) (A) be the sum of the k largest diagonal elements of A. Let, for a symmetric matrix A, |A| ′ (k) denote the sum of the k largest eigenvalues of A (it is equal to |A| (k) if A is positive definite, but can also include negative values).

Then, for any symmetric matrix A ∈ S d ,

## |A| ′

(k) = max trace (k) (U AU T ) : U ∈ CO d . [(2.5)](#) To show this, assume that V in O d diagonalizes A, so that D = V AV T is a diagonal matrix. Assume, without loss of generality, that the coefficients λ j = D(j, j) are nonincreasing. Fix U ∈ O d , let B = U AU T and W = V U T so that D = W BW T , or B = W T DW . Then, for any j ≤ d,

$B(j, j) = d i=1 W (i, j) 2 D(i, i).$Then, for any 1

$≤ j 1 < • • • < j k ≤ d k l=1 B(j l , j l ) = d i=1 D(i, i) k l=1 W (i, j l ) 2 = k i=1 D(i, i) + k i=1 D(i, i) k l=1 W (i, j l ) 2 -1 + d i=k+1 D(i, i) k l=1 W (i, j l ) 2 = k i=1 D(i, i) + k i=1 (D(i, i) -D(k, k)) k l=1 W (i, j l ) 2 -1 + d i=k+1 (D(i, i) -D(k, k)) k l=1 W (i, j l ) 2 + D(k, k)         n i=1 k j=1 W (i, j l ) 2 -k         .$Because W is orthogonal, we have k l=1 W (i, j l ) 2 ≤ 1 and

$n i=1 k j=1 W (i, j l ) 2 = k.$This shows that the terms after k i=1 D(i, i) in the upper bound are negative or zero, so that

$k l=1 B(j l , j l ) ≤ k i=1 D(i, i).$The maximum of the left-hand side is trace (k) [(B)](#). Noting that we get an equality when choosing U = V , the proof of (2.5) is complete.

Using the same argument as that made above for the nuclear norm, one deduces from this that

$|A + B| ′ (k) ≤ |A| ′ (k) + |B| ′ (k)$for all A, B ∈ S d and all k = 1, . . . , d. Now, let A ∈ M n,d and consider the symmetric matrix

$Ã = 0 A T A 0 ∈ S n+d .$Write a vector u ∈ R n+d as u = u 1 u 2 with u 1 ∈ R d and u 2 ∈ R n . Then u is an eigenvector of Ã for an eigenvalue λ if and only if A T u 2 = λu 1 and Au 1 = λu 2 , which implies that A T Au 1 = λ 2 u 1 and λ 2 is a singular value of A. Conversely, if µ is a nonzero singular value of A, associated with eigenvector u 1 , then

1/ √ µ and -1/ √ µ are eigenvalues of Ã, associated with eigenvectors u 1 ±Au 1 / √ µ . It follows from this that |A| (k) = | Ã| ′ (k) for k ≤ min(n, d) and therefore satisfies the triangle inequality. ■

We refer to [[28]](#b46) for more examples of matrix norms, including, in particular those provided by taking pth powers in Ky Fan's norms, defining

$|A| (k,p) = (λ p 1 + • • • + λ p k ) 1/p .$Chapter 3

## Introduction to Optimization

This chapter summarizes some fundamental concepts in optimization that will be used later in the book. The reader is referred to textbooks, such as Beck [[22]](#), Eiselt et al. [[68]](#b86), Nocedal and Wright [[146]](#b164), Boyd et al. [[40]](#b58) and many others for proofs and deeper results.

## Basic Terminology

1. If I is a subset of R, a lower bound of I is an element u ∈ [-∞, +∞] such that u ≤ x for all x ∈ I. Among these lower bounds, there exists a largest element, denoted inf I ∈ [-∞, +∞], called the infimum of I (by convention, the infimum of an empty set is +∞). Similarly, one defines the supremum of I, denoted sup I, as the smallest upper bound of I (and the supremum of an empty set is -∞). Every set in R has an infimum and a supremum, but these numbers do not necessarily belong to I. When they do, they are respectively called minimal and maximal elements of I, and are denoted min I and max I. So, the statement "u = min I" means u ∈ I and u ≤ v for all v ∈ I.

2. If F : Ω → R is a real-valued function defined on a subset Ω ⊂ R d , the infimum of F over Ω is defined by inf

$Ω F = inf{F(x) : x ∈ Ω}$and its supremum is sup

$Ω F = sup{F(x) : x ∈ Ω}.$As seen above both numbers are well defined, and can take infinite values. One says that x ∈ Ω is a (global) minimizer (resp. maximizer) of F if F(y) ≥ F(x) (resp. F(y) ≤ F(x)) for all y ∈ Ω. One also says that F reaches its minimum (resp. maximum), or is minimized (resp. maximized) at x. Equivalently, x is a minimizer (resp. maximizer) of F if and only if x ∈ Ω and F(x) = min{F(y) : y ∈ Ω} (resp. max{F(y) : y ∈ Ω}).

In such cases, one also writes F(x) = min Ω F or F(x) = max Ω F. In particular, the notation u = min Ω F indicates that u = inf Ω F and that there exists an x in Ω such that F(x) = u (i.e., that the infimum of F over Ω is realized at some x ∈ Ω). Note that the infimum of a function always exists, but not necessarily its minimum. Also note that minimizers, when they exist, are not necessarily unique. We will denote by argmin Ω F (resp. argmax Ω F) the (possibly empty) set of minimizers (resp. maximizers) of F 3. One says that x is a local minimizer (resp. maximizer) of F on Ω if there exists an open ball B ⊂ R d such that x ∈ B and F(x) = min Ω∩B F (resp. F(x) = max Ω∩B F).

4. An optimization problem consists in finding a minimizer or maximizer of an "objective function" F. Focusing from now on on minimization problems (statements for maximization problems are symmetric), we will always implicitly assume that a minimizer exists. The following provides some general assumptions on F and Ω that ensure this fact.

The sublevel sets of F in Ω are denoted [F ≤ u] Ω (or simply [[F ≤ u]](#) when

$Ω = R d ) for u ∈ [-∞, +∞] with [F ≤ u] Ω = {x ∈ Ω : F(x) ≤ u} .$
## Note that argmin

$Ω F = u>inf F [F ≤ u] Ω .$A typical requirement for F is that its sublevel sets are closed in R d , which means that, if a sequence (x n ) in Ω satisfies, for some u ∈ R, F(x n ) ≤ u for all n and converges to a limit x, then x ∈ Ω and F(x) ≤ u. If this is true, one says that F is lower semicontinuous, or l.s.c, on Ω. If, in addition to being closed, the sublevel sets of F are bounded (at least for u small enough-larger than inf F), then argmin Ω F is an intersection of nested compact sets, and is therefore not empty (so that the optimization problem has at least one solution).

5. Different assumptions on F and Ω lead to different types of minimization problems, with specific underlying theory and algorithms.

1. If F is C 1 or smoother and Ω = R d , one speaks of an unconstrained smooth optimization problem. 2. For constrained problems, Ω is often specified by a finite number of inequalities, i.e., Ω = {x ∈ R d : γ i (x) ≤ 0, i = 1, . . . , q}.

If F and all functions γ 1 , . . . , γ q are C 1 one speaks of smooth constrained problems.

3. If Ω is a convex set (i.e., x, y ∈ Ω ⇒ [x, y] ∈ Ω, where [x, y] is the closed line segment connecting x and y) and F is a convex function (i.e., F((1t)x + ty) ≤ (1t)F(x) + tF(y) for all x, y ∈ Ω), one speaks of a convex optimization problem. [4](#b22). Non-smooth problems are often considered in data science, and lead to interesting algorithms and solutions.

5. When both F and γ 1 , . . . , γ q are affine functions, one speaks of a linear programming problem (or a linear program). (An affine function is a mapping

$x → b T x + β, b ∈ R d , β ∈ R.)$If F is quadratic (F(x) = 1  2 x T Axb T x), and all γ i 's are affine, one speaks of a quadratic programming problem.

6. Finally, some machine learning problems are specified over discrete or finite sets Ω (for example Z d , or {0, 1} d ), leading to combinatorial optimization problems. (

## Unconstrained Optimization Problems

The following result summarizes (non-identical) necessary and sufficient conditions that are applicable to such a solution.

Theorem 3.1 Necessary conditions. Assume that F is differentiable over Ω, and that x * is a local minimum of F. Then ∇F(x * ) = 0.

If F is C 2 , then, in addition, ∇ 2 F(x * ) must be positive semidefinite.

Sufficient conditions. Assume that F ∈ C 2 (Ω). If x * ∈ Ω is such that ∇F(x * ) = 0 and ∇ 2 F(x * ) is positive definite, then x * is a local minimum of F.

## Proof Necessary conditions:

Since Ω is open, it contains an open ball centered at x * , with radius ϵ 0 and therefore all segments [x * , x * + ϵh] for all ϵ ∈ [0, ϵ 0 ] and all unit norm vectors h. Since x * is a local minimum, we can choose ϵ 0 so that F(x * + ϵh) ≥ F(x * ) for all h with |h| = 1.

Using Taylor formula, we get (for ϵ ∈ [0, ϵ 0 ], |h| = 1)

$0 ≤ F(x * + ϵh) -f (x * ) = ϵ 1 0 dF(x * + tϵh)hdt .$If dF(x * )h 0 for some h, then, for small enough ϵ, dF(x * + tϵh)h cannot change sign for t ∈ [0, 1] and therefore 1 0 dF(x * + tϵh)hdt has the same sign as dF(x * )(h) which must therefore be positive. But the same argument can be made with h replaced by -h, implying that dF(x * )(-h) = -dF(x * )h is also positive, and this gives a contradiction. We therefore have dF(x * )(h) = 0 for all h, i.e., ∇F(x * ) = 0. Assume that F is C 2 . Then, making a second-order Taylor expansion, one gets

$0 ≤ F(x * + ϵh) -F(x * ) = ϵ 2 1 0 (1 -t)d 2 F(x * + tϵh)(h, h)dt.$The same argument as above shows that, if d 2 F(x * )(h, h) 0, then it must be positive. This shows that d 2 F(x * )(h, h) ≥ 0 for all h and d 2 F(x * ) (or its associated matrix ∇ 2 F(x * )) is positive semidefinite. Now, assume that F is C 2 and ∇ 2 F(x * ) positive definite. One still has

$F(x * + ϵh) -F(x * ) = ϵ 2 1 0 (1 -t)d 2 F(x * + tϵh)(h, h)dt$If ∇ 2 F(x * ) ≻ 0, then ∇ 2 F(x * + tϵh) ≻ 0 for small enough ϵ, showing the the r.h.s. of the identity is positive for h 0, and that F(x * + ϵh) > F(x * ).

## ■

Because maximizing F is the same as minimizing -F, necessary (resp. sufficient) conditions for optimality in maximization problems are immediately deduced from the above: it suffices to replace positive semidefinite (resp. positive definite) by negative semidefinite (resp. negative definite).

## Convex sets and functions

Definition 3.2 One says that a set Ω ⊂ R d is convex if and only if, for all x, y ∈ Ω, the closed segment [x, y] also belongs to Ω.

A function F : R d → (-∞, +∞] is convex if, for all λ ∈ [0, 1] and all x, y ∈ R d , one has

$F((1 -λ)x + λy) ≤ (1 -λ)F(x) + λF(y). (3.2)$If, whenever the lower bound is not infinite, the inequality above is strict for λ ∈ (0, 1), one says that F is strictly convex.

Note that, with our definition, convex functions can take the value +∞ but not -∞. In order for the upper-bound to make sense when F takes infinite values, one makes the following convention: a + (+∞) = +∞ for any a ∈ (-∞, +∞]; λ • (+∞) = +∞ for any λ > 0; 0 • (+∞) is not defined but 0 • (+∞) + (+∞) = +∞. Definition 3.3 The domain of F, denoted dom(F) is the set of x ∈ R d such that F(x) < ∞.

One says that F is proper if dom(F) ∅.

We will only consider proper convex functions in our discussions, which will simply be referred to as convex functions for brevity. [Proposition 3.4](#) If F is a convex function, then dom(F) is a convex subset of R d . Conversely, if Ω is a convex set and F satisfies (3.2) for all x, y ∈ Ω (i.e., F is convex on Ω), then the extension F defined by F(x) = F(x) if x ∈ Ω and F(x) = +∞ is a convex function defined on R d (such that dom( F) = Ω).

Proof The first statement is a direct consequence of (3.2), which implies that F is finite on [x, y] as soon as it is finite at x and at y. For the second statement, (3.2) for F is true for x, y ∈ Ω, since it is true for F, and the uper-bound is +∞ otherwise. ■ This proposition shows that there was no real loss of generality in requiring convex functions to be defined on the full space R d . Note also that the upper bound in [(3.2)](#) is infinite unless both x and y belong to dom(F), so that the inequality only needs to be checked in that case.

One says that a function F is concave if and only if -F is convex. All definitions and properties made for convex functions then easily transcribe into similar statements for concave functions. We say that a function f : I → (-∞, +∞] (where I is an interval) is non-decreasing if, for all x, y ∈ I, x < y implies f (x) ≤ f (y). We say that f is increasing if if, for all x, y ∈ I, x < y implies f (x) < f (y) if f (x) < ∞ and f (y) = ∞ otherwise.

Inequality [(3.](#)2) has important consequences on minimization problems. For example, it implies the following proposition. Proposition 3.5 Let F be a convex (resp. strictly convex) function on R d . If x ∈ dom(F) and y ∈ R d , the function

$λ ∈ (0, 1] → 1 λ (F((1 -λ)x + λy) -F(x)) (3.3)$is non-decreasing (resp. increasing).

Conversely, let Ω ⊂ R d be a convex set and F : Ω → (-∞, +∞) be a function such that the expression in [(3.3](#)) is non-decreasing (resp. increasing) for all x ∈ dom(F) and y ∈ R d .

Then, the extension F of F defined in proposition 3. [4](#b22)

## is convex (resp. strictly convex).

Proof Let f (λ) = (F((1λ)x + λy) -F(x))/λ. Let µ ≤ λ denote z λ = (1λ)x + λy, z µ = (1µ)x + µy. One has z µ = (1ν)x + νz λ , with ν = µ/λ, so that F(z µ ) ≤ (1µ/λ)F(x) + (µ/λ)F(z λ ) .

The coordinates (λ (0) , . . . , λ (k) ) are uniquely associated to x ∈ aff(Ω) and depend continuously on x. They are indeed obtained by solving the linear system xx 0 = λ (1) 

$(x 1 -x 0 ) + • • • + λ (k) (x k -x 0 )$which has a unique solution for x ∈ aff(Ω) by linear independence. To see continuity, one can introduce the k × k matrix G with entries G (ij) given by the inner products (x i -x 0 ) T (x j -x 0 ) and the vector h(x) ∈ R k with entries h (j) (x) = (xx 0 ) T (x jx 0 ). Continuity is then clear since λ = G -1 h(x). [Definition 3.7](#) If Ω is a convex set, then its relative interior, denoted relint(Ω), is the set of all x ∈ Ω such that there exists ϵ > 0 such that aff(Ω) ∩ B(x, ϵ) ⊂ Ω.

We have the following important property. [Proposition 3.8](#) Let Ω be a nonempty convex set. If x ∈ relint(Ω) and y ∈ Ω, then x λ = (1λ)x + λy ∈ relint(Ω) for all λ ∈ [0, 1).

## Moreover relint(Ω) is a nonempty convex set.

Proof Take ϵ such that B(x, ϵ) ∩ aff(Ω) ⊂ Ω. Take any z ∈ B(x λ , (1λ)ϵ) ∩ aff(Ω). Define z such that z = (1λ)z + λy, i.e. z = zλy 1λ .

Then z ∈ aff(Ω) and

$|z -x| = |z -x λ | 1 -λ < ϵ$so that z, and therefore z belongs to Ω. This proves that B(x λ , (1λ)ϵ) ∩ aff(Ω) ⊂ Ω so that x λ ∈ relint(Ω).

If both x and y belong to relint(Ω), then x λ ∈ relint(Ω) for λ ∈ [0, 1], showing that this set is convex.

We now show that relint(Ω) ∅. Let k be the dimension of aff(Ω), so that there exist x 0 , x 1 , . . . , x k ∈ Ω such that x 1x 0 , . . . , x kx 0 are linearly independent. Consider the "simplex" S = {λ (0) x 0 + • • • + λ (k) x k :, λ (0) + • • • + λ (k) = 1, λ (j) ≥ 0, j = 0, . . . , k}, which is included in Ω. Then the average x = (x 0 + • • • + x k )/(k + 1) is such that B(x, ϵ) ∩ aff(Ω) ⊂ S for small enough ϵ. Otherwise, there would exist a sequence (k) (n) = 1 and at least one λ (j) (n) < 0 that converges to x. Let y j be the set of elements in this sequence such that λ (j) (n) < 0. This set is infinite for at least one j and provides a subsequence of y that also converges to x. But this would imply that the j th barycentric coordinate, which depends continuously on x, is non-positive, which is a contradiction.

$y(n) = λ (0) (n)x 0 + • • • + λ (k) (n)x k such that λ (0) (n) + • • • + λ$We therefore have x ∈ relint(Ω), which completes the proof.

## ■

The following proposition provides an equivalent definition of the relative interior.

Proposition 3.9 If Ω is a convex set, then relint(Ω) = {x ∈ Ω : ∀y ∈ Ω, ∃ϵ > 0 such that xϵ(yx) ∈ Ω} .

(3.4) So x belongs in the relative interior of Ω if, for all y ∈ Ω, the segment [x, y] can be extended on the x side and still remain included in Ω.

Proof Let A be the set in the r.h.s. of [(3.4)](#). The proof that relint(Ω) ⊂ A is straightforward and left to the reader. We consider the reverse inclusion.

Let x ∈ A, and let y ∈ relint(Ω), which is not empty. Then, for some ϵ > 0, we have z = xϵ(yx) ∈ Ω.

## Since

x = 1 1 + ϵ (ϵy + z), proposition 3.8 implies that x ∈ relint(Ω).

## ■

Convex functions have important regularity properties in the relative interior of their domain, that we will denote ridom(F). Importantly: ridom(F) = relint(dom(F)) int(dom(F)).

A first such property is provided by the next proposition. Proposition 3.10 Let F be a convex function. Then F is locally Lipschitz continuous on ridom(F), i.e., for every compact subset C ⊂ ridom(F), there exists a constant L > 0 such that |F(x) -F(y)| ≤ L|x -y| for all x, y ∈ C. This implies, in particular, that F is continuous on ridom(F).

Proof Take x ∈ ridom(F). Let K = h ∈ --→ aff (dom(F)), |h| = 1 . Then, the segment [xah, x + ah] is included in ridom(F) for small enough a and all h ∈ K. Since F is convex, we have, for t ≤ a, F(x + th) -F(x) ≤ t a (F(x + ah) -F(x))

Writing x = λ(xah) + (1λ)(x + th) with λ = t/(t + a), we also have

$F(x) ≤ t t + a (F(x -ah) + a t + a F(x + th))$which can be rewritten as

$F(x) -F(x + th) ≤ t a (F(x -ah) -F(x)).$These two inequalities show that F is continuous at x along any direction in --→ aff (dom(F)), which implies that F is continuous at x. Given this, the differences F(x+ah)-F(x) are bounded over the compact set C, by some constant M and, the previous inequalities show that

$|F(y) -F(x)| ≤ M a |x -y| if y ∈ ridom(F), |y -x| ≤ a. ■$
## Derivatives of convex functions and optimality conditions

The following theorem provides a stronger version of optimality conditions for the minimization of differentiable convex functions. Note that we have only defined differentiability of functions defined over open sets.

Theorem 3.11 Let F be a convex function, with int(dom(F)) ∅. Assume that x ∈ int(dom(F)) and that F is differentiable at x. Then, for all y ∈ R d : ∇F(x) T (yx) ≤ F(y) -F(x) .

(3.5)

If F is strictly convex, the inequality is strict for y x. In particular, ∇F(x) = 0 implies that x is a global minimizer of F. It is the unique minimizer if F is strictly convex.

Conversely, if F is C 1 on an open convex set Ω and satisfies (3.5) for all x, y ∈ Ω, then F is convex.

$Proof Equation (3.3) implies 1 λ (F((1 -λ)x + λy) -F(x)) ≤ F(y) -F(x), 0 < λ ≤ 1.$Taking the limit of the lower bound for λ → 0, λ > 0 yields [(3.5)](#). If F is strictly convex, the inequality is strict for λ < 1 and, since the l.h.s. is increasing in λ, it remains strict when λ ↓ 0.

Conversely, assuming (3.5) for all x, y ∈ Ω, the derivative of λ → 1 λ (F((1λ)x + λy) -F(x)) is 1 λ 2 (λ∇F(x + λh) T h -F(x + λh) + F(x)) with h = yx, which is non-negative by [(3.5)](#). This proves that F is convex. If [(3.5)](#) holds with a strict inequality, then the derivative is positive and 1 λ (F((1λ)x + λy) -F(x)) is increasing.

## ■

The next proposition describes C 2 convex functions in terms of their second derivatives. Proposition 3.12 Let F be convex and twice differentiable at x ∈ int(dom(F)). Then ∇ 2 F(x) is positive semi-definite.

Conversely, assume that Ω = dom(F) is an open set and that F is C 2 on Ω with a positive semi-definite second derivative. Then F (or, rather, its extension F) is convex. If the second derivative is everywhere positive definite, then F is strictly convex.

Proof Using Taylor formula (1.10) at order 2, we get, for any h ∈ R d with |h| = 1,

$1 2 d 2 F(x)(h, h) = 1 2t 2 d 2 F(x)(th, th) = 1 t 2 (F(x + th) -F(x) -t∇F(x) T h) + ϵ(t) ≥ ϵ(t)$with ϵ(t) → 0 when t → 0, the last inequality deriving from [(3.5)](#). This shows that d 2 F(x)(h, h) ≥ 0.

To prove the second statement, assume that F is C 2 and ∇ 2 F is positive semidefinite everywhere. Then [(1.8)](#) implies

$F(y) -F(x) -∇F(x) T (y -x) = 1 2 (y -x) T ∇ 2 F(z)(y -x)$for some z ∈ [x, y]. Since the r.h.s. is non-negative, (3.5) holds. If ∇ 2 F is positive definite everywhere, then the r.h.s. is positive if y x and (3.5) holds with a strict inequality.

## ■

If F is C 2 and ∇ 2 F is positive definite and strictly convex, then [(1.8)](#) implies that, for some z ∈ [x, y],

$F(y) -F(x) -∇F(x) T (y -x) = 1 2 (y -x) T ∇ 2 F(z)(y -x) ≥ ρ min (∇ 2 F(z)) 2 |y -x| 2$where ρ min (A) denotes the smallest eigenvalue of A. If this smallest eigenvalue is bounded from below away from zero, there exists a constant m > 0 such that

$F(y) -F(x) -∇F(x) T (y -x) - m 2 |y -x| 2 ≥ 0. (3.6)$This property is captured by the following definition, which does not require F to be C 2 .

$Definition 3.13 A C 1 function F is strongly convex if 1. int(dom(F)) ∅ 2.$There exists m > 0 such that [(3.6](#)) holds for all x ∈ int(dom(F)) and y ∈ R d .

We have the following proposition.

Proposition 3.14 If F is strongly convex, then it is strictly convex, so that, in particular argmin F has at most one element.

If dom(F) = R d , then argmin F is not empty.

Proof The first part is a direct consequence of (3.6) and theorem 3.11.

For the second part, (3.6) implies that F.

$F(x) -F(0) ≥ ∇F(0) T x + m 2 |x| 2 ≥ |x| m 2$The set in the r.h.s. involves the minimization of a continuous function on a compact set, and is therefore not empty.

## ■

We will use the following definition. If F is L-C k , then Taylor formula ((1.9)) implies

$f (x + h) -f (x) -df (x)h - 1 2 d 2 f (x)(h, h) -• • • - 1 k! d k f (x)(h, . . . , h) ≤ L|h| k+1 (k + 1)! (3.7)$for which we used the fact that

$1 0 t(1 -t) k-1 dt = 1 0 (1 -t) k-1 dt - 1 0 (1 -t) k dt = 1 k - 1 k + 1 = 1 k(k + 1)$.

If F is strongly convex and is, in addition, L-C 1 for some L, then using (3.7), one gets the double inequality, for all x, y ∈ int(dom(F)):

$m 2 |y -x| 2 ≤ F(y) -F(x) -∇F(x) T (y -x) ≤ L 2 |y -x| 2 . (3.8)$The following proposition will be used later.

Proposition 3.16 Assume that F is strongly convex, satisfying [(3.6)](#), and that argmin F = {x * } with x * ∈ int(dom(F)). Then, for all x ∈ int(dom(F)):

$m 2 |x -x * | 2 ≤ F(x) -F(x * ) ≤ 1 2m |∇F(x)| 2 (3.9)$Proof Since ∇F(x * ) = 0, the first inequality is a consequence of (3.6) applied to x = x * . Switching the role of x and x * , we have

$F(x * ) -F(x) -∇F(x) T (x * -x) ≥ m 2 |x -x * | 2 so that 0 ≤ F(x) -F(x * ) ≤ -∇F(x) T (x * -x) - m 2 |x -x * | 2 ≤ |∇F(x)| |x -x * | - m 2 |x -x * | 2 (3.10)$The maximum of the r.h.s. with respect to |xx * | is attained at |∇F(x)|/m, showing that

$F(x) -F(x * ) ≤ 1 2m |∇F(x)| 2 ,$which is the second inequality. 

## Direction of descent and steepest descent

Gradient-based algorithms for optimization iteratively update the variable x, creating a sequence governed by an equation taking the form x t+1 = x t + α t h t with α t > 0 and h t ∈ R d . To ensure that the objective function F decreases at each step, h t is chosen to be a direction of descent for F at x t , a notion which, as seen below, is closely connected with the direction of ∇F(x t ).

Definition 3. [17](#b35) Let Ω be open in R d and F : Ω → R be a C 1 function. A direction of descent for F at x ∈ Ω is a vector h 0 ∈ R d such that there exists ϵ 0 > 0 such that F(x + ϵh) < F(x) for all ϵ ∈ (0, ϵ 0 ].

Proposition 3.18 Assume that F : Ω → R is C 1 and take x ∈ Ω. Then any direction h such that h T ∇F(x) < 0 is a direction of descent for F at x. Conversely, if h is a direction of descent, then h T ∇F(x) ≤ 0.

Proof We have the first-order expansion F(x+ϵh)-F(x) = ϵh T ∇F(x)+o(ϵ). If h T ∇F(x) < 0, the r.h.s. is negative for small enough ϵ and h is a direction of descent. Similarly, if h T ∇F(x) > 0, the r.h.s. is positive for small enough ϵ and h cannot be a direction of descent.

## ■

In particular, h = -∇F(x) is always a direction of descent. It is called the steepest descent direction because it minimizes h → ∂ α F(x + αh)| α=0 over all h such that |h| 2 = 1. However, this designation has a character of optimality that may be misleading, because using the Euclidean norm for the condition |h| 2 = 1 is not necessarily adapted to the optimization problem at hand. In the absence of additional information on the problem, it does have a canonical nature, as it is (up to rescaling) the only norm invariant to rotations (including permutations) of the coordinates. Such invariance is not necessarily desirable when the variable x has a known structure (e.g., it is organized on a graph) which would be broken by permutation. Also, steepest refers to a local "greedy" evaluation, but may not be optimal from a global perspective. A simple example to illustrate this is the case of a quadratic function

$F(x) = 1 2 x T Ax -b T x$where A ∈ S ++ n is a positive definite symmetric matrix. Then ∇F(x) = Axb, but one may argue that ∇ A F(x) = A -1 ∇F(x) (defined in [(1.3)](#)) is a better choice, because it allows the algorithm to reach the minimizer of F in one step, since x -∇ A F(x) = A -1 b (this statement disregards the cost associated in solving the system Ax = b, which can be an important factor in large dimension). Importantly, if F is any C 1 function, and A ∈ S ++ n , the minimizer of h → ∂ α F(x + αh)| α=0 over all h such that h T Ah = 1 is given by -∇ A F(x), i.e., -∇ A F(x) is the steepest descent for the norm associated with A. This yields a general version of steepest descent methods, iterating

$x t+1 = x t -α t ∇ A t F(x t )$with α t > 0 and A t ∈ S ++ n .

One can also notice that ∇ A F(x) is also a minimizer of

$F(x) + ∇F(x) T h + 1 2 h T Ah.$When ∇ 2 F(x) is positive definite, it is then natural to choose it as the matrix A, therefore taking h = -∇ 2 F(x) -1 ∇F(x). This provides Newton's method for optimization. However, Newton method requires computing second derivatives of F, which can be computationally costly. It is, moreover, not a gradient-based method, which is the focus of this discussion.

## Convergence

We now consider a descent algorithm

x t+1 = x t + α t h t [(3.11)](#) where h t is a direction of descent at x t for the objective function F. To ensure convergence, suitable choices for the direction of descent and the step must be made at each iteration, and some assumptions on the objective function are needed.

Regarding the direction of descent, which must satisfy h T k ∇F(x k ) ≤ 0, we will assume a uniform control away from orthogonality to the gradient, with the condition

$-h T t ∇F(x t ) ≥ ϵ|h t | |∇F(x t )| (3.12a)$for some fixed ϵ > 0. Without loss of generality (given that a multiplicative step α t must also be chosen), we assume that h t is commensurable to the gradient, namely, that

$γ 1 |∇F(x t )| ≤ |h t | ≤ γ 2 |∇F(x t )| (3.12b)$for fixed 0 < γ 1 ≤ γ 2 . If h t = ∇ A t F, these assumptions are satisfied as soon as the smallest and largest eigenvalues of A t are controlled along the trajectory.

We have the following proposition.

Proposition 3.19 Assume that F is L-C 1 . Assume that x t satisfies [(3.11)](#) and that (3.12a) and (3.12b) hold. Then, there exist constants ᾱ > 0 and C > 0 that depends on γ 1 , γ 2 and ϵ, such that, for α t ≤ ᾱ, one has

$F(x t+1 ) -F(x t ) ≤ -Cα t |∇F(x t )| 2 . (3.13)$Proof Applying (3.7) to x t and x t+1 , we get

$F(x t+1 ) -F(x t ) -α t ∇F(x t ) T h t ≤ L 2 α 2 t |h t | 2$Using (3.11) and (3.12a), this gives 2 . It suffices to take ᾱ = ϵγ 1 /Lγ 2  2 and C = ϵγ 1 /2 to obtain (3.13).

$F(x t+1 ) -F(x t ) + α t ϵγ 1 |∇F(x t )| 2 ≤ L 2 α 2 t γ 2 2 |∇F(x t )| 2 so that F(x t+1 ) -F(x t ) ≤ -α t ϵγ 1 -α t γ 2 2 L/2 |∇F(x t )|$■ Iterating (3.13) for t = 1 to t = T -1 yields

$T t=1 α t |∇F(x t )| 2 ≤ 1 C (F(x 1 ) -F(x T )).$If F is bounded from below, and one takes α t = ᾱ for all t, one deduces that min |∇F(x t )| 2 : t = 1, . . . , T ≤ F(x 1 )inf F CT ᾱ .

We can deduce from this, for example, that there exists a sequence

$t 1 < • • • < t n < • • • such that ∇F(x t k ) → 0 when k → ∞.$In particular, if one runs [(3.11)](#) until |∇F(x t )| is smaller than a given tolerance level (which is standard), the procedure is guaranteed to terminate in a finite number of steps.

Stronger results may be obtained under stronger assumptions on F and on the algorithm. The first assumption is an inequality similar to [(3.13)](#) and requires that, for some constant C > 0,

$F(x t+1 ) -F(x t ) ≤ -C|∇F(x t )| 2 .$(3.14)

Such an inequality can be deduced from [(3.13)](#) under the additional assumption that α t is bounded from below and we will discuss later line search strategies that ensure its validity. The second assumption is that F is convex.

Theorem 3.20 Assume that F is convex and finite and that its sub-level set [F ≤ F(x 0 )] is bounded. Assume that argmin F is not empty and let x * be a minimizer of F. If [(3.14)](#) is true, then

$F(x t ) -F(x * ) ≤ R 2 C(t + 1) with R = max{|x -x * | : F(x) ≤ F(x 0 )}.$Proof Note that the algorithm never leaves [F ≤ F(x 0 )]. We have

$F(x t+1 ) -F(x * ) ≤ F(x t ) -F(x * ) -C|∇F(x t )| 2 .$Moreover, by convexity, F(x * ) -F(x t ) ≥ ∇F(x t ) T (x *x t ), so that F(x t ) -F(x * ) ≤ ∇F(x t ) T (x tx * ) ≤ |∇F(x t )|R.

Combining these two inequalities, we get

$F(x t+1 ) -F(x * ) ≤ F(x t ) -F(x * ) - C R 2 (F(x t ) -F(x * )) 2 .$Introducing δ t = (C/R 2 )(F(x t ) -F(x * )), this inequality implies

$δ t+1 ≤ δ t (1 -δ t ) .$Taking inverses, we get 1 δ t+1

$≥ 1 δ t + 1 1 -δ t ≥ 1 δ t + 1$which implies 1 δ t ≥ t + 1 or δ t ≤ 1/(t + 1), which in turn implies the statement of the theorem.

## ■

A faster convergence rate can be obtained if F is assumed to be strongly convex. Indeed, if [(3.6](#)) and [(3.14)](#) are satisfied, then (using proposition 3.16), F(x t+1 ) -F(x * ) ≤ F(x t ) -F(x * ) -C|∇F(x t )| 2  ≤ F(x t ) -F(x * ) -2Cm(F(x t ) -F(x * )) = (1 -2Cm)(F(x t ) -F(x * )) .

We therefore get the proposition:

Proposition 3.21 If F is finite and satisfies [(3.6)](#), and if the descent algorithm satisfies [(3.14)](#), then F(x t ) -F(x * ) ≤ (1 -2Cm) t (F(x 0 ) -F(x * )).

## Line search

Proposition 3.19 states that, to ensure that [(3.14)](#) holds, it suffices to take a small enough step parameter α. However, the values of α that are acceptable depend on properties of the objective function that are rarely known in practice. Moreover, even if a valid choice is determined (this can sometimes be done in practice by trial and error), setting a fixed value of α for the whole algorithm is often too conservative, as the best α when starting the algorithm may be different from the best one close to convergence.

For this reason, most gradient descent procedures select a parameter α t at each step using a line search. Given a current position and direction of descent h, a line search explores the values of F(x + αh), α ∈ (0, α max ] in order to discover some α * that satisfies some desirable properties. We will assume in the following that x and h satisfy (3.12a) and (3.12b) for fixed ϵ, γ 1 , γ 2 .

One possible strategy is to define α * as a minimizer of the scalar function f h (α) = F(x + αh) over (0, α max ] for a given upper-bound ϵ max . This can be implemented using, e.g., binary or ternary search algorithms, but such algorithms would typically require a large number of number of evaluations of the function F, and would be too costly to be run at each iteration of a gradient descent procedure.

Based on the previous convergence study, we should be happy with a line search procedure that ensures that [(3.14)](#) is satisfied for some fixed value of the constant C. One such condition is the so-called Armijo rule that requires (with a fixed, typically small, value of c 1 > 0): f h (α) ≤ f h (0) + c 1 αh T ∇f (x) . [(3.15)](#) We know that, under the assumptions of proposition 3.19, this condition can always be satisfied with a small enough value of α. Such a value can be determined using a "backtracking procedure," which, given α max and ρ ∈ (0, 1), takes α = ρ k α max where k is the smallest integer such that [(3.15)](#) is satisfied. This value of k is then determined iteratively, trying α max , ρα max , ρ 2 α max , . . . until [(3.15)](#) is true (this provides the "backtracking method").

A stronger requirement in the line search is to ensure that ∂f h (α) is not "too negative" since one would otherwise be able to further reduce f h by taking a larger value of α. This leads to the weak Wolfe conditions, which combine the Armijo's rule in [(3.15)](#) and showing that α ≥ (1c 2 )ϵ/(Lγ 2 2 ). Moreover

$∂f h (α) = h T ∇F(x + αh) ≥ c 2 h T ∇F(x)(3$$F(x + αh) ≤ F(x) + c 1 αh T ∇f (x) ≤ F(x) -c 1 αϵ|∇F(x)| 2 so that F(x + αh) ≤ F(x) - c 1 (1 -c 2 )ϵ 2 Lγ 2 2 |∇F(x)| 2 .$We have just proved the following proposition.

Proposition 3.22 Assume that F is L-C 1 and that (3.12a), (3.12b), [(3.15](#)) and (3.16a) are satisfied. Then there exists C > 0, depending only of L, ϵ, γ 2 , c 1 and c 2 such that

$F(x + αh) ≤ F(x) -C|∇F(x)| 2 .$The Wolfe conditions can always be satisfied by some α as soon as F is C 1 and bounded from below, and h T ∇F(x) < 0. The next proposition shows this result for the weak condition, while providing an algorithm finding an α that satisfies it in a finite number of steps. Proposition 3.23 Let f : α → f (α) be a C 1 function defined on [0, +∞) such that f is bounded from below and ∂ α f (0) < 0. Let 0 < c 1 < c 2 < 1.

Let α 0,0 = α 0,1 = 0 and α 0 > 0. Define recursively sequences α n,0 , α n,1 and α n as follows.

(i) If f (α n ) ≤ f (0) + c 1 α n ∂ α f (0) and ∂f (α n ) ≥ c 2 ∂ α f (0) stop the construction.

(ii) If f (α n ) > f (0) + c 1 α n ∂ α f (0) let α n+1 = (α n + α n,0 )/2, α n+1,1 = α n and α n+1,0 = α n,0 .

$(iii) If f (α n ) ≤ f (0) + c 1 α n ∂ α f (0) and ∂f (α n ) < c 2 ∂ α f (0):$(a) If α n,1 = 0, let α n+1 = 2α n , α n+1,0 = α n and α n+1,1 = α n,1 . (b) If α n,1 > 0, let α n+1 = (α n + α n,1 )/2, α n+1,0 = α n and α n+1,1 = α n,1 .

Then the sequences are always finite, i.e., the algorithm terminates in a finite number of steps.

Proof Assume, to get a contradiction, that the algorithm runs indefinitely, so that case (i) never occurs. If case (ii) never occurs, then one runs step (iii-a) indefinitely, so that α n → ∞ with f (α n ) ≤ f (0) + c 1 α n ∂ α f (0), and f cannot be bounded from below, yielding a contradiction. As soon as case (ii) occurs, we have, at every step, α n,0 ≥ α n-1,0 , α n,1 ≤ α n-1,1 , α n ∈ [α n,0 , α n,1 ], f (α n,1 ) > f (0) + c 1 α n,1 ∂ α f (0), f (α n,0 ) ≤ f (0) + c 1 α n,0 ∂ α f (0) and ∂f (α n,0 ) < c 2 ∂ α f (0). This implies that f (α n,1 )f (α n,0 ) > c 1 (α n,1α n,0 )∂ α f (0). Moreover, the updates imply that (α n+1,1α n+1,0 ) = (α n,1α n,0 )/2. This requires that the three sequences α n , α n,0 and α n,1 converge to the same limit, α. We have

$∂ α f (α) = lim n→∞ f (α n,1 ) -f (α n,0 ) α n,1 -α n,0 ≥ c 1 ∂ α f (0) and ∂ α f (α) = lim n→∞ ∂ α f (α n,0 ) ≤ c 2 ∂ α f (0) yielding c 1 ∂ α f (0) ≤ c 2 ∂ α f (0) which is impossible since c 2 > c 1 and ∂ α f (0) < 0.$
## ■

The existence of α satisfying the strong Wolfe condition is a consequence of the following proposition, which also provides an algorithm. Proposition 3.24 Let f : α → f (α) be a C 1 function defined on [0, +∞) such that f is bounded from below and ∂ α f (0) < 0. Let 0 < c 1 < c 2 < 1.

Let α 0,0 = α 0,1 = 0 and α 0 > 0. Define recursively sequences α n,0 , α n,1 and α n as follows.

$(i) If f (α n ) ≤ f (0) + c 1 α n ∂ α f (0) and |∂ α f (α n )| ≤ c 2 |∂ α f (0)| stop the construction. (ii) If f (α n ) > f (0) + c 1 α n ∂ α f (0) let α n+1 = (α n + α n,0 )/2, α n+1,1 = α n and α n+1,0 = α n,0 . (iii) If f (α n ) ≤ f (0) + c 1 α n ∂ α f (0) and |∂ α f (α n )| > c 2 |∂ α f (0)|:$(a) If α n,1 = 0 and ∂ α f (α n ) > -c 2 ∂ α f (0), let α n+1 = 2α n , α n+1,0 = α n,0 and α n+1,1 = α n,1 .

(b) If α n,1 = 0 and ∂ α f (α n ) < c 2 ∂ α f (0), let α n+1 = 2α n , α n+1,0 = α n and α n+1,1 = α n,1 .

(c) If α n,1 > 0 and ∂ α f (α n ) > -c 2 ∂ α f (0), let α n+1 = (α n + α n,0 )/2, α n+1,1 = α n and α n+1,0 = α n,0 .

(d) If α n,1 > 0 and ∂ α f (α n ) < c 2 ∂ α f (0), let α n+1 = (α n + α n,1 )/2, α n+1,0 = α n and α n+1,1 = α n,1 .

Then the sequences are always finite, i.e., the algorithm terminates in a finite number of steps.

Proof Assume that the algorithm runs indefinitely in order to get a contradiction. If the algorithm never enters case (ii), then α n,1 = 0 for all n, α n tends to infinity and f (α n ) ≤ f (0) + c 1 α n ∂ α f (0), which contradicts the fact that f is bounded from below.

As soon as the algorithm enter (ii), we have, for all subsequent iterations: α n,0 ≤ α n ≤ α n,1 , α n+1,0 ≥ α n,0 , α n+1,1 ≤ α n,1 and α n+1,1α n+1,0 = (α n,1α n,0 )/2. This implies that both α n,0 and α n,1 converge to the same limit α.

Moreover, we have, at each step:

$f (α n,1 ) > f (0) + c 1 α n,1 ∂ α f (0) or ∂ α f (α n,1 ) > -c 2 ∂ α f (0) and f (α n,0 ) ≤ f (0) + c 1 α n,0 ∂ α f (0) and ∂ α f (α n,0 ) ≤ c 2 ∂ α f (0) .$This implies that, at each step:

$f (α n,1 ) -f (α n,0 ) α n,1 -α n,0 > c 1 ∂ α f (0) or ∂ α f (α n,1 ) > -c 2 ∂ α f (0) and ∂ α f (α n,0 ) ≤ c 2 ∂ α f (0) .$There inequalities remain satisfied at the limit, and we must have

$∂ α f (α) > c 1 ∂ α f (0) or ∂ α f (α) > -c 2 ∂ α f (0) and ∂ α f (α) ≤ c 2 ∂ α f (0) ,$which is a contradiction since c 2 > c 1 and ∂ α f (0) < 0. In some situations, the computation of ∇F can be too costly, if not intractable, to run gradient descent updates while a low-cost stochastic approximation is available. For example, if F is an average of a sum of many terms, the approximation may simply be based on averaging over a randomly selected subset of the terms. This leads to a stochastic approximation algorithm [[163,](#b181)[113,](#b131)[25,](#b43)[67]](#b85) called stochastic gradient descent (SGD).

A general stochastic approximation algorithm of the Robbins-Monro type updates a parameter, denoted x ∈ R d , using stochastic rules. One associates to each x a probability distribution (π x ) on some set S, and, for some function H : R d × S → R d , considers the sequence of random iterations:

$ξ t+1 ∼ π X t X t+1 = X t + α t+1 H(X t , ξ t+1 ) (3.17)$where ξ t+1 is a random variable and the notation ξ t+1 ∼ π X t should be interpreted as the more precise statement that the conditional distribution of ξ t+1 given all past random variables U t = (ξ 1 , X 1 , . . . , ξ t , X t ) only depends on X t and is given by π X t .

It is sometimes assumed in the literature that π x does not depend on x. This is no real loss of generality because under mild assumptions, a random variable ξ following π x can be generated as function U (x, ξ) where ξ follows a fixed distribution (such as that of a family of independent uniformly distributed variables) and one can replace H(x, ξ) by H(x, U (x, ξ)). On the other hand, allowing π to depend on x brings little additional complication in the notation, and corresponds to the natural form of many applications.

More complex situations can also be considered, in which ξ t+1 is not conditionally independent of the past variables given X t . For example, the conditional distribution of ξ t+1 given the past may also depends on ξ t , which allows for the combination of stochastic gradient methods with Markov chain Monte-Carlo methods. This situation is studied, for example, in Métivier and Priouret [[138]](#b156), Benveniste et al. [[25]](#b43), and we will discuss an example in section 17.2.2.

## Deterministic approximation and convergence study

Introduce the function H(x) = E π x (H(x, •))

and write X t+1 = X t + α t+1 H(X t ) + α t+1 η t+1

with η t+1 = H(X t , ξ t+1 ) -H(X t ) in order to represent the evolution of X t in (3.17) as a perturbation of the deterministic algorithm xt+1 = xt + α n+1 H( xt ) [(3.18)](#) by the "noise term" α t+1 η t+1 . In many cases, the deterministic algorithm provides the limit behavior of the stochastic sequence, and one should ensure that this limit is as desired. By definition, the conditional expectation of η t+1 given U t (the past) is zero and one says that α t+1 η t+1 is a "martingale increment." Then,

$M T = T t=0 α t+1 η t+1 (3.19)$is called a "martingale." The theory of martingales offers numerous tools for controlling the size of M T and is often a key element in proving the convergence of the method.

Many convergence results have been provided in the literature and can be found in textbooks or lecture notes such as Benaïm [[23]](#b41), Kushner and Yin [[113]](#b131), Benveniste et al. [[25]](#b43). These results rely on some smoothness and growth assumptions made on the function H, and on the dynamics of the deterministic equation [(3.18)](#). Depending on these assumptions, proofs may become quite technical. We will here restrict to a reasonably simple context and assume that (H1) There exists a constant C such that, for all x ∈ R d ,

$E π x (|H(x, •)| 2 ) ≤ C(1 + |x| 2 ).$(H2) There exists x * ∈ R d and µ > 0 such that, for all x ∈ R d

$(x -x * ) T H(x) ≤ -µ|x -x * | 2 .$Assuming this, let A t = |X tx * | 2 and a t = E(A t ). Then, using [(3.17)](#),

$A t+1 = A t + 2α t+1 (X t -x * ) T H(X t , ξ t+1 ) + α 2 t+1 |H(X t , ξ t+1 )| 2 .$Taking the conditional expectation given past variables yields

$E(A t+1 | U t ) = A t + 2α t+1 (X t -x * ) T H(X t ) + α 2 t+1 E π x t (|H(X t , •)| 2 ) ≤ A t -2α t+1 µA t + α 2 t+1 C(1 + |X t | 2 ) ≤ (1 -2α t+1 µ + Cα 2 t+1 )A t +$α 2 t+1 C with C = 1 + |x * | 2 . Taking expectations on both sides yields a t+1 ≤ (1 -2α t+1 µ + Cα 2 t+1 )a t + α 2 t+1 C. (3.20) We state the next step in the computation as a lemma. Lemma 3.25 Assume that the sequence a t satisfies the recursive inequality a t+1 ≤ (1δ t )a t + ϵ t (3.21) with 0 ≤ δ t ≤ 1. Let v k,t = t j=k+1 (1δ j ). Then a t ≤ a 0 v 0,t + t k=1 ϵ k v k,t . (3.22) Proof Letting b t = a t /v 0,t , we get b t+1 ≤ b t + ϵ t+1 v 0,t+1 so that b t ≤ b 0 + t k=1 ϵ k v 0,k , and a t ≤ a 0 v 0,t + t k=1 ϵ k v k,t . ■ Using (3.20), we can apply this lemma with ϵ t = Cα 2 t and δ t = 2α t µ-Cα 2 t , making the additional assumption that, for all t, α t < min( 1 2µ , 2µ C

), which ensures that 0 < δ t < 1.

Starting with a simple case, assume that the steps γ t are constant, equal to some value γ (yielding also constant δ and ϵ). Then, [(3.22)](#) gives

$a t ≤ a 0 (1 -δ) t + ϵ t k=1 (1 -δ) t-k-1 ≤ a 0 (1 -δ) t + ϵ δ . (3.23)$Returning to the expression of δ and ϵ as functions of α, this gives

$a t ≤ a 0 (1 -2αµ + α 2 C) t + α C 2µ -αC .$This shows that limsup a t = O(α).

Return to the general case in which the steps depend on t, we will use the following simple result, that we state as a lemma for future reference. [Lemma 3.26](#) Assume that the double indexed sequence w st , s ≤ t of non-negative numbers is bounded and such that, for all s, lim t→∞ w st = 0. Let β 1 , β 2 , . . . be such that

∞ t=1 |β t | < ∞. Then lim t→∞ t s=1 β s w st = 0. Proof For any t 0 , we have t s=1 β s w st ≤ max s |β s | t 0 s=1 w st + max s,t |w st | s=t 0 +1 |β s | so that lim sup t→∞ t s=1 β s w st ≤ max s,t |w st | s=t 0 +1 |β s | and since this upper bound can be made arbitrarily small, the result follows. ■ Lemma 3.25 implies that a t ≤ a 0 v 0,t + C t s=1 α 2 s+1 v s,t .

Assume that

$(H3) ∞ k=1 α k = ∞ and ∞ k=1 α 2 k < ∞,$Then lim t→∞ v st = 0 for all s and lemma 3.26 implies that a t tends to zero. So, we have just proved that, if (H1), (H2) and (H3) are true, the sequence X t converges in the L 2 sense to x * . Actually, under these conditions, one can show that X t converge to x * almost surely, and we refer to Benveniste et al. [[25]](#b43), Chapter 5, for a proof (the argument above for an L 2 convergence follows the one given in Nemirovski et al. [[145]](#b163)).

Under (H3), one can say much more on the asymptotic behavior of the algorithm by comparing it with an ordinary differential equation. The "ODE method," introduced in Ljung [[120]](#b138), is indeed a fundamental tool for the analysis of stochastic approximation algorithms. The correspondence between discrete and continuous times is provided by the sequence α t . More precisely, let τ 0 = 0 and τ t = τ t-1 + α t , t ≥ 1. From (H3), τ t → ∞ when t → ∞. Define the piecewise linear interpolation x ℓ (ρ) of the sequence x t by X ℓ (ρ) = X t + ρτ t α t+1 (X t+1 -X t ), ρ ∈ [τ t , τ t+1 ).

Switching to continuous time allows us to interpret the average iteration xt+1 = xt + α t+1 H( xt ) as an Euler discretization scheme for the ordinary differential equation (ODE) ∂ ρ x = H( x). [(3.24)](#) Most of the insight on long-term behavior of stochastic approximations results from the fact that the random process x behaves asymptotically like solutions of this ODE. One has, for example, the following result, for which we introduce some additional notation.

Assume that [(3.24)](#) has unique solutions for given initial conditions on any finite interval, and denote by ϕ(ρ, ω) its solution at time ρ initialized with x(0) = ω. Let α c (ρ) and η c (ρ) be piecewise constant interpolations of (α t ) and (η t ) defined by α c (ρ) = α t+1 and η c (ρ) = η t+1 on the interval [τ t , τ t+1 ). The following proposition (see [[23]](#b41)) compares the tails of the process x ℓ (i.e., the functions x ℓ (ρ + s), s ≥ 0) with the solutions of the ODE over finite intervals. Proposition 3.27 (Benaim) Assume that H is Lipschitz and bounded. Then, for some constant C(T ) that only depends on T and H, one has, for all ρ ≥ 0 sup h∈[0,T ]

$|X ℓ (ρ + h) -ϕ(h, X ℓ (ρ))| ≤ C(T ) ∆(ρ -1, T + 1) + max s∈[ρ,ρ+T ] α c (s) . (3.25)$Recall that H being Lipschitz means that there exists a constant C such that

$| H(w) -H(w ′ )| ≤ C|w -w ′ |$for all w, w ′ ∈ R p .

In the upper-bound in [(3.25)](#), the term ∆(ρ -1, T + 1) is a random variable. It can be related to the variations

$∆ ′ (t, N ) = max k=0,...,N |M t+k -M t |,$where M is defined in [(3.19)](#), because, if m(ρ) is the largest integer t such that τ t ≤ ρ, then ∆ ′ (m(ρ) + 1, m(ρ + T )m(ρ)) ≤ ∆(ρ, T ) ≤ ∆ ′ (m(ρ), m(ρ + T )m(t) + 1).

In the case we are considering, one can use martingale inequalities (called Doob's inequalities) to control ∆ ′ . One has, for example,

$P max 0≤k≤N |M t+k -M t | > λ ≤ E(|M t+N -M t | 2 ) λ 2 . (3.26)$Furthermore, using the fact that E(η k+1 η l+1 ) = 0 if k l, one has

$E(|M t+N -M t | 2 ) = t+N k=t α 2 k+1 E(|η t+1 | 2 ).$If we assume (to simplify) that H is bounded and ∞ k=1 α 2 k < ∞ then, for some constant C, we have

$E(|M t+N -M t | 2 ) ≤ C ∞ k=t α 2$k+1 → 0 and inequality [(3.26)](#) can then be used in [(3.25)](#) to control the probability of deviation of the stochastic approximation from the solution of the ODE over finite intervals (a little more work is required under weaker assumptions on H, such as (H1)).

Proposition 3.27 cannot be used with T = ∞ because the constant C(T ) typically grows exponentially with T . In order to draw conclusions on the limit of the process W , one needs additional assumptions on the stability of the ODE. We refer to [[23]](#b41) for a collection of results on the relationship between invariant sets and attractors of the ODE and limit trajectories of the stochastic approximation. We here quote one of these results which is especially relevant for SGD. Proposition 3. [28](#b46) Assume that H = -∇E is the gradient of a function E and that ∇E only vanishes at a finite number of points. Assume also that X t is bounded. Then X t converges to a point x * such that ∇E(x * ) = 0. Some additional conditions on H can ensure that stochastic approximation trajectories remain bounded. The simplest one assumes the existence of a "Lyapunov function" that controls the ODE at infinity. The following result is a simplified version of Theorem 17 in Benveniste et al. [[25]](#b43). [Theorem 3.29](#) In addition to the hypotheses previously made, assume that there exists a C 2 function U with bounded second derivatives and K 0 > 0 such that, for allx such that |x| ≥ K 0 , ∇U (x) T H(x) ≤ 0, U (x) ≥ γ|x| 2 , γ > 0.

Then, the trajectories X ℓ (ρ) are almost surely bounded.

Note that hypothesis (H2) above implies the theorem's assumptions.

## The ADAM algorithm

ADAM (for adaptive moment estimation [[102]](#b120)) is a popular variant of stochastic gradient descent. When dealing with high-dimensional vectors W , using a single "gain" parameter (γ n+1 in (11.2)) is a limiting assumption since all parameters do not need to scale the same way. This can sometimes be handled by reweighting the components of H, i.e., using iterations

$X t+1 = X t + α t D t H(X t , ξ t+1 )$where D t is a (typically diagonal) matrix. The previous theory can be applied to situations in which D may be random, provided it converges almost surely to a fixed matrix.

The ADAM algorithm provides such a construction (without the theoretical guarantees) in which D t is computed using past iterations of the algorithm. It requires several parameters, namely: α: the algorithm gain, taken as constant (e.g., α = 0.001); Two parameters β 1 and β 2 for moment estimates (e.g. β 1 = 0.9 and β 2 = 0.999); A small number ϵ (e.g., ϵ = 10 -8 ) to avoid divisions by 0. In addition, ADAM defines two vectors: a mean m and a second moment v, respectively initialized at 0 and 1. The ADAM iterations are given below, in which g ⊗2 denotes the vector obtained by squaring each coefficient of a vector g.

## Algorithm 3.1 (ADAM)

1. Let X t be the current state, m t and v t the current mean and variance.

2. Generate ξ t+1 and let g t+1 = H(X t , ξ t+1 ).

## Update

$m t+1 = β 1 m t + (1 -β 1 )g t+1 . 4. Update v t+1 = β 2 v t + (1 -β 2 )g ⊙2 t+1 . 5. Let mt+1 = m t+1 /(1 -β t+1 1 ) and vt+1 = v t+1 /(1 -β t+1 2 ) 6. Set X t+1 = X t -α mt+1 √ vt+1 + ϵ$Note that the iteration on m t and v t correspond to defining

$mt = β 1 1 -β t 1 t k=0 (1 -β 1 ) t-k g k and vt = β 2 1 -β t 2 t k=0 (1 -β 2 ) t-k g ⊙2 k .$3.4 Constrained optimization problems

## Lagrange multipliers

A constrained optimization problem minimizes a function F over a closed subset Ω of R d , with Ω R d . This restriction invalidates, in a large part, the optimality conditions discussed in section 3.2. These conditions indeed apply to minimizers belonging to the interior of Ω, and therefore do not hold when they lie at its boundary, which is a very common situation in practice (Ω often has an empty interior).

In this section, which follows the discussion given in Wright and Recht [[204]](#b222), we review conditions for optimality for constrained minimization of smooth functions, in two cases. The first one, discussed in this section, is when Ω is defined by a finite number of smooth constraints, leading, under some assumptions, to the Karush-Kuhn-Tucker (or KKT) conditions. The second one, in the next section, specializes to closed convex Ω.

## KKT conditions

We introduce some notation. Let γ i , for i ∈ C, be C 1 functions γ i : R d → R, where C is a finite set of indices. We assume that C is divided into two non-intersecting parts, C = E ∪ I and consider minimization problems searching for

$x * ∈ argmin Ω F (3.27)$where

$Ω = {x ∈ R d : γ i (x) = 0, i ∈ E and γ i (x) ≤ 0, i ∈ I }.$(3.28)

The set Ω of all x that satisfy the constraints is called the feasible set for the considered problem. We will always assume that it is non-empty. If x ∈ Ω, one defines the set A(x) of active constraints at x to be

$A(x) = {i ∈ C : γ i (x) = 0} .$One obviously has E ⊂ A(x) for x ∈ Ω.

To be valid, the KKT conditions require some additional assumptions on potential minimizers, called "constraint qualifications." An instance of such assumptions is provided by the next definition. (MF2) There exists a vector h ∈ R d such that h T ∇γ i (x) = 0 for all i ∈ E and h T ∇γ i (x) < 0 for all i ∈ A(x) ∩ I .

A sufficient (and easier to check) condition for x to satisfy these constraints is when the vectors (∇γ i (x), i ∈ A(x)) are linearly independent [[37]](#b55). Indeed, if the latter "LI-CQ" condition is true, then any set of values can be assigned to h T ∇γ i (x) with the existence of a vector h that achieves them.

We introduce the Lagrangian

$L(x, λ) = F(x) + i∈C λ i γ i (x) (3.29)$where the real numbers λ i , i ∈ C are called Lagrange multipliers. The following theorem (stated without proof, see, e.g., [[146,](#b164)[35]](#b53)) provides necessary conditions satisfied by solutions of the constrained minimization problem that satisfy the constraint qualifications.

Theorem 3.31 Assume x * ∈ Ω is a solution of (3.27), and that x * satisfies the MF-CQ conditions. Then there exist Lagrange multipliers λ i , i ∈ C, such that

$∂ x L(x * , λ) = 0 λ i ≥ 0 if i ∈ I , with λ i = 0 when i A(x * ) (3.30)$Conditions [(3.30)](#) are the KKT conditions for the constrained optimization problem.

The second set of conditions is often called the complementary slackness conditions and state that λ i = 0 for an inequality constraint unless this constraint is satisfied with an equality. The next section provides examples in which the MF-CQ conditions are not satisfied and Theorem 3.31 does not hold. However, these conditions are not needed in the special case when the constraints are affine.

Theorem 3.32 Assume that for all i ∈ A(x * ), the functions γ i are affine, i.e., γ i (x) = b T i x + β i for some b ∈ R d and β ∈ R. Then (3.30) holds at any solution of (3.27).

## Remark 3.33

We have taken the convention to express the inequality constraints as γ i (x) ≤ 0, i ∈ I . With the reverse convention, i.e., γ i (x) ≥ 0, i ∈ I , one generally defines the Lagrangian as

$L(x, λ) = F(x) - i∈C λ i γ i (x)$and the KKT conditions remain unchanged. ♦

Examples. Constraint qualifications are important to ensure the validity of the theorem. Consider a problem with equality constraints only, and replace it by

$x * ∈ argmin Ω F subject to γi (x) = 0, i ∈ E, with γi = γ 2 i .$We clearly did not change the problem. However, the previous theorem applied to the Lagrangian

$L(x, λ) = F(x) + i∈C λ i γi (x)$would require an optimal solution to satisfy ∇F(x) = 0, because ∇ γi (x) = 2γ i (x)∇γ i (x) = 0 for any feasible solution. Minimizers of constrained problems do not necessarily satisfy ∇F(x) = 0, however. This is no contradiction with the theorem since ∇ γi (x) = 0 for all i shows that no feasible point satisfies the MF-CQ.

To take a more specific example, still with equality constraints, let d = 3, C = {1, 2} with F(x, y, z) = x/2+y and γ 1 (x, y, z) = x 2 -y 2 , γ 2 (x, y, z) = y -z 2 . Note that γ 1 = γ 2 = 0 implies that y = |x|, so that, for a feasible point, F(x, y, z) = |x| + x/2 ≥ 0 and vanishes only when x = y = 0, in which case z = 0 also. So (0, 0, 0) is a global minimizer. We have dF(0) = (1/2, 1, 0), dγ 1 (0) = (0, 0, 0) and dγ 2 (0) = (0, 1, 0) so that 0 does not satisfy the MF-CQ. The equation

$dF(0) + λ 1 dγ 1 (0) + λ 2 dγ 2 (0) = 0$has no solution (λ 1 , λ 2 ), so that the conclusion of the theorem does not hold.

## Convex constraints

We now consider the case in which Ω is a closed convex set. To specify the optimality conditions in this case, we need the following definition.

$Definition 3.34 Let Ω ⊂ R d be convex and let x ∈ Ω. The normal cone to Ω at x is the set N Ω (x) = {h ∈ R d : h T (y -x) ≤ 0 for all y ∈ Ω} (3.31)$The normal cone is an example of convex cone. (A convex subset Γ of R d is called a convex cone, if it is such that λx ∈ Γ for all x ∈ Γ and λ ≥ 0, a property obviously satisfied by N Ω (x).) It should also be clear from the definition that non-zero vectors in N Ω (x) always point outside Ω, i.e., x + h Ω if h ∈ N Ω (x), h 0. Here are some examples.

• If x is in the interior of Ω, then N Ω (x) = {0}.

• Assume that Ω is a half space, i.e., Ω = {x : b T x + β ≤ 0} with |b| = 1, and take x ∈ ∂Ω, i.e., b T x + β = 0. Then

$N Ω (x) = {h = µb : µ ≥ 0} .$Indeed, any element of R d can be written as y = x +λb +q with q T b = 0, and y ∈ Ω if and only if λ ≤ 0. Fix such a y and take h ∈ R d , decomposed as h = µb + r, with r T b = 0. We have h T (yx) = λµ + r T q. Clearly, if µ < 0, or if r 0, one can find λ ≤ 0 and q ⊥ b such that h T (yx) > 0. One the other hand, if µ ≤ 0 and r = 0, we have h T (yx) ≤ 0 for all y ∈ Ω, which proves the above statement.

• With a similar argument, if Ω = {x : b T x + β = 0} is a hyperplane, one finds that

$N Ω (x) = {h = λb : λ ∈ R} .$One can build normal cones to domains associated with multiple inequalities or equalities based on the following theorem.

## Theorem 3.35

Let Ω 1 and Ω 2 be two convex sets with relint(Ω 1 )∩relint(Ω 2 ) ∅.

$Then, if x ∈ Ω 1 ∩ Ω 2 N Ω 1 ∩Ω 2 (x) = N Ω 1 (x) + N Ω 2 (x)$Here, the addition is the standard sum between sets in a vector space:

$A + B = {x + y : x ∈ A, y ∈ B}.$Finally, we note that, if x ∈ relint(Ω), then

$N Ω (x) = {h ∈ R d : h T (y -x) = 0, y ∈ Ω}. (3.32)$Indeed, if y ∈ Ω, then x + ϵ(yx) ∈ Ω for small enough ϵ (positive or negative). For h ∈ N Ω (x), the condition ϵh T (yx) ≤ 0 for small enough ϵ requires that h T (yx) = 0.

With this definition in hand, we have the following theorem.

Theorem 3.36 Let F be a C 1 function and Ω a closed convex set. If x * ∈ argmin Ω F, then -∇F(x * ) ∈ N Ω (x * ). (3.33) If F is convex and (3.33) holds, then x * ∈ argmin Ω F.

$Proof Assume that x * ∈ argmin Ω F. If y ∈ Ω, then x * + t(y -x * ) ∈ Ω for all t ∈ [0, 1] and the function f (t) = F(x + t(y -x * )) is C 1 on [0, 1],$with a minimum at t = 0. This requires that

$∂ t f (0) = ∇F(x * ) T (y -x * ) ≥ 0, because, if ∂ t f (0) < 0, a Taylor expansion would show that f (t) < f (0) for small enough t > 0.$If F is convex and (3.33) holds, we have F(y) ≥ F(x * )+∇F(x * ) T (y -x * ) by convexity, so that

$F(x * ) ≤ F(y) + (-∇F(x * )) T (y -x * ) ≤ F(y). ■$
## Applications

Lagrange multipliers revisited. Consider Ω defined by (3.28), with the additional assumptions that

$γ i (x) = b T i x + β i for i ∈ E and γ i is convex for i ∈ I , which ensure that Ω is convex. Define N ′ γ (x) =          g = i∈A(x) λ i ∇γ i (x) : λ i ≥ 0, i ∈ A(x) ∩ I          .$Then, the KKT conditions in (3.30) can be rewritten as

$-∇F(x * ) ∈ N ′ γ (x * ).$Note that one always have

$N ′ γ (x) ⊂ N Ω (x) since, for g = i∈A(x) λ i ∇γ i (x) ∈ N ′ γ (x), one has, for y ∈ Ω, g T (y -x) = i∈A(x) λ i ∇γ i (x) T (y -x) = i∈E λ i (a T i y -a T i x) + i∈A(x)∩I λ i (γ i (x) + ∇γ i (x) T (y -x)) = i∈A(x)∩I λ i (γ i (x) + ∇γ i (x) T (y -x)) ≤ λ i γ i (y) ≤ 0,$in which the have used the facts that a T i x = a T i y = -β i for x, y ∈ Ω, i ∈ E, γ i (x) = 0 for i ∈ A(x) and the convexity of γ i . Constraint qualifications such as those considered above are sufficient conditions that ensure the identity between the two sets.

Consider now the situation of theorem 3.32, and assume that all constraints are affine inequalities, γ i (x) = b T i x + β ≤ 0, i ∈ I . Then, the statement N Ω (x) ⊂ N ′ γ (x) can be reexpressed as follows. All h ∈ R d such that h T (yx) ≤ 0 as soon as b T i (yx) ≤ 0 for all i ∈ A(x) must take the form

$h = i∈A(x) λ i b i$with λ (i) ≥ 0. This property is called Farkas's lemma (see, e.g. [[167]](#b185)). Note that affine equalities b T i x + β = 0 can be included as two inequalities b T i x + β ≤ 0, -b T i xβ ≤ 0, which removes the sign constraint on the corresponding λ (i) and therefore yields theorem 3.32.

Positive semi-definite matrices. We now take an example in which theorem 3.32 does not apply directly. Let Ω = S + n be the space of positive semidefinite n × n matrices, considered as a subset of the space M n of n × n matrices, itself identified with R n 2 . With this identification, the Euclidean inner product between two matrices can be expressed as (A, B) → trace(A T B).

We have A ∈ S + n if and only if, for all u ∈ R d , u T Au ≥ 0, which provides an infinity of linear inequality constraints on A. Elements of N S + (A) are matrices H ∈ M n such that trace(H T (B -A)) ≤ 0 for all B ∈ S + n , and we want to make this normal cone explicit. We first note that, every square matrix H can be decomposed as the sum of a symmetric matrix, H s and of a skew symmetric one, H a (namely, H s = (H + H T )/2 and H a = (H -H T )/2). We have moreover trace(H T a (B -A)) = 0, so the condition is only on the symmetric part of H.

For any u ∈ R d , one can take B = A+uu T , which belongs to S + n , with trace(H T s (B-A)) = u T H s u. This shows that, for H to belong to N S + n (A), one needs H s ⪯ 0. Now, take an eigenvector u of A with eigenvalue ρ > 0. Then B = Aαuu T is also in S + n as soon as 0 ≤ α ≤ ρ, and trace(H T s (B -A)) = -αu T H s u. So, if H ∈ N S + n (A), we have u T H s u ≥ 0, and since H s ⪯ 0, this gives u T H s u = 0. Still because H s is negative semi-definite, this implies H s u = 0. (This can be shown, for example, using Schwarz's inequality which says that (u

$T H s v) 2 ≤ (u T H s u)(v T H s v) for all v ∈ R d .)$Decomposing A with respect to its non-zero eigenvectors, i.e., writing

$A = p k=1 ρ k u k u T k$where p = rank(A), we get AH s = H s A = 0. We therefore obtained the proposition

Proposition 3.37 Let A ∈ S + n . Then H ∈ M n belongs to N S + n (A) if and only if -H s ∈ S + n and H s A = 0, where H s

$= (H + H T )/2.$Now, if one wants to minimize a function F over positive semidefinite matrices, and A * is a minimizer, we get the necessary condition that A * (∇F(A * )) s = 0 with (∇F(A * )) s positive semidefinite. These conditions are sufficient if F is convex.

For example, take

$F(A) = 1 2 trace(A 2 ) -trace(BA) (3.34) with B ∈ S n . Then (∇F(A)) s = A -B and the condition is A(A -B) = 0 with A ⪰ B. If B is diagonalized in the form B = U T DU$, with U orthogonal and D diagonal, then the solution is A * = U T D + U where D + is deduced from U by replacing non-negative entries by zeros.

## Projection.

Let Ω be closed convex,

$x 0 ∈ R d and F(x) = 1 2 |x -x 0 | 2 . We have min Ω F = min Ω∩ B(0,R)$
## F

for large enough R (e.g., larger than F(x) for any fixed point in Ω), and since the latter minimization is over a compact set, argmin Ω F is not empty. The function F being strongly convex, its minimizer over Ω is unique and called the projection of x 0 on Ω, denoted proj Ω (x 0 ).

Since ∇F(x) = xx 0 , theorem 3.36 implies that proj Ω (x 0 ) is characterized by proj Ω (x 0 ) ∈ Ω and x 0proj Ω (x 0 ) ∈ N Ω (proj Ω (x 0 )) [(3.35)](#) or (x 0proj Ω (x 0 )) T (yproj Ω (x 0 )) ≤ 0 for all y ∈ Ω.

(3.36)

$If x 0 Ω, then proj Ω (x 0 ) ∈ ∂Ω, since otherwise we would have N Ω (proj Ω (x 0 )) = {0} and x 0 = proj Ω (x 0 ), a contradiction. Of course, if x 0 ∈ Ω, then proj Ω (x 0 ) = x 0 .$Here are some important examples.

## 1.

Let Ω = z 0 + V , where z 0 ∈ R d and V is a linear space (i.e., Ω is an affine subset of R d ). Then N Ω (x) = z 0 + V ⊥ = x + V ⊥ for all x ∈ Ω, where V ⊥ is the vector space of vectors orthogonal to V , and proj Ω (x 0 ) is characterized by proj Ω (x 0 ) ∈ Ω and

$(x 0 -proj Ω (x 0 )) ∈ V ⊥$which is the usual characterization of the orthogonal projection on an affine space (compare to section 6.4).

## 2.

If Ω = B(0, 1), the closed unit sphere, then N Ω (x) = R + x for x ∈ ∂Ω (i.e., |x| = 1).

One can indeed note that, if h 0 in normal to Ω at x, then h/|h| ∈ Ω so that

$h T h |h| -x ≤ 0 which yields |h| ≤ h T x.$The Cauchy-Schwartz inequality implying that h T x ≤ |h| |x| = |h|, we must have equality, h T x = |h| |x|, which is only possible when x and h are collinear.

Given x 0 ∈ R d with x 0 ≥ 1, we see that proj Ω (x 0 ) must satisfy the conditions |proj Ω (x 0 )| = 1 (to be in ∂Ω) and x 0proj Ω (x 0 ) = λx 0 for some λ ≥ 0, which gives proj Ω (x 0 ) = x 0 /|x 0 |.

## 3.

If Ω = S + n and B (taking the role of x 0 ) is a symmetric matrix, then proj Ω (B) was found in the previous section, and is given by A = U T D + U where U T DU provides a diagonalization of B.

The projection has the important property of being 1-Lipschitz.

$Proposition 3.38 Let Ω be a closed convex subset of R d . Then, for all x, y ∈ R d |proj Ω (x) -proj Ω (y)| ≤ |x -y|. (3.37)$Proof This proposition is a special case of proposition 3.55 below.

## ■

## Projected gradient descent

The projected gradient descent algorithm minimizes F over Ω by iterating

$x t+1 = proj Ω (x t -α t ∇F(x t )),(3.38)$which provides a feasible method when proj Ω is easy to compute. An equivalent formulation is

$x t+1 = argmin Ω F(x t ) + ∇F(x t ) T (x -x t ) + 1 2α t |x -x t | 2 . (3.39)$To justify this last statement it suffices to notice that the function in the r.h.s. can be written as

$1 2α t |x -x t + α t ∇F(x t )| 2 - α t 2 |∇F(x t )| 2 + F(x t )$and apply the definition of the projection.

The convergence properties of this algorithm will be discussed in section 3.5.5, in a more general context. 

$epi(F) = (x, a) ∈ R d × R : F(x) ≤ a . (3.40) One says that F is closed if epi(F) is a closed subset of R d × R, that is: if x = lim n x n and a = lim n a n with F(x n ) ≤ a n , then F(x) ≤ a. Clearly, if (x, a) ∈ epi(F), then x ∈ dom(F). It should also be clear that epi(F) is always convex when F is convex: If (x, a), (y, b) ∈ epi(F), then F((1 -t)x + ty) ≤ (1 -t)F(x) + tF(y) ≤ (1 -t)a + tb so that (1 -t)(x, a) + t(y, b) ∈ epi(F).$To illustrate the definition, consider a simple example. Let F be the function defined on R by F(x) = |x| if |x| < 1 and F(x) = +∞ otherwise. It is convex, but not closed, as can be seen by taking the sequence (1 -1/n, 1) ∈ epi(F), with, at the limit, F(1) = +∞ > 1. In contrast, the function defined by F(x) = |x| if |x| ≤ 1 and F(x) = +∞ otherwise is convex and closed.

We have the following proposition.

## Proposition 3.40 A convex function F is closed if and only if all its sub-level sets

$Λ a (F) = x ∈ R d : F(x) ≤ a are closed subsets of R d . Proof If F is closed, then Λ a (F) is the intersection of the set {(x, a) : x ∈ R d }, which is$obviously closed, and of epi(F). It is therefore a closed set.

Conversely, assume that all Λ a (F) are closed and take a sequence (x n , a n ) in epi(F) that converges to (x, a). Then, fixing ϵ > 0, x n ∈ Λ a+ϵ for large enough n, and since this set is closed, F(x) ≤ a + ϵ. Since this is true for all ϵ > 0, we have F(x) ≤ a and (x, a) ∈ epi(F).

## ■

Note that, if F is continuous, then it is closed, so that closedness generalizes continuity for convex functions, but it also applies to the non-smooth case.

If Ω is a convex subset of R d , its indicator function σ Ω (such that σ Ω (x) = 0 for x ∈ Ω and σ Ω (x) = +∞ otherwise) is closed if and only if Ω is a closed subset of R d . This is obvious since Λ a (σ Ω ) = Ω if a ≥ 0 and ∅ otherwise.

## Subgradients

Several machine learning problems involve convex functions that are not C 1 , requiring a generalization of the notion of derivative provided by the following definition. Definition 3.41 If F is a convex function and x ∈ dom(F), a vector g ∈ R d such that

$F(x) + g T (y -x) ≤ F(y) (3.41) for all y ∈ R d is called a subgradient of F at x.$The set of subgradients of F at x is denoted ∂F(x) and called the subdifferential of F at x.

$If x ∈ int(dom(F)) and F is differentiable at x, (3.5) implies that ∇F(x) ∈ ∂F(x).$This is in this case the only element of ∂F(x).

$Proposition 3.42 If F is differentiable at x ∈ int(dom(F)), then ∂F(x) = {∇F(x)}.$Proof We need to prove that there is no other subgradient. Assume that ∇F(x) exists and take

$y = x + ϵu in (3.41) (u ∈ R d ). One gets, for g ∈ ∂F(x), ϵg T u ≤ F(x + ϵu) -F(x) = ϵ∇F(x) T u + o(ϵ)$Dividing by |ϵ| and letting ϵ → 0 gives (depending on the sign of ϵ) g T u ≤ ∇F(x) T u andg T u ≤ -∇F(x) T u This is only possible if g T u = ∇F(x) T u for all u ∈ R d , which itself implies g = ∇F. ■ The next theorem, which is an obvious consequence of definition 3.41, characterizes minimizers of convex functions in the general case.

$Theorem 3.43 Let F : R d → R be convex. Then x is a (global) minimizer of F if and only if 0 ∈ ∂F(x).$The following result shows that subgradients exist under generic conditions. We note that g ∈ ∂F(x) if and only if proj--→ aff (dom(F)) (g) ∈ ∂F, because (3.41) is trivial if F(y) = +∞. So ∂F cannot be bounded unless aff(dom(D)) = R d . However, it is the part of this set that is included in the

$--→ aff (dom(F)) that is of interest. Proposition 3.44 For all x ∈ R d , ∂F(x) is a closed convex set (possibly empty, in par- ticular for x dom(F)). If x ∈ ridom(F), then ∂F(x) ∅ and ∂F(x) ∩ --→ aff (dom(F)) is compact.$Proof The convexity and closedness of ∂F(x) is clear from the definition. If x ∈ ridom(F), there exists ϵ > 0 such that x + ϵh ∈ ridom(F) for all h ∈ --→ aff (dom(F)) with

$|h| = 1. For all g ∈ ∂F(x) ∩ --→ aff (dom(F)), one has |g| = max{g T h : h ∈ --→ aff (dom(F)), |h| = 1} ≤ max((F(x + ϵh) -F(x))/ϵ : h ∈ --→ aff (dom(F)), |h| = 1)$and the upper bound is finite because it is the maximum of a continuous function over a bounded set. This shows that ∂F(x) is bounded. We defer the proof that ∂F(x) ∅ to section 3.7.

## ■

Subdifferentials are, under mild conditions, additive. More precisely, we have the following proposition. Theorem 3.45 Let F 1 and F 2 be convex functions such that

$ridom(F 1 ) ∩ ridom(F 2 ) ∅ . Then, for all x ∈ R d , ∂(F 1 + F 2 )(x) = ∂F 1 (x) + ∂F 2 (x).$Note that the inclusion

$∂F 1 (x) + ∂F 2 (x) ⊂ ∂(F 1 + F 2 )(x)$as can be immediately checked by summing the inequalities satisfied by subgradients. The reverse inclusion requires the use of separation theorems for convex sets (see section 3.7).

Another important point is how the chain rule works with compositions with affine functions.

$Theorem 3.46 Let F be a convex function on R d , A a d × m matrix and b ∈ R d . Let G(x) = F(Ax + b), x ∈ R m . Assume that there exists x 0 ∈ R m such that Ax 0 ∈ ridom(F). Then, for all x ∈ R m , ∂G(x) = A T ∂F(Ax + b).$One direction is straightforward and does not require the condition on ridom(F). If

$g ∈ ∂F(Ax + b), then F(z) -F(Ax + b) ≥ g T (z -Ax -b), z ∈ R d and applying this inequality to z = Ay + b for y ∈ R m yields G(y) -G(x) ≥ g T A(y -x)$so that A T g ∈ ∂G and A T ∂F ⊂ ∂G. The reverse inclusion is proved in section 3.7.

Subdifferentials can be seen as generalizations of normal cones. Proposition 3.47 Assume that Ω is a closed convex subset of R d . Then σ Ω (the indicator function of Ω) has a subdifferential everywhere on Ω with

$∂σ Ω (x) = N Ω (x), x ∈ Ω Proof For x ∈ Ω, (3.41) is g T (y -x) ≤ σ Ω (y) for y ∈ R d , but since σ Ω (y) = +∞ outside of Ω, g ∈ ∂σ Ω (x) is equivalent to g T (y -x) ≤ 0$for y ∈ Ω, which is exactly the definition of the normal cone.

## ■

Given this proposition, it is also clear (after noting that σ

$Ω 1 + σ Ω 2 = σ Ω 1 ∩Ω 2 ) that theorem 3.$45 is a generalization of theorem 3.35.

## Directional derivatives

From proposition 3.5, applied with y = x + h, we see that

$t → 1 t (F(x + th) -F(x))$is increasing as a function of t. This property allows us to define directional derivatives of F at x. Definition 3.48 Let F be convex and x ∈ dom(F). The directional derivative of F at x in the direction h ∈ R d is defined by

$dF(x, h) = lim t↓0 1 t (F(x + th) -F(x)),(3.42$)

$and belong to [-∞, +∞].$Note that, still from proposition 3.5, one has, for all x ∈ dom(F) and y ∈ R d :

$F(y) ≥ F(x) + dF(x, y -x) (3.43)$We have the proposition:

$Proposition 3.49 If F is convex, then x * ∈ argmin(F) if and only if dF(x * , h) ≥ 0 for all h ∈ R d .$Proof If dF(x * , h) ≥ 0, then F(x * +th)-F(x * ) ≥ 0 for all t > 0 and this being true for all h implies that x * is a minimizer. Conversely, if x * is a minimizer, dF(x * , h) is a limit of non-negative numbers and is therefore non-negative.

## ■

Proposition 3.50 If F is convex and x ∈ dom(F), then dF(x, h) is positively homogeneous and subadditive (hence convex) as a function of h, namely

$dF(x, λh) = λdF(x, h), λ > 0 and dF(x, h 1 + h 2 ) ≤ dF(x, h 1 ) + dF(x, h 2 ).$Proof Positive homogeneity is straightforward and left to the reader. For the second one, we can write

$F(x + th 1 + th 2 ) ≤ 1 2 (F(x + th 1 /2) + F(x + th 2 /2))$by convexity so that

$1 t (F(x + th 1 + th 2 ) -F(x)) ≤ 1 2 1 t (F(x + th 1 /2) -F(x)) + 1 t (F(x + th 2 /2) -F(x)) .$Taking t ↓ 0,

$dF(x; h 1 + h 2 ) ≤ 1 2 (dF(x; h 1 /2) + dF(x, h 2 /2)) = dF(x, h 1 ) + dF(x, h 2 ). ■ Proposition 3.51 If F is convex and x ∈ dom(F), then dF(x, h) ≥ sup{g T h, g ∈ ∂F(x)}. If x ∈ ridom(F), then dF(x, h) = max{g T h, g ∈ ∂F(x)}. Proof If g ∈ ∂F(x), then for all t > 0 F(x + th) -F(x) ≥ tg T h.$Dividing by t and passing to the limit yields dF(x, h) ≥ g T h.

We prove that the maximum is attained at some g ∈ ∂F(x) when x ∈ ridom(F). In this case, the domain of the convex function G : h → dF(x, h) is the vector space parallel to aff(dom(F)), namely

$dom(G) = {h : x + h ∈ aff(dom(F))}.$Indeed, for any h in this set, there exists ϵ > 0 such that x + th ∈ dom(F) for 0 < t < ϵ and dF(x, h) ≤ (F(x + th) -F(x))/t < ∞. Conversely, if h ∈ dom(G), then F(x + th) -F(x) must be finite for small enough t, so that x + th ∈ dom(F) and x + h ∈ aff(dom(F)).

As a consequence, for any h ∈ aff(dom(F)), there exists ĝ ∈ ∂G(h), which therefore satisfies dF(x, h) ≥ dF(x, h) + ĝT ( hh)

$for any h ∈ R d (the upper bound is infinite if h dom(G)). Letting h → 0, we get dF(x, h) ≤ ĝT h.$Also, by positive homogeneity, we have

$tdF(x, h) ≥ dF(x, h) + ĝT (t h -h)$for all t > 0, which requires dF(x, h) ≥ ĝT h for all h, and in particular dF(x, h) = ĝT h.

$Since F(x + h) -F(x) ≥ dF(x, h) ≥ ĝT h$we see that ĝ ∈ ∂F(x), with ĝT h = dF(x, h), which concludes the proof.

## ■

The next proposition gives a criterion for a vector g to belong to ∂F(x) based on directional derivatives.

$Proposition 3.52 Assume that x ∈ dom(F) where F is convex. If g ∈ R d is such that dF(x, h) ≥ g T h for all h ∈ R d , then g ∈ ∂F(x).$Proof Just use the fact that dF(x, h) ≤ F(x + h) -F(x). ■

## Subgradient descent

When F is a non-differentiable a convex function, directions g such that -g ∈ ∂F(x) do not always provide directions of descent. Indeed, g ∈ ∂F(x) implies

$F(x -αg) ≥ F(x) -α|g| 2$but the inequality goes in the "wrong direction." However, we know that, for any h ∈ R d , there exists g h ∈ ∂F(x) such that

$dF(x, -h) = -g T h h ≥ -g T h$for all g ∈ ∂F(x). As a consequence, any non-vanishing solution of the equation h = g h will provide a direction of descent. This suggests looking for h ∈ ∂F(x) such that h 0 and |h| 2 ≤ g T h for all g ∈ ∂F(x).

Since g T h ≤ |g| |h|, this requires that |h| ≤ |g| for all g ∈ ∂F(x), i.e., h = argmin ∂F(x) (g → |g|). (3.44) Conversely, if h is the minimal-norm element of ∂F(x) (which is necessarily unique since the norm is strictly convex and ∂F(x) is convex and compact), then |h| 2 ≤ |h + t(gh)| 2 for all g ∈ ∂F(x) and t ∈ [0, 1], and taking the difference yields 2th

T

$(g -h) + t 2 |g -h| 2 ≥ 0.$The fact that this holds for all t ≥ 0 requires that h T (gh) ≥ 0 as required. We have therefore proved that h defined by (3.44) is a descent direction for F at x (it is actually the steepest descent direction: see [[204]](#b222) for a proof), justifying the algorithm

$x t+1 = x t -α t argmin ∂F(x) (g → |g|)$as subgradient descent iterations.

Example. Consider the minimization of

$F(x) = ψ(x) + λ n i=1 |x (i) |$where

$ψ is a C 1 convex function on R d . Let A(x) = {i : x (i) = 0}. Then ∂F(x) =          ∇ψ(x) + λ i A(x) sign(x (i) ) + λ i∈A(x) ρ i e i : |ρ i | ≤ 1, i ∈ A(x)         $where e i is the ith vector of the canonical basis of R d .

For g = ∇ψ(x) + λ i A(x) sign(x (i) ) + λ i∈A(x) ρ i e i , we have

$|g| 2 = i A(x) (∂ i F(x) + λ sign(x (i) )) 2 + i∈A(x) (∂ i ψ(x) -λρ i ) 2 .$Define s(t) = sign(t) min(|t|, 1).

Then h satisfying (3.44) is given by

$h (i) = ∂ i ψ(x) -λ sign(x (i) ) if i A(x) λ s(∂ i ψ(x)/λ) if i ∈ A(x).$In more complex situations, the extra minimization step at each iteration of the algorithm can be challenging computationally. The following subgradient method uses an averaging approach to minimize F without requiring finding subgradients with minimal norms. It simply defines

$x t+1 = x t -α t g t , g t ∈ ∂F(x t ) and computes xt = t j=1 α j x j t j=1 α j .$We refer to [[204]](#b222) for a proof of convergence of this method.

## Proximal Methods

Proximal operator. We start with a few simple facts. Let F be a closed convex function and ψ be convex and differentiable, with dom

$(ψ) = R d . Let G = F + ψ.$Then G is a closed convex function. Indeed, consider the sub-level set Λ a (G) = {x : G(x) ≤ a} and assume that x n → x with x n ∈ Λ a (g). Then ψ(x n ) → ψ(x) by continuity, and for all ϵ > 0, we have, for large enough n, F(x n ) ≤ aψ(x) + ϵ. This inequality remains true at the limit because F is closed, yielding G(x) ≤ a + ϵ for all ϵ > 0, so that x ∈ Λ a (G).

We have ridom(F) ∩ ridom(ψ) ∅ so that (by theorem 3.45 and proposition 3.42) ∂G(x) = ∇ψ(x) + ∂F(x). In particular, x * is a minimizer of G if and only if -∇ψ(x * ) ∈ ∂F(x * ).

It one assumes that ψ is strongly convex, so that there exists m and L such that

$m 2 |y -x| 2 ≤ ψ(y) -ψ(x) -∇ψ(x) T (y -x) ≤ L 2 |y -x| 2$for all x, y ∈ R d , then a minimizer of G exists and is unique. To see this, fix x 0 ∈ ridom(F) and consider the closed convex set

$Ω 0 = Λ G(x 0 ) (G) = {x : G(x) ≤ G(x 0 )}.$Any minimizer of G must clearly belong to Ω 0 . If x ∈ Ω 0 , we have

$F(x) + ψ(x 0 ) + ∇ψ(x 0 ) T (x -x 0 ) + m 2 |x -x 0 | 2 ≤ G(x) ≤ G(x 0 ) .$Moreover, there exists (from proposition 3.44) an element g ∈ ∂F(x 0 ) so that F(x) ≥ F(x 0 ) + g T (x -x 0 ) for all x ∈ R d . We therefore get

$F(x 0 ) + ψ(x 0 ) + (g + ∇ψ(x 0 )) T (x -x 0 ) + m 2 |x -x 0 | 2 ≤ G(x 0 ) .$for all x ∈ Ω 0 , which shows that Ω 0 must be bounded and therefore compact. There exists a minimizer x * of G on Ω 0 , and therefore on all R d . This minimizer is unique, since the sum of a convex function and a strictly convex function is strictly convex.

In particular, for any closed convex F, we can apply the previous remarks to

$G : v → F(v) + 1 2 |x -v| 2 where x ∈ R d is fixed. The function ψ : v → |v -x| 2 /2 is strongly convex (with L = m = 1)$and G therefore has a unique minimizer v * . This is summarized in the following definition.

Definition 3.53 Let F be a closed convex function. The proximal operator associated to F is the mapping prox F : R d → dom(F) defined by

$prox F (x) = argmin R d (v → F(v) + 1 2 |x -v| 2 ). (3.45)$From the previous discussion, we also deduce Proposition 3.54 Let F be a closed convex function and α > 0. We have

$x ′ = prox αF (x) if and only if x ∈ x ′ + α∂F(x ′ ). In particular, x * is a minimizer of F if and only if x * = prox αF (x * )$Let us take a few examples.

• Let F(x) = λ|x|, x ∈ R d , for some λ > 0. Then F is differentiable everywhere except at x = 0 and dom(F) = R d . We have ∂F(x) = λx/|x| for x 0. A vector g belongs to ∂F(0) if and only if g T x ≤ λ|x| for all x ∈ R d , which is equivalent to |g| ≤ λ so that ∂F(0) = B(0, λ).

We have

$x ′ = prox F (x) if and only if x ′ 0 and x = x ′ + λx ′ /|x ′ | or x ′ = 0 and |x| ≤ λ. For |x| > λ, the equation x = x ′ + λx ′ /|x ′ | is solved by x ′ = |x| -λ |x| x yielding prox F (x) =          |x| -λ |x| x if |x| ≥ λ 0 otherwise (3.46)$• Let Ω be a closed convex set. Then prox σ Ω = proj Ω , the projection operator on Ω, as directly deduced from the definition.

The following proposition can then be compared to proposition 3.38.

$Proposition 3.55 Let F be a closed convex function. Then prox F is 1-Lipschitz: for all x, y ∈ R d , | prox F (x) -prox F (y)| ≤ |x -y|. (3.47)$Proof Let x ′ = prox F (x) and y ′ = prox F (y). Then, there exists g ∈ ∂F(x ′ ) and h ∈ ∂ F (y ′ ) such that x = x ′ + g and y = y ′ + h. Moreover, we have

$F(y ′ ) -F(x ′ ) ≥ g T (y ′ -x ′ ) F(x ′ ) -F(y ′ ) ≥ h T (x ′ -y ′ ) from which we deduce g T (y ′ -x ′ ) ≤ h T (y ′ -x ′ ) or (h -g) T (x ′ -y ′ ) ≥ 0. Expressing g, h in terms or x, x ′ , y, y ′ , we get (y -x -y ′ + x ′ ) T (y ′ -x ′ ) ≥ 0 or |y ′ -x ′ | 2 ≤ (y -x) T (y ′ -x ′ ) ≤ |y -x| |y ′ -x ′ | which is only possible if |y ′ -x ′ | ≤ [y -x|. ■ If F is differentiable, then x ′ = prox αF (x) satisfies x ′ = x -α∇F(x ′ )$so that x → prox αF (x) can be interpreted as an implicit version of the standard gradient step x → x-α∇F(x). The iterations x(t+1) = prox α t F (x(t)) provide an algorithm that converges to a minimizer of F (this will be justified below). This algorithm is rarely practical, however, since the minimization required at each step is not necessarily much easier to perform than minimizing F itself. The proximal operator, however, is especially useful when combined with splitting methods.

Proximal gradient descent. Assume that the objective function F takes the form

$F(x) = G(x) + H(x) (3.48)$where G is C 1 on R d and H is a closed convex function. We first note that

$dF(x, h) = lim t↓0 F(x + th) -F(x) t is well defined (even if G is not convex, because it is smooth), with dF(x, h) = ∇G(x) T h + dH(x, h)$In particular, if x * be a minimizer of F, then dF(x, h) ≥ 0 for all h so that dH(x, h) ≥ -∇G(x) T h for all h. Using proposition 3.52, this shows that -∇G(x) ∈ ∂H(x), which is a necessary condition for optimality for F (which is sufficient if G is convex).

Proximal gradient descent implements the algorithm

$x t+1 = prox α t H (x t -α t ∇G(x t )). (3.49)$We note that a stationary point of this algorithm, i.e. a point x such that x = prox α t H (xα t ∇G(x)) must be such that xα t ∇G(x) ∈ x + α t ∂H(x), so that -∇G(x) ∈ ∂H(x). This shows that the property of being stationary does not depend on α t > 0, and is equivalent to the necessary optimality condition that was just discussed.

We first study this algorithm under the assumption that G is L-C 1 , which implies that, for all x, y ∈ R d .

$G(y) ≤ G(x) + ∇G(x) T (y -x) + L 2 |x -y| 2 .$At iteration t, we have

$x t -α t ∇G(x t ) ∈ x t+1 + α t ∂H(x t+1 )$which implies, in particular

$α t H(x t )-α t H(x t+1 ) ≥ (x t -x t+1 ) T (x t -x t+1 -α t ∇G(x t )) = |x t -x t+1 | 2 +α t ∇G(x t ) T (x t+1 -x t )$Dividing by α t and adding G(x t ) -G(x t+1 ), we get

$F(x t ) -F(x t+1 ) ≥ 1 α t |x t -x t+1 | 2 + G(x t ) + ∇G(x t ) T (x t+1 -x t ) -G(x t+1 ) ≥ 1 α t - L 2 |x t -x t+1 | 2 (3.50)$so that proximal gradient descent iterations reduce the objective function as soon as

$α t ≤ 2/L.$Assuming that α t < 2/L, (3.50) can be rewritten as

$x t+1 -x t α t 2 ≤ 2 α t (2 -α t L) (F(x t ) -F(x t+1 ).$This inequality should be compared to [(3.11)](#) in the unconstrained case. It yields, in particular, the inequality

$min x t+1 -x t α t : t ≤ T ≤ F(x 0 ) -min F 2T min{α t (2 -α t L), t ≤ T } . (3.51)$As a consequence, if one runs proximal gradient descent until |x t+1x t |/α t is small enough, the algorithm will terminate in finite time as soon as α t is bounded from below (and, in particular, if α t is constant).

If we assume that G is convex, in addition to being L-C 1 , then we have a stronger result. Let x * be a minimizer of F. Then, using again x t -α t ∇G(x t ) ∈ x t+1 +α t ∂H(x t+1 ), we have

$α t H(x * ) -α t H(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 -α t ∇G(x t ))$and

$α t F(x * ) -α t F(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 ) -α t (x * -x t+1 ) T ∇G(x t )) + α t G(x * ) -α t G(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 ) -α t (x * -x t ) T ∇G(x t )) + α t G(x * ) + α t (x t+1 -x t ) T ∇G(x t ) -α t G(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 ) - α t L 2 |x t -x t+1 | 2$Assuming that α t L ≤ 1, then

$α t F(x * ) -α t F(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 ) - 1 2 |x t -x t+1 | 2 = 1 2 (|x t+1 -x * | 2 -|x t -x * | 2 ),$which we rewrite as

$α t (F(x t+1 ) -F(x * )) ≤ 1 2 (|x t -x * | 2 -|x t+1 -x * | 2 )$Note that, from (3.50), we also have

$F(x t+1 ) ≤ F(x t ) - 1 2α t |x t -x t+1 | 2$when α t L ≤ 1, which shows, in particular that F(x t ) is decreasing. Fixing a time T , we have, from these two observations

$α t (F(x T ) -F(x * )) ≤ 1 2 (|x t -x * | 2 -|x t+1 -x * | 2 )$for all t ≤ T -1, and summing over T ,

$(F(x T ) -F(x * )) T -1 t=0 α t ≤ 1 2 (|x 0 -x * | 2 -|x T -x * | 2 ) yielding F(x T ) -F(x * ) ≤ |x 0 -x * | 2 2 T -1 t=0 α t . (3.52)$We summarize this in the following theorem, specializing to the case of constant step α t .

Theorem 3.56 Let G be am L-C 1 function defined on R d and H be closed convex. Assume that F = G+H has a minimizer x * . Then the algorithm (3.49) run with α t = α ≤ 1/L for all t is such that, for all T > 0,

$F(x T ) -F(x * ) ≤ |x 0 -x * | 2 2αT . (3.53)$Also, when G = 0, F = H, we retrieve the proximal iterations algorithm

$x t+1 = prox αF (x t ),(3.54)$and we have just proved that it converges for any α > 0 as soon as F is a closed convex function.

One gets a stronger result under the assumption that G is C 2 , and is such that the eigenvalues of

$∇ 2 G(x) are included in a fixed interval [m, L] for all x ∈ R d with m > 0.$Such a G is strongly convex, which implies that F has a unique minimizer. We have

$|x t+1 -x * | = prox α t H (x t -α t ∇G(x t )) -prox α t H (x * -α t ∇G(x * ) ≤ |x t -x * -α t (∇G(x t )) -∇G(x * ))| . Write |x t -x * -α t (∇G(x t )) -∇G(x * )| = 1 0 (Id R n -α t ∇ 2 G(x * + t(x t -x * )))(x t -x * )dt ≤ 1 0 (Id R n -α t ∇ 2 G(x * + t(x t -x * )))(x t -x * ) dt ≤ max(|1 -α t m|, |1 -α t L|)|x t -x * |$where we have use the fact that the eigenvalues of Id R nα t ∇ 2 G(x) are included in [1-α t L, 1-α t m] for all x ∈ R d . If one assumes that α t ≤ 1/L, so that max(|1-α t m|, |1-

$α t L|) ≤ 1 -α t m, one gets |x t+1 -x * | ≤ (1 -α t m)|x t -x * | .$Iterating this inequality, we get the theorem that we state for constant α t .

$Theorem 3.57 Let F = G + H where G is a C 2 convex function and H is a closed convex function. Assume that the eigenvalues of ∇ 2 G are uniformly included in [m, L] with m > 0. Let x * argmin F. Let (x t ) satisfy (3.49) with constant α t = α < 1/L. Then |x t -x * | ≤ (1 -αm) t |x 0 -x * |.$Note that these results also apply to projected gradient descent (section 3.4.4), which is a special case (taking G = σ Ω ).

## Duality

## Generalized KKT conditions

A constrained convex minimization problem consists in the minimization of a closed convex function F over a closed convex set Ω ⊂ ridom(F). We have seen in theorem 3.36 that, for smooth F, any solution x * of this problem had to satisfy -∇F(x * ) ∈ N Ω (x) where

N Ω (x) = {h : h T (yx) ≤ 0 for all y ∈ Ω} .

The next theorem generalizes this property to the non-smooth convex case, for which the necessary optimality condition is also sufficient.

$Theorem 3.58 Let F be a closed convex function, Ω ⊂ ridom(F) a nonempty closed con- vex set. Then x * ∈ argmin Ω F if and only if 0 ∈ ∂F(x * ) + N Ω (x * ) Proof Introduce the indicator function σ Ω . Then minimizing F over Ω is the same as minimizing G = F+σ Ω over R d . The assumptions imply that ridom(σ Ω ) = relint(Ω) ⊂ ridom(F) and therefore ∂G(x) = ∂F(x) + ∂σ Ω (x) for all x ∈ Ω. Since ∂σ Ω (x) = N Ω (x)$the result follows for the characterization of minimum of convex functions.

## ■

In the following, we will restrict to the situation in which F is finite (i.e., dom(F) = R d ) and Ω is defined through a finite number of equalities and inequalities, taking the form

$Ω = x ∈ R d : γ i (x) = 0, i ∈ E and γ i (x) ≤ 0, i ∈ I for functions (γ i , i ∈ C = E ∪I ) such that γ i : x → b T i x+β t is$affine for all i ∈ E and γ i is closed convex for all i ∈ I . This is similar to the situation considered in section 3.4.1, with additional convexity assumptions, but without assuming smoothness. We recall the definition of active constraints from section 3.4.1, namely, for x ∈ Ω,

$A(x) = {i ∈ C : γ i (x) = 0}.$Following the discussion in the smooth case, define the set

$N ′ γ (x) ⊂ R d by N ′ γ (x) =          i∈A(x) λ i s i : s i ∈ ∂γ i (x), i ∈ A(x), λ i ≥ 0, i ∈ A(x) ∩ I          .$The property 0 ∈ ∂F(x * ) + N γ (x * ) is the expression of the KKT conditions in the nonsmooth case. It holds for x * ∈ argmin Ω F as soon as N Ω (x * ) = N ′ γ (x * ), which is true under appropriate constraint qualifications. We here replace the MF-CQ in definition 3.30 by the following conditions that do not involve gradients. Definition 3.59 Let (γ i , i ∈ C = E ∪ I ) be a set of equality and inequality constraints, with γ i : x → b T i x + β i , i, ∈ E and γ i closed convex, i ∈ I . One says that these constraints satisfy the Slater constraint qualifications (Sl-CQ) if and only if:

$(Sl 1) The vectors (b i , i ∈ E) are linearly independent. (Sl 2) There exists x ∈ R d such that γ i (x) = 0 for i ∈ E and γ i (x) < 0 for i ∈ I .$The first constraint is a very mild condition. When it is not satisfied, this means that some b i 's are linear combinations of others, and equality constraints for the latter implies equality constraints for the former. These redundancies can therefore be removed without changing the problem.

Note that (Sl2) can be replaced by the apparently weaker condition that, for all i ∈ I , there exists x i ∈ R d satisfying all the constraints and γ i (x i ) < 0. Indeed, if this is true, then the average, x, of (x i , i ∈ I ) also satisfies the equality constraints by linearity, and if i ∈ I ,

$γ i ( x) ≤ 1 |I | j∈I γ i (x (j) ) ≤ 1 |I | γ i (x (i) ) < 0.$The following proposition makes a connection between the Slater conditions and the MF-CQ in definition 3.30. Proposition 3.60 Assume that γ i , i ∈ I are convex C 1 functions. Then, if there exists a feasible point x * that satisfies the MF-CQ, there exists another point x satisfying the Sl-CQ. Conversely, if there exists x satisfying the Sl-CQ, then every feasible point x * satisfies the MF-CQ.

Proof The linear independence conditions on equality constraints are the same in MF-CQ and Sl-CQ, so that we only need to consider inequality constraints.

Let x * satisfy MF-CQ, and take h 0 such that b T i h = 0 for all i ∈ E, and ∇γ i (x * ) T h < 0, i ∈ A(x) ∩ I . Then x * + th satisfies the equality constraints for all t ∈ R. If i ∈ I is not active, then γ i (x * ) < 0 and this will remain true at x * + th for small t by continuity. If i ∈ A(x) ∩ I , then a first order expansion gives γ i (x * + th) = t∇γ i (x * ) T h + o(|h|), which is also negative for small enough t > 0. So, x * + th satisfies the Sl-CQ for small enough t > 0.

Conversely, let x satisfy the Sl-CQ. Take a feasible point x * . If x * = x, then there is no active inequality constraint and x * satisfies MF-CQ. Assume x * x and let h = xx * . Then b T i h = 0 for all i ∈ E, and if i ∈ I ∩ A(x * ),

$0 > γ i (x) = γ i (x * + h) ≥ γ i (x * ) + ∇γ i (x * ) T h = ∇γ i (x * ) T h$so that x * satisfies MF-CQ.

## ■

The following theorem, that we give without proof, states that the Slater conditions implies that the KKT conditions are satisfied for a minimizer. Theorem 3.61 Assume that all the constraints are affine, or that they satisfy the Sl-CQ in definition 3. [59](#b77)

$. Let x * ∈ argmin Ω F. Then N Ω (x * ) = N ′ γ (x * ), so that there exist s 0 ∈ ∂F(x * ), s i ∈ ∂γ i (x * ), i ∈ A(x * ), (λ i , i ∈ A(x * )) with λ i ≥ 0 if i ∈ I ∩ A(x * ), such that s 0 + i∈A(x) λ i s i = 0 (3.55)$
## Dual problem

Consider the Lagrangian

$L(x, λ) = F(x) + i∈C λ i γ i (x)$defined in [(3.29)](#) and let D = {λ : λ i ≥ 0, i ∈ I }. Because the functions γ i are nonpositive on Ω, we have

$L(x, λ) ≤ F(x)$for all x ∈ Ω and λ ∈ D, which implies that We did not need much of our assumptions (not even F to be convex) to reach this conclusion. When the converse inequality is true (so that the duality gap pd vanishes), the dual problem provides important insights on the primal problem, as well as alternative ways to solve it. This is true under the Slater conditions. Theorem 3.62 The duality gap vanishes when the constraints are all affine, or when they satisfy the Sl-CQ in definition 3.59. In this case, any solution λ * of the dual problem provides Lagrange multipliers in theorem 3.61 and conversely.

$L * (λ) = inf{L(x, λ) : x ∈ R d } is such that L * (λ) ≤ F(x)$We justify this statement, as a consequence of theorem 3.61 and the following analysis. The Lagrangian L(x, λ) is linear in λ, and when λ ∈ D, is a convex function of x. Moreover, one can use subdifferential calculus (theorem 3.45) to conclude that, for any λ ∈ D, [(3.55)](#) expresses the fact that 0 ∈ ∂ x L(x * , λ), i.e., that

$x * ∈ argmin R d L(•, λ). Fixing x ∈ R d , one can also consider the maximization of L in λ ∈ D. Clearly, if$x Ω, so that γ i (x) 0 for some i ∈ E or γ i (x) > 0 for some i ∈ I , then max D L(x, λ) = +∞. If x ∈ Ω, then the slackness conditions, which require λ (i) 

$γ i (x) = 0, i ∈ I , ensure that λ ∈ argmax D L(x, •).$As a consequence, any pair We therefore obtain the equivalence of the two properties, for (x * , λ * ) ∈ R d × D:

$x * ∈ Ω, λ * ∈ D satisfying the KKT conditions is such that L(x * , λ) ≤ L(x * , λ * ) ≤ L(x, λ * ) (3.$(i) x * ∈ Ω and (x * , λ * ) satisfies the KKT conditions.

(ii) Equation (3.56) holds for all (x, λ) ∈ R d × D.

Consider now the additional condition that

$(iii) x * ∈ argmin Ω F and λ * ∈ argmax D L * .$We already know that, if (x * , λ * ) satisfy the KKT conditions, then

$x * ∈ argmin Ω F (because N ′ γ (x * ) ⊂ N Ω (x * )). Moreover, if (3.56) holds, then the inequality L(x * , λ) ≤ L(x * , λ * ) implies that L * (λ) ≤ L(x * , λ * ) for all λ ∈ D. The inequality L(x * , λ * ) ≤ L(x, λ * ) for all x implies that L(x * , λ * ) ≤ L * (λ * ).$We therefore obtain the fact that λ * ∈ argmax L * (λ).

To summarize, we have (i) ⇔ (ii) ⇒ (iii).

To obtain the final equivalence, we need to assume constraints qualifications, such as Slater's conditions, to ensure that N ′ γ (x * ) = N Ω (x * ). If this holds, then (iii) implies (via theorem 3.61) that there exists λ such that (i) and (ii) are satisfied for (x * , λ), with L(x * , λ) = L * ( λ) and λ ∈ argmin D L * . This shows that L * ( λ) = L * (λ * ). Moreover, from [(3.56)](#), we have

$L(x * , λ * ) ≤ L(x * , λ) = L * ( λ),$and, by definition of L * , L(x * , λ * ) ≥ L * (λ * ). This shows that L(x * , λ * ) = L(x * , λ). As a consequence, for all (x, λ) ∈ R d × D:

$L(x * , λ) ≤ L(x * , λ) = L(x * , λ * ) = L * (λ * ) = inf R d L(•, λ * ) ≤ L(x, λ * )$so that (x * , λ * ) satisfies (ii).

## Example: Quadratic programming

## Quadratic programming problems minimize

$F(x) = 1 2 x T Ax -b T x, where A is a pos- itive semidefinite matrix and b ∈ R d , subject to affine constraints c T i x -d i = 0, i ∈ E and c T i x -d i ≤ 0, i ∈ I .$We here consider the following objective function. Introduce variables x ∈ R d , x 0 ∈ R and ξ ∈ R N and minimize, for a fixed parameter γ,

$F(x, x 0 , ξ) = 1 2 |x| 2 + γ N k=1$ξ (k)   subject to constraints, for k = 1, . . . , N ξ (k) ≥ 0 and

$b k (x 0 + x T a k ) + ξ (k) ≥ 1$where b k ∈ {-1, 1} and a k ∈ R n respectively denote the kth output and input training sample. This algorithm minimizes a quadratic function of the input variables (x, x 0 , ξ) subject to linear constraints, and is an instance of a quadratic programming problem (this is actually the support vector machine problem for classification, which will be described in section 8.4.1).

Introduce Lagrange multipliers η k for the constraint ξ (k) ≥ 0 and α k for b k (x 0 + x T a k ) + ξ (k) ≥ 1. The Lagrangian then takes the form

$L(x, x 0 , ξ, α, η) = 1 2 |x| 2 + γ N k=1 ξ (k) - N i=1 η k ξ (k) - N k=1 α k (b k (x 0 + x T a k ) + ξ (k) -1) = 1 2 |x| 2 + N k=1 (γ -η k -α k )ξ (k) -x 0 N k=1 α k b k -x T N k=1 α k b k a k + N k=1 α k .$
## DUALITY

We compute the dual Lagrangian L * by minimizing with respect to the primal variables. We note that L * (α, η) = -∞ when N k=1 alpha k b k 0, so that N k=1 α k b k = 0 is a constraint for the dual problem. The minimization in ξ (k) also gives -∞ unless γη kα k = 0, which is therefore another constraint. Finally, the optimal values of x is

$x = N k=1 α k b k a k$and we obtain the expression of the dual problem, which maximizes

$- 1 2 N k,l=1 α k α l b k b l a T k a l + N k=1 α k subject to η k , α k ≥ 0, γ -η k -α k = 0 and N k=1 α k b k = 0.$The conditions on η k and α k can be rewritten as 0 ≤ α k ≤ γ, η k = γα k , and since the rest of the problem does not depends on η, the dual problem can be reduced to maximizing

$L * (α) = - 1 2 N k,l=1 α k α l a T k a l + N k=1 α k subject to 0 ≤ α k ≤ γ and N k=1 α k b k = 0.$
## Proximal iterations and augmented Lagrangian

The concave function L * can be maximized by minimizing -L * using proximal iterations ((3.54)):

$λ(t + 1) = prox -α t L * (λ(t)) = argmax D (λ → L * (λ) - 1 2α t |λ -λ(t)| 2 ).$Introduce the function

$ϕ(x, λ) = F(x) + i∈C λ (i) γ i (x) - 1 2α t |λ -λ(t)| 2 so that λ(t + 1) = argmax µ∈D inf x∈R n ϕ(x, µ).$The function ϕ is convex in x and strongly concave in µ. Results in "minimax theory" [[27]](#b45) implies that one has the equality

$max µ∈D inf x∈R n ϕ(x, µ) = inf x∈R d sup µ∈D ϕ(x, µ) (3.57)$(Note that the left-hand side of this equation is never larger than the right-hand side, but their equality requires additional hypotheses-which are satisfied in our context-in order to hold.)

Importantly, the maximization in µ in the right-hand side has a closed form solution. It requires to maximize

$i∈C µ i γ i (x) - 1 2α t (µ i -λ i (t)) 2$subject to µ i ≥ 0 for i ∈ I , and each µ i can be computed separately. For i ∈ E, there is no constraint on µ i , and one finds

$µ i = λ i (t) + α t γ i (x),$and

$µ i γ i (x) - 1 2α t (µ i -λ i (t)) 2 = λ i (t)γ i + α t 2 γ i (x) 2 = 1 2α t (λ i (t) + α t γ i (x)) 2 - λ i (t) 2 2α t .$For i ∈ I , the solution is

$µ i = max(0, λ i (t) + α t γ i (x))$and one can check that, in this case:

$µ i γ i (x) - 1 2α t (µ i -λ i (t)) 2 = 1 2α t max(0, λ i (t) + α t γ i (x)) 2 - λ i (t) 2 2α t$As a consequence, the right-hand side of (3.57) requires to minimize

$G(x) = F(x) + 1 2α t i∈E (λ i (t) + α t γ i (x)) 2 + 1 2α t i∈I max(0, λ i (t) + α t γ i (x))) 2 - 1 2α t i∈C λ i (t) 2 .$If we assume that the sub-level sets {x ∈ Ω : F(x) ≤ ρ} are bounded (or empty) for any ρ ∈ R, then so are the sets {x ∈ R n : G(x) ≤ ρ}, and this is a sufficient condition for the existence of a saddle point for ϕ, which is a pair (x * , λ * ) such that, for all (x, λ) ∈ R n ×D,

$ϕ(x * , λ) ≤ ϕ(x * , λ * ) ≤ ϕ(x, λ * ).$One can then check that this implies that x * ∈ argmin R n G while λ * = λ(t + 1), so that the latter can be computed as follows:

$                             x(t) = argmin x∈R n F(x) + 1 2α t i∈E (λ i (t) + α t γ i (x)) 2 + 1 2α t i∈I max(0, λ i (t) + α t γ i (x))) 2 λ i (t + 1) = λ i (t) + α t γ i (x(t)), i ∈ E λ i (t + 1) = max(0, λ i (t) + α t γ i (x(t))), i ∈ I (3.58)$These iterations define the augmented Lagrangian algorithm. Starting this algorithm with some λ(0) ∈ R |C| , and constant α, λ(t) will converge to a solution λ of the dual problem. The last two iterations stabilizing imply that γ i (x(t)) converges to 0 for i ∈ E, and also for i ∈ I such that λi > 0, and that lim sup γ i (x(t)) = 0 otherwise. This shows that, if x(t) converges to a limit x, then G( x) = F( x). However, for any x ∈ Ω, we have

$G(x(t)) ≤ G(x) ≤ F(x)$(the proof being left to the reader), showing that x ∈ argmin Ω F.

Note that the augmented Lagrangian method can also be used in non-convex optimization problems [[146]](#b164), requiring in that case that α is small enough.

## Alternative direction method of multipliers

We return to a situation considered in section 3.5.5 where the function to minimize takes the form F(x) = G(x) + H(x). Here, we do not assume that G or H is smooth, but we will need their respective proximal operators to be easy to compute.

The problem can be reformulated as a minimization with equality constraints, namely that of minimizing F(x, z) = G(x) + F(z) subject to x = z. We will actually consider a more general situation, namely the problem minimizing a function F(x, z) subject to constraints Ax + Bz = c where A and B are respectively

$d × n and d × m matrices, x ∈ R n , z ∈ mR m , c ∈ R d .$The augmented Lagrangian algorithm applied to this problem leads to iterate (with only equality constraints)

$         x t , z t = argmin{G(x) + F(z) + 1 2α t |λ t + α t (Ax + Bz -c)| 2 } λ t+1 = λ t + α t (Ax t + Bz t -c) with λ t ∈ R d .$One can now consider splitting the first step in two and iterate:

$                   x t = argmin{G(x) + F(z t-1 ) + 1 2α t |λ t + α t (Ax + Bz t-1 -c)| 2 } z t = argmin{G(x t ) + F(z) + 1 2α t |λ t + α t (Ax t + Bz -c)| 2 } λ t+1 = λ t + α t (Ax t + Bz t -c) (3.59)$These iterations constitute the "alternative direction method of multipliers," or ADMM (it is also sometimes called Douglas-Rachford splitting). It is not equivalent to the augmented Lagrangian algorithm (one would need to iterate a large number of times over the first two steps before applying the third one for this), but still satisfies good convergence properties. The reader can refer to Boyd et al. [[40]](#b58) for a relatively elementary proof that shows that this algorithm converges as soon as, in addition to the hypotheses that were already made, the Lagrangian

$L(x, z, λ) = G(x) + H(z) + λ T (Ax + Bz -c)$has a saddle point: there exists x * , z * , y * such that

$max y L(x * , z * , λ) = L(x * , z * , λ * ) = min x,z L(x, z, λ * ).$
## Convex separation theorems and additional proofs

We conclude this chapter by completing some of the proofs left aside when discussion convex functions. These proofs use convex separation theorems, stated below (without proof).

Theorem 3.63 (c.f., Rockafellar [[167]](#b185)) Let Ω 1 and Ω 2 be two nonempty convex sets with relint(Ω 1 ) ∩ relint(Ω 2 ) = ∅. Then there exists b ∈ R d and β ∈ R such that b 0, b T x ≤ β for all x ∈ Ω 1 and b T x ≥ β for all x ∈ Ω 2 , with a strict inequality for at least one

$x ∈ Ω 1 ∪ Ω 2 .$Theorem 3.64 Let Ω 1 and Ω 2 be two nonempty convex sets with for all y ∈ Ω with a strict inequality for some y ∈ Ω. One says that b and β provide a supporting hyperplane for Ω at x.

$Ω 1 ∩ Ω 2 = ∅ and Ω 1 compact. Then there exists b ∈ R n , β ∈ R and ϵ < 0 such that b T x ≤ β -ϵ for all x ∈ Ω 1 and b T x ≥ β + ϵ for all x ∈ Ω 2 .$$Now, if F is a convex function, with epi(F) = {(y, a) ∈ R d × R : F(y) ≤ a} then relint(epi(F)) = {(y, a) ∈ ridom(F) × R : F(y) < a}$(this simple fact is proved in lemma 3.65 below). In particular, if x ∈ dom(F), then (x, F(x)) must be in the relative boundary of epi(F). This implies that there exists (b, b 0 ) (0, 0) ∈ R d × R such that, for all (y, a) ∈ epi(F):

$b T y + b 0 a ≥ b T x + b 0 F(x) .$If one assumes that x ∈ ridom(F), then, necessarily, b 0 0. To show this, assume otherwise, so that b T y ≥ b T x for all y ∈ dom(F), with b 0. We get a contradiction using the fact that, for some ϵ > 0, [y, x-ϵ(y -x)] belongs to dom(Ω), because b T (y -x) cannot have a constant sign on this segment. So b 0 0 and necessarily b 0 > 0 to ensure that b T y + b 0 a is bounded from below for all a ≥ F(y). Without loss of generality, we can assume b 0 = 1 and we get, for all y ∈ dom(F)

$F(y) + b T y ≥ F(x) + b T x$which shows that -b ∈ ∂F(x), justifying the fact that ∂F(x) ∅ for x ∈ ridom(F).

We now state and prove the result announced above on the relative interior of the epigraph of a convex function. Then, (x, F(x)) ∈ epi(dom(F)) and (y, a)ϵ((x, F(x)) -(y, a)) ∈ epi(F) for small enough ϵ, showing that F(yϵ(xy)) ≤ (1 + ϵ)a -ϵF(x) and yϵ(xy) ∈ dom(F). This proves that y ∈ ridom(F) and the fact that relint(epi(F)) ⊂ Γ .

Take (y, a) ∈ Γ , and (x, b) ∈ epi(F). We need to show that (yϵ(xy), aϵ(ba)) ∈ epi(F) for small enough ϵ, i.e., that

$F(y -ϵ(x -y)) ≤ a -ϵ(b -a)$for small enough ϵ. But this is an immediate consequence of the facts that F is continuous at y ∈ ridom(G) and F(y) < a. Assume that there exists x ∈ ridom(F 1 ) ∩ ridom(F 2 ). Take x ∈ dom(F 1 ) ∩ dom(F 2 ) and g ∈ ∂(F 1 + F 2 )(x). We want to show that g = g 1 + g 2 with g 1 ∈ ∂F 1 (x) and g 2 ∈ ∂F 2 (x).

By definition, we have

$F 1 (y) + F 2 (y) ≥ F 1 (x) + F 2 (x) + g T (y -x)$for all y. We want to decompose g as g = g 1 + g 2 with g 1 ∈ ∂F 1 (x) and g 2 ∈ ∂F 2 (x). Equivalently, we want to find g 2 ∈ R d such that, for all y ∈ R d ,

$F 1 (y) ≥ F 1 (x) + (g -g 2 ) T (y -x) F 2 (y) ≥ F 2 (x) + g T 2 (y -x)$First note that we can replace F 1 by y → F 1 (y) -F 1 (x)g T (yx) and F 2 by y → F 2 (y) -F 2 (x) and assume with loss of generality that F 1 (x) = F 2 (x) = 0 and g = 0.

Making this assumption, we need to find g 2 such that

$F 1 (y) ≥ -g T 2 (y -x) F 2 (y) ≥ g T 2 (y -x)$for all y ∈ R d and some g 2 ∈ R d , under the assumption that F 1 (y) + F 2 (y) ≥ 0 for all y. Introduce the two convex sets in R d × R

$Ω 1 = epi(F 1 ) = {(y, a) ∈ R d × R : F 1 (y) ≤ a} Ω 2 = {(y, a) ∈ R d × R : F 2 (y) ≤ -a} .$The set Ω 2 is the image of epi(F 2 ) by the transformation (y, a) → (y, -a). We have

$relint(Ω 1 ) = epi(F 1 ) = {(y, a) ∈ ridom(F 1 ) × R : F 1 (y) < a} relint(Ω 2 ) = {(y, a) ∈ ridom(F 2 ) × R : F 2 (y) < -a} .$Since F 1 + F 2 ≥ 0, Ω 1 and Ω 2 have non-intersecting relative interiors. We can apply the first separation theorem, providing b

$= (b, b 0 ) ∈ R d × R and β ∈ R such that b (0, 0), b T y + b 0 a -β ≤ 0 for (y, a) ∈ Ω 1 and b T y + b 0 a -β ≥ 0 for (y, b) ∈ Ω 2 ,$with a strict inequality for at least one point in Ω 1 ∪ Ω 2 . We therefore obtain the fact that, for all y and a,

$F 1 (y) ≤ a ⇒ b T y + b 0 a -β ≤ 0 F 2 (y) ≤ -a ⇒ b T y + b 0 a -β ≥ 0.$We claim that b 0 0. Indeed, if b 0 = 0, the statement for F 1 would imply that b T yβ ≤ 0 for all y ∈ dom(F 1 ) and the one on F 2 that b T yβ ≥ 0 for y ∈ dom(F 2 ). The point x ∈ relint(Ω 1 ) ∩ relint(Ω 2 ) should then satisfy b T xβ = 0. We know that there exists a point y ∈ Ω 1 ∪ Ω 2 such that b T y β. Assume that y ∈ Ω 1 , so that b T yβ < 0 and take ϵ >

$0 such that ỹ = x -ϵ(y -x) ∈ Ω 1 . Then b T ỹ -β = -ϵ(b T y -β) < 0,$which is a contradiction. A similar contradiction is obtained when y belongs to Ω 2 , yielding the fact that b 0 cannot vanish.

Moreover, we clearly need b 0 < 0 to ensure that b T y + b 0 aβ ≤ 0 for all large enough a if y ∈ dom(Ω 1 ). There is then no loss of generality in assuming b 0 = -1 and we get

$F 1 (y) ≤ a ⇒ b T y -β ≤ a F 2 (y) ≤ -a ⇒ b T y -β ≥ a, which is equivalent to -F 2 (y) ≤ b T y -β ≤ F 1 (y)$Taking y = x gives β = b T x and we get the desired inequality with g 2 = -b.

## Proof of theorem 3.46

Let x ∈ R m such that A x ∈ ridom(F). We need to prove that ∂G(x)

$⊂ A T ∂F(Ax + b) when G(x) = F(Ax + b).$We assume in the following that b = 0, since the theorem with

$G(x) = F(x + b) is obvious. If g ∈ ∂G(x), we have F(Ay) ≥ F(Ax) + g T (y -x)$for all y ∈ R m . We want to show that there exists h ∈ R d such that g = A T h and, for all z ∈ R d ,

$F(z) ≥ F(Ax) + h T (z -Ax) = F(Ax) + h T z -g T x.$Let

$Ω 1 = epi(F) = {(z, a) :, z ∈ R d , F(z) ≤ a} and Ω 2 = {(Ay, a) : y ∈ R m , a = g T (y -x) + G(x)} ⊂ R d × R.$Note that Ω 2 is an affine space with relint(Ω 2 ) = Ω 2 . If (z, a) ∈ relint(Ω 1 ) ∩ Ω 2 , then z = Ay for some y ∈ R m and g T (yx) + G(x) > F(z) = G(y). This contradicts the fact that g ∈ ∂G(x) and shows that relint(Ω 1 ) ∩ Ω 2 = ∅. As a consequence, there exist (b, b 0 ) (0, 0) and β such that

$F(z) ≤ a ⇒ b T z + b 0 a ≤ β z = Ay, a = g T (y -x) + G(x) ⇒ b T z + b 0 a ≥ β$Assume, to get a contradiction, that b 0 = 0 (so that b 0). Then b T Ay ≥ β for all y, which is only possible if b is perpendicular to the range of A and β ≤ 0. On the other hand,

$F(A x) < ∞ implies that 0 = b T A x + b 0 F(A x) ≤ β, so that β = 0. Furthermore,$we know that one of the inequalities above has to be strict for at least one element of Ω 1 ∪ Ω 2 , but this cannot be true on Ω 2 , so there exists

$z ∈ dom(F) such that b T z < 0. Since b T A x = 0 and A x ∈ ridom(F), we have A x -ϵ(z -A x) ∈ dom(F), so that b T (-ϵz) ≤ 0, yielding a contradiction.$So, we need b 0 0, and the first pair of inequalities clearly requires b 0 < 0, so that we can take b 0 = -1. This shows that

$b T z -β ≤ F(z)$for all z and b T Ayβ ≥ g T (yx) + F(Ax) for all y. Taking y = x, z = Ax, we find that β = b T Ax -F(Ax) yielding

$F(z) -F(Ax) ≥ b T (z -x)$for all z and b T A(yx) ≥ g T (yx) for all y. This last inequality implies that g = A T b and the first one that b ∈ ∂F(Ax), therefore concluding the proof.

## Introduction: Bias, Variance and Density Estimation

In this chapter, we illustrate the bias variance dilemma in the context of density estimation, in which problems are similar to those encountered in classical parametric or non-parametric statistics [[159,](#b177)[60,](#b78)[154]](#b172).

For density estimation, one assumes that a random variable X is given with unknown p.d.f. f and we want to build an estimator, i.e., a mapping (x, T ) → f (x; T ) that provides an estimation of f (x) based on a training set T = (x 1 , . . . , x N ) containing N i.i.d. realizations of X (i.e., T is a realization of T = (X 1 , . . . , X N ), N independent copies of X). Alternatively, we will say that the mapping T → f ( • ; T ) is an estimator of the full density f . Note that, to further illustrate our notation, f (x; T ) is a number while f (x; T ) is a random variable.

## Parameter estimation and sieves

Parameter estimation is the most common density estimation method, in which one restrict f to belong to a finite-dimensional parametric class, denoted (f θ , θ ∈ Θ), with Θ ⊂ R p . For example, f θ can be a family of Gaussian distributions on R d . With our notation, a parametric model provides estimators taking the form

$f (x; T ) = f θ(T ) (x)$and the problem becomes the estimation of the parameter θ.

There are several, well-known methods for parameter estimation, and, since this is not the focus of the book, we only consider the most common one, maximum likelihood, which consists in computing θ that maximizes the log-likelihood

$C(θ) = 1 N N k=1 log f θ (x k ) . (4.1)$The resulting θ (when it exists) is called the maximum likelihood estimator of θ, or m.l.e.

If the true f belongs to the parametric class, so that f = f θ * for some θ * ∈ Θ, standard results in mathematical statistics [[29,](#b47)[118]](#b136) provide sufficient conditions for θ to converge to θ * when N tends to infinity. However, the fact that the true p.d.f. belongs to the finite dimensional class (f θ ) is an optimistic assumption that is generally false. In this regard, the standard theorems in parametric statistics may be regarded as analyzing a "best case scenario," or as performing a "sanity check," in which one asks whether, in the ideal situation in which f actually belongs to the parametric class, the designed estimator has a proper behavior. In non-parametric statistics, a parametric model can still be a plausible approach in order to approximate the true f , but the relevant question should then be whether f provides (asymptotically), the best approximation to f among all f θ , θ ∈ Θ. The maximum likelihood estimator can be analyzed from this viewpoint, if one measures the difference between two density functions by the Kullback-Liebler divergence (also called differential entropy):

$KL(f ∥f θ ) = R d log f (x) f θ (x) f (x)dx (4.2)$which is positive unless f = f θ (and may be equal to +∞).

This expression of the divergence is a simplification of its general measure-theoretic definition, that we now provide for completeness-and future use. Let µ and ν be two probability measures on a set Ω. One says that µ is absolutely continuous with respect to ν, with notation µ ≪ ν, if, for every (measurable) subset A ⊂ Ω, ν(A) = 0 implies µ(A) = 0. The Radon-Nikodym theorem [[31]](#b49) states that µ ≪ ν is and only if there exists a non-negative function g defined on Ω such that

$µ(A) = A g(x)dν(x).$In terms of random variables, this says that, if X : Ω → Ω and Y : Ω → Ω are two random variables with respective distributions µ and ν, and ϕ :

$Ω → R is measurable, then E(ϕ(X)) = E(g(Y )ϕ(Y ))$. The function g is called the Radon-Nikodym derivative of µ with respect to ν and is denoted dµ/dν (it is defined up to a modification on a set of ν-probability zero). The general definition of the Kullback-Liebler divergence between µ and ν is then:

$KL(µ∥ν) =            Ω log dµ dν dµ dν dν if µ ≪ ν + ∞ otherwise (4.3)$In the case when µ = f dx and ν = f dx are both probability measures on R d with respective p.d.f.'s f and f , µ ≪ ν means that f / f is well defined everywhere except on a set of ν-probability zero. It is then equal to dµ/dν. If µ ≪ ν, we can therefore write

$KL(µ∥ν) = R d f (x) f (x) log f (x) f (x) f (x)dx = R d log f (x) f (x) f (x)dx$and we will make the abuse of notation of writing KL(f ∥ f ) for KL(f dx∥ f dx), which gives the expression provided in (4.2).

The general definition also gives a simple expression when Ω is a finite set, with

$KL(µ∥ν) = x∈ Ω log µ(x) ν(x) µ(x),$that we will use later in these notes (if there exists x such that µ(x) > 0 and ν(x) = 0, then KL(µ∥ν) = ∞). The most important property for us is that the Kullback-Liebler divergence can be used as a measure of discrepancy between two probability distribution, based on the following proposition. Proof Assume that µ ≪ ν since the statement is obvious otherwise and let g = dµ/dν. We have Ω gdν = 1 (since, by definition, it is equal to µ( Ω)) so that

$KL(µ∥ν) = Ω (g log g + 1 -g)dν.$We have t log t + 1t ≥ 0 with equality if and only t = 1 (the proof being left to the reader) so that KL(µ∥ν) = 0 if and only if g = 1 with ν-probability one, i.e., if and only if µ = ν.

## ■

Minimizing KL(f ∥f θ ) with respect to θ is equivalent to maximizing

$E f (log f θ ) = R d log f θ (x)f (x)dx ,$and an empirical evaluation of this expectation is[foot_1](#foot_1)

$N N k=1 log f θ (x k )$, which provides the maximum likelihood method. Seen in this context, consistency of the maximum likelihood estimator states that this estimator almost surely converges to a best approximator of the true f in the class (f θ , θ ∈ Θ). More precisely, if one assumes that the function θ → log f θ (x) is continuous 1 in θ for almost all x and that, for all θ ∈ Θ, there exists a small enough δ > 0 such that

$R d sup |θ ′ -θ|<δ log f θ ′ (x) f (x) dx < ∞$then, letting Θ * denote the set of maximizers of E f (log f θ ), and assuming that it is not empty, the maximum likelihood estimator θN is such that, for all ϵ > 0 and all compact subsets K ⊂ Θ, lim

$N →∞ P d( θN , Θ * ) > ϵ and θN ∈ K → 0$where d( θN , Θ * ) is the Euclidean distance between θN and the set Θ * . The interested reader can refer to Van der Vaart [[194]](#b212), Theorem 5.14, for a proof of this statement. Note that this assertion does not exclude the situation in which θN goes to infinity (i.e., steps out of ever compact subset K in Θ), and the boundedness of the m.l.e. is either asserted from additional properties of the likelihood, or by simply restricting Θ to be a compact set.

If Θ * = {θ * } and the m.l.e. almost surely converges to θ * , the speed of convergence can also be quantified by a central limit theorem (see Van der Vaart [[194]](#b212), Theorem 5.23) ensuring that, in standard cases √ N ( θNθ * ) converges to a normal distribution.

Even though these results relate our present subject to classical parametric statistics, they are not sufficient for our purpose, because, when f f θ * , the convergence of the m.l.e. to the best approximator in Θ still leaves a gap in the estimation of f . This gap is often called the bias of the class (f θ , θ ∈ Θ). One can reduce it by considering larger classes (e.g., with more dimensions), but the larger the class, the less accurate the estimation of the best approximator becomes for a fixed sample size (the estimator has a larger variance). This issue is known as the "bias vs. variance dilemma," and to address it, it is necessary to adjust the class Θ to the sample size in order to optimally balance the two types of error (and all non-parametric estimation methods have at least one mechanism that allows for this). When the "tuning parameter" is the dimension of Θ, the overall approach is often referred to as the method of sieves [[83,](#b101)[80]](#b98), in which the dimension of Θ is increased as a function of N in a suitable way.

Gaussian mixture models provide one of the most popular choices with the method of sieves. Modeling in this setting typically follows some variation of the following construction. Fix a sequence (m N , N ≥ 1) and let

$Θ N = f : f (x) = m N j=1 α j e -|x-µ j | 2 /2σ 2 (2πσ 2 ) d/2 , µ 1 , . . . , µ m N ∈ R d , α 1 + • • • + α m N = 1, α 1 , . . . , α m N ∈ [0, +∞), σ > 0 . (4.4)$There are therefore (d + 1)m N free parameters in Θ N . The integer m N allows one to adjust the dimension of Θ N and therefore controls the bias-variance trade-off. If m N tends to infinity "slowly enough," the m.l.e. will converges (almost surely) to the true p.d.f. f [[80]](#b98). However, determining optimal sequences N → m N remains a challenging and largely unsolved problem.

In practice the computation of the m.l.e. in this context uses an algorithm called EM, for expectation-maximization. This algorithm will be described later in chapter 16.

## Kernel density estimation

Kernel density estimators [[150,](#b168)[177,](#b195)[178]](#b196) provide alternatives to the method of sieves. They also lend themselves to some analytical developments that provide elementary illustrations of the bias-variance dilemma.

## Define a kernel function as a function

$K : R d → [0, +∞) such that R d K(x)dx = 1, R d |x|K(x) dx < ∞, R d xK(x) dx = 0. (4.5)$Note that the third equation is satisfied, in particular, when K is an even function, i.e., K(-x) = K(x).

Given K and a scalar σ > 0, the rescaled kernel is defined by

$K σ (x) = 1 σ d K x σ .$Using the change of variable y = x/σ (so that dy = dx/σ d ) one sees that K σ satisfies (4.5) as soon as K does.

Based on a training set T = (x 1 , . . . , x N ), the kernel density estimator defines the family of densities

$fσ (x; T ) = 1 N N k=1 K σ (x -x k ) One has R d K σ (x -x k ) dx = 1$so that it is clear that fσ is a p.d.f. In addition,

$R d xK σ (x -x k ) dx = R d (y + x k )K σ (y) dy = x k so that R d x fσ (x; T ) dx = x where x = (x 1 + • • • + x N )/N . A typical choice for K is a Gaussian kernel, K(y) = e -|y| 2 /2 /(2π) d/2$. In this case, the estimated density is a sum of bumps centered at the data points x 1 , . . . , x N . The width of the bumps is controlled by the parameter σ . A small σ implies less rigidity in the model, which will therefore be more affected by changes in the data: the estimated density will have a larger variance. The converse is true for large σ , at the cost of being less able to adapt to variations in the true density: the model has a larger bias (see fig. As we now show, in order to get a consistent estimator, one needs to let σ = σ N depend on the size of the training set. We have, taking expectations with respect to training data,

$E( fσ (x; T )) = 1 N σ d N k=1 E K((x -X k )/σ ) = 1 σ d R d K((x -y)/σ )f (y)dy = R d K(z)f (x -σ z)dz$The bias of the estimator, i.e., the average difference between fσ (x; T ) and f (x) is therefore given by

$E( fσ (x; T )) -f (x) = R d K(z)(f (x -σ z) -f (x))dz.$Interestingly, this bias does not depend on N , but only on σ , and it is clear that, under mild continuity assumptions on f , it will go to zero with σ .

The variance of fσ (x; T ) is given by var( fσ (x; T )) = 1 N σ 2d var(K((x -X)/σ )) with

$1 N σ 2d var(K((x -X)/σ )) = 1 N σ 2d d R K((x -y)/σ ) 2 f (y)dy - 1 N σ 2d R d K((x -y)/σ )f (y)dy 2 = 1 N σ d R d K(z) 2 f (x -σ z)dz - 1 N R d K(z)f (x -σ z)dz 2$The total mean-square error of the estimator is

$E(( fσ (x) -f (x)) 2 ) = var( fσ (x)) + (E( fσ (x)) -f (x)) 2 .$Clearly, this error cannot go to zero unless we allow σ = σ N to depend on N . For the bias term to go to zero, we know that we need σ N → 0, in which case we can expect the second term in the variance to decrease like 1/N , while, for the first term to go to zero, we need N σ d N to go to infinity. This illustrates the bias-variance dilemma: σ N must go to zero in order to cancel the bias, but not too fast in order to also cancel the variance. There is, for each N , an optimal value of σ that minimizes the error, and we now proceed to a more detailed analysis and make this statement a little more precise.

Let us make a Taylor expansion of both bias and variance, assuming that f has at least three bounded derivatives and that R d |x| 3 K(x) dx < ∞. We can write

$f (x -σ z) = f (x) -σ z T ∇f (x) + σ 2 2 z T ∇ 2 f (x)z + O(σ 3 |z| 3 ),$where ∇ 2 f (x) denotes the matrix of second derivatives of f at x. Since zK(z)dz = 0, this gives

$E( fσ (x; T )) -f (x) = σ 2 2 M f (x) + o(σ 2 ) with M f = K(z) z T ∇ 2 f (x)z dz. Similarly, letting S = K 2 (z) dz, var( fσ (x)) = 1 N σ d Sf (x) + o(σ d + σ 2 ) .$Assuming that f (x) > 0, we can obtain an asymptotically optimal value for σ by minimizing the leading terms of the mean square error, namely

$σ 4 4 M 2 f + S N σ d f (x) which yields σ N = O(N -1/(d+4)$) and

## E((

$fσ N (x; T ) -f (x)) 2 ) = O(N -4/(d+4) ).$If f has r + 1 derivatives, and K has r -1 vanishing moments (this excludes the Gaussian kernel) one can reduce this error to N -2r 2r+d . These rates can be shown to be "optimal," in the "min-max" sense, which roughly expresses the fact that, for any other estimator, there exists a function f for which the convergence speed is at least as "bad" as the one obtained for kernel density estimation. This result says that, in order to obtain a given accuracy ϵ in the worst case scenario, N should be chosen of order (1/ϵ) 1+(d/2r) which grows exponentially fast with the dimension. This is the curse of dimensionality which essentially states that the issue of density estimation may be intractable in large dimensions. The same statement is true also for most other types of machine learning problems. Since machine learning essentially deals with high-dimensional data, this issue can be problematic.

Obviously, because the min-max theory is a worst-case analysis, not all situations will be intractable for a given estimator, and some cases that are challenging for one of them may be quite simple for others: even though all estimators are "cursed," the way each of them is cursed differs. Moreover, while many estimators are optimal in the min-max sense, this theory does not give any information on "how often" an estimator performs better than its worst case, or how it will perform on a given class of problems. (For kernel density estimation, however, what we found was almost universal with respect to the unknown density f , which indicates that this estimator is not a good choice in large dimensions.)

Another important point with this curse of dimensionality is that data may very often appear to be high dimensional while it has a simple, low-dimensional structure, maybe because many dimensions are irrelevant to the problem (they contain, for example, just random noise), or because the data is supported by a non-linear low-dimensional space, such as a curve or a surface. This information is, of course, not available to the analysis, but can sometimes be inferred using some of the dimension reduction methods that will be discussed later in chapter 20. Sometimes, and this is also important, information on the data structure can be provided by domain knowledge, that is, by elements, provided by experts, that specify how the data has been generated (such as underlying equations) and reasonable hypotheses that are made in the field. This source of information should never be ignored in practice.

## Chapter 5

Prediction: Basic Concepts

## General Setting

The goal of prediction is to learn, based on training data, an input-output relationship between two random variables X and Y , in the sense of finding, for a specified criterion, the best function of the input X that predicts the output Y . (In statistics, Y is often called the dependent variable, and X the independent variable.) We will, as always, assume that all the variables mentioned in this chapter are defined on a fixed probability space (Ω, P). We assume that X : Ω → R X , where R X is the input space, and Y : Ω → R Y , where R Y is the output space. The input-output relationship is therefore captured by an unknown function f : R X → R Y , the predictor.

The following two subclasses of prediction problems are important enough to have learned their own names and specific literature.

• Quantitative output: R Y = R q (often with q = 1). One then speaks of a regression problem.

• Categorical output: R Y = {g 1 , . . . , g q } is a finite set. One then speaks of a classification problem.

In most cases, the input space is Euclidean, i.e., R X = R d . Note also that, in classification, instead of a function f : R → R Y , one sometimes estimates a function f : R X → Π(R Y ), where Π(R Y ) is the space of probability distributions on R Y . We will return to this in remark 5.4.

The quality of a prediction is assessed through the definition of a risk function. Such a function, denoted r, is defined on R Y ×R Y , takes values in [0, +∞) and should be understood as r(True output, Predicted output), [(5.1)](#) so that r(y, y ′ ) assigns a cost to the situation in which a true y is predicted by y ′ . Note that this definition is asymmetric, and there is no requirement that r(y, y ′ ) = r(y ′ , y). It is important to remember our convention that the first variable is the true observation and the second one is a place-holder for a prediction. Risk functions are also called loss functions, or simply cost functions and we will use these terms as synonyms.

The goal in prediction is to minimize the expected risk, also called the generalization error:

$R(f ) = E(r(Y , f (X))).$We will prove that an optimal f can be easily described based on the joint distribution of X and Y (which is, unfortunately, never available). We will need for this to use conditional expectations and conditional probabilities and proceed first to a reminder of their definitions and properties.

## Conditional expectation

$If ξ : Ω → R ξ and η : Ω → R η ⊂ R d are discrete random variables, then P(η = η | ξ = ξ) = P(η = η, ξ = ξ)/P(ξ = ξ)$if P(ξ = ξ) > 0 and is undefined otherwise. Then, if η is real-valued and discrete, one defines the conditional expectation of η given ξ, denoted E(η | ξ), by

$E(η | ξ)(ω) = η∈R η ηP(η = η | ξ = ξ(ω))$for all ω such that P(ξ = ξ(ω)) > 0. Note that E(η | ξ) is a random variable, defined over Ω. It however only depends on the values of ξ, in the sense that

$E(η | ξ)(ω) = E(η | ξ)(ω ′ ) if ξ(ω) = ξ(ω ′ ). We will use the notation E(η | ξ = ξ) = η∈R η ηP(η = η | ξ = ξ), which is now a function defined on R ξ . One has E(η | ξ)(ω) = E(η | ξ = ξ(ω)).$
## One can characterize E(η | ξ) by the properties

$       E(η | ξ) is a function of ξ ∀f : R η → R, E(E(η | ξ)f (ξ)) = E(ηf (ξ)). (5.2)$The proof that our definition of E(η | ξ) for discrete random variables is the only one satisfying these properties is left to the reader. The interest of reformulating the definition of the conditional expectation via [(5.2)](#) is that this provides a definition that works for general random variables (with the additional assumption that f is measurable), not only for discrete ones. We assume below that (R ξ , S ξ ) and (R η , S η ) are measurable spaces.

$Definition 5.1 Assume that R η = R d . Let ξ : Ω → R ξ and η : Ω → R η be two random variables with E(|η|) < ∞. The conditional expectation of η given ξ is a random variable ζ : Ω → R η such that (i) There exists a function h : R ξ → R such that ζ = h • ξ almost surely.$(ii) For any measurable function g : R ξ → [0, +∞), one has

$E(ηg • ξ) = E(ζg • ξ).$
## The variable ζ is then denoted E(η|ξ) and the function h in

$(i) is denoted E(η|ξ = •).$Importantly, functions ζ satisfying conditions (i) and (ii) always exists and are almost surely unique, in the sense that if another function ζ ′ satisfies these conditions, then ζ = ζ ′ with probability one. One obtains an equivalent definition if one restricts functions g in (ii) to indicators of measurable sets, yielding the condition that, if A ⊂ R ξ is measurable, E(η1 ξ∈B ) = E(ζ1 ξ∈B ).

Taking g(ξ) = 1 for all ξ ∈ R ξ in condition (ii), one gets the well-known identity

## E(E(η|ξ)) = E(η).

Moreover, for any function g defined on R ξ we have E(ηg

$• ξ|ξ) = (g • ξ)E(η|ξ),$which can be checked by proving that the right-hand side satisfies the conditions (i) and (ii).

Conditional expectations share many of the properties of simple expectations. For example, if η ≤ η ′ , both taking scalar values, then

$E(η | ξ) ≤ E(η ′ | ξ) almost surely. Jensen's inequality also holds: if γ : R d → R is convex and γ • η is integrable, then γ • E(η | ξ) ≤ E(γ • η | ξ).$We will discuss convex functions in chapter

3, but two important examples for this section are γ(η) = |η| and γ(η) = |η| 2 . The first one implies that |E(η | ξ)| ≤ E(|η| | bf xi) and, taking expectations on both sides: E(|E(η | ξ)|) ≤ E(|η|), the upper bound being finite by assumption. For the square norm, we find that, if η is square integrable, then so is E(η | ξ) and

$E(|E(η | ξ)| 2 ) ≤ E(|η| 2 ).$If η is square integrable, then this inequality shows that E(η | ξ) minimizes E[|η -ζ| 2 ] among all square integrable functions ζ : Ω → R η that satisfy (i). In other terms, the conditional expectation is the optimal least-square approximation of η by a function of ξ. To see this, just write

$E[|η -ζ| 2 | ξ] = E[|η| 2 | ξ] -2E[η T ζ | ξ] + |ζ| 2 = E[|η| 2 | ξ] -2E(η | ξ) T ζ + |ζ| 2 = E[|η| 2 | ξ] -|E(η | ξ)| 2 + |E(η | ξ) -ζ| 2 = E[|η -E(η | ξ)| 2 | ξ] + |E(η | ξ) -ζ| 2 ≥ E[|η -E(η | ξ)| 2 | ξ]$and taking expectations on both sides yields the desired result.

$If A is a measurable subset of R η , the conditional expectation E(1 A | ξ) (resp. E(1 A | ξ = ξ)) is denoted P(η ∈ A | ξ) (resp. P(η ∈ A | ξ = ξ)), or P η (A | ξ) (resp. P η (A | ξ = ξ))$. While these functions are defined separately up to modifications on sets of probability zero. Under general assumptions on the set R η and its σ -algebra (always satisfied in our discussions), these conditional probabilities can be defined together so that, for all ω

$∈ Ω A → P η (A | ξ)(ω) is a probability distribution on R η such that E(η | ξ) = R η ηP(dη | ξ).$Assume that the the sets R ξ and R η are equipped with measures, say µ ξ and µ η such that the joint distribution of (ξ, η) is absolutely continuous with respect to µξ ⊗ µ η , so that there exists a function ϕ : R ξ × R η → R (the p.d.f. of (ξ, η) with respect to µξ ⊗ µ η ) such that

$P(ξ ∈ A, η ∈ B) = A×B ϕ(ξ, η)µ ξ ⊗ µ η (dx, dη).$Then P η (• | ξ) is absolutely continuous with respect to µ η , with density given by the conditional p.d.f. of η given ξ, namely,

$ϕ(• | ξ) : (η, ω) → ϕ(η, ξ(ω)) R η ϕ(η ′ , ξ(ω)) µ η (dη ′ ) = ϕ(η | ξ = ξ(ω)). (5.3) Note that P        ω : R η ϕ(η ′ , ξ(ω)) µ η (dη ′ ) = 0        = 0$so that the conditional density can be defined arbitrarily when the numerator vanishes 1 .

The most common example is when R ξ and R η are Euclidean spaces and µ ξ , µ η are Lebesgue's measures, in which case [(5.3)](#) is the usual definition of conditional p.d.f.'s. Note also that, for discrete random variables, (5.3) coincides with the definition of conditional probabilities P(η = η | ξ = ξ(ω)) when µ x and µ η are counting measures. As a last example, if

$R ξ = R d , µ ξ is Lebesgue's measure and η is discrete, then ϕ(η | ξ = ξ(ω)) = ϕ(η, ξ(ω)) η ′ ∈R η ϕ(η ′ , ξ(ω))$.

## Bayes predictor

Recall that r : (y, y ′ ) → r(y, y ′ ) denotes the risk function and that we want to minimize

$R(f ) = E(r(Y , f (X)) over all possible predictors f . Definition 5.2 A Bayes predictor is a measurable function f : R X → R Y such that, for all x ∈ R X , E r(Y , f (x)) | X = x = min E r(Y , y ′ ) | X = x : y ′ ∈ R Y$There can be multiple Bayes predictors if the minimum in the proposition is not uniquely attained. Note that, if f * is a Bayes predictor and f any other predictor, we have, by definition

$E r(Y , f * (X)) | X ≤ E r(Y , f (X)) | X .$Passing to expectations, this implies R(f * ) ≤ R( f ). We therefore have the following result:

Theorem 5.3 Any Bayes predictor f * is optimal, in the sense that it minimizes the generalization error R.

Example 1. Regression with mean-square error. When R X = R d and R Y = R q , the most common risk function is the squared norm r(y, y ′ ) = |yy ′ | 2 . The resulting generalization error is called the MSE (mean square error) and given by R(f

$) = E(|Y - f (X)| 2 ). The Bayes predictor is such that f * (x) minimizes t → E(|Y -t| 2 | X = x). 1 Letting ϕ ξ (ξ) = R η ϕ(η ′ , ξ) µ η (dη ′ )$, which is the marginal p.d.f. of ξ with respect to µ ξ , we have

$P(ϕ ξ (ξ) = 0) = R ξ 1 ϕ ξ (ξ)=0 ϕ ξ (ξ)µ ξ (dξ) = 0. Let f * (x) = E(Y | X = x) and write E(|Y -t| 2 | X = x) =E(|Y -f * (x)| 2 | X = x) + 2E((Y -f * (x)) T (f * (x) -t) | X = x) + |f * (x) -t| 2 =E(|Y -f * (x)| 2 | X = x) + 2E((Y -f * (x)) T | X = x)(f * (x) -t) + |f * (x) -t| 2 =E(|Y -f * (x)| 2 | X = x) + |f * (x) -t| 2 .$This proves that E(Y | X = x) is the unique Bayes classifier (up to a modification on a set of probability 0).

Example 2. Classification with zero-one loss. Let R X = R d and R Y be a finite set. The zero-one loss function is defined by r(y, y ′ ) = 1 if y y ′ and 0 otherwise. From this, it results that the generalization error is the probability of misclassification R(f ) = P (Y f (X)) (also called the misclassification error).

The Bayes predictor is such that f * (x) minimizes

$g → P(Y g | X = x) = 1 -P(Y = g | X = x).$It is therefore given by the so-called posterior mode:

$f * (x) = argmax g P(Y = g | X = x).$Remark 5.4 As mentioned at the beginning of the chapter, one sometimes replaces a pointwise prediction of the output by a probabilistic one, so that f (x) is a probability distribution on R Y . If A is a (measurable) subset of R Y , we will write f (x, A) rather than f (x)(A).

In such a case, the loss function, r, is defined on R Y × Π(R Y ), and the expected risk is still defined by E(r(Y , f (X))).

It is quite natural to require that π → r(y, π) is minimized. For classification problems, where R Y is finite, one can choose

$r(y, π) = -log π(y) (5.4)$The Bayes estimator is then a minimizer of π

$→ -E(log π(Y ) | X = x). The solution is (unsurprisingly) f (x, y) = P(Y = y | X = x) since we always have -E(log π(Y ) | X = x) = - y∈R Y log π(y)f (x, y) ≥ - y∈R Y log f (x, y)f (x, y).$The difference between these terms is indeed

$y∈R Y log f (x, y) π(y) f (x, y) = KL(f (x, •), π) ≥ 0.$For regression problems, with R Y = R q , one can choose

$r(y, π) = R q |z -y| 2 π(dz)$which is indeed minimum when π is concentrated on y. Here, the Bayes estimator minimizes (with respect to π)

$R q R q |z -y| 2 π(dz)P Y (x, dy) = R q R q |z -y| 2 P Y (x, dy) π(dz)$where

$P Y (x, •) is the conditional distribution of Y given X = x.$For any z, one has

$R q |y -z| 2 f (x, dy) ≥ R q |y -E(Y | X = x)| 2 f (x, dy)$♦ which shows that the Bayes estimator is, in this case, the Dirac measure concentrated at E(Y | X = x).

## Examples: model-based approach

Bayes predictors are never available in practice, because the true distribution of (X, Y ), or that of Y given X, are unknown. These distributions can only be inferred from observations, i.e., from a training set: T = (x 1 , y 1 , . . . , x N , y N ). This is the approach followed by model-based, or generative methods, namely using training data to approximate the joint distribution of X and Y with a statistical model estimated from data before using the Bayes estimator derived from this model for prediction. We now illustrate this approach with a few examples.

## Gaussian models and naive Bayes

Consider a regression problem with R Y = R, and model the joint distribution of (X, Y ) as a (d + 1)-dimensional Gaussian distribution with mean µ and covariance matrix Σ, which must be estimated from data. Write µ = m µ 0 , with

$µ 0 ∈ R, m ∈ R d$and Σ in the form, for some symmetric matrix S and d-dimensional vector u

$Σ = S u u T σ 2 00 .$Then, letting ∆ = σ 2 00u T S -1 u,

$Σ -1 = 1 ∆ ∆S -1 + S -1 uu T S -1 -S -1 u -u T S -1 1 .$This shows that the joint p.d.f. of (X, Y ) is proportional to

$exp - 1 2∆ (y -µ 0 ) 2 -2u T S -1 (x -m)(y -µ 0 ) + (terms not depending on y) .$In particular

$E(Y |X) = µ 0 + u T S -1 (x -m),$which provides the least-square linear regression predictor. (In this expression, u is the covariance between X and Y and S is the covariance matrix of X.)

If one restricts the model to having a diagonal covariance matrix S, then

$E(Y |X) = µ 0 + d j=1 u (j) s jj (x (j) -m (j) ).$This predictor is often called the naive Bayes predictor for regression.

## Kernel regression

Let R X = R d and R Y = R. Let K 1 : R d → R and K 2 : R → R be two kernels, therefore satisfying

$R d K 1 (x)dx = R K 2 (x)dx = 1; R d xK 1 (x)dx = R yK 2 (y)dy = 0. Let K(x, y) = K 1 (x)K 2 (y) so that R d+1 K(x, y)dydx = 1 R d+1 yK(y, x)dydx = 0 R d+1 xK(y, x)dydx = 0.$The kernel estimator of the joint p.d.f., ϕ, of (X, Y ) at scale σ is, in this case:

$φ(x, y) = 1 N N k=1 1 σ d+1 K 1 x -x k σ K 2 y -y k σ .$Based on φ, the conditional expectation of Y given X is

$f (x) = 1 N N k=1 1 σ d+1 R yK 1 x-x k σ K 2 y-y k σ dy 1 N N k=1 1 σ d+1 R K 1 x-x k σ K 2 y-y k σ dy .$Using the fact that σ -1 R yK 2 y-y k σ dy = y k , we can simplify this expression to obtain

$f (x) = N k=1 y k K 1 x-x k σ N k=1 K 1 x-x k σ .$This the kernel-density regression estimator [[139,](#b157)[202]](#b220).

## A classification example

Let R Y = {0, 1} and assume R X = N = {0, 1, 2, . . .}. Let p = P(Y = 1) and assume that conditionally to Y = g, X follows a Poisson distribution with mean µ g . Assume that

$µ 0 < µ 1 .$The posterior distribution of Y given X = x is[foot_2](#foot_2)

$P(Y = g | X = x) ∝ (1 -p)µ x 0 e -µ 0 if g = 0 pµ x 1 e -µ 1 if g = 1 A Bayes classifier is then provided by taking f (x) = 1 if log p + x log µ 1 -µ 1 ≥ log(1 -p) + x log µ 0 -µ 0 that is: x log µ 1 µ 0 ≥ log 1 -p p + µ 1 -µ 0$Since we are assuming that µ 1 > µ 0 , we find that f (x) = 1 if[foot_3](#foot_3)

x ≥ log((1p)/p) + µ 1µ 0 log(µ 1 /µ 0 ) and 0 otherwise.

## Empirical risk minimization 5.5.1 General principles

Model-based approaches for prediction are based on the estimation of the joint distribution of the input and output variables, which is arguably a harder problem than prediction [[196]](#b214). Since the goal is to find f minimizing the expected risk

$R(f ) = E(r(Y , f (X))$, one may prefer a direct approach and consider the minimization of an empirical estimate of this risk, based on training data T = (x 1 , y 1 , . . . , x N , y N ), namely

$R(f ) = 1 N N k=1 r(y k , f (x k )).$This strategy is called empirical risk minimization.

Importantly, R must be minimized over a restricted class, F , of predictors to avoid overfitting. For example, with R Y = R and R = R d , one can take

$F =        f : f (x) = β 0 + d i=1 b (i) x (i) : β 0 , b (1) . . . , b (d) ∈ R        . Minimizing the empirical mean-square error R(f ) = 1 N N k=1 (y k -f (x k )) 2$over f ∈ F leads to the standard least-square regression estimator.

As another example, consider

$F =          f : f (x) = p j=1 w j ψ        β j0 + d i=1 β ji x (i)        , w j , β ji ∈ R          .$with a fixed function ψ. This corresponds to a two-layer perceptron model.

As a last example for now (we will see many others in the rest of this book), taking d = 1, the set

$F = f : R f ′′ (x) 2 dx < µ$(with µ > 0) provides an infinite dimensional space of predictors, which leads to spline regression.

## Bias and variance

We give a further illustration of the bias-variance dilemma in the regression case, using the mean-square error and taking q = 1 to simplify. Denote the Bayes predictor by

$f * (x) = E(Y | X = x).$Fix a function space F , and let f * be the optimal predictor in F , in the sense that it minimizes E(|Yf (X)| 2 ) over f ∈ F . Then, letting fN ∈ F denote an estimated predictor,

$R( fN ) = E(|Y -fN (X)| 2 ) = E(|Y -f * (X)| 2 ) + E(| fN (X) -f * (X)| 2 ) +2E((Y -f * (X))( f * (X) -fN (X))$Let us make the assumption that there exists ϵ > 0 such that

$f λ = f * + λ( fN -f * ) belongs to F for λ ∈ [-ϵ, ϵ].$This happens when F is a linear space, or more generally when F is convex and f * is in its relative interior (see chapter 3). Let ψ : λ → E(|Yf λ (X)| 2 ), which is minimal at λ = 0. We have

$ψ(λ) =E(|Y -f * (X) -λ( fN (X) -f * (X))| 2 ) =E(|Y -f * (X)| 2 ) -2λE((Y -f * (X))( fN (X) -f * (X))) + λ 2 E(| fN (X) -f * (X))| 2 ) and 0 = ψ ′ (0) = 2E((Y -f * (X))( f * (X) -fN (X)))$We therefore get the identity

$R( fN ) = E(|Y -f * (X)| 2 ) + E(| fN (X) -f * (X)| 2 ) = "Bias" + "Variance".$The bias can be further decomposed as

$E(|Y -f * (X)| 2 ) = E(|Y -f * (X)| 2 ) + E(|f * (X) -f * (X)| 2 )$because f * is the conditional expectation. As a result, we obtain an expression the generalization error with three contributions, namely,

$R( fN ) ≤ E(|Y -f * (X)| 2 ) + E(|f * -f * (X)| 2 ) + E(| fN (X) -f * (X)| 2 ).$The first term is the Bayes error. It is fixed by the joint distribution of X and Y and measures how well Y can be approximated by a function of X. The second term compares f * to its best approximation in F , and is therefore reduced by taking larger model spaces. The last term is the error caused by using the data to estimate f * . It increases with the size of F . This is illustrated in Figure [5](#fig_1).1.

Remark 5.5 If the assumption made on f * is not valid, one can write

$R( fN ) = E(|Y -fN (X)| 2 ) ≤ 2 E(|Y -f * (X)| 2 ) + E(| fN (X) -f * (X)| 2 )$and still obtain a control (as an inequality) of the generalization error by a bias-plusvariance sum.

$♦ P P * F f f * f *$
## Probability space

Predictor space Figure [5](#fig_1).1: Sources of errors in statistical Learning: When P * is the distribution of the data, the optimal predictor f * minimizes the expected loss function. Based on data Z 1 , . . . , Z N , the sample-based distribution is P = (δ

$Z 1 + • • • + δ Z N )/N$and the empirical loss is minimized over a subset S of the space of all possible estimators. The expected discrepancy between the resulting estimator and the one minimizing the true expected loss on the subspace is the "variance" of the method, and the expected discrepancy between this subspace-constrained estimator and and the optimal one is the "bias."

5.6 Evaluating the error 5.6.1 Generalization error Given input and output variables X : Ω → R X and Y : Ω → R Y and a risk function r : R Y × R Y → [0. + ∞), we have defined the generalization (or prediction) error as R(f ) = E(r(Y , f (X))) . Recall that a training set T = ((x 1 , y 1 ), . . . , (x N , y N )) is a realization T = T (ω) of the random variable T = ((X 1 , Y 1 ), . . . , (X N , Y N )), an i.i.d. sample of the joint distribution of (X, Y ). A learning algorithm is a function T → fT defined on the set of training sets, namely, ∞ N =1 (R × R Y ) N and taking values in F .

For a given T and a specific algorithm, one is primarily interested in evaluating R( fT ), the generalization error of the predictor estimated from observed data. To emphasize the fact that the training set is fixed in this expression, one often writes:

$R( fT ) = E(r(Y , fT (X))|T = T )$If we also take the expectation with respect to T (for fixed N ), we obtain the averaged generalization risk as

$E(R( fT )) = E(r(Y , fT (X))),$which provides an evaluation of the average quality of the algorithm when evaluated on random training sets of size N . If A : T → fT denotes the learning algorithm, we will denote R N (A) = E(R( fT )).

Since their computation requires the knowledge of the joint distribution of X and Y , these errors are not available in practice. Given a training set T and a predictor f , one can compute the empirical error

$RT (f ) = 1 N N k=1 r(y k , f (x k )) .$Under the usual moment conditions, the law of large numbers implies that RT (f ) → R(f ) with probability one for any given predictor f . However, the law of large numbers cannot be applied to assess whether the in-sample error,

$E T ∆ = RT ( fT ) = 1 N N k=1 r(y k , fT (x k )),$is a good approximation of the generalization error R( fT ). This is because each term in the sum depends on the full data set, so that E T is not a sum of independent terms. The in-sample error typically under-estimates the generalization error, sometimes with a large discrepancy.

When one has enough data, however, it is possible to set some of it aside to form a test set. Formally, a test set is a collection

$T ′ = (x ′ 1 , y ′ 1 , . . . , x ′ N ′ , y ′ N ′ ) considered as a realization of an i.i.d. sample of (X, Y ), T ′ = (X ′ 1 , Y ′ 1 , . . . , X ′ N ′ , Y ′ N ′ ), independent of T .$The test set error is then given by

$E T ,T ′ = RT ′ ( fT ) = 1 N ′ N ′ k=1 r(y ′ k , fT (x ′ k )).$The law of large numbers (applied conditionally to T = T ) implies that E T ,T ′ converges to R( fT ) with probability one when N ′ → ∞.

However, in many applications, data acquisition is difficult or expensive (e.g., in the medical field) and sparing a part of it in order to form a test set is not a reasonable option. In such situations, cross-validation is generally a preferred alternative.

## Cross validation

## Cross-validation error

The n-fold cross-validation method (see, e.g., Stone [[184]](#b202)) separates the training set into n non-overlapping sets of equal sizes, and estimates n predictors by leaving out one of these subsets as a temporary test set. A generalization error is estimated from each test set and averaged over the n results.

Let us formalize this computation after introducing some notation. We represent training data in the form T = (z 1 , . . . , z N ), a sample of a random variable Z. With this notation, we can include supervised problems, such as prediction (taking Z = (X, Y )) and unsupervised ones such as density estimation (taking Z = X). One tries to estimate a function f within a given class (e.g., a predictor, or a density) and one has a measure of "loss", denoted ℓ(f , z) ≥ 0 measuring how badly f performs on the data z. For prediction, one takes ℓ(f , z) = r(y, f (x)) with z = (x, y) and for density estimation, e.g., ℓ(f , z) =log f (z), the negative log-likelihood. One then lets R(f ) = E(ℓ(f , Z)). For an algorithm A : T → fT , the loss R(A) is the quantity of interest.

## Given another set T

$′ = (z ′ 1 , . . . , z ′ N ′ ), the empirical loss is RT ′ (f ) = 1 N ′ N ′ k=1 ℓ(f , z ′ k )$and, using T as a training set and T ′ as a test set, we let

$E T ,T ′ = RT ′ ( fT ).$To define an n-fold cross-validation estimator of the error, one assumes that the training set T is partitioned into n subsets of equal sizes (up to one element if N is not a multiple of n), T 1 , . . . , T n , so that T i and T j are non-intersecting if i j, and T = n i=1 T i . For each i, let T (i) = T \ T i , which provides the training data with the elements of T i removed. Then, the n-fold cross-validation error is defined by

$E CV (T ) = 1 n n i=1 E T (i) ,T i .$Assuming, to simplify, that N is a multiple of n, the expectation of the crossvalidation error is E(R( fT N ′ )), where the average is made over training sets T N ′ of size N ′ = N -N /n. Note that the cross-validation error is an estimate of the average error of the algorithm over random training sets, not necessarily that of the current estimator fT . It returns an evaluation of the algorithm A : T → fT . When needed, one can emphasize this and write RCV,T (A).

The limit case when n = N is called leave-one-out (LOO) cross validation. In this case E CV is an almost unbiased estimator of E(R( fT )), but, because it is an average of functions of the training set that are quite similar (and that will therefore be positively correlated), its variance (as a function of T ) may be quite large. Conversely, smaller values of n will have smaller variances, but larger biases. In practice, it is difficult to assess which choice of n is optimal, although 5-or 10-fold cross-validation is quite popular. LOO cross-validation is also often used, especially when N is small.

## Model selection using cross validation

Because it evaluates the quality of an algorithm, cross-validation is often used to perform model selection. Indeed, many learning algorithms depends on a parameter, that we will denote λ. In kernel density estimation, for example, λ = σ is the kernel width. For mixtures of Gaussian, λ = m is the number of Gaussian terms in the mixtures. Formally, this means that one has, for every λ, an algorithm A λ : T → fT ,λ . Once this λ * is obtained, the final estimator is fT ,λ * (T ) , obtained by rerunning the algorithm one more time on the full training set. This defines a new training algorithm, A * : T → fT ,λ * (T ) . It is a common mistake to consider that the cross-validation error associated to this algorithm is still given by e(λ * (T )). This is false, because the computation of λ * uses the full training set. To compute the cross-validation error of A * , one needs to encapsulate this model selection procedure in an other cross-validation loop. So, one needs to compute, using the previous notation,

$E * CV (T ) = 1 n n i=1 RT i ( fT (i) ,λ * (T (i) ) )$where each fT (i) ,λ * (T (i) ) is computed by running a cross-validated model selection procedure restricted to T (i) . This is often called a double-loop cross-validation procedure (the number of folds in the inner and outer loops do not have to coincide). Note that each λ * (T (i) ) that does not necessarily coincide with the optimal λ * (T ) obtained with the full training set.

## Chapter 6

Inner Products and Reproducing Kernels

## Introduction

We will discuss later in this book various methods that specify the prediction is as a linear function of the input. These methods are often applied after taking transformations of the original variables, in the form x → h(x) (i.e., the prediction algorithm is applied to h(x) instead of x). We will refer to h as a "feature function," which typically maps the initial data x ∈ R to a vector space, sometimes of infinite dimensions, that we will denote H (the "feature space").

The present chapter provides a formal description of this framework, focusing, in particular, on situations in which H has an inner product, as this inner product is often instrumental in the design of linear methods on H. Many machine learning methods can indeed be expressed either as functions of the coordinates of the input data in some space, or as functions of the inner products between the input samples. Such methods can bypass the difficulty of using high-dimensional features with the help of the theory of "reproducing kernels," [[12,](#b30)[201]](#b219) which ensures that the inner product between special classes of feature functions h(x) and h(x ′ ) can be explicitly computed as a function of x and x ′ .

## Basic Definitions

## Inner-product spaces

We recall that a real vector space[foot_4](#foot_4) is a set, H, on which an addition and a scalar product are defined, namely (h,

$h ′ ) ∈ H × H → h + h ′ ∈ H and (λ, h) ∈ R × H → λh ∈$H, and we assume that the reader is familiar with the theory of finite-dimensional CHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS spaces.

An inner product on a vector space H is a bilinear function, typically denoted (ξ, η) → ⟨ξ , η⟩ such that ⟨ξ , ξ⟩ ≥ 0 with ⟨ξ , ξ⟩ = 0 if and only if ξ = 0. A vector space equipped with an inner product is called an inner-product space. We will often denote the inner product with a subscript referring to the space (e.g., ⟨• , •⟩ H ). Given such a product, the function ξ → ∥ξ∥ H = ⟨ξ , ξ⟩ H is a norm, so that H is also a normed space (but not all normed spaces are innerproduct spaces) [2](#foot_5) .

When a normed space is complete with respect to the topology induced by its norm, it is called a Banach space, or a Hilbert space when the norm is associated with an inner product. Completeness means that Cauchy sequences in this space always have a limit, i.e., if the sequence (ξ n ) is such that, for any ϵ > 0, there exists n 0 > 0 such that ∥ξ nξ m ∥ H < ϵ for all n, m ≥ n 0 , then there exists ξ such that ∥ξ n -ξ∥ H → 0. Completeness is a very natural property. It allows, for example, for the definition of integrals such as h(t)dt as limits of Riemann sums for suitable functions h : R → H, leading (with more general notions of integrals) to proper definitions of expectations of H-valued random variables. Using a standard (abstract) construction, one can prove that any normed space (resp. inner-product) can be extended to a Banach (resp. Hilbert) space within which it is dense.

Note that finite-dimensional normed spaces are always complete.

## Feature spaces and kernels

Now, consider an input set, say R, and a mapping h from R to H, where H is an inner product space. For us, R is the set over which the original input data is observed, typically R d , and H is the feature space. One can define the function

$K h : R × R → R by K h (x, y) = ⟨h(x) , h(y)⟩ H .$The function K h satisfies the following two properties.

[K1] K h is symmetric, namely K h (x, y) = K h (y, x) for all x and y in R.

[K2] For any n > 0, for any choice of scalars λ 1 , . . . , λ n ∈ R and any x 1 , . . . , x n ∈ R, one

$has n i,j=1 λ i λ j K h (x i , x j ) ≥ 0. (6.1)$The first property is obvious, and the second one results from the fact that one can write

$n i,j=1 λ i λ j K h (x i , x j ) = n i,j=1 λ i λ j ⟨h(x i ) , h(x j )⟩ H = n i=1 λ i h(x i ) 2 H ≥ 0. (6.2)$This leads us to the following definition. One says that the kernel is positive definite if the sum in (6.1) cannot vanish unless (i)

$λ 1 = • • • = λ n = 0 or (ii) x i = x j for some i j.$An equivalent definition of positive kernels can be given using kernel matrices, for which we introduce a notation. Definition 6.2 If K : R × R → R is given, we define, for every x 1 , . . . , x n ∈ R, the kernel matrix K K (x 1 , . . . , x n ) with entries K(x i , x j ), for i, j = 1, . . . , n. (If K is understood from the context, we will simply write K(x 1 , . . . , x n ) instead of K K (x 1 , . . . , x n ).)

Given this notation, it is clear that K is a positive kernel if and only if for all x 1 , . . . , x n ∈ R, the matrix K K (x 1 , . . . , x n ) is symmetric, positive semidefinite. It is a positive definite kernel if K K (x 1 , . . . , x n ) is positive definite as soon as all x j 's are distinct. This latter condition is obviously needed since, if x i = x j , the ith and jth columns of K coincide and this matrix cannot be full-rank. Remark 6.3 It is important to point out that K being a positive kernel does not require that K(x, y) ≥ 0 for all x, y ∈ R (see examples in the next section). However, it does imply that K(x, x) ≥ 0 for all x ∈ R, since diagonal elements of positive semi-definite matrices are non-negative. ♦

The function K h defined above is therefore always a positive kernel, but not always positive definite, as seen below. We will also see later that the converse statement is true: any positive kernel K : R × R → R can be expressed as K h for some feature function h between R and some feature space H.

Given a feature function h : R → H, we will denote by V h = span(h(x), x ∈ R) the vector space generated by the features, which, by definition, is the space of all linear combinations

$ξ = n i=1 λ i h(x i )$with λ 1 , . . . , λ m ∈ R, x 1 , . . . , x n ∈ R and n ≥ 0 (by convention, ξ = 0 if n = 0). Then K h is positive definite if and only if any family (h(x 1 ), . . . , h(x n )) with distinct x i 's is linearly independent. This is a direct consequence of (6.2).

$n i,j=1 λ i λ j K h (x i , x j ) = n i=1 λ i h(x i ) 2 H .$This implies in particular that positive-definite kernels over infinite input spaces R can only be associated to infinite-dimensional spaces H, since V h ⊂ H.

## First examples 6.3.1 Inner product

Clearly, if R is an inner product space, it has an associated reproducing kernel, defined by

$K(x, y) = ⟨x , y⟩ R .$This kernel is equal to K h with H = R and h = id (the identity mapping). In particular K(x, y) = x T y is a positive kernel if R = R d . This kernel can obviously take positive and negative values.

Notice that this kernel is not positive definite, because the rank of K(x 1 , . . . , x n ) is equal to the dimension of span(x 1 , . . . , x n ), which can be less than n even when the x i 's are distinct.

## Polynomial Kernels

Consider R = R d and define

$h(x) = (x (i 1 ) . . . x (i k ) , 1 ≤ i 1 , . . . , i k ≤ d),$which contains all products of degree k formed from variables x (1) , . . . , x (d) , i.e., all monomials of degree k in x. This function takes its values in the space H = R N k , where N k = d k . Using, in H, the inner product ⟨ξ , η⟩ H = ξ T η, we have

$K h (x, y) = 1≤i 1 ,...,i k ≤d (x (i 1 ) y (i 1 ) ) • • • (x (i k ) y (i k ) ) = (x T y) k .$This provides the homogeneous polynomial kernel of order k.

If one now takes all monomials of order less than or equal to k, i.e., h(x) = (x (i 1 ) . . .

$x (i l ) , 1 ≤ i 1 , . . . , i l ≤ d, 0 ≤ l ≤ k), which now takes values in a space of dimension 1 + d + • • • + d k , the corresponding kernel is K h (x, y) = 1 + (x T y) + • • • + (x T y) k = (x T y) k+1 -1 x T y -1 .$This provides a polynomial kernel of order k. It is important to notice here that, even though the dimension of the feature space increases exponentially in k, so that the computation of the feature function rapidly becomes intractable, the computation of the kernel itself remains a relatively mild operation.

One can make variations on this construction. For example, choosing any family c 0 , c 1 , . . . , c k of positive numbers, one can take

$h(x) = (c l x (i 1 ) . . . x (i l ) , 1 ≤ i 1 , . . . , i l ≤ d, 0 ≤ l ≤ k) yielding K h (x, y) = c 2 0 + c 2 1 (x T y) + • • • + c 2 k (x T y) k . Taking c l = k l 1/2$α l for some α > 0, we get another form of polynomial kernel, namely, K h (x, y) = (1 + α 2 x T y) k .

## Functional Features

We now consider an example in which H is infinite dimensional. Let R = R d . We assume that a function s : R d → R is chosen, such that s is both (absolutely) integrable and square integrable. We also fix a scaling parameter ρ > 0. Associate to x ∈ R d the function

$ξ x : y → s((y -x)/ρ),$which is also square integrable (as a function of y). We define the feature function

$h : x → ξ x from R d to H = L 2 (R d$), the space of square integrable functions on R d with inner product

$⟨ξ , η⟩ H = R d ξ(z)η(z)dz.$The resulting kernel is

$K h (x, y) = R d s(z/ρ -x)s(z/ρ -y) dz = ρ d R d s(z)s(z -(y -x)/ρ) dz.$Note that K h (x, y) is "translation-invariant," which means that it only depends on xy. It takes the form K h (x, y) = ρ d Γ ((yx)/ρ) where

$Γ (u) = R d s(z)s(z -u) dz.$is the convolution[foot_6](#foot_6) of s with s : z → s(-z).

Let σ be the Fourier transform of s, i.e.,

$σ (ω) = R d e -2iπω T u s(u)du.$Because s is real-valued, we have σ (-ω) = σ (ω), the complex conjugate of σ . Moreover, σ is also the Fourier transform of s. Using the fact that the Fourier transform of the convolution of two functions is the product of their Fourier transforms, we see that the Fourier transform of Γ = s * s is equal to |σ | 2 . Applying the inverse transform, we find

$Γ (u) = R d e 2iπω T u |σ (ω)| 2 dω = R d e -2iπω T u |σ (-ω)| 2 dω .$This form is (almost) characteristic of translation-invariant kernels.

Let us consider a few examples of kernels that can be obtained in this way.

(1) Take d = 1 and let s be the indicator function of the interval [-1 2 , 1  2 ]. Then, one finds Γ (t) = max(1 -|t|, 0) .

In this case, the space V h is the space of all functions expressed as finite sums

$z → n j=1 λ j 1 [x j -ρ/2,x j +ρ/2] (z) ,$and therefore is a space of compactly-supported piecewise constant functions. Such a function computed with distinct x j 's cannot vanish everywhere unless all λ j 's vanish, so that K h is positive definite. Indeed, let

$f (z) = n j=1 λ j 1 [x j -ρ/2,x j +ρ/2] (z)$and assume without loss of generality that x 1 < x 2 < • • • < x n and let x n+1 = ∞. Let i 0 be the smallest index j such that λ j 0, assuming that such an index exists. Then

$f (z) = λ i 0 > 0 for all z ∈ [x i 0 -ρ/2, x i 0 +1 -ρ/2$) which is a non-empty interval. So, if f vanishes almost everywhere, we must have λ j = 0 for all j = 1, . . . , n.

(2) Still with d = 1, let s(z) = e -|z| . Then, for t > 0,

$Γ (t) = ∞ -∞ e -|z| e -|z-t| dz = 0 -∞$e z e z-t dz + t 0 e -z e z-t dz

$+ ∞ t e -z e -z+t dz = e -t 2 + te -t + e -t 2 = (1 + t)e -t$Using the fact that Γ (-t) = Γ (t) (make the change of variable z → -z in the integral), we get

$Γ (t) = (1 + |t|)e -|t| .$for all t. This shows that

$K(x, y) = (1 + |x -y|)e -|x-y|$is a positive kernel on R d .

(

$) Take s(z) = e -|z| 2 /2 , z ∈ R d . Then Γ (u) = R d e -|z| 2 +|u-z| 2 2 dz = e -|u| 2 4 R d e -|z-u/2| 2 dz = (4π) d/2 e -|u| 2 4 .3$This provides a special case of Gaussian kernel.

## General construction theorems Translation invariance

As introduced above, a kernel K is translation invariant if it takes the form K(x, y) = Γ (x -y) for some continuous function Γ defined on R d . Bochner's theorem [[33]](#b51) states that such a K is a positive kernel if and only if Γ is the Fourier transform of a positive measure, namely,

$Γ (x) = R d e -2iπ⟨x,ω⟩ dµ(ω)$where µ is a positive and symmetric (invariant by sign change) measure on R d . For example one can take dµ(ω) = ν(ω)dω, where ν is a integrable, positive and even function.

This theorem provides an at least numerical, and sometimes analytical, method for constructing kernels. The previous section exhibited a special case of translationinvariant kernel for which ν = |σ | 2 .

## Radial kernels

A radial kernel takes the form K(x, y) = γ(|x -y| 2 ), for some continuous function γ defined on [0, +∞). Shoenberg's theorem [[173]](#b191) states that, if this function γ is universally valid, i.e., K is a kernel for all dimensions d, then, it must take the form

$γ(t) = ∞ 0 e -λt dµ(λ)$for some positive finite measure µ on [0, +∞).

For example, when µ is a Dirac measure, i.e., µ = δ (2a) -1 for some a > 0, then K(x, y) = exp(-|x -y| 2 /2a), which is the Gaussian kernel. Taking dµ = e -aλ dλ yields γ(t) = 1/(t + a), and dµ = λe -aλ dλ yields γ(t) = 1/(a + t) 2 .

There is also, in Schoenberg [[173]](#b191), a characterization of radial kernels for a fixed dimension d. Such kernels must take the form

$γ(t) = +∞ 0 Ω d (tλ)dµ(λ) with Ω d (t) = Γ (d/2)(2/t) (d-2)/2 J (d-2)/2 (t)$where J (d-2)/2 is Bessel's function of the first kind.

## Operations on kernels

Kernels can be combined in several ways as described in the next proposition. Proposition 6.4 Let K 1 : R × R → R and K 2 : R × R → R be positive kernels. Then the following assertions hold.

$(i) If λ 1 , λ 2 > 0, λ 1 K 1 + λ 2 K 2 is a positive kernel. It is positive definite as soon as either K 1 or K 2 is positive definite. (ii) For any function f : R ′ → R, K ′ 1 (x ′ , y ′ ) ∆ = K 1 (f (x ′ ), f (y ′ )$) is a positive kernel. It is positive definite as soon as K 1 is positive definite and f is one-to-one.

(iii) K(x, y) = K 1 (x, y)K 2 (x, y) is a positive kernel. It is positive definite as soon as K 1 and K 2 are positive definite.

(iv) Let K 1 and K 2 be translation-invariant with R = R d , taking the form

$K i (x, y) = Γ i (x -y), where Γ i is continuous ( i = 1, 2). Assume that one of the two functions Γ 1 , Γ 2 is integrable on R d . Then K(x, y) = R d K 1 (x, z)K 2 (z,$y)dz is also a positive kernel. Proof Point (i) is obvious. Point (ii) is almost as simple, because, for any λ 1 , . . . , λ n ∈ R and x ′ 1 , . . . , x ′ n ∈ R ′ , n i,j=1

$λ i λ j K ′ 1 (x ′ i , x ′ j ) = n i,j=1 λ i λ j K 1 (f (x ′ i ), f (x ′ j )) ≥ 0.$If K 1 is positive definite, then the latter sum can only vanish if all λ i are zero, or some of the points in (f (x ′ 1 ), . . . , f (x ′ n )) coincide. If, in addition, f is one-to-one, then this is equivalent to all λ i are zero, or some of the points in (x ′ 1 , . . . , x ′ n ) coincide, so that K ′ 1 is positive definite.

To prove point (iii), take x 1 , . . . , x N ∈ R d and form the matrices

$K i = K i (x 1 , . . . , x N ), i = 1, 2$, which are, by assumption positive semi-definite. The matrix K = K(x 1 , . . . , x N ) is the element-wise (or Hadamard) product of K 1 and K 2 , and the conclusion follows from the linear algebra result stating that the Hadamard product of two positive semi-definite (resp. positive definite) matrices A = (a(i, j), 1 ≤ i, j ≤ N ) and B = (b(i, j), 1 ≤ i, j ≤ N ) is positive semi-definite (resp. positive definite). This is proved by diagonalizing, say, A in an orthonormal basis u 1 , . . . , u N , with eigenvalues λ 1 , . . . , λ N and writing

$N i,j=1 α (i) a(i, j)b(i, j)α (j) = N i,j,k=1 α (i) u (k) i u (k) j λ k b(i, j)α (j) = N k=1 λ k N i,j=1 (α (i) u (k) i )(α (j) u (k) j ))b(i, j) ≥ 0$If B is positive definite, then the sum above can be zero only if, for each k, either λ k = 0 or α (i) u (k) i = 0 for all i. If A is also positive definite, then the only possibility is α (i) u (k) i = 0 for all i and k, which implies α (i) = 0 for all i since u i 0.

To prove point (iv) [4](#foot_7) , we first note that a translation invariant kernel K ′ (x, y) = Γ ′ (xy) is always bounded. Indeed, the matrix

$K ′ (x, 0) is positive semi-definite, with determinant Γ ′ (0) 2 -Γ ′ (x) 2 > 0, showing that |Γ ′ (x)| < Γ ′ (0)$. This shows that the integral defining K(x, y) converges as soon as one of the two functions Γ 1 or Γ 2 is integrable. Moreover, we have K(x, y) = Γ (xy) with

$Γ (x) = R d Γ 1 (x -z)Γ 2 (z) dz = R d Γ 1 (x -u)Γ 2 (u -y) du$Using the fact that both Γ 1 and Γ 2 are even, and making the change of variable z → -z, one easily shows that Γ (x) = Γ (-x), which implies that K is symmetric.

We proceed with the assumption that Γ 2 is integrable and use Bochner's theorem to write

$Γ 1 (y) = R d e -iξ T y dµ 1 (ξ)$for some positive finite measure µ 1 . Then

$Γ (x) = R d R d e -2iπξ T (x-z) dµ 1 (ξ) Γ 2 (z) dz = R d e -2iπξ T x R d e 2iπξ T z Γ 2 (z) dz dµ 1 (ξ)$The shift in the order of the variables ξ and z uses Fubini's theorem. The function

$ψ(ξ) = R d e 2iπξ T z Γ 2 (z) dz$is the inverse Fourier transform of Γ 2 . Because Γ 2 is bounded and integrable, it is also square integrable, which implies that its inverse Fourier transform is also a square integrable function. Since Bochner's theorem implies that Γ 2 is the Fourier transform of a positive measure µ 2 , we find, using the injectivity of the Fourier transform, that ψ is non-negative. So Γ is the Fourier transform of the finite positive measure ψdµ 1 , which implies that K is a positive kernel.

■ Point (iv) can be related to the following discrete statement on symmetric matrices: assume that A and B are positive semi-definite and that they commute, so that AB = BA: then AB is positive semi-definite (see ??). In the case of kernels, one may consider the symmetric linear operators K i : f → R d K i (•, y)f (y)dy which maps the space of square integrable functions into itself. Then K 1 and K 2 commute and K = K 1 K 2 .

## Canonical Feature Spaces

Let K be a positive kernel on a set R. The following construction, which is fundamental, shows that K can always be associated with a feature function h taking values in a suitably chosen inner-product space H.

Associate to each x ∈ R the function ξ x : y → K(y, x) (we will also write ξ x = K(•, x)), and let H K = span(ξ x , x ∈ R), a subspace of the vector space of all functions from R to R. Define the feature function h : x → ξ x from R to H K . There is a unique inner product on H K such that K = K h . Indeed, by definition, this requires

$⟨K(•, x) , K(•, y)⟩ H K = K(x,$y) . (6.3) Moreover, by linearity, for any ξ = n i=1 λ i K(•, x i ) and η = m i=1 µ i K(•, y i ), one needs ⟨ξ , η⟩ H K = n i=1 m j=1

λ i µ j K(x i , y j ) , so that the inner product is uniquely specified on H K . To make sure that this innerproduct is well defined, we must check that there is no ambiguity, in the sense that, if ξ has an alternative decomposition ξ = n ′ i=1 λ ′ i K(•, x ′ i ), then, the value of ⟨ξ , η⟩ H K remains unchanged. But this is clear, because one can also write ⟨ξ , η⟩ H K = m j=1 µ j ξ(y j ) , which only depends on ξ and not on its decomposition. The linearity of the product with respect to ξ is also clear from this expression, and the bilinearity by symmetry.

The Schwartz inequality implies that

$|⟨ξ , η⟩ H K | ≤ ∥ξ∥ H K ∥η∥ H K$From which we deduce that ∥ξ∥ H K = 0 implies that ⟨ξ , η⟩ H K = 0 for η ∈ H K . Since ⟨ξ , K(•, y)⟩ H K = ξ(y) for all y, this also implies that ξ = 0, completing the proof that H K is an inner-product space. Equation (6.3) is the "reproducing property" of the kernel for the inner-product on H K . In functional analysis, the completion, ĤK , of H K for the topology associated to its norm is then a Hilbert space, and is referred to as a "reproducing kernel Hilbert space," or RKHS.

More generally, an inner-product space H of functions h : R → R is a reproducing kernel Hilbert space if H is a complete space (which makes it Hilbert) and there exists a positive kernel K such that,

$[RKHS1] For all x ∈ R, K(•, x) belongs to H, [RKHS2] For all h ∈ H and x ∈ R, ⟨h , K(•, x)⟩ H = h(x) .$Returning to the example of functional features in section 6.3.3, we have two different representations of the kernel in feature space, namely in H = L 2 (R d ), or in H K , with a different inner product. There is not a contradiction, and simply shows that the representation of a positive kernel in terms of a feature function is not unique.

## Projection on a finite-dimensional subspace

If H is an inner-product space and V is a subspace of H, one defines the orthogonal projection of an element ξ ∈ H on V as its closest point in V , that is, the element η * of V minimizing the function F : η → ∥η -ξ∥ 2  H over all η ∈ V . This closest point does not always exist, but it does in the special case in which V is finite dimensional (or, more generally, when V is a closed subspace of H; see Yosida [[205]](#b223)). We state, without proof, some of the properties of this operation.

Assuming that V is closed, this minimizer is unique and will be denoted η * = π V (ξ). Moreover, π V is a linear transformation from H to V , and η * is characterized by the properties

$η * ∈ V ξ -η * ⊥ V ,$the last condition meaning that ⟨ξη * , η⟩ H = 0 for all η ∈ V .

Because

$∥ξ∥ 2 H = ∥π V (ξ)∥ 2 H + ∥ξ -π V (ξ)∥ 2 H , one always has ∥π V (ξ)∥ H ≤ ∥ξ∥ H , with inequality if and only if π V (ξ) = ξ, i.e., if and only if ξ ∈ V . If V is finite-dimensional and η 1 , . . . , η n is a basis of V , then π V (ξ) is given by π V (ξ) = n i=1 α (i) η i$with α (considered as a column vector in R n ) given by α = Gram(η 1 , . . . , η n ) -1 λ , where λ ∈ R n is the vector with coordinates λ (i) = ⟨ξ , η i ⟩ H , i = 1, . . . , n. The Gram matrix of η 1 , . . . , η n , denoted Gram(η 1 , . . . , η n ), is the n by n matrix with entries ⟨η i , η j ⟩ H for i, j = 1, . . . , n.

If A is a subset of H, the set A ⊥ consists of all vectors perpendicular to A, namely

$A ⊥ = h ∈ H : ⟨h , h⟩ H = 0 for all h ∈ A .$If V is a finite-dimensional (or, more generally, closed) subspace of H, then any point in h is decomposed as

$h = π V (h) + h -π V (h) with h -π V (h) ∈ V ⊥ . This shows that π V ⊥ is well defined and equal to id H -π V .$Orthogonal projections can be applied to function interpolation in an RKHS. Indeed, assuming that H is an RKHS, as described at the end of the previous section, with a positive-definite kernel. Given distinct points x 1 , . . . , x N ∈ R and values α 1 , . . . , α N ∈ R, the interpolation problem consists in finding h ∈ H with minimal norm satisfying h(x k ) = α k , k = 1, . . . , N . Consider the finite dimensional space

$V = span {K(•, x k ), k = 1, . . . N } .$Then there exists an element h 0 ∈ V that satisfies the constraints. Indeed, looking for h 0 in the form

$h 0 (x) = N l=1 K(x, x l )λ l one has h 0 (x k ) = N l=1 K(x k , x l )λ l so that           λ 1 . . . λ N           = K(x 1 , . . . , x N ) -1           α 1 . . . α N          $Any other function h satisfying the constraints satisfies h(x k )h 0 (x k ) = 0, which, using RKHS2, is equivalent to ⟨hh 0 , K(•, x k )⟩ H = 0, i.e., to hh 0 ∈ V ⊥ . This shows that h 0 = π V (h), so that ∥h∥ H ≥ ∥h 0 ∥ H and h 0 provides the optimal interpolation. We summarize this in the proposition: Proposition 6.5 Let H is an RKHS with a positive-definite kernel. Let x 1 , . . . , x N ∈ R be distinct points and α 1 , . . . , α N ∈ R. Then the function h ∈ H with minimal norm satisfying

$h(x k ) = α k , k = 1, . . . , N takes the form h(x k ) = N l=1 K(x k , x l )λ l (6.4a) with           λ 1 . . . λ N           = K(x 1 , . . . , x N ) -1           α 1 . . . α N           . (6$
## .4b)

A variation of this problem replaces the constraint by a penalty that complete the minimization associated with the orthogonal projection, namely, minimizing (in h ∈ H)

$∥h∥ 2 H + σ 2 N k=1 |h(x k ) -α k | 2 .$Letting h 0 = π V (h), so that h 0 (x k ) = h(x k ) for all k, this expression can be rewritten as

$∥h 0 ∥ 2 H + ∥h -h 0 ∥ 2 H + σ 2 N k=1 |h 0 (x k ) -α k | 2 .$This shows that the optimal h must coincide with its projection on V , and therefore belong to that subspace. Looking for h in the form

$h(•) = N l=1 K(•, x l )λ l ,$the objective function is rewritten as

$N k,l=1 K(x k , x l )λ k λ l + σ 2 N k=1 N l=1 K(x k , x l )λ l -α k 2 ,$which, in vector notation gives, writing λ =

$          λ 1 . . . λ N           and α =           α 1 . . . α N           , λ T K(x 1 , . . . , x N )λ + σ 2 (K(x 1 , . . . , x N )λ -α) T (K(x 1 , . . . , x N )λ -α) .$The differential of this expression in λ is K(x 1 , . . . , x N )λ + 2σ 2 K(x 1 , . . . , x N )(K(x 1 , . . . , x N )λα).

Assuming that x 1 , . . . , x N are distinct, this vanishes if and only if

$λ = (K(x 1 , . . . , x N ) + (1/σ 2 )Id R N ) -1 α.$We have just proved the proposition: Proposition 6.6 Let H is an RKHS with a positive-definite kernel. Let x 1 , . . . , x N ∈ R be distinct points and α 1 , . . . , α N ∈ R. Then the unique minimizer of

$h → ∥h∥ 2 H + σ 2 N k=1 |h(x k ) -α k | 2$on H is given by

$h(x k ) = N l=1 K(x k , x l )λ l (6.5a) with           λ 1 . . . λ N           = (K(x 1 , . . . , x N ) + (1/σ 2 )Id R N ) -1           α 1 . . . α N           . (6.5b)$Chapter 7

## Linear Models for Regression

In regression, linear models refer to situations in which one tries to predict the de-

$pendent variable Y ∈ R Y = R q by a function f (X) of the dependent variable X ∈ R X ,$where f is optimized over a linear space F . The most common situation is the "standard linear model," for which R X = R d and

$F = {f (x) = a 0 + b T x : a 0 ∈ R q , b ∈ M d,q (R)}.(7.1)$More generally, with q = 1, given a mapping h : R → H, where H is an innerproduct space, one can take:

$F = {f (x) = a 0 + ⟨b , h(x)⟩ H : a 0 ∈ R, b ∈ H}. (7.2)$Note that h can be nonlinear, and F can be infinite dimensional. Such sets corresponds to linear models using feature functions, and will be addressed using kernel methods in this chapter.

Note also that, even if the model is linear, the associated training algorithms can be nonlinear, and we will review in fact several situations in which solving the estimation problem requires nonlinear optimization methods.

## Least-Square Regression

## Notation and Basic Estimator

We denote by Y and X the dependent and independent variables of the regression problem. We will assume that Y takes values in R q and that X takes values in a set R X , which will, by default, be equal to R d , except when discussing kernel methods, for which this set can be arbitrary (provided that there is a mapping h from R X to an inner product space H with an easily computable kernel).

Least-square regression uses the risk function r(y, y ′ ) = |yy ′ | 2 . The prediction error is then R(f ) = E(|Yf (X)| 2 ) for any predictor f and the Bayes predictor is the conditional expectation x → E(Y | X = x) (see item Example 1. in section 5.3). We also start with the standard setting where R X = R d and F given by (7.1).

We will use the following notation, which sometimes simplifies the computation.

$If x ∈ R d , we let x = 1 x$, which belongs to R d+1 . The linear predictor f (x) = a 0 + b T x with a 0 ∈ R q , b ∈ M d,q (R) can then be written as

$f (x) = β T x with β = a T 0 b ∈ M d+1,q (R).$In a model-based approach, the linear model is a Bayes predictor under the generative assumption that Y = a 0 + b T X + ϵ where ϵ is a residual noise satisfying E(ϵ | X) = 0, which is true, for example, when ϵ is centered and independent of X. If one further specifies the model so that ϵ is Gaussian, centered and independent of X, and one assumes that the distribution of X does not depend on a 0 and b, then the maximum likelihood estimator of these parameters based on a training set T = ((x 1 , y 1 ), . . . , (x N , y N )) must minimize the "residual sum of squares:"

$RSS(β) ∆ = N R(f ) ∆ = N k=1 |y k -f (x k )| 2 = N k=1 |y k -β T xk | 2 .$In other terms, the model-based approach is identical, under these (standard) assumptions, to empirical risk minimization (section 5.5), on which we now focus. (Recall that, even when using a model-based approach, one does not make assumptions on the true distribution of X and Y ; one rather treats the model as an approximation of these distributions, estimated by maximum likelihood, and uses the Bayes predictor for the estimated model.)

The computation of the optimal regression parameters is made easier by the introduction of the following matrices. Introduce the N × (d + 1) matrix X with rows xT 1 , . . . , xT N and the N × q matrix Y with rows y T 1 , . . . , y T N , that is:

$X =             1 x (1) 1 • • • x (d) 1 . . . . . . . . . 1 x (1) N • • • x (d) N             , Y =             y (1) 1 • • • y (q) 1 . . . . . . y (1) N • • • y (q) N             .$With this notation, we have RSS(β) = |Y -X β| 2 2 . with |A| 2 2 = trace(A T A) for a rectangular matrix A. The solution of the problem is then provided by the following theorem. Theorem 7.1 Assume that the matrix X has rank d + 1. Then the RSS is minimized for β = (X T X ) -1 X T Y Proof We provide two possible proofs of this elementary problem. The first one is an optimization argument noting that F(β) ∆ = RSS(β) is a convex function defined on M d+1,q (R) and with values in R. Since F is quadratic, we have, for any matrix

$h ∈ M d+1,q (R), dF(β)h = ∂ ϵ F(β + ϵh)| ϵ=0 = -2trace(h T X T (Y -X β)) and dF(β) = 0 ⇔ X T (Y -X β) = 0 ⇔ β = β.$One can alternatively proceed with a direct computation. We have

$RSS(β) = |Y | 2 2 -2trace(β T X T Y ) + trace(β T X T X β) = |Y | 2 2 -2trace(β T X T X β) + trace(β T X T X β).$Replacing β by β and simplifying yields

$RSS( β) = |Y | 2 2 -trace( βT X T X β) It follows that RSS(β) =RSS( β) + trace( βT X T X β) -2trace(β T X T X β) + trace(β T X T X β) =RSS( β) + |X ( β -β)| 2 2$so that the left-hand side is minimized at β = β. ■ Remark 7.2 If X does not have rank d + 1, then optimal solutions exist, but they are not unique. By convexity, the solutions are exactly the vectors β at which the gradient vanishes, i.e., those that satisfy X T X β = X T Y . The set of solutions can be obtained by introducing the SVD of X in the form X = U DV T and letting γ = V T β and

$Z = U T Y . Then X T X β = X T Y ⇔ D T Dγ = D T Z.$Letting d (1) , . . . , d (m) denote the nonzero diagonal entries of D (so that m ≤ d + 1), we find γ (i) = z (i) /d (i) for i ≤ m (the other equalities being 0 = 0). So, the d + 1m last entries of γ can be chosen arbitrarily (and β = V γ). ♦

An alternate representation of the solution use a two-step computation that estimates b first, then a 0 . Indeed, for fixed b, the minimum of x k .

$N k=1 |y k -a 0 -x T k b| 2$This shows that b itself must be a minimizer of

$N k=1 |y k -ȳ -(x k -x) T b| 2 .$Denote by Y c and X c the matrices

$X c =             x (1) 1 -x (1) • • • x (d) 1 -x (d) . . . . . . x (1) N -x (1) • • • x (d) N -x (d)             , Y c =             y (1) 1 -y (1) • • • y (q) 1 -y (q) . . . . . . y (1) N -y (1) • • • y (q) N -y (q)             . Then b must minimize |Y c -X c b| 2 , yielding b = (X T c X c ) -1 X T c Y c , â0 = ȳ -xT b.$The reader may want to double-check that this solution coincides with the one provided in theorem 7.1.

## Limit behavior

The matrix

$ΣXX = 1 N X T c X c = 1 N N k=1 (x k -x)(x k -x) T$is a sample estimate of the covariance matrix of X, that we will denote Σ XX . Similarly, ΣXY = X T c Y c /N is a sample estimate of Σ XY , the covariance between X and Y . With this notation, we have b = Σ-1 XX ΣXY , which, by the law of large numbers, converges to b

$* = Σ -1 XX Σ XY . Let a * 0 = m Y -m T X b * . Then f * (x) = a * 0 +(b * ) T$x is the least-square optimal approximation of Y by a linear function of X, and the linear predictor f (x) = â0 + bT x converges a.s. to f * (x). Of course, f * generally differs from f : x → E(Y | X = x), which is the least-square optimal approximation of Y by any (square-integrable) function of X, so that the linear estimator will have a residual bias.

## Gauss-Markov theorem

If one makes the (unlikely) assumption that the linear model is exact, i.e., f (x) = f * (x), one has:

$E( β) = E(E( β | X )) = E((X T X ) -1 X T E(Y | X )) = E((X T X ) -1 X T X β) = β$and the estimator is "unbiased." Under this parametric assumption, many other properties of linear estimators can be proved, among which the well-known Gauss-Markov theorem on the optimality of least-square estimation that we now state and prove. For this theorem, for which we take (for simplicity) q = 1, we also assume that var(Y | X = x), the variance of Y for its conditional distribution given X does not depend on x, and denote it by σ 2 . This typically correspond to the standard regression model in which one assumes that Y = f (X) + ϵ where ϵ is independent of X with variance σ 2 .

Recall that a symmetric matrix A is said to be larger than or equal to another symmetric matrix, B, writing A ⪰ B, if and only if A -B is positive semi-definite.

Theorem 7.3 (Gauss-Markov) Assume that an estimator β takes the form β = A(X )Y (it is linear) and is unbiased conditionally to X : E β ( β | X ) = β (for all β). Then (under the assumptions above) the covariance matrix of β cannot be smaller than that of the least square estimate, β.

Proof We write A = A(X ) for short. The condition that E(AY | X ) = β for all β yields AX β = β for all β, or AX = Id R d+1 (A is a (d + 1) × N matrix). Since β is unbiased, its covariance matrix is

## E(AY

$Y T A T ) -ββ T and E(AY Y T A T ) = E(E(AY Y T A T | X )) = σ 2 E(AA T ).$For β = β, for which A = (X T X ) -1 X T , we get E(AY Y T A T ) = σ 2 E((X T X ) -1 ). We therefore need to show that E(AA T ) ⪰ E(X T X ), i.e., that for any u ∈ R d+1 ,

$u T E(AA T )u ≥ u T E((X T X ) -1 )u$as soon as AX = Id R d+1 . We in fact have the stronger result (without expectations):

$AX = Id R d+1 ⇒ AA T ⪰ (X T X ) -1 .$To see this, fix u and consider the problem of minimizing F u (A) = A → u T AA T u subject to the linear constraint AX = Id R d+1 . The Lagrange multipliers for this affine constraint can be organized in a matrix C and the Lagrangian is

$u T AA T u + trace(C T (AX -Id R d+1 )).$Taking the derivative in A, we find that optimal solutions must satisfy 2u T AH T u + trace(C T HX ) = 0 for all H, which yields trace(H T (2uu T A + CX T )) = 0 for all H. This is only possible when 2uu T A + CX T = 0, which in turn implies that 2uu T AX = -CX T X . Using the constraint, we get

$C = -2uu T (X T X ) -1$so that uu T A = uu T (X T X ) -1 X T . This implies that A = (X T X ) -1 X T (the least-square estimator) is a minimizer of F u (A) for all u.

Any other solution that satisfies uu T A = uu T (X T X ) -1 X T for all u. Taking u = e i and summing over i (with d+1 i=1 e i e T i = Id R d+1 ) yields A = (X T X ) -1 X T . ■

## Kernel Version

We now assume that X takes its values in an arbitrary set R X , with a representation h : R X → H into an inner-product space. This representation does not need to be explicit or computable, but the associated kernel K(x, y) = ⟨h(x) , h(y)⟩ H is assumed to be known and easy to compute. (Recall that, from chapter 6, a positive kernel is always associated with an inner-product space.) In particular, any algorithm in this context should only rely on the kernel, and the function h only has a conceptual role.

Assume that q = 1 to lighten the notation, so that the dependent variable is scalarvalued. We here let the space of predictors be

$F = {f (x) = a 0 + ⟨b , h(x)⟩ H : a 0 ∈ R, b ∈ H}.$The residual sum of squares associated with this function space is

$RSS(a 0 , b) = N k=1 (y k -a 0 -⟨b , h(x k )⟩) 2 .$The following result (or results similar to it) is a key step in almost all kernel methods in machine learning. Proposition 7.4 Let V = span(h(x 1 ), . . . , h(x N )) be the finite-dimensional subspace of H generated by the feature functions evaluated on training input data. Then

$RSS(a 0 , b) = RSS(a 0 , π V (b)).$where π V is the orthogonal projection on V .

Proof The justification is immediate: since h(x k ) ∈ V , we have 

$⟨b , h(x k )⟩ H = ⟨π V (b) , h(x k )⟩ H for all b ∈ H.$$= N k=1 α k h(x k ) (7.3)$and the regression problem can be reformulated as a function of the coefficients α 1 , . . . , α N ∈ R, with

$f (x) = a 0 + N k=1 α k ⟨h(x) , h(x k )⟩ H = a 0 + N k=1 α k K(x, x k ),$which only depends on the kernel. (This reduction is often referred to as the "kernel trick.") However, the solution of the problem is, in this context, not very interesting. Indeed, assume that K is positive definite and that all observations in the training set are distinct. Then the matrix K(x 1 , . . . , x N ) formed by the kernel evaluations K(x i , x j ) is invertible, and one can solve exactly the equations

$y k = N j=1 α j K(x k , x j ), k = 1, . . . , N$to get a zero RSS with a 0 = 0. Unless there is no noise, such a solution will certainly overfit the data. If K is not positive definite, and the dimension of V is less than N (since this would place us in the previous situation otherwise), then it is more efficient to work directly in a basis of V rather than using the over-parametrized kernel representation. We will see however, starting with the next section, that kernel methods become highly relevant as soon as the regression is estimated with some control on the size of the regression coefficients, b.

## Ridge regression and Lasso

## Ridge Regression

Method. When the set F of possible predictors is too large, some additional complexity control is needed to reduce the estimation variance. One simple approach is to limit the number of parameters to be estimated, which, for regression, corresponds to limiting the number of possible predictors. This is related to the methods of Sieves mentioned in section 4.1. In contrast, ridge regression and lasso control the size of the parameters, as captured by their norm.

In both cases, one assigns a measure of complexity, denoted f → γ(f ) ≥ 0, to each element f ∈ F . Given γ, one can either optimize this predictor (using, for example, the RSS) with the constraint that γ(f ) ≤ C for some constant C, or add a penalty λγ(f ) to the objective function for some λ > 0. In general, the two approaches (constraint or penalty) are equivalent.

In linear spaces, complexity measures are often associated with a norm, and ridge regression uses the sum of squares of coefficients of the prediction matrix b, minimizing

$N k=1 |y k -a 0 -b T x k | 2 + λ trace(b T b) , (7.4)$which can be written in vector form as

$|Y -X β| 2 2 + λ trace(β T ∆β),$where ∆ = diag(0, 1, . . . , 1). In the following, we will work with an unspecified (d + 1) × (d + 1) symmetric positive semi-definite matrix ∆. Various choices are indeed possible, for example, ∆ = diag(0, σ 2 (1), . . . , σ 2 (d)), where σ 2 (i) is the empirical variance of the ith coordinate of X in the training set. This last choice is quite natural, because it ensures that, whenever one of the variable X (i) is rescaled by a factor c, the corresponding optimal i th row of b T is rescaled by 1/c, leaving the predictor unchanged.

Under this assumption, the optimal parameter is βλ = (X T X + λ∆) -1 X T Y , with a proof similar to that made for least-square regression. We obviously retrieve the original formula for regression when λ = 0.

Alternatively, assuming that ∆ = 0 0 0 ∆ ′ , so that no penalty is imposed on the intercept, we have bλ

$= (X T c X c + λ∆ ′ ) -1 X T c Y c (7.5)$and â0 λ = ȳ -( bλ ) T x. The proof of these statements is left to the reader.

Analysis in a special case To illustrate the impact of the penalty term on balancing bias and variance, we now make a computation in the special case when Y = Xβ + ϵ, where var(ϵ) = σ 2 and ϵ is independent of X. In the following computation, we assume that the training set is fixed (or rather, compute probabilities and expectations conditionally to it). Also, to simplify notation, we denote

$S λ = X T X + λ∆ = N k=1 xT k xk + λ∆$and Σ = E( XT X) for a single realization of X. Finally, we assume that q = 1, also to simplify the discussion.

The mean-square prediction error is

$R(λ) = E((Y -XT βλ ) 2 ) = E(( XT (β -βλ ) + ϵ) 2 ) = ( βλ -β) T Σ( βλ -β) + σ 2 .$Denote by ϵ k the (true) residual ϵ k = y k -xT k β on training data and by ϵ the vector stacking these residuals. We have, writing S 0 = S λ -λ∆,

$βλ = S -1 λ X T Y = S -1 λ S 0 β + S -1 λ X T ϵ = β -λS -1 λ ∆β + S -1 λ X T ϵ$So we can rewrite

$R(λ) = λ 2 β T ∆S -1 λ ΣS -1 λ ∆β -2λϵ T X S -1 λ ΣS -1 λ ∆β + ϵ T X S -1 λ ΣS -1 λ X T ϵ + σ 2 .$Let us analyze the quantities that depend on the training set in this expression. The first one is S λ = S 0 + λ∆. From the law of large numbers, S 0 /N → Σ when N tends to infinity, so that, assuming in addition that λ = λ N = O(N ), we have

$S -1 λ = O(1/N ). The second one is ϵ T X = N k=1 ϵ k xk$which, according to the central limit theorem, is such that

$N -1/2 ϵ T X ∼ N (0, σ 2 Var( X))$when N → ∞. So, we can expect the coefficient of λ 2 in R(λ) to have order N -2 , the coefficient of λ to have order N -3/2 and the constant coefficient of have order N -1 . This suggests taking λ = µ √ N so that all coefficients have roughly the same order when expanding in powers of µ. This gives S λ = N (S 0 /N + µ∆/ √ N ) ≃ N Σ and we make the approximation, letting ξ = N -1/2 σ -1/2 ϵX T and γ = Σ -1/2 ∆β, that

$N (R(λ) -σ 2 ) ≃ µ 2 |γ| 2 -2µξ T γ + ξ T ξ.$With this approximation, the optimal µ should be

$µ = ξ T γ |γ| 2 .$Of course, this µ cannot be computed from data, but we can see that, since ξ converges to a centered Gaussian random variable, its value cannot be too large. It is therefore natural to choose µ to be constant and use ridge regression in the form

$N k=1 (y k -xT k β) 2 + √ N µβ T ∆β.$In all cases, the mere fact that we find that the optimal µ is not 0 shows that, under the simplifying (and optimistic) assumptions that we made for this computation, allowing for a penalty term always reduces the prediction error. In other terms, introducing some estimation bias in order to reduce the variance is beneficial.

## Kernel Ridge Regression

We now return to the feature-space situation and take h : R X → H with associated kernel K. We still take q = 1 for simplicity. One formulates the ridge regression problem in this context as the minimization of

$N k=1 (y l -a 0 -⟨b , h(x l )⟩ H ) 2 + λ∥b∥ 2 H$with respect to β = (a 0 , b). Introducing the space V generated by the feature function evaluated on the training set, we know from proposition 7.4 that replacing b by π V (b) leaves the residual sum of squares invariant. Moreover, one has ∥π

$V (b)∥ 2 H ≤ ∥b∥ 2$H with equality if and only if b ∈ V . This shows that the solution b must belong to V and therefore take the form [(7.3)](#).

Using this expression, one finds that the problem is reduced to finding the minimum of

$N k=1        y k -a 0 - N l=1 K(x l , x k )α l        2 + λ N k,l=1 α k α l K(x k , x l )$with respect to a 0 , α 1 , . . . , α N . Recall that we have denoted by K = K(x 1 , . . . , x N ) the kernel matrix with entries K(x i , x j ), i, j = 1, . . . , N . We will assume in the following that K is invertible.

Introduce the vector 1 N ∈ R N with all coordinates equal to one. Let K = 1 N K and K ′ = 0 0 0 K .

Let α ∈ R N be the vector with coefficients α 1 , . . . , α N and α = a 0 α . With this notation, the function to minimize is

$F(α) = |Y -K α| 2 + λ αT K ′ α.$This takes the same form as standard ridge regression, replacing β by α, X by K and ∆ by K ′ . The solution therefore is

$αλ = ( KT K + λK ′ ) -1 KT Y .$Note that K being invertible implies that KT K + λK ′ is invertible. 1   To write the equivalent of (7.5), we need to use the equivalent of the matrix X c , that is, the matrix K with the average of the jth column subtracted to each (i, j) entry, given by:

$K c = K - 1 N 1 N 1 T N K.$Introduce the matrix P = Id -1 N 1 T N /N . It is easily checked that P 2 = P (P is a projection matrix). Since K c = P K, we have K T c K c = KP K. One deduces from this the expression of the optimal vector α λ , namely,

$α λ = (KP K + λK) -1 KP Y c = (P K + λId R N ) -1 Y c$where we have, in addition, used the fact that P Y c = Y c . Finally, the intercept is given by

$a 0 = y - 1 N (α λ ) T K1 N .$
## Equivalence of constrained and penalized formulations

Case of ridge regression. Returning to the basic case (without feature space), we now introduce an alternate formulation of ridge regression. Let ridge(λ) denote the ridge regression problem that we have considered so far, for some parameter 1 Indeed, let u = w 0 w with w 0 ∈ R and w ∈ R N be such that u T ( KT K + λK ′ )u = 0. This requires Ku = 0 and u T K ′ u = 0. The latter quantity is w T Kw, which shows that w = 0 since K has rank N . Then K = 1 N w 0 so that w 0 = 0 also.

λ. Consider now the following problem, which will be called ridge ′ (C): minimize N k=1 |y k -xT k β| 2 subject to the constraint β T ∆β ≤ C. We claim that this problem is equivalent to the ridge regression problem, in the following sense: for any C, there exists a λ such that the solution of ridge ′ (C) coincides with the solution of ridge(λ) and vice-versa. Indeed, fix a C > 0. Consider an optimal β for ridge ′ (C). Assuming as above that ∆ is symmetric positive semi-definite, we let V be its null space and P V the orthogonal projection on V . Write β = β 1 + β 2 with β 1 = P V β. Let d 1 and d 2 be the respective dimensions of V and V ⊥ so that d 1 + d 2 = d. Identifying R d with the product space V × V ⊥ (i.e., making a linear change of coordinates), the problem can be rewritten as the minimization of

$|Y -X 1 β 1 -X 2 β 2 | 2 subject to β T 2 ∆β 2 ≤ C, where X 1 (resp. X 2 ) is N × d 1 (resp. N × d 2 ).$The gradient of the constraint γ(

$β 2 ) = β T 2 ∆β 2 -C is ∇γ(β 2 ) = 2∆β 2 .$Assume first that ∆β 2 0. Then the solution must satisfy the KKT conditions, which require that there exists µ ≥ 0 such that β is a stationary point of the Lagrangian

$|Y -X 1 β 1 -X 2 β 2 | 2 + µβ T 2 ∆β 2 ,$with µ > 0 only possible if β T ∆β = C. This requires that

$X T 1 X 1 β 1 + X T 1 X 2 β 2 = X T Y , X T 2 X 1 β 1 + X T 2 X 2 β 2 + µ∆β 2 = X T Y .$Since ∆β 1 = 0, and using X = (X 1 , X 2 ), we have

$β = (X T X + µ∆) -1 X T Y ,$which is the only solution of ridge(µ).

If ∆β 2 = 0, then, necessarily, β 2 = 0. Since C > 0, β must then be the solution of the unconstrained problem, which is ridge(0). Conversely, any solution β of ridge(λ) satisfies the first-order optimality conditions for ridge ′ (C) for C = β T ∆β (or any C ≥ β T ∆β if λ = 0). This shows the equivalence of the two problems.

General case. We now consider this equivalence in a more general setting. Consider a penalized optimization problem, denoted var(λ) which consists in minimizing in β some objective function of the form U (β) + λϕ(β), λ ≥ 0. Consider also the family of problems var ′ (C), with C > inf(ϕ), which minimize U (β) subject to ϕ(β) ≤ C.

We make the following assumptions.

(i) U and ϕ are continuous functions from R n to R.

$(ii) ϕ(β) → ∞ when β → ∞.$(iii) For any λ ≥ 0, there is a unique solution of var(λ), denoted β λ .

(iv) For any C, there is a unique solution of var ′ (C). denoted β ′ C .

Assumptions (ii) and (iv) are true, in particular, when U is strictly convex, ϕ is convex and U has compact level sets. We show that, with these assumptions, the two families of problems are equivalent.

We first discuss the penalized problems and prove the following proposition, which has its own interest. Moreover, β λ varies continuously as a function of λ.

Proof Consider two parameters λ and λ ′ . We have

$U (β λ ) + λϕ(β λ ) ≤ U (β λ ′ ) + λϕ(β λ ′ ) and U (β λ ′ ) + λ ′ ϕ(β λ ′ ) ≤ U (β λ ) + λ ′ ϕ(β λ )$since both left-hand sides are minimizers. This implies

$λ(ϕ(β λ ) -ϕ(β λ ′ )) ≤ U (β λ ′ ) -U (β λ ) ≤ λ ′ (ϕ(β λ ) -ϕ(β λ ′ )). (7.6)$In particular: (λ ′λ)(ϕ(β λ )ϕ(β λ ′ )) ≥ 0. Assume that λ < λ ′ . Then this last inequality implies ϕ(β λ ) ≥ ϕ(β λ ′ ) and (7.6) then implies that U (β λ ) ≤ U (β λ ′ ), which proves the first part of the proposition. Now assume that there exists ϵ > 0 such that ϕ(β λ ) > inf ϕ + ϵ for all λ ≥ 0. Take β such that ϕ( β) ≤ inf ϕ + ϵ/2. For any λ > 0, we have

$U (β λ ) + λϕ(β λ ) ≤ U ( β) + λϕ( β) so that U (β λ ) < U ( β) -λϵ/2. Since U (β λ ) ≥ U (a 0 ), we get U (a 0 ) = -∞,$which is a contradiction. This shows that ϕ(β λ ) tends to inf(ϕ) when λ tends to infinity.

We now prove that λ → β λ is continuous. Define G(β, λ) = U (β)+λϕ(β). Since we assume that ϕ(β) → ∞ when β → ∞, and we have just proved that ϕ(β λ ) ≤ ϕ(a 0 ) for any λ, we obtain the fact that the set (|β λ |, λ ≥ 0) is bounded, say by a constant B ≥ 0.

Consider a sequence λ n that converges to λ. We want to prove that β λ n → β λ , for which (because β λ is bounded) it suffices to show that if any subsequence of (β λ n ) converges to some β, then β = β λ .

So, consider such a converging subsequence, that we will still denote by β λ n for convenience. Since G is continuous, one has G(β λ n , λ n ) → G( β, λ) when n tends to infinity. Let us prove that G(β λ , λ) is continuous in λ. For any pair λ, λ ′ and any β, we have

$G(β λ ′ , λ ′ ) ≤ G(β λ , λ ′ ) = G(β λ , λ) + (λ ′ -λ)ϕ(β λ ) ≤ G(β λ , λ) + |λ ′ -λ|ϕ(a 0 ) .$This yields, by symmetry,

$|G(β λ ′ , λ ′ ) -G(β λ , λ)| ≤ ϕ(a 0 )|λ -λ ′ |, proving the continuity in λ.$So we must have G( β, λ) = G(β λ , λ). This implies that both β and β λ are solutions of var(λ), so that β λ = β because we assume that the solution is unique.

## ■

We now prove that the classes of problems var(λ) and var

$′ (C) are equivalent. First, β λ is a minimizer of U (β) subject to the constraint ϕ(β) ≤ C, with C = ϕ(β λ ). Indeed, if U (β) < U (β λ ) for some β with ϕ(β) ≤ ϕ(β λ ), then U (β) + λϕ(β) < U (β λ ) + λϕ(β λ ) which is a contradiction. So β λ = β ′ ϕ(β λ )$. Using the continuity of β λ and ϕ, this proves the equivalence of the problems when C is in the interval (a, ϕ(a 0 )) where a = lim λ→∞ ϕ(β λ ) = inf(ϕ). So, it remains to consider the case C > ϕ(a 0 ). For such a C, the solution of var ′ (C) must be a 0 since it is a solution of the unconstrained problem, and satisfies the constraint.

## Lasso regression

Problem statement Assume that the output variable is scalar, i.e., q = 1. Let σ 2 (i) be the empirical variance of the ith variable X (i) . Then, the lasso estimator is defined as a minimizer of

$N k=1 (y k -xT k β) 2 subject to the constraint d i=1 σ (i)|β (i) | ≤ C.$Compared to ridge regression, the sum of squares for β is simply replaced by a weighted sum of absolute values, but we will see that this change may significantly affect the nature of the solutions.

As we have just seen, the penalized formulation, minimizing

$N k=1 (y k -xT k β) 2 + λ d i=1 σ (i)|β (i) |$provides an equivalent family of problems, on which we will focus (because it is easier to analyze). Since one uses a non-Euclidean norm in the penalty, there is no kernel version of the lasso and we only discuss the method in the original input space R = R d .

For a vector a ∈ R k , we let |a| 1 = |a (1) 

$| + • • • + |a (k) |,$the ℓ 1 norm of a. Using the previous notation for Y and X , the quantity to minimize can be rewritten as |Y -X β| 2 + λ|Dβ| 1 where D is the d × (d + 1) matrix with d(i, i + 1) = σ (i) for i = 1, . . . , d and all other coefficients equal to 0. This is a convex optimization problem which, unlike ridge regression, does not have a closed form solution. ADMM. The alternating direction method of multipliers (ADMM) that was described in section 3.6, (3.59) is one of the state-of-the-art algorithm to solve the lasso problem, especially in large dimensions. Other iterative methods include subgradient descent (see the example in section 3.5.4) and proximal gradient descent. Since x has a different meaning here, we change the notation in (3.59) by replacing x, z, u by β, γ, τ, and rewrite the lasso problem as the minimization of |Y -X β| 2 + λ|γ| 1 subject to Dβγ = 0. Applying (3.59) with A = D, B = -Id and c = 0, the ADMM iterations are

$                     β(n + 1) = argmin β |Y -X β| 2 + 1 2ρ |Dβ -γ(n) + τ(n)| 2 γ (i) (n + 1) = argmin t λ|t| + 1 2ρ (t -Dβ (i) (n + 1) -τ (i) (n)) 2 , i = 1, . . . , d τ(n + 1) = τ(n) + Dβ(n + 1) -γ(n + 1)$The solutions of both minimization problems are explicit, yielding the following algorithm, which converges to a solution if ρ is small enough.

Algorithm 7.1 (ADMM for lasso) Let ρ > 0 be chosen. Starting with initial values β (0) , γ (0) and τ (0) , the ADMM algorithm for lasso iterates:

$                 β(n + 1) = X T X + D T D 2ρ -1 X T Y + D T 2ρ (γ(n) -τ(n)) γ (i) (n + 1) = S λρ Dβ (i) (n + 1) + τ (i) (n) , i = 1, . . . , d τ(n + 1) = τ(n) + Dβ(n + 1) -γ(n + 1)$until the difference between the variables at steps n and n + 1 is below a small tolerance level. Here, S λρ is the so-called shrinkage operator

$S λρ (v) =            v -λρ if v ≥ λρ 0 if |v| ≤ λρ v + λρ if v ≤ -λρ$Note that the ADMM algorithm makes an iterative approximation of the constraints, so that they are only satisfied at some precision level when the algorithm is stopped.

Exact computation. We now provide a more detailed characterization of the solution of the lasso problem and analyze, in particular, how this solution changes when λ (or C) varies. To simplify the exposition, and without loss of generality, we will assume that the variables have been normalized so that σ (i) = 1 and the penalty simply is the sum of absolute values. Let

$G λ (β) = N k=1 (y k -a 0 -x T k b) 2 + λ d i=1 |b(i)|.$The following proposition, in which we let

$r b = 1 N N k=1 (y k -a 0 -x T k b)x k ,$characterizes the solution of the lasso.

Proposition 7. [6](#b24) The pair (a 0 , b) is the optimal solution of the lasso problem with parameter λ if and only if a 0 = ȳ -xT b and, for all i = 1, . . . , d,

$|r (i) b | ≤ λ 2N (7.7) with r (i) b = sign(b (i) ) λ 2N if b (i) 0. (7.8)$In particular |r

$(i) b | < λ/(2N ) implies b (i) = 0.$Proof Using the subdifferential calculus in theorem 3.45, one can compute the subgradients of G by adding the subdifferentials of the terms that compose it. All these terms are differentiable except |b (i) | when b (i) = 0, and the subdifferential of t → |t| at t = 0 is the interval [[-1, 1]](#).

This shows that g ∈ ∂G λ (β) if and only if g = -2N r b + λz with z (i) = sign(b (i) ) if b (i) 0 and |z (i) | ≤ 1 otherwise. Proposition 7.6 immediately follows by taking g = 0.

■ Let ζ = sign(b), the vector formed by the signs of the coordinates of b, with sign(0) = 0. Then proposition 7.6 uniquely specifies a 0 and b once λ and ζ are known. Indeed, let J = J ζ denote the ordered subset of indices j ∈ {1, . . . , d} such that ζ (j) 0, and let b(J), x k (J), ζ(J), etc., denote the restrictions of vectors to these indices. Equation (7.8) can be rewritten as (after replacing a 0 by its optimal value)

$X c (J) T X c (J)b(J) = X c (J) T Y c - λ 2 ζ(J)$where

$X c (J) =           (x 1 (J) -x(J)) T . . . (x N (J) -x(J)) T           . This yields b(J) = (X c (J) T X c (J)) -1 X c (J) T Y c - λ 2 ζ(J) ,(7.9)$which fully determine b since b (j) = 0 if j J, by definition.

For given λ, only one sign configuration ζ will provide the correct solution, with correct signs for nonzero values of b above, and correct inequalities on r b . Calling this configuration ζ λ , one can note that if ζ λ is known for a given value of λ, it remains valid if we increase or decrease λ until one of the optimality conditions changes, i.e., either one of the coordinates b (i) , i ∈ J ζ λ , vanishes, or one of the inequalities for i J ζ λ becomes an equality. Moreover, proposition 7.6 shows that between these events both b and therefore r b depend linearly on λ, which makes easy the task of determining maximal intervals around a given λ over which ζ remains unchanged.

Note that solutions are known for λ = 0 (standard least squares) and for λ large enough (for which b = 0). Indeed, for b = 0 to be a solution, it suffices that

$λ > λ 0 ∆ = 2 max i N k=1 (y k -y)(x (i) k -x (i) ) .$These remarks set the stage for an algorithm computing the optimal solution of the lasso problem for all values of λ, starting either from λ = 0 or λ > λ 0 . We will describe this algorithm starting for λ > λ 0 , which has the merit to avoid complications due to underconstrained least squares when d is large. For this purpose, we need a little more notation. For a given ζ, let

$b ζ = (X c (J ζ ) T X c (J ζ )) -1 X c (J ζ ) T Y c and u ζ = 1 2 (X c (J ζ ) T X c (J ζ )) -1 ζ(J ζ ), so that b(J ζ ) = b ζ -λu ζ .$The residuals then take the form

$r (i) b = 1 N N k=1 (y k -a 0 -b T ζ x k )x (i) k + λ N N k=1 (x T k u ζ )(x (i) k -x (i) ) = ρ (i) ζ + λd (i) ζ ,$where the last equation defines ρ ζ and d ζ .

Assume that one wants to minimize G λ * for some λ * > 0. We need to describe the sequence of changes to the minimizers of G λ when λ decreases from some value larger than λ 0 to the value λ * .

If λ * ≥ λ 0 , then the optimal solution is b = 0, so we can assume that λ * < λ 0 . When λ is slightly smaller than λ 0 , one needs to introduce some non-zero values in ζ. Those values are at the indexes i such that

$λ 0 = 2 N k=1 (y k -y)(x (i) k -x (i) )$The sign of ζ (i) is also determined since sign(b (i) ) = sign(r

$(i) b ) when b (i) 0.$The algorithm will then continue by progressively adding non-zero entries to ζ when the covariance between some unused variables and the residual becomes too large, or by removing non-zero values when the optimal b crosses a zero. We now describe it in detail. Algorithm 7.2 (Exact minimization for lasso)

1. Initialization: let λ(0) = 1 + λ 0 , σ (0) = 0 and the corresponding values a 0 (0) = y and b(0) = 0.

2. Assume that the algorithm has reached step n with current variables λ(n), σ (n), a 0 (n) and b(n).

## Determine the first λ

$′ < λ(n) for which either (i) For some i, ζ (i) (n) 0 and b (i) ζ(n) -λ ′ u (i) ζ(n) = 0. (ii) For some i, ζ (i) (n) = 0 and (1 -2N d (i) ζ(n) )λ ′ -2N ρ ζ(n) = 0. (iii) For some i, ζ (i) (n) = 0 and (1 + 2N d (i) ζ(n) )λ ′ + 2N ρ ζ(n) = 0.$4. Then, there are two cases: 

$(a) If λ ′ ≥ λ * , set λ(n + 1) = λ ′ . Let ζ (i) (n + 1) = ζ (i) (n) if i does not satisfy (i), (ii) or (iii). If i is in case (i), set ζ (i) (n+1) = 0. For i in case (ii) (resp. (iii)), set ζ (i) (n+1) = 1 (resp. -1). (b) If λ ′ < λ * ,$$(i) = b (i) ζ(n) -λ * u (i) ζ(n) , ζ (i) (n) 0$and a 0 = ȳb T x to obtain the final solution.

## Other Sparsity Estimators

## LARS estimator

Algorithm. The LARS algorithm can be seen as a simplification of the previous lasso algorithm in which one always adds active variables at each step. We assume as above that input variables are normalized such that σ (i) = 1.

Given a current set J of selected variables, the algorithm will decide either to stop or to add a new variable to J according to a criterion that depends on a parameter λ > 0. Let b (J) ∈ R |J| be the least-square estimator based on variables in J b

$(J) = (X c (J) T X c (J)) -1 X c (J) T Y c . Let b J ∈ R d such that b (i) J = b (i)$(J) for i ∈ J and 0 otherwise. The covariances between the remaining variables and the residuals are given by

$r (i) J = 1 N N k=1 (y k -y -(x k -x) T b J )(x (i) k -x (i) ), i J.$If, for all i ∈ J, |r

$(i) J | ≤ √ λ/N ,$the procedure is stopped. Otherwise, one adds to J the variable i such that |r (i) J | is largest and continues. Justification. Recall the notation |b| 0 for the number of non-zero entries of b. Consider the objective function

$L(b) = |Y c -X c b| 2 + λ|b| 0 .$Let J be the set currently selected by the algorithm, and b J defined as above. We consider the problem of adding one non-zero entry to b. Fix i J, and let b ∈ R d have all coordinates equalt to those of b J for all except the ith one, which is therefore allowed to be non-zero. Then

$L( b) = N k=1 y k -y - j∈J (x (j) k -x)b (j) -(x (i) k -x) b(i) 2 + λ|J| + λ,$so that (using σ (i) = 1)

$L( b) = L(b J ) -2N r (i) J b(i) + N ( b(i) ) 2 + λ$Now, L( b) is an upper-bound for L(b J∪{i} ), and so is its minimum with respect to b(i) . This yields:

$L(b J∪{i} ) ≤ L(b J ) -N (r(i)$J ) 2 + λ The LARS algorithm therefore finds the value of i that minimizes this upper-bound, provided that the resulting minimum is less that L(b J ).

Variant. The same argument can be made with |b| 0 replaced by |b| 1 and one gets

$L( b) = L(b J ) -2N r (i) J b(i) + N ( b(i) ) 2 + λ| b(i) |$Minimizing this expression with respect to b(i) yields the upper bound:

$L(b J∪{i} ) ≤            L(b J ) -N |r (i) J | - λ 2N 2 if |r (i) J | ≥ λ 2N L(b J ) if |r (i) J | ≤ λ 2N$This leads to the following alternate form of LARS. Given a current set J of selected variables, compute

$r (i) J = 1 N N k=1 (y k -y -(x k -x) T b J )(x (i) k -x (i) ), i J .$If, for all i J, |r (i) J | ≤ λ/2N , stop the procedure. Otherwise, add to J the variable i such that |r (i) J | is largest and continue. This form tends to add more variables since the stopping criterion decreases in 1/N instead of 1/ √ N .

Why "least angle"? Let µ J,k = y k -y -(x k -x) T b J denote the residual after regression. The empirical correlation between µ and x (i) is equal to the cosine of the angle, say θ (i) J between µ J ∈ R N and x (i) -x both considered as vectors in R N . This cosine is also equal to cos θ

$(i) J = µ T J (x (i) -x (i) ) |x (i) -x (i) | |µ J | = √ N r (i) J |µ J |$where we have used the fact that |x (i) -

$x (i) |/ √ N = σ (i) = 1.$Since |µ J | does not depend on i, looking for the largest value of |r (i) J | is equivalent to looking for the smallest value of |θ (i)

J |, so that we are looking for the unselected variable for which the angle with the current residual is minimal.

## The Dantzig selector

Noise-free case. Assume that one wants to solve the equation X β = Y when the dimension, N , of Y is small compared to number of columns, d, in X . Since the system is under-determined, one needs additional constraints on β and a natural one is to look for sparse solutions, i.e., find solutions with a maximum number of zero coefficients. However, this is numerically challenging, and it is easier to minimize the ℓ 1 norm of β instead (as seen when discussing the lasso, using this norm often provides sparse solutions). In the following, we assume that the empirical variance of each variable is normalized, so that, denoting X (i) the ith column of X , we have

$|X (i)| = 1.$The Dantzig selector [[47]](#b65)

$minimizes d i=1 |β (i) |$subject to the constraint X β = Y . This results in a linear program (therefore easy to implement). More precisely, introducing slack variables, it is indeed equivalent to minimize

$d i=1 ξ(i) + d i=1 ξ * (i) subject to constraints ξ(i) ≥ β (i) , ξ(i) * ≥ -β (i) , ξ(i) ≥ 0, ξ * (i) ≥ 0 and X β = Y .$Sparsity recovery Under some assumptions, this method does recover sparse solutions when they exist. More precisely, let β be the solution of the linear programming problem above. Assume that there is a set J * ⊂ {1, . . . , d} such that X β = Y for some β ∈ R d with β (i) = 0 if i J * . Conditions under which β is equal to β are provided in Candes and Tao [[47]](#b65) and involve the correlations between pairs of columns of X , and the size of J.

That the size of J * must be a factor is clear, since, for the statement to make sense, there cannot exist two β's satisfying X β = Y and β (i) = 0 for i J * . Uniqueness is obviously not true if |J| > N , because, even if one knew J, the condition would be under-constrained for β. Since the set J * is not known, and we also want to avoid any other solution associated to a set of same size. So, there cannot exist β and β respectively vanishing outside of J * and J * , where J * and J * have same cardinality, such that X β = Y = X β. The equation X (β -β) = 0 would be under-constrained as soon as the number of non-zero coefficients of β -β is larger than N , and since this number can be as large as

$|J * | + | J * | = 2|J * |, we see that one should impose at least |J * | ≤ N /2.$Given this restriction, another obvious remark is that, if the set J on which β does not vanish is known, with |J| small enough, then X β = Y is over-constrained and any solution is (typically) unique. So the issue really is whether the set J β listing the non-zero indexes of a solution β is equal to y J * .

As often, precious insight on the solution of this minimization problem is obtained by considering the dual problem. Introducing Lagrange multipliers λ(i) ≥ 0, i = 1, . . . , d for the constraints ξ(i)β (i) ≥ 0, λ * (i) ≥ 0, i = 1, . . . , d for ξ * (i) + β (i) ≥ 0, γ(i), γ * (i) ≥ 0 for ξ(i) ≥ 0 and ξ * (i) ≥ 0, and α ∈ R N for X β = Y , the Lagrangian is

$L(β, ξ, λ, λ * , α) = (1 d -λ -γ) T ξ + (1 d -λ * -γ * ) T ξ * + (λ -λ * + X T α) T β -α T Y . The KKT conditions require γ = 1 d -λ, γ * = 1 d -λ * , X α = λ * -λ and the comple- mentary slackness conditions give (1 -λ(i))ξ(i) = (1 -λ * i )ξ * (i) = 0, λ(i)(β (i) -ξ(i)) = λ * (i)(β (i) + ξ * (i)) = 0.$The dual problem requires to minimize α T Y subject to the constraints X T α = λ *λ and 0 ≤ λ(i), λ * (i) ≤ 1. Assume that (α, λ, λ * ) is a solution of this dual problem. One has the following cases.

(1) If λ(i) ∈ (0, 1), then ξ(i) = β (i)ξ(i) = 0, which implies ξ(i) = β (i) = 0, and, as a consequence (1λ * (i))ξ * (i) = λ * (i)ξ * (i) = 0, so that also ξ * (i) = 0 .

(2) Similarly, λ * (i) ∈ (0, 1) implies

$ξ(i) = ξ * (i) = β (i) = 0. (3) If λ(i) = λ * (i) = 1, then β (i) -ξ(i) = β (i) + ξ(i) = 0 with ξ(i), ξ * (i) ≥ 0, so that also ξ(i) = ξ * (i) = β (i) = 0. (4) If λ(i) = λ * (i) = 0, then ξ(i) = ξ * (i) = 0 and since β (i) ≤ ξ(i) and β (i) ≤ -ξ * (i), we get β (i) = 0.$(5) The only remaining situation, in which β (i) can be non-zero, is when λ(i) = 1λ * (i) ∈ {0, 1}, or, equivalently, when |λ(i)λ * (i)| = 1. This discussion allows one to reconstruct the set J β associated with the primal problem given the solution of the dual problem. Note that |λ(i)λ * (i)| = |α T X (i)|, so that the set of indexes with |λ(i)λ * (i)| = 1 is also

$I α ∆ = i : |α T X (i)| = 1 .$One has

$α T Y = α T X β = d i=1 β (i) α T X (i) ≤ i∈J β |β (i) | |α T X (i)| ≤ i∈J β |β (i) |.$The upper-bound is achieved when α T X (i) = sign(β (i) ) for i ∈ J β . So, if a vector α can be found such that

$(i) α T X (i) = sign(β (i) ) for i ∈ J * , (ii) |α T X (j)| < 1 for j J * ,$then it is a solution of the dual problem with J α = J * .

Let s J = (s (j) , j ∈ J) be defined by s (j) = sign(β (j) ). One can always decompose α ∈ R N in the form α = X J * ρ + w where ρ ∈ R |J * | and w ∈ R N is perpendicular to the columns of X J * . From X T J * α = s J , we get ρ = (X T J * X J * ) -1 s J * . Letting α J * be the solution with w = 0, the question is therefore whether one can find w such that w T X (j) = 0,

$j ∈ J * |α T J X (k) + w T X (k)| < 1, k J *$Denote for short Σ JJ ′ = X T J X J ′ . One can show that such a solution exists when the matrices Σ JJ are close to the identity as soon as |J| is small enough [[47]](#b65). More precisely, denote, for q ≤ d δ(q) = max

$|J|≤q max(∥Σ JJ ∥, ∥Σ -1 JJ ∥ -1 ) -1,$in which one uses the operator norm on matrices, and

$θ(q, q ′ ) = max z T Σ T T ′ z ′ : |J|, |J ′ | ≤ q.J ∩ J ′ = ∅, |z| = |z ′ | = 1 .$Then, the following proposition is true.

Proposition 7.7 (Candes-Tao) Let q = |J * | and s = (s (j) , j ∈ J * ) ∈ R q . Assume that δ(2q) + θ(q, 2q) < 1. Then there exists α ∈ R N such that α T X (j) = s (j) for j ∈ J * and

$|α T X (j)| ≤ θ(q, q) 1 -δ(2q) -θ(q, 2q) if j J * .$So α has the desired property as soon as δ(2q) + θ(q, q) + θ(q, 2q) ≤ 1. to control subsets of variables of size less than 3q to obtain the conclusion, which is important, of course, when q is small compared to d.

Noisy case Consider now the noisy case. We here again introduce quantities that were pivotal for the lasso and LARS estimators, namely, the covariances between the variables and the residual error. So, we define, for a given β r (i)

$β = X (i) T (Y -X β)$which depends linearly on β. Then, the Dantzig selector is defined by the linear program: Minimize:

$d j=1 |β (j) | subject to the constraint: max j=1,...,d |r (j) β | ≤ C.$The explicit expression of this problem as a linear program is obtained as before by introducing slack variables ξ(j), ξ * (j), j = 1, . . . , d and minimizing

$d j=1 ξ(j) + d j=1 ξ * (j) with constraints ξ(j), ξ * (j) ≥ 0, ξ ≥ β, ξ * ≥ -β, max j=1,...,d |r (j) β | ≤ C.$Similar to the noise-free case, the Dantzig selector can identify sparse solutions (up to a small error) if the columns of X are nearly orthogonal, with the same type of conditions [[48]](#b66). Interestingly enough, the accuracy of this algorithm can be proved to be comparable to that of the lasso in the presence of a sparse solution [[30]](#b48).

7.4 Support Vector Machines for regression 7.4.1 Linear SVM Problem formulation We start by discussing support vector machines (SVM) [[196,](#b214)[197]](#b215) with R X = R d equipped with the standard inner product (generally referred to as linear SVM) and will extend the theory to kernel methods in the next section. SVMs solve a linear regression problem, but replace the least-squares loss function by (y, y ′ ) → V (yy ′ ) with A plot of the function V is provided in fig. [7](#).1. This function is an example of what is often called a robust loss function. The quadratic error used in linear regression had the advantage of providing closed form expressions for the solution, but is quite sensitive to outliers. For robustness, it is preferable to use loss functions that, like V , increase at most linearly at infinity. One sometimes choose them as smooth convex functions, for example V (t) = (1cos γt)/(1cos γϵ) for |t| < ϵ and f (t) = |t| for t ≥ ϵ, where γ is chosen so that γϵ sin γϵ/(1cos γϵ) = 1. In such a case, minimizing

$V (t) = 0 if |t| < ϵ |t| -ϵ if |t| ≥ ϵ (7.10)$$F(β) = N k=1 V (y k -a 0 -x T k b)$can be done using gradient descent methods. Using V in (7.10) will require a little more work, as we see now.

The SVM regression problem is generally formulated as the minimization of

$N k=1 V (y k -a 0 -x T k b) + λ|b| 2 ,$and we will study a slightly more general problem, minimizing

$F(a 0 , b) = N k=1 V (y k -a 0 -x T k b) + λb T ∆b ,$where ∆ is a symmetric positive-definite matrix. This objective function exhibits the following features:

• A penalty on the coefficients of b, similar to ridge regression.

• A linear penalty (instead of quadratic) for large errors in the prediction.

• An ϵ-tolerance for small errors, often referred to as the margin of the regression SVM.

We now describe the various steps in the analysis and reduction of the problem. They will lead to simple minimization algorithms, and possible extensions to nonlinear problems.

Reduction to a quadratic programming problem Introduce slack variables ξ k , ξ * k , k = 1, . . . , N . The original problem is equivalent to the minimization, with respect to (a 0 , b, ξ, ξ * ), of

$N k=1 (ξ k + ξ * k ) + λb T ∆b under the constraints:            ξ k , ξ * k ≥ 0 ξ k -y k + a 0 + x T k b + ϵ ≥ 0 ξ * k + y k -a 0 -x T k b + ϵ ≥ 0 (7.11)$The simple proof of this equivalence, which results in a quadratic programming problem, is left to the reader. As often, one gains additional insight by studying the dual problem.

Dual problem Introduce 4N non-negative Lagrange multipliers for the 4N constraints in the problem, namely, η k , η * k ≥ 0 for the positivity constraints, and α k , α * k ≥ 0 for the last two in [(7.11)](#). The resulting Lagrangian is

$L(a 0 , b, ξ, ξ * , α, α * , η, η * ) = N k=1 (ξ k + ξ * k ) + λb T ∆b - N k=1 (η k ξ k + η * k ξ * k ) - N k=1 α k (ξ k -y k + a 0 + x T k b + ϵ) - N k=1 α * k (ξ * k + y k -a 0 -x T k b + ϵ).$In this formulation, (a 0 , b, ξ, ξ * ) are the primal variables, and α, α * , η, η * the dual variables.

The KKT conditions are provided by the system:

$                                               N k=1 (α k -α * k ) = 0 2λ∆b - N k=1 (α k -α * k )x k = 0 1 -η k -α k = 0 1 -η * k -α * k = 0 α k (ϵ + ξ k -y k + a 0 + x T k b) = 0 α * k (ϵ + ξ * k + y k -a 0 -x T k b) = 0 η k ξ k = η * k ξ * k = 0 (7.12)$The first four equations are the derivatives of the Lagrangian with respect to a 0 , b, ξ k , ξ * k in this order and the last three are the complementary slackness conditions.

The dual problem maximizes the function

$L * (α, α * , η, η * ) = inf β,ξ,ξ * L.$under the previous positivity constraints. Since the Lagrangian is linear in a 0 , ξ k and ξ * k , its minimum is -∞ unless the coefficients vanish. The linear terms must therefore vanish for L * to be finite. With these conditions plus the fact that ∂ b L = 0, we retrieve the first four equations of system (7.12). Using

$η k = 1 -α k , η * k = 1 -α * k and b = 1 2λ N k=1 (α k -α * k )∆ -1 x k (7.13)$one can express L * uniquely as a function of α, α * , yielding

$L * (α, α * ) = - 1 4λ N k,l=1 (α k -α * k )(α l -α * l )x T k ∆ -1 x l -ϵ N k=1 (α k + α * k ) + N k=1 (α k -α * k )y k .$This quantity must be maximized subject to the constraints 0 ≤ α k , α * k ≤ 1 and

$N k=1 (α k -α * k ) = 0.$This still is a quadratic programming problem, but it now has nice additional features and interpretations.

Step 3: Analysis of the dual problem The dual problem only depends on the x k 's through the matrix with coefficients x T k ∆ -1 x l , which is the Gram matrix of x 1 , . . . , x N for the inner product associated with ∆ -1 . This property will lead to the the kernel version of SVMs discussed in the next section. The obtained predictor can also be expressed as a function of these products, since

$y = a 0 + x T b = a 0 + 1 2λ N k=1 (α k -α * k )(x T k ∆ -1 x) .$Moreover, the dimension of the dual problem is 2N , which allows the method to be used in large (possibly infinite) dimensions with a bounded cost.

We now analyze the solutions α, α * of the dual problem. The complementary slackness conditions reduce to:

$           α k (ϵ + ξ k -y k + a 0 + x T k b) = 0 α * k (ϵ + ξ * k + y k -a 0 -x T k b) = 0 (1 -α k )ξ k = (1 -α * k )ξ * k = 0 (7.14)$These conditions have the following consequences, based on the prediction error made for each training sample.

(i) First consider indexes k such that the error is strictly within the tolerance margin ϵ: |y ka 0x T k b| < ϵ. Then the terms between parentheses in first two equations of (7.14) are strictly positive, which implies that α k = α * k = 0. The last two equations in [(7.14)](#) then imply ξ k = ξ * k = 0. (ii) Consider now the case when the prediction is strictly less accurate than the tolerance margin. Assume that y ka 0x T k b > ϵ. The second and third equations in [(7.14)](#)

$imply that α * k = ξ * k = 0.$The assumption also implies that

$ξ k = y k -a 0 -x T k b -ϵ > 0 and α k = 1. The case y k -a 0 -x T k b < -ϵ is symmetric and provides α k = ξ k = 0, ξ * k > 0 and α * k = 1.$(iii) Finally, consider samples for which the prediction error is exactly at the tolerance margin. If

$y k -a 0 -x T k b = ϵ, we have α * k = ξ k = ξ * k = 0. The fact that α * k = ξ * k = 0 is clear.$To prove that ξ k = 0, we note that would have otherwise ξ k -y k +a 0 +x T k b+ϵ > 0, which would imply that α k = 0 and we reach a contradiction with [(](#)

$1-α k )ξ k = 0. Sim- ilarly, y k -a 0 -x T k b = -ϵ implies that α k = ξ k = ξ * k = 0. The points for which |y k -a 0 -x T k b| = ϵ are called support vectors.$One important information deriving from this discussion is that the variables (α k , α * k ) have prescribed values as long as the error y ka 0x T k b is not exactly ϵ in absolute value: (1, 0) if the error is larger than ϵ, (0, 0) if it is strictly between -ϵ and ϵ and (0, 1) if it is less than -ϵ. Also in all cases, at least one of α k and α * k must vanish. Only in the case of support vectors does the previous discussion fail to provide a value for one of these variables. Now, we want to reverse the discussion and assume that the dual problem is solved to see how the variables a 0 and b of the primal problem can be retrieved. For b, this is easy, thanks to (7.13). For a 0 a direct computation can be made if a support vector is identified, either because 0

$< α k < 1, which implies that a 0 = y k -x T k b -ϵ, or because 0 < α * k < 1, which yields a 0 = y k -x T k b + ϵ.$If no support vector can be identified, a 0 is not uniquely determined (note that the objective function is not strictly convex in a 0 ). However, the coefficients α k , α * k provide some information on this intercept, in the form of inequalities. More precisely, let

$J + = {k : α k = 1}, J -= {k : α * k = 1} and J 0 = {k : α k = α * k = 0}. Then k ∈ J + implies that y k -a 0 -b T x k ≥ ϵ, so that a 0 ≤ y k -b T x k -ϵ. Similarly, k ∈ J -implies that a 0 ≥ y k -b T x k + ϵ. Finally, k ∈ J 0 implies that a 0 ≥ y k -b T x k -ϵ and a 0 ≤ y k -b T x k + ϵ.$As a consequence, one can take a 0 to be any point in the interval [a 0 -, a 0 + ], where

$a 0 -= max max k∈J -(y k -x T k b + ϵ), max k∈J 0 (y k -x T k b -ϵ) a 0 + = min min k∈J + (y k -x T k b -ϵ), min k∈J 0 (y k -x T k b + ϵ) .$
## The kernel trick and SVMs

Returning to our feature space notation, let X take values in R X and h : R X → H be a feature function with values in an inner-product space H with associated kernel K. SVMs in feature space must minimize, with

$a 0 ∈ R and b ∈ H F(a 0 , b) = N k=1 V y k -a 0 -⟨h(x k ) , b⟩ H + λ∥b∥ 2 H .$Letting as before V = span(h(x 1 ), . . . , h(x N )), the same argument as that made for ridge regression works, namely that the first term in F is unchanged if b is replaced by π V (b) and the second one is strictly reduced unless b ∈ V , leading to a finitedimensional formulation in which

$b = N k=1 c k h(x k )$and one minimizes

$F(a 0 , c) = N k=1 V y k -a 0 - N l=1 K(x k , x l )c l + λ N k,l=1 K(x k , x l )c k c l .$This function has the same form as the one studied in the linear case with b replaced by c ∈ R N , x k replaced by the vector with coefficients K(x k , x l ), l = 1, . . . , N , that we will denote K (k) and

$∆ = K = K(x 1 , . . . , x N ). Note that K (k) is the kth column of K, so that K (k) T K -1 K (l) = K(x k , x l ).$Using this, we find that the dual problem requires to maximize

$L * (α, α * ) = - 1 4λ N k,l=1 (α k -α * k )(α l -α * l )K(x k , x l ) -ϵ N k=1 (α k + α * k ) + N k=1 (α k -α * k )y k . with                  0 ≤ α k ≤ 1 0 ≤ α * k ≤ 1 N k=1 (α k -α * k ) = 0$The associated vector c satisfies

$2λc = N k=1 (α k -α * k )K -1 K (k) = α -α * .$and the regression function is

$f (x) = a 0 + ⟨b , h(x)⟩ H = a 0 + 1 2λ N k=1 (α k -α * k )K(x, x k ) .$Finally, the discussions on the values of α, α * and on the computation of a 0 remain unchanged.

Chapter 8

## Models for linear classification

In this chapter, Y is categorical and takes values in the finite set R Y = g 1 , . . . , g q . The goal is to predict this class variable from the input X, taking values in a set R X .

Using the same progression as in the regression case, we will first discuss basic linear methods, for which R X = R d before extending them, whenever possible, to kernel methods, for which R X can be arbitrary as soon as a feature space representation is available.

Classifiers will be based on a training set T = ((x 1 , y 1 ), . . . , (x N , y N )) with x k ∈ R X and y k ∈ R Y for k = 1, . . . , N . For g ∈ R Y , we will also let N g denote the number of samples in the training set such that y k = g, i.e.,

$N g = {k : y k = g} = N k=1 1 y k =g .$
## Logistic regression 8.1.1 General Framework

Logistic regression uses the fact that, in order to apply Bayes's rule, only the conditional distribution of the class variables Y given X is needed, and trains a parametric model of this distribution. More precisely, if one denotes by p(g|x) the probability that Y = g conditional to X = x, logistic regression assumes that, for some parameters (a 0 (g), b(g), g ∈ R Y ) with a 0 (g) ∈ R and b(g

$) ∈ R d , one has p = p a 0 ,b with log p a 0 ,b (g | x) = a 0 (g) + x T b(g) -log(C(a 0 , b, x)), where C(a 0 , b, x) = g∈R Y exp(a 0 (g) + x T b(g)).$Introduce the functions, defined over mappings µ : R Y → R (which can be identified with vectors in R q )

$F g (µ) = µ(g) -log g ′ ∈R Y e µ(g ′ ) . (8.1)$With this notation, letting

$β(g) = a 0 (g) b(g) and x = 1 x , one has log p β (g|x) = F g (β T x),$where

$β T x is the function (g ′ → β(g ′ ) T x).$For any constant function (g → µ 0 ∈ R) one has

$F g (µ + µ 0 ) = µ(g) + µ 0 -log g ′ ∈R Y e µ(g ′ )+µ 0 = µ(g) + µ 0 -µ 0 -log g ′ ∈R Y e µ(g ′ ) = F g (µ).$As a consequence, if one replaces, for all g, β(g) by β(g

$) = β(g) + γ, with γ ∈ R d+1 , then βT x = β T x + γ T x and log p β (g | x) = log p β (g | x).$This shows that the model is over-parametrized. One therefore needs a (d + 1)dimensional constraint to ensure uniqueness, and we will enforce a linear constraint in the form

$g∈R Y ρ g β(g) = c$with g ρ g 0.

## Conditional log-likelihood

The conditional log-likelihood computed from the training set is:

$ℓ(β) = N k=1 log p β (y k | x k ) .$Logistic regression computes a maximizer β of this log-likelihood. The classification rule given a new input x then chooses the class g for which p β (g | x) is largest, or, equivalently, the class g that maximizes xT β(g).

$Proposition 8.1 Let mg = k:y g =k x k /N g . The conditional log-likelihood ℓ is concave with first derivative ∂ β(g) ℓ = N g mT g - N k=1 xT k p β (g|x k ) (8.2)$and negative semi-definite second derivative

$∂ β(g) ∂ β(g ′ ) ℓ = -1 [g=g ′ ] N k=1 xk xT k p β (g|x k ) + N k=1 xk xT k p β (g|x k )p β (g ′ |x k ) . (8.3) Remark 8.2$In this discussion, we consider ℓ as a function defined over collections (β(g), g ∈ R Y ), or, if one prefers, on the q(d + 1)-dimensional linear space, F , of functions β : R Y → R q+1 . With this in mind, the differential dℓ(β) is a linear form from F to R, therefore associating to any family u = (u(g), g ∈ R Y ), the expression

$dℓ(β) u = g∈R Y ∂ β(g) ℓ u(g).$Similarly, the second derivative is the bilinear form

$d 2 ℓ(β)(u, u ′ ) = g,g ′ ∈R Y u(g) T ∂ β(g) ∂ β(g ′ ) ℓ u(g ′ ).$The last statement in the proposition expresses the fact that

$d 2 ℓ(β)(u, u) ≤ 0 for all u ∈ F . ♦ Proof First consider the function F g in (8.1), so that ℓ(β) = N k=1 F y k (β T xk ).$We have, for ζ : R Y → R,

$dF g (µ)ζ = ζ(g) - g ′ ∈R Y e µ(g ′ ) ζ(g ′ ) g ′ ∈R Y e µ(g ′ )$as can be easily computed by evaluating the derivative of F(µ + ϵu) at ϵ = 0. Introducing the notation

$q µ (g) = e µ(g) g∈R Y e µ(g) and ⟨ζ⟩ µ = g∈R Y ζ(g)q µ (g), we have dF g (µ)ζ = ζ(g) -⟨ζ⟩ µ .$Evaluating the derivative of dF g (µ + ϵu ′ )(ζ) at ϵ = 0, one gets (the computation being left to the reader)

$d 2 F g (µ)(ζ, ζ ′ ) = -⟨ζζ ′ ⟩ µ + ⟨ζ⟩ µ ⟨ζ ′ ⟩ µ . (8.4) Note that -d 2 F g (µ)(ζ, ζ)$is the variance of ζ for the probability mass function q µ and is therefore non-negative (so that F µ is concave). This immediately shows that ℓ is concave as a sum of concave functions.

Using the chain rule, we have, for u :

$R Y → R q , dℓ(β)u = N k=1 dF y k (β T xk ) xT k u(•) = N k=1 xT k u(y k ) - N k=1 ⟨ xT k u(•)⟩ β T xk .$Reordering the first sum in the right-hand side according to the values of y k gives

$N k=1 u(y k ) T xk = g∈R Y N g u(g) T mg .$Noting that q β T x = p β (•|x), we find

$dℓ(β)(u) = g∈R Y N g mT g u(g) - g∈R Y xT k u(g)p β (g|x k ), yielding(8.2)$. Applying the chain rule again, we have

$d 2 ℓ(β)(u, u ′ ) = N k=1 d 2 F y k (β T xk )( xT k u(•), xT k u ′ (•))(8.5)$with

$d 2 F y k (β T xk )( xT k u(•), xT k u ′ (•)) = -⟨u(•) T xk xT k u ′ (•)⟩ β T xk + ⟨ xT k u(•)⟩ β T xk ⟨ xT k u ′ (•)⟩ β T xk = - g∈R Y u(g) T xk xT k u ′ (g)p β (g|x k ) + g,g ′ ∈R Y u(g) T xk xT k u ′ (g ′ )p β (g|x k )p β (g ′ |x k ) from which (8.3) follows. ■ Remark 8.3 From F g (µ + µ 0 ) = F g (µ) when µ 0 is constant on R Y , one deduces (taking the derivative at µ 0 = 0) that dF g (µ)1 = 0 for all µ, where 1 denotes the constant function equal to 1 on R Y . For h ∈ R d+1 , let c h denote the constant function c h (g) = h, g ∈ R Y . We have dℓ(β) c h = N k=1 dF y k (β T xk ) xT k c h = N k=1 dF y k (β T xk )( xT k h1) = N k=1 ( xT k h)dF y k (β T xk )1 = 0.$Taking one extra derivative we see that

$dℓ(β)(c h , u) = 0$for all functions u : R Y → R q .

## ♦

We now discuss whether there are other elements in the null space of the second derivative of ℓ. We will use notation introduced in the proof of proposition 8.1. From (8.4), we have d 2 F g (µ)(ζ, ζ) = 0 if and only if the variance of ζ for q µ vanishes, which, since q µ > 0, is equivalent to ζ being constant. So, the null space of d 2 F g (µ) is one-dimensional, and composed of scalar multiples of 1. Using (8.5), we see that

$d 2 ℓ(u, u) = 0 if and only if , for all k = 1, . . . , N , (g → xT k u(g)) is a constant function.$Assume that this is true. Then, letting ū =[foot_8](#foot_8) q g∈R Y u(g), one has, for all g ∈ R Y and k = 1, . . . , N , xT k u(g) = xT k ū so that u(g) -ū is in the null space of the matrix X . This leads to the following proposition.

Proposition 8.4 Assume that X has rank d + 1. Then the null space of d 2 ℓ(β) is the set of all vectors u = c h for h ∈ R d+1 . In particular, for any c ∈ R d+1 , the function ℓ restricted to the space

$M =          β : g∈R Y ρ g β(g) = c          is strictly concave as soon as the scalar coefficients (ρ g , g ∈ R Y ) are such that g∈R Y ρ g 0.$Proof From the discussion before the proposition, u ∈ Null(d 2 ℓ) implies that X (u(g)ū) = 0 for all g, and since we assume that X has rand d + 1, this requires that u(g) = ū for all g, i.e., u = c ū . This proves the first point.

If one restricts ℓ to M, then we must restrict d 2 ℓ(β) to those u's such that g∈R Y ρ g u(g) = 0. But if d 2 ℓ(β)(u, u) = 0 for such an u, then u = c ū and

$g∈R Y ρ g u(g) = g∈R Y ρ g ū.$Since we assume that g∈R Y ρ g 0, this requires ū = 0, and therefore u = 0. This shows that the second derivative of the restriction of ℓ to M is negative definite, so this restriction is strictly concave. 

## Training algorithm

Given that we have expressed the first and second derivatives of ℓ in closed form 1 , we can use Newton-Raphson gradient ascent to maximize ℓ over the affine space:

$M =          β : g∈R Y ρ g β(g) = c         $with g∈R Y ρ g 0. We assume in the following that the matrix X has rand d + 1 so that proposition 8.4 applies. Since the constraint is affine, it is easy to express one of the parameters β(g) as a function of the others and solve the strictly concave problem as a function of the remaining variables. It is not much harder, and arguably more elegant to solve the problem without breaking its symmetry with respect to the class indexes, as described below.

Let

$M 0 =          β : g∈R Y ρ g β(g) = 0          .$We still have the second order expansion

$ℓ(β + u) = ℓ(β) + dℓ(β)u + 1 2 d 2 ℓ(β)(u, u) + o(|u| 2 )$and we consider the maximization of the first three terms, simply restricting to vectors u ∈ M 0 . To allow for matrix computation, we use our ordering R Y = (g 1 , . . . , g q ) and identify a with the column vector

$          u(g 1 ) . . . u(g q )           ∈ R q(d+1)$Similarly, we let

$∇ℓ(β) =           ∂ β(g 1 ) ℓ . . . ∂ β(g q ) ℓ          $and let ∇ 2 (ℓ)(β) be the block matrix with i, j block given by ∂ β(g i ) ∂ β(g j ) ℓ(β). We let ρ be the (d + 1) × q(d + 1) row block matrix

$ρ(g 1 )Id R d+1 • • • ρ(g q )Id R d+1 so that u ∈ M 0 is just ρu = 0 in vector notation. Given this we have ℓ(β + u) = ℓ(β) + ∇ℓ(β) T u + 1 2 u T ∇ 2 (ℓ)(β)u + o(|u| 2 ). The maximum of ℓ(β) + u T ∇ℓ(β) + 1 2 u T ∇ 2 (ℓ)(β)u subject to ρu = 0 is a stationary point of the Lagrangian L = ℓ(β) + u T ∇ℓ(β) + 1 2 u T ∇ 2 (ℓ)(β)u + λ T ρu$for some λ ∈ R d+1 and is characterized by

$∇ 2 (ℓ)(β)u + ∇ℓ(β) + ρT λ = 0 ρu = 0$This shows that the Newton-Raphson iterations can be implemented as

$β n+1 = β n -ϵ n+1 u n+1 (8.6) with u n+1 λ = ∇ 2 (ℓ)(β n ) ρT ρ 0 -1 ∇ℓ(β n ) 0 . (8.7)$We summarize this discussion in the following algorithm.

Algorithm 8.1 (Logistic regression with Newton's gradient ascent)

$(1) Input: (i) training data (x 1 , y 1 , . . . , x N , y N ) with x i ∈ R d and y i ∈ R Y ; (ii) coefficients ρ g , g ∈ R Y$with non-zero sum and target value c ∈ R; (iii) algorithm step ϵ small enough.

(2) Initialize the algorithm with β 0 such that g ρ g β 0 (g) = c.

(3) At iteration n, compute ∇ℓ(β n ) and ∇ 2 (ℓ)(β n ) as provided by proposition 8.1.

(4) Update β n using (8.6) and (8.7), with ϵ n+1 = ϵ. Alternatively, optimize ϵ n+1 using a line search.

(5) Stop the procedure if the change in the parameter is below a small tolerance level. Otherwise, return to step 2.

## Penalized Logistic Regression

Logistic regression can be combined with a penalty term, e.g., maximizing

$ℓ 2 (β) = ℓ(β) -λ d i=1 |b (i) | 2 (8.8) or ℓ 1 (β) = ℓ(β) -λ d i=1 |b (i) | (8.9)$where b (i) is the q-dimensional vector formed with the ith coefficients of b(g) for g ∈ R Y . Similarly to penalized regression, one generally normalizes the x variables to have unit standard deviation before applying the method.

Maximization with the ℓ 2 norm The problem in [(8.8)](#) relates to ridge regression and can be solved using a Newton-Raphson method (Algorithm 8.1) with minor changes. More precisely, letting

$∆ = 0 0 0 Id R d we have, considering β as a d + 1 by q matrix, ℓ 2 (β) = ℓ(β) -λtrace(β T ∆β) and dℓ 2 (β)u = dℓ(β)u -2λtrace(β T ∆u), d 2 ℓ 2 (β)(u, u ′ ) = dℓ(β)(u, u ′ ) -2λtrace(u T ∆u ′ ).$In addition, when λ > 0, the problem is over-parametrized only up to the addition of a constant to (g → a 0 (g)), so that one only needs a single constraint g ρ g a 0 (g) = 0 and the Lagrange coefficient in (8.7) is one dimensional.

Maximization with the ℓ 1 norm The maximization in (8.9) can be run using proximal gradient ascent (section 3.5.5), writing the objective function in the form

$ℓ 1 (a 0 , b) = ℓ(a 0 , b) -λγ(a 0 , b) with γ(a 0 , b) = d i=1 g∈R Y b (i) (g) 2 .$Here, ℓ is concave and γ is convex and the proximal gradient iterations are

$β n+1 = prox ϵλγ (β n + ϵ∇ℓ(β n )) (8.10)$where ∇ℓ is the gradient of ℓ projected on the set of functions u :

$R Y → R d+1 satis- fying g∈R Y ρ g u (0) (g) = 0$where u (0) (g) is the first coordinate of u(g). This projection can be computed by subtracting

$g ′ ∈R Y ρ(g ′ )∂ a 0 (g ′ ) ℓ g ′ ∈R Y ρ(g ′ ) 2 ρ(g)$to ∂ a 0 (g) ℓ. This algorithm will converge for small enough ϵ.

We already know the gradient of ℓ, so it only remains to determine the proximal operator of γ to make (8.10) explicit. Let us denote the coordinates of a function u : R Y → R d+1 as u (i) (g) for i = 0, . . . , d and g ∈ R Y .

$prox ϵλγ (u) = argmin ũ       γ( ũ) + 1 2λϵ d i=0 g∈R Y (u (i) (g) -ũ(i) (g)) 2       .$Since γ does not depend on ũ(0) (•), the optimal ũ(0

$) ( ˙) is ũ(0) (•) = u (0) (•). One can optimize separately each group ( ũ(i) (g), g ∈ R Y ), which must minimize g∈R Y ũ(i) (g) 2 + 1 2λϵ g∈R Y (u (i) (g) -ũ(i) (g)) 2 .$The function t → √ t being differentiable everywhere except at 0, we first search for a solution for which at least one ũ(i) (g) does not vanish. If such a solution exists, it must satisfy, for all

$g ∈ R Y ũ(i) (g) g ′ ∈R Y ũ(i) (g ′ ) 2 + 1 λϵ ( ũ(i) (g) -u (i) (g)) = 0 Letting | ũ(i) (•)| = g∈R Y ũ(i) (g) 2 we get ũ(i) (•)(| ũ(i) (•)| + λϵ) = u (i) (•)| ũ(i) (•)|$Taking the norm on both sides and dividing by | ũ(i, •)| (which is assumed not to vanish) yields

$| ũ(i) (•)| + λϵ = |u (i) (•)|,$which has a positive solution only if |u (i) (•)| > λϵ, and gives in that case

$ũ(i) (•) = |u (i) (•)| -ϵλ |u (i) (•)| u (i) (•) If |u (i) (•)| ≤ λϵ, then we must take ũ(i) (•) = 0.$We have therefore obtained:

$prox ϵλg (a) = ũ with ũ(0) (•) = u (0) (•) and ũ(i) (•) = max |u (i) (•)| -ϵλ |u (i) (•)| , 0 u (i) (•)$for i ≥ 1. We summarize this discussion in the next algorithm. (2) Initialize the algorithm with β 0 = (a 00 , b 0 ) such that g ρ g a 00 (g) = c.

(

$) At iteration n, compute u = β n + ϵ∇ℓ(β n ), with β n = (a 0,n , b n ). (4) Let a n+1,0 (•) = u (0) (•) and for i ≥ 1, b (i) n+1 (•) = max |u (i) (•)| -ϵλ |u (i) (•)| , 0 u (i) (•)3$(5) Stop the procedure if the change in the parameter is below a small tolerance level. Otherwise, return to step 2.

## Kernel logistic regression

Let h : R X → H be a feature function with values in a Hilbert space

$H with K(x, x ′ ) = ⟨h(x) , h(x ′ )⟩ H .$The kernel version of logistic regression uses the model:

$log p a 0 ,b (g | x) = a 0 (g) + ⟨h(x) , b(g)⟩ H -log g∈R Y exp(a 0 ( g) + ⟨h(x) , b( g)⟩ H ) with b(g) ∈ H for g ∈ R Y .$Using the usual kernel argument, one sees that, when maximizing the log-likelihood, there is no loss of generality is assuming that each b(g) belongs to V = span(h(x 1 ), . . . , h(x N )). Taking

$b(g) = N k=1 α k (g)h(x k ), we have log p α (g | x) = a 0 (g) + N k=1 α k (g)K(x, x k ) -log         g∈R Y exp(a 0 ( g) + N k=1 α k ( g)K(x, x k ))         .$To avoid overfitting, one must include a penalty term in the likelihood, and (in order to take advantage of the kernel), one can take this term proportional to g ∥b(g)∥ 2

H . The complete learning procedure then requires to maximize the concave penalized likelihood

$ℓ(α) = N k=1 log p α (y k | x k ) -λ g∈R Y N k,l=1 α k (g)α l (g)K(x k , x l ).$The computation of the first and second derivatives of this function is similar to that for the original version, and we skip the details.

## Linear Discriminant analysis 8.2.1 Generative model in classification and LDA

Generative model In classification, the class variable Y generally has a causal role upon which the variable X is produced. Prediction can therefore be seen as an inverse problem where the cause is deduced from the result. In terms of generative modeling, one should therefore model the distribution of Y , followed by the the conditional distribution of X given Y .

Taking R X = R d , denote by f g the conditional p.d.f. of X given Y = g and let π g = P (Y = g). The Bayes estimator for the 0-1 loss maximizes the posterior probability

$P(Y = g | X = x) = π g f g (x) g ′ ∈R Y π g ′ f g ′ (x)$. Since the denominator does not depend on g the Bayes estimator equivalently maximizes (taking logarithms) log f g (x) + log π g .

One generally speaks of a linear classification method when the prediction is based on the maximization in g of a function U (g, x) where U is affine in x. In this sense, logistic regression is linear, and kernel logistic regression is linear in feature space. For the generative approach, this occurs when one uses the following model, which provides the generative form of linear discriminant analysis (LDA). Assume that the distributions f g are all Gaussian with mean m g and common variance S, so that [.11)](#) In this case, the optimal predictor must maximize (in g)

$f g (x) = 1 (2π) d det Σ e -1 2 (x-m g ) T S -1 (x-m g ) . (8$$- 1 2 (x -m g ) T S -1 (x -m g ) + log π g . Introduce m = E(X) = g∈R Y π g m g .$Then the optimal classifier must maximize

$- 1 2 (x -m) T S -1 (x -m) + (x -m) T S -1 (m g -m) - 1 2 (m g -m) T S -1 (m g -m) + log π g .$Since the first term does not depend on g, it is equivalent to maximize

$(x -m) T S -1 (m g -m) - 1 2 (m g -m) T S -1 (m g -m) + log π g (8.12)$with respect to the class g, which provides an affine function of x.

Training Training for LDA simply consists in estimating the class means and common variance in [(8.11)](#) from data. We introduce some notation for this purpose (this notation will be reused through the rest of this chapter).

Recall that N g , g ∈ R Y denotes the number of samples with class g in the training set T = (x 1 , y 1 , . . . , x N , y N ). We let c g = N g /N and C be the diagonal matrix with diagonal coefficients c g 1 , . . . , c g q . We also let ζ ∈ R q denote the vector with the same coordinates. For g ∈ R Y , µ g denotes the class average

$µ g = 1 N g n k=1$x k 1 y k =g and µ the global average

$µ = 1 N N k=1 x k = g∈R Y c g µ g .$Let Σ g denote the sample covariance matrix in class g, defined by

$Σ g = 1 N g N k=1 (x k -µ g )(x k -µ g ) T 1 y k =g ,$and Σ w the pooled class covariance (also called within-class covariance) defined by

$Σ w = 1 N N k=1 (x k -µ y k )(x k -µ y k ) T = g∈R Y c g Σ g .$Let, in addition, Σ b denotes the "between-class" covariance matrix, given by

$Σ b = g∈R Y c g (µ g -µ)(µ g -µ) T$The global covariance matrix, given by,

$Σ XX = 1 N N k=1 (x k -µ)(x k -µ) T satisfies Σ XX = Σ w + Σ b . This identity is proved by noting that, for any g ∈ R Y , 1 N g N k=1 (x k -µ)(x k -µ) T 1 y k =g = Σ g + (µ g -µ)(µ g -µ) T .$We will finally denote by M the matrix

$M =            (µ g 1 -µ) T . . . (µ g q -µ) T            . Note that Σ b = M T CM.$Given this notation, one can in particular take mg = µ g , m = µ and Ŝ = Σ w in (8.12). The class probabilities, π g , can be deduced from the normalized frequencies of y 1 , . . . , y N . However, in many applications, one prefers to simply fix π g = 1/q, in order to balance the importance of each class. Remark 8.5 If one relaxes the assumption of common class variances, one needs to use Σ g in place of Σ w for class g. The decision boundaries are not linear in this case, but provided by quadratic equations (and the resulting method if often called quadratic discriminant analysis, or QDA). QDA requires the estimation of qd(d + 3)/2 coefficients, which may be overly ambitious when the sample size is not large compared to the dimension, in which case QDA is prone to overfitting. (Even LDA, which involves qd +d(d +1)/2 parameters, may be unrealistic in some cases.) We also note a variant of QDA that uses class covariance matrices given by Σg = αΣ w + (1α)Σ g .

## Dimension reduction

One of the interests of LDA is that it can be combined with a rank reduction procedure. LDA with q classes can always be seen as a (q -1)-dimensional problem after suitable projection on a data-dependent affine space. Recall that the classification rule after training requires to maximize w.r.t. g ∈ R Y the function

$(x -µ) T Σ -1 w (µ g -µ) - 1 2 (µ g -µ) T Σ -1 w (µ g -µ) + log π g . Define the "spherized" data 2 by xk = Σ -1/2 w (x k -µ), where Σ 1/2 w is the positive sym- metric square root of Σ w . Also let μg = Σ -1/2 w (µ g -µ).$With this notation, the predictor chooses the class g that maximizes

$xT μg - 1 2 | μg | 2 + log π g with x = Σ -1/2 w (x -μ). Now, let V = span{ μg , g ∈ R Y }.$Since g c g μg = 0, this space is at most (q -1)dimensional. Let P V denote the orthogonal projection on V . We have xT z = (P V x) T z for any z ∈ V and x ∈ R d .

The classification rule can then be replaced by maximizing

$(P V x) T μg - 1 2 | μg | 2 + log π g with x = Σ -1/2 w (x -μ). Recall that M =            (µ g 1 -µ) T . . . (µ g q -µ) T            and let M =            μT g 1 . . . μT g q           $. The dimension, denoted r, of V is equal to the rank of M. Let ( ẽ1 , . . . , ẽr ) be an orthonormal basis of V . One has

$P V x = r j=1 ( xT ẽj ) ẽj .$Given an input x, one must therefore compute the "scores" γ j (x) = xT ẽj and maximize

$r j=1 γ j (x)γ j (µ g ) - 1 2 r j=1 γ j (µ g ) 2 + log π g .$The following proposition is key to the practical implementation of LDA with dimension reduction.

## Proposition 8.6 An orthonormal basis of

$V = span( μg , g ∈ CG) is provided by the the first r eigenvectors of M T C M associated with eigenvalues λ 1 ≥ • • • ≥ λ r > 0 (all other eigenvalues being zero). Proof Indeed, if x is perpendicular to V , we have MT C M x = g∈R Y c g ( μT g x) μg = 0 so that V ⊥ ⊂ Null( MT C M)$, and both spaces coincide because they have the same dimension (dr). This shows that

$V = Null( M T C M) ⊥ = Range( MT C M). Since M T C M is symmetric, Null( M T C M)$⊥ is generated by eigenvectors with non-zero eigenvalues.

## ■

Returning to the original variables, we have M = MΣ -1/2 w and M T CM = Σ b , the between class covariance matrix. This implies that

$M T C M = Σ -1/2 w Σ b Σ -1/2$w and each eigenvector ẽj therefore satisfies

$Σ b Σ -1/2 w ẽj = λ j Σ 1/2 w ẽj = λ j Σ w (Σ -1/2 w ẽj ).$Therefore, letting e j = Σ -1/2 w ẽj , (e 1 , . . . , e r ) are the solutions of the generalized eigenvalue problem Σ b e = λΣ w e that are associated with non-zero eigenvalues (they are however normalized so that e T j Σ w e j = 1). Moreover, the scores are given by

$γ j (x) = xT ẽj = (x -µ) T Σ -1/2 w ẽj = (x -µ) T e j$and can therefore be computed directly from the original data and the vectors e 1 , . . . , e r . An example of training data and its representation in the LDA space (associated with the scores) in provided in fig. [8](#fig_135).1.

We can now describe the LDA learning algorithm with dimension reduction.

$Algorithm 8.3 (LDA with dimension reduction) 1. Compute µ g , g ∈ R Y , Σ w and Σ b from training data. 2. Estimate (if needed) π g , g ∈ R Y$3. Solve the generalized eigenvalue problem Σ b e = λΣ w e. Let e 1 , . . . , e r be the eigenvectors associated with non-zero eigenvalues, normalized so that e T j Σ w e j = 1. 4. Choose a reduced dimension r 0 ≤ r.

## Precompute mean scores

$γ j (µ g ) = (µ g -µ) T e j , g ∈ R Y , j = 1, . . . , r 0 .$6. To classify a new example x, compute γ j (x) = (xµ) T e j and choose the class that maximizes

$r 0 j=1 γ j (x)γ j (µ g ) - 1 2 r 0 j=1 γ j (µ g ) 2 + log π g .$
## Fisher's LDA

This characterization leads to the discriminative interpretation of LDA, also called Fisher's LDA. Indeed, the generalized eigenvalue problem Σ b e = λΣ w e is directly related to the maximization of the ratio e T Σ b e subject to e T Σ w e = 1, which provides directions that have a large between-class variance for within class variance equal to 1. More precisely, e 1 is the direction that achieves the maximum; e 2 is the second best direction, constrained to being perpendicular to e 1 , and so on until e r which is the optimal constrained to be perpendicular to (e 1 , . . . , e r-1 ). We are therefore looking for directions that have the largest ratio of between-class variance to within-class variance.

## Kernel LDA

Mean and covariance in feature space We assume the usual construction where h :

$R X → H is a feature function, H a Hilbert space with kernel K(x, x ′ ) = ⟨h(x) , h(x ′ )⟩ H . ($The assumption that H is a complete space is here required for the expectations below to be meaningful.)

We now discuss the kernel version of LDA by plugging the feature space representation directly in the classification rule. So, consider h : R → H. Let X : Ω → R be a random variable such that E(∥h(X)∥ 2 H ) < ∞. Then, its mean feature m = E(h(X)) is well defined as an element of H , and so are the class averages,

$m g = E(h(X) | Y = g).$In this possibly infinite-dimensional setting, the covariance "matrix" is defined as a linear operator S : H → H such that, for all ξ, η ∈ H: [(8.13)](#) which is equivalent to defining

$⟨ξ , Sη⟩ H = E ⟨h(X) -m , ξ⟩ H ⟨h(X) -m , η⟩ H ,$$Sη = E(⟨h(X) -m , η⟩ H (h(X) -m))$for η ∈ H. This definition generalizes the identity for a random variable U : Ω → R d :

$S U w = E((U -E(U ))(U -E(U )) T )w = E(((U -E(U )) T w) (U -E(U )))$One can similarly define the covariance matrix in class g, S g , by conditioning the right-hand side in [(8.13](#)) by Y = g and replacing m by m g .

## LDA in feature space

Following the LDA model, we assume that the operators S g are all equal to a fixed operator, the within-class covariance operator denoted S.

Assuming that S is invertible, one can generalize the LDA classification rule to data represented in feature space by classifying a new input x in class g when

$⟨h(x) -m , S -1 (m g -m)⟩ H - 1 2 ⟨m g -m , S -1 (m g -m)⟩ H + log π g (8.14)$is maximal over all classes. Notice that this is a transcription of the finite-dimensional Bayes rule, but cannot be derived from a generative model, because the assumption that h(X) is Gaussian is not valid in general. (It would require that h takes values in a d-dimensional linear space, which would eliminate all interesting kernel representations.)

Let, as before, T = (x 1 , y 1 , . . . , x N , y N ) be the training set, N g denote the number of examples in class g and c g = N g /N . When h is known (which, we recall, is not a practical assumption, but we will fix this later), one can estimate the class averages from training data by

$µ g = 1 N g N k=1 h(x k )1 y k =g$and the within-class covariance operator by

$⟨ξ , Σ w η⟩ H = 1 N N k=1 ⟨h(x k ) -µ y k , ξ⟩ H ⟨h(x k ) -µ y k , η⟩ H .$Unfortunately, the resulting variance estimator cannot be directly used in [(8.14)](#), because it is not invertible if dim(H) > N . Indeed, one has Σ w η = 0 as soon as η is perpendicular to V ∆ = span(h(x 1 ), . . . , h(x N )).

One way to address the degeneracy of the estimated covariance operator is to add to Σ w a small multiple of the identity, say ρId H , [3](#foot_10) and let the classification rule maximize in g:

$⟨h(x) -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H - 1 2 ⟨µ g -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H + log π g . (8.15)$where µ is the average of h(x 1 ), . . . , h(x N ). Taking this option, we still need to make this expression computable and remove the dependency in the feature function h.

Reduction We have µ g ∈ V for all g ∈ R Y and, since

$Σ w η = 1 N N k=1 ⟨h(x k ) -µ y k , η⟩ H (h(x k ) -µ y k ),$this operator maps H to V , which implies that Σ w + ρId H maps V into itself. Moreover, this mapping is onto: If v ∈ V and u = (Σ w + ρId H ) -1 v, then, u ∈ V . Indeed, for any z ⊥ V , we have ⟨z , Σ w u + ρu⟩ H = ⟨z , v⟩ H . We have ⟨z , Σ w u⟩ H = 0 (because Σ w maps H to V ) and ⟨z , v⟩ H = 0 (because v ∈ V ), so that we can conclude that ⟨z , u⟩ H = 0. Since this is true for all z ⊥ V , this requires that u ∈ V . 4   We now express the classification rule in (8.15) as a function of the kernel associated with the feature-space representation. Denote, for any vector u ∈ R N ,

$ξ(u) = N k=1 u (k) h(x k ), therefore defining a mapping ξ from R N onto V . Letting as usual K = K(x 1 , . . . , x N )$be the matrix formed by pairwise evaluations of K on training inputs, we have the identity

$⟨ξ(u) , ξ(u ′ )⟩ H = u T Ku ′ .$for all u, u ′ ∈ R N . For simplicity, we will assume in the rest of the discussion that K is invertible.

We have µ g = ξ(1 g /N g ), where 1 g ∈ R N is the vector with kth coordinate equal to 1 if y k = g and 0 otherwise. Also µ = ξ(1/N ) (recall that 1 is the vector with all coordinates equal to 1).

For u ∈ R N , we want to characterize v ∈ R N such that Σ w ξ(u) = ξ(v). Let δ k denote the vector with 1 at the kth entry and 0 otherwise. We have

$Σ w ξ(u) = 1 N N k=1 ⟨ξ(u) , h(x k ) -µ y k ⟩ H (h(x k ) -µ y k ) = 1 N N k=1 ⟨ξ(u) , ξ(δ k -1 y k /N y k )⟩ H ξ(δ k -1 y k /N y k ) = 1 N N k=1 ((δ k -1 y k /N y k ) T Ku) ξ(δ k -1 y k /N y k ) = ξ        1 N N k=1 ((δ k -1 y k /N y k ) T Ku) (δ k -1 y k /N y k )        so that Σ w ξ(u) = ξ(P Ku) with P = 1 N N k=1 (δ k -1 y k /N y k )(δ k -1 y k /N y k ) T 4 One has (V ⊥ ) ⊥ = V for finite-dimensional-or more generally closed-subspaces of H$Note that one has

$• N k=1 δ k δ T k = Id R N , • N k=1 1 y k N y k δ T k = g∈R Y 1 g N g k:y k =g δ T k = g∈R Y 1 g 1 T g N g = N k=1 δ k 1 y k N y k T , • N k=1 1 y k N y k 1 y k N y k T = g∈R Y 1 g 1 T g N g .$This shows that P can be expressed as

$P = 1 N         Id R N - g∈R Y 1 g 1 T g /N g         .$We have therefore proved that

$• (Σ w + ρId H )ξ(u) = ξ ((P K + ρId R N )u) • (Σ w + ρId H ) -1 ξ( ũ) = ξ (P K + ρId R N ) -1 ũ .$Recall that the feature-space LDA classification rule maximizes

$⟨h(x) -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H - 1 2 ⟨µ g -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H + log π g .$All terms belong to V , except h(x), but this term can be replaced by its orthogonal projection on V without changing the result. This projection can be made explicit in terms of the representation ξ as follows. For x ∈ R, let ξ(ψ(x)) denote the orthogonal projection of h(x) on V (this defines the function ψ). If v(x) denotes the vector with coordinates K(x, x k ), k = 1, . . . , N , then ψ(x) = K -1 v(x), as can be obtained by identifying the inner products ⟨h(x) , h(x k )⟩ H and ⟨ξ(ψ(x)) , h(x k )⟩ H .

We are now ready to rewrite the kernel LDA classification rule in terms of quantities that only involve K. We have

$⟨h(x) -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H = ⟨ξ(ψ(x) -1/N ) , ξ((P K + ρId R N ) -1 (1 g /N g -1/N ))⟩ H = (ψ(x) -1/N ) T K(P K + ρId R N ) -1 (1 g /N g -1/N ) Given this, the classification rule must maximize (ψ(x) -1/N ) T K(P K + ρId R N ) -1 (1 g /N g -1/N ) - 1 2 (1 g /N g -1/N ) T K(P K + ρId R N ) -1 (1 g /N g -1/N ) + log π g . (8.16) Dimension reduction Note that K(P K + ρId R N ) -1 = K(KP K + ρK) -1 K is a symmet- ric matrix.$So, the expression in [(8.16](#)) can be written as

$(v(x) -ν) T R -1 (ν g -ν) - 1 2 (ν g -ν) T R -1 (ν g -ν) + log π g .$with R = KP K + ρK, ν g = K1 g /N g and ν = K1/N . Clearly, if v 1 , . . . , v N are the column vectors K, we have

$ν g = 1 N g N k=1 v k 1 y k =g , ν = 1 N N k=1 v k .$We therefore retrieve an expression similar to finite-dimensional LDA, provided that one replaces x by v(x), x k by v k and Σ w by R. Letting

$Q = 1 N g∈R Y N g (ν g -ν)(ν g -ν) T$be the between-class covariance matrix, the discriminant directions are therefore solutions of the generalized eigenvalue problem

$Qf j = λ j Rf j with f T j Rf j = 1 with R = (KP K + ρK). Note that KP K = 1 N N k=1 (v k -νy k )(v k -νy k ) T$is the within-class covariance matrix for the training data (v 1 , y 1 , . . . , v N , y N ).

The following summarizes the kernel LDA classification algorithm.

## Algorithm 8.4 (Kernel LDA)

(1) Select a positive kernel K and a coefficient ρ > 0.

(2) Given T = (x 1 , y 1 , . . . , x N , y N ) and a kernel K, compute the kernel matrix K = K(x 1 , . . . , x N ) and the matrix R = KP K + ρK. Let v 1 , . . . , v N be the column vectors of K.

(3) Compute, for g ∈ R Y ,

$ν g = 1 N g N k=1 v k 1 y k =g , ν = 1 N N k=1 v k and let Q = 1 N g∈R Y N g (ν g -ν)(ν g -ν) T .$(4) Fix r 0 ≤ q-1 and compute the eigenvectors f 1 , . . . , f r 0 associated with the r 0 largest eigenvalues for the generalized eigenvalue problem Qf = λRf , normalized such that f T j Rf j = 1.

(5) Compute the scores γ jg = (ν g -ν) T f j .

(6) Given a new observation x, let v(x) be the vector with coordinates K(x, x k ), k = 1, . . . , N . Compute the scores γ j (x) = (v(x) -ν) T f j , j = 1, . . . , r 0 . Classify x in the class g maximizing

$r i=1 γ i (x)γ ig - 1 2 r i=1 γ 2 ig + log π g . (8$.17)

## Optimal Scoring

It is possible to apply linear regression (chapter 7) to solve a classification problem by mapping the set R Y to a collection of r-dimensional row vectors, or "scores."

These scores (which have a different meaning from the LDA scores) will be represented by a function θ : R Y → R r . As an example, one can take r = q and θ(g

$1 ) =                    1 0 0 . . . 0                    , θ(g 2 ) =                    0 1 0 . . . 0                    , . . . , θ(g q ) =                    0 0 . . . 0 1                    .$Given a training set T = (x 1 , y 1 , . . . , x N , y N ) and a score function θ, a linear model can then be estimated from data by minimizing

$N k=1 |θ y k -a 0 -b T x k | 2$where b is a d ×q matrix and a 0 ∈ R q . Letting as before β be the matrix with a T 0 added as first row to b and X the matrix with first row containing only ones and subsequent rows given by x T 1 , . . . , x T N , one gets the least square estimator β = (X T X ) -1 X T Y , where Y is the N × q matrix of stacked θ T y k row vectors.

Given an input vector x, the row vector xT β will generally not coincide with one of the score vectors. Assignment to a class can then be made by minimizing

$|a 0 + b T x -θ g | over all g in R Y .$Since the scores θ are free to choose, one may also try to optimize them, resulting in the optimal scoring algorithm. To describe it, we will need the notation already introduced for LDA, plus the following. We will write, for short, θ j = θ(g j ) and

$introduce the q × r matrix Θ =           θ T 1 . . . θ T q          $. We also denote by ρ 1 , . . . , ρ r the column vectors of Θ, so that Θ = [ρ 1 , . . . , ρ r ]. Let u g i , for i = 1, . . . , q, denote the q-dimensional vector with ith coordinate equal to 1 and all others equal to 0. As before, N g denote the class sizes, c g = N g /N , C is the diagonal matrix with coefficients c g 1 , . . . , c g q and ζ =

$          c g 1 . . . c g q          $.

The goal of optimal scoring is to minimize, now with respect to θ, a 0 and b, the function

$F(θ, a 0 , b) = N k=1 |θ(y k ) -a 0 -b T x k | 2 .$Some normalizing conditions are clearly needed, because this problem is underconstrained. (In the form above, the optimal choice is to take all free parameters equal to 0.) We now discuss the various indeterminacies and redundancies in the model, (a) If R is an r × r orthogonal matrix, then F(Rθ, Ra 0 , bR T ) = F(θ, a 0 , b), yielding an infinity of possible equivalent solutions (that all lead to the same classification rule). This implies that there is no loss of generality in assuming that Θ T CΘ is diagonal (introducing C here will turn out to be convenient). Indeed, given any (θ, a 0 , b), one can just take R such that RΘ T CΘR T is diagonal and replace Θ by RΘ, a 0 by Ra 0 and b by bR T to get an equivalent solution satisfying the constraint.

(b) Let D be an r by r diagonal matrix with positive entries. Replace θ, a 0 and b respectively by Dθ, Da 0 and bD. The resulting objective function is

$F(Dθ, Da 0 , bD T ) = N k=1 |Dθ(y k ) -Da 0 -Db T x k | 2 = r j=1 N k=1 d 2 jj θ(y k , j) -a 0 (j) - d i=1 b(i, j)x k (i) 2$If the coefficient d jj is free to chose, then the objective function can always be reduced by letting d jj → 0, which removes one of the dimensions in θ. In order to avoid this, one needs to fix the diagonal values of Θ T CΘ, and, by symmetry, it is natural to require

$Θ T CΘ = Id R r . (c) Given any δ ∈ R r , one has F(θ, a 0 , b) = F(θ -δ, a 0 + δ, b$), with identical classification rule. One can therefore without loss of generality introduce r linear constraints, and a convenient choice is

$Θ T ζ = g∈R Y c g θ g = 0.$Given this reduction, we can now describe the optimal scoring problem as the minimization of

$N k=1 |θ y k -a 0 -b T x k | 2 subject to Θ T CΘ = Id R r and Θ T ζ = 0.$The optimal a 0 is given by â0

$= 1 N N k=1 θ y k -b T µ = -b T µ,$so that the problem is reduced to minimizing

$N k=1 |θ y k -b T (x k -µ)| 2$subject to the same constraints. Using the facts that

$θ y k = Θ T u y k , that N k=1 u y k u T y k = g∈R Y N g u g u T g = N C$and that

$N k=1 u y k (x k -µ) T = N g∈R Y u g k:y k =g (x k -µ) T = N g∈R Y u g N g (µ g -µ) T = N CM, one can write N k=1 |θ y k -b T (x k -µ)| 2 = N k=1 |Θ T u y k -b T (x k -µ)| 2 = N k=1 u T y k ΘΘ T u y k -2 N k=1 (x k -µ) T bΘ T u y k + N k=1 (x k -µ) T bb T (x k -µ) = N k=1 trace(Θ T u y k u T y k Θ) -2 N k=1 trace(Θ T u y k (x k -µ) T b) + N k=1 trace(b T (x k -µ)(x k -µ) T b) = N trace(Θ T CΘ) -2N trace(Θ T CMb) + N trace(b T Σ XX b) .$Note that, since Θ T CΘ = Id R r , then trace(Θ T CΘ) = r. We therefore obtain a concise form of the optimal scoring problem: minimize

$-2trace(Θ T CMb) + trace(b T Σ XX b). subject to Θ T CΘ = Id R r and Θ T ζ = 0.$Given Θ, the optimal b is Σ -1 XX M T CΘ, and replacing it in the objective function, one finds that Θ must minimize

$-2trace(Θ T CMΣ -1 XX M T CΘ) + trace(Θ T CMΣ -1 XX M T CΘ)$i.e., maximize trace(Θ T CMΣ -1 XX M T CΘ) subject to Θ T CΘ = Id R r and Θ T ζ = 0. We now recall the following linear algebra result (see chapter 2). Proposition 8.7 Let A and B be respectively positive definite and non-negative semidefinite symmetric q by q matrices. Then, the maximum, over all q by r matrices S such that trace(S T AS) = Id R r , of trace(S T BS) is attained at S = [σ 1 , . . . , σ r ], where the columns vectors σ 1 , . . . , σ r are the solutions of the generalized eigenvalue problem Bσ = λAσ associated with the largest eigenvalues, normalized so that σ T i Aσ i = 1 for i = 1, . . . , r..

Given this proposition, let ρ 1 , . . . , ρ r be the r first eigenvectors for the problem

$CMΣ -1 XX M T Cρ = λCρ. (8.18)$Assume that r is small enough so that the associated eigenvalues are not zero. Let Θ = [ρ 1 , . . . , ρ r ]. We now prove that Θ is indeed a solution of the optimal scoring problem, and the only point to show to complete the statement is that this Θ satisfies the constraints Θ T ζ = 0. But we have

$M T C1 q = g c g (µ g -μ) = 0,$which implies that 1 q is a solution of the generalized eigenvalue problem associated with λ = 0. This in turn implies that 1

$T q Cρ i = ζ T ρ i = 0, which is exactly Θ T ζ = 0.$To summarize, we have found that the solution θ, b minimizing

$-2trace(Θ T CMb) + trace(b T Σ XX b) subject to Θ T CΘ = Id R r and Θ T ζ = 0 is given by (i) Θ = [ρ 1 , . . . , ρ r ]$where ρ 1 , . . . , ρ r are the eigenvectors for the problem

$CMΣ -1 XX M T Cρ = λCρ$associated with the r largest eigenvalues, normalized so that ρ T Cρ = 1.

$(ii) b = Σ -1 XX M T CΘ.$The computation can, however, be further simplified. Let λ 1 , . . . , λ r be the eigenvalues associated with ρ 1 , . . . , ρ r . Letting D be the associated diagonal matrix, one can write

$CMΣ -1 XX M T CΘ = CΘD. This yields Θ = MΣ -1 XX M T CΘD -1 = MbD -1 , from which we deduce that θ g = Θ T u g = D -1 b T (µ g -μ)$. So, given a new input vector x, the decision rule is to assign it to the class g for which

$|θ g -b T (x -μ)| 2 = |Θ T u g -b T (x -μ)| 2 = |D -1 b T (µ g -μ) -b T (x -μ)| 2 is minimal. Letting b 1 , . . . , b r denote the r columns of b, this is equivalent to mini- mizing, in g r j=1 (b T j (µ g -μ)) 2 /λ 2 j -2 r j=1 (b T j (x -μ))(b T j (µ g -μ))/λ j . (8.19) From b = Σ -1 XX M T CΘ and Θ = MbD -1 we see that bD = Σ -1 XX M T CMb, so that Σ b b = Σ XX bD. This shows that the columns of b are solution of the eigenvalue problem Σ b u = λΣ XX u. Moreover, from Θ T CΘ = Id R r , we get b T Σ b b = D 2 . Since b T Σ b b = b T Σ XX bD, we get that b must be normalized to that b T Σ XX b = D.$This shows that the solution of the optimal scoring problem can be reformulated uniquely in terms of b: if b 1 , . . . , b r are the r principal solutions of the eigenvalue problem

$Σ b u = λΣ XX u, normalized so that u T Σ XX u = λ, a new input is classified into the class g minimizing r j=1 γ j (µ g ) 2 /λ 2 j -2 r j=1 γ j (x)γ j (µ g )/λ j . with γ j (x) = b T j (x -μ).$Remark 8.8 The following computation shows that optimal scoring is closely related to LDA. Recall the identity

$Σ XX = Σ w + Σ b . It implies that a solution of Σ b u = λΣ XX u is also a solution of Σ b u = λΣ w u with λ = λ/(1 -λ). If u T Σ XX u = λ, then u T Σ w u = λ -u T Σ b u = λ -λ 2 = λ (1 + λ) 2 , which shows that ũ = 1 + λ √ λ u satisfies ũT Σ w ũ = 1.$So,

$e j = 1 + λj λj b j$coincide with the LDA directions. We have, letting γj (x) = e T j (x -μ) = λj γ j (x)/(1 + λj ):

$r j=1 γ j (µ g ) 2 /λ 2 j -2 r j=1 γ j (x)γ j (µ g )/λ j = r j=1 γj (µ g ) 2 / λj -2 r j=1 γ j (x)γ j (µ g )/(1 + λj )$which relates the classification rules for the two methods. ♦ Remark 8.9 Optimal scoring can be modified by adding a penalty in the form

$γ r i=1 b T i Ωb i = γtrace(b T Ωb) (8.20)$where Ω is a weight matrix. This only modifies the previous discussion by adding γΩ/N to both Σ XX and Σ w . ♦

## Kernel optimal scoring

Let h : R X → H be the feature function and K the associated kernel, as usual. Optimal scoring in feature space requires to minimize

$N k=1 |θ y k -a 0 -b(h(x k ))| 2 + γ∥b∥ 2 H ,$where we have introduced a penalty on b. Here, b is a linear operator from H to R r , therefore taking the form

$b(h) =           ⟨b 1 , h⟩ H . . . ⟨b r , h⟩ H          $with b 1 , . . . , b r ∈ H, and we take

$∥b∥ 2 H = r i=1 ∥b i ∥ 2 H .$It is once again clear (and the argument is left to the reader) that the problem can be reduced to the finite dimensional space V = span(h(x 1 ), . . . , h(x N )), and that the optimal b 1 , . . . , b r must take the form

$b j = N l=1 α li h(x l ) .$Introduce the kernel matrix K = K(x 1 , . . . , x N ) with kth column denoted K (k) . Let α be the N by r matrix with entries α kj , k = 1, . . . , N , j = 1, . . . , r. Then b(h(x k )), which is the vector with coordinates

$⟨b j , h(x k )⟩ = N l=1 α li K(x k , x l ), is equal to α T K (k) . Moreover ∥b∥ 2 H = r j=1 N k,l=1 α kj K(x k , x l )α lj = trace(α T Kα).$We therefore need to minimize

$N k=1 |θ y k -a 0 -α T K (k) | 2 + γtrace(α T Kα),$so that the problem is reduced to penalized optimal scoring, with x k replaced by K (k) , b replaced by α and the matrix Ω in (8.20) replaced by K. Introducing the matrix P = Id R N -11 T /N and K c = P K, the covariance matrix

$Σ XX becomes K T c K c /N = KP K/N .$The class averages µ g are equal to K1(g)/N g while µ = K1/N , so that the matrix

$M is equal to            1(g 1 ) T /N g 1 -1 T /N . . . 1(g q ) T /N g q -1 T /N            K which gives Σ b = M T CM = KQK, where Q is Q = P CP = g∈R Y N g N 1(g) N g - 1 N 1(g) N g - 1 N T$So, the columns of α are the r principal eigenvectors ρ 1 , . . . , ρ r of the problem

$KQKρ = 1 N (KP K + γK)ρ.$Given α, one then has, for any x ∈ R d ,

$⟨b i , h(x)⟩ H = N k=1 α ki K(x, x k ) and a 0 (i) = 1 N N k,l=1 α ki K(x l , x k ).$
## Separating hyperplanes and SVMs

## One-layer perceptron and margin

In this whole section, we restrict to two-class problems, and let

$R Y = {-1, 1}. Given a 0 ∈ R and b 0 ∈ R d , the equation a 0 + b T x = 0 defines a hyperplane in R d . The function f (x) = sign(a 0 + x T b$) defines a classifier that attributes a class ±1 to x according to which side of the hyperplane it belongs (we ignore the ambiguity when x is on the hyperplane). With this notation, a pair (x, y), where y is the true class, is correctly classified if and only if y(a 0 + x T b) > 0.

Let T = (x 1 , y 1 ), . . . , (x N , y N ) denote, as usual, the training data. A hyperplane, represented by the parameters (a 0 , b) is separating for T if it correctly classifies all its samples, i.e., if y k (a 0 + x T k b) > 0 for k = 1, . . . , N . If such a hyperplane exists, one says that T is linearly separable.

The perceptron algorithm computes a 0 and b by minimizing

$L(β) = N k=1 [y k (a 0 + x T k b)] -$with u -= max(0, -u), , or more precisely, fixing a small positive number δ:

$L(β) = N k=1 [δ -y k (a 0 + x T k b)] + .$The problem can be recast as a linear program, i.e., minimize

$N k=1 ξ k subject to ξ k ≥ 0, ξ k + y k (a 0 + x T k b) -δ ≥ 0 for k = 1, . . . , N .$However, when T is linearly separable, separating hyperplanes are not uniquely defined, and there is in general (depending on the choice made for δ) an infinity of solutions to the perceptron problem. Intuitively, one should prefers a solution that classifies the training data with some large margin, rather than one for which training points may be very close to the separating boundary (see fig.  This leads to the maximum margin separating hyperplane classifier, also called linear SVM, introduced by Vapnik and Chervonenkis [[196,](#b214)[197]](#b215).

## Maximizing the margin

We will use the following result.

$Proposition 8.10 The distance of a point x ∈ R d to the hyperplane M : a 0 + b T x = 0 is given by |a 0 + x T b|/|b|. Proof By definition, distance(x, M) = |x -π M (x)| where π M is the orthogonal projec- tion on M. Since b is normal to M and letting h = π M (x), we have x = λb + h so that |λb| = distance(x, M). Writing a 0 + b T h = 0 in this equation implies a 0 + b T x = λ|b| 2 so that |λ| |b| = |a 0 + x T b|/|b|.$■ Assume that T is linearly separable and let M : a 0 + b T x = 0 be a separating hyperplane. The classification margin is defined as the minimal distance of the input vectors x 1 , . . . , x N to this hyperplane, i.e.,

$m(a 0 , b) = min{|a 0 + x T k b|/|b| : k = 1, . . . , N }.$Because the hyperplane is separating, we have y k (a 0 + x T k b) = |a 0 + x T k b| for all k, so that we also have

$m(a 0 , b) = min{y k (a 0 + x T k b)/|b| : k = 1, . . . , N }.$We want to maximize this margin among all separating hyperplanes. This can be expressed as maximizing, with respect to a 0 , b, the quantity

$min{y k (a 0 + x T k b)/|b| : k = 1, . . . , N } subject to the constraint that the hyperplane is separating, namely y k (a 0 + x T k b) ≥ 0, k = 1, . . . , N .$Introducing a new variable C representing the margin, the previous problem is equivalent to maximizing C subject to

$y k (a 0 + x T k b) ≥ C|b|, k = 1, . . . , N .$The problem is now overparametrized, and there is no loss of generality in enforcing the additional constraint C|b| = 1. Noting that maximizing C is the same as minimizing |b| 2 , we can now reformulate the maximum margin hyperplane problem as minimizing |b| 2 /2 subject to

$y k (a 0 + x T k b) ≥ 1, k = 1, .$. . , N , with C (the margin) given by C = 1/|b|. This results in a quadratic programming problem.

If the data is not separable, there is no feasible point for this problem. To also account for this situation (which is common), we can replace the constraint by a penalty and minimize, with respect to a 0 and b:

$|b| 2 2 + γ N k=1 (1 -y k (a 0 + x T k b)) +$for some γ > 0. (Recall that x + = max(x, 0).) This is equivalent to minimizing the perceptron objective function, with δ = 1, and with an additional penalty term equal to |b| 2 /(2γ). This minimization problem is equivalent to a quadratic programming problem obtained by introducing slack variables ξ k , k = 1, . . . , N and minimizing

$1 2 |b| 2 + γ N k=1 ξ k , subject to the constraints ξ k ≥ 0, y k (a 0 + x T k b) + ξ k ≥ 1, for k = 1, . . . , N .$
## KKT conditions and dual problem

Introduce Lagrange multipliers η k ≥ 0 for ξ k ≥ 0 and

$α k ≥ 0 for y k (a 0 + x T k b) + ξ k ≥ 1. The Lagrangian is then given by L = 1 2 |b| 2 + γ N k=1 ξ k - N k=1 η k ξ k - N k=1 α k y k (a 0 + x T k b) + ξ k -1 .$The KKT conditions are

$                                     b - N k=1 α k y k x k = 0 N k=1 α k y k = 0 γ -η k -α k = 0, k = 1, . . . , N ξ k η k = 0, k = 1, . . . , N α k y k (a 0 + x T k b) + ξ k -1 = 0, k = 1, . . . , N(8.21)$Minimizing L with respect to a 0 , b and ξ 1 , . . . , ξ N and ensuring that the minimum is finite provides the first three KKT conditions. The resulting dual formulation therefore requires to maximize

$N k=1 α k - 1 2 N k,l=1 α k α l y k y l x T k x l subject to the constraints 0 ≤ α k ≤ γ, N k=1 α k y k = 0.$We now discuss the consequences of the complementary slackness conditions based on the position of training sample relative to the separating hyperplane.

(i) First consider indices k such that (x k , y k ) is correctly classified beyond the margin, i.e., y k (a 0 + x T k b) > 1. The last KKT condition and the constraint ξ k ≥ 0 require α k = 0, and the third one then gives ξ k = 0.

(ii) For samples that are misclassified or correctly classified below the margin [5](#foot_11) , i,e.,

$y k (a 0 + x T k b) < 1, the constraint y k (a 0 + x T k b) + ξ k ≥ 1 implies ξ k > 0, so that α k = γ and y k (a 0 + x T k b) + ξ k = 1. (iii) If (x k , y k$) is correctly classified exactly at the margin, then ξ k = 0 and there is no constrain on α k beside belonging to [0, γ]. Training samples that lie exactly at the margin are called support vectors.

Given a solution α 1 , . . . , α N of the dual problem, one immediately recovers b via the first equation in [(8.21)](#). For a 0 , one must, similarly to the regression case, rely on support vectors, which can be identified when 0 < α k < γ. In this case, one can take

$a 0 = y k -x T k b.$If no support vector is found, then a 0 is not uniquely determined, and can be any value such that

$y k (a 0 + b T x k ) ≥ 1 if α k = 0 and y k (a 0 + b T x k ) ≤ 1 if α k = γ. This shows that a 0 can be any point in the interval [β - 0 , β + 0 ] with a 0 -= max{y k -x T k b : (y k = 1 and α k = 0) or (y k = -1 and α k = γ)} a 0 + = min{y k -x T k b : (y k = -1 and α k = 0) or (y k = 1 and α k = γ)}.$
## Kernel version

We make the usual assumptions: h : R X → H is a feature map with values in an inner-product space with K(x, y) = ⟨h(x) , h(y)⟩ H . The predictors take the form f (x) = sign(a 0 + ⟨b , h(x)⟩ H ), a 0 ∈ R and b ∈ H, and the goal is to minimize

$1 2 ∥b∥ 2 H + γ N k=1 ξ k , subject to ξ k ≥ 0, y k (a 0 + ⟨h(x k ) , b⟩ H ) + ξ k ≥ 1, k = 1, . . . , N . Let V = span(h(x 1 ), . . . , h(x N )).$The usual projection argument implies that the optimal b must belong to V and therefore take the form

$b = N k=1 u k h(x k ).$We therefore need to minimize

$1 2 N k,l=1 u k u l K(x k , x l ) + γ N k=1 ξ k , subject to y k a 0 + N l=1 K(x k , x l )a l + ξ k ≥ 1 for k = 1, . . . , N .$Introducing the same Lagrange multipliers as before, the Lagrangian is

$L = 1 2 N k,l=1 u k u l K(x k , x l ) + γ N k=1 ξ k - N k=1 η k ξ k - N k=1 α k y k a 0 + N l=1 K(x k , x l )u l + ξ k -1 .$Using vector notation, we have

$L = 1 2 u T Ku + ξ T (γ1 -η -α) -a 0 α T y -(α ⊙ y) T Ku + α T 1$where y ⊙ α is the vector with coordinates y k α k . The infimum of L is -∞ unless γ1ηα = 0 and α T y = 0. If these identities are true, then the optimal u is u = α ⊙ y and the minimum of L is

$- 1 2 (α ⊙ y) T K(α ⊙ y) + α T 1$The dual problem therefore requires to minimize

$1 2 (α ⊙ y) T K(α ⊙ y) -α T 1 = α T (K ⊙ yy T )α -α T 1 subject to γ1 -η -α = 0 and α T y = 0.$This is exactly the same problem as the one we obtained in the linear case, up to the replacement of the Euclidean inner products x T k x l by the kernel evaluations K(x k , x l ). Given the solution of the dual problem, the optimal b is b

$= k u k h(x k ) = N k=1 α k y k h(x k ).$It is no computable, but the classification rule is explicit and given by

$f (x) = sign        a 0 + N k=1 α k y k K(x k , x)        .$Similarly to the linear case, the coefficient a 0 can be identified using a support vector, or is otherwise not uniquely determined. More precisely, if one of the α k 's is strictly between 0 and γ, then a 0 is given by a 0 = y kl α l y l K(x k , x l ). Otherwise, a 0 is any number between

$β - 0 = max        y k - l α l y l K(x k , x l ) : (y k = 1 and α k = 0) or (y k = -1 and α k = γ)       $and

$β + 0 = min        y k - l α l y l K(x k , x l ) : (y k = -1 and α k = 0) or (y k = 1 and α k = γ)        . Chapter 9$Nearest-Neighbor Methods

Unlike linear models, nearest-neighbor methods are completely non-parametric and assume no regularity on the decision rule or the regression function. In their simplest version, they require no training and rely on the proximity of a new observation to those that belong to the training set. We will discuss in this chapter how these methods are used for regression and classification, and study some of their theoretical properties.

## Nearest neighbors for regression 9.1.1 Consistency

We let R X denote the input space, and R Y = R q be the output space. We assume that a distance, denoted dist is defined on R X . This means that dist : R X × R X → [0, +∞] (we allow for infinite values) is a symmetric function such that dist(x, x ′ ) = 0 if and only if x = x ′ and, for all

$x, x ′ , x ′′ ∈ R X dist(x, x ′ ) ≤ dist(x, x ′′ ) + dist(x ′′ , x ′ ),$which is the triangle inequality.

$Let T = (x 1 , y 1 , . . . , x N , y N ) be the training set. For x ∈ R X , let D T (x) = (dist(x, x k ), k = 1, . . . , N )$be the collection of all distances between x and the inputs in the training set. We consider regression estimators taking the form

$f (x) = N k=1 W k (x)y k (9.1)$where W 1 (x), . . . , W N (x) is a family of coefficients, or weights, that only depends on D T (x).

We will, more precisely, use the following construction [[183]](#b201). Assume that a family of numbers

$w 1 ≥ w 2 ≥ • • • ≥ w N ≥ 0 is chosen, with N j=1 w j = 1. Given x ∈ R d and k ∈ {1, . . . , N }, we let r + k (x) denote the number of indexes k ′ such that dist(x, x k ′ ) ≤ dist(x, x k ) and r - k (x) the number of such indexes such that d(x, x k ′ ) < d(x, x k ).$The coefficients defining f in (9.1) are then chosen as:

$W k (x) = r + k (x) k ′ =r - k (x)+1 w k ′ r + k (x) -r - k (x) . (9.2)$To emphasize the role of (w 1 , . . . , w N ) is this definition, we will denote the resulting estimation as fw . If there is no tie in the sequence of distances between x and elements of the training set, then r + k (x) = r - k (x) + 1 is the rank of x k when training data is ordered according to their proximity to x, and

$W k (x) = w r + k (x) . In this case, defining l 1 , . . . , l N such that d(x, x l 1 ) < • • • < d(x, x l N ), we have fw (x) = N j=1 w j y l j .$In the general case, the weights w j associated with tied observations are averaged.

If p is an integer, the p-nearest neighbor (p-NN) estimator (that we will denote fp ) is associated to the weights w j = 1/p for j = 1, . . . , p and 0 otherwise. If there is no tie for the definition of the pth nearest neighbor of x, W k (x) = 1/p if k is among the p nearest-neighbors and W k (x) = 0 otherwise, so that fp is the average of the output values over these p nearest neighbors. If the pth nearest neighbors are tied, their output value is averaged before being used in the sum. For example, assume that N = 5 and p = 2 and let the distances between x and x k for k = 1, . . . , 5 be respectively 9, 3, 2, 4, 6. Then f2 (x) = (y 2 + y 3 )/2. If the distances were 9, 3, 2, 3, 6, then we would have f2

$(x) = (y 2 + y 4 )/4 + y 3 /2. When R X = R d and d(x, x ′ ) = |x -x ′ |, the following result is true. Theorem 9.1 ([183]) Assume that E(Y 2 ) < ∞. Assume that, for each N , a sequence w (N ) = w (N ) 1 ≥ • • • ≥ w (N ) N ≥ 0 is chosen with N j=1 w (N ) j = 1. Assume, in addition, that (i) lim N →∞ w (N ) 1 = 0 (ii) lim N →∞ j≥αN w (N ) j$→ 0, for some α ∈ (0, 1).

Then the corresponding classifier fw (N ) converges in the L 2 norm to E(Y | X):

$E | fw (N ) (X) -E(Y | X)| 2 → 0.$For nearest-neighbor regression, (i) and (ii) mean that the number of nearest neighbors p N must be chosen such that p N → ∞ and p N /N → 0.

Proof We give a proof under the assumption that f :

$x → E(Y | X = x) is uniformly$continuous and bounded (one can, in fact, prove that it is always possible to reduce to this case).

To lighten the notation, we will not make explicit the dependency on N in of quantities such as W or w. One has

$fw (X) -E(Y | X) = N k=1 W k (X)(f (X k ) -f (X)) + N k=1 W k (X)(Y k -f (X k )) (9.3)$and the two sums can be addressed separately.

We start with the first sum and write, by Schwartz's inequality:

$       k W k (X)(f (X k ) -f (X))        2 ≤ k W k (X)(f (X k ) -f (X)) 2 .$It therefore suffices to study the limit of

$E( k W k (X)(f (X k ) -f (X)) 2 . Fix ϵ > 0. By assumption, there exists M, a > 0 such that |f (x)| ≤ M for all x and |x -y| ≤ a ⇒ |f (x) -f (y)| 2 ≤ ϵ. Then E        k W k (X)(f (X k ) -f (X)) 2        =E        k W k (X)(f (X k ) -f (X)) 2 1 |X k -X|≤a        + E        k W k (X)(f (X k ) -f (X)) 2 1 |X k -X|>a        ≤ ϵ 2 + 4M 2 E        k W k (X)1 |X k -X|>a        .$Since ϵ can be made arbitrarily small, we need to show that, for any positive a, the second term in the upper-bound tends to 0 when N → ∞. We will use the following fact, which requires some minor measure theory argument to prove rigorously.

$Define S = {x : ∀δ > 0, P(|X -x| < δ) > 0} .$This set is called the support of X. Then, one can show that P(X ∈ S) = 1. This means that, if X is independent from X with the same distribution, then, for any δ > 0, P(|X -X| < δ|X) > 0 with probability one. 1   Let

$N a (x) = | {k : |X k -x| ≤ a} |.$We have, for all x ∈ S and a > 0, and using the law of large numbers,

$N a (x) N = 1 N N k=1 1 |X k -x|≤a → P (|X -x| ≤ a) > 0. If |X -X k | > a, then r - k (X) > N a (x) so that k W k (X)1 |X k -X|>a ≤ j≥N a (X)$w j , and we have, taking 0

$< α < P (|X -x| ≤ a), E         j≥N a (X) w j         ≤ j≥αN w j + P(N a (X) < αN )$and both terms in the upper bound converge to 0. This shows that the first sum in (9.3) tends to 0.

We now consider the second sum in [(9.3)](#).

$Let Z k = Y k -E(Y | X k ). We have E(Z k | X k ) = 0 and E(Z 2 k ) < ∞. We can write E         N k=1 W k (X)Z k 2         = E         E         N k=1 W k (X)Z k 2 X, X 1 , . . . , X N                 = E        N k=1 W k (X) 2 E(Z 2 k | X k )        + N k l=1 E(W k (X)W l (X)E(Z k Z l | X i , X j ))$1 This statement is proved as follows (with the assumption that X is Borel measurable). Let S c denote the complement of S. Then S c is open. Indeed if x S, there exists δ x > 0 such that, letting B(x, δ x ) denote the open ball with radius δ x , P(X ∈ B(x, δ x )) = 0. Then P(X ∈ B(x ′ , δ x /3)) = 0 as soon as

$|x -x ′ | < δ x /3, so that B(x, δ x /3) ⊂ S c . If K ⊂ S c is compact, then K ⊂ x∈K B(x, δ x )$and one can find a finite subset M ⊂ K such that K ⊂ x∈M B(x, δ x ), which proves that P(X ∈ K) = 0. Since P(X ∈ S c ) = max K P(X ∈ K) where the maximum is over all compact subsets of S c , we find P(X ∈ S c ) = 0 as required.

The cross products in the last term vanish because E(Z k | X k ) = 0 and the samples are independent. So it only remains to consider

$E        N k=1 W k (X) 2 E(Z 2 k | X k )        The random variable E(Z k | X k ) = E(Y 2 k | X k ) -E(Y k | X k ) 2 is a fixed non-negative$function of X k , that we will denote h(X k ). We have

$E        N k=1 W k (X) 2 h(X k )        ≤ w 1 E        N k=1 W k (X)h(X i )        with w 1 → 0 and the proof is concluded by showing that E N k=1 W k (X)h(X k ) is bounded.$Recall that the weights W k are functions of X and of the whole training set, and we will need to make this dependency explicit and write W i (X, T X ) where T X = (X 1 , . . . , X N ). Similarly, the ranks in (9.2) will be written r + j (X, T X ) and r - j (X, T X ).

Because X, X 1 , . . . , X N are i.i.d., we can switch the role of X and X k in the kth term of the sum, yielding

$E        N k=1 W k (X, T X )h(X k )        = E               N i=1 W k (X k , T (k) X )        h(X)        with T (k) X = (X 1 , . . . , X k-1 , X, X k+1 , . . . , X N ). We now show that N k=1 W k (X k , T (k) X ) is bounded independently of X, X 1 , . . . , X N .$For this purpose, we group X 1 , . . . , X N according to approximate alignment with X. For u ∈ R d with |u| = 1 and for δ ∈ (0, π/4), denote by Γ (u, δ) the cone formed by all vectors v in R d such that ⟨v , u⟩ > |v| cos δ (i.e., the angle between v and u is less

$than δ). Notice that if v, v ′ ∈ Γ (u, δ), then ⟨v , v ′ ⟩ ≥ cos(2δ)|v| |v ′ | and if |v ′ | ≤ |v|, then |v| 2 -|v -v ′ | 2 = |v ′ |(2|v| cos(2δ) -|v ′ |) > 0 (9.4) because cos(2δ) > 1/2.$Fixing δ, let C d (δ) be the minimal number of such cones needed to cover R d .

Choosing such a covering Γ (u 1 , δ), . . . , Γ (u M , δ) where M = C d (δ), we define the following subsets of {1, . . . , M}:

$I 0 = {k : X k = X} I q = k I 0 : X k -X ∈ Γ (u q , δ) , q = 1, . . . , M$(these sets may overlap). We have

$N k=1 W k (X k , T (k) X ) ≤ M q=0 k∈I q W k (X k , T (k) X ) If k ∈ I 0 , then r - k (X k , T (k) X ) = 0 and r + k (X k , T (k) X ) = c with c = |I 0 |. This implies that, for k ∈ I 0 , we have W k (X k , T (k) X ) = c j=1 w j /c and k∈I 0 W k (X k , T (k) X ) = c j=1 w j .$We now consider I q with q ≥ 1. Write

$I q = {i 1 , . . . , i r } ordered so that |X i j -X| is non-decreasing. If j ′ < j, we have (using (9.4)) |X i j -X i j ′ | < |X -X i j |. This implies that r - i j (X i j , T (i j ) X ) ≥ j -1 and r + i j (X i j , T (i j ) X ) -r -i j (X i j , T (i j ) X ) ≥ c + 1. Therefore, W i j (X i j , T (i j ) X ) ≤ 1 c + 1 c+j j ′ =j w j ′ and k∈I q W k (X k , T (k) X ) ≤ 1 c + 1 N j=1 c+j j ′ =j w j ′ = 1 c + 1         c j ′ =1 j ′ w j ′ + (c + 1) N j ′ =c+1 w j ′         . This yields N k=1 W k (X k , T (k) X ) ≤ c j=1 w j + C d (δ)         1 c + 1 c j ′ =1 j ′ w j ′ + N j ′ =c+1 w j ′         ≤ C d (δ) + 1.$We therefore have

$E        N k=1 W k (X) 2 E(Z 2 k | X k )        ≤ w 1 (C d (δ) + 1)E(h(X)) → 0,$which concludes the proof. ■ Theorem 9.1 is proved in Stone [[183]](#b201) with weaker hypotheses allowing for more flexibility in the computation of distances, in which, for example, differences X -X i can be normalized by dividing them by a factor σ i that may depend on the training set. These relaxed assumptions slightly complicate the proof, and we refer the reader to Stone [[183]](#b201) for a complete exposition.

## Optimality

The NN method can be shown to be optimal over some classes of functions. Optimality is in the min-max sense, and works as follows. We assume that the regression function f

$(x) = E(Y | X = x) belongs to some set F of real-valued functions on R d .$Most of the time, the estimation methods must be adapted to a given choice of F , and various choices have arisen in the literature: classes of functions with r bounded derivatives, Sobolev or related spaces, functions whose Fourier transforms has given properties, etc.

Consider now an estimator of f , denoted fN , based on a training set of size N . We can measure the error by, say:

$fN -f 2 = R d ( fN (x) -f (x)) 2 dx 1/2$Since fN is computed from a random sample, this error is a random variable. One can study, when b N → 0, the probability

$P f ∥ fN -f ∥ 2 2 ≥ cb N$for some constant c and, for example, for the model: Y = f (X) + noise. Here, the notation P f refers to the model assumption indicating the unobserved function f . The min-max method considers the worst case and computes

$M N (c) = sup f ∈F P f ∥ fN -f ∥ 2 2 ≥ cb N .$This quantity now only depends on the estimation algorithm. One defines the notion of "lower convergence rate" as a sequence b N such that, for any choice of the estimation algorithm, M N (c) can be found arbitrarily close to 1 (i.e., ∥ fNf ∥ 2 2 ≥ cb N with arbitrarily high probability for all f ∈ F ), for arbitrarily large N (and for some choice of c). The mathematical statement is

$∃c > 0 : lim inf N →∞ M N (c) = 1.$So, if b N is a lower convergence rate, then, for every estimator, there exists a constant c such that the accuracy cb N cannot be achieved.

On the other hand, one says that b N is an achievable rate of convergence if there exists an estimator such that, for some c ′ , lim sup

$N →∞ M N (c ′ ) = 0.$This says that for large N , and for some c ′ , the accuracy is higher than c ′ b N for the given estimator. Notice the difference: a lower rate holds for all estimators, and an achievable rate for at least one estimator.

The final definition of a min-max optimal rate is that it is both a lower rate and an achievable rate (obviously for different constants c and c ′ ). And an estimator is optimal in the min-max sense if it achieves an optimal rate. One can show that the p-NN estimator is optimal (under some assumptions on the ratio p N /N ) when F is the class of Lipschitz functions on R d , i.e., the class of functions such that there exists a constant K with

$|f (x) -f (y)| ≤ K|x -y|$for all x, y ∈ R d . In this case, the optimal rate is b N = N -1/(2+d) (notice again the "curse of dimensionality": to achieve a given accuracy in the worst case, the number of data points must grow exponentially with the dimension).

If the function class consists of smoother functions (for example, several derivatives), the p-NN method is not optimal. This is because the local averaging method is too crude when one knows already that the function is smooth. But it can be modified (for example by fitting, using least squares, a polynomial of some degree instead of computing an average) in order to obtain an optimal rate.

## p-NN classification

Let (x 1 , y 1 , . . . , x N , y N ) be the training set, with x i ∈ R d and y i ∈ R Y where R Y is a finite set of classes. Using the same notation as in the previous section, define

$π w (y|x) = N k=1 W k (x)1 y k =y .$Let the corresponding classifier be fw

$(x) = argmax y∈R Y π w (y|x). Theorem 9.1 may be applied, for y ∈ R Y , to the function f y (x) = π(y | x) = E(1 Y =y | X = x$), which allows one to interpret the estimator π(y | x) as a nearest-neighbor predictor of the random variable 1 Y =y as a function of X. We therefore obtain the consistency of the estimated posteriors when N → ∞ under the same assumption as those of theorem 9.1. This implies that, for large N , the classification will be close to Bayes's rule.

An asymptotic comparison with Bayes's rule can already be made with p = 1. Let ŷN (x) be the 1-NN estimator of Y given x and a training set of size N , and let ŷ(x) be the Bayes estimator. We can compute the Bayes error by

$P( ŷ(X) Y ) = 1 -P( ŷ(X) = Y ) = 1 -E(P( ŷ(X) = Y |X)) = 1 -E(max y∈R Y π(y|X))$For the 1-NN rule, we have

$P( ŷN (X) Y ) = 1 -P( ŷN (X) = Y ) = 1 -E(P( ŷN (X) = Y |X))$Let us make the assumption that nearest neighbors are not tied (with probability one). Let k * (x, T ) denote the index of the nearest neighbor to x in the training set T . We have

$P( ŷN (X) = Y | X) = E(P( ŷN (X) = Y | X, T )) = E N k=1 P(Y = Y k | X, T )1 k * (X,T )=k = E N k=1 P(Y = Y k | X, X k )1 k * (X,T )=k = E N k=1 g∈R Y P(Y = g, Y k = g | X, X k )1 k * (X,T )=k = E N k=1 g∈R Y π(g | X)π(g | X k )χ k * (X,T )=k = E g∈R Y π(g | X)π(g | X k * (X,T ) )$Now, assume the continuity of x → π(g | x) (although the result can be proved without this simplifying assumption). We know that X k * (X,T ) → X when N → ∞ (see the proof of theorem 9.1), which implies that π(g | X k * (X,T ) ) → π(x | X) and at the limit

$P( ŷN (X) = Y | X) → g∈R Y π(g | X) 2 .$This implies that the asymptotic 1-NN misclassification error is always smaller than 2 times the Bayes error, that is

$1 -E g∈R Y π(g | X) 2 ≤ 2(1 -E(max g π(g | X)))$Indeed, the left-hand term is smaller than 1 -E(max g π(g|x) 2 ) and the result comes from the fact that for any t ∈ R. 1t 2 ≤ 2 -2t.

Remark 9.2 Nearest neighbor methods may require large computation time, since, for a given x, the number of comparisons which are needed is the size of the training set. However, efficient (tree-based) search algorithms can be used in many cases to reduce it to a logarithm in the size of the database, which is acceptable. A reduction of the size of the training set by clustering also is a possibility for improving the efficiency.

The computation time is also generally proportional to the dimension d of the input x. When d is large, a reduction of dimension is often a good idea. Principal components (see chapter 20), or LDA directions (see chapter 8) can be used for this purpose. ♦

## Designing the distance

LDA-based distance The most important factor in the design of a NN procedure probably is the choice of the distance, something we have not discussed so far. Intuitively, the distance should increase fast in the directions "perpendicular" to the regions of constancy of the class variables, and slowly (ideally not at all) within these regions. The following construction uses discriminant analysis [[87]](#b105).

For g ∈ R Y , let Σ g be the covariance matrix in class g, and Σ w = g∈R Y π g Σ g be the within-class variance, where π g is the frequency of class g. Let Σ b denote the between-class covariance matrix (see section 8.2).

For x ∈ R d , define the spherized vector x * = Σ -1/2 w x. The between-class variance computed for spherized data is

$Σ * b = Σ -1/2 w Σ b Σ -1/2 w . A direction is discriminant if it is close to the principal eigenvectors of Σ * b .$This suggests the introduction of the norm

$|x| 2 * = (x * ) T Σ * b x * = x T Σ -1/2 w (Σ -1/2 w Σ b Σ -1/2 w )Σ -1/2 w x = x T Σ -1 w Σ b Σ -1 w x.$This replaces the standard Euclidean norm (the method can be made more robust by adding ϵId R d to Σ * b .)

Tangent distance Designing the distance, however, can sometimes be based on a priori knowledge on some invariance properties associated with the classes. A successful example comes from character recognition, where it is known that transforming images by slightly rotating, scaling, or translating the character should not change its class. This corresponds to the following general framework.

For each input x ∈ R d , assume that one can make small transformations without changing the class of x. We model these transformations as parametrized functions

$x → x θ = ϕ(x, θ) ∈ R d , such that ϕ(x, 0) = x and ϕ is smooth in θ, which is a q- dimensional parameter.$The assumption is that ϕ(x, θ) and x should be from the same class, at least for small θ. This will be used to improve on the Euclidean distance on R d .

Take x, x ′ ∈ R d . Ideally, one would like to use the distance

$D(x, x ′ ) = inf θ,θ ′ dist(x θ , x θ ′ )$where θ and θ ′ are restricted to a small neighborhood of 0. A more tractable expression can be based on first-order approximations

$x θ ≃ x + ∇ θ ϕ(x, 0)u = x + q i=1 u i ∂ θ i ϕ(x, 0) and x ′ θ ≃ x ′ + ∇ θ ϕ(x ′ , 0)u ′ = x ′ + q i=1 u ′ i ∂ θ i ϕ(x ′ , 0)$yielding the approximation (also called the tangent distance)

$D(x, x ′ ) 2 ≃ inf u,u ′ ∈R q x -x ′ + ∇ θ ϕ(x, 0)u -∇ θ ϕ(x ′ , 0)u ′ 2 .$The computation now is a simple least-squares problem, for which the solution is given by the system

$∇ θ ϕ(x, 0) T ∇ θ ϕ(x, 0) -∇ θ ϕ(x, 0) T ∇ θ ϕ(x ′ , 0) -∇ θ ϕ(x ′ , 0) T ∇ θ ϕ(x, 0) ∇ θ ϕ(x ′ , 0) T ∇ θ ϕ(x ′ , 0) u v = ∇ θ ϕ(x, 0) T (x ′ -x) ∇ θ ϕ(x ′ , 0) T (x -x ′ ) .$A slight modification, to ensure that the norms of u and u ′ are not too large, is to add a penalty λ(|u| 2 + |u ′ | 2 ), which results in adding λId R q to the diagonal blocs of the above matrix.

## Chapter 10

Tree-based Algorithms, Randomization and Boosting

## Recursive Partitioning

Recursive partitioning methods implement a "divide and conquer" strategy to address the prediction problem. They separate the input space R X into small regions on which prediction is "easy," i.e., such that the observed values of the output variable are (almost) constant for input values in these regions. The regions are estimated by recursive divisions until they become either too small or homogeneous. These divisions are conveniently represented in the form of binary trees.

## Binary prediction trees

Define a binary node to be a structure ν that contains the following information (note that the definition is recursive):

• A label L(ν) that uniquely identifies the node.

• A set of children, C(ν), which is either empty or a pair of nodes (l(ν), r(ν)).

• A binary feature, i.e., a function γ ν : R X → {0, 1}, which is "None" (i.e., irrelevant) if the node has no children.

• A predictor, f ν : R X → R Y , which is "None" if the node has children.

A node without children is called a terminal node, or a leaf.

A binary prediction tree T is a finite set of nodes, with the following properties:

(i) Only one node has no parent (the root, denoted ρ or ρ T );

(ii) Each other node has exactly one parent;

(iii) No node is a descendent of itself.

## Training algorithm

Assume that a family Γ of binary features γ : R X → {0, 1} is chosen, together with a family F of predictors f : R X → R Y . Assume also the existence of two "algorithms" as follows:

• Feature selection: Given the feature set Γ and a training set T , return an optimized binary feature γ T ,Γ ∈ Γ .

• Predictor optimization: Given the predictor set F and a training set T , return an optimized predictor fT ,F ∈ F .

Finally, assume that a stopping rule is defined, as a function of training sets σ : T → σ (T ) ∈ {0, 1}, where 0 means "continue", and 1 means "stop".

Given a training set T 0 , the algorithm builds a binary tree T using a recursive construction. Each node ν ∈ T will be associated to a subset of T 0 , denoted T ν . We define below a recursive operation, denoted Node(T , j) that adds a node ν to a tree T given a subset T of T 0 and a label j. Starting with T = ∅, calling Node(T 0 , 0) will then create the desired tree.

Algorithm 10.1 (Node insertion: Node(T , j)) (a) Given T and j, let T ν = T and L(ν) = j.

$(b) If σ (T ) = 1, let C(ν) = ∅, γ ν = "None" and f ν = fT ,F . (c) If σ (T ) = 0, let f ν = "None", γ ν = γ T ,Γ and C(ν) = (l(ν), r(ν)) with l(ν) = Node(T l , 2j + 1), r(ν) = Node(T r , 2j + 2)$where

$T l = {(x, y) ∈ T : γ ν (x) = 0}, T r = {(x, y) ∈ T : γ ν (x) = 1}$(d) Add ν to T and return.

Remark 10.1 Note that, even though the learning algorithm for prediction trees can be very conveniently described in recursive form as above, efficient computer implementations should avoid recursive calls, which may be inefficient and memory demanding. Moreover, for large trees, it is likely that recursive implementations will reach the maximal number of recursive calls imposed by compilers. ♦

## Resulting predictor

Once the tree is built, the predictor x → fT (x) is recursively defined as follows.

(a) Initialize the computation with ν = ρ.

(b) At a given step of the algorithm, let ν be the current node.

• If ν has no children: then let fT (x) = f ν (x).

• Otherwise: replace ν by l(ν) if γ ν (x) = 0 and by r(ν) if γ ν (x) = 1 and go back to (b).

## Stopping rule

The function σ , which decides whether a node is terminal or not is generally defined based on very simple rules. Typically, σ (T ) = 1 when one the following conditions is satisfied:

• The number of training examples in T is small (e.g., less than 5).

• The values y k in T have a small variance (regression) or are constant (classification).

## Leaf predictors

When one reaches a terminal node ν (so that σ (T ν ) = 1), a predictor f ν must be determined. This function can be optimized within any set F of predictors, using any learning algorithm, but in practice, one usually makes this fairly simple and defines F to be the family of constant functions taking values in R Y . The function fT ,F is then defined as:

• the average of the values of y k , for (x k , y k ) ∈ T (regression);

• the mode of the distribution of y k , for (x k , y k ) ∈ T (classification).

## Binary features

The space Γ of possible binary features must be specified in order to partition nonterminal nodes. A standard choice, used in the CART model [[43]](#b61) with

$R X = R d , is Γ = γ(x) = 1 [x (i) ≥θ] , i = 1, . . . , d, θ ∈ R (10.1)$where x (i) is the ith coordinate of x. This corresponds to splitting the space using a hyperplane parallel to one of the coordinate axes.

The binary function γ T ,Γ can be optimized over Γ using a greedy evaluation of the risk, assuming that the prediction is based on the two nodes resulting from the split. For γ ∈ Γ , f 0 , f 1 ∈ F , define

$F γ,f 0 ,f 1 (x) = f 0 (x) if γ(x) = 0 f 1 (x) if γ(x) = 1$Given a risk function r, one then evaluates

$E T (γ) = min f 0 ,f 1 ∈F (x,y)∈T r(y, F γ,f 0 ,f 1 (x))$One then chooses γ T ,Γ = argmin γ∈Γ (E T (Γ )).

Example 10.2 (Regression) Consider the regression case, taking squared differences as risk and letting F contain only constant functions. Then

$E T (γ) = min m 0 ,m 1 (x,y)∈T (y -m 0 ) 2 1 γ(x)=0 + (y -m 1 ) 2 1 γ(x)=1 .$Obviously, the optimal m 0 and m 1 are the averages of the output values, y, in each of the subdomains defined by γ. For CART (see (10.1)), this cost must be minimized over all choices (i, θ) with i = 1, . . . , d and θ ∈ R where γ i,θ (x) = 1 if x(i) > θ and 0 otherwise. ♦ Example 10.3 (Classification.) For classification, one can apply the same method, with the 0/1 loss, letting

$E T (γ) = min g 0 ,g 1 (x,y)∈T 1 y g 0 1 γ(x)=0 + 1 y g 1 1 γ(x)=1 .$The optimal g 0 and g 1 are the majority classes in T ∩ {γ = 0} and T ∩ {γ = 1}. ♦ Example 10.4 (Entropy selection for classification) For classification trees, other splitting criteria may be used based on the empirical probability p T on the set T , defined as

$p T (A) = 1 N |{k : (x k , y k ) ∈ A}| for A ⊂ R X × R Y . The previous criterion, E T (γ), is proportional to p T (γ = 0)(1 -max g p T (g | γ = 0)) + p T (γ = 1)(1 -max g p T (g | γ = 1)).$One can define alternative objectives in the form

$p T (γ = 0)H(p T (g | γ = 0)) + p T (γ = 1)H(p T (g | γ = 1))$where π → H(π) associates to a probability distribution π a "complexity measure" that is minimal when π is concentrated on a single class (which is the case for π → 1max g π(g)).

Many such measures exists, and many of them are defined as various forms of entropy designed in information theory. The most celebrated is Shannon's entropy [[176]](#b194), defined by

$H(p) = - g∈R Y p(g) log p(g) .$It is always positive, and minimal when the distribution is concentrated on a single class. Other entropy measures include:

• The Tsallis entropy: H(p) = 1 1-q g∈R Y (p(g) q -1), for q 1. (Tsallis entropy for q = 2 is sometimes called the Gini impurity index.)

• The Renyi entropy: H(p) = 1  1-q log g∈R Y p(g) q , for q ≥ 0, q 1. ♦

## Pruning

Growing a decision tree to its maximal depth (given the amount of available data) generally leads to predictors that overfit the data. The training algorithm is usually followed by a pruning step that removes some some nodes based on a complexity penalty.

Letting τ(T) denote the set of terminal nodes in the tree T and fT the associated predictor, pruning is represented as an optimization problem, where one minimizes, given the training set T ,

$U λ (T, T ) = RT ( fT ) + λ|τ(T)|$where RT is as usual the in-sample error measured on the training set T .

To prune a tree, one selects one or more internal nodes and remove all their descendants (so that these nodes become terminal). Associate to each node ν in T its local in-sample error E T ν equal to the error made by the optimal classifier estimated from the training data associated with ν. Then,

$U λ (T, T ) = ν∈τ(T) |T ν | |T | E T ν + λ|τ(T)|$If ν is a node in T (internal or terminal), let T ν be the subtree of T containing ν as a root and all its descendants. Let T (ν) be the tree T will all descendants of ν removed (keeping ν). Then

$U λ (T, T ) = U 0 (T (ν) , T ) - |T ν | |T | (E T ν -U 0 (T ν , T ν )) + λ(|τ(T ν )| -1).$Note also that, if ν is internal, and ν ′ , ν ′′ are its children, then

$U 0 (T ν , T ν ) = |T ν ′ | |T ν | U 0 (T ν ′ , T ν ′ ) + |T ν ′′ | |T ν | U 0 (T ν ′′ , T ν ′′ )$This formula can be used to compute U 0 (T ν ) recursively for all nodes, starting with leaves for which

$U 0 (T ν ) = E(T ν ). (We also have |τ(T ν )| = |τ(T ν ′ )| + |τ(T ν ′′ )|.)$The following algorithm converges to a global minimizer of U λ .

Algorithm 10.2 (Pruning)

(1) Start with a complete tree T(0) built without penalty.

(2) Compute, for all nodes U 0 (T ν ) and |τ(T ν )|. Let

$ψ ν = |T ν | |T | (E T ν -U 0 (T ν )) -λ(|τ(T ν )| -1).$(3) Iterate the following steps.

• If ψ ν < 0 for all internal nodes ν, exit the program and return the current T(n).

• Otherwise choose an internal node ν such that ψ ν is largest.

$• Let T(n + 1) = T (ν) (n). Subtract λ(|τ(T ν (n))| -1) to ρ ν ′ for all ν ′ ancestor of ν.$10.2 Random Forests

## Bagging

A random forest [[7,](#b25)[42]](#b60) is a special case of composite predictors (we will see other examples later in this chapter when describing boosting methods) that train multiple individual predictors under various conditions and combine them, through averaging, or majority voting. With random forests, one generates individual trees by randomizing the parameters of the learning process. One way to achieve this is to randomly sample from the training set before running the training algorithm.

Letting as before T 0 = (x 1 , y 1 , . . . , x N , y N ) denote the original set, with size N , one can create "new" training data by sampling with replacement from T 0 . More precisely, consider the family of independent random variables ξ = (ξ 1 , . . . , ξ N ), with each ξ j following a uniform distribution over {1, . . . , N }. One can then form the random training set

$T 0 (ξ) = (x ξ 1 , y ξ 1 , . . . , x ξ N , y ξ N ).$Running the training algorithm using T 0 (ξ) then provides a random tree, denoted T(ξ). Now, by sampling K realizations of ξ, say ξ (1) , . . . , ξ (K) , one obtains a collection of K random trees (a random forest) T * = (T 1 , . . . , T K ), with T j = T(ξ (j) ) that can be combined to provide a final predictor. The simplest way to combine them is to average the predictors returned by each tree (assuming, for classification, that this predictor is a probability distribution on classes), so that

$f T * (x) = 1 K K j=1 f T j (x). (10.2)$For classification, one can alternatively let each individual tree "vote" for their most likely class.

Obviously, randomizing training data and averaging the predictors is a general approach that can be applied to any prediction algorithm, not only to decision trees. In the literature, the approach described above has been called bagging [[41]](#b59), which is an acronym for "bootstrap aggregating" (bootstrap itself being a general resampling method in statistics that samples training data with replacement to determine some properties of estimators). Another way to randomize predictors (especially when d, the input dimension is large), is to randomize input data by randomly removing some of the coordinates, leading to a similar construction.

With decision trees one can in addition randomize the binary features use to split nodes, as described next. While bagging may provide some enhancement to predictors, feature randomization for decision trees often significantly improves the performance, and is the typical randomization method used for random forests.

## Feature randomization

When one decides to split a node during the construction of a prediction tree, one can optimize the binary feature γ over a random subset of Γ rather than exploring the whole set. For CART, for example, one can select a small number of dimensions i 1 , . . . , i q ∈ {1, . . . , d} with q ≪ d, and optimize γ by thresholding one of the coordinates x (i j ) for j ∈ {1, . . . , q}. This results in a randomized version of the node insertion function. (c) If σ (T ′ ) = 0, sample (e.g., uniformly without replacement) a subset Γ ν of Γ and let

$f ν = "None", γ ν = γT ,Γ ν and C(ν) = (l(ν), r(ν)) with l(ν) = Node(T l , 2j + 1) r(ν) = Node(T r , 2j + 2)$where

$T l = {(x, y) ∈ T : γ ν (x) = 0} T r = {(x, y) ∈ T : γ ν (x) = 1}$(d) Add ν to T and return. Now, each time the function RNode(T 0 , 0) is run, it returns a different, random, tree. If it is called K times, this results in a random forest T * = (T 1 , . . . T K ), with a predictor F T * given by (10.2). Note that trees in random forests are generally not pruned, since this operation has been observed to bring no improvement in the context of randomized tress.

## Top-Scoring Pairs

Top-Scoring Pair (TSP) classifiers were introduced in Geman et al. [[78]](#b96) and can be seen as forests formed with depth-one classification trees in which splitting rules are based on the comparison of pairs of variables. More precisely, define

$γ ij (x) = 1 x (i) >x (j) .$A decision tree based on these rules only relies on the order between the features, and is therefore well adapted to situations in which the observations are subject to increasing transformations, i.e., when the observed variable X is such that X (j) = ϕ(Z (j) ), where ϕ : R → R is random and increasing and Z is a latent (unobserved) variable. Obviously, in such a case, order-based splitting rules do not depend on ϕ. Such an assumption is relevant, for example, when experimental conditions (such as temperature) may affect the actual data collection, without changing their order, which is the case when measuring high-throughput biological data, such as microarrays, for which the approach was introduced.

Assuming two classes, a depth-one tree in this context is simply the classifier f ij = γ ij . Given a training set, the associated empirical error is

$E ij = 1 N N k=1 1 γ ij (x k ) y k = 1 N N k=1 |y k -γ ij (x k )| 10.4. ADABOOST$and the balanced error (better adapted to situations in which one class is observed more often than the other) is

$E b ij = N k=1 w k |y k -γ ij (x k )| with w k = 1/(2N y k )$, where N 0 , N 1 are the number of observations with y k = 0, y k = 1. Pairs (i, j) with small errors are those for which the order between the features switch with high probability when passing from class 0 to class 1.

In its simplest form, the TSP classifier defines the set

$P = argmin ij E b ij$of global minimizers of the empirical error (which may just be a singleton) and predicts the class based on a majority vote among the family of predictors (f ij , (i, j) ∈ P ). Equivalently, selected variables maximize the score

$∆ ij = 1 -E b ij , leading to the method's name.$Such classifiers, which are remarkably simple, have been found to be competitive among a wide range of "advanced" classification algorithms for large-dimensional problems in computational biology. The method has been refined in Tan et al. [[190]](#b208), leading to the k-TSP classifier, which addresses the following remarks. First, when j, j ′ are highly correlated, and (i, j) is a high-scoring pair, then (i, j ′ ) is likely to be one too, and their associated decision rules will be redundant. Such cases should preferably be pruned from the classification rules, especially if one wants to select a small number of pairs. Second, among pairs of features that switch with the same probability, it is natural to prefer those for which the magnitude of the switch is largest, e.g., when the pair of variables switches from a regime in which one of them is very low and the other very high to the opposite. In Tan et al. [[190]](#b208), a rank-based tie-breaker is introduced, defined as

$ρ ij = N k=1 w k (R k (i) -R k (j))(2y k -1), where R k (i) denotes the rank of x (i) k in x (1) k , . . . , x (d)$k . One can now order pairs (i, j) and (i ′ , j ′ ) by stating that the former scores higher if (i)

$∆ ij > ∆ i ′ j ′ , or (ii) ∆ ij = ∆ i ′ j ′ and ρ ij > ρ i ′ j ′ .$The k-TSP classifier is formed by selecting pairs, starting from the highest scoring one, and use as lth pair (for l ≤ k) the highest scoring ones among all those that do not overlap with the previously selected ones. In [[190]](#b208), the value of k is optimized using cross-validation.

## Adaboost

Boosting methods refer to algorithms in which classifiers are enhanced by recursively making them focus on harder data. We first address the issue of classification, and describe one of the earliest algorithms (Adaboost). We will then interpret it as a greedy gradient descent algorithm, as this interpretation will lead to further extensions.

## General set-up

We first consider binary classification problems, with R Y = {-1, 1}. We want to design a function x → F(x) ∈ {-1, 1} on the basis of a training set T = (x 1 , y 1 , . . . , x N , y N ). With the 0-1 loss, minimizing the empirical error is equivalent to maximizing

$E T (F) = 1 N N k=1 y k F(x k ).$Boosting algorithms build the function F as a linear combination of "base classifiers," f 1 , . . . , f M , taking

$F(x) = sign         M j=1 α j f j (x)         .$We assume that each base classifier, f j , takes values in [-1, 1] (the interval).

The sequence of base classifiers is learned by progressively focusing on the hardest examples. We will therefore assume that the training algorithm for base classifiers takes as input the training set T as well a family of positive weights W = (w 1 , . . . , w N ). More precisely, letting

$p W (k) = w k N k=1 w k ,$the weighted algorithm should implement (explicitly or implicitly) the equivalent of an unweighted algorithm on a simulated training set obtained by sampling with replacement K ≫ N elements of T according to p W (ideally letting K → ∞). Let us take a few examples.

• Weighted LDA: one can use LDA as described in section 8.2 with

$c g = k:y k =g p W (k), µ g = 1 c g k:y k =g p W (k)x k , µ = g∈R Y c g µ g 10.4. ADABOOST$and the covariance matrices:

$Σ w = N k=1 p W (k)(x k -µ y k )(x k -µ y k ) T , Σ b = g∈R Y c g (µ g -μ)(µ g -μ) T .$• Weighted logistic regression: just maximize

$N k=1 p W (k) log π θ (y k |x k )$where π θ is given by the logistic model.

• Empirical risk minimization algorithms can be modified in order to minimize

$RT ,W (f ) = N k=1 w k r(y k , f (x k )).$• Of course, any algorithm can be run on a training set resampled using p W .

## The Adaboost algorithm

Boosting algorithms keep track of a family of weights and modify it after the jth classifier f j is computed, increasing the importance of misclassified examples, before computing the next classifier. The following algorithm, called Adaboost [[172,](#b190)[73]](#b91), describes one such approach.

Algorithm 10.4 (Adaboost)

• Start with uniform weights, letting W (1) = (w 1 (1), . . . , w N (1)) with w k (1) = 1/N , k = 1, . . . , N . Fix a number ρ ∈ (0, 1] and an integer M > 0.

• Iterate, for j = 1, . . . , M:

(1) Fit a base classifier f j using the weights W (j) = (w 1 (j), . . . , w N (j)). Let

$S + w (j) = N k=1 w k (j)(2 -|y k -f j (x k )|) (10.3a) S - w (j) = N k=1 w k (j)|y k -f j (x k )| (10.3b)$and define α j = ρ log S + w (j)/S - w (j) (2) Update the weights by

$w k (j + 1) = w k (j) exp α j |y k -f j (x k )|/2 .$• Return the classifier:

$F(x) = sign         M j=1 α j f j (x)         . If f j is binary, i.e., f j (x) ∈ {-1, 1}, then |y k -f j (x k )| = 21 y k f j (x k ) , so that S + W /2$is the weighted number of correct classifications and S - W /2 is the weighted number of incorrect ones.

For α j to be positive, the jth classifier must do better than pure chance on the weighted training set. If not, taking α j ≤ 0 reflects the fact that, in that case, -f j has better performance on training data.

Algorithms that do slightly better than chance with high probability are called "weak learners" [[172]](#b190). The following proposition [[73]](#b91) shows that, if the base classifiers reliably perform strictly better than chance (by a fixed, but not necessarily large, margin), then the boosting algorithm can make the training-set error arbitrarily close to 0.

Proposition 10.5 Let E T be the training set error of the estimator F returned by Algorithm 10.4, i.e.,

$E T = 1 N N k=1 1 y k F(x k ) .$Then

$E T ≤ M j=1 ϵ ρ j (1 -ϵ j ) 1-ρ + ϵ 1-ρ j (1 -ϵ j ) ρ$where

$ϵ j = S - W (j) S + W (j) + S - W (j)$. Proof We note that example k is misclassified by the final classifier if and only if

$M j=1 α j y k f j (x k ) ≤ 0 or M j=1 e -α j y k f j (x k )/2 ≥ 1 10.4. ADABOOST 225 Noting that |y k -f j (x k )| = 1 -y k f j (x k ), we see that example k is misclassified when M j=1 e α j |y k -f j (x k )|/2 ≥ M j=1 e α j /2 .$This shows that

$E T = 1 N N k=1 1 y k F(x k ) = 1 N N k=1 1 M j=1 e α j |y k -f j (x k )|/2 ≥ M j=1 e α j /2 ≤ 1 N N k=1 M j=1 e α j |y k -f j (x k )|/2 M j=1 e -α j /2 .$Let, for q ≤ M,

$U q = 1 N N k=1 q j=1 e α j |y k -f j (x k )|/2 .$Since

$w k (q) = 1 N q-1 j=1 e α j |y k -f j (x k )|/2 ,$we also have U q = N k=1 w k (q + 1) = (S + W (q + 1) + S - W (q + 1))/2.

We will use the inequality 1 1 This inequality is clear for α = 0. Assuming α 0, the difference between the upper and lower bound is

$e αt ≤ 1 -(1 -e α )t,$$q(t) = 1 -e αt -(1 -e α )t.$The function q is concave (its second derivative is -α 2 e αt ) with q(0) = q(1) = 0 and therefore nonnegative over [0, 1].

which is true for all α ∈ R and t ∈ [0, 1], to write

$U q ≤ 1 N N k=1 q-1 j=1 e α j |y k -f j (x k )|/2 (1 -(1 -e α q )|y k -f q (x k )|/2) = N k=1 w k (q)(1 -(1 -e α q )|y k -f q (x k )|/2) = N k=1 w k (q) -(1 -e α q ) N k=1 w k (q)|y k -f q (x k )|/2 = U q-1 (1 -(1 -e α q )ϵ q )$This gives (using U 0 = 1)

$U M ≤ M j=1 1 -(1 -e α j ϵ j )$and

$E T ≤ M j=1 1 -(1 -e α j )ϵ j e -α j /2 .$It now suffices to replace e α j by (1ϵ j ) ρ ϵ -ρ j and note that

$1 -(1 -(1 -ϵ j ) ρ ϵ -ρ j )ϵ j (1 -ϵ j ) -ρ/2 ϵ ρ/2 j = ϵ ρ j (1 -ϵ j ) 1-ρ + ϵ 1-ρ j (1 -ϵ j ) ρ$to conclude the proof.

## ■

For ϵ ∈ [0, 1], one has

$ϵ ρ (1 -ϵ) 1-ρ + ϵ 1-ρ (1 -ϵ) ρ = 1 -(ϵ ρ -(1 -ϵ) ρ )(ϵ 1-ρ -(1 -ϵ) -1-ρ ) ≤ 1$with equality if and only if ϵ = 1/2, so that each term in the upper-bound reduces the error unless the corresponding base classifier does not perform better than pure chance. The parameter ρ determines the level at which one increases the importance of misclassified examples for the next step. Let S+ W (j) and S-W (j) denote the expressions in (10.3a) and (10.3b) with w k (j) replaced by w k (j + 1). Then, in the case when the base classifiers are binary, ensuring that |y kf j (x k )|/2 = 1 y k f j (x k ) , one can easily check that S+ W (j)/ S-W (j) = (S + W (j)/S - W (j)) 1-ρ . So, the ratio is (of course) unchanged if ρ = 0, and pushed to a pure chance level if ρ = 1. We provide below an interpretation of boosting as a greedy optimization procedure that will lead to the value ρ = 1/2.

## Adaboost and greedy gradient descent

We here restrict to the case of binary base classifiers and denote their linear combination by

$h(x) = M j=1 α j f j (x).$Whether an observation x is correctly classified in the true class y is associated to the sign of the product yh(x), but the value of this product also has an important interpretation, since, when it is positive, it can be thought of as a margin with which x is correctly classified.

Assume that the function F is evaluated, not only on the basis of its classification error, but also based on this margin, using a loss function of the kind

$Ψ (h) = N k=1 ψ(y k h(x k )) (10.4)$where ψ is decreasing. The boosting algorithm can then be interpreted as an classifier which incrementally improves this objective function.

Let, for j < M, h (j) = j q=1 α q f q .

The next combination h (j+1) is equal to h (j) + α j+1 f j+1 , and we now consider the problem of minimizing, with respect to f j+1 and α j+1 , the function Ψ (h (j+1) ), without modifying the previous classifiers (i.e., performing a greedy optimization). So, we want to minimize, with respect to the base classifier f and to α ≥ 0, the function

$U (α, f ) = N k=1 ψ y k h (j) (x k ) + αy k f (x k )$Using the fact that f is a binary classifier, this can be written

$U (α, f ) = N k=1 ψ(y k h (j) (x k ) + α)1 y k = f (x k ) + N k=1 ψ(y k h (j) (x k ) -α)1 y k f (x k ) (10.5) = N k=1 (ψ(y k h (j) (x k ) -α) -ψ(y k h (j) (x k ) + α))1 y k f (x k ) + N k=1 ψ(y k h (j) (x k ) + α).$This shows that α and f have inter-dependent optimality conditions. For a given α, the best classifier f must minimize a weighted empirical error with non-negative weights (since ψ is decreasing)

$w k = ψ(y k h (j) (x k ) -α) -ψ(y k h (j) (x k ) + α).$Given f , α must minimize the expression in [(10.5)](#). One can use an alternative minimization procedure to optimize both f (as a weighted basic classifier) and α. However, for the special choice ψ(t) = e -t , this optimization turns out to only require one step.

In this case, we have

$U (α, f ) = N k=1 (e α -e -α )e -y k h (j) (x k ) 1 y k f (x k ) + e -α N k=1 e -y k h (j) (x k ) = e -α (j) (e α -e -α ) N k=1 w k (j)1 y k f (x k ) + e -α (j) e -α N k=1 w k (j) with w k (j +1) = e α (j) -y k h (j) (x k ) and α (j) = α 1 +• • •+α j . This shows that f should minimize N k=1 w k (j + 1)1 y k f (x k ) .$We note that

$w k (j + 1) = w k (j)e α j (1-y k f j (x k )) = w k (j)e α j |y k -f k (x k )| ,$which is identical to the weight updates in algorithm Algorithm 10.4 (this is the reason why the term α (j) was introduced in the computation). The new value of α must minimize (using the notation of Algorithm 10.4)

$e -α S + W (j) + e α S - W (j), which yields α = 1 2 log S + W (j)/S - W (j)$. This is the value α j+1 in Algorithm 10.4 with ρ = 1/2.

## Gradient boosting and regression 10.5.1 Notation

The boosting idea, and in particular its interpretation as a greedy gradient procedure, can be extended to non-linear regression problems [[75]](#b93). Let us denote by F 0 the set of base predictors, therefore functions from R X = R d to R Y = R q , since we are considering regression problems. The final predictor is a linear combination

$F(x) = M j=1 α j f j (x)$with α 1 , . . . , α M ∈ R and f 1 , . . . , f M ∈ F 0 . Note that the the coefficients α j are redundant when the class F 0 is invariant by multiplication by a scalar. Replacing if needed F 0 by {f = αg, α ∈ R, g ∈ F 0 }, we will assume that this property holds and therefore remove the coefficients α j from the problem.

In accordance with the principle of performing greedy searches, we let

$F (j) (x) = j q=1 f q (x),$and consider the problem of minimizing over f ∈ F 0 ,

$U (f ) = N k=1 r(y k , F (j) (x k ) + f (x k )),$where T = (x 1 , y 1 , . . . , x N , y N ) is the training data and r is the loss function.

## Translation-invariant loss

In the case, which is frequent in regression, when r(y, y ′ ) only depends on yy ′ , the problem is equivalent to minimizing

$U (f ) = N k=1 r(y k -F (j) (x k ), f (x k )),$i.e., to let f j+1 be the optimal predictor (in F 0 and for the loss r) of the residuals

$y (j) k = y k -F (j) (x k ).$In this case, this provides a conceptually very simple algorithm.

## Algorithm 10.5 (Gradient boosting for regression with translation-invariant loss)

• Let T = (x 1 , y 1 , . . . , x N , y N ) be a training set and r a loss function such that r(y, y ′ ) only depends on yy ′ .

• Let F 0 be a function class such that f ∈ F 0 ⇒ αf ∈ F 0 for all α ∈ R.

• Select an integer M > 0 and let F (0) = 0, y

$(0) k = y k , k = 1, . . . , N . • For j = 1, . .$
## . , M:

(1) Find the optimal predictor f j ∈ F 0 for the training set (x 1 , y (j-1) 1

## , . . . , x N , y (j-1) N

).

(2) Let y

$(j) k = y (j-1) k -f j (x k ) • Return F = M k=1 f j .$Remark 10.6 Obviously, the class F 0 should not be a linear class for the boosting algorithm to have any effect. Indeed, if f , f ′ ∈ F 0 implies f +f ′ ∈ F 0 , no improvement could be made to the predictor after the first step. ♦

A successful example of this algorithm uses regression trees as base predictors. Recall that the functions output by such trees take the form

$f (x) = A∈C w A 1 x∈A$where C is a finite partition of R d . Each set in the partition is specified by the value taken by a finite number of binary features (denoted by γ in our discussion of prediction trees) and the maximal number of such features is the depth of the tree. We assume that the set Γ of binary features is shared by all regression trees in F 0 , and that the depth of these trees is bounded by a fixed constant. These restrictions prevent F 0 from forming a linear class. [2](#foot_12) Note that the maximal depth of tree learnable from a finite training set is always bounded, since such trees cannot have more nodes than the size of the training set (but one may want to restrict the maximal depth of base predictors to be way less than N ).

## General loss functions

We now consider situations in which the loss function is not necessarily a function of the difference between true and predicted output. We are still interested in the problem of minimizing U (f ), but we now approximate this problem using the firstorder expansion

$U (f ) = N k=1 r(y k , F (j) (x k )) + N k=1 ∂ 2 r(y k , F (j) (x k )) T f (x k ) + o(f ),$where ∂ 2 r denotes the derivative of r with respect to its second variable. This suggests (similarly to gradient descent) to choose f such that f

$(x k ) = -α∂ 2 r(y k , F (j) (x k ))$for some α > 0 and all k = 1, . . . , N . However, such an f may not exist in the class F 0 , and the next best choice is to pick f = α f with f minimizing

$N k=1 | f (x k ) + ∂ 2 r(y k , F (j) (x k ))| 2$over all f ∈ F 0 . This is similar to projected gradient descent in optimization, and α such that f = α f should minimize

$N k=1 r(y k , F (j) (x k ) + α f (x k )).$This provides a generic "gradient boosting" algorithm [[75]](#b93), summarized below.

## Algorithm 10.6 (Gradient boosting)

• Let T = (x 1 , y 1 , . . . , x N , y N ) be a training set and r a differentiable loss function.

• Let F 0 be a function class such that f ∈ F 0 ⇒ αf ∈ F 0 for all α ∈ R.

• Select an integer M > 0 and let F (0) = 0.

• For j = 1, . . . , M:

$(1) Find fj ∈ F 0 minimizing N k=1 | f (x k ) + ∂ 2 r(y k , F (j-1) (x k ))| 2 over all f ∈ F 0 .$(2) Let f j = α j fj where α j minimizes

$N k=1 r(y k , F (j-1) (x k ) + α fj (x k )).$(3) Let F (j) = F (j-1) + f j .

• Return F = F (M) .

Remark 10.7 Importantly, the fact that F 0 is stable by scalar multiplication implies that the function fj satisfies

$N k=1 f (x k ) T ∂ 2 r(y k , F (j-1) (x k )) ≤ 0,$♦ that is, excepted in the unlikely case in which the above sum is zero, it is a direction of descent for the function U (because one could otherwise replace fj by -fj and improve the approximation of the gradient).

## Return to classification

A slight modification of this algorithm may also be applied to classification, provided that the classifier f is obtained by learning the conditional distribution, denoted g → p(g|x), of the output variable (assumed to take values in a finite set R Y ) given the input (assumed to take values in R X = R d ).

Our goal is to estimate an unknown target conditional distribution, µ, therefore taking the form µ(g|x) for g ∈ R Y and x ∈ R d . We assume that a family µ k , k = 1, . . . , N of distributions on the set R Y is observed, where each µ k is assumed to be an approximation of the unknown µ(•|x k ) (typically, µ k (g) = 1 g=y k , i.e., µ k = δ y k ). The risk function must take the form r(µ, µ ′ ) where µ, µ ′ ∈ S(R Y ), the set of probability distributions on R Y . We will work with

$r(µ, µ ′ ) = - g∈R Y µ(g) log µ ′ (g). One can note that r(µ, µ ′ ) = KL(µ∥µ ′ ) + r(µ, µ),$which is therefore minimal when µ ′ = µ. Moreover, in the special case

$µ k = δ y k , the empirical risk is R(p) = N k=1 r(µ k , p(•|x k )) = - N k=1 log p(y k |x k ),$so that minimizing it is equivalent to maximizing the conditional likelihood that was used for logistic regression.

Before applying the previous algorithm, one must address the issue that probability distributions do not form a vector space, and cannot be added to form new probability distributions. In Friedman [[75]](#b93), Hastie et al. [[87]](#b105), it is suggested to use the representation, which can be associated with any function

$F : (g, x) → F(g|x) ∈ R, p F (g|x) = e F(g|x)$h∈R Y e F(h|x) . Because the representation if not unique (p F = p F ′ if F -F ′ only depends on x), we will require in addition that h∈R Y F(h|x) = 0 for all x ∈ R d . The space formed by such functions F is now linear, and we can consider the empirical risk

$R(F) = - N k=1 g∈R Y µ k (g) log p F (g|x k ) = - N k=1 g∈R Y µ k (g)F(g|x k ) + N k=1 log         g∈R Y e F(g|x k )         .$One can evaluate the derivative of this risk with respect to a change on F(g|x k ), and a short computation gives

$∂R ∂F(g|x k ) = - N k=1 (µ k (g) -p F (g|x k )).$Now assume that a basic space F 0 of functions f : (g, x) → f (g|x) is chosen, such that all function in F 0 satisfy g∈R Y f (g|x) = 0 for all x ∈ R d . The gradient boosting algorithm then requires to minimize (in Step (1)):

$N k=1 g∈R Y (µ k (g) -p F (j-1) (g|x k ) -f (g|x k )) 2$with respect to all functions f ∈ F 0 . Given the optimal fj , the next step requires to minimize, with respect to α ∈ R:

$-α N k=1 g∈R Y µ k (g) fj (g|x k ) + N k=1 log         g∈R Y e F (j-1) (g|x k )+α fj (g|x k )        $. This is a scalar convex problem that can be solved, e.g., using gradient descent.

## Gradient tree boosting

We now specialize to the situation in which the set F 0 contains regression trees. In this situation, the general algorithm can be improved by taking advantage of the fact that the predictors returned by such trees are piecewise constant functions, where the regions of constancy are associated with partitions C of R d defined by the leaves of the trees. In particular, fj (x) in Step (1) takes the form

$fj (g|x) = J A∈C fj,A (g)1 x∈A .$The final f at Step (2) should therefore take the form A∈C α fj,A (g)1 x∈A but not much additional complexity is introduced by freely optimizing the values of f j on A, that is, by looking at f in the form

$A∈C f j,A (g)1 x∈A$where the values f j,A (g) optimize the empirical risk. This risk becomes

$- N k=1 A∈C g∈R Y µ k (g)f j,A (g)1 x k ∈A + N k=1 A∈C log         g∈R Y e F (j-1) (g|x k )+f j,A (g)         1 x k ∈A .$The values f j,A (g), g ∈ R Y can therefore be optimized separately, minimizing

$- k=1:x k ∈A g∈R Y µ k (g)f j,A (g) + k:x k ∈A log         g∈R Y e F (j-1) (g|x k )+f j,A (g)         1 x k ∈A .$This is still a convex program, which has to be run at every leaf of the optimized tree. If computing time is limited (or for large-scale problems), the determination of f j,A (g) may be restricted to one step of gradient descent starting at f j,A = 0. A simple computation indeed shows that the first derivative of the function above with respect to f j,A (g) is

$a A (g) = - k:x k ∈A (µ k (g) -p F (g|x k )).$The derivative of this expression with respect to f j,A (g) (for the same g)

$is b A (g) = k:x k ∈A p F (g|x k )(1 -p F (g|x k )).$The off-diagonal terms in the second derivative are, for g h,

$- k:x k ∈A p F (g|x k )p F (h|x k ).$In Friedman et al. [[74]](#b92), it is suggested to use an approximate Newton step, where the off-diagonal terms in the second derivative are neglected. This corresponds to minimizing

$g∈R Y a A (g)f j,A (g) + 1 2 g∈R Y b A (g)f j,A (g) 2 .$The solution is (introducing a Lagrange multiplier for the constraint g f j,A (g) = 0)

$f j,A (g) = - a A (g) -λ b A (g) with λ = g∈R Y a A (g)/b A (g) g∈R Y 1/b A (g)$.

A small value ϵ can be added to b A to avoid divisions by zero. We refer the reader to Friedman et al. [[74]](#b92), Friedman [[75]](#b93), Hastie et al. [[87]](#b105) for several variations on this basic idea. Note that an approximate but highly efficient implementation of boosted trees, called XGBoost, has been developed in Chen and Guestrin [[53]](#b71).

Chapter 11

## Iterated Compositions of Functions and Neural Nets 11.1 First definitions

We now discuss a class of methods in which the predictor f is built using iterated compositions, with a main application to neural nets. We will structure these models using directed acyclic graphs (DAG). These graphs are composed with a set of vertexes (or nodes) V = {0, . . . , m + 1} and a collection L of directed edges i → j between some vertexes. If an edge exists between i and j, one says that i is a parent of j and j a child of i and will use the notation pa(i) (resp. ch(i)) denote the set of parents (resp. children) of i. The graphs we consider must satisfy the following conditions:

(i) No index is a descendant of itself, i.e., that the graph is acyclic.

(ii) The only index without parent is i = 0 and the only one without children in i = m + 1.

To each node i in the graph, one associates a dimension d i and a variable z i ∈ R d i .

The root node variable, z 0 = x, is the input and z m+1 is the output. One also associates to each node i 0 a function ψ i defined on the product space j∈pa(i) R d j and taking values in R d i . The input-output relation is then defined by the family of equations:

$z i = ψ i (z pa(i) )$where z pa(i) = (z j , j ∈ pa(i)). Since there is only one root and one terminal node, these iterations implement a relationship y = z m+1 = f (x), with z 0 = x. We will refer to the z 1 , . . . , z m as the latent variables of the network.

Each function ψ i is furthermore parametrized by an s i -dimensional vector w i ∈ R s i , so that we will write

$z i = ψ i (z pa(i) ; w i ).$We let W denote the vector containing all parameters w 1 , . . . , w m+1 , which therefore has dimension s = s 1 + • • • + s m+1 . The network function f is then parametrized by W and we will write y = f (x; W ).

## Neural nets 11.2.1 Transitions

Most neural networks iterate functions taking the form (1 +   j∈pa(i) d j )-dimensional); ρ is defined on and takes values in R, and we make the abuse of notation, for any d and

$ψ i (z; w) = ρ(bz + β 0 ), z ∈ R d j where b is a d i × ( j∈pa(i) d j ) matrix and β 0 ∈ R d i (so that w = (b, β 0 ) is s i = d i$$u ∈ R d ρ(u) =           ρ(u (1) ) . . . ρ(u (d) )           .$The most popular choice for ρ is the positive part, or ReLU (for rectified linear unit), given by ρ(t) = max(t, 0). Other common choices are ρ(t) = 1/(1 + e -t ) (sigmoid function), or ρ(t) = tanh(z).

Residual neural networks (or ResNets [[89]](#b107)) are discussed in section 11.6. They iterate transitions between inputs and outputs of same dimension, taking

$z i+1 = z i + ψ(z i ; w). (11.1)$
## Output

The last node of the graph provides the prediction, y. Its expression depends on the type of predictor that is learned

• For regression, y can be chosen as an affine function of is its parents: z m+1 = bz pa(m+1) + a 0 .

• For classification, one can also use a linear model z m+1 = bz pa(m+1) + a 0 where z m+1 is q-dimensional and let the classification be argmax(z (i) m+1 , i = 1, . . . , q). Alternatively, one uses "softmax" transformation, with

$z (i) m+1 = e ζ (i) m+1 q j=1 e ζ (j) m+1 with ζ m+1 = bz pa(m+1) + a 0 .$
## GEOMETRY

## Image data

Neural networks have achieved top performance when working with organized structures such as images. A typical problem in this setting is to categorize the content of the image, i.e., return a categorical variable naming its principal element(s). Other applications include facial recognition or identification. In this case, the transition function can take advantage of the 2D structure, with some special terminology.

Instead of speaking of the total dimension, say, d, of the considered variables, writing z = (z (1) , . . . , z (d) ), images are better represented with three indices z(u, v, λ) where u = 1, . . . , U and U is the width of the image, v = 1, . . . , V and V is the height of the image, λ = 1, . . . , Λ and Λ is the depth of the image. (With this notation d = U V Λ.) Typical images have length and width of one or two hundred pixels, and depth Λ = 3 for the three color channels. This three-dimensional structure is conserved also for latent variables, with different dimensions. Deep neural networks often combine compression in width and height with expansion in depth while transitioning from input to output.

The linear transformation b mapping one layer with dimensions U i , V i , Λ i to another with dimensions U i+1 , V i+1 , Λ i+1 is then preferably seen as a collection of numbers: b(u ′ , v ′ , λ ′ , u, v, Λ) so that the transition from z i to z i+1 is given by

$z i+1 (u ′ , v ′ , λ ′ ) = ρ         β 0 (u ′ , v ′ , λ ′ ) + U i u=1 V i v=1 Λ i λ=1 b(u ′ , v ′ , λ ′ , u, v, λ)z i (u, v, λ)         .$For images, it is often preferable to use convolutional transitions, providing convolutional neural networks ( [[116,](#b134)[115]](#b133), or CNNs. If U i = U i+1 and V i = V i+1 , such a transition requires that b(u ′ , v ′ , λ ′ , u, v, λ) only depends on λ, λ ′ and on the differences u ′ -u and v -v ′ . In general, one also requires that b(u ′ , v ′ , λ ′ , u, v, λ) is non-zero only if |u ′ -u| and |v ′ -v| are both less than a constant, typically a small number. Also, there is generally little computation across depths: each output at depth λ ′ only uses values from a single input depth. These restrictions obviously reduce dramatically the number of free parameters involved in the transition.

After one or a few convolutions, the dimension is often reduced by a "pooling" operation, dividing the image into small non-overlapping windows and replacing each such window by a single value, either the max (max-pooling) or the average.

## Geometry

In addition to the transitions between latent variables and resulting changes of dimension, the structure of the DAG defining the network is an important element in the design of a neural net. The simplest choice is a purely linear structure (as shown in Figure [11](#fig_120).1), as was, for example, used for image categorization in [[110]](#b128).

More complex architectures have been introduced in recent years. Their design is in a large part heuristic and based on an analysis of the kind of computation that should be done in the network to perform a particular task. For example, an architecture used for image segmentation in summarized in fig. [11.](#fig_120)2.

An important feature of neural nets is their modularity, since "simple" architectures can be combined (e.g., by placing the output of a network as input of another one) and form a more complex network that still follows the basic structure defined above. One example of such a building block is the "attention module," which take as input three vectors Q, K, V (for query, key, and value) and return softmax(QK T )V .

These modules are fundamental elements of "transformer networks" [[198]](#b216), that are used, among other tasks, for automatic translation.

## Objective function 11.4.1 Definitions

We now return to the general form of the problem, with variables z 0 , . . . , z m+1 satisfying

$z i = ψ i (z pa(i) ; w i ) Let T = (x 1 , y 1 , . . . , x N , y N ) denote the training data.$For regression problems, the objective function minimized by the algorithm is typically the empirical risk, the simplest choice being the mean square error, which gives

$F(W ) = 1 N N k=1 |y k -z k,m+1 (W )| 2 . with z k;m+1 (W ) = f (x k ; W ).$For classification, with the dimension of the output variable equal to the number of classes and the decision based on the largest coordinate, one can take (letting z k,m+1 (i; W ) denote the ith coordinate of z k,m+1 (W )):

$F(W ) = 1 N N k=1        -z k,m+1 (y k ; W ) + log q i=1 exp(z k,m+1 (i; W ))        .$This objective function is similar to that minimized in logistic regression.

## Differential

General computation. The computation of the differential of F with respect to W may look daunting, but it has actually a simple structure captured by the backpropagation algorithm. Even if programming this algorithm can often be avoided by using an automatic differentiation software, it is important to understand how it works, and why the implementation of gradient-descent algorithms remains feasible.

Consider the general situation of minimizing a function G(W , z) over W ∈ R s and z ∈ R r , subject to a constraint γ(W , z) = 0 where γ is defined on R s × R r and takes values in R r (here, it is important that the number of constraints is equal to the dimension of z). We will denote below by ∂ W and ∂ z the derivatives of these functions with respect to the multi-dimensional variables W and z. We make the assumptions that ∂ z γ, which is an r × r matrix, is invertible, and that the constraints can be solved to express z as a function of W , that we will denote Z (W ). This allows us to define the function F(W ) = G(W , Z (W )) and we want to compute the gradient of F. (Clearly, the function F in the previous section satisfies these assumptions). Taking h ∈ R s , we have

$dF(W )h = ∂ W Gh + ∂ z G dZ h.$Moreover, since γ(W , Z ) = 0 by definition of Z , we have

$∂ W γh + ∂ z γ dZ h = 0, so that dF(W )h = ∂ W Gh -∂ z G ∂ z γ -1 ∂ W γh.$Let p ∈ R r be the solution of the linear system

$∂ z γ T p = ∂ z G T . Then, dF(W )h = (∂ W G -p T ∂ W γ)h or ∇F = ∂ W G T -∂ W γ T p.$Note that, introducing the "Hamiltonian"

$H (p, z, W ) = p T γ(W , z) -G(W , z),$one can summarize the previous computation with the system

$           ∂ p H = 0 ∂ z H = 0 ∇F = -∂ W H T .$Application: back-propagation. In our case, we are minimizing a function of the form

$G(W , z 1 , . . . , z N ) = 1 N N k=1 r(y k , z k,m+1 ) subject to constraints z k,i+1 = ψ i (z k,pa(i) ; w i ), i = 0, . . . , m, z k,0 = x k .$We focus on one of the terms in the sum, therefore fixing k, that we will temporarily drop from the notation.

So, we evaluate the gradient of G(W , z) = r(y, z m+1 ) with z i+1 = ψ i (z pa(i) ; w i ), i = 0, . . . , m, z 0 = x. With the notation of the previous paragraph, we take γ = (γ 1 , . . . , γ m+1 ) with

$γ i (W , z) = ψ i (z pa(i) ; w i ) -z i$These constraints uniquely define z as a function of W , which was one of our assumptions. For the derivative, we have, for u = (u 1 , . . . , u m+1 ) ∈ R r (with

$r = d 1 + • • • + d m+1 , u i ∈ R d i )$, and for i = 1, . . . , m + 1

$∂ z γ i u = j∈pa(i) ∂ z j ψ i (z pa(i) ; w i )u j -u i$Taking p = (p 1 , . . . , p m+1 ) ∈ R r , we get

$p T ∂ z γu = m+1 i=1 j∈pa(i) p T i ∂ z j ψ i (z pa(i) ; w i )u j - m+1 i=1 p T i u i = m+1 j=1 i∈ch(j) p T i ∂ z j ψ i (z pa(i) ; w i )u j - m+1 j=1 p T j u j$This allows us to identify ∂ z γ T p as the vector g = (g 1 , . . . , g m+1 ) with

$g j = i∈ch(j) ∂ z j ψ i (z pa(i) ; w i ) T p i -p j .$For j = m + 1 (which has no children), we get g m+1 = -p m+1 , so that the equation ∂ z γ T p = g can be solved recursively by taking p m+1 = -g m+1 and propagating backward, with

$p j = -g j + i∈ch(j) ∂ z j ψ i (z pa(i) ; w i ) T p i for j = m, . . . , 1.$To compute the gradient of G, the propagation has to be applied to g = ∂ z G. Since G only depends on z m+1 , we have g m+1 = ∂ z m+1 r(y, z m+1 ) and g j = 0 for j = 1, . . . , m. Moreover, G does not depend on W , so that ∂ W G = 0. We have

$∂ W γ i = ∂ w i ψ i (z pa(i) , w i ) yielding ∂ W γ T p = (ζ 1 , . . . , ζ m ) with ζ j = ∂ w j ψ j (z pa(j) , w j ) T p j .$We can now formulate an algorithm that computes the gradient of F with respect to W , reintroducing training data indexes in the notation. Algorithm 11.1 (Back-propagation) Let (x 1 , y 1 , . . . , x N , y N ) be the training set and R k (z) = r(y k , z) so that

$F(W ) = 1 N N k=1 R k (z k,m+1 (W )) with z k,m+1 (W ) = f (x k , W ).$Let W be a family of weights. The following steps compute ∇F(W ).

1. For all k = 1, . . . , N and all i = 1, . . . , m + 1, compute z k,i (W ) (forward computation through the network).

## Initialize variables

$p k,m+1 = -∇R k (z k,m+1 (W )), k = 1, . . . , N .$3. For all k = 1, . . . , N and all j = 1, . . . , m, compute p k,j using iterations

$p k,j = i∈ch(j) ∂ z j ψ i (z k,pa(i) , w i ) T p k,i . 4. Let ∇F(W ) = - 1 N N k=1 m+1 i=1 D T i ∂ w i ψ i (z k,pa(i) , w i ) T p k,i ,$where D i is the s i × s matrix such that D i h = h i .

## Complementary computations

The back-propagation algorithm requires the computation of the gradient of the costs R k and of the derivatives of the functions ψ i , and this can generally be done in closed form, with relatively simple expressions.

$• If R k (z) = |y k -z| 2 (which is the typical choice for regression models) then ∇R k (z) = 2(z -y k ). • In classification, with R k (z) = -z(y k ) + log q i=1 exp(z (i) ) , one has ∇R k (z) = -u y k + exp(z) q i=1 exp(z (i) )$where u y k ∈ R d is the vector with 1 at position y k and zero elsewhere, and exp(z) is the vector with coordinates exp(z (i) ), i = 1, . . . , d.

• For dense transition functions in the form ψ(z; w) = ρ(bz

$+ β 0 ) with w = (β 0 , b), then ∂ z ψ(z, w) = diag(ρ ′ (β 0 + bz))b so that ∂ z ψ(z, w) T p = b T diag(ρ ′ (β 0 + bz))p • Similarly ∂ w ψ(z, w) T p = diag(ρ ′ (β 0 + bz))p, diag(ρ ′ (β 0 + bz))pz T .$Note that neural network packages implement these functions (and more) automatically. 

## Stochastic Gradient Descent

$= (ξ 1 , . . . , ξ N ) such that ξ k ∈ {0, 1} and N k=1 ξ k = ℓ. Define H(W , ξ) = ∇ W        1 ℓ N k=1 ξ k r(y k , f (x k , W ))        = 1 ℓ N k=1 ξ k ∇ W r(y k , f (x k , W ))$where ξ follows the uniform distribution on B ℓ . Consider the stochastic approximation algorithm:

$W n+1 = W n -γ n+1 H(W n , ξ n+1 ). (11.2) Because E(ξ k ) = ℓ/N , we have E(H(W , ξ)) = ∇ W E T (f (•, W )) and (11.$2) provides a stochastic gradient descent algorithm to which the discussion in section 3.3 applies. Such an approach is often referred to as "mini-batch" selection in the deep-learning literature, since it correspond to sampling ℓ examples from the training set without replacement and only computing the gradient of the empirical loss computed from these examples.

## Dropout

Introduced for deep learning in Srivastava et al. [[181]](#b199), "dropout" is a learning paradigm that brings additional robustness (and, maybe, reduces overfitting risks) to massively parametrized predictors.

Assume that a random perturbation mechanism of the model parameters has been designed. We will represent it using a random variable η (interpreted as noise) and a transformation W ′ = ϕ(W , η) describing how η affects a given weight configuration W to form a perturbed one W ′ . In order to shorten notation, we will write ϕ(W , η) = η • W , borrowing the notation for a group action from group theory. As a typical example, η can be chosen as a vector of Bernoulli random variables (therefore taking values in {0,1}), with same dimension as W and one can simply let η •W = η ⊙W be the pointwise multiplication of the two vectors. This corresponds to replacing some of the parameters by zero ("dropping them out") while keeping the others unchanged. One generally preserves the parameters of the final layer (g m ), so that the corresponding η's are equal to one, and let the other ones be independent, with some probability p of being one, say, p = 1/2.

Returning to the general case, in which η is simply assumed to be a random variable with known probability distribution, the dropout method replaces the objective function

$F(W ) = E T (f (•, W )) by its expectation over perturbed predictors G(W ) = E(E T (f (•, η • W )))$where the expectation is taken with respect to the random variable η. While this expectation cannot be computed explicitly, its minimization can be performed using stochastic gradient descent, with

$W n+1 = W n -γ n+1 L(W n , η n+1 ),$where η 1 , η 2 , . . . is a sequence of independent realizations of η and

$L(W , η) = ∇ W (E T (f (•, η • W ))) . Then, averaging in η L(W ) = E(∇ W F(η ⊙ W )) = ∇G(W ).$In the special case where η • W is just pointwise multiplication, then

$L(W , η) = η ⊙ ∇F(η ⊙ W ).$So this quantity can be evaluated by using back-propagation to compute ∇F(η • W ) and multiplying the result by η pointwise. Obviously, random weight perturbation can be combined with mini-batch selection in a hybrid stochastic gradient descent algorithm, the specification of which being left to the reader. We also note that stochastic gradient descent in neural networks is often implemented using the ADAM algorithm (section 3.3.3).

11.6 Continuous time limit and dynamical systems 11.6.1 Neural ODEs Equation (11.1) expresses the difference of the input and output of a neural transition as a non-linear function f (z; w) of the input. This strongly suggests passing to continuous time and replacing the difference by a derivative, i.e., replacing the neural network by a high-dimensional parametrized dynamical system. The continuous model then takes the form [52] ∂ t z(t) = ψ(z(t); w(t)) (11.3)

where t varies in a a fixed interval, say, [0, T ]. The whole process is parametrized by W = (w(t), t ∈ [0, T ]). We need to assume existence and uniqueness of solutions of (11.3), which usually restricts the domain of admissibility of parameters W .

Typical neural transition functions are Lipschitz functions whose constant depend on the weight magnitude, i.e., are such that

$|ψ(z, w) -ψ(z ′ , w)| ≤ C(w)|z -z ′ | (11.4)$where C is a continuous function of W . For example, for ψ(z, w) = ρ(bz + β 0 ), w = (b, β 0 ), one can take C(w) = C ρ |b| op . The Caratheodory theorem [[17]](#b35) implies that solutions are well-defined as soon as

$T 0 C(w(t))dt < ∞.(11.5)$This is a relatively mild requirement, on which we will return later. Assuming this, we can consider z(T ) as a function of the initial value, z(0) = x and of the parameters, writing z(T ) = f (x, W ).

Given a training set, we consider the problem of minimizing

$F(W ) = 1 N N k=1 r(y k , f (x k , W )). (11.6)$The discussion in section section 11.4.2 applies-formally, at least-to this continuous case, and we can consider the equivalent problem of minimizing

$G(W , z 1 , . . . , z N ) = 1 N N k=1 r(y k , z k (T )) with ∂ t z k (t) = ψ(z k (t); w(t)), z k (0) = x k .$Once again, we consider each k separately, which boils down to considering N = 1 and we drop the index k from the notation, letting

$F(W ) = r(y, f (x, W )) G(W , z) = r(y, z(T )).$We define γ(W , z) to return the function

$t → γ(W , z)(t) = ψ(z(t); w(t)) -∂ t z(t).$Let p : [0, T ] → R d . We want to determine the expression of u = ∂ z γ T p, which satisfies

$T 0 u(t) T δz(t)dt = T 0 p(t) T (∂ z ψ(z(t), w(t))δz(t) -∂ t δz(t))dt$After an integration by parts, the r.h.s. becomes

$-p(T ) T δz(T ) + T 0 ∂ t p(t) T δz(t)dt + T 0 p(t) T ∂ z ψ(z(t), w(t))δz(t))dt which gives u(t) = -p(T )δ T + ∂ t p(t) + ∂ z ψ(z(t), w(t)) T p(t).$The equation

$∂ z γ T p = ∂ z G T therefore gives -p(T )δ T + ∂ t p(t) + ∂ z ψ(z(t), w(t)) T p(t) = ∂ 2 r(y, z(T ))δ T ,$so that p satisfies p(T ) = -∂ 2 r(y, z(T )) and

$∂ t p(t) = -∂ z ψ(z(t), w(t)) T p(t). (11.7)$We have

$∂ W G = 0 and v = ∂ W γ T p satisfies T 0 v(t) T δw(t)dt = T 0 p(t) T ∂ w ψ(z(t), w(t))δw(t)dt so that ∇F(W ) = (t → -∂ w ψ(z(t), w(t)) T p(t)).$This informal derivation (more work is needed to justify the existence of various differentials in appropriate function spaces) provides the continuous-time version of the back-propagation algorithm, which is also known as the adjoint method in the optimal control literature [[91,](#b109)[123]](#b141). In that context, z represents the state of the control system, w is the control and p is called the costate, or covector. We summarize the gradient computation algorithm, reintroducing N training samples. Algorithm 11.2 (Adjoint method for neural ODE) Let (x 1 , y 1 , . . . , x N , y N ) be the training set and R k (z) = r(y k , z) so that

$F(W ) = 1 N N k=1 R k (z k (T , W )) with ∂ t z k = ψ(z k , W ), z k (0) = x k .$Let W be a family of weights. The following steps compute ∇F(W ).

1. For all k = 1, . . . , N and all t ∈ [0, T ], compute z k (t, W ) (forward computation through the dynamical system).

## Initialize variables p

$k (T ) = -∇R k (z k (T , W ))/N , k = 1, . . . , N .$3. For all k = 1, . . . , N and all j = 1, . . . , m, compute p k (t) by solving (backwards in time)

$∂ t p k (t) = -∂ z ψ(z k (t), w(t)) T p k (t). 4. Let, for t ∈ [0, T ], ∇F(W )(t) = - N k=1 ∂ w ψ(z k (t), w(t)) T p k (t).$Of course, in numerical applications, the forward and backward dynamical systems need to be discretized, in time, resulting in a finite number of computation steps. This can be done explicitly (for example using basic Euler schemes), or using ODE solvers [[52]](#b70) available in every numerical software.

## Adding a running cost

Optimal control problems are usually formulated with a "running cost" that penalizes the magnitude of the control, which in our case is provided by the function W : t → w(t). Penalties on network weights are rarely imposed with discrete neural networks, but, as discussed above, in the continuous setting, some assumptions on the function W , such as [(11.5)](#), are needed to ensure that the problem is well defined.

It is therefore natural to modify the objective function in [(11.6](#)) by adding a penalty term ensuring the finiteness of the integral in (11.5), taking, for example, for some λ > 0,

$F(W ) = λ T 0 C(w(t)) 2 dt + N k=1 r(y k , f (x k , W )). (11.8)$The finiteness of the integral of the squared C(w) 2 implies, by Cauchy-Schwartz, the integrability of C(w) itself, and usually leads to simpler computations.

If C(w) is known explicitly and is differentiable, the previous discussion and the back-propagation algorithm can be adapted with minor modifications for the minimization of [(11.8)](#). The only difference appears in Step 4 of Algorithm 11.2, with

$∇F(W )(t) = 2λ∇C(w(t)) - 1 N N k=1 ∂ w ψ(z k (t), w(t)) T p k (t).$Computationally, one should still ensure that C and its gradient are not too costly to compute. If ψ(z, w) = ρ(bz + β 0 ), w = (b, β 0 ), the choice C(w) = C ρ |b| op is valid, but not computationally friendly. The simpler choice C(w) = C ρ |b| 2 is also valid, but cruder as an upper-bound of the Lipschitz constant. It leads however to straightforward computations.

The addition of a running cost to the objective is important to ensure that any potential solution of the problem leads to a solvable ODE. It does not guarantee that an optimal solution exists, which is a trickier issue in the continuous setting than in the discrete setting. This is an important theoretical issue, since it is needed, for example, to ensure that various numerical discretization schemes lead to consistent approximations of a limit continuous problem. The existence of minimizers is not known in general for ODE networks. It does hold, however, in the following nonparametric (i.e., weight-free) context that we now describe.

The function ψ in the r.h.s. of (11.3), is, for any fixed w, a function that maps z ∈ R d to a vector ψ(z, w) ∈ R d . Such functions are called vector fields on R d , and the collection ψ(•, w), w ∈ R s is a parametrized family of vector fields.

The non-parametric approach replaces this family of functions by a general vector field, v so that the time-indexed parametrized family of vector fields (t → ψ(•, w(t))) becomes an unconstrained family (t → f (t, •)). Following the general non-parametric framework in statistics, one needs to define a suitable function space for the vector fields, and use a penalty in the objective function.

We will assume that, at each time, f (t, •) belongs to a reproducing kernel Hilbert space (RKHS), as introduced in chapter 6. However, because we are considering a space of vector fields rather than scalar-valued functions, we need work with matrixvalued kernels [[5]](#b23), for which we give a definition that generalizes definition 6.1 (which corresponds to q = 1 below). T for all x and y in R d .

$Definition 11.1 A function K : R d × R d → M q (R) satisfying [K1-vec] K is symmetric, namely K(x, y) = K(y, x)$[K2-vec] For any n > 0, for any choice of vectors λ 1 , . . . , λ n ∈ R q and any x 1 , . . . ,

$x n ∈ R d , one has n i,j=1 λ T i K(x i , x j )λ j ≥ 0. (11.9)$is called a positive (matrix-valued) kernel.

One says that the kernel is positive definite if the sum in (6.1) cannot vanish, unless (i)

$λ 1 = • • • = λ n = 0 or (ii) x i = x j for some i j.$If κ is a "scalar kernel" (satisfying definition 6.1), then K(x, y) = κ(x, y)Id R q is a matrix-valued kernel.

A reproducing kernel Hilbert space of vector-valued functions is a Hilbert space H of functions from R d to R q such that there exists a reproducing kernel K : R d × R d → M q (R) with the following properties

$[RKHS1] For all x ∈ R d and λ ∈ R q , K(•, x)λ belongs to H, [RKHS2] For all h ∈ H, x ∈ R d and λ ∈ R q , ⟨h , K(•, x)λ⟩ H = λ T h(x) .$Proposition 6.5 remains valid in the for vector-valued RKHS, with the following modifications: λ 1 , . . . , λ N and α 1 , . . . , α N are q-dimensional vectors and the matrix K(x 1 , . . . , x N ) is now an N q × N q block matrix, with q × q blocks given by K(x k , x l ), k, l = 1, . . . , N .

Returning to the specification of the nonparametric control problem, we will assume that a vector-valued RKHS, H, has been chosen, with q = d in definition 11.1. We further assume that elements of H are Lipschitz continuous, with

$|v(z) -v(z)| ≤ C∥v∥ H |z -z|(11.10)$for some constant C and all v ∈ H. We note that, for every λ ∈ R d ,

$|λ T (v(z) -v(z))| 2 = |⟨v , K(•, z)λ -K(•, z)λ⟩ H | 2 ≤ ∥v∥ 2 H ∥K(•, z)λ -K(•, z)λ∥ 2 H = ∥v∥ 2 H (λ T K(z, z)λ -2λ T K(z, z)λ + λ T K(z, z)) ≤ |λ| 2 ∥v∥ 2 H |K(z, z) -2K(z, z) + K(z, z)| .$This shows that (11.10) can be derived from regularity properties of the kernel, namely, that

$|K(z, z) -2K(z, z) + K(z, z)| ≤ C|z -z| 2$for some constant C and all z, z ∈ R d . This property is satisfied by most of the kernels that are used in practice.

Let η : t → η(t) be a function from [0, 1] to H. This means that, for each t, η(t) is a vector field x → η(t)(x) on R d , and we will write indifferently η(t) and η(t, •), with a preference for η(t, x) rather than η(t)(x). We consider the objective function

$F(f ) = λ T 0 ∥η(t)∥ 2 H dt + 1 N N k=1 r(y k , z k (1)), (11.11) with ∂ t z k (t) = η(t, z k (t)), z k (0) = x k .$To compare with (11.8), the finite-dimensional w ∈ R s is now replaced with an infinite-dimensional parameter, η, and the transition ψ(z, w) becomes η(z).

Using the vector version of proposition 6.5 (or the kernel trick used several times in chapters 7 and 8), one sees that there is no loss of generality in replacing η(t) by its projection onto the vector space

$V (t) =        N l=1 K(•, z l (t))w l : w 1 , . . . , w N ∈ R d        . Noting that, if η(t) takes the form η(t) = N l=1 K(•, z l (t))w l (t), then ∥η(t)∥ 2 H = N k,l=1 w k (t) T K(z k (t), z l (t))w l (t).$This allows us to replace the infinite-dimensional parameter η by a family W = (w(t), t ∈ [0, T ] with w(t) = (w k (t), k = 1, . . . , N ). The minimization of F in [(11.11](#)) can be replaced by that of

$F(W ) = λ T 0 N k,l=1 w k (t) T K(z k (t), z l (t))w l (t)dt + 1 N N k=1 r(y k , z k (1)),(11.12)$with

$∂ t z k (t) = N l=1 K(z k (t), z l (t))w l (t).$This optimal control problem has a similar form to that considered in [(11.8)](#), where the running cost C(w) 2 is replaced by a cost that depends on the control (still denoted w) and the state z. The discussion in section section 11.6.1 can be applied with some modifications. Let K(z) be the dN × dN matrix formed with d × d blocks K(z k (t), z l (t)) and w(t) the dN -dimensional vector formed by stacking w 1 , . . . , w N . Let

$G(W , z) = λ T 0 w(t) T K(z(t))w(t)dt + 1 N N k=1 r(y k , z k (1)) and γ(W , z)(t) = K(z(t))w(t) -∂ t z(t) .$The backward ODE in step 3. of Algorithm 11.2 now becomes

$∂ t p k (t) = -∂ z k (w(t) T K(z(t))p(t)) + λ∂ z k (w(t) T K(z(t))w(t)) for k = 1, . . . , N . Step 4. becomes (for t ∈ [0, T ]), ∇F(W )(t) = K(z(t))(2λ -p(t)).$The resulting algorithm was introduced in [[209]](#b227). It has the interesting property (shared with neural ODE models with smooth controlled transitions) to determine an implicit diffeomorphic transformation of the space, i.e., the function x → f (x; W , z) = z(T ) which returns the solution at time T of the ODE

$∂ t z(t) = N l=1 K(z(t), z l (t))w l (t)$(or ∂z(t) = ψ(z(t); w(t)) for neural ODEs) is smooth, invertible, with a smooth inverse.

## Chapter 12

## Monte-Carlo Sampling

The goal of this section is to describe how, from a basic random number generator that provides samples from a uniform distribution on [0, 1], one can generate samples that follow, or approximately follow, complex probability distributions on finite or general spaces. This, combined with the law of large numbers, permits to approximate probabilities or expectations by empirical averages over a large collection of generated samples.

We assume that as many as needed independent samples of the uniform distribution are available, which is only an approximation of the truth. In practice, computer programs are only able to generate pseudo-random numbers, which are highly chaotic recursive sequences, but still deterministic. Also, these numbers are generated as integers, which only provide, after normalization, a distribution on a finite discretization of the unit interval. We will neglect these facts, however, and work as if the output of the function random (or any similar name) in a computer program is a true realization of the uniform distribution.

## General sampling procedures

Real-valued variables. We will use the following notation for the left limit of a function F at a given point z

$F(z -0) = lim y→z,y<z F(y)$assuming, of course that this limit exists (which is always true, for example when F is non-decreasing). Recall that F is left continuous if and only if F = F( • -0). Moreover, it is easy to see that F( • -0) is left-continuous [1](#foot_13) . Note also that, if F is non-decreasing, one always has F(z) ≤ F(y -0) whenever z < y. The following proposition provides a basic mechanism for Monte-Carlo sampling.

Proposition 12.1 Let Z be a real-valued random variable with c.d.f.

$F Z . For u ∈ [0, 1], define F - X (u) = max{z : F Z (z -0) ≤ u}. Let U be uniformly distributed over [0, 1]. Then F - Z (U ) has the same distribution as Z. Proof Let A z = {u ∈ [0, 1] : F - Z (u) ≤ z}. Assume first that u < F Z (z). Then F Z (y -0) ≤ u implies that y ≤ z, since y > z would imply that F Z (z) ≤ F Z (y -0). This shows that sup{z ′ : F Z (z ′ -0) ≤ u} ≤ z, i.e., u ∈ A z .$Now, take u > F Z (z). Because c.d.f.'s are right continuous, there exists y > z such that u > F Z (y), which implies that F - Z (u) ≥ y and u A z .

We have therefore shown that [0,

$F Z (z)) ⊂ A z ⊂ [0, F Z (z)]. If U is uniformly dis- tributed on [0, 1], then P (U < F Z (z)) = P (U ≤ F Z (z)) = F Z (z), showing that P(F - Z (U ) ≤ z) = P(U ∈ A z ) = F Z (z).$
## ■

This proposition shows that one can generate random samples of a real-valued random variable Z as soon as one can compute F - Z and generate uniformly distributed variables. Note that, if F Z is strictly increasing, then F - Z = F -1 Z , the usual function inverse.

The proposition also shows how to sample from random variables taking values in finite sets. Indeed, if Z takes values in Ω Z = {z 1 , . . . , z n } with p i = P(Z = z i ), sampling from Z is equivalent to sampling from the integer valued random variable Z with P(

$Z = i) = p i . For this variable, F - Z (u) is the largest i such that p 1 + • • • + p i-1 ≤ u (this sum being zero if i = 1$), which provides the standard sampling scheme for discrete probability distributions.

## Rejection sampling

While the previous approach can be generalized to multivariate distributions, it quickly becomes unfeasible when the dimension gets large, excepting simple cases in which the variables are independent, or, say, Gaussian. Rejection sampling is a simple algorithm that allows, in some cases, for the generation of samples from a complicated distribution based on repeated sampling of a simpler one.

## Without loss of generality, we can assume that y

$′ ≥ z ′ , yielding |F(z -0) -F(y -0)| ≤ 2ϵ, showing the left continuity of F( • -0).$Let us assume that we want to sample from a variable Z taking values R Z , and that there exists a measure µ on R Z with respect to which the distribution of Z is absolutely continuous, i.e., so that this distribution has a density f Z with respect to µ. For example, R Z = R d , and f Z is the p.d.f. of Z with respect to Lebesgue's measire. Assume that g is another density functions (with respect to µ) from which it is "easy" to sample. Consider the following algorithm, which includes a function a : z → a(z) ∈ [0, 1] that will be specified later.  The probability of exiting at step 3 is ρ = R d g(z)a(z)µ(dz). So, the algorithm simulates a random variable with p.d.f.

$f (z) = g(z)a(z)(1 + (1 -ρ) + (1 -ρ) 2 + • • • ) = g(z)a(z) ρ .$As a consequence, in order to simulate f Z , one must choose a so that f Z (z) is proportional to g(z)a(z), which, (assuming that g(z) > 0 whenever f Z (z) > 0), requires that a(z) is proportional to f Z (z)/g(z). Since a(z) must take values in [0, 1], but should otherwise be chosen as large as possible to ensure that fewer iterations are needed, one should take

$a(z) = f Z (z) cg(z)$where c = max{f Z (z)/g(z) : z ∈ R d }, which must therefore be finite. This fully specifies a rejection sampling algorithm for f Z . Note that g is free to choose (with the restriction that f Z (z)/g(z) must be bounded), and should be selected so that sampling from it is easy, and the coefficient c above is not too large.

## Markov chain sampling

When dealing with high-dimensional distributions, the constant c in the previous procedure is typically extremely large, and the rejection-sampling algorithm becomes unfeasible, because it keeps rejecting samples for very long times. In such cases, one can use alternative simulation methods that iteratively updates the variable Z by making small changes at each step, resulting in a procedure that asymptotically converges to a sample of the target distribution. Such sampling schemes are usually described as Markov chains, leading to the name Markov-chain Monte Carlo (or MCMC) sampling.

Assume that we want to sample from a random variable that takes values in some (measurable) set B = R X . [2](#foot_14) A Markov chain is the probabilistic analogous of a recursive sequence X n+1 = Φ(X n ), which is fully defined by the function Φ : B → B and the initial value X 0 ∈ B.

## Definitions

For Markov chains, X 0 is a random variable, which therefore does not have a fixed value, but follows a probability distribution that we will generally denote µ 0 : P 0 (x) = P (X 0 = x). The computation of X n+1 given X n is not deterministic either, but given the conditional probabilities

$P n,n+1 (x, A) = P(X n+1 ∈ A | X n = x).$where A ⊂ B is measurable. The left-hand side of this equation, P n,n+1 is called a transition probability, according to the following definition. Definition 12.2 Let F 1 and F 2 be two sets equipped with σ -algebras A 1 and A 2 . A transition probability from

$F 1 to F 2 is a function p : F 1 × A 2 → [0, 1] such that, for all x ∈ F 1 , the function A → p(x, A) is a probability on F 2 and for all A ∈ A 2 , the function x → p(x, A), x ∈ F 1 , is measurable.$When F 2 is discrete, the probabilities are fully specified by their values on singleton sets, and we will write p(x, y) for p(x, {y}).

When P n,n+1 (x, •) does not depend on n, the Markov chain is said to be homogeneous. To simplify notation, we will restrict to homogeneous chains (and therefore only write P (x, A)), although some of the chains used in MCMC sampling may be inhomogeneous. This is not a very strong loss of generality, however, because inhomogeneous Markov chains can be considered as homogeneous by extending the space Ω on which they are defined to Ω × N, and defining the transition probability

$p (x, n), A × {r} = 1 r=n+1 p n,n+1 (x, A).$An important special case is when B is countable, in which case one only needs to specify transition probabilities for singletons A = {y}, and we will write

$p(x, y) = P (x, {y}) = P(X n+1 = y | X n = x)$for the p.m.f. associated with P (x, •).

Another simple situation is when B = R d and each P (x, •) has a p.d.f. that we will also denote as p(x, •). In this latter case, assuming that P 0 also have a p.d.f. that we will denote by µ 0 , the joint p.d.f. of (X 0 , . . . , X n ) on (R d ) n+1 is given by

$f (x 0 , x 1 , . . . , x n ) = µ 0 (x 0 )p(x 0 , x 1 ) • • • p(x n-1 , x n ).$(12.1)

The same expression holds for the joint p.m.f. in the discrete case.

In the general case (invoking measure theory), the joint distribution is also determined by the transition probabilities, and we leave the derivation of the expression to the reader. An important point is that, in both special cases considered above, and under some very mild assumptions in the general case , these transition probabilities also uniquely define the joint distribution of the infinite process (X 0 , X 1 , . . .) on B ∞ , which gives theoretical support to the consideration of asymptotic properties of Markov chains. In this discussion, we are interested in conditions ensuring that the chain asymptotically samples from a target probability distribution Q, i.e., whether P(X n ∈ A) converges to Q(A) (one says that X n converges in distribution to Q). In practice, Q is given or modeled, and the goal is to determine the transition probabilities. Note that the marginal distribution of X n is computed by integrating (or summing) (12.1) with respect to x 0 , . . . , x n-1 , which is generally computationally challenging.

Given a transition probability P on B, we will use the notation, for a function f : B → R:

$P f (x) = B f (y)P (x, dy).$If Q is a probability distribution on B, it will also be convenient to write

$Qf (x) = B f (y)Q(dy).$
## Convergence

We will denote P x (•) the conditional distribution P(• | X 0 = x) and P n (x, A) = P x (X n ∈ A), which is a probability distribution on B. The goal of Markov Chain Monte Carlo sampling is to design the transition probabilities such that P n x (A) converges to Q(A) when n tends to infinity. One furthermore wants to complete this convergence with a law of large numbers, ensuring that

$1 n n k=1 f (X k ) → B f (x)Q(dx)$when n → ∞, where X n is the generated Markov chain and f is Q-integrable.

Introduce the total variation distance between two probability measures on a given probability space,

$D var (µ 1 , µ 2 ) = sup A (µ 1 (A) -µ 2 (A)). (12.2)$where the supremum is taken over all measurable sets A. We will say that the Markov chain with transition P asymptotically samples from

$Q if lim n→∞ D var (P n (x, •), Q) = 0 (12.3) for Q-almost all x ∈ B.$The chain must satisfy specific conditions for this to be guaranteed.

We now discuss some properties of the total variation distance that will be useful later. First, we note that the supremum in the r.h.s. of (12.2) is achieved. Indeed, there exists a set A 0 such that, for all measurable sets A, µ

$1 (A ∩ A 0 ) ≥ µ 2 (A ∩ A 0 ) and µ 1 (A∩A c 0 ) ≤ µ 2 (A∩A c 0 ). If B is a finite set, it suffices to let A 0 = {x ∈ B : µ 1 (x) ≥ µ 2 (x)}; if both µ 1$and µ 2 have p.d.f.'s ϕ 1 and ϕ 2 with respect to Lebesgue's measure (with B = R d ), then one can take A 0 = {x ∈ B : ϕ 1 (x) ≥ ϕ 2 (x)}. In the general case, one can take µ = µ 1 + µ 2 so that µ 1 , µ 2 ≪ µ, and letting ϕ i = dµ i /dµ, also take

$A 0 = {x ∈ B : ϕ 1 (x) ≥ ϕ 2 (x)}.$(This is also a special case of the Hahn-Jordan decomposition of signed measures [[66]](#b84)).

## Now, it is clear that, for any

$A µ 1 (A) -µ 2 (A) = µ 1 (A ∩ A 0 ) -µ 2 (A ∩ A 0 ) + µ 1 (A ∩ A c 0 ) -µ 2 (A ∩ A c 0 ) ≤ µ 1 (A ∩ A 0 ) -µ 2 (A ∩ A 0 ) ≤ µ 1 (A ∩ A 0 ) -µ 2 (A ∩ A 0 ) + µ 1 (A c ∩ A 0 ) -µ 2 (A c ∩ A 0 ) = µ 1 (A 0 ) -µ 2 (A 0 ) showing that D var (µ 1 , µ 2 ) = µ 1 (A 0 ) -µ 2 (A 0 ).$The following proposition lists additional properties.

$Proposition 12.3 (i) If µ 1 , µ 2 have a densities ϕ 1 , ϕ 2$with respect to some positive measure µ (such as µ 1 + µ 2 ), then

$D var (µ 1 , µ 2 ) = 1 2 B |ϕ 1 (x) -ϕ 2 (x)|µ(dx).$In particular, if B is finite

$D var (µ 1 , µ 2 ) = 1 2 x∈B |µ 1 (x) -µ 2 (x)|.$(ii) For general B,

$D var (µ 1 , µ 2 ) = sup f B f (x)µ 1 (dx) - B f (x)µ 2 (dx) . (12.4)$where the supremum is taken over all measurable functions f taking values in [0, 1].

$(iii) If f : B → R is bounded, define the maximal oscillation of f by osc(f ) = sup{f (x) -f (y) : x, y ∈ B}.$Then

$D var (µ 1 , µ 2 ) = sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : osc(f ) ≤ 1 (iv) Conversely, for any bounded measurable f : B → R, osc(f ) = sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : D var (µ 1 , µ 2 ) ≤ 1 Proof If one takes A 0 = {x ∈ B : ϕ 1 (x) ≥ ϕ 2 (x)}, then D var (µ 1 , µ 2 ) = A 0 (ϕ 1 (x) -ϕ 2 (x))µ(dx) = A 0 |ϕ 1 (x) -ϕ 2 (x)|µ(dx).$But, because both µ 1 and µ 2 are probability measures

$B (ϕ 1 (x) -ϕ 2 (x))µ(dx) = 0 so that A c 0 (ϕ 1 (x) -ϕ 2 (x))µ(dx) = - A 0 (ϕ 1 (x) -ϕ 2 (x))µ(dx).$However, the l.h.s. is also equal to

$- A c 0 |ϕ 1 (x) -ϕ 2 (x)|µ(dx) so that B |ϕ 1 (x) -ϕ 2 (x)|µ(dx) = 2 A 0 (ϕ 1 (x) -ϕ 2 (x)) = 2D var (µ 1 , µ 2 ),$which proves (i).

To prove (ii), first notice that, for all A,

$µ 1 (A) -µ 2 (A) = B f (x)µ 1 (dx) - B f (x)µ 2 (dx) for f = 1 A , so that D var (µ 1 , µ 2 ) ≤ sup f B f (x)µ 1 (dx) - B f (x)µ 2 (dx) .$Conversely, using A 0 as above, and taking f with values in [0, 1]

$B f (x)µ 1 (dx) - B f (x)µ 2 (dx) = A 0 f (x)(µ 1 -µ 2 )(dx) + A c 0 f (x)(µ 1 -µ 2 )(dx) ≤ A 0 f (x)(µ 1 -µ 2 )(dx) ≤ A 0 (µ 1 -µ 2 )(dx) = D var (µ 1 , µ 2 )$This shows (ii). For (iii), one can note that, if f takes values in [0, 1], then osc(f ) ≤ 1 so that

$D var (µ 1 , µ 2 ) ≤ sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : osc(f ) ≤ 1 Conversely, take f such that osc(f ) ≤ 1, ϵ > 0 and y such that f (y) ≥ inf x f (x) + ϵ. Let f ϵ (x) = (f (x) -f (y) + ϵ)/(1 + ϵ), which takes values in [0, 1]. Then D var (µ 1 , µ 2 ) ≥ B f ϵ (x)µ 1 (dx) - B f ϵ (x)µ 2 (dx) = 1 1 + ϵ B f (x)µ 1 (dx) - B f (x)µ 2 (dx)$and since this is true for all ϵ > 0, we get

$B f (x)µ 1 (dx) - B f (x)µ 2 (dx) ≤ D var (µ 1 , µ 2 )$which completes the proof of (iii).

Using (iii), we find, for any µ 1 , µ 2 and any bounded f

$B f (x)µ 1 (dx) - B f (x)µ 2 (dx) ≤ D var (µ 1 , µ 2 )osc(f ) which shows that sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : D var (µ 1 , µ 2 ) ≤ 1 ≤ osc(f ).$However, taking µ 1 = δ x and µ 2 = δ y , so that D var (µ 1 , µ 2 ) = 0 is x = y and 1 otherwise, we get

$f (x) -f (y) = B f (x)µ 1 (dx) - B f (x)µ 2 (dx) ≤ sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : D var (µ 1 , µ 2 ) ≤ 1$which yields (iv) after taking the supremum with respect to x and y.

■

## Invariance and reversibility

If a Markov chain converges to Q, then Q must be an "invariant distribution," in the sense that, if X n ∼ Q for some n, then so does X n+1 and as a consequences all X m for n ≥ m. This can be seen by writing

$P n+1 (x, A) = P x (X n+1 ∈ A) = E x (P (X n , A)) = E P n (x,•) (P (•, A)) If P n (x,$•) (and therefore also P n+1 (x, •)) converges to Q, then passing to the limit above yields

$Q(A) = E Q (P (•, A))$and this states that, if X n ∼ Q, then so does X n+1 . If Q has a p.d.f. (resp. p.m.f.), say, q, this gives

$q(y) = R d p(x, y)q(x)dx, (resp. q(y) = x∈B p(x, y)q(x)).$So, if one designs a Markov chain with a target asymptotic distribution Q, the first thing to ensure is that Q is invariant.

While invariance leads to an integral equation for q, a stronger condition, called reversibility is easier to assess.

Assume that Q is invariant by P . Make the assumption that P (x, •) has a density p * with respect to Q (this is, essentially, no loss of generality, see argument below), so that

$P (x, A) = A p * (x, y)Q(dy).$Taking A = B above, we have

$B p * (x, y)Q(dy) = P (x, B) = 1 but we also have, because Q is invariant, that B p * (x, y)Q(dx) = Q(B) = 1.$One says that the density is "doubly stochastic" with respect to Q.

Conversely, if a transition probability P has a doubly stochastic density p * with respect to some probability Q on B, then Q is invariant by P , since

$B P (x, A)Q(dx) = B A p * (x, y)Q(dy)Q(dx) = A B p * (x, y)Q(dx)Q(dy) = A Q(dy) = Q(A).$The property of being doubly stochastic can be reinterpreted in terms of time reversal for Markov chains. Let Q 0 be an initial distribution for a Markov chain with transition P (not necessarily invariant) so that, for any n ≥ 0, the distribution of X n is Q n = Q 0 P n . Fixing any m > 0, we are interested in the reversed process Xk = X m-k . We first notice that the conditional distribution of X n given its future X n+1 , . . . , X m (with n < m) only depends on X n+1 , so that the reversed process is also Markov. Indeed, for any positive function f : B → R, g : B m-n → R, one has, using the fundamental properties of conditional expectations and the fact that

$(X n ) is a Markov chain, E(f (X n )g(X n+1 , . . . , X m )) = E (E(f (X n )g(X n+1 , . . . , X m ) | X n , X n+1 )) = E (f (X n )E(g(X n+1 , . . . , X m ) | X n , X n+1 )) = E (f (X n )E(g(X n+1 , . . . , X m ) | X n+1 )) = E (E(f (X n ) | X n+1 )E(g(X n+1 , . . . , X m ) | X n+1 )) = E (E(f (X n ) | X n+1 )g(X n+1 , . . . , X m )) . This shows that E(f (X n ) | X n+1 , . . . , X m ) = E(f (X n ) | X n+1 ),$which is what we wanted. To identify the conditional distribution of X n given X n+1 , we note that for any x ∈ B, the transition probability P (x, •) is absolutely continuous with respect to Q n+1 since

$Q n+1 (A) = B P (x, A)Q n (dx)$and the r.h.s. is zero only if P (x, A) = 0 Q n -almost everywhere [3](#foot_15) . This shows that there exists a function r n+1 : B × B → R such that, for all

$A P (x, A) = A r n+1 (x, y)Q n+1 (dy).$Given this point, one can write

$E(f (X n )g(X n+1 ) = B 2 f (x n )g(x n+1 )P (x n , dx n+1 )Q n (dx n ) = B 2 f (x n )g(x n+1 )r n+1 (x n , x n+1 )Q n+1 (dx n+1 )Q n (dx n ) = B B f (x n )r n+1 (x n , x n+1 )Q n (dx n ) g(x n+1 )Q n+1 (dx n+1 )$which shows that the conditional distribution of X n given X n+1 = x n+1 has density x n → r n+1 (x n , x n+1 ) relatively to Q n . Note that, for discrete probabilities, one has

$r n+1 (x, y) = P (x, y) Q n+1 (y)$and

$P(X n = x | X n+1 = y) = Q n (x)P (x, y) Q n+1 (y) . (12.5)$The formula is identical if both Q 0 and P (x, •) have p.d.f.'s with respect to a fixed reference measure µ on B (for example, Lebesgue's measure when B = R d ), denoting these p.d.f's by q 0 and p(x, •). Then, the p.d.f. of the distribution of X n given X n+1 = y is pn (y, x) = q n (x)p(x, y) q n+1 (y) [(12.6)](#) where q n is the p.d.f. of Q n . Note that the transition probabilities of the reversed Markov chain depend on n, i.e., the reversed chain is non-homogeneous in general.

However, if one assumes that Q 0 = Q is invariant by P , then Q n = Q for all n and therefore r n (x, y) = p * (x, y), using the previous notation. In this case, the reversed chain has transitions independent of n and its transition probability has density p * (x, y) = p * (y, x) with respect to Q. In the discrete case, letting p(x, y

$) = P (X n+1 = y | X n = x), we have p * (x, y) = p(x, y)/Q(y), so that the reversed transition (call it p) is such that p(x, y) Q(y) = p(y, x) Q(x) , i.e., Q(y)p(y, x) = Q(x) p(x, y). (12.7)$One retrieves easily the fact that if p is such that there exists Q and p such that (12.7) is satisfied, then (summing the equation over y) Q is an invariant probability for p.

Let Q be a probability on B. One says that the Markov chain (or the transition probability p) is Q-reversible if and only if p(x, •) has a density p * (x, •) with respect to Q such that p * (x, y) = p * (y, x) for all x, y ∈ B. Since such a density is necessarily doubly stochastic, Q is then invariant by p. Reversibility is equivalent to the property that, whenever X n ∼ Q, the joint distribution of (X n , X n+1 ) coincides with that of (X n+1 , X n ). Alternatively, Q-reversibility requires that for all A, B ⊂ B,

$A P (z, B)dQ(z) = B P (z, A)dQ(z).$(12.8)

In the discrete case, (12.8) is equivalent to the "detailed balance" condition:

$Q(y)p(y, x) = Q(x)p(x, y). (12.9)$While Q can be an invariant distribution for a Markov chain without that chain being Q-reversible, the latter property is easier to ensure when designing transition probabilities, and most sampling algorithms are indeed reversible with respect to their target distribution.

Remark 12.4 A simple example of non-reversible Markov chain with invariant probability Q is often obtained in practice by alternating two or more Q-reversible transition probabilities. Assume, to simplify, that B is discrete and that p 1 and p 2 are transition probabilities that satisfy (12.9). Consider a composite Markov chain for which the transition from X n to X n+1 consists in generating first Y n according to p 1 (X n , •) and then X n+1 according to p 2 (Y n , •). The resulting composite transition probability is p(x, y)

$= z∈B p 1 (x, z)p 2 (z, y).$Trivially, Q is invariant by p, since it is invariant by p 1 and p 2 , but p is not Qreversible. Indeed, p satisfies (12.7) with p(x, y) = z∈B p 2 (x, z)p 1 (z, y). ♦

## Irreducibility and recurrence

While necessary, invariance is not sufficient for a Markov chain to converge to Q in distribution. However, it simplifies the general ergodicity conditions compared to the general theory of Markov chains [[147,](#b165)[160]](#b178), as summarized below, following [[192]](#b210) (see also [[13]](#b31)). We therefore assume that the transition probability P is such that Q is P -invariant.

One says that the Markov chain is Q-irreducible (or, simply, irreducible in what follows) if and only if, for all z ∈ B and all (measurable) B ⊂ B such that Q(B) > 0, there exists n > 0 with P z (X n ∈ B) > 0. (Irreducibility implies that Q is the only invariant probability of the Markov chain.) A Markov chain is called periodic if there exists m > 1 such that B can be covered by disjoint subsets B 0 , . . . , B m-1 that satisfy P (x, B j ) = 1 for all x ∈ B j-1 if j ≥ 1 and all x ∈ B m-1 if j = 0. In other terms, the chain loops between the sets B 0 , . . . , B m-1 . If such a decomposition does not exists, the chain is called aperiodic.

A periodic chain cannot satisfy [(12.3)](#). Indeed, periodicity implies that P x (X n ∈ B i ) = 0 for all x ∈ B i unless i = 0 (mod d). Since the sets B i cover B, (12.3) is only possible with Q = 0. Irreducibility and aperiodicity are therefore necessary conditions for ergodicity. Combined with the fact that Q is an invariant probability distribution, these conditions are also sufficient, in the sense that (12.3) is true for Q-almost all x. (See [[192]](#b210) for a proof.) Without the knowledge that the chain has an invariant probability, showing convergence usually requires showing that the chain is recurrent, which means that, for any set B such that Q(B) > 0, the probability that, starting from x, X n ∈ B for an infinite number of n, written as P x (X n ∈ B i.o.) (for infinitely often) is positive for all

x ∈ E and equal to 1 Q-almost surely. The fact that irreducibility and aperiodicity combined with Q-invariance imply recurrence (or, more precisely, Q-positive recurrent [[147]](#b165)) is an important remark that significantly simplifies the theory for MCMC simulation. Note that, by restricting B to a suitable set of Q-probability 1, one can assume that P x (X n ∈ B i.o.) = 1 for all x ∈ B, which is called Harris recurrence. It the chain is Harris recurrent, then (12.3) holds with µ 0 = δ x for all x ∈ B. 4   One says that C ⊂ B is a "small" set if Q(C) > 0 and there exists a triple (m 0 , ϵ, ν), with ϵ > 0 and ν a probability distribution on B, such that

$P m 0 (x, •) ≥ ϵν(•)$for all x ∈ C. A slightly different result, proved in [[13]](#b31), replaces irreducibility by the property that there exists a small set C ⊂ B such that P x (∃n : X n ∈ C) > 0 4 Harris recurrence is also associated with the uniqueness of right eigenvectors of P , that is functions h : B → R such that

$P h(x) ∆ = B P (x, dy)h(y) = h(x).$Such functions are also called harmonic for P . Because P is a transition probability, constant functions are always harmonic. Harris recurrence, in the current context, is equivalent to the fact that every bounded harmonic function is constant.

for Q-almost all x ∈ B. One then replaces aperiodicity by the similar condition that the greatest common divisor of the set of integers m such that there exists ϵ m with P m (x, •) ≥ ϵ m ν(•) for all x ∈ C is equal to 1. These two conditions combined with Q-invariance also imply that (12.3) holds for Q-almost all x ∈ B.

## Speed of convergence

It is also important to quantify the speed of convergence in [(12.3)](#). Efficient algorithms typically have a geometric convergence speed, namely

$D var (P n x , Q) ≤ M(x)r n (12.10)$for some 0 ≤ r < 1 and some function M(x), or uniformly geometric convergence speed, for which the function M is bounded (or, equivalently, constant).

A sufficient condition for geometric ergodicity is provided in Nummelin [[147,](#b165)[Proposition 5.21]](#). Assume that the chain is Harris recurrent and that there exist r > 1, a small set C and a "drift function" h with

$sup x C (rE(h(X n+1 ) | X n = x) -h(x)) < 0 (12.11a) and sup x∈C E(h(X n+1 )1 X n+1 C | X n = x) < ∞. (12.11b)$Then the Markov chain is geometrically ergodic. Note that E(h(X n+1 ) | X n = x) = P h(x). Equations (12.11a) and (12.11b) can be summarized in a single equation [[136]](#b154), namely P h(x) ≤ βh(x) + M1 C (x) (12.12)

with β = 1/r < 1 and M ≥ 0.

## Models on finite state spaces

Uniform geometric ergodicity is implied by the simple condition that the whole set B is small, requiring in a uniform lower bound, for some probability distribution ν,

$P m 0 (x, •) ≥ ϵν(•) (12.13) for all x ∈ B.$Such uniform conditions usually require strong restrictions on the space B, such as compactness or finiteness. To illustrate this consider the case in which the set B is finite. Assume, to simplify, that Q(x) > 0 for all x ∈ B (one can restrict the Markov chain to such x's otherwise). Arbitrarily labeling elements of B as B = {x 1 , . . . , x N }, we can consider p(x, y) as the coefficients of a matrix P = (p(x k , x l ), k, l = 1, . . . , N ). Such a matrix, which has non-negative entries and row sums equal to 1, is called a stochastic matrix.

We will denote the nth power of P as P n = (p (n) (x k , x l ), k, l = 1, . . . , N ). One immediately sees that irreducibility is equivalent to the fact that, for all x, y ∈ B, there exists m (that may depend of x and y) such that p (m) (x, y) > 0. One can furthermore show that the chain is irreducible and aperiodic if one can choose m independent of x and y above, that is, if there exists m such that P m has positive coefficients. This condition clearly implies uniformly geometric ergodicity, which is therefore valid for all irreducible and aperiodic Markov chains on finite sets.

This result can also be deduced from properties of matrices with non-negative or positive coefficients. The Perron-Frobenius theorem [[93]](#b111) states that the eigenvalue 1 (associated with the eigenvector 1) is the largest, in modulus, eigenvalue of a stochastic matrix P with positive entries, that it has multiplicity one and that all other eigenvalues have a modulus strictly smaller that one. If P m has positive entries, this implies that all the eigenvalues of (P m -1Q) (where Q is considered as a row vector) have modulus strictly less than one. This fact can then be used to prove uniformly geometric ergodicity.

## Examples on R d

To take a geometrically ergodic example that is not uniform, consider the simple random walk provided by the iterations

$X n+1 = ρX n + τ 2 ϵ n$where ϵ n ∼ N (0, Id R d ), τ 2 > 0 and 0 < ρ < 1. One shows easily by induction that the conditional distribution of X n given X 0 = x is Gaussian with mean m n = ρ n x and covariance matrix σ 2 n Id R d with

$σ 2 n = 1 -ρ 2n 1 -ρ 2 τ 2 .$In particular, the distribution

$Q = N (0, σ 2 ∞ Id R d ), with σ 2 ∞ = τ 2 /(1 -ρ 2 )$, is invariant. Estimates on the variational distances between Gaussian distributions, such as those provided in Devroye et al. [[61]](#b79), can then be used to show that

$D var (P n x , Q) ≤ M(x)ρ n$where M grows linearly in x but is not bounded.

Situations in which on can, as above, compute the probability distributions of X n are rare, however, and proving geometric convergence is significantly more difficult than for finite-state chains. For chains on R d (or, more generally, locally compact metric spaces), the drift function criterion (12.12) can be used. Assume that P h(•), given by

$P h(x) = E(h(X n+1 ) | X n = x) = R d h(y)P (x, dy)$is continuous as soon as the function h : R d → R is continuous (one says that the chain is weak Feller). This true, for example, if P (x, •) has a p.d.f. with respect to Lebesgue's measure which is continuous in x. In such a situation, one can see that compact sets are small sets, and (12.12) can be restated as the existence if a positive function h with compact sub-level sets and such that h(x) ≥ 1, of a compact set C ⊂ R d and of positive constants β < 1 and b such that, for all x ∈ R d ,

$P h(x) ≤ βh(x) + b1 C (x).$(12.14)

As an example, consider the Markov chain defined by

$X n+1 = X n -δ∇H(X n ) + τϵ n+1$where ϵ 2 , ϵ 2 , . . . are i.i.d. standard d-dimensional Gaussian variables and

$H : R d → R is C 2 .$This chain is clearly irreducible (with respect to Lebesgue's measure). One has

$P h(x) = 1 (2πτ 2 ) d/2 R d h(y)e -1 2τ 2 |y-x+δ∇H(x)| 2 dy = 1 (2π) d/2 R d h(x-δ∇H(x)+τu)e -|u| 2 2 dy.$Let us make the assumption that H is L-C 1 for some L > 0 (c.f. definition 3.15) and furthermore assume that |∇H(x)| tends to infinity when x tends to infinity, ensuring the fact that the sets {x : |∇H(x)| ≤ c} are compact for c > 0. We want to show that, if δ is small enough, (12.14) holds for h(x) = exp(mH(x)) and m small enough.

We first compute an upper bound of

$g(x, u) = mH(x -δ∇H(x) + τu) - |u| 2 2 .$Using the L-C 1 property, we have

$g(x, u) ≤ mH(x) + m(-δ∇H(x) + τu) T ∇H(x) + mL 2 |δ∇H(x) -τu| 2 - |u| 2 2 = mH(x) -mδ(1 -δL/2)|∇H(x)| 2 + mτ(1 -τL)∇H(x) T u - 1 -mLτ 2 2 |u| 2 = mH(x) - 1 -mLτ 2 2 u - mτ(1 -τL) 1 -mLτ 2 ∇H(x) 2 -m δ(1 -δL/2) - mτ 2 (1 -τL) 2 2(1 -mLτ 2 ) |∇H(x)| 2$Assume that mLτ 2 ≤ 1. It follows that

$P h(x) = 1 (2π) d/2 R d e g(x,u) du ≤ h(x) (1 -mLτ 2 ) d/2 exp -m δ(1 -δL/2) - mτ 2 (1 -τL) 2 2(1 -mLτ 2 ) |∇H(x)| 2$Using this upper bound, we see that [(12.14)](#) will hold if one first chooses δ such that δL < 2, then m such that mLτ 2 < 1 and

$mτ 2 (1 -τL) 2 2(1 -mLτ 2 ) < δ(1 -δL/2)$and finally choose C = {x : |∇H(x)| ≤ c} where c is large enough so that

$1 (1 -mLτ 2 ) d/2 exp -m δ(1 -δL/2) - mτ 2 (1 -τL) 2 2(1 -mLτ 2 ) c 2 < 1.$Note that this Markov chain is not in detailed balance. Since P (x, •) has a p.d.f., being in detailed balance requires the ratio p(x, y)/p(y, x) to simplify as a ratio q(y)/q(x) for some function q, which does not hold. However, we can identify the invariant distribution approximately with small δ and τ, that we will assume to satisfy τ = a √ δ for a fixed a > 0, with δ a small number.

We can write

$p(x, y) = 1 (2πτ 2 ) d/2 exp - 1 2τ 2 |y -x + δ∇H(x)| 2 = 1 (2πτ 2 ) d/2 exp - 1 2τ 2 |y -x| 2 - δ τ 2 (y -x) T ∇H(x) - δ 2 2τ 2 |∇H(x)| 2 .$If q is a density, we have

$qP (y) = R d q(x)p(x, y)dx = 1 (2π) d/2 R d q(y + a √ δu) exp - 1 2 |u| 2 + √ δ a u T ∇H(y + a √ δu) - δ 2a 2 |∇H(y + a √ δu)| 2 du$Make the expansions:

$q(y + a √ δu) = q(y) + a √ δ∇q(y) T u + a 2 δ 2 u T ∇ 2 q(y)u + o(δ|u| 2 ) and exp √ δ a u T ∇H(y + a √ δu) - δ 2a 2 |∇H(y + a √ δu)| 2 = 1 + √ δ a u T ∇H(y) - δ 2a 2 |∇H(y)| 2 + δu T ∇ 2 H(u)u + δ 2a 2 (u T ∇H(y)) 2 + o(δ|u| 2 ).$Taking the product and using the fact that (2π

$) -d/2 R d u exp(-|u| 2 /2)du = 0 and that (2π) -d/2$R d u T Au exp(-|u| 2 /2)du = trace(A) for any symmetric matrix A, we can write, taking the product:

$qP (y) = q(y) + δ a 2 2 ∆q(y) + ∇H(y) T ∇q(y) + q(y)∆H(y) + o(δ)$This indicates that, if q is invariant by P , it should satisfy

$a 2 2 ∆q(y) + ∇H(y) T ∇q(y) + q(y)∆H(y) = o(1).$The partial differential equation 

$a 2 .$Assuming that this function is integrable, this computation suggests that, for small δ, the Markov chain approximately samples from the probability distribution

$q 0 = 1 Z e - 2H(x) a 2 .$This is further discussed in the next remark that involves stochastic differential equations. We will also present a correction of this Markov chain that samples from q 0 for all δ in section 12.5.2.

Remark 12.5 (Langevin equation) This chain is indeed the Euler discretization [[106]](#b124) of the stochastic differential equation,

$dx t = -∇H(x t )dt + adw t (12.16)$where w t is a Brownian motion. Under general hypotheses, this stochastic diffusion equation, called a Langevin equation, indeed converges in distribution to q 0 (x). 5   5 Providing a rigorous account of the theory of stochastic differential equations is beyond our scope, and we refer the reader to the many textbooks on the subject, such as McKean [[131]](#b149), Ikeda and Watanabe [[96]](#b114), Ethier and Kurtz [[69]](#b87) (see also [Berglund [26]](#) for a short introduction). Such diffusions are continuous-time Markov processes (X t , t ≥ 0), which means that the probability distribution of X t+s given all events before and including time s only depends on X s and is provided by a transition probability P t , with

$P(X t+1 ∈ A | X s = x) = P t (x, A).$Similarly to deterministic ordinary differential equations, one shows that under sufficient regularity conditions (e.g., ∇H is C 1 ), equations such as [(12.16](#)) have solutions up to some positive (random) explosion time, and that this explosion time is finite under additional conditions that ensure that |∇H(x)| does not grow too fast when x tends to infinity.

If ϕ is a smooth enough function (say, C 2 , with compact support), the function (t, x) → P t ϕ(x) satisfies the partial differential equation, called Kolmogorov's backward equation,

$∂ t P t ϕ(x) = -∇H(x) T ∇P t ϕ(x) + a 2 2 ∆P t ϕ(x)$with initial condition P 0 ϕ(x) = ϕ(x). If P t (x, •) has at all times t a p.d.f. p t (x, •), then this p.d.f. must satisfy the forward Kolmogorov equation:

$∂ t p t (x, y) = ∇ 2 • (∇H(y)p t (x, y)) + a 2 2 ∆ 2 p t (x, y)$where ∇ 2 and ∆ 2 indicate differentiation with respect to the second variable (y). (Recall that δf denotes the Laplacian of f .) Moreover, if Q is an invariant distribution with p.d.f. q, it satisfies the equation

$∇ • (q∇H) + a 2 2 ∆q(y) = 0.$Noting that ∇ • (q∇H) = ∇q T ∇H + q∆H, we retrieve [(12.15)](#). Convergence properties (and, in particular, geometric convergence) of the Langevin equation to its limit distribution are studied in Roberts and Tweedie [[166]](#b184), using methods introduced in Meyn and Tweedie [[134,](#b152)[135,](#b153)[136](#b154)] ♦ 12.4 Gibbs sampling

## Definition

The Gibbs sampling algorithm [[79]](#b97) was introduced to sample from a distribution on large sets for which direct sampling is intractable and rejection samping is inefficient. It generates a Markov chain that converges (under some hypotheses) in distribution to this target probability. A general version of this algorithm is described below.

Let Q be a probability distribution on B. Consider a finite family U 1 , . . . , U K of random variables defined on B with values in measurable spaces B ′ 1 , . . . ,

$B ′ K . Let Q i = Q U i denote the image of Q by U i , defined by Q i (B i ) = Q(U i ∈ B i ) for B i ⊂ B i .$Also, assume that there exists, for all i, a regular family of conditional probabilities for Q given U i , defined as a collection of transition probabilities (u i , A)

$→ Q i (u i , A) for u i ∈ B i and A ⊂ B, that satisfy A g(U i (x))Q(dx) = B i Q i (u i , A)g(u i )Q i (du i )$for all nonnegative measurable functions g : B i → R. In simpler terms, Q i (u i , A) determine a consistent set of conditional probabilities for Q(A | U i = u i ). For discrete random variables (resp. variables with p.d.f.'s on R d ), they are just elementary conditional probabilities.

We then consider the following algorithm.

## Algorithm 12.2 (Gibbs sampling)

Initialize the algorithm with some z(0) = z 0 ∈ B and iterate the following two update steps given a current z(n) ∈ B:

(1) Select j ∈ {1, . . . , K} according to some pre-defined scheme, i.e., at random according to a probability distribution π (n) on the set {1, . . . , K}.

(2) Sample a new value z(n+1) according to the probability distribution Q j (U j (z(n)), •).

## One typically chooses the probability distribution in

Step 1 equal to the uniform distribution on {1, . . . , K} (in which case it is independent on n), or to π (n) = δ j n where j n = 1+(n (mod) K) (periodic scheme). Strictly speaking, Gibbs sampling is a Markov chain if π (n) does not depend on n, and we will make this simplifying assumption in the rest of our discussion (therefore replacing π (n) by π). One obvious requirement for the feasibility of the method is that step (2) can be performed efficiently since it must be repeated a very large number of times.

One can see that the Markov chain generated by this algorithm is Q-reversible. Indeed, assume that X n ∼ Q. For any (measurable) subsets A and B in B, one has, using the definition of conditional expectations,

## P(X

$n ∈ A, X n+1 ∈ B) = K i=1 E 1 Z∈A Q i (U i (Z), B) π(i).$(12.17) Now, for any i

$E 1 Z∈A Q i (U i (Z), B) = A Q i (U i (z), B)Q(dz) = B i Q i (u i , A)Q i (u i , B)Q i (du i ) which is symmetric in A and B.$Note that, in the discrete case

$P (z, z) = n i=1 π(i) Q(z)1 U i (z)=U i (z) z ′ :U i (z ′ )=U i (z) Q(z ′ ) (12.18)$and the relation

$Q(z)P (z, z) = Q(z)P (z, z) is obvious.$The conditioning variables U 1 , . . . , U K should ensure, at least, that the associated Markov chain is irreducible and aperiodic. For irreducibility, this requires that Z can visits Q-almost all elements of B by a sequence of steps that lead one of the U i 's invariant.

Remark 12.6 In the standard version of Gibbs sampling, B is a product space B 1 × • • • × B K , and

$B ′ j = B 1 × • • • × B j-1 × B j+1 × • • • × B K .$One then takes U j (z (1) , . . . , z (K) ) = (z (1) , . . . , z (i-1) , z (i+1) , . . . , z (K) ). In other terms, step 2 in the algorithm replaces the current value of z (j) (n) by a new one sampled from the conditional distribution of Z (j) given the current values of z (i) (n), i j. ♦ Remark 12.7 We have considered a fixed number of conditioning variables, U 1 , . . . , U K , for simplicity, but the same analysis can be carried on if one replaces U j by a function U : (x, θ) → U θ (x) defined on a product space B × Θ, taking values in some space B, where Θ is a probability space equipped with a probability distribution π and B is measurable. The previous discussion corresponds to Θ = {1, . . . , K} and

$B = K i=1 {i} × B i (so that U i (x) is replaced by (i, U i (x))).$One may then define Q θ as the image of Q by U θ and let Q θ (u, A) provide a version of Q(A | U θ = u). The only change in the previous discussion (besides using θ in index) is that (12.17) becomes

## P(X

$n ∈ A, X n+1 ∈ B) = Θ E 1 Z∈A Q θ (U θ (Z), B) π(dθ).$
## ♦

Remark 12.8 Using notation from the previous remark, and allowing π = π (n) to depend on n, it is possible to allow π (n) to depend on the current state z(n) using the following construction.

For every step n, assume that there exists a subset Θ n of Θ such that π (n) (z, Θ n ) = 1 and that, for all θ ∈ Θ n , π (n) can be expressed in the form

$π (n) (z, •) = ψ (n) θ (U θ (z), •)$for some transition probability ψ 

$(n) θ from B θ to Θ n . The resulting chain remains Q-reversible, since P(X n ∈ A, X n+1 ∈ B) = B Θ n 1 z∈A Q θ (U θ (z), B)π (n) (z, dθ)Q(dz) = Θ n B 1 z∈A Q θ (U θ (z), B)ψ (n) θ (U θ (z), dθ)Q(dz) = Θ n B Q θ (u, A)Q θ (u, B)ψ (n) θ (u, dθ)Q θ (du). ♦12$$q(z) = 1 C exp         L j=1 αz (j) + L i,j=1,i<j β ij z (i) z (j)         .$Note that, although B is a finite set, its cardinality, 2 L , is too large for the enumerative procedure described in section 12.1 to be applicable as soon as L is, say, larger than 30. In practical applications of this model, L is orders of magnitude larger, typically in the thousands or tens of thousands.

We here apply standard Gibbs sampling, as described in remark 12.6, defining B j = {0, 1} and U i (z (1) , . . . , z (L) ) = (z (1) , . . . , z (i-1) , z (i+1) , . . . , z (L) ).

The conditional distribution of Z (j) given U j (z) is a Bernoulli distribution with parameter

$q Z (j) (1 | U j (z)) = exp(α + L j ′ =1,j ′ j β jj ′ z (j ′ ) ) 1 + exp(α + L j ′ =1,j ′ j β ij z (j) )$(taking β jj ′ = β j ′ j for j > j ′ ). Gibbs sampling for this model will generate a sequence of variables Z(0), Z(1), . . . by fixing Z(0) arbitrarily and, given Z(n) = z, applying the two steps:

1. Select j ∈ {1, . . . , L} at random according to a probability distribution π (n) on the set {1, . . . , L}.

2. Sample a new value ζ ∈ {0, 1} according to the Bernoulli distribution with parameter q Z (j) n (1 | U j (z)), and set Z (j) (n + 1) = ζ and Z (j ′ ) (n + 1) = Z (j ′ ) (n) for j ′ j.

Let us now consider the Ising model with fixed total activation, namely the previous distribution conditional to S(z)

$∆ = z (1) + • • • + z (L) = h where 0 < h < L.$The distribution one wants to sample from now is

$q h (z) = 1 C h exp         L j=1 αz (j) + L i,j=1,i<j β ij z (i) z (j)         1 S(z)=h .$In that case, the previous choice for the one-step transitions does not work, because fixing all but one coordinate of z also fixes the last one (so that the chain would not move from its initial value and would certainly not be irreducible). One can however fix all but two coordinates, therefore defining U ij (z (1) , . . . , z (L) ) = (z (1) , . . . , z (i-1) , z (i+1) , . . . , z (j-1) , z (j+1) , . . . , z (L) )

and

$B ij = {0, 1} 2 . If U ij (z)$is fixed, the only acceptable configurations are z and the configuration z ′ deduced from z by switching the value of z (i) and z (j) . Thus, there is no possible change is z (i) = z (j) . If z (i) z (j) , then the probability of flipping the values of z (i) and z (j) is q h (z ′ )/(q h (z) + q h (z ′ )).

12.5 Metropolis-Hastings

## Definition

Gibbs sampling is a special case of a generic MCMC algorithm called Metropolis-Hastings that is defined as follows [[133,](#b151)[88]](#b106). Assume that the distribution Q has a density q with respect to a measure µ on B. Specify a transition probability on B, represented by a family of density functions with respect to µ, (g(z, •), z ∈ B), and a family of acceptance functions (z, z ′ ) → a(z, z ′ ) ∈ [0, 1]. Two basic examples are when B is finite, µ is the counting measure, and q and g are probability mass functions, and when B = R d , µ is Lebesgue's measure and q and g are probability density functions.

The sampling algorithm is then defined as follows. It invokes a function a that will be specified below.

## Algorithm 12.3 (Metropolis-Hastings)

Initialize the algorithm with Z(0) = z(0) ∈ B. At step n, the current value Z(n) = z is then updated as follows.

• "Propose" a new configuration z ′ drawn according to g(z, •).

• "Accept" z ′ (i.e., set Z(n + 1) = z ′ ) with probability a(z, z ′ ). If the new value is rejected, keep the current one, i.e., let Z(n + 1) = z.

The transition probabilities for this process are p(x, y) = g(x, y)a(x, y) if x y and p(x, x) = 1y x p(x, y). The chain is Q-reversible is the detailed balance equation

$q(z)g(z, z ′ )a(z, z ′ ) = q(z ′ )g(z ′ , z)a(z ′ , z) (12.19)$is satisfied. The functions g and a are part of the design of the algorithm, but [(12.19)](#) suggest that g should satisfy the "weak symmetry" condition:

$∀x, y ∈ Ω : g(x, y) = 0 ⇔ g(y, x) = 0. (12.20)$Note that this condition is necessary to ensure (12.19) if q(z) > 0 for all z. If q(z) > 0, the fact that acceptance probabilities are less than 1 requires that a(z, z ′ ) ≤ min 1, q(z ′ )g(z ′ , z) q(z)g(z, z ′ ) .

If one takes a(z, z ′ ) equal to the r.h.s., so that

$a(z, z ′ ) = min 1, q(z ′ )g(z ′ , z) q(z)g(z, z ′ ) , (12.21)$then [(12.19](#)) is satisfied as soon as q(z) > 0. If q(z) = 0, then this definition ensures that a(z ′ , z) = 0 and (12.19) is also satisfied. Note also that the case g(z, z ′ ) = 0 is not relevant, since z ′ is not attainable from z in one step in this case. This shows that (12.21) provides a Q-reversible chain. Obviously, if g already satisfies q(z)g(z, z ′ ) = q(z ′ )g(z ′ , z), which is the case for Gibbs sampling, then one should take a(z, z ′ ) = 1 for all z and z ′ .

## Sampling methods for continuous variables

While the Gibbs sampling and Metropolis-Hastings methods can be (and were) formulated for general variables and probability distributions, proving that the related chains are ergodic, and checking conditions for geometric convergence speed is much harder when dealing with general state spaces than with finite or compact spaces (see, e.g., [[164,](#b182)[132,](#b150)[6,](#b24)[165]](#b183)). On the other hand, interesting choices of proposal transitions for Metropolis-Hastings are available when B = R d and µ is Lebesgue's measure, taking advantage, in particular, of differential calculus. More precisely, assume that q takes the form

$q(z) = 1 C exp(-H(z))$for some smooth function H (at least C 1 ), such that exp(-H) is integrable. We saw in section 12.3.7 that, under suitable assumptions, the Markov chain

$X n+1 = X n - δ 2 ∇H(X n ) + √ δϵ n+1 (12.22)$with ϵ n+1 ∼ N (0, Id R d ) has q as invariant distribution in the limit δ → 0. Its transition probability, such that g(z, •) is the p.d.f. of N (z-δ 2 ∇H(z), δId R d ), is therefore a natural choice for a proposal distribution in the Metropolis-Hastings algorithm. In addition to converging from the exact target distribution, this "Metropolis Adjusted Langevin Algorithm" (or MALA) can also be proved to satisfy geometric convergence under less restrictive hypotheses than [(12.22)](#)[[166]](#b184).

Another approach, similar to MALA is the Hamiltonian Monte-Carlo methods (or hybrid Monte-Carlo) [[65,](#b83)[142]](#b160). Inspired by physics, the method introduces a new variable, p ∈ R d , called "momentum," and defines the "Hamiltonian:"

$H(z, m) = H(z) + 1 2 |m| 2 .$Fix a time θ > 0. The proposal transition g(z, •) is then defined as the value ζ(θ) that is obtained by solving the Hamiltonian dynamical system

$∂ t ζ(t) = ∂ p H(ζ(t), µ(t)) = µ(t) ∂ t µ(t) = -∂ z H(ζ(t), µ(t)) = -∇H(ζ(t)) (12.23) with ζ(0) = z and µ(0) ∼ N (0, Id R d ). One can easily see that ∂ t H(ζ(t), µ(t)) = 0, which implies that H(ζ(t)) + 1 2 |µ(t)| 2 = H(z) + 1 2 |µ(0)| 2$at all times t, or, denoting by ϕ N the p.d.f. of the d-dimensional standard Gaussian,

$q(ζ(t))ϕ N (µ(t)) = q(ζ(0))ϕ N (µ(0)).$Moreover, if one denotes by Φ t (z, m) = (z t (z, m), m t (z, m)) the solution (ζ(t), µ(t)) of the system started with ζ(0) = z and µ(0) = m, one can also see that det(dΦ t (z, m)) = 1 at all times. Indeed, applying (1.5) and the chain rule, we have

$∂ t log det(dΦ t (z, m)) = trace(dΦ t (z, m) -1 ∂ t dΦ t (t, m)). From ∂ t z t (z, m) = m t (z, p) ∂ t p t (z, m) = -∇H(z t (z, m)) we get ∂ t dΦ t (z, m) = ∂ z m t (z, m) ∂ m m t (z, m) -∇ 2 H(z t (z, m))∂ z z t (z, m) -∇ 2 H(z t (z, m))∂ m z t (z, m) = 0 Id R d -∇ 2 H(z t (z, m)) 0 dΦ t (z, m).$We therefore get

$∂ t log det(dΦ t (z, m)) = trace 0 Id R d -∇ 2 H(z t (z, m)) 0 = 0$showing that the determinant is constant. Since Φ 0 (z, m) = (z, m) by definition, we get det(dΦ t (z, m)) = 1 at all times.

Let qt denote the p.d.f. of Φ t (z, m) and assume that q0 (z, m) = q(z)ϕ N (m). We have, using the change of variable formula

$qt (Φ t (z, m))| det dΦ t (z, m)| = q(z)ϕ N (m)$but the r.h.s. is, from the remarks above also equal to

$q(z t (z, m))ϕ N (m t (z, m))| det dΦ t (z, m)| yielding the identification qt (z ′ , m ′ ) = q(z ′ )ϕ N (m ′ )$This shows that Q (with p.d.f. q) is left invariant by this Markov chain. One can actually show that chain is in detailed balance for the joint density q(z, m) = q(z)ϕ N (m). This is due to the fact that the system (12.23) is reversible, in the sense that

$Φ t (z t (z, m), -m t (z, m)) = (z, -m),$i.e., the system solved from its end point after changing the sign of the momentum returns to its initial state after changing the sign of the momentum a second time.

In other terms, letting J(z, m) = (z, -m), we have We have

$Φ -1 t = JΦ t •J. So, consider a function f : (R d × R d ) 2 → R.$$E(f (Z n , M n , Z n+1 , M n+1 )) = f (z, m, z(z, m), m)ϕ N (m)ϕ N ( m)ϕ N ( m)q(z)dmd md mdz.$Make the change of variables z ′ = z(z, m), m ′ = m(z, m), which has Jacobian determinant 1, and is such that z = z(z ′ , -m ′ ), m = -m(z ′ , -m ′ ). We get

$E(f (Z n , M n , Z n+1 , M n+1 )) = f (z(z ′ , -m ′ ), m, z ′ , m)ϕ N (-m(z ′ , -m ′ ))ϕ N ( m)ϕ N ( m)q(z(z ′ , -m ′ ))dm ′ d md mdz ′ = f (z(z ′ , -m ′ ), m, z ′ , m)ϕ N (m(z ′ , -m ′ ))ϕ N ( m)ϕ N ( m)q(z(z ′ , -m ′ ))dm ′ d md mdz ′ = f (z(z ′ , -m ′ ), m, z ′ , m)ϕ N (-m ′ )ϕ N ( m)ϕ N ( m)q(z ′ )dm ′ d md mdz ′ ,$using the conservation of H. Making the change of variables m ′ → -m ′ , we get

$E(f (Z n , M n , Z n+1 , M n+1 ) = f (z(z ′ , m ′ ), m, z ′ , m)ϕ N (m ′ )ϕ N ( m)ϕ N ( m)q(z ′ )dm ′ d md mdz ′ which is equal to E(f (Z n+1 , M n+1 , Z n , M n ))$showing the reversibility of the chain. This simulation scheme can potentially make large moves in the current configuration z while maintaining detailed balance (therefore not requiring an accept/reject step). However, practical implementations require discretizing (12.23), which breaks the conservation properties that were used in the argument above, therefore requiring a Metropolis-Hastings correction. For example, a second-order Runge Kutta (RK2) scheme with time step α gives

$           Z n+1 = Z n + αM n - α 2 2 ∇H(Z n ) M n+1 = M n - α 2 (∇H(Z n ) + ∇H(Z n + hM n ))$Only the update for Z n matters, however, since M n+1 is discarded and resampled at each step. Importantly, if we let δ = √ α the first equation in the system becomes

$Z n+1 = Z n - δ 2 ∇H(Z n ) + δM n$with M n ∼ N (0, 1), which is exactly [(12.22)](#). Note that one can, in principle, solve [(12.23](#)) for more that one discretization step (the continuous equation can be solved for an arbitrary time), but one must then face the challenge of computing the Metropolis correction since the Hamiltonian is not conserved at each step.

One can however use schemes that are more adapted to solving Hamiltonian systems [[119]](#b137), such as the Störmer-Verlet scheme, which is

$               M n+1/2 = M n - α 2 ∇H(Z n ) Z n+1 = Z n + αM n+1/2 M n+1 = M n+1/2 - α 2 ∇H(Z n+1 ) This scheme computes ψ 1 • ψ 2 • ψ 1 (z, m) with ψ 1 (z, m) = (z, m -(α/2)∇H(z)$) and ψ 2 (z, m) = (z + αm, m). Because both ψ 1 and ψ 2 have a Jacobian determinant equal to 1, so does their composition. This scheme is also reversible, since we have

$               -M n+1/2 = -M n+1 - α 2 ∇H(Z n+1 ) Z n = Z n+1 -αM n+1/2 -M n = -M n+1/2 - α 2 ∇H(Z n )$These properties are conserved if one applies the Störmer-Verlet scheme more than once at each iteration, that is, fixing some N > 0 and letting

$Φ(z, m) = (ψ 1 •ψ 2 •ψ 1 ) •N , then Φ -1 = JΦ • J, with J(z, m) = (z, -m) with det dΦ = 1. Considering again the aug- mented chain which, starting from (Z n , M n ), samples M ∼ N (0, Id R d ), then computes (Z ′ , M′ ) = Φ(Z n , M)$and finally samples M ′ ∼ N (0, Id R d ) as a Metropolis-Hastings proposal to sample from (z, m) → q(z)ϕ N (m), then, assuming that (Z, M) follows this target distribution and letting (Z ′ , M ′ ) be the result of the proposal distribution, we have, as computed above

$E(f (Z, M, Z ′ , M ′ )) = f (z, m, z(z, m), m)ϕ N (m)ϕ N ( m)ϕ N ( m)q(z)dmd md mdz = f (z(z ′ , m ′ ), m, z ′ , m)ϕ N (m(z ′ , m ′ ))ϕ N ( m)ϕ N ( m)q(z(z ′ , m ′ ))dm ′ d md mdz ′$This shows that the acceptance probability in the Metropolis step is

$a(z, m, z ′ , m ′ ) = min 1, ϕ N (m(z ′ , m ′ ))q(z(z ′ , m ′ )) ϕ N (m)q(z) = exp (-max (H(z(z ′ , m ′ ), m(z ′ , m ′ )) -H(z, m)) , 0)$While the Hamiltonian is not kept invariant by the Störmer-Verlet scheme, so that an accept-reject step is needed, it is usually quite stable over extended periods of time so that the acceptance probability is generally close to one.

## Perfect sampling methods

The Markov chain simulation methods, provided in the previous sections do not provide exact samples from the distribution q, but only increasingly accurate ap-proximations. Perfect sampling algorithms [[156,](#b174)[157,](#b175)[71]](#b89) use Markov chains "backwards" to generate exact samples. To describe them, it is easier to describe a Markov chain as a stochastic recursive equation of the form

$X n+1 = f (X n , U n+1 ) (12.24)$where U n+1 is independent of X n , X n-1 , . . ., and the U k 's are identically distributed. In the discrete case (assumed in this section), and given a stochastic matrix P , one can take U n to be the uniformly distributed variable used to sample from (p(X n , x), x ∈ B).

Conversely, the transition probability associated to (12.24) is p(x, y) = P (f (x, U ) = y).

It will be convenient to consider negative times also. For n > 0, recursively define F -n (x, u -n+1 , . . . , u 0 ) by

$F -n-1 (x, u -n , . . . , u 0 ) = F -n (f (x, u -n ), u -n+1 , . . . , u 0 ) and F -1 (x, u 0 ) = f (x, u 0 ). Denote, for short, U 0 -n = (U -n , . . . , U 0 ). The function F -n (x, u 0 -n+1 ) provides the value of X 0 when X -n = x and U 0 -n+1 = u 0 -n+1 .$For an infinite past sequence, u 0 -∞ , let ν(u 0 -∞ ) denote the first integer n such that F -n (x, u 0 -n+1 ) does not depend on x (the function "coalesces"). Then, the following theorem is true: Theorem 12.9 Assume that the chain defined by (12.24) is ergodic, with invariant distribution Q. Then ν = ν(U 0 -∞ ) is finite with probability 1, and

$X * := F -ν (x, U 0 -ν+1 ) (12.25$$)$(which is independent of x) has distribution Q.

Proof Because the chain is ergodic, we know that there exists an integer N such that one can pass from any state to any other with positive probability. So the chain can, starting from anywhere, coalesce with positive probability in N steps; ν being infinite would imply that this event never occurs in an infinite number of trials, and this has probability 0.

For any k > 0 and any x ∈ B, we have

$X * = F -ν (f -k (x, U -ν -ν-k+1 ), U 0 -ν+1 ) = F -ν-k (x, U 0 -ν-k+1 ). (12.26)$But, because the chain is ergodic, we have, for any x ∈ B

$lim k→∞ P(F -k (x, U 0 -k+1 ) = y) = Q(y).$We can write

$P(F -k (x, U 0 -k+1 ) = y) = P(F -k (x, U 0 -k+1 ) = y, ν ≤ k) + P(F -k (x, U 0 -k+1 ) = y, ν > k) = P(X * = y, ν ≤ k) + P(F -k (x, U 0 -k+1 ) = y, ν > k)$The right-hand side tends to P(X * = y) when k tends to infinity (because P(ν > k) tends to 0), and the left-hand side tends to Q(y), which gives the second part of the theorem.

■ From (12.26), which is the key step in proving that X * follows the invariant distribution, one can see why it is important to consider sampling that expands backward in time rather than forward. More specifically, consider the coalescence time for the forward chain, letting ν(u ∞ 0 ) be the first index for which X * := F ν (x, u ν 0 ) is independent from the starting point, x. For any k ≥ 0, one still has the fact that F ν+k (x, u ν+k 0 ) does not depend on x, but its value depends on k and will not be equal to X * anymore, which prevents the rest of the proof of theorem 12.9 to carry on. An equivalent algorithm is described in the next proposition (the proof is easy and left to the reader).

Proposition 12.10 Using the same notation as above, the following algorithm generates a perfect sample, ξ * , of the invariant distribution of an ergodic Markov chain.

Assume that an infinite sample u 0 -∞ of U is available. Given this sequence, the algorithm, starting with t 0 = 2, is:

1. For all x ∈ B, define ξ x -t , t = -t 0 , . . . , 0 by ξ x -t 0 = x and ξ x -t+1 = f (ξ x -t , u -t+1 ).

## If ξ x

0 is constant (independent of x), let ξ * be equal to this constant value and stop. Otherwise, return to step 1 replacing t 0 with 2t 0 .

In practice, the u -k 's are only generated when they are needed. But it is important to consider the sequence as fixed: once u -k is generated, it must be stored (or identically regenerated, using the same seed) for further use. It is important to strengthen the fact that this algorithm works backward in time, in the sense that the first states of the sequence are not identical at each iteration, because they are generated using random numbers with indexes further in the past. Such an algorithm is not feasible when |B| is too large, since one would have to consider an intractable number of Markov chains (one for each x ∈ B). However there are cases in which the constancy of ξ x 0 over all B can be decided from its constancy over a small subset of B.

One situation in which this is true is when the Markov chain is monotone, according to the following definition. Assume that B can be partially ordered, and that f in (12.24) is increasing in x, i.e.,

$x ≤ x ′ ⇒ ∀u, f (x, u) ≤ f (x ′ , u).$(12.27)

Let B min and B max be the set of minimal and maximal elements in B. Then the sequence coalesces for the algorithm above if and only if it coalesces over B min ∪ B max . Indeed, any x ∈ B is smaller than some maximal element, and larger than some minimal element in B. By (12.27), these inequalities remain true at each step of the sampling process, which implies that when chains initialized with extremal elements coalesce, so do the other ones. Therefore, it suffices to run the algorithm with extremal configurations only.

One can rewrite (12.27) in terms of transition probabilities p(x, y), assuming that U follows a uniform distribution on [0, 1] and, for all x ∈ B, there exists a partition (I xy , y ∈ B) of B, such that f (x, u) = y ⇔ u ∈ I x,y and I xy is an interval with length p xy . Condition (12.27) is then equivalent to

$x ≤ x ′ ⇒ ∀y ∈ B, I xy ⊂ y ′ ≥y I x ′ y ′ .$This requires in particular that y≥y 0 p(x, y) ≤ y≥y 0 p(x ′ , y) whenever x ≤ x ′ (one says that p(x, •) is stochastically smaller than p(x ′ , •)).

One example in which this reduction works is with the ferromagnetic Ising model, for which B = {-1, 1} L and

$q(x) = 1 C exp L s,t=1,s<t β st x (s) x (t)$with β st ≥ 0 for all {s, t}. Then, the Gibbs sampling algorithm iterates the following steps: take a random s ∈ {1, . . . , L} and update x (s) according to the conditional distribution g s (y (s) | x (s c ) ) = e y (s) v s (x) e -v s (x) + e v s (x) with v s (x) = t s β st x (t) . Order B so that x ≤ x if and only if x (s) ≤ x(s) for all s = 1, . . . , L. The minimal and maximal elements are unique in this case, with x (s) min ≡ -1 and x (s) max ≡ 1. Moreover, because all β st are non-negative, v s is an increasing function of x so that, if x ≤ x, then g s [(](#)

$1 | x (s) ) ≤ g s (1 | x(s) ).$To define the stochastic iterations, first introduce

$f s (x, u) =        1 (s) ∧ x (s c ) if u ≤ q s (1 | x (s) ) (-1) (s) ∧ x (s c ) if u > q s (1 | x (s) ),$which satisfies [(12.27)](#). The whole updating scheme can then be implemented with the function

$f (x, (u, ũ)) = L s=1 δ I s ( ũ)f s (x, u)$where (I s , s ∈ V ) is any partition of [0, 1] in intervals of length 1/L. This is still monotonic. The algorithm described in proposition 12.10 can therefore be applied to sample exactly, in finite time, from the ferromagnetic Ising model.

## Application: Stochastic approximation with Markovian transitions

Using the material developed in this chapter, we now discuss the convergence of stochastic approximation methods (such as stochastic gradient descent) when the random random variable in the update term follows Markovian transitions. In section 3.3, we considered algorithms in the form

$ξ t+1 ∼ π X t X t+1 = X t + α t+1 H(X t , ξ t+1 )$where ξ t : Ω → R ξ is a random variable. We now want to addres situations in which the random variable ξ t+1 is obtained through a transition probability, therefore considering the algorithm ξ t+1 ∼ P X t (ξ t , •)

$X t+1 = X t + α t+1 H(X t , ξ t+1 )(12.28)$Here P x is, for all x, a transition probability from R ξ to R ξ . We will assume that, for all x ∈ R d , the Markov chain with transition P x is geometrically ergodic, and we denote by π x its invariant distribution. We let, as in section 3.3, H(x) = E π x (H(x, •)).

We will use the notation for a function f : R d × R ξ → R

$P x f : (x ′ , ξ) ∈ R d × R ξ → P x f (x ′ , ξ) = R ξ f (x ′ , ξ ′ )P x (ξ, dξ ′ ) and π x f : x ′ ∈ R d → π x f (x ′ ) = R ξ f (x ′ , ξ)π x (dξ).$In particular, H(x) = π x H(x). We also define h(x, ξ) = H(x, ξ) -H(x) and h(x, ξ) = P x h(x, ξ). We make the following assumptions.

(H1) There exists constants C 0 , C 1 , c 2 such that, for all x, y ∈ R d , sup

$ξ∈R ξ |H(x, ξ)| ≤ C 0 , (12.29a) sup ξ∈R ξ | h(x, ξ)| ≤ C 1 , (12.29b) sup ξ∈R ξ | h(x, ξ) -h(y, ξ)| ≤ C 1 |x -y|, (12.29c) D var (π x , π y ) ≤ C 2 |x -y| (12.29d) (H2) There exists x * ∈ R d and µ > 0 such that, for all x ∈ R d (x -x * ) T H(x) ≤ -µ|x -x * | 2 .$(12.30) (H3) We assume that there exists a constant M and a non-decreasing function ρ : [0, +∞) → [0, 1) such that, for all probability distributions Q and

$Q ′ on R ξ , D var (QP n x , Q ′ P n x ) ≤ Mρ(|x|) n D var (Q, Q ′ ). (12$.31) (H4) The sequence α 1 , α 2 , . . . is non-increasing, with ∞ t=1 α t = +∞ and ∞ t=1 α 2 t < +∞. (12.32a) Let σ t = t s=1 α s . If C 1 > 0, we also require that lim t→∞ α t σ t (1ρ(σ t )) -1 = 0 (12.32b) and t s=2 α 2 s

$σ s (1 -ρ(σ s )) -2 < ∞.(12.32c)$Given this, the following theorem holds.

Theorem 12.11 Assuming (H1) to (H4), the sequence defined by (12.28) is such that

$lim t→∞ E(|X t -x * | 2 ) = 0$Remark 12.12 Condition (H1) assumes that H is bounded and uniformly Lipschitz in x, which is more restrictive than what was assumed in section 3.3.2, but applies, for example, to situations considered in Younes [[206]](#b224) and later in this book in section 17.2.2.

Condition (H3) implies that the Markov chain with transition P x is uniformly geometrically ergodic, but the ergodicity rate may depend on x and in particular converge to 1 when x tends to ∞, which is the situation targeted in this theorem.

The reader may refer to [[208]](#b226) for a general discussion of this problem with relaxed hypotheses and almost sure convergence, at the expense of significantly longer proofs.

♦

Proof We note that, from (12.29a), one has

$|X t -x * | ≤ C 0 σ t |X 0 -x * |.(12.33)$Similarly to section 3.3.2, we let A t = |X tx * | 2 and a t = E(A t ). One can then write

$A t+1 = A t +2α t+1 (X t -x * ) T H(X t )+2α t+1 (X t -x * ) T (H(X t , ξ t+1 )-H(X t ))+α 2 t+1 |H(X t , ξ t+1 )| 2 but we do not have E((X t -x * ) T (H(X t , ξ t+1 ) -H(X t )) | U t ) = 0$anymore, where U t is the σ -algebra of all past events up to time t (all events depending of X s , ξ s , s ≤ t). Indeed the Markovian assumption implies that

$E((X t -x * ) T (H(X t , ξ t+1 ) -H(X t )) | U t ) = (X t -x * ) T       R ξ H(X t , ξ)P X t (ξ t , dξ) -H(X t )       = (X t -x * ) T ((P X t H(X t , •))(ξ t ) -H(X t )),$which does not vanish in general. Following Benveniste et al. [[25]](#b43), this can be addressed by introducing the solution g(x, •) of the "Poisson equation"

$g(x, •) -P x g(x, •) = h(x, •).(12.34)$(Recall that h(x, ξ) = H(x, ξ) -H(x).) One can then write

$(X t -x * ) T h(X t , ξ t+1 ) = (X t -x * ) T (g(X t , ξ t+1 ) -P X t g(X t , ξ t+1 )$and

$A t+1 ≤ (1 -2α t+1 µ)A t + 2α t+1 (X t -x * ) T (g(X t , ξ t+1 ) -P X t g(X t , ξ t ))) + 2α t+1 (X t -x * ) T P X t g(X t , ξ t ) -2α t+1 (X t -x * ) T P X t g(X t , ξ t+1 ) + α 2 t+1 |H(X t , ξ t+1 )| 2$Introducing the notation

$η st = E((X s -x * ) T P X s g(X s , ξ t )).$Using the fact that

$E (X t -x * ) T (g(X t , ξ t+1 ) -P X t g(X t , ξ t ))) | U t = 0$and and noting that |H(X t , ξ t+1 )| 2 ≤ C 2 0 , this gives, after taking expectations,

$a t+1 ≤ (1 -2α t+1 µ)a t + 2α t+1 η tt -2α t+1 η t,t+1 + α 2 t+1 C 2 0 .$Applying lemma 3.25, and letting v s,t = t j=s+1 (1 -2α j+1 µ), we get

$a t ≤ a 0 v 0,t + 2 t s=1 v s,t α s+1 (η ss -η s,s+1 ) + C 2 0 t s=1 v s,t α 2 s+1 .$We now want to ensure that each term in the upper bound converges to 0. Similarly to section 3.3.2, (12.32a) implies that this holds the first and last terms and we therefore focus on the middle one, writing

$t s=1 v s,t α s+1 (η ss -η s,s+1 ) = v 1,t α 2 η 11 -α t+1 η t,t+1 + t s=2 (v s,t α s+1 -v s-1,t α s )η ss (12.35) + t s=2 v s-1,t α s (η ss -η s-1,s )$We will need the following estimates on the function g in (12.34), which is defined by

$g(x, ξ) = ∞ n=0 P n x h(x, ξ) = h(x, ξ) + ∞ n=0 P n x h(x, ξ).$Lemma 12. [13](#b31) We have

$|g(x, •)| ≤ C 0 + 2C 1 M(1 -ρ(x)) -1 , (12.36a$)

$|P x g(x, •)| ≤ 2C 1 M(1 -ρ(x)) -1 . (12.36b)$and, for all x, y ∈ R d and ξ ∈ R ξ Using lemma lemma 12.13 (which is proved at the end of the section), we can control the terms intervening in [(12.35)](#). Note that the first term, v 1t α 2 η 11 , converges to 0 since (12.32a) implies that v 1t converges to 0.

$|P x g(x, ξ) -P y (g(y, ξ)| = M 2 C 1 C 2 (1 -ρ) -2 + MC 1 (1 + C 2 )(1 -ρ) -1 . (12$We have,

$α t+1 |E((X t -x * ) T P X t g(X t , ξ t+1 ))| ≤ 2MC 1 α t+1 σ t (1 -ρ(σ t )) -1 ,$so that (12.32b) implies that α t+1 η t,t+1 → 0. and since α s+1 ≤ α s , we have

$t s=2 (v s,t α s+1 -v s-1,t α s )η ss ≤ t s=2 |v st α s -v s,t α s+1 | |η ss | ≤ MC 1 t s=2 |v s-1,t α s -v s,t α s+1 | α s+1 σ s (1 -ρ(σ s )) -1 ≤ C t s=2 |v s-1,t α s -v s,t α s+1 | for some constant C, since α s+1 σ s (1 -ρ(σ s )) -1 is bounded. Writing v s,t α s+1 -v s-1,t α s = v st (α s+1 -α s + 2µα 2 s ), we get (using α s+1 ≤ α s ) t s=2 |v s-1,t α s -v s,t α s+1 | ≤ t s=2 v st (α s -α s+1 ) + t s=2 v st 2µα 2 s .$Since both s (α sα s+1 ) and t s=2 α 2 s converge (the former is just α 1 ), lemma 3.26 implies that t s=2 (v s,t α s+1v s-1,t α s )η ss tends to zero. The last term to consider is

$t s=2 v s-1,t α s (η ss -η s-1,s ) = t s=2 v s-1,t α s E((X s -X s-1 ) T P X s g(X s , ξ s )) + t s=2 v s-1,t α s E((X s-1 -x * ) T (P X s g(X s , ξ s )) -P X s-1 g(X s-1 , ξ s ))).$We have

$t s=2 v s-1,t α s E((X s -X s-1 ) T P X s g(X s , ξ s )) ≤ 2C 0 C 1 M t s=2 v s-1,t α 2 s (1 -ρ(σ s )) -1$and

$t s=2 v s-1,t α s E((X s-1 -x * ) T (P X s g(X s , ξ s )) -P X s-1 g(X s-1 , ξ s ))) ≤ 2M 2 C 0 C 1 (1 + C 2 )|X 0 -x * | t s=2 v s-1,t α 2 s σ s (1 -ρ(σ s )) -2$and lemma 3.26 implies that both terms vanish at infinity. This concludes the proof of theorem 12.11.

## ■

Proof (Proof of lemma 12.13) Condition (H3) and proposition 12.3 and imply that (since π x h = 0)

$|P n x h(x, ξ)| ≤ D var (P n x (ξ, •), π x )osc( h(x, •)) ≤ 2C 1 Mρ(x) n so that g is well defined with |g(x, •)| ≤ C 0 + 2C 1 M(1 -ρ(x)) -1 , |P x g(x, •)| ≤ 2C 1 M(1 -ρ(x)) -1 .$We will also need to control differences of the kind P x g(x, ξ) -P y g(y, ξ).

We consider the nth term in the series, writing

$P n x h(x, ξ) -P n y h(y, ξ) = n-1 k=0 (P n-k x P k y h(y, ξ) -(P n-k-1$x P k+1 y h(y, ξ))

$+ P n x h(x, ξ) -P n x h(y, ξ).$This gives

$P n x h(x, ξ) -P n y h(y, ξ) = n-1 k=0 P n-k-1 x (P x P k y h(y, ξ) -P k+1 y h(y, ξ) -π x P k y h(y) + π x P k+1 y h(y)) + n-1 k=0 (π x P k y h(y) -π x P k+1 y h(y)) + P n x h(x, ξ) -P n x h(y, ξ) = n-1 k=0 P n-k-1 x (P x P k y h(y, ξ) -P k+1 y h(y, ξ) -π x P k y h(y) + π x P k+1 y h(y))$+ π x h(y)π x P n y h(y) + P n x h(x, ξ) -P n x h(y, ξ)

$Finally P n x h(x, ξ) -P n y h(y, ξ) = n-1 k=0 P n-k-1$x (P x P k y h(y, ξ) -P k+1 y h(y, ξ)π x P k y h(y) + π x P k+1 y h(y))

$+ P n x ( h(x, ξ) -h(y, ξ) + π x h(y)) -(π x -π y )P n y h(y)$Using proposition 12.3, we can write, letting ρ = max(ρ(|x|), ρ(|y|)),

$|P n-k-1 x (P x P k y h(y, ξ) -P k+1 y h(y, ξ) -π x P k y h(y, ξ) + π x P k+1 y h(y, ξ))| ≤ M ρn-k-1 osc(P x P k y h(y, ξ) -P k+1 y h(y, ξ)) ≤ C 2 M ρn-k-1 |x -y|osc(P k y h(y, ξ)) ≤ C 2 C 1 M 2 ρn-1 |x -y| We also have |P n x ( h(x, ξ) -h(y, ξ) + π x h(y, ξ))| ≤ MC 1 ρn |x -y| and |(π x -π y )P n y h(y, ξ)| ≤ MC 2 C 1 ρn |x -y| so that |P n x h(x, ξ) -P n y h(y, ξ)| ≤ MC 1 ρn-1 (nMC 2 + (1 + C 2 ) ρ)|x -y|$From this, it follows that

$|P x g(x, ξ) -P y (g(y, ξ)| ≤ MC 1 ∞ n=1 ρn-1 (nMC 2 + (1 + C 2 )|x -y| = M 2 C 1 C 2 (1 -ρ) -2 + MC 1 (1 + C 2 )(1 -ρ) -1 . ■ Chapter 13$
## Markov Random Fields

With this chapter, we start a discussion of large-scale statistical models in data science, starting with graphical models (Markov random fields and Bayesian networks) before discussing more recent approaches using, notably, deep learning. Important textbook references for the present chapter include Pearl [[151]](#b169), Ancona et al. [[8]](#b26), Winkler [[203]](#b221), Lauritzen [[114]](#b132), Cowell et al. [[56]](#b74), Koller and Friedman [[108]](#b126).

13.1 Independence and conditional independence

## Definitions

We consider random variables X, Y , Z . . ., and denote by R X , R Y , R Z . . . the sets in which they take their values. We discuss in this section concepts of independence and conditional independence between random variables. To simplify the exposition, we will work (unless mentioned otherwise) with discrete random variables (X is discrete if R X is finite or countable) [1](#foot_16) . We start with a basic definition.

$Definition 13.1 Two discrete random variables X : Ω → R X and Y : Ω → R Y are inde- pendent if and only if ∀x ∈ R X , ∀y ∈ R Y : P(X = x, Y = y) = P(X = x)P(Y = y).$The general definition for arbitrary r.v.'s is that

$E(f (X)g(Y )) = E(f (X)) E(g(Y ))$for any pair of (measurable) non-negative functions f : R X → [0, +∞) and g : R Y → [0, +∞).

One can easily check that X and Y are independent if and only if, for any nonnegative function g : R Y → R, one has

$E(g(Y ) | X) = E(g(Y )).$Notation 13.2 Independence is a property that involves two variables X and Y and an underlying probability distribution P. Independence of X and Y relative to P will be denoted (X Y ) P . However we will only write X Y when there is no ambiguity on P. ♦ More than independence, the concept of conditional independence will be fundamental in this chapter. It requires three variables, say X, Y , Z. Returning to the discrete case, one says that X and Y are conditionally independent given Z is, for any

$x ∈ R X , y ∈ R Y and z ∈ R Z such that P(Z = z) > 0, P(X = x, Y = y | Z = z) = P(X = x | Z = z) P(Y = y | Z = z). (13.1)$An equivalent statement is that, for any z such that P(Z = z) 0, X and Y are independent when P is replaced by the conditional distribution P(• | Z = z).

In the general case conditional independence means that, for any pair of nonnegative measurable functions f and g,

$E(f (X)g(Y ) | Z) = E(f (X) | Z) E(g(Y ) | Z). (13.2)$From now, we restrict our discussion to discrete random variables.

Multiplying both terms in (13.1) by P(Z = z) 2 , we get the equivalent statement:

X and Y are conditionally independent given Z if and only if, ∀x, y, z : P(X = x, Y = y, Z = z)P(Z = z) = P(X = x, Z = z) P(Y = y, Z = z). [(13.3)](#) Note that the identity is meaningful, and always true, for P(Z = z) = 0, so that this case does not need to be excluded anymore.

Conditional independence can be interpreted by the statement that X brings no more information on Y than what is already provided by Z: one has

$P(Y = y | X = x, Z = z) = P(Y = y, X = x, Z = z) P(X = x, Z = z) = P(Y = y, Z = z) P(Z = z)$as directly deduced from [(13.3)](#). (This computation being valid as soon as P(X =

x, Z = z) > 0.)

$Notation 13.3$To indicate that X and Y are conditionally independent given Z for the distribution P, we will write (X Y | Z) P or simply (X Y | Z).

$♦ (CI2) Decomposition: (X (Y , W ) | Z) ⇒ (X Y | Z). (CI3) Weak union: (X (Y , W ) | Z) ⇒ (X Y | (Z, W )). (CI4) Contraction: (X Y | Z) and (X W | (Z, Y )) ⇒ (X (Y , W ) | Z).$(CI5) Intersection: assume that the joint distribution of W , Y and Z is positive. Then

$(X W | (Z, Y )) and (X Y | (Z, W )) ⇒ (X (Y , W ) | Z).$Proof Properties (CI1) and (CI2) are easily deduced from (13.3) and left to the reader. To prove the last three, we will use the notation P (x), P (x, y) etc. instead of P(X = x), P(X = x, Y = y), etc. to save space. Identities are assumed to hold for all

x, y, z, w unless stated otherwise.

For (CI3), we must prove, according to (13.3), that P (x, y, z, w)P (z, w) = P (x, z, w)P (y, z, w) (13.4) whenever P (x, y, z, w)P (z) = P (x, z)P (y, z, w). Summing this last equation over y (or applying (CI2)) yields P (x, z, w)P (z) = P (x, z)P (z, w). We can note that all terms in (13.4) vanish when P (z) = 0, so that the identity is true in this case. When P (z) 0, the right-hand side of (13.4) becomes (P (x, z)P (z, w)/P (z))P (y, z, w) = (P (x, z)P (y, z, w)/P (z))P (z, w) = P (x, y, z, w)P (z, w), using once again the hypothesis. This proves (CI3).

For (CI4), the hypotheses are

$      $P (x, y, z)P (z) = P (x, z)P (y, z) P (x, y, z, w)P (y, z) = P (x, y, z)P (y, z, w)

and the conclusion must be P (x, y, z, w)P (z) = P (x, z)P (y, z, w). (13.5) Since (13.5) is true when P (y, z) = 0, we assume that this probability does not vanish and write P (x, y, z, w)P (z) = P (x, y, z)P (z)P (y, z, w)/P (y, z) = P (x, z)P (y, z)P (y, z, w)/P (y, z) = P (x, z)P (y, z, w) yielding (13.5).

For (CI5), assuming

$      $P (x, y, z, w)P (y, z) = P (x, y, z)P (y, z, w) P (x, y, z, w)P (z, w) = P (x, z, w)P (y, z, w), [(13.6)](#) we want to show that P (x, y, z, w)P (z) = P (x, z)P (y, z, w).

Since this identity is true when any of the events W = w, Y = y or Z = z has zero probability, we can assume that their probabilities are positive, which, by assumption, also implies that all joint probabilities are positive. From the two identities, we get P (x, y, z, w)/P (y, z, w) = P (x, y, z)/P (y, z) = P (x, z, w)/P (z, w)

This implies P (x, y, z) = P (y, z)P (x, z, w)/P (z, w)

that we can sum over y to obtain P (x, z) = P (z)P (x, z, w)/P (z, w)

We therefore get P (x, y, z, w)/P (y, z, w) = P (x, z, w)/P (z, w) = P (x, z)/P (z),

which is what we wanted.

## ■

A counter-example of (CI5) when the positivity assumption is not satisfied can be built as follows: let X be a Bernoulli random variable, and let Y = W = X. Let Z be any Bernoulli random variable, independent from X. Given Z and W , X and Y are constant and therefore independent. Similarly, given Z and Y , X and W are constant and therefore independent. However, given Z, X and (Y , W ) are not independent (they are equal and non constant).

## Mutual independence

Another concept of interest is the mutual (conditional) independence of more than two random variables. The random variables (X 1 , . . . , X n ) are mutually conditionally independent given Z if and only if

$E(f 1 (X 1 ) • • • f n (X n ) | Z) = E(f 1 (X 1 ) | Z) • • • E(f n (X n ) | Z)$for any non-negative measurable functions f 1 , . . . , f n . In terms of discrete probabilities, this can be written as

$P (X 1 = x 1 , . . . , X n = x n , Z = z)P (Z = z) n-1 = P (X 1 = x 1 , Z = z) • • • P (X n = x n , Z = z).$This will be summarized with the notation

$(X 1 • • • X n | Z).$We have the proposition Proposition 13.6 For variables X 1 , . . . , X n and Z, the following properties are equivalent.

$(i) (X 1 • • • X n | Z);$(ii) For all S, T ⊂ {1, . . . , n} with S ∩ T = ∅, we have:

$((X i , i ∈ S) (X j , j ∈ T ) | Z);$(iii) For all s ∈ {1, . . . , n}, we have: (X s (X t , t s) | Z);

(iv) For all s ∈ {2, . . . , n}, we have:

$(X s (X 1 , . . . , X s-1 ) | Z).$Proof It is clear that (i) ⇒ • • • ⇒ (iv) so it suffices to prove that (iv) ⇒ (i). For this, simply write (applying (iv) repeatedly to

$s = n -1, n -2, . . .) E(f 1 (X 1 ) • • • f n (X n ) | Z) = E(f 1 (X 1 ) • • • f n-1 (X n-1 ) | Z) E(f n (X n ) | Z) = E(f 1 (X 1 ) • • • f n-2 (X n-2 ) | Z) E(f n-1 (X n-1 ) | Z) E(f n (X n ) | Z) . . . = E(f 1 (X 1 ) | Z) • • • E(f n (X n ) | Z).$
## Relation with Information Theory

Several concepts in information theory are directly related to independence between random variables. Recall that the (Shannon) entropy of a discrete probability distribution over a finite set R is defined by

$H(P ) = - ω∈R log P (ω)P (ω). (13.7)$Similarly, the entropy of a random variable X : Ω → R X is defined by

$H(X) ∆ = H(P X ) = - x∈R X log P (X = x)P (X = x). (13.8)$The entropy is always non-negative, and provides a measure of the uncertainty associated to P . For a given finite set R, it is maximal when P is uniform over R, and minimal (and vanishes) when P is supported by a single ω ∈ R (i.e. P (ω) = 1).

One defines the entropy of two or more random variables as the entropy of their joint distribution, so that, for example,

$H(X, Y ) = - (x,y)∈R X ×R Y log P(X = x, Y = y)P(X = x, Y = y).$We have the proposition: Proposition 13.7 For random variables X 1 , . . . , X n , one has

$H(X 1 , . . . , X n ) ≤ H(X 1 ) + • • • + H(X n )$with equality if and only if (X 1 , . . . , X n ) are mutually independent.

Proof The proof of this proposition uses properties of the Kullback-Leibler divergence (c.f. (4.3)), given by, for two probability distributions π and π ′ on a finite set B,

$KL(π∥π ′ ) = ω∈B π(ω) log π(ω) π ′ (ω) .$with the convention π log(π/π ′ ) = 0 if π = 0 and = ∞ if π > 0 and π ′ = 0. Returning to proposition 13.7, a straightforward computation (which is left to the reader) shows that

$H(X 1 ) + • • • + H(X n ) -H(X 1 , . . . , X n ) = KL(π∥π ′ ) with π(x 1 , . . . , x n ) = P(X 1 = x 1 , . . . , X n = x n ) and π ′ (x 1 , . . . , x n ) = n k=1 P(X k = x k$). This makes proposition 13.7 a direct consequence of proposition 4.1.

## ■

The mutual information between two random variables X and Y is defined by

$I (X, Y ) = H(X) + H(Y ) -H(X, Y ). (13$.9)

From proposition 13.7, I (X, Y ) is nonnegative and vanishes if and only if X and Y are independent. Also from the proof of proposition 13.7, I (X, Y ) is equal to KL(P (X,Y ) ∥P X ⊗ P Y ) where the first probability is the joint distribution of X and Y and the second one the product of the marginals of X and Y , which coincides with P X,Y if and only if X and Y are independent.

If X and Y are two random variables, and y ∈ R Y with P(Y = y) > 0, the entropy of the conditional probability

$x → P(X = x | Y = y) is denoted H(X | Y = y), and is a function of y. The conditional entropy of X given Y , denoted H(X | Y ) is the expectation of H(X | Y = y) for the distribution of Y , i.e., H(X | Y ) = y∈R Y H(X | Y = y)P(Y = y) = - x∈R X y∈R Y log P(X = x | Y = y)P(X = x, Y = y).$So, we have (with a straightforward proof) Proposition 13.8 Given two random variables X and Y , we have

$H(X | Y ) = -E X,Y (log P(X = • | Y = •)) (13.10) = H(X, Y ) -H(Y )$This proposition also immediately yields:

$I (X, Y ) = H(X) -H(X | Y ) = H(Y ) -H(Y | X).$(13.11)

The identity H(X, Y ) = H(X | Y ) + H(Y ) that is deduced from proposition 13.8 can be generalized to more than two random variables (the proof being left to the reader), yielding, if X 1 , . . . , X n are random variables:

$H(X 1 , . . . , X n ) = n k=1 H(X k | X 1 , . . . , X k-1 ). (13.12)$If Z is an additional random variable, the following identity is obtained by applying the previous one to conditional distributions given Z = z and taking averages over z:

$H(X 1 , . . . , X n | Z) = n k=1 H(X k | X 1 , . . . , X k-1 , Z). (13.13)$The following proposition characterizes conditional independence in terms of entropy.

Proposition 13.9 Let X, Y and Z be three random variables. The following statements are equivalent.

(i) X and Y are conditionally independent given Z.

$(ii) H(X, Y | Z) = H(X | Z) + H(Y | Z) (iii) H(X | Y , Z) = H(X | Y )$Moreover, when (i) to (iii) are satisfied, we have:

$(iv) I (X, Y ) ≤ min(I (X, Z), I (Y , Z)).$Proof From proposition 13.7, we have, for any three random variables X, Y , Z, and any z such that P (Z = z) > 0,

$H(X, Y | Z = z) ≤ H(X | Z = z) + H(Y | Z = z).$Taking expectations on both sides implies the important inequality

$H(X, Y | Z) ≤ H(X | Z) + H(Y | Z) (13.14)$and equality occurs if and only if P(

$X = x, Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z) whenever P(Z = z) > 0,$that is, if and only if X and Y are conditionally independent given Z. This proves that (i) and (ii) are equivalent. The fact that (ii) and (iii) are equivalent comes from (13.13), which gives, for any three random variables

$H(X, Y | Z) = H(X | Y , Z) + H(Y | Z). (13.15)$To prove that (i)-(iii) implies (iv), we note that (13.14) and (13.15) imply that, for any three random variables:

$H(X | Y , Z) ≤ H(X | Y ).$If X and Y are conditionally independent given Z, then the right-hand side is equal to H(X | Z) and this yields

$I (X, Y ) = H(X) -H(X | Y ) ≤ H(X) -H(X | Z) = I (X, Z).$By symmetry, we must also have I (X, Y ) ≤ I (Y , Z) so that (iv) is true. ■ Statement (iv) is often called the data-processing inequality, and has been used to infer conditional independence within gene networks [[125]](#b143).

## Models on undirected graphs 13.2.1 Graphical representation of conditional independence

An undirected graph is a collection of vertexes and edges, in which edges link pairs of vertexes without order. Edges can therefore be identified to subsets of cardinality two of the set of vertexes, V . This yields the definition: Definition 13.10 An undirected graph G is a pair G = (V , E) where V is a finite set of vertexes and elements e ∈ E are subsets e = {s, t} ⊂ V .

Note that edges in undirected graphs are defined as sets, i.e., unordered pairs, which are delimited with braces in these notes. Later on, we will use parentheses to represent ordered pairs, (s, t) (t, s). We will write s ∼ G t, or simply s ∼ t to indicate that s and t are connected by an edge in G (we also say that s and t are neighbors in G). Definition 13.11 A path in an undirected graph G = (V , E) is a finite sequence (s 0 , . . . , s N ) of vertexes such that s k-1 ∼ s k ∈ E. (A sequence, (s 0 ), of length 1 is also a path by extension.) We say that s and t are connected by a path if either s = t or there exists a path (s 0 , . . . , s N ) such that s 0 = s and s N = t.

A subset S ⊂ G is connected if any pair of elements in S can be connected by a path.

A subset T ⊂ G separates two other subsets S and S ′ if all paths between S and S ′ must pass in T . We will write (S S ′ | T ) in such a case.

One of the goals of this chapter is to relate the notion of conditional independence within a set of variables to separation in a suitably chosen undirected graph with vertexes in one-to-one correspondence with the variables. This will also justifies the similarity of notation used for separation and conditional independence.

We have the following simple fact: Lemma 13.12 Let G = (V , E) be an undirected graph, and S, S ′ , T ⊂ V . Then

$(S S ′ | T ) ⇒ S ∩ S ′ ⊂ T .$Indeed, if (S S ′ | T ) and s 0 ∈ S ∩ S ′ , the path (s 0 ) links S and S ′ and therefore must pass in T . Proposition 13.5 translates into similar properties for separation: Proposition 13.13 Let (V , E) be an undirected graph and S, T , U , R be subsets of V . The following properties hold

$(i) (S T |U ) ⇔ (T S|U ). (ii) (S T ∪ R|U ) ⇒ (S T |U ). (iii) (S T ∪ R|U ) ⇒ (S T |U ∪ R). (iv) (S T | U ) and (S R | U ∪ T ) ⇔ (S T ∪ R | U ). (v) U ∩ R = ∅, (S R | U ∪ T ) and (S U | T ∪ R) ⇒ (S U ∪ R | T ).$Proof (i) is obvious, and for (ii) (and (iii)), if any path between S and T ∪ R must pass by U , the same is obviously true for a path between S and T .

For the ⇒ part of (iv), if a path links S and T ∪ R, then it either links S and T and must pass through U by the first assumption, or link S and R and therefore pass through U or T by the second assumption. But if the path passes through T , it must also pass through U before by the first assumption. In all cases, the path passes through U . The ⇐ part of (iv) is obvious.

Finally, consider (v) and take a path between two distinct elements in S and U ∪R. Consider the first time the path hits U or R, and assumes that it hits U (the other case being treated similarly by symmetry). Notice that the path cannot hit both U and R at the same point since U ∩ R = ∅. From the assumptions, the path must hit T ∪ R before passing by U , and the intersection cannot be in R, so it is in T , which is the conclusion we wanted.

## ■

To make a connection between separation in graphs and conditional independence between random variables, we consider a graph G = (V , E) and a family of random variables (X (s) , s ∈ V ) indexed by V . Each variable is assumed to take values in a set F s = R X (s) . The collection of values taken by the random variables will be called configurations, and the sets F s , s ∈ V are called the state spaces.

Letting F denote the collection (F s , s ∈ V ), we will denote the set of such configurations as F (V , F ). Then F is clear from the context, we will just write F (V ). If S ⊂ V and x ∈ F (V , F ), the restriction of x to S is denoted x (S) = (x (s) , s ∈ S). The set formed by those restrictions will be denoted F (S, F ) (or just F (S)).

Remark 13.14 Some care needs to be given to the definition of the space of configurations, to avoid ambiguities when two sets F s coincide. The configuration x = (x (s) , s ∈ V ) should be understood, in an emphatic way, as the collection x = ((s, x (s) ), s ∈ V ), which makes explicit the fact that x (s) is the value observed at vertex s. Similarly the emphatic notation for x (S) ∈ F (V , F ) is x(S) = ((s, x (s) ), s ∈ S).

In the following, we will not use the emphatic notation to avoid overly heavy expressions, but its relevance should be clear with the following simple example. Take V = {1, 2, 3} and F 1 = F 2 = F 3 = {0, 1}. Let x (1) = 0, x (2) = 0 and x (3) = 1. Then the sub-configurations x ({1,3}) and x ({2,3}) both corresponds to values (0, 1), but we consider them as distinct. In the same spirit, x (1) = x (2) , but

$x ({1}) x ({2}) ♦ If S, T ⊂ V with S ∩ T = ∅, x (S) ∈ F (S, F ), y (T ) ∈ F (T , F$), we will denote their concatenation by x (S) ∧ y (T ) , which is the configuration z

$= (z s , s ∈ S ∪ T ) ∈ F (T ∪ S, F ) such that z (s) = x (s) if s ∈ S and z (s) = y (s) if s ∈ T .$We define a random field over V as a random configuration X : Ω → F (V , F ), that we will denote for short X = (X (s) , s ∈ V ). If S ⊂ V , the restriction X (S) will also be denoted (X (s) , s ∈ S).

We can now write the definition: Definition 13.15 Let G = (V , E) be an undirected graph and X = (X (s) , s ∈ V ) a random field over V . We say that X is Markov (or has the Markov property) relative to G (or is G-Markov, or is a Markov random field on G) if and only if, for all S, T , U ⊂ V :

$(S T | U ) ⇒ (X (S) X (T ) | X (U ) ).$(13.16)

Letting the observation over an empty set S be empty, i.e., X ∅ = ∅, this definition includes the statement that, if S and T are disconnected (i.e., there is no path between them: they are separated by the empty set), then (X (S) X (T ) | ∅): X (S) and X (T ) are independent.

We will say that a probability distribution π on

$F (V ) is G-Markov if its associated canonical random field X = (X (s) , s ∈ V ) defined on Ω = F (V ) by X (s) (x) = x (s) is G- Markov.$
## Reduction of the Markov property

We now proceed, in a series of steps, to a simplification of definition 13.15 in order to obtain a minimal number of conditional independence statements. Note that, in its current form, definition 13.15 requires to check (13.16) for any three subsets of V , which provides a huge number of conditions. Fortunately, as we will see, these conditions are not independent, and checking a much smaller number of them will ensure that all of them are true.

The first step for our reduction is provided by the following lemma.

Lemma 13.16 Let G = (V , E) be an undirected graph and X = (X s , s ∈ V ) a set of random variables indexed by V . Then X is G-Markov if and only if, for S, T , U ⊂ V ,

$S ∩ U = T ∩ U = ∅ and (S T | U ) ⇒ (X (S) X (T ) | X (U ) ).$(13.17)

Proof Assume that (13.17) is true, and take any S, T , U with

$(S T | U ). Let A = S ∩ U , B = T ∩ U and C = A ∪ B. Partition S in S = S 1 ∪ A, T in T 1 ∪ B and U in U 1 ∪ C. From (S T | U ), we get (S 1 T 1 | U ). Since S 1 ∩ U = T 1 ∩ U = ∅, this implies (X (S 1 ) X (T 1 ) | X (U )$). But this implies ((X (S 1 ) , X (A) ) (X (T 1 ) , X (B) ) | X (U ) ). Indeed, this property requires

$P X (x (S 1 ) ∧ x (A) ∧ x (T 1 ) ∧ x (B) ∧ x (U 1 ) ∧ y (C) )P X (x (U 1 ) ∧ y (C) ) = P X (x (S 1 ) ∧ x (A) ∧ x (U 1 ) ∧ y (C) )P X (x (T 1 ) ∧ x (B) ∧ x (U 1 ) ∧ y (C) )$If the configurations x (A) , x (B) , y (C) are not consistent (i.e., x (t) y (t) for some t ∈ C), then both sides vanish. So we can assume x (C) = y (C) and remove x (A) and x (B) from the expression, since they are redundant. The resulting identity is true since it exactly states that (X (S 1 ) X (T 1 ) | X (U ) ).

## ■

Define the set of neighbors of s ∈ V (relative to the graph G) as the set of t s such that {s, t} ∈ E and denote this set by V s . For S ⊂ V define also

$V S = S c ∩ s∈S V s$which is the set of neighbors of all vertexes in S that do not belong to S. (Here S c denotes the complementary set of S, S c = V \ S.) Finally, let W S denote the vertexes that are "remote" from S, W S = (S ∪ V S ) c .

We have the following important reduction of the condition in definition 13.15.

Proposition 13.17 X is Markov relative to G if and only if, for any S ⊂ V , (X (S) X (W S ) | X (V S ) ). [(13.18)](#) This says that P(

$X (S) = x (S) | X (S c ) = x (S c ) )$only depends (when defined) on variables x (t) for t ∈ S ∪ V S .

Proof First note that (S W S | V S ) is always true, since any path reaching S from S c must pass through V S . This immediately proves the "only if" part of the proposition.

Consider now the "if" part. Take S, T , U such that (S T | U ). We want to prove that (X S X T | X U ). According to lemma 13.16, we can assume, without loss of generality, that S ∩ U = T ∩ U = ∅.

Define R as the set of vertexes v in V such that there exists a path between S and v that does not pass in U . Then:

1. S ⊂ R: the path (s) for s ∈ S does not pass in U since S ∩ U = ∅.

2. U ∩ R = ∅ by definition.

3. V R ⊂ U : assume that there exists a point r in V R which is not in U . Then r has a neighbor, say r ′ in R. By definition of R, there exists a path from S to r ′ that does not hit U , and this path can obviously be extended by adding r at the end to obtain a path that still does not hit U . But this implies that r ∈ R, which contradicts the fact that

$V R ∩ R = ∅. 4. T ∩ (R ∪ V R ) = ∅: if t ∈ T , then t R from (S T | U ) and t V R from T ∩ U = ∅.$We can then write (each decomposition being a partition, implicitly defining the sets A, B and C, see Fig. [13](#fig_160).

$1) R = S ∪ A, U = V R ∪ C, (R ∪ V R ) c = T ∪ C ∪ B, and from (X (R) X (W R ) | X (V R ) ), we get ((X (S) , X (A) ) (X (T ) , X (C) , X (B) ) | X (V R ) )$Figure [13](#fig_160).1: See proof of proposition 13.17 for details which implies ((X (S) , X (A) ) (X (T ) , X (B) ) | X (U ) ) by (CI3), which finally implies (X (S) X (T ) | X (U ) ) by (CI2).

## ■

For positive probabilities, it suffices to consider singletons in proposition 13.17.

Proposition 13.18 If the joint distribution of (X (s) , s ∈ V ) is positive and, for any s ∈ V , (X (s) X (W s ) | X (V s ) ), [(13.19)](#) then X is Markov relative to G. The converse statement is true without the positivity assumption.

Proof It suffices to prove that, if [(13.18](#)) is true for S and T ⊂ V , with T ∩ S = ∅, it is also true for S ∪ T . The result will then follow by induction.

$So, let U = V S∪T and R = W S∪T = V \ (S ∪ T ∪ U ). Then, we have (X (S) X (W S ) | X (V S ) ) ⇒ (X (S) X (R) | (X (U ) , X (T ) ))$because R ⊂ W S (if s ∈ V S , then it is either in U or in T and therefore cannot be in R). Similarly, (X (T ) X (R) | (X (U ) , X (S) )), and (CI5) (for which we need P positive) now implies ((X (T ) , X (S) ) X (R) | X (U ) ).

## ■

To see that the positivity assumption is needed, consider the following example with six variables X (1) , . . . , X (6) , and a graph linking consecutive integers and closing with an edge between 1 and 6. Assume that X (1) = X (2) = X (4) = X (5) , and that X (1) , X (3)  and X (6) are independent. Then [(13.19](#)) is true, since, for k = 1, 2, 4, 5, X (k) is constant given its neighbors, and X (3) (resp. X (6) ) is independent of the rest of the variables. But (X (1) , X (2) ) is not independent of (X (4) , X (5) ) given the neighbors X (3) , X (6) .

Finally, another statement equivalent to proposition 13.18 is the following:

Proposition 13. [19](#b37) If the joint distribution of (X (s) , s ∈ V ) is positive and, for any s, t ∈ V ,

$s ≁ G t ⇒ (X (s) X (t) | X (V \{s,t}) ),$then X is Markov relative to G. The converse statement is true without the positivity assumption.

Proof Fix s ∈ V and assume that (X (s) X (R) | X (V \R) ) for any R ⊂ W s with cardinality at most k (the statement is true for k = 1 by assumption). Consider a set R ⊂ W s of cardinality k + 1, that we decompose into R ∪ {t} for some t ∈ R. We have (X (s) X (t) | X (V \ R) , X R ) from the initial hypothesis and (X (s) X (R) | X (V \ R) , X t ) from the induction hypothesis. Using property (CI5), this yields (X (s) X ( R) | X (V \ R) ). This proves the proposition by induction.

## ■

Remark 13.20 It is obvious from the definition of a G-Markov process that, if X is Markov for a graph G = (V , E), it is automatically Markov for any richer graph, i.e., any graph G = (V , Ẽ) with E ⊂ Ẽ. This is because separation in G implies separation in G. Moreover, any X is G-Markov for the complete graph on V , for which s ∼ t for all s t ∈ V . This is because no pair of sets can be separated in a complete graph.

Any graph with respect to which X is Markov must be richer than the graph G X = (V , E X ) defined by s ≁ G X t if and only (X (s) X (t) | X ({s,t} c ) ). This is true because, for any graph G for which X is Markov, we have

$s ≁ G t ⇒ (X (s) X (t) | X ({s,t} c ) ) ⇒ s ≁ G X t.$Interestingly, proposition 13.19 states that X is G X -Markov as soon as its joint distribution is positive. This implies that G X is the minimal graph over which X is Markov in this case. ♦

## Restricted graph and partial evidence

Assume that some variables X (T ) = (X (t) , t ∈ T ) (with T ⊂ V ) have been observed, with observed values x (T ) = (x (t) , t ∈ T ). One would like to use this partial evidence to get additional information on the remaining variables, X (S) where S = V \T . From the probabilistic point of view, this means computing the conditional distribution of X (S) given X (T ) = x (T ) .

One important property of G-Markov models is that the Markov property is essentially conserved when passing to conditional distributions. We introduce for this the following definitions.

$Definition 13.21 If G = (V , E) is an undirected graph, a subgraph of G is a graph G ′ = (V ′ , E ′ ) with V ′ ⊂ V and E ′ ⊂ E.$If S ⊂ V , the restricted graph, G S , of G to S is defined by G S = (S, E S ) with E S = {e = {s, t} : s, t ∈ S and e ∈ E} .

(13.20)

We have the following proposition.

Proposition 13.22 Let G = (V , E) be an undirected graph and X be G-Markov. Let S ⊂ V and T = S c . Given a partial evidence x (T ) such that P (X (T ) = x (T ) ) > 0, X (S) , conditionally to

$X (T ) = x (T ) , is G S -Markov.$Proof The proof is straightforward once it is noticed that

$(A B | C) G S ⇒ (A B | C ∪ T ) G ■ so that (A B | C) G S ⇒ (X (A) X (B) | X (C) , X (T ) ) P ⇒ (X (A) X (B) | X (C) ) P (•|X (T ) =x (T ) )$
## Marginal distributions

The effect of taking marginal distributions for a G-Markov model is, unfortunately, not as much a mild operation as computing conditional distributions, in the sense that the conditional independence structure of the marginal distribution may be much more complex than the original one.

Let G = (V , E) be an undirected graph, and let S be a subset of V . Define the graph G S = (S, E S ) by {s, t} ∈ E S if and only if {s, t} ∈ E or there exist u, u ′ ∈ S c such that {s, u} ∈ E, {t, u ′ } ∈ E and u and u ′ are connected by a path in S c . In other terms E S links all s, t ∈ S that can be connected by a path, all but the extremities of which are included in S c . With this notation, the following proposition holds.

Proposition 13.23 Let G = (V , E) be an undirected graph, and

$S ⊂ V . Assume that X = (X (s) , s ∈ V ) is a family of random variables which is G-Markov. Then X (S) = (x (s) , s ∈ S) is G S -Markov.$Proof It suffices to prove that, for A, B, C ⊂ S,

$(A B | C) G S ⇒ (A B | C) G .$So, assume that A and B are separated by C in G S . If a path connects A and B in G, we can, by definition of E S , remove from this path any portion that passes in S c and obtain a valid path in G S . By assumption, this path must pass in C, and therefore so does the original path.

## ■

The graph G S can be much more complex than the restricted graph G S introduced in the previous section (note that, by definition, G S is richer than G S ). Take, for example, the graph that corresponds to "hidden Markov models," for which (cf. fig. [13.](#fig_160)2)

$V = {1, . . . , N } × {0, 1}$and edges {s, t} ∈ E have either s = (k, 0) and t = (l, 0) with |k -l| = 1, or s = (k, 0) and t = (k, 1). Let S = {1, . . . , N } × {1}. Then, G S is totally disconnected (E S = ∅), since no edge in G links two elements of S. In contrast, any pair of elements in S is connected by a path in S c , so that G S is a complete graph.

Figure [13](#fig_160).2: In this graph, variables in the lower row are conditionally independent given the first row, while their marginal distribution requires a completely connected graph.

## The Hammersley-Clifford theorem

The Hammersley-Clifford theorem, which will be proved in this section, gives a complete description of positive Markov processes relative to a given graph, G. It states that positive G-Markov models are associated to families of positive local interactions indexed by cliques in the graph. We now introduce each of these concepts.

13.3.1 Families of local interactions Definition 13.24 Let V be a set of vertexes and (F s , s ∈ V ) a collection of state spaces. A family of local interactions is a collection of non-negative functions Φ = (ϕ C , C ∈ C) indexed over some subset C of P (V ), such that each ϕ C only depends on configurations restricted to C (i.e., it is defined on F (C)), with values in [0, +∞). (Recall that P (V ) is the set of all subsets of V .) Such a family has order p if no C ∈ C has cardinality larger than p. A family of local interactions of order 2 is also called a family of pair interactions.

Such a family is said to be consistent, if there exists an x ∈ F (V ) such that C∈C ϕ C (x (C) ) 0.

To a consistent family of local interactions, one associates the probability distribution π Φ on F (V ) defined by

$π Φ (x) = 1 Z Φ C∈C ϕ C (x (C) ) (13.21)$for all x ∈ F (V ), where Z Φ is a normalizing constant.

Given C ⊂ P (V ), define the graph G C = (V , E C ) by letting {s, t} ∈ E C if and only if there exists C ∈ C such that {s, t} ∈ C. We then have the following proposition.

$Proposition 13.25 Let Φ = (ϕ C , C ⊂ C) be a consistent family of local interactions, asso- ciated to some C ⊂ P (V ). Then the associated distribution π Φ is G C -Markov.$Proof Let X be a random field associated with π = π Φ . According to proposition 13.17, we must show that, for any S ⊂ V , one has

$(X (S) X (W S ) | X (V S ) )$where V S is the set of neighbors of S in G C and

$W S = V \ (V S ∪ S). Define the set U S by U S = C∈C,S∩C ∅ C so that V S = U S \ S and W S = V \ U S .$To prove conditional independence, we need to prove that, for any x ∈ F:

$π(x)π V S (x (V S ) ) = π U S (x (U S ) )π V \S (x (V \S) )(13.22)$(where we denote π A the marginal distribution of π on F (A).)

From the definition of π, we have

$π(x) = 1 Z C∈C ϕ C (x (C) ) = 1 Z C:C∩S ∅ ϕ C (x (C) ) C:C∩S=∅ ϕ C (x (C) ).$The first term in the last product only depends on x (U S ) , and the second one only on

$x (V \S) . Introduce the notation                  µ 1 (x (V S ) ) = y (U S ) :y (V S ) =x (V S ) C:C∩S ∅ ϕ C (x (C) ) µ 2 (x (V S ) ) = y (V \S) :y (V S ) =x (V S ) C:C∩S=∅ ϕ C (x (C) ).$With this notation, we have:

$                     π U S (x (U S ) ) = (µ 2 (x (V S ) )/Z) C:C∩S ∅ ϕ C (x (C) ) π V \S (x (V \S) ) = (µ 1 (x (V S ) )/Z) C:C∩S=∅ ϕ C (x (C) ) π V S (x (V S ) ) = µ 1 (x (V S ) )µ 2 (x (V S ) )/Z$from which [(13.22](#)) can be easily obtained.

## ■

We now discuss conditional distributions and marginals for processes associated with local interactions. If T ⊂ V , we let π T = π Φ T denote the marginal distribution of π on T .

We start with a discussion of conditionals. Let π be associated with Φ, and let S ⊂ V and T = V \S. Assume that a configuration y (T ) is given, such that π T (y (T ) ) > 0, and consider the conditional distribution π S|T (x (S) | y (T ) ) = π(x (S) ∧ y (T ) )/π T (y (T ) ).

(13.23)

We have the following proposition. 

$C S = C : C ⊂ S, ∃C ∈ C : C = C ∩ S and ϕ C|y (T ) (x ( C) ) = C∈C:C∩S= C ϕ C (x ( C) ∧ y (C∩T ) ).$Proof From (13.23) and the definition of π, it is easy to sees that

$π S|T (x (S) | y (T ) ) = 1 Z(y (T ) ) C:C∩S ∅ ϕ C (x (C∩S) ∧ y (C∩T ) ),$where Z(y (T ) ) is a constant that only depends on y (T ) . The fact that π S|T (• | y (T ) ) is associated to Φ |y (T ) is then obtained by reorganizing the product over distinct S ∩ C's.

## ■

This result, combined with proposition 13.25, is consistent with proposition 13.22, in the sense that the restriction to G C to S coincides with the graph G C S . The easy proof is left to the reader.

We now consider marginals, and more specifically marginals when only one node is removed, which provides the basis for "node elimination." Proposition 13.27 Let π be associated to Φ = (ϕ C , C ∈ C) as above. Let t ∈ V and S = V \ {t}. Define C t ∈ P (V ) as the set

$C t = {C ∈ C : t C} ∪ { Ct } with Ct = C∈C:t∈C C \ {t}.$
## Define a family of local interactions

$Φ t = ( φ C , C ∈ C t ) by φ C = ϕ C if C Ct and: • If Ct C: φ Ct (x ( Ct ) ) = y (t) ∈F t C∈C,t∈C ϕ C (x ( Ct ) ∧ y (t) ). • If Ct ∈ C: φ Ct (x ( Ct ) ) = ϕ C t (x ( Ct ) ) y (t) ∈F t C∈C,t∈C ϕ C (x (C t ) ∧ y (t) )$Then the marginal, π S , of π over S is the distribution associated to Φ t .

The proof is almost straightforward by summing over possible values of y t in the expression of π and left to the reader.

## Characterization of positive G-Markov processes

Using families of local interactions is a typical way to build graphical models in applications. The previous section describes a graph with respect to which the obtained process is Markov. Conversely, given a graph G, the Hammersley-Clifford theorems states that families of local interactions over the cliques of G are the only ways to build positive graphical models, which reinforces the importance of this construction. We now pass to the statement and proof of this theorem, starting with the following definition.

Definition 13.28 Let G = (V , E) be an undirected graph. A clique in G is a nonempty subset C ⊂ V such that s ∼ G t whenever s, t ∈ C, s t. (In particular, subsets of cardinality one are always cliques.) Cliques therefore form complete subgraphs of G.

The set of cliques of a graph G will be denoted C G .

A clique that cannot be strictly included in any other clique is called a maximal clique, and their set denoted C * G .

(Note that some authors call cliques what we refer to as maximal cliques.)

Given G = (V , E), consider a random field X = (X (s) , s ∈ V ). We assume that X (s) takes values in a finite set F s with P(X (s) = a) > 0 for any a ∈ F s (this is no loss of generality since one can always restrict F s to such a's). If S ⊂ V , we denote as before F (S) the set of restrictions of configurations to S. With this notation, X is positive, according to definition 13.4, if and only if P (X = x) > 0 for all x ∈ F (V ). We will let π = P X be the probability distribution of X, so that π(x) = P(X = x) and use as above

$the notation: for S, T ⊂ V        π S (x (S) ) = P(X (S) = x (S) ) π S|T (x (S) | x (T ) ) = P(X (S) = x (s) | X (T ) = x (T ) ).$(13.24) (For the first notation, we will simply write π if S = V .)

We will also need to fix a reference, or "zero," configuration in F (V ) that we will denote 0 = (0 (s) , s ∈ V ), with 0 (s) ∈ F s for all s. We can choose it arbitrarily. Given this, we have the theorem: Theorem 13.29 (Hammersley-Clifford) With the previous notation, X is a positive G-

## Markov process if and only if its distribution, π, is associated to a family of local interactions

$Φ = (ϕ C , C ⊂ C G ) such that ϕ C (x (C) ) > 0 for all x (C) ∈ F (C).$Moreover, Φ is uniquely characterized by the additional constraint: ϕ C (x (C) ) = 1 as soon as there exists s ∈ C such that x (s) = 0 (s) .

Letting λ C =log ϕ C , we get an equivalent formulation of the theorem in terms of potentials, where a potential is defined as a family of functions

$Λ = (λ C , C ∈ C)$indexed by a subset C of P (V ), such that λ C only depends on x (C) . The distribution associated to Λ is

$π(x) = 1 Z Λ exp - C∈C λ C (x (C) ) . (13.25)$With this terminology, we trivially have an equivalent formulation:

$Theorem 13.30 X is a positive G-Markov process if and only if its distribution, π, is associated to a potential Λ = (λ C , C ⊂ C G ).$Moreover, Λ is uniquely characterized by the additional constraint: λ C (x (C) ) = 0 as soon as there exists s ∈ C such that x (s) = 0 (s) .

We now prove this theorem.

Proof Let us start with the "if" part. If π is associated to a potential over C G , we have already proved that π is G C G -Markov, so that it suffices to prove that

$G C G = G, which is almost obvious: If s ∼ G t, then {s, t} ∈ C G and s ∼ G C G t by definition of G C G . Conversely, if s ∼ G C G t, there exists C ∈ C G such that {s, t} ⊂ C, which implies that s ∼ G t, by definition of a clique.$We now prove the "only if" part, which relies on a combinatorial lemma, which is one of Möbius's inversion formulas.

## Lemma 13.31

Let A be a finite set and f :

$P (A) → R, B → f B . Then, there is a unique function λ : P (A) → R such that ∀B ⊂ A, f B = C⊂B λ C , (13.26)$and λ is given by

$λ C = B⊂C (-1) |C|-|B| f B . (13.27)$To prove the lemma, first notice that the space F of functions f : P (A) → R is a vector space of dimension 2 |A| and that the transformation ϕ : λ → f with f B = C⊂B λ C is linear. It therefore suffices to prove that, given any f , the function λ given in (13.27) satisfies ϕ(λ) = f , since this proves that ϕ is onto from F to F and therefore necessarily one to one.

So consider f and λ given by (13.27). Then

$ϕ(λ)(B) = C⊂B λ C = C⊂B B⊂C (-1) |C|-| B| f B = B⊂B         C⊃ B,C⊂B (-1) |C|-| B|         f B = f B$The last identity comes from the fact that, for any finite set B ⊂ B, B B, we have So the lemma is proved. We now proceed to proving the existence and uniqueness statements in theorem 13.30. Assume that X is G-Markov and positive. Fix x ∈ F (V ) and consider the function, defined on P (V ) by

$f B (x (B) ) = -log π(x (B) ∧ 0 (B c ) ) π(0) . Then, letting λ C (x (C) ) = B⊂C (-1) |C|-|B| f B (x (B) ), we have f B (x (B) ) = C⊂B λ C (x (C) ).$In particular, for B = V , this gives

$π(x) = 1 Z exp - C⊂V λ C (x (C) )$with Z = P (0). We now prove that λ C (x (C) ) = 0 if x (s) = 0 (s) for some s ∈ V or if C C G .

This will prove (13.25) and the existence statement in theorem 13.30.

So, assume x (s) = 0 (s) . Then, for any B such that s B, we have f B (x (B) ) = f {s}∪B (x ({s}∪B) ).

Now take C with s ∈ C. We have

$λ C (x (C) ) = B⊂C,s∈B (-1) |C|-|B| f B (x (B) ) + B⊂C,s B (-1) |C|-|B| f B (x (B) ) = B⊂C,s B (-1) |C|-|B∪{s}| f B∪{s} (x (B∪{s}) ) + B⊂C,s B (-1) |C|-|B| f B (x (B) ) = B⊂C,s B ((-1) |C|-|B∪{s}| + (-1) |C|-|B| )f B (x (B) ) = 0.$Now assume that C is not a clique, and let s t ∈ C such that s ≁ t. We can write, using decompositions similar to the above,

$λ C (x (C) ) = B⊂C\{s,t} (-1) |C|-|B| f B∪{s,t} (x (B∪{s,t}) ) -f B∪{s} (x (B∪{s}) ) -f B∪{t} (x (B∪{t}) ) + f B (x (B) ) . But, for B ⊂ C \ {s, t}, we have f B∪{s,t} (x (B∪{s,t}) ) -f B∪{s} (x (B∪{s}) ) = -log π(x (B∪{s,t}) ∧ 0 (B c \{s,t}) ) π(x (B∪{s}) ∧ 0 (B c \{s}) ) = log π t (x (t) | x (B∪{s}) ∧ 0 (B c \{s,t}) ) π t (0 (t) | x (B∪{s}) ∧ 0 (B c \{s,t}) )$and

$f B∪{t} (x (B∪{t}) ) -f B (x (B) ) = -log π(x (B∪{t}) ∧ 0 (B c \{t}) ) π(x (B) ∧ 0 (B c ) ) = log π t (x (t) | x (B) ∧ 0 (B c \{t}) ) π t (0 (t) | x (B) ∧ 0 (B c \{t}) )$.

So, we can write

$λ C (x (C) ) = B⊂C\{s,t} (-1) |C|-|B| log π t (x (t) | x (B∪{s}) ∧ 0 (B c \{s,t}) )π t (0 (t) | x (B) ∧ 0 (B c \{t}) ) π t (0 (t) | x B∪{s} ∧ 0 (B c \{s,t}) )π t (x (t) | x (B) ∧ 0 (B c \{t}) )$which vanishes, because

$π t (x (t) | x (B∪{s}) ∧ 0 (B c \{s,t}) ) = π t (x (t) | x (B) ∧ 0 (B c \{t}) )$when s ≁ t.

To prove uniqueness, note that, for any zero-normalized Λ satisfying (13.25), we must have π(0) = 1/Z and therefore, for any x,

$-log π(x (B) ∧ 0 (B c ) ) π(0) = C⊂B λ C (x (C) )$(extending Λ so that λ C = 0 for C C G ). But, from lemma 13.31, this uniquely defines Λ.

## ■

The exponential form of the distribution in the Hammersley-Clifford theorem is related to what is called a Gibbs distribution in statistical mechanics. More precisely: Definition 13.32 Let F be a finite set and W : F → R be a scalar function. The Gibbs distribution with energy W at temperature T > 0 is defined by

$π(x) = 1 Z T e -W (x) T , x ∈ F$The normalizing constant Z T = y∈F exp(-W (y)/T ) is called the partition function.

$If Λ = (λ C , C ⊂ V$) is a potential then its associated energy is

$W (x) = C⊂V λ C (x (C) ).$So the Hammersley-Clifford theorem implies that any positive G-Markov model is associated to a unique zero-normalized potential defined over the cliques of G. This representation can also be used to provide an alternate proof of proposition 13.19, which is left to the reader. Finally, one can restate proposition 13.26 in terms of potentials, yielding:

Proposition 13.33 Let P be a Gibbs distribution associated with a zero-normalized potential λ = (λ C , C ⊂ V ). Let S ⊂ V and T = S c . Then the conditional distribution of X (S)  given X (T ) = x (T ) is the Gibbs distribution associated with the zero-normalized potential λ = ( λC , C ⊂ S) where

$λC (y (C) ) = C ′ ⊂V ,C ′ ∩S=C λ C ′ (y (C) ∧ x (T ∩C ′ ) ).$13.4 Models on acyclic graphs

## Finite Markov chains

We now review a few important examples of Markov processes X associated to specific graphs G = (V , E). We will always denote by F s the space in which X (s) takes his values, for s ∈ V .

The simplest example of G-Markov process (for any graph G) is the case when X = (X (s) , s ∈ V ) is a collection of independent random variables. In this case, we can take G X = (V , ∅), the totally disconnected graph on V . Another simple fact is that, as already remarked, any X is Markov for the complete graph (V , P 2 (V )) where P 2 (V ) contains all subsets of V with cardinality 2.

Beyond these trivial (but nonetheless important) cases, the simplest graph-Markov processes are those associated with linear graphs, providing finite Markov chains. For this, we let V be a finite ordered set, say,

$V = {0, . . . , N } .$We say that X is a finite Markov chain if, for any k = 1, . . . , N (X (k) (X (0) , . . . , X (k-2) ) | X (k-1) ).

So we have the identity

$P(X (0) = x (0) , . . . , X (k) = x (k) )P (X (k-1) = x (k-1) ) = P(X (0) = x (0) , . . . , X (k-1) = x (k-1) )P(X (k-1) = x (k-1) , X (k) = x (k) ).$The distribution of a Markov chain is therefore fully specified by P(X (0) = x (0) ), x 0 ∈ F 0 (the initial distribution) and the conditional probabilities

$p k (x (k-1) , x (k) ) = P(X (k) = x (k) | X (k-1) = x (k-1) ) (13.28)$(with an arbitrary choice when P(X (k-1) = x (k-1) ) = 0). Indeed, assume that P(X (0) = x (0) , . . . , X (k-1) = x (k-1) ) is known (for all x (0) , . . . , x (k-1) ). Then, either: (i) P(X (0) = x (0) , . . . , X (k-1) = x (k-1) ) = 0, in which case P(X (0) = x (0) , . . . , X (k) = x (k) ) = 0 for any x (k) , or:

(ii) P(X (0) = x (0) , . . . , X (k-1) = x (k-1) ) > 0, in which case, necessarily, P(X (k-1) = x (k-1) ) > 0, and

$P(X (0) = x (0) , . . . , X (k) = x (k) ) = p k (x (k-1) , x (k) )P(X (0) = x (0) , . . . , X (k-1) = x (k-1) ).$Note that p k in (13.28) is a transition probability (according to definition 12.2) between F k-1 and F k .

We have the following identification of a finite Markov chain with a graph-Markov process:

$Proposition 13.34 Let X = (X (0) , . . . , X (N ) ) be a finite Markov chain, such that X is posi- tive. Then X is G-Markov for the linear graph G = (V , E) with V = {1, . . . , N } E = {{1, 2}, . . . , {N -1, N }} .$The converse is true without the positivity assumption: a G-Markov process for the graph above is always a finite Markov chain.

Proof We prove the direct statement (the converse one being obvious). Let s and t be nonconsecutive distinct integers, with, say, s < t. From the Markov chain assumption, we have (X (t) (X (s) , X ({1,t-2}\{s,}) ) | X (t-1) ), which, using (CI3), yields (X (t) X (s) | X ({1,...,t-1}\{s}) ). Define Y (u) = X ({1,...,u}\{s,t}) : what we have proved is (X (t) X (s) | Y (t) ).

We now proceed by induction and assume that (X (t) X (s) | Y (u) ) for some u ≥ t. Then, we have (X (u+1) (X (s) , X (t) , Y (u-1) ) | X (u) ), which implies (from (CI3)) (X (u+1) X (t) | X (s) , Y (u) ). Applying (CI4) to (X (t) X (s) | Y (u) ) and (X (t) X (u+1) | X (s) , Y (u) ), we obtain (X (t) (X (s) , X (u+1) ) | Y (u) ) and finally, (X (t) X (s) | Y (u+1) ). By induction, this gives (X (t) X (s) | Y (N ) ) and therefore proposition 13.19 now implies that X is G-Markov.

(The proposition can also be proved as a consequence of the decomposition P(X (0) = x (0) , . . . , X (N ) = x (N ) ) = P(X (0) = x (0) )p 1 (x (0) , x (1) ) . . . p N (x (N -1) , x (N ) ).) ■

## Undirected acyclic graph models and trees

The situation with acyclic graphs is only slightly more complex than with linear graphs, but will require a few new definitions, including those of directed graphs and trees.

The difference between directed and undirected graphs is that the edges of the former are ordered pairs, namely:

$Definition 13.35 A (finite) directed graph G is a pair G = (V , E) where V is a finite set of vertexes and E is a subset of V × V \ {(s, s), s ∈ V } , which satisfies, in addition, (s, t) ∈ E ⇒ (t, s) E.$So, for directed graphs, edges (s, t) and (t, s) have different meanings, and we allow at most one of them in E. We say that the edge e = (s, t) stems from s and points to t.

The parents of a vertex s are the vertexes t such that (t, s) ∈ E, and its children are the vertexes t such that (s, t) ∈ E. We will also use the notation s → G t to indicate that (s, t) ∈ E (compare to s ∼ G t for undirected graphs).

Definition 13.36 A path in a directed graph G = (V , E) is a sequence (s 0 , . . . , s N ) such that, for all k = 1, . . . , N , s k → G s k+1 (this includes the "trivial", one-vertex, paths (s 0 )).

(The definition was the same for undirected graph, replacing s k → G s k+1 by s k ∼ G s k+1 .) For both directed and undirected cases, one says that a path is closed if s 0 = s N .

In an undirected graph, a path is folded if it can be written as (s 0 , . . . , s N -1 , s N , s N -1 , . . . , s 0 ).

$If G = (V , E$) is directed, one says that t ∈ V is a descendant of s ∈ V (or that s is an ancestor of t) if there exists a path starting at s and ending at t. In particular, every vertex is both a descendant and an ancestor of itself.

We finally define acyclic graphs. Definition 13.37 A loop in a directed (resp. an undirected) graph G is a path (s 0 , s 1 , . . . , s N ), with N ≥ 3, such that s N = s 0 , which passes only once through s 0 , . . . , s N -1 (no selfintersection except at the end).

## A (directed or undirected) graph G is acyclic if it contains no loop.

The following property will be useful.

## Proposition 13.38 In a directed graph, any non-trivial closed path contains a loop (i.e., one can delete vertexes from it to finally obtain a loop.)

In an undirected graph, any non-trivial closed path which is not a union of folded paths contains a loop.

Proof Take γ = (s 0 , s 1 , . . . , s N ) with s N = s 0 . The path being non-trivial means N > 1.

First take the case of a directed graph. Clearly, N ≥ 3 since a two-vertex path cannot be closed in an directed graph. Consider the first occurrence of a repetition, i.e., the first index for which s j ∈ {s 0 , . . . , s j-1 }.

Then there is a unique j ′ ∈ {0, . . . , j -1} such that s j ′ = s j , and the path (s j ′ , . . . , s j-1 ) must be a loop (any repetition in the sequence would contradict the fact that j was the first occurrence. This proves the result in the directed case.

Consider now an undirected graph. We can recursively remove all folded subpaths, by keeping everything but their initial point, since each such operation still provide a path at the end. Assume that this is done, still denoting the remaining path (s 0 , s 1 , . . . , s N ), which therefore has no folded subpath. We must have N ≥ 3 since N = 1 implies that the original path was a union of folded paths, and N = 2 provides a folded path. Let, 0 ≤ j ′ < j be as in the directed case. Note that one must have j ′ < j -2, since j ′ = j -1 would imply an edge between j and itself and j ′ = j -2 induces a folded subpath. But this implies that (s j ′ , . . . , s j-1 ) is a loop.

## ■

Directed acyclic graphs (DAG) will be important for us, because they are associated with Bayesian networks that we will discuss later. For now, we are interested with undirected acyclic graphs and their relation to trees, which form a subclass of directed acyclic graphs, defined as follows.

Definition 13.39 A forest is a directed acyclic graph with the additional requirement that each of its vertexes has at most one parent.

A root in a forest is a vertex that has no parent. A forest with a single root is called a tree.

It is clear that a forest has at least one root, since one could otherwise describe a nontrivial loop by starting from a any vertex and passing to its parent until the sequence self-intersects (which must happen since V is finite). We will use the following definition.

$Definition 13.40 If G = (V , E) is a directed graph, its flattened graph, denoted G ♭ = (V , E ♭$) is the undirected graph obtained by forgetting the edge ordering, namely

${s, t} ∈ E ♭ ⇔ (s, t) ∈ E or (t, s) ∈ E.$The following proposition relates forests and undirected acyclic graphs.

$Proposition 13.41 If G is a forest, then G ♭ is an undirected acyclic graph.$Conversely, if G is an undirected acyclic graph, there exists a forest G such that G♭ = G.

Proof Let G = (V , E) be a forest and, in order to reach a contradiction, assume that G ♭ has a loop, s 0 , . . . , s N -1 , s N = s 0 . Assume that (s 0 , s 1 ) ∈ E; then, also (s 1 , s 2 ) ∈ E (otherwise s 1 would have two parents), and this propagates to all (s k , s k+1 ) for k = 0, . . . , N -1. But, since s N = s 0 , this provides a loop in G which is not possible. This proves thet G ♭ has no loop since the case (s 1 , s 0 ) ∈ E is treated similarly. Now, let G be an undirected acyclic graph. Fix a vertex s ∈ V and consider the following procedure, in which we recursively define sets S k of processed vertexes, and Ẽk of oriented edges, k ≥ 0, initialized with S 0 = {s} and Ẽ0 = ∅.

-At step k of the procedure, assume that vertexes in S k have been processed and edges in Ẽk have been oriented so that (S k , Ẽk ) is a forest, and that Ẽ♭ k is the set of edges {s, t} ∈ E such that s, t ∈ S k (so, oriented edges at step k can only involve processed vertexes).

-If S k = V : stop, the proposition is proved.

-Otherwise, apply the following construction. Let F k be the set of edges in E that contain exactly one element of S k .

(1) If F k = ∅, take any s ∈ V \ S k as a new root and let S k+1 = S k ∪ {s}, Ẽk+1 = Ẽk .

(2) Otherwise, add to Ẽk the oriented edges (s, t) such that s ∈ S k and {s, t} ∈ F k , yielding Ẽk+1 , and add to S k the corresponding children (t's) yielding S k+1 .

We need to justify the fact that Gk+1 = (S k+1 , Ẽk+1 ) above is still a forest. This is obvious after Case (1), so consider Case (2). First Gk+1 is acyclic, since any oriented loop is a fortiori an unoriented loop and G is acyclic. So we need to prove that no vertex in S k+1 has two parents. Since we did not add any parent to the vertexes in S k and, by assumption, (S k , Ẽk ) is a forest, the only possibility for a vertex to have two parents in S k+1 is the existence of t such that there exists s, s ′ ∈ S k with {s, t} and {s ′ , t} in E. But, since s and s ′ have unaccounted edges containing them, they cannot have been introduced in S k before the previously introduced root has been added, so they are both connected to this root: but the two connections to t would create a loop in G which is impossible.

So the procedure carries on, and must end with S k = V at some point since we keep adding points to S k at each step.

## ■

Note that the previous proof shows there is more than one possible orientation of a connected undirected tree into a tree is not unique, although uniquely specified once a root is chosen. The proof is constructive, and provides an algorithm building a forest from an undirected acyclic graph.

We now define graphical models supported by trees, which constitute our first Markov models associated with directed graphs. Define the depth of a vertex in a tree G = (V , E) to be the number of edges in the unique path that links it to the root. We will denote by G d the set of vertexes in G that are at depth d, so that G 0 contains only the root, G 1 the children of the root and so on. Using this, we have the definition:

$Definition 13.42 Let G = (V , E) be a tree. A process X = (X (s) , s ∈ V ) is G-Markov if$and only, for each d ≥ 1, and for each s ∈ G d , we have

$(X (s) (X (G d \{s}) , X (G q \{pa(s)}) , q < d) | X (pa(s)) ) (13.29)$where pa(s) is the parent of s.

So, conditional to its parent, X (s) is independent from all other variables at depth smaller or equal to the depth of s.

Note that, from (CI3), definition 13.42 implies that, for all

$s ∈ G d , (X (s) X (G d \{s}) | X (G q ) , q < d),$which, using proposition 13.6, implies that the variables (X (s) , s ∈ G d ) are mutually independent given X (G q ) , q < d. This implies that, for d = 1 (letting s 0 denote the root in G):

P(X (G 1 ) = x (G 1 ) , X (s 0 ) = x (s 0 ) ) = P(X (s 0 ) = x (s 0 ) )

$s∈G 1 P(X (s) = x (s) | X (s 0 ) = x (s 0 ) ).$(If P(X (s 0 ) = x (s 0 ) ) = 0, the choice for the conditional probabilities can be made arbitrarily without changing the left-hand side which vanishes.) More generally, we have, letting

$G <d = G 0 ∪ • • • ∪ G d-1 , P(X (G ≤d ) = x (G ≤d ) ) = s∈G d P(X (s) = x (s) | X (pa(s)) = x (pa(s)) )P(X (G <d ) = x (G <d ) )$(with again an arbitrary choice for conditional probabilities that are not defined) so that, we obtain, by induction, for x ∈ F (V ) P(X = x) = P(X (s 0 ) = x (s 0 ) ) s s 0 p s (x (pa(s)) , x (s) ) [(13.30)](#) where p s (x (pa(s)) , x (s) )

$∆ = P(X (s) = x (s) | X (pa(s)) = x (pa(s))$) are the tree transition probabilities between a parent and a child. So we have the following proposition.

Proposition 13.43 A process X is Markov relative to a tree G = (V , E) if and only if there exists a probability distribution p 0 on F s 0 and a family (p st , (s, t) ∈ E) such that p st is a transition probability from F s to F t and

$P X (x) = p 0 (x s 0 ) (s,t)∈E p st (x (s) , x (t) ), x ∈ F (V ). (13.31)$We only have proved the "only if" part, but the "if" part is obvious from [(13.31)](#).

Another property that becomes obvious with this expression is the first part of the following proposition.

$Proposition 13.44 If a process X is Markov relative to a tree G = (V , E) then it is G ♭ Markov. Conversely, if G = (V , E) is an undirected acyclic graph and X is G-Markov, then X is Markov relative to any tree G such that G♭ = G.$Proof To prove the converse part, assume that G = (V , E) is undirected acyclic and that X is G-Markov. Take G such that G♭ = G. For s ∈ V and its parent pa(s) in G, the sets {s} and G≤d \ {s, pa(s)} are separated by pa(s) in G. To see this, assume that there exists a t ∈ G≤d \ {s, pa(s)} with a path from t to s that does not pass through pa(s).

Then we can complete this path with the path from t to the first common ancestor (in G) of t and s and back to s to create a path from s to s that passes only once through {pa(s), s} and therefore contains a loop by proposition 13.38.

The G-Markov property now implies (X (s) (X ( Gd \{s}) , X ( Gq \{pa(s)}) , q < d) | X (pa(s)) )

which proves that X is G-Markov.

## ■

Remark 13. [45](#b63) We see that there is no real gain in generality with passing from undirected to directed graphs when working with trees. This is an important remark, because directionality in graphs is often interpreted as causality. For example, there is a natural causal order in the statements (it rains) → (car windshields get wet) → (car wipers are on) in the sense that each event can be seen as a logical precursor to the next one. However, because one can pass from this directed chain to an equivalent undirected chain and then back to a equivalent directed tree by choosing any of the three variables as roots, there is no way to infer, from the observation of the joint distribution of the three events (it rains, car windshields get wet, wipers are on), any causal relationship between them: the joint distribution cannot resolve whether wipers are on because it rains, or whether turning wipers on automatically wets windshields which in turn triggers a shower ! To infer causal relationships, one needs a different kind of observation, that would modify the distribution of the system. Such an operation (called an intervention), can be done, for example, by preventing the windshields from being wet (doing, for example, the observation in a parking garage), or forcing them to be wet (using a hose). Then, one can compare observations made with these new conditions, and those made with the original system, and check, for example, whether they modified the probability that rain occurs outside. The answer (likely to be negative !) would refute any causal relationship from "windshields are wet" to "it rains." On the other hand, the intervention might modify how wipers are used, which would indicate a possible causal relationship from "windshields are wet" to "wipers are on." ♦

## Examples of general "loopy" Markov random fields

We will see that acyclic models have very nice computational properties that make them attractive in designing distributions. However, the absence of loops is a very restrictive constraint, which is not realistic in many practical situations. Feedback effects are often needed, for example. Most models in statistical physics are supported by a lattice, in which natural translation/rotation invariance relations forbid using any non-trivial acyclic model. As an example, we now consider the 2D Ising model on a finite grid, which is a model for (anti)-ferromagnetic interaction in a spin system.

## Let G = (V , E). A (positive) G-Markov model is said to have only pair interactions if and only if can be written in the form

$π(x) = 1 Z exp - s∈G h s (x (s) ) - {s,t}∈E$h {s,t} (x (s,t) ) .

Relating to theorem 13.30, this says that π is associated to a potential involving cliques of order 2 at most (note that this does not mean that the cliques of the associated graph have order 2 at most; there can be higher-order cliques, which would then have a zero potential). The functions in the potential are indexed by sets, as they should be from the general definition. However, models with pair interactions are often written in the form

$π(x) = 1 Z exp - s∈G h s (x (s) ) - {s,t}∈E hst (x (s) , x (t) )$with hst (λ, µ) = hts (µ, λ) (which is equivalent, taking h = h/2).

## EXAMPLES OF GENERAL "LOOPY" MARKOV RANDOM FIELDS

The Ising model is a special case of models with pair interactions, for which the state space, F s , is equal to {-1, 1} for all s and h s (x (s) ) = α s x (s) , h {s,t} (x (s) , x (t) ) = β st x (s) x (t) .

In fact, for binary variables, this is the most general pair interaction model. The Ising model is moreover usually defined on a regular lattice, which, in two dimensions, implies that V is a finite rectangle in Z 2 , for example

$V = {-N , . . . , N } 2 .$The simplest choice of a translation-and 90-degree rotation-invariant graph is the nearest-neighbor graph for which {(i, j), [13.3](#fig_160)). With this graph, one can furthermore simplify the model to obtain the isotropic Ising model given by

$(i ′ , j ′ )} ∈ E if and only if |i -i ′ | + |j -j ′ | = 1 (see fig.$$π(x) = 1 Z exp -α s∈V x (s) -β s∼t x (s) x (t) .$When β < 0, the model is ferromagnetic: each pair of neighbors with identical signs brings a negative contribution to the energy, making the configuration more likely (since lower energy implies higher probability).

The Potts model generalizes the Ising model to finite, but non-necessarily binary, state spaces, say, F s = F = {1, . . . , n}. Define the function δ(λ, µ) = 1 if λ = µ and (-1) otherwise. Then the Potts model is given by

$π(x) = 1 Z exp -α s∈V h(x (s) ) -β s∼t δ(x (s) , x (t) ) (13.32)$for some function h defined on F.

## General state spaces

Our discussion of Markov random fields on graphs was done under the assumption of finite state spaces, which notably simplifies many of the arguments and avoids relying too much on measure theory. While this situation does cover a large range of application, there are cases in which one wants to consider variables taking values in continuous spaces, or in countable (infinite) spaces.

The results obtained for discrete variables can most of the time be extended to variables whose distribution has a p.d.f. with respect to a product of measures on the sets in which they take their values. For example, let X, Y , Z takes values in R X , R Y , R Z , equipped with σ -algebras S X , S Y , S Z and measures µ X , µ Y , µ Z . Assume that P X,Y ,Z is absolutely continuous with respect to µ X ⊗ µ Y ⊗ µ Z , with density ϕ XY Z . In such a situation, (13.3) remains valid, in that X is conditionally independent of Y given Z if and only if ϕ XY Z (x, y, z)ϕ Z (z) = ϕ XZ (x, z)ϕ Y Z (y, z) [(13.33)](#) almost everywhere (relative to µ X ⊗ µ Y ⊗ µ Z ). Here, ϕ XZ , ϕ Y Z , ϕ Z are marginal densities of the indexed random variables. The only difficulty in the argument, provided below for the interested reader, is dealing properly with sets of measure zero.

Proof (Proof of (13.33)) Introduce the conditional densities

$ϕ XY |Z (x, y | z) = ϕ XY Z (x, y, z) ϕ Z (z)$and similarly ϕ X|Z and ϕ Y |Z , which are defined when z M Z = {z ∈ R Z : ϕ Z (z) = 0}. By definition of conditional independence, we have, for all

$A ∈ S X , B ∈ S X A×B ϕ XY |Z (x, y | z)µ X (dx)µ Y (dy) = A×B ϕ X|Z (x | z)ϕ Y |Z (y | z)µ X (dx)µ Y (dy)$for all z M Z , which implies that, for all z M Z , there exists a set

$N z ⊂ R X × R Y such that µ X × µ Y (N z ) = 0 and ϕ XY |Z (x, y | z) = ϕ X|Z (x | z)ϕ Y |Z (y | z)$for all z M Z and (x, y) N z . This immediately implies (13.33) for those (x, y, z).

$If z ∈ M Z , then 0 = ϕ Z (z) = R X ϕ XZ (x, z)µ X (dx) = R Y ϕ Y Z (x, z)µ Y (dy)$implying that ϕ XZ (x, z) = ϕ Y Z (y, z) = 0 excepted on some set N z such that µ X ⊗ µ Y (N z ) = 0, and (13.33) is therefore also true outside of this set. Now, letting N = {(x, y, z) : (x, y) ∈ N z }, we find that (13.33) is true for all (x, y, z) N and

$µ X ⊗µ Y ⊗µ Z (N ) = R X ×R Y ×R Z 1 (x,y)∈N z µ X (dx)µ Y (dy)µ Z (dz) = R Z µ X ⊗µ Y (N z )µ Z (dz) = 0.$(This argument involves Fubini's theorem [[171]](#b189).)

■ With this definition, the proof of proposition 13.5 can be caried on without change, with the positivity condition expressing the fact that there exists RX ⊂ R X , RY ⊂ R Y and RZ ⊂ R X such that ϕ XY Z (x, y, z) > 0 for all x, y, z ∈ RX × RY × RZ . (This proposition is actually valid in full generality, with a proper definition of positivity.)

When considering random fields with general state spaces, we will restrict to the similar situation in which each state space F s is equipped with a σ -algebra S s and a measure µ s , and the joint distribution, P X of the random field X = (X s , s ∈ V ) is absolutely continuous with respect to µ ∆ = s∈V µ s , denoting by π the corresponding p.d.f. We will says that π is positive if there exists F = ( Fs , s ∈ V ) with measurable Fs ⊂ F s such that π(x) > 0 for all x ∈ F (V , F). Without loss of generality unless one considers multiple random fields with different supports, we will assume that Fs = F s for all s.

The definition of consistent families of local interactions (definition 13.24) must be modified by adding the condition that

$F (V ) C∈C ϕ C (x (C) )µ(dx) < ∞. (13.34)$This requirement is obviously needed to ensure that the normalizing constant in (13.21) is finite. Proposition 13.25 is then true (with sums replaced by integrals in the proof) and so are propositions 13.26 and 13.27. Finally, the Hammersley-Clifford theorem (theorem 13.29) extends to this context.

Even though it is a natural requirement, condition (13.34) may be hard to assess with general families of local interactions. In the case of Gaussian distributions, however, one can provide relatively simple conditions. Assume that F s = R for all s ∈ V , and condider a potential Λ = (λ C , C ∈ C) with only univariate and bivariate interactions, such that, for some vector a ∈ R d (with d = |V |) and symmetric matrix

$b ∈ S d ,          λ {s} (x ({s}) ) = -a (s) x (s) + 1 2 b ss (x (s) ) 2 λ {s,t} (x ({s,t}) ) = b st x (s) x (t)$Then, considering x ∈ F (V ) as a d-dimensional vector, we have

$π(x) = 1 Z exp a T x - 1 2 x T bx ,$with the integrability requirement that b ≻ 0 (positive definite). The random field then follows a Gaussian distribution with mean m = b -1 a and covariance matrix Σ = b -1 . The normalizing constant, Z, is given by

$Z = e -1 2 a T ba (2π) d/2 √ det b .$This Markov random field parametrization of Gaussian distributions emphasizes the conditional structure of the variables rather than their covariances. It is useful when the associated graph, represented by the matrix b is sparse. In particular, the conditional distribution of X (s) given the other variables is Gaussian, with mean (a (s)t s b st x (t) )/b ss and variance 1/b ss .

## Chapter 14

## Probabilistic Inference for Random Fields

Once the joint distribution of a family of variables has been modeled as a random field, this model can be used to estimate the probabilities of specific events, or the expectations of random variables of interest. For example, if the modeled variables relate to a medical condition, in which variables such as diagnosis, age, gender, clinical evidence can interact, one may want to compute, say, the probability of someone having a disease given other observable factors. Note that, being able to compute expectations of the modeled variables for G-Markov processes also ensures that one can compute conditional expectations of some modeled variables given others, since, by proposition 13.22, conditional G-Markov distributions are Markov over restricted graphs.

We assume that X is G-Markov for a graph G = (V , E) and restrict (unless specified otherwise) to finite state spaces. We condider the basic problem to compute P(X (S) = x (S) ) when S ⊂ V , starting with one-vertex marginals, P(X (s) = x (s) ).

The Hammersley-Clifford theorem provides a generic form for general positive G-Markov processes, in the form

## P(X

$= x) = π(x) = 1 Z exp - C∈C G h C (x (C) ) . (14.1)$So, formally, marginal distributions are given by the ratio

$P(X (S) = x (S) ) = y∈F (V ),y (S) =x (S) exp -C∈C G h C (y (C) ) y∈F (V ) exp -C∈C G h C (y (C) )$.

The problem is that the sums involved in this ratio involve a number of terms that grows exponentially with the size of V . Unless V is very small, a direct computation of these sums is intractable. An exception to this is the case of acyclic graphs, as we will see in section 14.2. But for general, loopy, graphs, the sums can only be approximated, using, for example, Monte-Carlo sampling, as described in the next section.

## Monte Carlo sampling

Markov chain Monte Carlo methods are well adapted to sampling from Markov random fields, because conditional distributions used in Gibbs sampling, or, more generally, ratios of probabilities used in the Metropolis-Hastings algorithm do not in require the computation of the normalizing constant Z in (14.1). The simplest use of Gibbs sampling generalizes the Ising model example of section 12.4.2. Using the notation of Algorithm 12.2, one lets B ′ s = F (s c ) (with the notation

$s c = V \ {s}) and U s (x) = x (s c ) . The conditional distribution given U s is Q s (U s (x), y) = P(X (s) = y (s) | X (s c ) = x (s c ) )1 y (s c ) =x (s c ) .$The conditional probability in the r.h.s. of this equation takes the form

$π s (y (s) | x (s c ) ) ∆ = P(X (s) = y (s) | X (s c ) = x (s c ) ) = 1 Z s (x (s c ) ) exp         - C∈C,s∈∈C h C (y (s) ∧ x (C∩s c ) )         with Z s (x (s c ) ) = z (s) ∈F s exp         - C∈C,s∈∈C h C (z (s) ∧ x (C∩s c ) )         .$The Gibbs sampling algorithm samples from Q s by visiting all s ∈ V infinitely often, as described in Algorithm 12.2. Metropolis-Hastings schemes are implemented similarly, the most common choice using a local update scheme in Algorithm 12.3 such that g(x, •) only changes one coordinate, chosen at random, so that

$g(x, y) = 1 |V | s∈V 1 y (s c ) =x (s c ) g s (y (s) )$where g s is some probability distribution on F s . The acceptance probability a(x, y) is equal to 1 when y = x. If y x and g(x, y) > 0, there is a unique s for which y (s c ) = x (s c ) and a(x, y) = min 1, π(y)g(y, x) π(x)g(x, y) with π(y)g(y, x) π(x)g(x, y) = π s (y (s) | x (s c ) )g s (x (s) ) π s (x (s) | x (s c ) )g s (y (s) ) .

Note that the latter equation avoids the computation of the local normalizing constant Z s (x (s c ) ), which simplifies in the ratio.

Both algorithms have a transition probability P that satisfies P m (x, y) > 0 for all x, y ∈ F (V ), with m = |V | (for Metropolis-Hastings, one must assume that g s (y (s) ) > 0 for all y (s) ∈ F s . This ensures that the chain is uniformly geometrically ergodic, i.e., (12.10) is satisfied with a constant M and some ρ < 1. However, in many practical cases (especially for strongly structured distributions and large sets V ), the convergence rate, ρ can be very close to 1, resulting in a slow convergence.

Acceleration strategies have been designed to address this issue, which is often due to the existence of multiple configurations that are local modes of the probability π. Such configurations are isolated from other high-probability configurations because local updating schemes need to make multiple low-probability changes to access them from the local mode. The following two approaches provide examples designed to address this issue.

a. Cluster sampling. To facilitate escaping from such local modes, it is sometimes possible to augment the state space by introducing a new configuration space, with variable denoted ξ, and designing a joint distributions π(ξ, x) such that the marginal distribution on F (V ) (summing over ξ) is the targeted π. The additional variable can create high-probability bridges between local modes for π, and accelerate convergence.

To take an example, assume that all sets F s are identical (letting F = F s , s ∈ V ) and that the auxiliary variable ξ takes values in the set of functions from E to {0, 1}, that we will denote B(E), i.e., that it takes the form (ξ (st) , {s, t} ∈ E), with ξ (st) ∈ {0, 1}. For x ∈ F (V ), introduce the set B x containing all ξ ∈ B(E), such that for all {s, t} ∈ E,

$x (s) x (t) ⇒ ξ (st) = 1.$Assume that the conditional distribution of ξ given x is supported by B x , such that, for ξ ∈ B x

$P(ξ = ξ | X = x) = π(ξ | x) = 1 ζ(x) exp         - {s,t}∈E µ st ξ (st)         .$The coefficients µ st are free to choose (and one possible choice is to take µ st = 0 for all {s, t} ∈ E). For this distribution, all ξ (st) are independent conditionally to X = x, with ξ (st) = 1 with probability 1 if x (s) x (t) , and

$P (ξ (st) = 1 | X = x) = e -µ st 1 + e -µ st (14.2) if x (s) = x (t)$. This conditional distribution is, as a consequence, very easy to sample from. Moreover, the normalizing constant ζ(x) has closed form and is given by

$ζ(x) = {s,t}∈E (1 x (s) =x (t) + e -µ st ) = exp         {s,t}∈E log (1 + e -µ st ) + {s,t}∈E log(1 + e µ st )1 x (s) x (t)        $. Now consider the conditional probability that X = x given ξ = ξ. For this distribution, one has, with probability 1, X (s) = X (t) when ξ (st) = 0. This implies that X is constant on the connected components of the subgraph (V , E ξ ) of (V , E), where {s, t} ∈ E ξ if and only if ξ (st) = 0. Let V 1 , . . . , V m denote these connected components (these components and their number depend on ξ). The conditional distribution of X given ξ is therefore supported by the configurations such that there exists c 1 , . . . , c m ∈ F such that x (s) = c j if and only if s ∈ V j , that we will denote, with some abuse of notation:

$c (V 1 ) 1 ∧ • • • ∧ c (V m )$m . Given this remark, the conditional distribution of X given ξ = ξ is equivalent to a distribution on F m , which may be feasible to sample from directly if |F| and m are not too large. To sample from π, one now needs to alternate between sampling ξ given X and the converse, yielding the following first version of cluster-based sampling.

## Algorithm 14.1 (Cluster-based sampling: Version 1)

This algorithm samples from (14.1).

(1) Initialize the algorithm some configuration x ∈ F (V ).

(2) Loop over the following steps: a. Generate a configuration ξ ∈ B x such that ξ (st) = 1 with probability given by (14.2) when x (s) = x (t) . b. Determine the connected components, V 1 , . . . , V m , of the graph G ξ = (V , E ξ ) with edges given by pairs {s, t} such that ξ (st) = 1. c. Sample values c 1 , . . . , c m ∈ F according to the distribution

$q(c 1 , . . . , c m ) ∝ π(c (V 1 ) 1 ∧ • • • ∧ c (V m ) m ) ζ(c (V 1 ) 1 ∧ • • • ∧ c (V m ) m ) . d. Set x = c (V 1 ) 1 ∧ • • • ∧ c (V m ) m .$Step (2.c) takes a simple form in the special case when π is a non-homogeneous Potts model ((13.32)) with positive interactions, that we will write as

$π(x) = exp         - s∈V α s x (s) - {s,t}∈E β st 1 x (s) x (t)         with β st ≥ 0. Then π(x) ζ(x) ∝ exp         - s∈V α s x (s) - {s,t}∈E (β st -β ′ st )1 x s s t        $with β ′ st = log(1 + e µ st ). If one chooses µ st such that β ′ st = β st (which is possible since β st ≥ 0), then the interaction term disappears and the probability q in (2.c) is proportional to

$m j=1 exp          - s∈V j α s         $so that c 1 , . . . , c m can be generated independently. The resulting algorithm is the Swendsen-Wang sampling algorithm for the Potts model [[186]](#b204). The presentation given here adapts the one introduced in Barbu and Zhu [[16]](#b34).

For more general models, step (2.c) can be computationally costly, especially if the number of connected components is large. In this case, this step can be replaced by a Gibbs sampling step for one of the c ′ j s conditional to the others (and ξ) that we summarize in the following variation of Algorithm 14.1.

## Algorithm 14.2 (Cluster-based sampling: Version 2)

This algorithm samples from (14.1).

(1) Initialize the algorithm some configuration x ∈ F (V ).

(2) Loop over the following steps: a. Generate a configuration ξ ∈ B x such that ξ (st) = 1 with probability given by (14.2) when x (s) = x (t) . b. Determine the connected components, V 1 , . . . , V m , of the graph G ξ = (V , E ξ ) with edges given by pairs {s, t} such that ξ (st) = 1. Note that x is constant on each of these connected components, i.e., there exists c 1 , . . . , c m ∈ F such that x = c

$(V 1 ) 1 ∧ • • • ∧ c (V m )$m . c. Select at random one of the components, say, j 0 ∈ {1, . . . , m}. d. Sample the value cj 0 ∈ F according to the distribution

$q( cj 0 ) ∝ π( c(V 1 ) 1 ∧ • • • ∧ c(V m ) m ) ζ( c(V 1 ) 1 ∧ • • • ∧ c(V m ) m )$.

with cj = c j if j j 0 . e. Set x (s) = cj 0 for s ∈ V j 0 .

Unlike single-variable updating schemes, these algorithms can update large chunks of the configurations at each step, and may result in significantly faster convergence of the sampling procedure. Note that step (2.d) in Algorithm 14.2 can be replaced by a Metropolis-Hastings update with a proper choice of proposal probability [[16]](#b34).

b. Parallel tempering. We now consider a different kind of extension in which we allow π depends continuously on a parameter β > 0, writing π β and, the goal is to sample from π 1 . For example, one can extend (14.1) by the family of probability distributions

$π β (x) = 1 Z β exp         -β C∈C G h C (x (C) )        $for β ≥ 0. For small β, π β gets close to the uniform distribution on F (V ) (achieved for β = 0), so that it becomes easier to move from local mode to local mode. This implies that sampling with small β is more efficient and the associated Markov chain moves more rapidly in the configuration space.

Assume given, for all β, two ergodic transition probabilities on F (V ), q β and qβ such that (12.7) is satisfied with π β as invariant probability, namely π β (y)q β (y, x) = π β (x) qβ (x, y) (

for all x, y ∈ F (V ) (as seen in (12.7), qβ is the transition probability for the reversed chain). The basic idea is that q β provides a Markov chain that converges rapidly for small β and slowly when β is closer to 1. Parallel tempering (this algorithm was introduced in Neal [[140]](#b158) based on ideas developed in Marinari and Parisi [[126]](#b144)) leverages this fact (and the continuity of π β in β) to accelerate the simulation of π 1 by introducing intermediate steps sampling at low β values.

The algorithm specifies a sequence of parameters 0 ≤ β 1 ≤ • • • ≤ β m = 1. One simulation steps goes down, then up this scale, as described in the following algorithm.

## Algorithm 14.3 (Parallel Tempering)

Start with an initial configuration x 0 ∈ F (V ). This configuration is then updated at each step, using the following sequence of operations.

(1) For j = 1, . . . , m, generate a configuration x j according to qβ j (x j-1 , •).

(2) Generate a configuration z m-1 according to q β m (x m , •).

(3) For j = m -1, . . . , 1, generate a configuration z j-1 according to q β j (z j , •).

(4) Set x 0 = z 0 with probability min

$        1, π β 0 (z 0 ) π β 0 (x 0 )         m-1 j=1 π β j (x j-1 ) π β j (x j )         π β m (x m-1 ) π β m (z m-1 )         m-1 j=1 π β j (z j ) π β j (z j-1 )                 . (Otherwise, keep x 0 unchanged).$Importantly, the acceptance probability at step (4) only involves ratios of π ′ β s and therefore no normalizing constant. We now show that this algorithm is π β 0 -reversible. Let p(•, •) denote the transition probability of the chain. If z 0 x 0 , p(x 0 , z 0 ) corresponds to steps (1) to [(3)](#b21), with acceptance at step(4), and is therefore given by the sum, over all x 1 , . . . , x m and z 1 , . . . , z m , of products

$qβ 1 (x 0 , x 1 ) • • • qβ m (x m-1 , x m )q β m (x m , z m-1 ) • • • q β 1 (z 1 , z 0 ) min         1, π β 0 (z 0 ) π β 0 (x 0 )         m-1 j=1 π β j (x j-1 ) π β j (x j )         π β m (x m-1 ) π β m (z m-1 )         m-1 j=1 π β j (z j ) π β j (z j-1 )                 Applying (14.3), this is equal to min qβ 1 (x 0 , x 1 ) • • • qβ m (x m-1 , x m )q β m (x m , z m-1 ) • • • q β 1 (z 1 , z 0 ), π β 0 (z 0 ) π β 0 (x 0 ) q β 1 (x 0 , x 1 ) • • • q β m (x m-1 , x m ) qβ m (x m , z n-1 ) • • • qβ 1 (z 1 , z 0 ) So, π β 0 (x 0 )p(x 0 , z 0 ) = min π β 0 (x 0 ) qβ 1 (x 0 , x 1 ) • • • qβ m (x m-1 , x m )q β m (x m , z m-1 ) • • • q β 1 (z 1 , z 0 ), π β 0 (z 0 )q β 1 (x 1 , x 0 ) • • • q β m (x m , x m-1 ) qβ m (z m-1 , x m ) • • • qβ 1 (z 0 , z 1 )$where the sum is over all x 1 , . . . , x m , z 1 , . . . , z m-1 ∈ F (V ). The sum is, of course, unchanged if one renames x 1 , . . . , x m , z 1 , . . . , z m-1 to z 1 , . . . , z m , x 1 , . . . , x m-1 , but doing so provides the expression of π β 0 (z 0 )p(z 0 , x 0 ), proving the reversibility of the chain with respect to π β 0 .

## Inference with acyclic graphs

We now switch to deterministic methods to compute, or approximate, marginal probabilities of Markov random fields. In this section, we consider a directed acyclic graph G = (V , E). As we have seen, Markov processes for acyclic graphs are also Markov for any tree structure associated with the graph. Introducing such a tree, G = (V , Ẽ) with G♭ = G, we know that a Markov process on G can be written in the form (letting s 0 denote the root in G):

$π(x) = p s 0 (x (s 0 ) ) (s,t)∈ Ẽ p st (x (s) , x (t) ) (14.4)$where p s 0 is a probability and p st a transition probability.

We now show how to compute marginal probabilities of configurations x (S) , denoted π S (x (S) ), for a set S ⊂ V , starting with singletons S = {s}. The computation can be done by propagating down the tree as follows. For s = s 0 , the probability is known, with π s 0 = p s 0 . Now take an arbitrary s s 0 and let pa(s) be its parent. Then π s (x (s) ) = P(X (s) = x (s) ) = y (pa(s)) ∈F pa(s) P (X (s) = x (s) | X (pa(s)) = y (pa(s)) )P (x (pa(s)) = y (pa(s)) ) = y (pa(s)) ∈F pa(s) π pa(s) (y (pa(s)) )p pa(s) (y pa(s) , x (s) ) so that the marginal probability at any s s 0 can be computed given the marginal probability of its parent. We can propagate the computation down the tree, with a total cost for computing π s proportional to n k=1 |F t k-1 | |F t k | where t 0 = s 0 , t 1 , . . . , t n = s is the unique path between s 0 and s. This is linear in the depth of the tree, and quadratic (not exponential) in the sizes of the state spaces. The computation of all singleton marginals requires an order of (s,t)∈E |F s | |F t | operations. Now, assume that probabilities of singletons have been computed and consider an arbitrary set S ⊂ V . Let s ∈ V be an ancestor of every vertex in S, maximal in the sense that none of its children also satisfy this property. Consider the subtrees of G starting from each of the children of s, denoted G1 , . . . , Gn with Gk = (V k , Ẽk ). Let S k = S ∩ V k . From the conditional independence,

$π S (x (S) ) = y (s) ∈F s P (X (S\{s}) = x (S\ { s}) | X (s) = y (s) )π s (y (s) ) = y (s) ∈F s n k=1,S k ∅ P (X (S k ) = x (S k ) | X (s) = y (s) )π s (y s ) Now, for all k = 1, . . . , n, we have |S k | < |S|: this is obvious if S is not completely included in one of the V k 's. But if S ⊂ V k then the root, s k , of V k$is an ancestor of all the elements in S and is a child of s, which contradicts the assumption that s is maximal. So we have reduced the computation of π S (x S ) to the computations of n probabilities of smaller sets, namely P (X (S k ) = x (S k ) | X (s) = y (s) ) for S k ∅. Because the distribution of X (V k ) conditioned at s is a Gk -Markov model, we can reiterate the procedure until only sets of cardinality one remain, for which we know how to explicitly compute probabilities. This provides a feasible algorithm to compute marginal probabilities with trees, at least when its distribution is given in tree-form, like in [(14.4)](#). We now address the situation in which one starts with a probability distribution associated with pair interactions (cf. definition 13.24) over the acyclic graph G π

$(x) = 1 Z s∈V ϕ s (x (s) )${s,t}∈E ϕ st (x (s) , x (t) ). (14.5)

We assume these local interactions to be consistent, still allowing for some vanishing ϕ st (x (s) , x (t) ).

Putting π in the form (14.4) is equivalent to computing all joint probability distributions π st (x (s) , x (t) ) for {s, t} ∈ E, and we now describe this computation. Denote

$U (x) = s∈V ϕ s (x (s) )${s,t}∈E ϕ st (x (s) , x (t) ) so that Z = y∈F (V ) U (y). For the tree G = (V , Ẽ), and t ∈ V , we let Gt = (V t , Ẽt ) be the subtree of G rooted at t (containing t and all its descendants). For S ⊂ V , define

$U S (x (S) ) = s∈S ϕ s (x (s) ) {s,s ′ }∈E,s,s ′ ∈D ϕ ss ′ (x (s) , x (s ′ ) ) and Z t (x (t) ) = y (V * t ) ∈F (V * t ) U V t (x (t) ∧ y (V * t ) ).$with V * t = V t \ {t}.

Lemma 14.1 Let G = (V , E) be a directed acyclic graph and π = P X be the G-Markov distribution given by [(14.5)](#). With the notation above, we have π s 0 (x (s 0 ) ) = Z s 0 (x (s 0 ) )

$y (s 0 ) ∈F s 0 Z s 0 (y (s 0 ) ) (14.6)$and, for (s, t) ∈ Ẽ,

$p st (x (s) , x (t) ) = P (X (t) = x (t) | X (s) = x (s) ) = ϕ st (x (s) , x (t) )Z t (x (t) ) y (t) ∈F t ϕ st (x (s) , y (t) )Z t (y (t) ) (14.7)$Proof Let W t = V \V t . Clearly, Z = x (0) ∈F s 0 Z s 0 (x (0) ) and π s 0 (x (0) ) = Z s 0 (x (0) )/Z which gives [(14.6)](#). Moreover, if s ∈ V , we have

$P(X (V * s ) = x (V * s ) | X (s) = x (s) ) = y (W s ) U (x (V s ) ∧ y (W s ) ) y (V * s ) ,y (W s ) U (x (s) ∧ y (V * s ) ∧ y (W s )$) .

We can write

$U (x (s) ∧ y (V * s ) ∧ y (W s ) ) = U V s (x (s) ∧ y (V * s ) )U {s}∪W s (x (s) ∧ y (W s ) )ϕ s (x (s) ) -1$yielding the simplified expression

$P(X (V * s ) = x (V * s ) | X (s) = x (s) ) = U V s (x (V s ) )ϕ s (x (s) ) -1 y W s U {s}∪W s (x (s) ∧ y (W s ) ) ϕ s (x (s) ) -1 y (V * s ) U V s (x (s) ∧ y (V * s ) ) y (W s ) U {s}∪W s (x (s) ∧ y (W s ) ) = U V s (x (V s ) ) Z s (x (s) )$Now, if t 1 , . . . , t n are the children of s, we have

$U V s (x (V s ) ) = ϕ s (x (s) ) n k=1 ϕ st k (x (s) , x (t k ) ) n k=1 U V t k (x (V t k ) ), so that P(X (t k ) = x (t k ) , k = 1, . . . , n | X (s) = x (s) ) = 1 Z s (x (s) ) y (V * t k ) ,k=1,...,n ϕ s (x (s) ) n k=1 ϕ st k (x (s) , x (t k ) ) n k=1 U V t k (x (t k ) ∧ y (V * t k ) ) = ϕ s (x (s) ) n k=1 ϕ st k (x (s) , x (t k ) ) n k=1 Z t k (x (t k ) ) Z s (x (s) )$This implies that the transition probability needed for the tree model, p st 1 (x (s) , x (t 1 ) ), must be proportional to ϕ st 1 (x (s) , x (t 1 ) )Z t 1 (x (t 1 ) ) which proves the lemma.

## ■

This lemma reduces the computation of the transition probabilities to the computation of Z s (x (s) ), for s ∈ V . This can be done efficiently, going upward in the tree (from terminal vertexes to the root). Indeed, if s is terminal, then V s = {s} and Z s (x (s) ) = ϕ s (x (s) ). Now, if s is non-terminal and t 1 , . . . , t n are its children, then, it is easy to see that Z s (x (s) ) = ϕ s (x (s) )

$x (t 1 ) ∈F t 1 ,...,x (t n ) ∈F t n n k=1 ϕ st k (x (s) , x (t k ) )Z t k (x (t k ) ) = ϕ s (x (s) ) n k=1 x (t k ) ∈F t k ϕ st k (x (s) , x (t k ) )Z t k (x (t k ) ) (14.8)$So, Z s (x (s) ) can be easily computed once the Z t (x (t) )'s are known for the children of s.

Equations (14.6) to (14.8) therefore provide the necessary relations in order to compute the singleton and edge marginal probabilities on the tree. It is important to note that these relations are valid for any tree structure consistent with the acyclic graph we started with. We now rephrase them with notation that only depend on this graph and not on the selected orientation.

Let {s, t} be an edge in E. Then s separates the graph G \ {s} into two components. Let V st be the component that contains t, and

$V * st = V st \ t. Define Z st (x t ) = y (V * st ) ∈F (V * st ) U V st (x (t) ∧ y (V * st ) ).$This Z st coincides with the previously introduced Z t , computed with any tree in which the edge {s, t} is oriented from s to t. Equation (14.8) can be rewritten with this new notation in the form:

$Z st (x (t) ) = ϕ t (x (t) ) t ′ ∈V t \{s} x (t ′ ) ∈F t ′ ϕ tt ′ (x (t) , x (t ′ ) )Z tt ′ (x (t ′ ) ) . (14.9)$This equation is usually written in terms of "messages" defined by

$m ts (x (s) ) = x (t) ∈F t ϕ st (x (s) , x (t) )Z st (x (t) )$which yields Z st (x (t) ) = ϕ t (x (t) )

$t ′ ∈V t \{s} m t ′ t (x (t) )$and the message consistency relation

$m ts (x (s) ) = x (t) ∈F t ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ).(14.10)$Also, because one can start building a tree from G ♭ using any vertex as a root, (14.6) is valid for any s ∈ V , in the form (applying (14.8) to the root)

$π s (x (s) ) = 1 ζ s ϕ s (x (s) )$t∈V s m ts (x (s) ) [(14.11)](#) where ζ s is chosen to ensure that the sum of probabilities is 1. (In fact, looking at lemma 14.1, we have Z s = Z, independent of s.)

Similarly, (14.7) can be written p st (x (s) , x (t) ) = m ts (x (s) ) -1 ϕ st (x (s) , x (t) )ϕ t (x (t) )

t ′ ∈V t \{s} m t ′ t (x (t) ) (14.12) which provides the edge transition probabilities. Combining this with (14.11), we get the edge marginal probabilities: s) ). (14.13) Remark 14.2 We can modify (14.10) by multiplying the right-hand side by an arbitrary constant q ts without changing the resulting estimation of probabilities: this only multiplies the messages by a constant, which cancels after normalization. This remark can be useful in particular to avoid numerical overflow; one can, for example, define q ts = 1/ x s ∈F s m ts (x s ) so that the messages always sum to 1. This is also useful when applying belief propagation (see next section) to loopy networks, for which (14.10) may diverge while the normalized version converges. ♦

$π st (x (s) , x (t) ) = 1 ζ ϕ st (x (s) , x (t) )ϕ s (x (s) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ) s ′ ∈V s \{t} m s ′ s (x ($The following summarizes this message passing algorithm.

## Algorithm 14.4 (Belief propagation on acyclic graphs)

Given a family of interactions ϕ s :

$F s → [0, +∞), ϕ st : F s × F t → [0, +∞),(1)$Initialize functions (messages) m ts : F s → R, e.g., taking m ts (x (s) ) = 1/|F s |.

(2) Compute unnormalized messages mts (•) =

$x (t) ∈F t ϕ st (•, x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) )$and let m ts (•) = q ts mts (•), for some choice of constant q ts , which must be a fixed function of mts (•), such as

$q ts =          x (s) ∈F s mts (x (s) )          -1$.

(3) Stop the algorithm when the messages stabilize (which happens after a finite number of updates). Compute the edge marginal distributions using (14.13).

It should be clear, from the previous analysis that messages stabilize in finite time, starting from the outskirts of the acyclic graph. Indeed, messages starting from a terminal t (a vertex with only one neighbor) are automatically set to their correct value in (14.10), m ts (x s ) =

x t ∈F t ϕ st (x s , x t )ϕ t (x t ), at the first update. These values then propagate to provide messages that satisfy (14.10) starting from the next-to-terminal vertexes (those that have only one neighbor left when the terminals are removed) and so on.

## Belief propagation and free energy approximation 14.3.1 BP stationarity

It is possible to run Algorithm 14.4 on graphs that are not acyclic, since nothing in its formulation requires this property. However, while the method stabilizes in finite time for acyclic graphs, this property, or even the convergence of the messages is not guaranteed for general, loopy, graphs. Convergence, however, has been observed in a large number of applications, sometimes with very good approximations of the true marginal distributions.

We will refer to stable solutions of Algorithm 14.4 as BP-stationary points, as formally stated in the next definition, which allows for a possible normalization of messages, which is particularly important with loopy networks. Definition 14.3 Let G = (V , E) be an undirected graph and Φ = (ϕ st , {s, t} ∈ E, ϕ s , s ∈ V ) a consistent family of pair interactions. We say that a family of joint probability distributions (π ′ st , {s, t} ∈ E) is BP-stationary for (G, Φ) if there exists messages x t ∈ F t → m st (x t ), constants ζ st for t ∼ s and α s for s ∈ V satisfying

$m ts (x (s) ) = α s ζ ts x (t) ∈F t ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ) (14.14) such that π ′ st (x (s) , x (t) ) = 1 ζ st ϕ st (x (s) , x (t) )ϕ s (x (s) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ) s ′ ∈V s \{t} m s ′ s (x (s) ).$
## (14.15)

There is no loss of generality in the specific form chosen for the normalizing constants in (14.14) and (14.15), in the sense that, if the messages satisfy (14.15) and

$m ts (x (s) ) = q ts x (t) ∈F t ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) )$for some constants q ts , then

$ζ st = x (s) ∈F s ,x (t) ∈F t ϕ st (x (s) , x (t) )ϕ s (x (s) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ) s ′ ∈V s \{t} m s ′ s (x (s) ) = 1 q ts x (s) ∈F s ϕ s (x (s) ) s ′ ∈V s m s ′ s (x (s) )$so that ζ st q ts (which has been denoted α s ) does not depend on t. Of course, the relevant questions regarding BP-stationarity is whether the collection of pairwise probability π ′ st exists, how to compute them, and whether π ′ st (x (s) , x (t) ) provides a good approximation of the marginals of the probability distribution π that is associated to Φ, namely

$π(x) = 1 Z s∈V ϕ s (x (s) )${s,t}∈E ϕ st (x (s) , x (t) ).

A reassuring statement for BP-stationarity is that it is not affected when the functions in Φ are multiplied by constants, which does not affect the underlying probability π. This is stated in the next proposition. Proposition 14.4 Let Φ be as above a family of edge and vertex interactions. Let c st , {s, t} ∈ E, c s , s ∈ V be families of positive constants, and define Φ = ( φst , φs ) by φst = c st ϕ st and φs = c s ϕ s . Then,

$π ′ is BP-stationary for (G, Φ) ⇔ π ′ is BP-stationary for (G, Φ).$Proof Indeed, if (14.14) and (14.15) are true for (G, Φ), it suffices to replace α s by α s c s and ζ st by ζ st c st c t to obtain (14.14) and (14.15) for (G, Φ).

## ■

It is also important to notice that, if G is acyclic, definition 14.3 is no more general than the message-passing rule we had considered earlier. More precisely, we have (see remark 14.2), Proposition 14.5 Let G = (V , E) be undirected acyclic and Φ = (ϕ st , {s, t} ∈ E, ϕ s , s ∈ V ) a consistent family of pair interactions. Then, the only BP-stationary distributions are the marginals of the distribution π associated to Φ.

## Free-energy approximations

A partial justification of the good behavior of BP with general graphs has been provided in terms of a quantity introduced in statistical mechanics, called the Bethe free energy. We let G = (V , E) be an undirected graph and assume that a consistent family of pair interactions is given (denoted Φ = (ϕ s , s ∈ V , ϕ st , {s, t} ∈ E)) and consider the associated distribution, π, on F (V ), given by

$π(x) = 1 Z s∈V ϕ s (x (s) )${s,t}∈E ϕ st (x (s) , x (t) ). (14.16)

It will also be convenient to use the function

$ψ st (x (s) , x (t) ) = ϕ s (x (s) )ϕ t (x (t) )ϕ st (x (s) , x (t) ) such that π(x) = 1 Z s∈V ϕ s (x (s) ) 1-|V s | {s,t}∈E$ψ st (x (s) , x (t) ). (14.17)

We will consider approximations π ′ of π that minimize the Kullback-Leibler divergence, KL(π ′ ∥π) (see (4.3)), subject to some constraints. We can write

$KL(π ′ ∥π) = -E π ′ (log π) -H(π ′ ) = -log Z - s∈V (1 -|V s |)E π ′ (log ϕ s ) - {s,t}∈E E π ′ (log ψ st ) -H(π ′ )$(where H(π ′ ) is the entropy of π ′ ). Introduce the one-and two-dimensional marginals of π ′ , denoted π ′ s ad π ′ st . Then

$KL(π ′ ∥π) = -log Z - s∈V (1 -|V s |)E π ′ (log ϕ s π ′ s ) - {s,t}∈E E π ′ (log ψ st π ′ st ) + s∈V (1 -|V s |)H(π ′ s ) + {s,t}∈E H(π ′ st ) -H(π ′ ).$The Bethe free energy is the function F β defined by

$F β (π ′ ) = - s∈V (1 -|V s |)E π ′ (log ϕ s π ′ s ) - {s,t}∈E E π ′ (log ψ st π ′ st ) ; (14.18) so that KL(π ′ ∥π) = F β (π ′ ) -log Z + ∆ G (π ′ ) with ∆ G (π ′ ) = s∈V (1 -|V s |)H(π ′ s ) + {s,t}∈E H(π ′ st ) -H(π ′ ).$Using this computation, one can consider the approximation problem: find π′ that minimizes KL(π ′ ∥π) over a class of distributions π ′ for which the computation of the first and second order marginals is easy. This problem has an explicit solution when the distribution π ′ is such that all variables are independent, leading to what is called the mean-field approximation of π. Indeed, in this case, we have

$∆ G (π ′ ) = {s,t}∈G (H(π ′ s ) + H(π ′ t )) + s∈S (1 -|V s |)H(π ′ s ) - s∈S H(π ′ s ) = 0 and F β (π ′ ) = - s∈V (1 -|V s |)E π ′ (log ϕ s π ′ s ) - {s,t}∈E E π ′ (log ψ st π ′ s π ′ t ) .$F β must be minimized with respect to the variables π ′ s (x (s) ), s ∈ S, x s ∈ F S subject to the constraints x s ∈F s π ′ s (x (s) ) = 1. The corresponding necessary optimality conditions equations provide the mean-field consistency equations, described in the following definition.

$Proposition 14.6 A local minimum of F β (π ′ ) over all probability distributions π ′ of the form π ′ (x) = s∈V π ′ s (x (s) )$must satisfy the mean field consistency equations:

$π s (x (s) ) = 1 Z s ϕ s (x (s) ) 1-|V s |$t∼s exp E π t (log ψ st (x (,) .)) . [(14.19)](#) Proof Since all constraints are affine, we can use Lagrange multipliers, denoted (λ s , s ∈ S) for each of the constraints, to obtain necessary conditions for a minimizer, yielding

$∂F β ∂π s (x s ) -λ s = 0, s ∈ S, x s ∈ F s .$This gives:

$-(1 -|V s |) log ϕ s (x s ) π s (x s ) -1 - t∼s x t ∈F t log ψ st (x s , x t ) π s (x s )π t (x t ) -1 π t (x t ) = λ s .$Solving this with respect to π s (x s ) and regrouping all constant terms (independent from x s ) in the normalizing constant Z s yields (14. [19](#b37)).

## ■

The mean field consistency equations can be solved using a root-finding algorithm or by directly solving the minimization problem. We will retrieve this method, with more details, in our discussion of variational approximations in chapter 16.

In the particular case in which G is acyclic and the approximation is made by G-Markov processes, the Kullback-Leibler distance is minimized with π ′ = π (since π belongs to the approximating class). A slightly non-trivial remark is that π is optimal also for the minimization of the Bethe free energy F β , because this energy coincides, up to the constant term log Z, with the Kullback-Leibler divergence, as proved by the following proposition.

## Proposition 14.7 If G is acyclic and π

$′ is G-Markov, then ∆ G (π ′ ) = 0.$This proposition is a consequence of the following lemma that has its own interest:

$Lemma 14.8 If G is acyclic and π is a G-Markov distribution, then π(x) = s∈V π s (x (s) ) 1-|V s | {s,t}∈E π st (x (s) , x (t) ).$(14.20)

Proof (of lemma 14.8) We know that, if G = (V , Ẽ) is a tree such that G♭ = G, we have, letting s 0 be the root in G

$π(x) = π s 0 (x (s 0 ) ) (s,t)∈ Ẽ p st (x (s) , x (t) ) = π s 0 (x (s 0 ) ) (s,t)∈ Ẽ(π st (x (s) , x (t) )π(x (s) ) -1 ). Each vertex s in V has |V s | -1 children in G, except s 0 which has |V s 0 | children. Using this, we get π(x) = π s 0 (x (s 0 ) )π s 0 (x (s 0 ) ) -|V s 0 | s∈V \{s 0 } π s (x (s) ) 1-|V s | (s,t)∈ Ẽ π st (x (s) , x (t) ) = s∈V π s (x (s) ) 1-|V s | {s,t}∈E π st (x (s) , x (t) ).$Proof (of proposition 14.7) If π ′ is given by (14.20), then

$H(π ′ ) = -E π ′ log π ′ = - s∈V (1 -|V s |)E π ′ log π ′ s - {s,t}∈E E π ′ log π ′ st = s∈V (1 -|V s |)H(π ′ s ) + {s,t}∈E H(π ′ st )$which proves that ∆ G (π ′ ) = 0.

## ■

In view of this, it is tempting to "generalize" the mean field optimization procedure and minimize F β (π ′ ) over all possible consistent singletons and pair marginals (π ′ s and π ′ st ), then use the optimal ones as an approximation of π s and π st . What we have just proved is that this procedure provides the exact expression of the marginals when G is acyclic. For loopy graphs, however, it is not justified, and is at best an approximation. A very interesting fact is that this procedure provides the same consistency equations as belief propagation. To see this, we first start with the characterization of minimizers of F β . Proposition 14.9 Let G = (V , E) be an undirected graph and π be given by [(14.16)](#). Consider the problem of minimizing the Bethe free energy F β in [(14.18)](#) with respect to all possible choices of probability distributions (π ′ st , {s, t} ∈ E), (π ′ s , s ∈ V ) with the constraints

$π ′ s (x (s) ) = x (t) ∈F t π ′ st (x (s) , x (t) ), ∀x (s) ∈ F s and t ∼ s.$Then a local minimum of this problem must take the form

$π ′ st (x (s) , x (t) ) = 1 Z st ψ st (x (s) , x (t) )µ st (x (t) )µ ts (x (s) ) (14.21)$where the functions µ st : F t → [0, +∞) are defined for all (s, t) such that {s, t} ∈ E and satisfy the consistency conditions:

$µ ts (x (s) ) -(|V s |-1) s ′ ∼s µ s ′ s (x (s) ) =          e Z st x (t) ∈F t ψ st (x (s) , x (t) )ϕ t (x (t) )µ st (x (t) )          |V s |-1 . (14.22)$Proof We introduce Lagrange multipliers: λ ts (x (s) ) for the constraint

$π ′ s (x (s) ) = x (t) ∈F t π ′ st (x (s) , x (t) )$and γ st for

$x (s) ,x (t) π ′ st (x (s) , x (t) ) = 1,$which covers all constraints associated to the minimization problem. The associated Lagrangian is

$F β (π ′ ) - s∈V x (s) ∈F s t∼s λ ts (x (s) )          x (t) ∈F t π ′ st (x (s) , x (t) ) -π ′ s (x (s) )          - {s,t}∈E γ st          x (s) ∈F s ,x (t) ∈F t π ′ st (x (s) , x (t) ) -1          .$The derivative with respect to π ′ st (x (s) , x (t) ) yields the condition

$log π ′ st (x (s) , x (t) ) -log ψ st (x (s) , x (t) ) + 1 -λ ts (x (s) ) -λ st (x (t) ) -γ st = 0.$which implies π ′ st (x (s) , x (t) ) = ϕ st (x (s) , x (t) ) exp(γ st -1) exp(λ ts (x (s) ) + λ st (x (t) )).

We let Z st = exp(1γ st ), with γ st chosen so that π ′ st is a probability. The derivative with respect to π ′ s (x (s) ) gives

$(1 -|V s |)(log π ′ s (x (s) ) -log ϕ s (x (s) ) + 1) + t∼s λ ts (x (s) ) = 0.$Combining this with the expression just obtained for π ′ st , we get, for t ∼ s,

$(1 -|V s |) log x (t) ∈F t ψ st (x (s) , x (t) )e λ st (x (t) ) + (1 -|V s |)λ ts (x (s) ) + (1 -|V s |)(1 -log Z st -log ϕ s (x (s) )) + s ′ ∼s λ s ′ s (x (s) ) = 0,$which gives (14.22) with µ st = exp(λ st ).

## ■

A family π ′ st satisfying conditions (14.21) and (14.22) of proposition 14.9 will be called Bethe-consistent. A very interesting remark states that Bethe-consistency is equivalent to BP-stationarity, as stated below. Proposition 14.10 Let G = (V , E) be an undirected graph and Φ = (ϕ st , {s, t} ∈ E, ϕ s , s ∈ V ) a consistent family of pair interactions. Then a family π ′ of joint probability distributions is BP-stationary if and only if it is Bethe-consistent.

Proof First assume that π ′ is BP-stationary with messages m st , so that (14.14) and (14.15) are satisfied. Take

$µ st = a t t ′ ∈V t ,t ′ s m t ′ t (x (t) )$for some constant a t that will be determined later. Then, the left-hand side of (14. [22](#formula_2063)) is

$µ ts (x (s) ) -(|V s |-1) s ′ ∈V s µ s ′ s (x (s) ) = a s         s ′ ∈V s ,s ′ t m s ′ s (x (s) )         -(|V s |-1) s ′ ∈V s s ′′ ∈V s ,s ′′ s ′ m s ′′ s (x (s) ) = a s m ts (x (s) ) |V s |-1 .$The right-hand side is equal to (using (14.14))

$ea t ζ st Z st α s m ts (x (s) ) |V s |-1$, so that we need to have

$a s = ea t ζ st Z st α s |V s |-1$.

We also need Z st =

x (s) ,x (t)   ψ st (x (s) , x (t) )µ st (x (t) )µ ts (x (s) ) = a s a t ζ st .

Solving these equations, we find that (14.21) and (14.22) are satisfied with

$       a s = (e/α s ) (|V s |-1)/|V s | Z st = ζ st a s a t which proves that π ′ is Bethe-consistent.$Conversely, take a Bethe-consistent π ′ , and µ st , Z st satisfying (14.21) and [(14.22)](#). For s such that |V s | > 1, define, for t ∈ V s , m ts (x (s) ) = µ ts (x (s) ) -1

$s ′ ∼s µ s ′ s (x (s) ) 1/(|V s |-1) . (14$.23) Define also, for |V s | > 1, ρ ts (x (s) ) = s ′ ∈V s ,s ′ t m s ′ s (x (s) ). (If |V s | = 1, take ρ ts ≡ 1.) Using (14.23), we find ρ ts = µ ts when |V s | > 1, and this identity is still valid when |V s | = 1, since in this case, (14.22) implies that µ ts (x (s) ) = 1. We need to find constants α t and ζ st such that (14.14) and (14.15) are satisfied. But (14.15) implies ζ ts = x t ,x s ψ st (x (s) , x (t) )ρ st (x (t) )ρ ts (x (s) ) and (14.21) implies ζ ts = Z ts . We now consider (14.14), which requires m ts (x (s) ) = α s ζ st x (t)

ϕ st (x (s) , x (t) )ϕ t (x (t) )ρ st (x (t) ).

It is now easy to see that this identity to the power |V s | -1 coincides with (14.22) as soon as one takes α s = e. ■

## Computing the most likely configuration

We now address the problem of finding a configuration that maximizes π(x) (mode determination). This problem turns out to be very similar to the computation of marginals, that we have considered so far, and we will obtain similar algorithms.

Assume that G is undirected and acyclic and that π can be written as

$π(x) = 1 Z {s,t}∈E ϕ st (x (s) , x (t) )$s∈V ϕ s (x (s) ).

Maximizing π(x) is equivalent to maximizing

$U (x) = {s,t}∈E ϕ st (x (s) , x (t) )$s∈V ϕ s (x (s) ). [(14.24)](#) Assume that a root has been chosen in G, with the resulting edge orientation yielding a tree G = (V , Ẽ) such that G♭ = G. We partially order the vertexes according to G, writing s ≤ t if there exists a path from s to t in G (s is an ancestor of t). Let V + s contain all t ∈ V with t ≥ s, and define

$U s (x (V + s ) ) = {t,u}∈E V + s ϕ tu (x (t) , x (u) ) t>s ϕ t (x (t) ) and U * s (x (s) ) = max U s (y (V + s ) ), y (s) = x (s) . (14.25)$Since we can write

$U s (x (V + s ) ) = t∈s + ϕ st (x (s) , x (t) )ϕ t (x (t) )U t (x (V + t ) ),(14.26)$we have

$U * s (x (s) ) = max x (t) ,t∈s +        t∈s + ϕ t (x (t) )ϕ st (x (s) , x (t) )U * t (x (t) )        = t∈s + max x t ∈F t (ϕ t (x (t) )ϕ st (x (s) , x (t) )U * t (x (t) )).(14.27)$This provides a method to compute U * s (x (s) ) for all s, starting with the leaves and progressively updating the parents. (When s is a leaf, U * s (x (s) ) = 1, by definition.)

Once all U * s (x (s) ) have been computed, it is possible to obtain a configuration x * that maximizes π. This is because an optimal configuration must satisfy U * s (x (s)

$* ) = U s (x (V + s ) *$) for all s ∈ V , i.e., x (V + s \{s}) * must solve the maximization problem in [(14.25)](#). But because of (14.26), we can separate this problem over the children of s and obtain the fact that, it t ∈ s + ,

$x (t) * = argmax x (t) ϕ t (x (t) )ϕ st (x (s) * , x (t) )U * t (x (t) ) .$This procedure can be rewritten in a slightly different form using messages similar to the belief propagation algorithm. It s ∈ t + , define

$µ st (x (t) ) = max x s ∈F s (ϕ t (x (t) )ϕ ts (x (t) , x (s) )U * s (x (s) ))$and ξ st (x (t) ) = argmax

x (s) ∈F s (ϕ t (x (t) )ϕ ts (x (t) , x (s) )U * s (x (s) )).

Using section 14.4, we get

$µ st (x (t) ) = max x (s) ∈F s        ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈s + µ us (x (s) )        , ξ st (x t ) = argmax x (s) ∈F s        ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈s + µ us (x (s) )        .$An optimal configuration can now be computed using x

$(t) * = ξ ts (x (s) * ), with s ∈ pa(t).$This resulting algorithm therefore first operates upwards on the tree (from leaves to root) to compute the µ st 's and ξ st 's, then downwards to compute x * . This is summarized in the following algorithm.

## Algorithm 14.5

A most likely configuration for

$π(x) = 1 Z {s,t}∈E ϕ st (x s , x t ) s∈V ϕ s (x s ).$can be computed after iterating the following updates, based on any acyclic orientation of G:

(1) Compute, from leaves to root:

$µ st (x (t) ) = max x (s) ∈F s        ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈s + µ us (x (s) )        and ξ st (x (t) ) = argmax x (s) ∈F s        ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈s + µ us (x (s) )        .$(2) Compute, from root to leaves:

$x (t) * = ξ ts (x (s) * ), with s = pa(t).$Similar to the computation of marginals, this algorithm can be rewritten in an orientation-independent form. The main remark is that the value of µ st (x (t) ) does not depend on the tree orientation, as long as it is chosen such that s ∈ t + , i.e., the edge {s, t} is oriented from t to s. This is because such a choice uniquely prescribes the orientation of the edges of the descendants of s for any such tree, and µ st only depends on this structure. Since the same remark holds for ξ st , this provides a definition of these two quantities for any pair s, t such that {s, t} ∈ E. The updating rule now becomes µ st (x (t) ) = max

$x (s) ∈F s         ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈V s \{t} µ us (x (s) )         , (14.28) ξ st (x (t) ) = argmax x (s) ∈F s         ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈V s \{t} µ us (x (s) )         (14.29) with x (t) * = ξ ts (x(s)$* ) for any pair s ∼ t. Like with the m ts in the previous section, looping over updating all µ ts in any order will finally stabilize to their correct values, although, if an orientation is given, going from leaves to roots is obviously more efficient.

The previous analysis is not valid for loopy graphs but section 14.4 and section 14.4 provide well defined iterations when G is an arbitrary undirected graph, and can therefore be used as such, without any guaranteed behavior.

## General sum-prod and max-prod algorithms 14.5.1 Factor graphs

The expressions we obtained for message updating with belief propagation and with mode determination respectively took the form

$m ts (x (s) ) ← x (t) ∈F t ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) )$and

$µ ts (x (s) ) ← max x (t) ∈F t         ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} µ t ′ t (x (t) )         .$They first one is often referred to as the "sum-prod" update rule, and the second as the "max-prod". In our construction, the sum-prod algorithm provided us with a method computing

$σ s (x (s) ) = y (V \{s}) U (x (s) ∧ y (V \{s}) ) with U (x) = s ϕ s (x (s) )${s,t}∈E ϕ st (x (s) , x (t) ).

Indeed, we have, according to (14.11)

$σ s (x (s) ) = ϕ s (x (s) )$t∈V s m ts (x (s) ).

Similarly, the max-prod algorithm computes

$ρ s (x (s) ) = max y V \{s} U (x (s) ∧ y (V \{s}) ) via the relation ρ s (x (s) ) = ϕ s (x (s) )$t∈V s µ ts (x (s) ).

We now discuss generalizations of these algorithms to situations in which the function U does not decompose as a product of bivariate functions. More precisely, let S be a subset of P (V ), and assume the decomposition

$U (x) = C⊂S ϕ C (x C ).$The previous algorithms can be generalized using the concept of factor graphs associated with the decomposition. The vertexes of this graph are either indexes s ∈ V or sets C ∈ S, and the only edges link indexes and sets that contain them. The formal definition is as follows.

Definition 14.11 Let V be a finite set of indexes and S a subset of P (V ). The factor graph associated to V and S is the graph G = (V ∪ S, E), E being constituted of all pairs {s, C} with C ∈ S and s ∈ C.

We assign the variable x (s) to a vertex s ∈ V of the factor graph, and the function ϕ C to C ∈ S. With this in mind, the sum-prod and max-prod algorithms are extended to factor graphs as follows. Definition 14.12 Let G = (V ∪S, E) be a factor graph, with associated functions ϕ C (x C ). The sum-prod algorithm on G updates messages m sC (x s ) and m Cs (x s ) according to the rules

$               m sC (x (s) ) ← C,s∈ C, C C m Cs (x (s) )$m Cs (x (s) ) ← y C :y (s) =x (s)   ϕ C (y (C) ) t∈C\{s} m tC (y (t) ) (14.30)

Similarly, the max-prod algorithm iterates

$               µ sC (x (s) ) ← C,s∈ C, C C µ Cs (x (s) )$µ Cs (x (s) ) ← max y (C) :y (s) =x (s)   ϕ C (y (C) ) t∈C\{s} µ tC (y (t) ) (14.31)

These algorithms reduce to the original ones when only single vertex and pair interactions exist. Let us check this with sum-prod. In this case, the set S contains all singletons C = {s}, with associated function ϕ s , and all edges {s, t} with associated function ϕ st . We have links between s and {s} and s and {s, t} ∈ E. For singletons, we have m s{s} (x (s) ) ← t∼s m s{s,t} (x (s) ) and m {s}s (x (s) ) ← ϕ s (x (s) ).

For pairs, m s{s,t} (x (s) ) ← ϕ s (x (s) )

t∈V s \{t} m {s, t}s (x (s) ) and m {s,t}s (x (s) ) ←

$y (t)$ϕ st (x (s) , y (t) )m t{s,t} (y (t) )

and, combining the last two assignments, it becomes clear that we retrieve the initial algorithm with m {s,t}s taking the role of what we previously denoted m ts .

The important question, obviously, is whether the algorithms converge. The following result shows that this is true when the factor graph is acyclic. Proposition 14.13 Let G = (V ∪ S, E) be a factor graph with associated functions ϕ C . Assume that G is acyclic. Then the sum-prod and max-prod algorithms converge in finite time.

After convergence, we have σ s (x (s) ) = C,s∈C m Cs (x (s) ) and ρ s (x (s) ) = C,s∈C µ Cs (x (s) ).

Proof Let us assume that G is connected, which is without loss of generality, since the following argument can be applied to each component of G separately. Since G is acyclic, we can arbitrarily select one of its vertexes as a root to form a tree. This being done, we can see that the messages going upward in the tree (from children to parent) progressively stabilize, starting with leaves. Leaves in the factor graph indeed are either singletons, C = {s}, or vertexes s ∈ V that belong to only one set C ∈ S. In the first case, the algorithm imposes (taking, for example, the sum-prod case) m {s}s (x (s) ) = ϕ s (x (s) ), and in the second case m sC (x (s) ) = 1. So the messages sent upward by the leaves are set at the first step. Since the messages going from a child to its parents only depend on the messages that it received from its other neighbors in the acyclic graph, which are its children in the tree, it is clear that all upward messages progressively stabilize until the root is reached. Once this is done, messages propagate downward from each parent to its children. This stabilizes as soon as all incoming messages to the parent are stabilized, since outgoing messages only depend on those. At the end of the upward phase, this is true for the root, which can then send its stable message to its children. These children now have all their incoming messages and can now send their messages to their own children and so on down to the leaves.

We now consider the second statement, proceeding by induction, assuming that the result is true for any smaller graph than the one considered. Let s 0 be the selected root, and consider all vertexes s s 0 such that there exists C s ∈ S such that s 0 and s both belong to C s . Given s, there cannot be more than one such C s since this would create a loop in the graph. For each such s, consider the part G s of G containing all descendants of s. Let V s be the set of vertexes among the descendants of s and C s the set of C's below s. Define

$U s (x (V s ) ) = C∈C s ϕ C (x (C) ).$Since the upward phase of the algorithm does not depend on the ancestors of s, the messages incoming to s for the sum-prod algorithm restricted to G s are the same as with the general algorithm, so that, using the induction hypothesis y (V s ) ,y (s) =x (s)   U s (y (V s ) ) = C∈C s ,s∈C m Cs (x (s) ) = m sC s (x (s) ). Now let C 1 , . . . , C n list all the sets in C that contain s 0 , which must be non-intersecting (excepted at {s 0 }), again not to create loops. Write

$C 1 ∪ • • • ∪ C n = {s 0 , s 1 , . . . , s q }.$Then, we have

$U (x) = n j=1 ϕ C j (x (C j ) ) q i=1 U s i (x (V s i ) )$and letting S = n j=1 C j \ {s 0 },

$σ s 0 (x (s 0 ) ) = y (V ) :y (s 0 ) =x (s 0 ) n j=1 ϕ C j (y (C j ) ) q i=1 U s i (y (V s i ) ) = y ( ) S:y (s 0 ) =x (s 0 ) n j=1 ϕ C j (y (C j ) ) q i=1 m s i C s i (y (s i ) ) = n j=1 y ( ) C j :y (s 0 ) =x (s 0 ) ϕ C j (y (C j ) ) s∈C j \{s 0 } m sC s (y (s) ) = n j=1 m C j s 0 (x (s 0 ) )$which proves the required result (note that, when factorizing the sum, we have used the fact that the sets C j \ {s 0 } are non intersecting). An almost identical argument holds for the max-prod algorithm.

■ Remark 14.14 Note that these algorithms are not always feasible. For example, it is always possible to represent a function U on F (V ) with the trivial factor graph in which S = {V } and E contains all {s, V }, s ∈ V (using ϕ V = U ), but computing m V s is identical to directly computing σ s with a sum over all configurations on V \ {s} which grows exponentially. In fact, the complexity of the sum-prod and max-prod algorithms is exponential in the size of the largest C in S which should therefore remain small. ♦ Remark 14.15 It is not always possible to decompose a function so that the resulting factor graph is acyclic with small degree (maximum number of edges per vertex). Sum-prod and max-prod can still be used with loopy networks, sometimes with excellent results, but without theoretical support. ♦ Remark 14. [16](#b34) One can sometimes transform a given factor graph into an acyclic one by grouping vertexes. Assume that the set S ⊂ P (V ) is given. We will say that a partition ∆ = (D 1 , . . . D k ) of V is S-admissible if, for any C ∈ S and any j ∈ {1, . . . , k}, one has either

$D j ∩ C = ∅ or D j ⊂ C.$If ∆ is S-admissible, one can define a new factor graph G as follows. We first let Ṽ = {1, . . . , k}. To define S ⊂ P ( Ṽ ) assign to each C ∈ S the set J C of indexes j such that D j ⊂ C. From the admissibility assumption, [(14.32)](#) so that C → J C is one-to-one. Let S = {J C , C ∈ S}. Group variables using x(k) = x (D k ) , so that Fk = F (D k ). Define Φ = ( φ C , C ∈ S) by φ C = ϕ C where C is given by (14.32).

$C = j∈J C D j ,$In other terms, one groups variables (x (s) , s ∈ V ) into clusters, to create a simpler factor graph, which may be acyclic even if the original one was not. For example, if V = {a, b, c, d}, S = {A, B} with A = {a, b, c} and B = {b, c, d}, then (A, c, B, b) is a cycle in the associated factor graph. If, however, one takes D 1 = {a}, D 2 = {b, c} and D 3 = {d}, then (D 1 , D 2 , D 3 ) is S-admissible and the associated factor graph is acyclic. In fact, in such a case, the resulting factor graph, considered as a graph with vertexes given by subsets of V , is a special case of a junction tree, which is defined in the next section.♦ 14.5.2 Junction trees Definition 14.17 Let V be a finite set. A junction tree on V is an undirected acyclic graph G = (S, E) where S ⊂ P (V ) is a family of subsets of V that satisfy the following property, called the running intersection constraint: if C, C ′ ∈ S and s ∈ C ∩ C ′ , then all sets C ′′ in the (unique) path connecting C and C ′ in G must also contain s.

Remark 14.18 Let us check that the clustered factor graph G defined in remark 14.16 is equivalent to a junction tree when acyclic. Using the same notation, let Ŝ = {D 1 , . . . , D k } ∪ S, removing if needed sets C ∈ S that coincide with one of the D j 's. Place an edge between D j and C if and only if D j ⊂ C.

Let (C 1 , D i 1 , . . . , D i n-1 , C n ) be a path in that graph. Assume that s ∈ C 1 ∩ C 2 . Let D i n be the unique D j that contains s. It is such that from the the admissibility assumption, D i n ⊂ C 1 and D i n ⊂ C n , which implies that (C 1 , D i 1 , . . . , C n , D i n , C 1 ) is a path in G. Since G is acyclic, this path must be a union of folded paths. But it is easy to see that any folded path satisfies the running intersection constraint. (Note that there was no loss of generality in assuming that the path started and ended with a "C", since any "D" must be contained in the C that follows or precedes it.) ♦

We now consider a probability distribution written in the form

$π(x) = 1 Z C∈S ϕ C (x (C) )$and we make the assumption that S can be organized as a junction tree.

Belief propagation can be extended to junction trees. Fixing a root C 0 ∈ S, we first choose an orientation on G, which induces as usual a partial order on S. For C ∈ S, define S + C as the set of all B ∈ S such that B > C. Define also

$V + C = B∈S + C B.$We want to compute sums

$σ C (x (C) ) = y (V \C) U (x (C) ∧ y (V \C) ),$where U (x) = C∈S ϕ C (x (C) ). We have

$σ C (x (C) ) = y (V \C) ϕ C (x (C) ) B∈S\{C} ϕ B (x (B∩C) ∧ y (B\C) ). Define σ + C (x (C) ) = y (V + C \C) B>C ϕ B (x (B∩C) ∧ y (B\C) ).$Note that we have σ C 0 = ϕ C 0 σ + C 0 at the root. We have the recursion formula

$σ + C (x (C) ) = y (V + C \C) C→B        ϕ B (x (B∩C) ∧ y (B\C) ) B ′ >B ϕ B ′ (x (B ′ ∩C) ∧ y (B ′ \C) )        = C→B y (B∪V + B \C) ϕ B (x (B∩C) ∧ y (B\C) ) B ′ >B ϕ B ′ (x (B ′ ∩C) ∧ y (B ′ \C) ) = C→B y (B\C) ϕ B (x (B∩C) ∧ y (B\C) )σ + B (x (B∩C) ∧ y (B\C) ).$The inversion between the sum and product in the second equation above was possible because the sets B ∪ V + B \ C, C → B are disjoint. Indeed, if there existed B, B ′ such that C → B and C → B ′ , and descendants C ′ of B ′ and C ′′ of B ′′ with a nonempty intersection, then this intersection would have to be included in every set in the (non-oriented) path connecting C ′ and C ′′ in G. Since this path contains C, the intersection must also be included in C, so that the sets

$B ∪ V + B \ C, with C → B are disjoint. Introduce messages m + B (x (C) ) = y (B\C) ϕ B (x (B∩C) ∧ y (B\C) )σ + B (x (B∩C) ∧ y (B\C) )$where C is the parent of B. Then

$m + B (x (C) ) = y (B\C) ϕ B (x (B∩C) ∧ y (B\C) ) B→B ′ m + B ′ (x (B∩C) ∧ y (B\C) ) with σ + C (x (C) ) = C→B m + B (x (C) )$which provides σ C at the root. Reinterpreting this discussion in terms of the undirected graph, we are led to introducing messages m BC (x (C) ) for B ∼ C in G, with the message-passing rule

$m BC (x (C) ) = y (B\C) ϕ B (x (B∩C) ∧ y (B\C) ) B ′ ∼B,B ′ C m B ′ B (x (B∩C) ∧ y (B\C) ). (14.33)$Messages progressively stabilize when applied in G, and at convergence, we have

$σ C (x (C) ) = ϕ C (x (C) ) B∼C m BC (x (C) ). (14.34)$Note that the complexity of the junction tree algorithm is exponential in the cardinality of the largest C ∈ S. This algorithm will therefore be unfeasible if S contains sets that are too large.

## Building junction trees

There is more than one family of set interactions with respect to which a given probability π can be decomposed (notice that, unlike in the Hammersley-Clifford Theorem, we do not assume that the interactions are normalized), and not all of them can be organized as a junction tree. One can however extend any given family into a new one on which one can build a junction tree. Definition 14.19 Let V be a set of vertexes, and S 0 ⊂ P (V ). We say that a set S ⊂ P (V ) is an extension of S 0 if, for any C 0 ∈ S 0 , there exists a C ∈ S such that C 0 ⊂ C.

A tree G = (S, E) is a junction-tree extension of S 0 if S is an extension of S 0 and G is a junction tree.

$If Φ 0 = (ϕ 0 C , C ∈ S 0$) is a consistent family of set interactions, and S is an extension of S 0 , one can build a new family, Φ = (ϕ C , C ∈ S), of set interactions which yields the same probability distribution, i.e., such that, for all x ∈ F (V ),

$C∈S ϕ C (x (C) ) ∝ C 0 ∈S 0 ϕ 0 C 0 (x (C 0 ) ).$For this, it suffices to build a mapping say T : S 0 → S such that C 0 ⊂ T (C 0 ) for all C 0 ∈ S 0 , which is always possible since S is an extension of S 0 (for example, arbitrarily order the elements of S and let T (S 0 ) be the first element of S, according to this order, that contains C 0 ). One can then define

$ϕ C (x (C) ) = C 0 :T (C 0 )=C ϕ 0 C 0 (x (C 0 ) ).$Given Φ 0 , our goal is to design a junction-tree extension which is as feasible as possible. So, we are not interested by the trivial extension G = (V , ∅), since the resulting junction-tree algorithm is unfeasible as soon as V is large. Theorem 14.24 in the next section will be the first step in the design of an algorithm that computes junction trees on a given graph.

## Triangulated graphs

Definition 14.20 Let G = (V , E) be an undirected graph. Let (s 1 , s 2 , . . . , s n ) be a path in G. One says that this path has a chord at s j , with j ∈ {2, . . . , n} , if s j-1 ∼ s j+1 , and we will refer to (s j-1 , s j , s j+1 ) as a chordal triangle. A path in G is achordal if it has no chord.

One says that G is triangulated (or chordal) if it has no achordal loop.

## Definition 14.21 The graph G is decomposable if it satisfies the following recursive condition: it is either complete, or there exists disjoint subsets

$(A, B, C) of V such that • V = A ∪ B ∪ C, • A and B are not empty, • C is clique in G, C separates A and B,$• the restricted graphs, G A∪C and G B∪C are decomposable.

These definitions are in fact equivalent, as stated in the following proposition.

## Proposition 14.22 An undirected graph is triangulated if and only if it is decomposable

Proof To prove the "if" part, we proceed by induction on n = |V |. Note that every graph for n ≤ 3 is both decomposable and triangulated (we leave the verification to the reader). Assume that the statement "decomposable ⇒ triangulated" holds for graphs with less than n vertexes, and take G with n vertexes. Assume that G is decomposable. If it is complete, it is obviously triangulated. Otherwise, there exists A, B, C such that V = A ∪ B ∪ C, with A and B non-empty such that G A∪C and G B∪C are decomposable, hence triangulated from the induction hypothesis, and such that C is a clique which separates A and B. Assume that γ is an achordal loop in G. Since it cannot be included in A ∪ C or B ∪ C, γ must go from A to B and back, which implies that it passes at least twice in C. Since C is complete, the original loop can be shortcut to form subloops in A ∪ C and B ∪ C. If one of (or both) these loops has cardinality 3, this would provide γ with a chord, which contradicts the assumption. Otherwise, the following lemma also provides a contradiction, since one of the two chords that it implies must also be a chord in the original γ. Lemma 14.23 Let (s 1 , . . . , s n , s n+1 = s 1 ) be a loop in a triangulated graph, with n ≥ 4.

## Then the path has a chord at two non-contiguous vertexes at least.

To prove the lemma, assume the contrary and let (s 1 , . . . , s n , s n+1 = s 1 ) be a loop that does not satisfy the condition, with n as small as possible. If n > 4, the loop must have a chord, say at s j , and one can remove s j from the loop to still obtain a smaller loop that must satisfy the condition in the lemma, since n was as small as possible. One of the two chords must be at a vertex other than the two neighbors of s j , and thus provide a second chord in the original loop, which is a contradiction. Thus n = 4, but G being triangulated implies that this 4-point loop has a diagonal, so that the condition in the lemma also holds, which provides a contradiction.

For the "only if" part of proposition 14.22, assume that G is triangulated. We prove that the graph is decomposable by induction on |G|. The induction will work if we can show that, if G is triangulated, it is either complete or there exists a clique in G such that V \ C is disconnected, i.e., there exist two elements a, b ∈ V \ C which are related by no path in V \C. Indeed, we will then be able to decompose V = A∪B∪C, where A and B are unions of (distinct) connected components of V \ C. Take, for example, A to be the set of vertexes connected to A in G \ C, and B = V \ (A ∪ C), which is not empty since it contains b. Note that restricted graphs from triangulated graphs are triangulated too. So, assume that G is triangulated, and not complete. Let C be a subset of V that satisfies the property that V \ C is disconnected, and take C minimal, so that V \ C ′ is connected for any C ′ ⊂ C, C ′ C. We want to show that C is a clique, so take s and t in C and assume that they are not neighbors to reach a contradiction.

Let A and B be two connected components of V \ C. For any a ∈ A, b ∈ B, and s, t ∈ C, we know that there exists a path between a and b in V \ C ∪ {s} and another one in V \C ∪{t}, the first one passing by s (because it would otherwise connect a and b in V \ C) and the second one passing by t. Any point before s (or t) in these paths must belong to A, and any point after them must belong to B. Concatenating these two paths, and removing multiple points if needed, we obtain a loop passing in A, then by s, then in B, then by t. We can recursively remove all points at which these paths have a chord. We can also notice that we cannot remove s nor t in this process, since this would imply an edge between A and B, and that we must leave at least one element in A and one in B because removing the last one would require s ∼ t. So, at the end, we obtain an achordal loop with at least four points, which contradicts the fact that G is triangulated.

## ■

We can now characterize graphs that admit junction trees over the set of their maximal cliques. (ii) G is triangulated/decomposable.

Proof The proof works by induction on the number of maximal cliques, |C * G |. If G has only one maximal clique, then G is complete, because any point not included in this clique will have to be included in another maximal clique, which leads to a contradiction. So G is decomposable, and, since any single node obviously provides a junction tree, (i) is true also. Now, fix G and assume that the theorem is true for any graph with fewer maximal cliques. First assume that C * G has a junction tree, T . Let C 1 be a leaf in T , connected, say, to C 2 , and let T 2 be T restricted to

$C 2 = C * G \{C 1 }. Let V 2 be the unions of maximal cliques from nodes in T 2 . A maximal clique C in G V 2 is a clique in G V and therefore included in some maximal clique C ′ ∈ C V . If C ′ ∈ C 2 , then C ′ is also a clique in G V 2 ,$and for C to be maximal, we need C = C ′ . If C ′ = C 1 , we note that we must also have

$C = C∈C 2 C ∩ C$and whenever C ∩ C is not empty, this set must be included in any node in the path in T that links C to C 1 . Since this path contains C 2 , we have

$C ∩ C ⊂ C 2 so that C ⊂ C 2 , but, since C is maximal, this would imply that C = C 2 = C 1 which is impossible. This shows that C * G 2 = C 2 .$This also shows that T 2 is a junction tree over C 2 . So, by the induction hypothesis, G V 2 is decomposable. If s ∈ V 2 ∩ C 1 , then s also belongs to some clique C ′ ∈ C 2 , and therefore belongs to any clique in the path between

$C ′ and C 1 , which includes C 2 . So s ∈ C 1 ∩ C 2 and C 1 ∩ V 2 = C 1 ∩ C 2 . So, letting A = C 1 \ (C 1 ∩ C 2 ), B = V 1 \ (C 1 ∩ C 2 ), S = C 1 ∩ C 2 ,$we know that G A∪S and G B∪S are decomposable (the first one being complete), and that S is a clique. To show that G is decomposable, it remains to show that S separates A from B. If a path connects A to B in G, it must contain an edge, say {s, t}, with s ∈ V \S and t ∈ S; {s, t} must be included in a maximal clique in G. If this clique is C 1 , we have s ∈ C 1 ∩ V 2 = S. The same argument shows that this is the only possibility, because, if {s, t} is included in some maximal clique in C 2 , then we would find t ∈ C 1 ∩ C 2 . So S separates A and B in G. Let us now prove the converse statement, and assume that G is decomposable. If G is complete, it has only one maximal clique and we are done. Otherwise, there exists a partition V = A ∪ B ∪ S such that G A∪S and G B∪S are decomposable, A and B separated by S which is complete. Let C * A be the maximal cliques in G A∪S and C * B the maximal cliques in G B∪S . By hypothesis, there exist junction trees T A and T B over C * A and C * B . Let C be a maximal clique in G A∪S . Assume that C intersect A; C can be extended to a maximal clique, C ′ , in G, but C ′ cannot intersect B (since this would imply a direct edge between A and B) and is therefore included in A ∪ S, so that C = C ′ . Similarly, all maximal cliques in G B∪S that intersect B also are maximal cliques in G. The clique S is included in some maximal clique S * A ∈ C * A . From the previous discussion, we have either S * A = S or S * A ∈ C * G . Similarly, S can be extended to a maximal clique S * B ∈ C * B , with S * B = S or S * B ∈ C * G . Notice also that at least one of S * A

or S * B must be a maximal clique in G: indeed, assume that both sets are equal to S, which, as a clique, can extended to a maximal clique S * in G; S * must be included either in A ∪ S or in B ∪ S, and therefore be a maximal clique in the corresponding graph which yields S * = S. Reversing the notation if needed, we will assume that

$S * A ∈ C * G .$All elements of C * G must belong either to C * A or C * B since any maximal clique, say C, in G must be included in either A ∪ S or B ∪ S, and therefore also provide a maximal clique in the related graph. So the nodes in T A and T B enumerate all maximal cliques in G, and we can build a tree T over C *

## G by identifying S *

A and S * B to S * and merging the two trees at this node. To conclude our proof, it only remains to show that the running intersection property is satisfied. So consider two nodes C, C ′ in T and take s ∈ C ∩ C ′ . If the path between these nodes remain in C * A , or in C * B , then s will belong to any set along that path, since the running intersection is true on T A and T B . Otherwise, we must have s ∈ S, and the path must contain S * to switch trees, and s must still belong to any clique in the path (applying the running intersection property between the beginning of the path and S * , and between S * and the end of the path).

## ■

This theorem delineates a strategy in order to build a junction tree that is adapted to a given family of local interactions Φ = (ϕ C , C ∈ C). Letting G be the graph induced by these interactions, i.e., s ∼ G t if and only if there exists C ∈ C such that {s, t} ⊂ C, the method proceeds as follows. Steps (JT4) and (JT5) have already been discussed, and we now explain how the first three steps can be implemented.

## Building triangulated graphs

First consider step (JT1). To triangulate a graph G = (V , E), it suffices to order its vertexes so that V = {s 1 , . . . , s n }, and then run the following algorithm. • Add an edge to any pair of neighbors of s k (unless, of course, they are already linked).

• Let E k-1 be the new set of edges.

Then the graph G * = (V , E 0 ) is triangulated. Indeed taking any achordal loop, and selecting the vertex with highest index in the loop, say s k , brings a contradiction, since the neighbors of s k have been linked when building E k-1 .

However, the quality of the triangulation, which can be measured by the number of added edges, or by the size of the maximal cliques, highly depends on the way vertexes have been numbered. Take the simple example of the linear graph with three vertexes A ∼ B ∼ C. If the point of highest index is B, then the previous algorithm will return the three-point loop A ∼ B ∼ C ∼ A. Any other ordering will leave the linear graph, which is already triangulated, invariant.

So, one must be careful about the order with which nodes will be processed. Finding an optimal ordering for a given global cost is an NP-complete problem. However, a very simple modification of the previous algorithm, which starts with s n having the minimal number of neighbors, and at each step defines s k to be the one with fewest neighbors that haven't been visited yet, provides an efficient way for building triangulations. (It has the merit of leaving G invariant if it is a tree, for example). Another criterion may be preferred to the number of neighbors (for example, the number of new edges that would be needed if s is added).

If G is triangulated, there exists an ordering of V such that the algorithm above leaves G invariant. We now proceed to a proof of this statement and also show that such an ordering can be computed using an algorithm called maximum cardinality search, which, in addition, allows one to decide whether a graph is triangulated. We start with a definition that formalizes the sequence of operations in the triangulation algorithm. Definition 14.25 Let G = (V , E) be an undirected graph. A node elimination consists in selecting a vertex s ∈ V and building the graph G (s) = (V (s) , E (s) ) with V (s) = V \ {s}, and E (s) containing all pairs {t, t ′ } ⊂ V (s) such that either {t, t ′ } ∈ E or {t, t ′ } ⊂ V s . G (s) is called the s-elimination graph of G. The set of added edges, namely E (s) \(E∩E (s) ) is called the deficiency set of s and denoted D(s) (or D G (s)).

So, the triangulation algorithm implements a sequence of node eliminations, successively applied to s n , s n-1 , etc. One says that such an elimination process is perfect if, for all k = 1, . . . , n, the deficiency set of s k in the graph obtained after elimination of s n , . . . , s k+1 is empty (so that no edge is added during the process). We will also say that (s 1 , . . . , s n ) provides a perfect ordering for G.

## Theorem 14.26 An undirected graph G = (V , E) admits a perfect ordering if and only if it is triangulated.

Proof The "only if" part is obvious, since, the triangulation algorithm following a perfect ordering does not add any edge to G, which must therefore have been triangulated to start with. We now proceed to the "if" part. For this it suffices to prove that for any triangulated graph, there exists a vertex s such that D G (s) = ∅. One can then easily prove the result by induction, since, after removing this s, the remaining graph G (s) is still triangulated and would admit (by induction) a perfect ordering that completes this first step.

To prove that such an s exists, we take a decomposition V = A ∪ S ∪ B, in which S is complete and separates A and B, such that |A ∪ S| is minimal (or |B| maximal). We claim that A ∪ S must be complete. Otherwise, since A ∪ S is still triangulated, There exists a similar decomposition A ∪ S = A ′ ∪ S ′ ∪ B ′ . One cannot have S ∩ A ′ and S ∩ B ′ non empty simultaneously, since this would imply a direct edge from A ′ to B ′ (S is complete). Say that S ∩ A ′ = ∅, so that A ′ ⊂ A. Then the decomposition

$V = A ′ ∪ S ′ ∪ (B ′ ∪ B) is such that S ′ separates A ′ from B ∪ B ′ . Indeed, a path from A ′ to b ∈ B ∪ B ′ must pass in S ′ if b ∈ B ′ ,$and, if b ∈ B, it must pass in S (since it links A and B). But S ⊂ S ′ ∪ B ′ so that the path must intersect S ′ . We therefore obtain a decomposition that enlarges B, which is a contradiction and shows that A ∪ S is complete. Given this, any element s ∈ A can only have neighbors in A ∪ S and is therefore such that D G (s) = ∅, which concludes the proof.

## ■

If a graph is triangulated, there is more than one perfect ordering of its vertexes. One of these orderings is provided the maximum cardinality search algorithm, which also allows one to decide whether the graph is triangulated. We start with a definition/notation. Definition 14.27 If G = (V , E) is an undirected graph, with |V | = n, any ordering V = (s 1 , . . . , s n ) can be identified with the bijection α : V → {1, . . . , n} defined by α(s k ) = k. In other terms, α(s) is the rank of s in the ordering. We will refer to α as an ordering, too.

Given an ordering α, we define incremental neighborhoods V α,k s , for s ∈ V and k = 1, . . . , n to be the intersections of V s with the sets α -1 ({1, . . . , k}), i.e.,

$V α,k s = {t ∈ V , t ∼ s, α(t) ≤ k} .$One says that α satisfies the maximum cardinality property if, for all k = {2, . . . , n}

$|V α,k-1 s k | = max α(s)≥k |V α,k-1 s |. (14.35)$where

$s k = α -1 (k).$Given this, we have the proposition:

$Proposition 14.28 If G = (V , E$) is triangulated, then any ordering that satisfies the maximum cardinality property is perfect.

Equation (14.35) immediately provides an algorithm that constructs an ordering satisfying the maximum cardinality property given a graph G. From proposition 14.28, we see that, if for some k, the largest set V α,k-1 s k is not a clique, then G is not triangulated. We now proceed to the proof of this proposition.

Proof Let G be triangulated, and assume that α is an ordering that satisfies [(14.35)](#). Assume that α is not proper in order to reach a contradiction.

Let k be the first index for which V α,k-1 s k is not a clique, so that s k has two neighbors, say t and u, such that α(t) < k, α(u) < k and t ≁ u. Assume that α(t) > α(u). Then t must have a neighbor that is not neighbor of s, say t ′ , such that α(t ′ ) < α(t) (otherwise, s would have more neighbors than t at order less than α(t), which contradicts the maximum cardinality property). The sequence t ′ , t, s, u forms a path that is such that α increases from t ′ to s, then decreases from s to u, and contains no chord. Moreover, t ′ and u cannot be neighbors, since this would yield an achordal loop and a contradiction. The proof of proposition 14.28 consists in showing that this construction can be iterated until a contradiction is reached.

More precisely, assume that an achordal path s 1 , . . . , s k has been obtained, such that α(s) is first increasing, then decreasing along the path, and such that, at extremities one either has α(s 1 ) < α(s k ) < α(s 2 ) or α(s k ) < α(s 1 ) < α(s k-1 ). In fact, one can switch between these last two cases by reordering the path backwards. Both paths (u, s, t) and (u, s, t, t ′ ) in the discussion above satisfy this property.

• Assume, without loss of generality, that α(s 1 ) < α(s k ) < α(s 2 ) and note that, in the considered path, s 1 and s k cannot be neighbors (for, if j is the last index smaller than k -1 such that s j and s k are neighbors, then j must also be smaller than k -2 and the loop s j , . . . , s k-1 , s k would be achordal).

• Since α(s 2 ) > α(s k ), and s 1 and s 2 are neighbors, s k must have a neighbor, say s ′ k , such that s ′ k is not neighbor of s 2 and α(s ′ k ) < α(s k ). • Select the first index j > 2 such that s j ∼ s ′ k , and consider the path (s 1 , . . . , s j , s ′ k ). This path is achordal, by construction, and one cannot have s 1 ∼ s ′ k since this would create an achordal loop. Let us show that α first increases and then decreases along this path. Since s 2 is in the path, α must first increase, and it suffices to show that α(s ′ k ) < α(s j ). If α increases from s 1 to s j , then α(s j ) > α(s 2 ) > α(s k ) > α(s ′ k ). If α started decreasing at some point before s j , then α(s j ) > α(s k ) > α(s ′ k ). • Finally, we need to show that the α-value at one extremity is between the first two α-values on the other end of the path. If α(s ′ k ) < α(s 1 ), and since we have just seen that α(s j ) > α(s k ) > α(s 1 ), we do get α(s

$′ k ) < α(s 1 ) < α(s j ). If α(s ′ k ) > α(s 1 ), then, since by construction α(s 2 ) > α(s k ) > α(s ′ k ), we have α(s 2 ) > α(s ′ k ) > α(s 1$). • So, we have obtained a new path that satisfies the same property that the one we started with, but with a maximum value at end points smaller than the initial one, i.e., max(α(s 1 ), α(s ′ k )) < max(α(s 1 ), α(s k )). Since α takes a finite number of values, this process cannot be iterated indefinitely, which yields our contradiction. ■

## Computing maximal cliques

At this point, we know that a graph must be triangulated for its maximal cliques to admit junction trees, and we have an algorithm to decide whether a graph is triangulated, and extend it into a triangulated one if needed. This provides the first step, (JT1), of our description of the junction tree algorithm. The next step, (JT2), requires computing a list of maximal cliques. Computing maximal cliques in general graph is an NP complete problem, for which a large number of algorithms has been developed (see, for example, [[149]](#b167) for a review). For graphs with a perfect ordering, however, this problem can always be solved in a polynomial time.

## Indeed, assume that a perfect ordering is given for

$G = (V , E), so that V = {s 1 , . . . , s n } is such that, for all k, V ′ s k := V s k ∩ {s 1 , . . . , s k-1 } is a clique. Let G k be G restricted to {s 1 , . . . , s k } and C * k be the set of maximal cliques in G k . Then the set C k := {s k } ∪ V ′$s k is the only maximal clique in G k that contains s k : it is a clique because the ordering is perfect, and any clique that contains s k must be included in it (because its elements are either s k or neighbors of s k ). It follows from this that the set C * k can be deduced from

$C * k-1 by        C * k = C * k-1 ∪ {C k } if V ′ k C * k-1 C * k = (C * k-1 ∪ {C k }) \ {V ′ k } if V ′ k ∈ C * k-1$This allows one to enumerate all elements in C * G = C * n , starting with C * 1 = {{s 1 }}.

## Characterization of junction trees

We now discuss the last remaining point, (JT3

). For this, we need to form the clique graph of G, which is the undirected graph G = (C * G , E) defined by (C, C ′ ) ∈ E if and only if C ∩ C ′ ∅. We then have the following fact: Proposition 14.29 The clique graph G of a connected triangulated undirected graph G is connected. Proof We proceed by induction, and assume that the result is true if |V | = n -1 (the proposition obviously holds if |V | = 1). Assume that a perfect order on G has been chosen, say V = {s 1 , . . . , s n }. Let G ′ be G restricted to {s 1 , . . . , s n-1 }, and G ′ the associated clique graph. Because {s n } ∪ V s n is a clique, any path in G provides a valid path in G ′ after removing all occurrences of s n (because any two neighbors of s n are linked). The induction hypothesis also implies that G ′ is connected. Since G is connected, V s n is not empty. Moreover, C := {s n } ∪ V s n must be a maximal clique in G (since we assume that the order is perfect) and it is the only maximal clique in G that contains s n (all other maximal cliques in G therefore are maximal cliques in G ′ also). To prove that G is connected, it suffices to prove that C is connected to any other maximal clique, C ′ , in G by a path in G. If t ∈ C, t s n , there exists a maximal clique, say C ′′ , in G ′ that contains t, and, since G ′ is connected, there exists a path (C 1

$= C ′ , . . . , C q = C ′′ ) connecting C ′ to C ′′ in G ′ .$Let j be the first integer such that C j = V n (take j = q + 1 if this never happens). Then (C 1 , . . . , C j-1 , C) is a path linking C ′ and C in G.

## ■

We hereafter assume that G, and hence G, is connected. This is not real loss of generality because connected components in undirected graphs yields independent processes that can be handled separately. We assign weights to edges of the clique graph of G by defining w(C, C ′ ) = |C ∩ C ′ |. A subgraph T of any given graph G is called a spanning tree if T is a tree with set of vertexes equal to the set of vertexes of G. If T = (C * G , E ′ ) is a spaning tree of G, we define the total weight

$w(T ) = {C,C ′ }∈E ′ w(C, C ′ ).$We then have the proposition: Proposition 14.30 [[99]](#b117) If G is a connected triangulated graph, the set of junction trees over C * G coincides with the set of maximizers of w(T ) over all spanning trees of G.

(Notice that G being connected implies that spanning trees over G exist.)

Before proving this proposition, we discuss some properties related to maximal (or maximum-weight) spanning trees over an undirected graph. For this discussion, we let G = (V , E) be any undirected graph with weight (w(e), e ∈ E). We will then apply these results to a clique graph when will switch back to the general notation of this section. Maximal spanning trees can be computed using the so-called Prim's algorithm [[98,](#b116)[155,](#b173)[63]](#b81). Algorithm 14.7 (Prim's algorithm) Initialize the algorithm with a single-node tree T 1 = ({s 1 }, ∅), for some arbitrary

$s 1 ∈ V . Let T k-1 = (V k-1 , E k-1$) be the tree obtained at step k -1 of the algorithm. If k ≤ n, the next tree is built as follows.

(1) Let

$V k = {s k } ∪ V k-1 (s k V k-1 .) (2) Let E k = {e k } ∪ E k-1 , such that e k = {s k , s} for some s ∈ V k-1 satisfying w(e k ) = max w({t, t ′ }), {t, t ′ } ∈ E, t V k-1 , t ′ ∈ V k-1 . (14.36)$The ability of this algorithm to always build a maximal spanning tree is summarized in the following proposition [[81,](#b99)[129]](#b147). [,](#)[E,](#)[w](#)) is a weighted, connected undirected graph, Prim's algorithm, as described above, provides a sequence T k = (V k , E k ), for k = 1, . . . , n of subtrees of G such that V n = V and, for all k, T k is a maximal spanning tree for the restriction

$Proposition 14.31 If G = (V$$G V k of G to V k .$Moreover, any maximal spanning tree of G, can be realized as T n , where (T 1 , . . . , T n ) is a sequence provided by Prim's algorithm.

Proof We first prove that, for all k, T k is a maximal spanning tree on the graph G V k .

We will prove a slightly stronger statement, namely, that, for all k, T k can be extended to form a maximal spanning tree of G. This is stronger, because, if T k = (V k , E k ) can be extended to a maximal spanning tree T = (V , E), and if

$T ′ k = (V k , E ′ k ) is a spanning tree for G V k such that w(T k ) < w(T ′ k ), then the graph T ′ = (V , E ′ ) with E ′ = (E \ E k ) ∪ E ′ k$would be a spanning tree for G with w(T ) < w(T ′ ), which is impossible. (To see that T ′ is a tree, notice that paths in T ′ are in one-to-one correspondence with paths in T by replacing any subpath within T ′ k by the unique subpath in T k that has the same extremities.) Clearly, T 1 , which only has one vertex, can be extended to a maximal spanning tree. Let k ≥ 1 be the last integer for which this property is true for all j = 1, . . . , k. If k = n, we are done. Otherwise, take a maximum spanning tree, T , that extends T k . This tree cannot contain the new edge added when building T k+1 , namely e k+1 = {s k+1 , s} as defined in Prim's algorithm, since it would otherwise also extend T k+1 . Consider the path γ in T that links s to s k . This path must have an edge e = {t, t ′ } such that t ∈ V k and t ′ V k , and by definition of e k+1 , we must have w(e) ≤ w(e k+1 ). Notice that e is uniquely defined, because a path leaving V k cannot return in this set, since one would be otherwise able to close it into a loop by inserting the only path in T k that connects its extremities.

Replace e by e k+1 in T . The resulting graph, say T ′ , is still a spanning tree for G. From any path in T , one can create a path in T ′ with the same extremities by replacing any occurrence of the edge, e, by the concatenation of the unique path in T going from t to s, followed by (s, s k+1 ), followed by the unique path in T going from s k+1 to t ′ . This implies that T ′ is connected. It is also acyclic, since any loop in T would have to contain e k+1 (since T is acyclic), but there is no other path than (s, s k+1 ) in T ′ that links s and s k , because this path would have to be in T , and we have removed the only possible one from T by deleting the edge e.

As a conclusion, T ′ is an extension of T k+1 , and a spanning tree with total weight larger or equal to the one of T , and must therefore be optimal, too. But this contradicts the fact that T k+1 cannot be extended to a maximal tree, so that k = n and the sequence of trees provided by Prim's algorithm is optimal.

To prove the second statement, let T be an optimal spanning tree. Let k be the largest integer such that there exists a sequence (T 1 , . . . , T k ) generated by Prim's algorithm, such that, for all j = 1, . . . , k, T j is a subtree of T . One necessarily has j ≥ 1, since T extends any one-vertex tree. If k = n, we are done. Assuming otherwise, let T k = (V k , E k ) and make one more step of Prim's algorithm, selecting an edge e k+1 = (s k+1 , s) satisfying [(14.36)](#). By assumption, e k+1 is not in T . Take as before the unique path linking s and s k+1 in T and let e be the unique edge at which this path leaves V k . Replacing e by e k+1 in T provides a new spanning tree, T ′ . One must have w(e) ≥ w(e k+1 ) because T is optimal, and w(e k+1 ) ≥ w(e) by [(14.36)](#). So w(e) = w(e k+1 ), and one can use e instead of e k+1 for the (k + 1)th step of Prim's algorithm. But this contradicts the fact that k was the largest integer in a sequence of subtrees of T that is generated by Prim's algorithm, and one therefore has k = n. ■ The proof of proposition 14.30, that we provide now, uses very similar "edgeswitching" arguments.

Proof (Proof of proposition 14.30) Let us start with a maximum weight spanning tree for G, say T , and show that it is a junction tree. Since T has maximum weight, we know that it can be obtained via Prim's algorithm, and that there exists a sequence T 1 , . . . , T n = T of trees constructed by this algorithm. Let T k = (C k , E k ).

We proceed by contradiction. Let k be the largest index such that T k can be extended to a junction tree for C * G , and let T ′ be a junction tree extension of T k . Assume that k < n, and let e k+1 = (C k+1 , C ′ ) be the edge that has been added when building T k+1 , with C k+1 = {C k+1 } ∪ C k . This edge is not in T ′ , so that there exists a unique edge e = (B, B ′ ) in the path between

$C k and C ′ in T ′ such that B ∈ C k and B ′ C k . We must have w(e) = |B ∩ B ′ | ≤ w(e k+1 ) = |C k+1 ∩ C ′ |.$But, since the running intersection property is true for T ′ , both B and B ′ must contain C k+1 ∩C ′ so that B∩B ′ = C k+1 ∩C ′ . This implies that, if one modifies T ′ by replacing edge e by edge e k+1 , yielding a new spanning tree T ′′ , the running intersection property is still satisfied in T ′ . Indeed if a vertex s ∈ V belongs to both extremities of a path containing B and B ′ in T ′ , then it must belong to B ∩ B ′ , and hence to C k+1 ∩ C ′ , and therefore to any set in the path in T ′ that linked C k+1 and C ′ . So we found a junction tree extension of T k+1 , which contradicts our assumption that k was the largest. We must therefore have k = n and T is a junction tree.

Let us now consider the converse statement and assume that T is a junction tree. Let k be the largest integer such that there exists a sequence of subgraphs of T that is provided by Prim's algorithm. Denote such a sequence by (T 1 , . . . , T k ), with T j = (C j , E j ). Assume (to get a contradiction) that k < n, and consider a new step for This implies that adding e instead of e k+1 at step k + 1 is a valid choice for Prim's algorithm, and contradicts the fact that k was the largest number of such steps that could provide a subtree of T . So k = n and T is maximal. We first introduce some notation. Let G = (V , E) be a directed acyclic graph. The parents of s ∈ V are vertexes t such that (t, s) ∈ E, and its children are t's such that (s, t) ∈ E. The set of parents of s is denoted pa(s), and the set of its children is ch(s), with V s = ch(s) ∪ pa(s).

Similarly to trees, the vertexes of G can be partially ordered by s ≤ G t if and only if there exists a path going from s to t. Unlike trees, however, there can be more than one minimal element in V , and we still call roots vertexes that have no parent, denoting V 0 = {s ∈ V : pa(s) = ∅} .

We also call leaves, or terminal nodes, vertexes that have no children. Unless otherwise specified, we assume that all graphs are connected.

Bayesian networks over G are defined as follows. We use the same notation as with Markov random fields to represent the set of configurations F (V ) that contains collections x = (x s , s ∈ V ) with x s ∈ F s .

## Definition 15.1 A random variable X with values in F (V ) is a Bayesian network over a DAG G = (V , E) if and only if its distribution can be written in the form

$P X (x) = s∈V 0 p s (x (s) ) s∈V \V 0 p s (x (pa(s)) , x (s) ) (15.1)$where p s is, for all s ∈ V , a probability distribution with respect to x (s) .

Using the convention that conditional distributions given the empty set are just absolute distributions, we can rewrite (15.1) as

$P X (x) = s∈V p s (x (pa(s)) , x (s) ). (15.2)$One can verify that x∈Ω P X (x) = 1. Indeed, when summing over x, we can start summing over all x (s) with ch(s) = ∅ (the leaves). Such x (s) 's only appear in the corresponding p s 's, which disappear since they sum to 1. What remains is the sum of the product over V minus the leaves, and the argument can be iterated until the remaining sum is 1 (alternatively, work by induction on |V |). This fact is also a consequence of proposition 15.5 below, applied with A = ∅.

## Conditional independence graph 15.2.1 Moral graph

Bayesian networks have a conditional independence structure which is not exactly given by G, but can be deduced from it. Indeed, fixing S ⊂ V , we can see, when computing the probability of X (S) = x (s) given X (S c ) = x (S c ) , which is

$P(X (S) = x (S) | X (S c ) = x (S c ) ) = 1 Z(x (S c ) ) s∈V p s (x (pa(s)) , x (s) ),$that the only variables x (t) , t S that can be factorized in the normalizing constant are those that are neither parent nor children of vertexes in S, and do not share a child with a vertex in S (i.e., they intervene in no p s (x (pa(s)) , x (s) ) that involve elements of S). This suggests the following definition.

Definition 15.2 Let G be a directed acyclic graph. We denote G ♯ = (V , E ♯ ) the undirected graph on V such that {s, t} ∈ E ♯ if one of the following conditions is satisfied

$• Either (s, t) ∈ E or (t, s) ∈ E. • There exists u ∈ V such that (s, u) ∈ E and (t, u) ∈ E.$G ♯ is sometimes called the moral graph of G (because it forces parents to marry !). A path in G ♯ can be visualized as a path in G ♭ (the undirected graph associated with G) which is allowed to jump between parents of the same vertex even if they were not connected originally.

The previous discussion implies:

Proposition 15.3 Let X be a Bayesian network on G. We have

$(S T | U ) G ♯ ⇒ (X (S) X (T ) | X (U ) ), i.e., X is G ♯ -Markov.$This proposition can be refined by noticing that the joint distribution of X (S) , X (T ) and X (U ) can be deduced from a Bayesian network on a graph restricted to the ancestors of S ∪T ∪U . Definition 13.21 for restricted graphs extends without change to directed graphs, and we repeat it below for convenience. Definition 15.4 Let G = (V , E) be a graph (directed or undirected), and A ⊂ V . The restricted graph G A = (A, E A ) is such that the elements of E A are the edges (s, t) (or {s, t}) in E such that both s and t belong to A.

Moreover, for a directed acyclic graph G and s ∈ V , we define the set of ancestors of s by

$A s = {t ∈ V , t ≤ G s} (15.3)$for the partial order on V induced by G.

If S ⊂ V , we denote A S = s∈S A s . Note that, by definition, S ⊂ A S . The following proposition is true. Proposition 15.5 Let X be a Bayesian network on G = (V , E) with distribution given by (15.2). Let S ⊂ V and A = A S . Then the distribution of X (A) is a Bayesian network over G A given by P(X (A) = x (A) ) = s∈A p s (x (pa(s)) , x (s) ). [(15.4)](#) There is no ambiguity in the notation pa(s), since the parents of s ∈ A are the same in G A as in G.

Proof One needs to show that s∈A p s (x (pa(s)) , x (s) ) =

x A c s∈V p s (x (pa(s)) , x (s) ).

This can be done by induction on the cardinality of V . Assume that the result is true for graphs of size n, and let |V | = n + 1 (the result is obvious for graphs of size 1).

If A = V , there is nothing to prove, so assume that A c is not empty. Then A c must contain a leaf in G, since otherwise, A would contain all leaves and their ancestors which would imply that A = V .

If s ∈ A c is a leaf in G, one can remove the variable x (s) from the sum, since it only appear in p s and transition probabilities sum to one. But one can now apply the induction assumption to the restriction of G to V \ {s}.

## ■

## Proof

Step 1. We first note that the v-junction clause is redundant in [(2)](#b20). It can be removed without affecting the condition. Indeed, if a path in G ♭ passes in V \ A {s,t}∪U one can follow this path downward (i.e., following the orientation in G) until a v-junction is met. This has to happen before reaching the extremities of the path, since u would be an ancestor of s or t otherwise. We can therefore work with the weaker condition (that we will denote (2)') in the rest of proof.

Step 2. Assume that U separates s and t in (G A {s,t}∪U ) ♯ . Take a path γ between s and t in G ♭ . We need to show that the path satisfies (1) or ( [2](#))'. So assume that (2)' is false (otherwise we are done) so that γ is included in A {s,t}∪U . We can modify γ by removing all the central nodes in v-junctions and still keep a valid path in (G A {s,t}∪U ) ♯ (since parents are connected in the moral graph). The remaining path must intersect U by assumption, and this cannot be at a v-junction in γ since we have removed them. So ( [1](#formula_13)) is true.

Step 3. Conversely, assume that (1) or ( [2](#)) is true for any path in G ♭ . Consider a path γ in (G A {s,t}∪U ) ♯ between s and t. Any edge in γ that is not in G ♭ must involve parents of a common child in A {s,t}∪U . Insert this child between the parents every time this occurs, resulting in a v-junction added to γ. Since the added vertexes are still in A {s,t}∪U , the new path still has no intersection with V \ A {s,t}∪U and must therefore satisfy [(1)](#b19). So there must be an intersection with U without a v-junction, and since the new additions are all at v-junctions, the intersection must have been originally in γ, which therefore passes in U . This shows that U separates s and t in (G A {s,t}∪U ) ♯ . ■ Condition (2) can be further restricted to provide the notion of d-separation. Proof It suffices to show that if condition ((D1) or (D2)) holds for any path between s and t in G ♭ , then so does ((1) or ( [2](#))). So take a path between s and t: if (D1) is true for this path, the conclusion is obvious, since (D1) and ( [1](#formula_13)) are the same. So assume that (D1) (and therefore (1)) is false and that (D2) is true. Let u be a vertex in V \ A U at which γ passes with a v-junction.

Assume that (2) is false. Then u must be an ancestor of either s or t. Say it is an ancestor of s: there is a path in G going from u to s without passing by U (otherwise u would be an ancestor of U ); one can replace the portion of the old path between s and u by this new one, which does not pass by u with a v-junction anymore. So the new path still does not satisfy (D1) and must satisfy (D2). Keep on removing all intersections with ancestors of s and t that have v-junctions to finally obtain a path that satisfies neither (D1) or (D2) and a contradiction to the fact that s and t are d-separated by U . 

## Chain-graph representation

The d-separability property involves both unoriented and oriented edges. It is in fact a property of the hybrid graph in which the orientation is removed from the edges that are not involved in a v-junction, and retained otherwise. Such graphs are particular instances of chain graphs. Definition 15.12 A chain graph G = (V , E, Ẽ) is composed with a finite set V of vertexes, a set E ⊂ P 2 (V ) of unoriented edges and a set Ẽ ⊂ E × E \ {(t, t), t ∈ E} of oriented edges with the property that E ∩ Ẽ♭ = ∅, i.e., two vertexes cannot be linked by both an oriented and an unoriented edge.

A path in a chain graph is a a sequence of vertexes s 0 , . . . , s N such that for all k ≥ 1, s k-1 and s k form an edge, which means that either

${s k-1 , s k } ∈ E or (s k-1 , s k ) ∈ Ẽ.$
## A chain graph is acyclic if it contains no loop. It is semi-acyclic if it contains no loop containing oriented edges.

We start with the following equivalence relation within vertexes in a semi-acyclic chain graph. Proposition 15.13 Let G = (V , E, Ẽ) be a semi-acyclic chain graph. Define the relation s R t if and only if there exists a path in the unoriented subgraph (V , E) that links s and t. Then R is an equivalence relation.

The proposition is obvious. This relation partitions V in equivalence classes, the set of which being denoted V R . If S ∈ V R , then any pair s, t in S is related by an unoriented path, and if S S ′ ∈ V R , no elements s ∈ S and t ∈ S ′ can be related by such a path. Moreover, no path in G between two elements of S ∈ V R , can contain a directed edge, since these elements must also be related by an undirected path, and this would create a loop in G containing an undirected edge. So the restriction of G to S is an undirected graph.

One can define a directed graph over equivalence classes as follows. Let G R = (V R , E R ) be such that (S, S ′ ) ∈ E R if and only if there exists s ∈ S and t ∈ S ′ such that (s, t) ∈ Ẽ. The graph G R is acyclic: any loop in G R would induce a loop in G containing at least one oriented edge.

We now can formally define a probability distribution on a semi-acyclic chain graph.

Definition 15.14 Let G = (V , E, Ẽ) be a semi-acyclic chain graph. One says that a random variable X decomposes on G if and only if: (X (S) , S ∈ V R ) is a Bayesian network on G R and the conditional distribution of X (S) given X (S ′ ) , S ′ ∈ pa(S) is G S -Markov, such that, for s ∈ S, P (X (s) = x (s) | X (t) , t ∈ S, X S ′ , S ′ ∈ pa(S)) only depends on x (t) with {s, t} ∈ E or (t, s) ∈ Ẽ.

Returning to our discussion on Bayesian networks, we have the following. Associate to a DAG G = (V , E) the chain graph G † = (V , E † , Ẽ † ) defined by: {s, t} ∈ E † if and only if (s, t) or (t, s) ∈ E and is not involved in a v-junction, and (s, t) ∈ Ẽ † if (s, t) ∈ E and is involved in a v-junction. This graph is acyclic; indeed, take any loop in G † : when its edges are given their original orientations in E, the sequence cannot contain a v-junction, since the orientation in v-junctions are kept in G † ; the path therefore constitutes a loop in G which is a contradiction.

All, excepted at most one, vertexes in an equivalence class S ∈ G † R have all their parents in S. Indeed, assume that two vertexes, s and t, in S have parents outside of S. There exists an unoriented path, s 0 = s, s 1 , . . . , s N = t, in G † connecting them, since they belong to the same equivalence class. The edge at s must be oriented from s to s 1 in G, since otherwise s 1 would be a second parent to s in G, creating a v-junction, and the edge would have remained oriented in G † . Similarly, the last edge in the path must be oriented from t to s N -1 in G. But this implies that there exists a vjunction in the original orientation along the path, which cannot be constituted with only unoriented edges in G † . So we get a contradiction. Thus, random variables that decompose on G † are "Bayesian networks" of acyclic graphs, or trees since we know these are equivalent. The root of each tree must have multiple (vertex) parents in the parent tree in G R . The following theorem states that all Bayesian networks are equivalent to such a process.

## Theorem 15.15 Let G = (V , E) be a DAG. The random variable X is a Bayesian network on G if and only if it decomposes over G † .

Proof Assume that X is a Bayesian network on G. We can obviously rewrite the probability distribution of X in the form

$π(x) = S∈G † R s∈S p s (x (pa(s)) , x (s) ).$Since every vertex in S has its parents in S or in T ∈pa(S) T , this a fortiori takes the form

$π(x) = S∈G † R p S ((x (T ) , T ∈ S -), x (s) ). So X (S) , S ∈ V R is a Bayesian network. Moreover, p S ((x (T ) , T ∈ S -), x (s) ) = s∈S p s (x (pa(s)) , x (s) )$is a tree distribution with the required form of the individual conditional distributions.

Now assume that X decomposes on G † . Then the conditional distribution of X (S) given X (T ) , T ∈ pa(S) is Markov for the acyclic undirected graph G S , and can therefore be expressed as a tree distribution consistent with the orientation of G. ■

## Markov equivalence

While the previous discussion provides a rather simple description of Bayesian networks in terms of chain graphs, it does not go all the way in reducing the number of oriented edges in the definition of a Bayesian network. The issue is, in some way, addressed by the notion of Markov equivalence, which is defined as follows.

## Definition 15.16 Two directed acyclic graphs on the same set of vertexes G = (V , E) and G = (V , Ẽ) are Markov-equivalent if any family of random variables that decomposes as a (positive) Bayesian network over one of them also decomposes as a Bayesian network over the other.

The notion of Markov equivalence is exactly described by d-separation. This is stated in the following theorem, due to Geiger and Pearl [[77,](#b95)[76]](#b94), that we state without proof.

## Theorem 15.17 G and G are Markov equivalent if and only if, whenever two vertexes are d-separated by a set in one of them, the same separation is true with the other.

This property can be expressed in a strikingly simple condition. One says that a v-junction (s, t, u) in a DAG is unlinked if s and u are not neighbors.

## Theorem 15.18 G and G are Markov equivalent if and only if G ♭ = G♭ and G and G have the same unlinked v-junctions.

Proof Step 1. We first show that a given pair of vertexes in a DAG is unlinked if and only if it can be d-separated by some set in the graph. Clearly, if they are linked, they cannot be d-separated (which is the "if" part), so what really needs to be proved is that unlinked vertexes can be d-separated. Let s and t be these vertexes and let U = A {s,t} \ {s, t}. Then U d-separates s and t since any path between s and t in (G A {s,t}∪U ) ♯ = (G A {s,t} ) ♯ must obviously pass in U .

Step 2. We now prove the only-if part of theorem 15.18 and therefore assume that G and G are Markov equivalent, or, as stated in theorem 15.17, that d-separation coincides in G and G. We want to prove that G ♭ = G♭ and unlinked v-junctions are the same.

Step 2.1. The first statement is obvious from Step 1: d-separation determines the existence of a link, so if d-separation coincides in the two graphs, then the same holds for links and G ♭ = G♭ .

Step 2.2. So let us proceed to the second statement and let (s, t, u) be an unlinked vjunction in G. We want to show that it is also a v-junction in G (obviously unlinked since links coincide). We will denote by ÃS the ancestors of some set S ⊂ V in G (while A S still denotes its ancestors in G). Let U = A {s,u} \ {s, u}. Then, as we have shown in Step 1, U d-separates s and u in G, so that, by assumption it also d-separates them in G. We know that t U , because it cannot be both a child and an ancestor of {s, u} in G (this would induce a loop). The path (s, t, u) links s and u and does not pass in U , which is only possible (since U d-separates s and t in G) if it passes in V -ÃU at a v-junction: so (s, t, u) is a v-junction in G, which is what we wanted to prove.

Step 3. We now consider the converse statement and assume that G ♭ = G♭ and unlinked v-junctions coincide. We want to show that d-separation is the same in G and G. So, we assume that U d-separates s and t in G, and we want to show that the same is true in G. Thus, what we need to prove is: Claim 1. Consider a path γ between s and t in G♭ = G ♭ . Then γ either (D1) passes in U without a v-junction in G, or (D2) in V \ ÃU with a v-junction in G.

We will prove Claim 1 using a series of lemmas. We say that γ has a three-point loop at u if (v, u, w) are three consecutive points in γ such that v and w are linked. So (v, u, w, v) forms a loop in the undirected graph.

Lemma 15.19 If γ is a path between s and t that does not satisfy (D2) for G and passes in U without three-point loops, then γ satisfies (D1) for G.

The proof is easy: since γ does not satisfy (D2) in G, it satisfies (D1) and passes in U without a v-junction in G. But this intersection cannot be a v-junction in G since it would otherwise have to be linked and constitute a three-point loop in γ, which proves that (D1) is true for γ in G. The next step is to remove the three-point loop condition in lemma 15.19. This will be done using the next two results.

Lemma 15.20 Let γ be a path with a three-point loop at u ∈ U for G. Assume that γ \ u (which is a valid path in G ♭ ) satisfies (D1) or (D2) in G. Then γ satisfies (D1) or (D2) in G.

To prove the lemma, let v and w be the predecessor and successor of u in γ. First assume that γ \ u satisfies (D1) in G. If this does not happen at v or at w, then this will apply also to γ and we are done, so let us assume that v ∈ U and that (v [then (v,](#)[u,](#)[w](#)) is not and (D1) is true too.

$′ , v, w) is not a v-junction in G, where v ′ is the predecessor of v. If (v ′ , v, u) is not a v-junction in G, then (D1) is true for γ in G. If it is a v-junction,$Assume now that (D2) is true for γ \u in G. Again, there is no problem if (D2) occurs for some point other than v or w, so let us consider the case for which it happens at v. This means that v ÃU and (v ′ , v, w) is a v-junction. But, since u ∈ U , the link between u and v must be from u to v in G so that there is no v-junction at u and (D1) is true in G. This proves lemma 15.20.

Lemma 15.21 Let γ be a path with a three-point loop at u ∈ U for G. Assume that γ does not satisfy (D2) in G. Then γ \ u does not satisfy this property either.

Let us assume that γ \ u satisfies (D2) and reach a contradiction. Letting (v, u, w) be the three-point loop, (D2) can only happen in γ \ u at v or w, and let us assume that this happens at v, so that, v ′ being the predecessor of v, (v ′ , v, w) is a v-junction in G with v A U . Since v A U , the link between u and v in G must be from u to v, but this implies that (v ′ , v, u) is a v-junction in G with v A U which is a contradiction: this proves lemma 15.21.

The previous three lemmas directly imply the next one.

## Lemma 15.22 If γ is a path between s and t that does not satisfy (D2) for G, then γ satisfies (D1) or (D2) for G.

Indeed, if we start with γ that does not satisfy (D2) for G, lemma 15.21 allows us to progressively remove three-point loops from γ until none remains with a final path that satisfies the assumptions of lemma 15.19 and therefore satisfies (D1) in G, and lemma 15.20 allows us to add the points that we have removed in reverse order while always satisfying (D1) or (D2) in G.

We now partially relax the hypothesis that (D2) is not satisfied with the next lemma.

## Lemma 15.23 If γ is a path between s and t that does not pass in V \ A U at a linked v-junction for G, then γ satisfies (D1) or (D2) for G.

Assume that γ does not satisfy (D2) for G (otherwise the result is proved). By lemma 15.22, γ must satisfy (D2) for G. So, take an intersection of γ with V \A U that occurs at a v-junction in G, that we will denote (v, u, w). This is still a v-junction in G since we assume it to be unlinked. Since (D2) is false in G, we must have u ∈ ÃU , and there is an oriented path, τ, from u to U in G.

We can assume that τ has no v-junction in G. If a v-junction exists in τ, then this v-junction must be linked (otherwise this would also be a v-junction in G and contradict the fact that τ is consistently oriented in G), and this link must be oriented from u to U in G to avoid creating a loop in this graph. This implies that we can bypass the v-junction while keeping a consistently oriented path in G, and iterate this until τ has no v-junction in G. But this implies that τ is consistently oriented in G, necessarily from U to u since u A U .

Denote τ = (u 0 = u, v 1 , . . . , u n ∈ U ). We now prove by induction that each (v, u k , w) is an unlinked v-junction. This is true when k = 0, and let us assume that it is true for k -1. Then (u k , u k-1 , v) is a v-junction in G but not in G: so it must be linked and there exists an edge between v and u k . In G, this edge must be oriented from v to u k , since (v, u k-1 , u k , v) would form a loop otherwise. For the same reason, there must be an edge in G from w to u k so that (v, u k , w) is an unlinked v-junction.

Since this is true for k = n, we can replace u by u n in γ and still obtain a valid path. This can be done for all intersections of γ with V \A U that occur at v-junctions. This finally yields a path (denote it γ) which does not satisfy (D2) in G anymore, and therefore satisfies (D1) or (D2) in G: so γ must either pass in U without a v-junction or in V \ ÃU at a v-junction. None of the nodes that were modified can satisfy any of these conditions, since they were all in U with a v-junction, so that the result is true for the original γ also. This proves lemma 15.23.

So the only unsolved case is when γ is allowed to pass in V \A U at linked v-junctions. We define an algorithm that removes them as follows. Let γ 0 = γ and let γ k be the path after step k of the algorithm. One passes from γ k to γ k+1 as follows.

• If γ k has no linked v-junctions in V \ A U for G, stop.

• Otherwise, pick such a v-junction and let (v, u, w) be the three nodes involved in it.

$(i) If v ∈ U , v ′ U and (v ′ , v, u) is a v-junction in G, remove v from γ k to define γ k+1 . (ii) Otherwise, if w ∈ U , w ′ U and (u, w, w ′ ) is a v-junction in G, remove w from γ k to define γ k+1 . (iii) Otherwise, remove u from γ k to define γ k+1 .$None of the considered cases can disconnect the path. This is clear for case (iii) since v and w are linked. For case (i), note that, in G, (v ′ , v, u) cannot be a v-junction since (v, u, w) is one. This implies that the v-junction in G must be linked and that v ′ and u are connected.

The algorithm will stop at some point with some γ n that does not have any linked v-junction in V \ A U anymore, which implies that (D1) or (D2) is true in G for γ n . To prove that this statement holds for γ, it suffices to show that if (D1) or (D2) is true in G with γ k+1 , it must have been true with γ k at each step of the algorithm. So let's assume that γ k+1 satisfies (D1) or (D2) in G.

First assume that we passed from γ k to γ k+1 via case (iii). Assume that (D2) is true for γ k+1 , with as usual the only interesting case being when this occurs at v or w. Assume it occurs at v so that (v ′ , v, w) is a v-junction and v ÃU . If (v ′ , v, u) is a v-junction, then (D2) is true with γ k . Otherwise, there is an edge from v to u in G which also implies an edge from w to u since (v, u, w, v) would be a loop otherwise. So (v, u, w) is a v-junction in G, and u cannot be in ÃU since its parent, v would be in that set also. So (D2) is true in G. Now, assume that (D1) is true at v, so that (v ′ , v, w) is not a v-junction and v ∈ U . If (v ′ , v, u) is not a v-junction either, we are done, so assume the contrary. If v ′ ∈ U , then we cannot have a v-junction at v ′ and (D1) is true. But v ′ U is not possible since this leads to case (i). Now assume that we passed from γ k to γ k+1 via case (i). Assume that (D1) is true for γ k : this cannot be at v ′ since v ′ U , neither at u since u A U , so it will also be true for γ k+1 . The same statement holds with (D2) since (v ′ , v, u) is a v-junction in G with v ∈ U which implies that both v ′ and u are in ÃU . Case (ii) is obviously addressed similarly.

With this, the proof of theorem 15.18 is complete. ■

## Probabilistic inference: Sum-prod algorithm

We now discuss the issue of using the sum-prod algorithm to compute marginal probabilities, P(X (s) = x (s) ) for s ∈ V when X is a Bayesian network on G = (V , E). By definition, P(X = x) can be written in the form

## P(X

$= x) = C∈C ϕ C (x (C) )$where C contains all subsets C s := {s} ∪ pa(s), s ∈ V . Marginal probabilities can therefore be computed easily when the factor graph associated to C is acyclic, according to proposition 14.13. However, because of the specific form of the ϕ C 's (they are conditional probabilities), the sum-prod algorithm can be analyzed in more detail, and provide correct results even when the factor graph is not acyclic.

The general rules for the sum-prod algorithm are

$               m sC (x (s) ) ← C,s∈ C, C C m Cs (x (s) ) m Cs (x (s) ) ← y (C) :y (s) =x (s) ϕ C (y (C) ) t∈C\{s} m tC (y (t) )$They take a particular form for Bayesian networks, using the fact that a vertex s belongs to C s , and to all C t for t ∈ ch(s). (s)   p t (x (s) ∧ y (pa(s)\{t}) , y (t) )m tC t (y (t) ) u∈pa(s),u t m uC t (y (u) ), for t ∈ ch(s).

$m sC s (x (s) ) ← t∈ch(s) m C t s (x (s) ), m sC t (x (s) ) ← m C s s (x (s) ) u∈ch(s),u t m C u s (x (s) ), for t ∈ ch(s), m C s s (x (s) ) ← y (C s ) ,y (s) =x (s) p s (y (pa(s)) , x (s) ) t∈pa(s) m tC s (y (t) ), m C t s (x (s) ) ← y (C t ) ,y (s) =x$These relations imply that, if pa(s) = ∅ (s is a root), then m C s s = p s (x (s) ). Also, if ch(s) = ∅ (s is a leaf) then m sC s = 1. The following proposition shows that many of the messages become constant over time.

Proposition 15.24 All upward messages, m sC s and m C t s with t ∈ ch(s) become constant (independent from x (s) ) in finite time.

Proof This can be shown recursively as follows. Assume that, for a given s, m tC t is constant for all t ∈ ch(s) (this is true if s is a leaf). Then, m C t s (x (s) ) ← peyC t ,y (s) =x (s)   p t (x (s) ∧ y (pa(s)\{t}) , y (t) )m tC t (y (t) ) u∈pa(s),u t m uC t (y (u) ), = m tC t y (C t ) ,y (s) =x (s)   p t (x (s) ∧ y (pa(s)\{t}) , y (t) )

$u∈pa(s),u t m uC t (y (u) ) = m tC t y (C t \{t}) ,y (s) =x (s) u∈pa(s),u t m uC t (y (u) ) = m tC t u∈pa(s),u t y (u) m uC t (y (u) ) which is constant. Now m sC s (x (s) ) ← t∈ch(s) m C t s (x (s) )$is also constant. This proves that all m sC s progressively become constant, and, as we have just seen, this implies the same property for m C t s , t ∈ ch(s).

## ■

This proposition implies that, if initialized with constant messages (or after a finite time), the sum-prod algorithm iterates (s)   p s (y ( ) pa(s), x (s) ) t∈pa(s) m tC s (y (t) )

$m sC s ← t∈ch(s) m C t s m C s s (x (s) ) ← y (C s ) ,y (s) =x$$m sC t (x (s) ) ← m C s s (x (s) ) u∈ch(s),u t m C u s , t ∈ ch(s) m C t s ← m sC s u∈pa(t),u s y (u) m uC s (y (u) ), t ∈ ch(s).$From this expression, we can conclude Proposition 15.25 If the previous algorithm is first initialized with upward messages, m sC s = m C t s all equal to 1, and if downward messages are computed top down from the roots to the leaves, the obtained configuration of messages is invariant for the sum-prod algorithm.

Proof If all upward messages are equal to 1, then clearly, the downward messages sum to 1 once they are updated from roots to leaves, and this implies that the upward messages will remain equal to 1 for the next round. The obtained configuration is invariant since the downward messages are recursively uniquely defined by their value at the roots.

## ■

The downward messages, under the previous assumptions, satisfy m sC t (x (s) ) = m C s s (x (s) ) for all t ∈ ch(s) and therefore m C s s (x (s) ) = y (C s ) ,y (s) =x (s)   π(y (pa(s)) , x (s) ) t∈pa(s) m C t t (y (t) ). [(15.5)](#) Note that the associated "marginals" inferred by the sum-prod algorithm are

$σ s (x (s) ) = C,s∈C m Cs (x (s) ) = m C s s (x (s) )$since m C t s (x (s) ) = 1 when t ∈ ch(s).

Although the sum-prod algorithm initialized with unit messages converges to a stable configuration if run top-down, the obtained σ s 's do not necessarily provide the correct single site marginals. There is a situation for which this is true, however, which is when the initial directed graph is singly connected, as we will see below.

Before this, let us analyze the complexity resulting from an iterative computation of the marginal probabilities, similar to what we have done with trees.

We define the depth of a vertex in G as follows.

Definition 15.26 Let G = (V , E) be a DAG. The depth of a vertex s in V is defined recursively by depth(s) = 0 if s has no parent.

depth(s) = 1 + max depth(t), t ∈ pa(s) otherwise.

The recursive computation of marginal distributions is made possible (although not always feasible) with the following remark. Lemma 15.27 Let X be a Bayesian network on the DAG G = (V , E), and S ⊂ V , such that all elements in S have the same depth. Let pa(S) be the set of parents of elements in S, and T = depth -(S) the set of vertexes in V with depth strictly smaller than the depth of S. Then (X (S) X (T \pa(S)) | X (pa(S)) ) and the variables X (s) , s ∈ S are conditionally independent given X (pa(S)) .

Proof It suffices to show that vertexes in S are separated from T \ pa(S) and from other elements of S by pa(S) for the graph (G S∪T ) ♯ . Any path starting at s ∈ S must either pass by a parent of s (which is what we want), or by one of its children, or by another vertex that shares a child with s in G S∪T . But s cannot have any child in G S∪T , since this child cannot have a smaller depth than s, and it cannot be in S either since all elements in S have the same depth.

## ■

This lemma allows us to work recursively as follows. Assume that we can compute marginal distributions over sets S with maximal depth no larger than d. Take a set S of maximal depth d + 1, and let S 0 be the set of elements of depth d + 1 in S. Then, letting T = depth -(S) = depth -(S 0 ), and S 1 = S \ S 0 , P(X (S) = x (S) ) = y (T \S 1 ) P(X (S 0 ) = x (S 0 ) | X (T ) = y (T \S 1 ) ∧ x (S 1 ) )P (X (T ∪S 1 ) = y (T \S 1 ) ∧ x (S 1 ) ) = y (pa(S)\S 1 ) s∈S 0 p s ((y ∧ x) (pa(s)) ∧ x (S 1 ) , x (s) )P(X (pa(S 0 )∪S 1 ) = y (pa(S 0 )\S 1 ) ∧ x (S 1 ) ) [(15.6)](#) Since pa(S)∪S 1 has maximal depth strictly smaller than the maximal depth of S, this indeed provides a recursive formula for the computation of marginal over subsets of V with increasing maximal depths. However, because one needs to add parents to the considered set when reducing the depth, one may end up having to compute marginals over very large sets, which becomes intractable without further assumptions.

## Conditional probabilities and interventions

One of the main interests of graphical models is to provide an ability to infer the behavior of hidden variables of interest given other, observed, variables. When dealing with oriented graphs the way this should be analyzed is, however, ambiguous. work interpretation of this graph is that both events (which may be true or false) "Bad weather" and "Broken HVAC" happen first, and that they are independent. Then, given their observation, the "No school" event may occur, probably more likely if the weather is bad or the HVAC is broken or snow, and even more likely if both happened at the same time.

Now consider the following passive observation: you wake up, you haven't checked the weather yet or the news yet, and someone tells you that there is no school today. Then you may infer that there is more chances than usual for bad weather or the HVAC broken at school. Conditionally to this information, these two events become correlated, even if they were initially independent. So, even if the "No school" event is considered as a probabilistic consequence of its parents, observing it influences our knowledge on them. Now, here is an intervention, or manipulation: the school superintendent has declared that he has given enough snow days for the year and declared that there would be school today whatever happens. So you know that the "no-school" event will not happen. Does it change the risk of bad weather of broken HVAC? Obviously not: an intervention on a node does not affect the distribution of the parents.

Manipulation and passive observation are two very different ways of affecting unobserved variables in Bayesian networks. Both of them may be relevant in applications. Of the two, the simplest to analyze is intervention, since it merely consists in clamping one of the variables while letting the rest of the network dynamics unchanged. This leads to the following formal definition of manipulation. Definition 15.30 Let G = (V , E) be a directed acyclic graph and X a Bayesian network on G. Let S be a subset of G and x (S) ∈ F S a given configuration on S. Then the manipulated distribution of X with fixed values x (S) on S is the Bayesian network on the restricted graph G S , with the same conditional probabilities, using the value x (s) every time a vertex

$s ∈ S is a parent of t ∈ V \ S in G.$So, if the distribution of X is given by (15.2), then its distribution after manipulation on S is π(y (V \S) ) =

t∈V \S p t (y (pa(t)) , y (t) )

where pa(t) is the set of parents of t in G, and y (s) = x (s) whenever s ∈ pa(t) ∩ S.

The distribution of a Bayesian network X after passive observation X (S) = x (S) is not so easily described. It is obviously the conditional distribution P (X (V \S) = y (V \S) | X (S) = x (S) ) and therefore requires using the conditional dependency structure, involving the moral graph and/or d-separation.

Let us discuss this first in the simpler case of trees, for which the moral graph is the undirected acyclic graph underlying the tree, and d-separation is simple separation on this acyclic graph. We can then use proposition 13.22 to understand the new structure after conditioning: it is a G ♭ V \S -Markov random field, and, for t ∈ V \ S, the conditional distribution of X (t) = y (t) given its neighbors is the same as before, using the value x (s) when s ∈ S. But note that when doing this (passing to G ♭ ), we broke the causality relation between the variables. We can however always go back to a tree (or forest, since connectedness may have been broken) with the same edge orientation as they initially were, but this requires reconstituting the edge joint probabilities from the new acyclic graph, and therefore using (acyclic) belief propagation.

With general Bayesian networks, we know that the moral graph can be loopy and therefore a source of difficulties. The following proposition states that the damage is circumscribed to the ancestors of S. Proposition 15.31 Let G = (V , E) be a directed acyclic graph, X a Bayesian network on G, S ⊂ V and x (A S ) ∈ F (A S ). Then the conditional distribution of X (A c S ) given by X (A S ) = x (A S ) coincides with the manipulated distribution in definition 15.30.

Proof The conditional distribution is proportional to s∈V p(y (pa(s)) , y (s) ) with y (t) = x (t) if t ∈ A S . Since s ∈ A S implies pa(s) ⊂ A S , all terms with s ∈ A S are constant in the sum and can be factored out after normalization. So the conditional distribution is proportional to s∈A c S p(y (pa(s)) , y (s) ) with y (t) = x (t) if t ∈ A S . But we know that such products sum to 1, so that the conditional distribution is equal to this expression and therefore provides a Bayesian network on G A c S .

■

## Structural equation models

Structural equation models (SEM's) provides an alternative (and essentially equivalent) formulation of Bayesian networks, which may be more convenient to use, especially when dealing with variables taking values in general state spaces.

Let G = (V , E) be a directed acyclic graph. SEMs are associated to families of functions Φ s : F (pa(s)) × B s → F s and random variables ξ s : Ω → B s (where B s is some measurable set), for s ∈ V . The random field X : Ω → F (V ) associated to the SEM satisfies the equations X s = Φ (s) (X (s-) , ξ (s) ). [(15.8)](#) Because of the DAG structure, these equations uniquely define X once ξ is specified. As a consequence, there exists a function Ψ such that X = Ψ (ξ).

The model is therefore fully specified by the functions Φ (s) and the probability distributions of the variables ξ (s) . We will assume that they have a density, denoted g (s) , s ∈ V , with respect to some measure µ s on B s . They are typically chosen as uniform distributions on B s (continuous and compact, or discrete) or as standard Gaussian when B s = R d s for some d s . One also generally assumes that the variables (ξ (s) , s ∈ V ) are jointly independent, and we make this assumption below.

Let V k , k ≥ 0, be the set of vertexes in V with depth k (c.f. definition 15.26) and

$V <k = V 0 ∪ • • •∪ V k-1 . Then (using the independence of (ξ (s) , s ∈ V ), for s ∈ V k , the con- ditional distribution of X (s) given X (V <k ) = x (V <k ) is the distribution of Φ (s) (x (s-) , ξ (s) ).$Formally this is given by

$Φ (s) (x (s-) , •) ♯ (g (s) µ s ),$the pushforward of the distribution of ξ (s) by Φ (s) (x (s-) , •).

More concretely, assume that ξ s follows a uniform distribution on B s = [0, 1] h for some h, and assume that F s is finite for all s. Then,

$P (X (s) = x (s) | X (V <k ) = x (V <k ) ) = Volume(U s (x (pa(s)) , x (s) )) ∆ = p s (x (pa(s)) , x (s) )$where

$U s (x (pa(s)) , x (s) ) = ξ ∈ [0, 1] h : Φ (s) (x (s-) , ξ) = x (s) .$Since variables X (s) , s ∈ V k are conditionally independent given X (V <k) , we find that X decomposes as a Bayesian network over G, P (X = x) = s∈V p s (x (pa(s)) , x (s) ).

Similarly, if

$F s = B s = R d s , ξ (s) ∼ N (0, Id R d s ), and ξ (s) → Φ (s) θ (x (pa(s)) , ξ (s) ) is invertible, with C 1 inverse x (s) → Ψ (s)$θ (x (pa(s)) , x (s) ), then X is a Bayesian network, with continuous variables, and, using the change of variable formula, the conditional distribution of X (s) given X (pa(s)) = x (s-) has p.d.f.

$p s (x (pa(s)) , x (s) ) = 1 (2π) d s /2 exp - 1 2 |x s -Ψ (s) θ (x (pa(s)) , x (s) )| 2 det(∂ x (s) Ψ (s) θ (x (pa(s)) , x (s) )) .$A simple and commonly used special case for this example are linear SEMs, with

$X (s) = a s + b T s X (s) + σ s ξ (s) .$In this case, the inverse mapping is immediate and the Jacobian determinant in the change of variables is 1/σ

$d s s .$Chapter 16

Latent Variables and Variational Methods

## Introduction

We will describe, in the next chapters, methods that fit a parametric model to the observation while introducing unobserved, or "latent," components in their models, whose inference typically attaches interpretable information or structure to the data. We have seen one such example in the form of the mixture of Gaussian in chapter 4, that we will revisit in chapter 19. We now provide a presentation of the variational Bayes paradigm that provides a general strategy to address latent variable problems [[143,](#b161)[97,](#b115)[14,](#b32)[100]](#b118).

The general framework is as follows. Variables in the model are divided in two groups: the observable part, that we denote X, and the latent part, denoted Z. In many models Z represents some unobservable structure, such that X conditional to Z has some relatively simple distribution (in a Bayesian estimation context, Z often contains model parameters). The quantity of interest, however, is the conditional distribution of Z given X (also called the "posterior distribution"), which allows one to infer the latent structure from the observations, and will also have an important role in maximum likelihood parametric estimation, as we will see below. This conditional distribution is not always easy to compute or simulate, and variational Bayes provides a framework under which it can be approximated.

## Variational principle

We consider a pair of random variables X and Z, where X is considered as "observed" and Z is hidden, or "latent". We will use U = (X, Z) to denote the two variables taken together. We denote as usual by P U the probability law of U , defined on R U = R X × R Z by P U (A) = P(U ∈ A). We will also assume that there exists a measure µ on R U that decomposes as a product measure µ = µ X × µ Z (where µ X and µ Z are measures on R X and R Z ), such that P U ≪ µ (π U is absolutely continuous with respect to µ). This implies that P U has a density with respect to µ that we will denote f U . If both R X and R Z are discrete, µ is typically the counting measure, and if they are both Euclidean space, µ can be the Lebesgue measure on the product. 1   The variables X and Z then have probability density functions with respect to µ X ad µ Z , given by

$f X (x) = R Z f U (x, z)µ Z (dz) and f Z (z) = R X f U (x, z)µ X (dx). The conditional distribution of X given Z = z, denoted P X ( • | Z = z), has density f X (x | z) = f U (x, z)/f Z (z) with respect to µ X and that of Z given X = x, denoted P Z ( • | X = x), has density f Z (z | x) = f U (x, z)/f X (x)$with respect to µ Z . We will be mainly interested by approximations of P Z ( • | X = x), assuming that P Z and P X ( • | Z = z) (and hence P U ) are easy to compute or simulate.

We will use the Kullback-Liebler divergence to quantify the accuracy of the approximation. As stated in proposition 4.1, we have

$P Z ( • | X = x) = argmin ν∈M 1 (R Z ) KL(ν ∥ P Z (• | X = x))$where M 1 (R Z ) denotes the set of all probability distributions on R Z . Note that all distributions ν for which KL(ν ∥ π Z (•|X = x)) is finite must be absolutely continuous with respect to µ Z and therefore take the form ν = gµ Z . One has

$KL(gdµ Z ∥P Z (•|X = x)) = R Z log g(z) f Z (z|x) g(z)µ Z (dz) = R Z log g(z) f U (x, z) g(z)µ Z (dz) + log f X (x). (16.1)$We will denote by P (µ Z ), or just P when there is no ambiguity, the set of all p.d.f.'s g with respect to µ Z , i.e., the set of all non-negative measurable functions on R Z with

$R Z g(z)µ Z (dz) = 1.$The basic principle of variational Bayes methods is to replace P by a subset P and to define the approximation 1 The reader unfamiliar with measure theory may want to read this discussion by replacing dµ X by dx, dµ Z by dz and dµ U by dx dz, i.e., in the context of continuous probability distributions having p.d.f.'s with respect to the Lebesgue's measure.

$P Z ( • |X = x) = argmin g∈ P KL(gµ Z ∥P Z ( • |X = x)).$For the approximation to be practical, the set P must obviously be chosen so that the computation of P Z ( • |X = x) is computationally feasible. We now review a few examples, before passing to the EM algorithm and its approximations.

## Examples

## Mode approximation

Assume that R Z is discrete and µ Z is the counting measure so that

$KL(gdµ Z ∥ P Z ( • | X = x)) -log f X (x) = z∈R Z log g(z) f U (x, z) g(z),$the sum being infinite if there exists z such that ν(z) > 0 and f U (x, z) = 0. Take

$P = {1 z : z ∈ R Z } ,$the family of all Dirac functions on R Z . Then,

$KL(1 z ∥ P Z ( • |X = x)) -log f X (x) = -log f U (x, z).$The variational approximation of P Z ( • | X = x) over P therefore is the Dirac measure at point(s) z ∈ R Z at which f U (x, z) is largest, i.e., the mode(s) of the posterior distribution. This approximation is often called the MAP approximation (for maximum a posteriori).

If R Z is, say, R q and µ Z = dz is Lebesgue's measure, then the previous construc-

tion does not work because 1 z is not a p.d.f. with respect to µ Z . In place of Dirac functions, one can use constant functions on small balls. Let B(z, ϵ) denote the open ball with radius ϵ, and let |B(z, ϵ)| denote its volume. Let u z,ϵ = 1 B(z,ϵ) /|B(z, ϵ)|. Fixing ϵ, we can consider the set

$P = u z,ϵ : z ∈ R q .$Now, one has (leaving the computation to the reader)

$KL(u z,ϵ dz ∥ P Z ( • |X = x)) -log f X (x) = -log 1 |B(z, ϵ) B(z,ϵ) f U (x, z ′ )dz ′ .$The limit for small ϵ (assuming that f U (x, •) is continuous at z, or defining the limit up to sets of measure zero) islog f U (x, z), justifying again choosing the mode of the posterior distribution of Z for the approximation.

The mode approximation has some limitations. First, it is in general a very crude approximation of the posterior distribution. Second, even with the assumption that f U has closed form, this p.d.f. is often difficult to maximize (for example when defining models over large discrete sets). In such cases, the mode approximation has limited practical use.

## Gaussian approximation

Let us still assume that R Z = R q and that µ Z = dz. Let P be the family of all Gaussian distributions N (m, Σ) on R q . Then, denoting by ϕ( • ; m, Σ) the density of N (m, Σ),

$KL(ϕ( • ; m, Σ)∥P Z ( • | X = x)) -log f X (x) = - q 2 log 2π - q 2 - 1 2 log det(Σ) - R q log f U (x, z)ϕ(z; m, Σ)dz.$In order to provide the best approximation, m and Σ must therefore maximize

$R q log f U (x, z)ϕ(z; m, Σ)dz + 1 2 log det(Σ). (16.$2)

The resulting optimization problem does not have a closed form solution in general (see section 18.2.2 for an example in which stochastic gradient methods are used to solve this problem). Another approach that is commonly used in practice is to push the approximation further by replacing log f U (x, z) by its second order expansion around its maximum as a function of z. Let m(x) be the posterior mode, i.e., the value of z at which x → log f U (x, z) is maximal, that we will assume to be unique. Let H(x) denote the q × q Hessian matrix formed by the second partial derivatives oflog f U (x, z) (with respect to z) at z = m(x). This matrix is positive semidefinite according to the choice made for m(x), and we will assume that it is positive definite. Since the first derivatives of log f U (x, z) at m(x) must vanish, we have the expansion:

$log f U (x, z) = log f U (x, m(x)) - 1 2 (z -m(x)) T H(x)(z -m(x)) + • • •$Plugging the expansion into the integral in (16.2) yields

$- 1 2 trace(H(x)Σ) - 1 2 (m -m(x)) T H(x)(m -m(x)) + 1 2 log det Σ.$To maximize this expression, one must clearly take m = m(x). Moreover,

$∂ Σ (-trace(H(x)Σ) + log det Σ) = -H(x) T + (Σ T ) -1 = -H(x) + Σ -1 ,$and we see that one must take Σ = H(x) -1 . This provides the Laplace approximation [[62]](#b80) of the posterior, N (m(x), H(x) -1 ), which is practical when the mode and corresponding second derivatives are feasible to compute.

## Mean-field approximation

This section generalizes the approach discussed in proposition 14.6 for Markov random fields. Assume that R Z can be decomposed into several components R

$[1] Z , . . . , R [K] Z ,$
## EXAMPLES

writing z = (z [1] , . . . , z [K] ) (for example, taking K = q and z [i] = z (i) , the ith coordinate of z if R Z = R q ). Also assume that µ Z splits into a product measure µ

$[1] Z ⊗ • • • ⊗ µ [K]$Z . Mean-field approximation consists in assuming that probabilities ν in P split into independent components, i.e., their densities g take the form: g(z) = g [1] (z [1] ) • • • g [K] (z [K] ).

Then,

$KL(ν ∥ P Z ( • | X = x)) -log f X (x) = K j=1 R [j] Z$log g [j] (z [j] )g [j] (z [j] )µ

[j] Z (dz [j] )

$- R Z log f U (x, z) q j=1$g [j] (z [j] )µ Z (dz). [(16.3)](#) The mean-field approximation may be feasible when log f U (x, z) can be written as a sum of products of functions of each z [j] . Indeed, assume that

$log f U (x, z) = α∈A K j=1 ψ α,j (z [j] , x) (16.4)$where A is a finite set. To shorten notation, let us denote by ⟨ψ⟩ the expectation of a function ψ with respect to the product p.d.f. g. Then, (16.3) can be written as

$KL(ν∥P Z ( • | X = x)) -log f X (x) = K j=1 ⟨log g (j) (z [j] )⟩ - α∈A K j=1$⟨ψ α,j (z [j] , x)⟩.

The following lemma will allow us to identify the form taken by the optimal p.d.f. g [j] . Lemma 16.1 Let Q be a set equipped with a positive measure µ. Let ψ : Q → R be a measurable function such that

$C ψ ∆ = Q exp(ψ(q))µ(dq) < ∞. Let g ψ (q) = 1 C ψ exp(ψ(q)).$Let g be any p.d.f. with respect to µ, and define

$F(g) = Q (log g(q) -ψ(q))g(q)µ(dq).$Then F(g ψ ) ≤ F(g).

Proof We note that g ψ > 0, and that

$KL(g∥g ψ ) = F(g) + log C ψ = F(g) -F(g ψ ),$which proves the result, since KL divergences are always non-negative.

## ■

Applying this lemma separately to each function g [j] implies that any optimal g must be such that g [j] (z [j] ) ∝ exp

$       α∈A M α,j ψ α,j (z [j] , x)        with M α,j = K j ′ =1,j ′ j ⟨ψ α,j ′ (z [j ′ ] , x)⟩.$We therefore have

$⟨ψ α,j (z [j] , x)⟩ = R [j] Z ψ α,j (z [j] , x) exp α ′ ∈A M α ′ ,j ψ α ′ ,j (z [j] , x) µ [j] Z (dz [j] ) R [j] Z exp α ′ ∈A M α ′ ,j ψ α ′ ,j (z [j] , x) µ [j]$Z (dz [j] ) [(16.5)](#) This specifies a relationship expressing ⟨ψ α,j (z [j] , x)⟩ as a function of the other expectations ⟨ψ α ′ ,j ′ (z (j ′ ) , x)⟩ for j j ′ . These equations put together are called the mean-field consistency equations. When these equations can be written explicitly, i.e., when the integrals in (16.5) can be evaluated analytically (which is generally the case when the p.d.f.'s g [j] can be associated with standard distributions), one obtains an algorithm that iterates (16.5) over all α and j until stabilization (each step reducing the objective function in [(16.3)](#)).

Let us retrieve the result obtained in proposition 14.6 using the current formalism. Assume that R X finite and R Z = {0, 1} L , where L can be a large number, with

$f U (x, z) = 1 C exp         L j=1 α j (x)z (j) + L i,j=1,i<j β ij (x)z (i) z (j)        $.

Take K = L, z [j] = z (j) . Applying the previous discussion, we see that g [j] must take the form g [j] (z (j) ) = exp α j (x)z (j) + i j β ij (x)⟨z (i) ⟩z (j)

1 + exp α j (x) + i j β ij (x)⟨z (i) ⟩

In particular

$⟨z (j) ⟩ = exp α j (x) + i j β ij (x)⟨z (i) ⟩ 1 + exp α j (x) + i j β ij (x)⟨z (i) ⟩$providing the mean-field consistency equations.

In this special case, it is also possible to express the objective function as a simple function of the expectations ⟨z (j) ⟩'s. We indeed have, letting ρ j = ⟨z (j) ⟩,

$z∈R Z log f U (x, z) L j=1 g [j] (z (j) ) = -log C + L j=1 α j (x)ρ j + L i,j=1,i<j β ij (x)ρ i ρ j .$The values of ρ 1 , . . . , ρ L are then obtained by maximizing

$L j=1 α j (x)ρ j + L i,j=1,i<j β ij (x)ρ i ρ j - L j=1 ρ j log ρ j + (1 -ρ j ) log(1 -ρ j ) .$The consistency equations express the fact that the derivatives of this expression with respect to each ρ j vanish.

## Maximum likelihood estimation 16.4.1 The EM algorithm

We now consider maximum likelihood estimation with latent variables and use the notation of section 16.2. The main tool is the following obvious consequence of (16.1).

## Proposition 16.2 One has

$log f X (x) = max g∈P (µ Z ) R Z log f U (x, z) g(z) g(z)dµ Z (z)$and the maximum is achieved for g

$(z) = f Z (z | x), the conditional p.d.f. of Z given X = x.$Proof Equation (16.1) implies that

$R Z log f U (x, z) g(z) g(z)dµ Z (z) = log f X (x) -KL(g µ Z ∥P Z ( • |X = x))$and the r.h.s. is indeed maximum when the Kullback-Liebler divergence vanishes, that is, when g is the p.d.f. of P Z ( • | X = x).

## ■

We will use this proposition for the derivation of the expectation-maximization (or EM) algorithm for maximum likelihood with latent variables. We now assume that P U , and therefore f U , is parametrized by θ ∈ Θ, and that a training set T = (x 1 , . . . , x N ) of X is observed. To indicate the dependence in θ, we will write f U (x, z ; θ), or f Z (z | x ; θ). The maximum likelihood estimator (m.l.e.) then maximizes

$ℓ(θ) = x∈T log f X (x ; θ) .$The EM algorithm is useful when the computation of the m.l.e. for complete observations, i.e., the maximization of log f U (x, z ; θ) when both x and z are given, is easy, whereas the same problem with the marginal distribution is hard.

From the proposition, we have:

$x∈T log f X (x ; θ) = x∈T max g x ∈P (µ Z ) R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz)$Therefore the maximum likelihood requires to compute max

$θ,g x ,x∈T x∈T R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz). (16.6)$The maximization can therefore be done by iterating the following two steps.

1. Given θ n , compute argmax

$g x ,x∈T x∈T R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz). 2. Given g 1 , . . . , g N , compute argmax θ x∈T R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz) = argmax θ x∈T R Z log (f U (x, z ; θ)) g x (z)µ Z (dz).$Step 1. is explicit and its solution is g x (z) = f Z (z | x ; θ). Using this, both steps can be grouped together, yielding the EM algorithm.

## Algorithm 16.1 (EM algorithm)

Let a statistical model with density f U (x, z ; θ) modeling an observable variable X and a latent variable Z be given, and a training set T = (x 1 , . . . , x N ). Starting with an initial guess of the parameter, θ(0), the EM algorithm iterate the following equation until numerical stabilization, .

$θ n+1 = argmax θ ′ x∈T R Z log (f U (x, z ; θ ′ )) f Z (z | x ; θ n )µ Z (dz). (16$.7) Equation (16.7) maximizes (in θ ′ ) a function defined as an expectation (for θ n ), justifying the name "Expectation-Maximization." 16.4.2 Application: Mixtures of Gaussian A mixture of Gaussian (MoG) model was introduced in chapter 4 ((4.4)). We now reinterpret it (in a slightly generalized version) as a model with partial observations and show how the EM algorithm can be applied. Let ϕ(x ; m, Σ) denote the p.d.f. of the d-dimensional multivariate Gaussian distribution with mean m and covariance matrix Σ. We model f X (x ; θ) as

$f X (x ; θ) = p j=1 α j ϕ(x, ; c j , Σ j ).$Here, θ contains all sequences α 1 , . . . , α p (non-negative numbers that sum to one), c 1 , . . . , c p ∈ R d and Σ 1 , . . . , Σ p (d × d positive definite matrices).

Using the previous notation, we therefore have R X = R d , and µ X the Lebesgue measure on that space. The variable Z will take values in R Z = {1, . . . , p}, with µ Z being the counting measure. We model the joint density function for (X, Z) as

$f U (x, z ; θ) = α z ϕ(x; c z , Σ z ). (16.8)$Clearly f X is the marginal p.d.f. of f U . One can therefore consider Z as a latent variable, and therefore estimate θ using the EM algorithm.

We now make (16.7) explicit for mixtures of Gaussian. For given θ and θ ′ and x ∈ R, let

$U x (θ, θ ′ ) = d 2 log 2π + R Z log (f U (x, z ; θ ′ )) f Z (z|x ; θ)dµ Z (z) = p z=1 log α ′ z - 1 2 log det Σ ′ z - 1 2 (x -c ′ z ) T Σ ′ z -1 (x -c ′ z ) f Z (z|x ; θ) with f Z (z | x ; θ) = (det Σ z ) -1 2 α z e -1 2 (x-c z ) T Σ -1 z (x-c z ) p j=1 (det Σ j ) -1 2 α j e -1 2 (x-c j ) T Σ -1 j (x-c j )$.

If θ n is the current parameter in the EM, the next one, θ n+1 must maximize

$N x∈T U x (θ n , θ ′$). This can be solved in closed form. To compute α ′ 1 , . . . , α ′ p , one must maximize

$x∈T p z=1 (log α ′ z )f Z (z|x ; θ)$subject to the constraint that z α ′ z = 1. This yields

$α ′ z = x∈T f Z (z|x ; θ) p j=1 x∈T f Z (j|x ; θ) = ζ z / N with ζ z = x∈T f Z (z|x ; θ). The centers c ′ 1 , . . . , c ′ p must minimize x∈T (x -c ′ z ) T Σ ′ z -1 (x -c ′ z )f Z (z|x ; θ), which yields c ′ z = 1 ζ z x∈T xf Z (z|x ; θ). Finally, Σ ′ z must minimize ζ z 2 log det Σ ′ z + 1 2 x∈T (x -c ′ z ) T Σ ′ z -1 (x -c ′ z )f Z (z|x ; θ),$which yields

$Σ ′ z = 1 ζ z x∈T (x -c ′ z )(x -c ′ z ) T f Z (z|x ; θ).$We can now summarize the algorithm.

Algorithm 16.2 (EM for Mixture of Gaussian distributions) 1. Initialize the parameter θ(0) = (α(0), c(0), Σ(0)). Choose a small constant ϵ and a maximal number of iterations M.

2. At step n of the algorithm, let θ = θ(n) be the current parameter, writing for short θ = (α, c, Σ).

## Compute, for x ∈ T and i

$= 1, . . . , p f Z (i | x ; θ) = (det Σ i ) -1 2 α i e -1 2 (x-c i ) T Σ -1 i (x-c i ) p j=1 (det Σ j ) -1 2 α j e -1 2 (x-c j ) T Σ -1 j (x-c j )$and let ζ i = x∈T f Z (i|x ; θ), i = 1, . . . , p.

## Let α

$′ i = ζ i /N . 5. For i = 1, . . . , p, let c ′ i = 1 ζ i x∈T xf Z (i | x ; θ).$6. For i = 1, . . . , p, let

$Σ ′ i = 1 ζ i x∈T (x -c ′ i )(x -c ′ i ) T f Z (i | x ; θ). 7. Let θ ′ = (µ ′ , c ′ , Σ ′ ). If |θ ′ -θ| < ϵ or n + 1 = M: return θ ′ and$exit the algorithm. 8. Set θ(n + 1) = θ ′ and return to step 2. Remark 16.3 Algorithm 16.2 can be simplified by making restrictions on the model. Here are some examples.

(i) One may restrict to Σ i = σ 2 i Id R d to reduce the number of free parameters. Then, Step 7 of the algorithm needs to be replaced by:

$(σ ′ i ) 2 = 1 dζ i x∈T |x -c ′ i | 2 f Z (i | x ; θ).$(ii) Alternatively, the model may be simplified by assuming that all covariance matrices coincide: Σ i = Σ for i = 1, . . . , p. Then, Step 7 becomes

$Σ ′ i = 1 N p i=1 x∈T (x -c ′ i )(x -c ′ i ) T f Z (i | x ; θ). ♦ (iii)$Finally, one may assume that Σ is known and fixed in the algorithm (usually in the form Σ = σ 2 Id R d for some σ > 0) so that Step 7 of the algorithm can be removed.

(iv) One may also assume also that the (prior) class probabilities are known, typically set to α i = 1/p for all i, so that Step 4 can be skipped.

## Stochastic approximation EM

The stochastic approximation EM (or SAEM) algorithm has been proposed by Delyon et al. [[58]](#b76) (see this reference for convergence results) to address the situation in which the expectations for the posterior distribution cannot be computed in closed form, but can be estimated using Monte-Carlo simulations. SAEM uses a special form of stochastic approximation, different from the SGD algorithm described in section 3.3. It updates, at each step n, an approximate objective function that we will denote λ n and a current parameter θ(n). It implements the following iterations:

$                   ξ (x) n+1 ∼ P Z ( • | X = x ; θ n ), x ∈ T λ n+1 (θ ′ ) = 1 - 1 n + 1 λ n (θ ′ ) + 1 n + 1 x∈T log f U (x, ξ (x) n+1 ; θ ′ ) -λ n (θ ′ ) , θ ′ ∈ Θ θ n+1 = argmax θ ′ λ n+1 (θ ′ ) (16.9)$The second step means that

$λ n (θ ′ ) = N x∈T         1 n n j=1 log f U (x, ξ (x) j ; θ ′ )         . Given that ξ (x) n+1 ∼ P Z ( • | X = x ; θ n ), one expects this expression to approximate x∈T R Z log (f U (x, z ; θ ′ )) f Z (z | x ; θ)dµ Z (z)$so that the third step of (16.9) can be seen as an approximation of (16.7). Sufficient conditions under which this actually happens (and θ(n) converges to a local maximizer of the likelihood) are provided in Delyon et al. [[58]](#b76) (see also [Kuhn and Lavielle [112]](#) for a convergence result under more general hypotheses on how ξ is simulated).

To be able to run this algorithm efficiently, one needs the simulation of the posterior distribution to be feasible. Importantly, one also needs to be able to update efficiently the function λ n . This can be achieved when the considered model belongs to an exponential family, which corresponds to assuming that the p.d.f. of U takes the form

$f U (x, z ; θ) = 1 C(θ) exp ψ(θ) T H(x, z)$for some functions ψ and H. For example, the MoG model of equation (4.4) takes this form, with

$ψ(θ) T = log α 1 - 1 2 m T 1 Σ -1 1 m 1 - 1 2 log det Σ 1 , . . . , log α p - 1 2 m T p Σ -1 p m p - 1 2 log det Σ p , Σ -1 1 m 1 , . . . , Σ -1 p m p , Σ -1 1 , . . . , Σ -1 p , H(x, z) T = 1 z=1 , . . . , 1 z=p , x1 z=1 , . . . , x1 z=p , - 1 2 xx T 1 z=1 , . . . , - 1 2 xx T 1 z=p and C(θ) = (2π) pd/2 .$For such a model, we can replace the algorithm in (16.9) by the more manageable one:

$                             ξ (x) n+1 ∼ P Z ( • | X = x ; θ n ), x ∈ T η (x) n+1 = 1 - 1 n + 1 η (x) n + 1 n + 1 (H(x, ξ (x) n+1 ) -η (x) n ) λ n+1 (θ ′ ) = ψ(θ ′ ) T x∈T η (x) n+1 -log C(θ ′ ) θ n+1 = argmax θ ′ λ n+1 (θ ′ ) (16.10)$We leave as an exercise the computation leading to the implementation of this algorithm for mixtures of Gaussian.

## Variational approximation

Returning to proposition 16.2 and (16.6), we see that one can make a variational approximation of the maximum likelihood by computing max [(16.11)](#) where P ⊂ P is a class of p.d.f. with respect to µ Z . The resulting algorithm is then implemented by iterating the computation of g x , x ∈ T , using approximations similar to those provided in section 16.3, and maximization in θ for given g x , x ∈ T . This variational approximation of the maximum likelihood estimator is therefore provided by the following algorithm.

$θ∈Θ,g x ∈ P ,x∈T x∈T R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz),$Algorithm 16.3 (Variational Bayes approximation of the m.l.e.) Let a statistical model with density f U (x, z ; θ) modeling an observable variable X and a latent variable Z be given, and a training set T = (x 1 , . . . , x N ) be observed. Let P be a set of p.d.f. on R Z and define

$g( • ; x, θ) = argmin g∈ P R Z log g(z) f U (x, z ; θ) g(z)µ Z (dz)$(assuming that this minimizer is uniquely defined).

Starting with an initial guess of the parameter, θ 0 , iterate the following equation until numerical stabilization:

$θ(n + 1) = argmax θ ′ x∈T R Z log (f U (x, z ; θ ′ )) g(z|x ; θ(n))µ Z (dz). (16.12)$Assume that the distributions in P are also parametrized, denoting their parameter by η, belonging to some Euclidean domain H. Let g(•; η) denote the p.d.f. in P with parameter η. Letting η = (η x , x ∈ T ) denote an element of H T (parameters in H indexed by elements of the training set), [(16.11)](#) can then be written as the maximization of [.13)](#) This expression is amenable to a stochastic gradient ascent implementation. We have

$F(θ, η) = x∈T R Z log f U (x, z ; θ) g(z; η x ) g(z; η x )µ Z (dz). (16$$∂ θ R Z log f U (x, z ; θ) g(z; η x ) g(z; η x )µ Z (dz) = R Z ∂ θ log f U (x, z ; θ)g(z; η x )µ Z (dz)$and

$∂ η x R Z log f U (x, z ; θ) g(z; η x ) g(z; η x )µ Z (dz) = R Z -∂ η log g(z; η x )g(z; η x ) + log f U (x, z ; θ) g(z; η x ) ∂ η g(z; η x ) µ Z (dz) = R Z log f U (x, z ; θ) g(z; η x ) ∂ η log g(z; η x )g(z; η x )µ Z (dz)$Here, we have used the fact that, for all η,

$R Z ∂ η log g(z; η)g(z; η) µ Z (dz) = R Z ∂ η g(z; η)µ Z (dz) = 0 16.5. REMARKS since R Z g(x, η)µ Z (dz) = 1.$Denote by π η the probability distribution of the random variable Z taking val-

$ues in R |T |$Z obtained by sampling Z = (Z x , x ∈ T ) such that the components Z x are independent and with p.d.f. g(•; η x ) with respect to µ Z . Define

$Φ 1 (θ, z) = x∈T ∂ θ log f U (x, z x ; θ) and Φ 2 (theta, η, z) = x∈T log f U (x, z ; θ) g(z; η x ) ∂ η log g(z x ; η x ).$Then, following section 3.3, one can maximize (16.13) using the algorithm

$θ n+1 = θ n + γ n+1 Φ 1 (θ n , Z n+1 ) η n+1 = η n + γ n+1 Φ 2 (θ n , η n , Z n+1 ) (16.14)$where Z n+1 ∼ π η n .

Alternatively (for example when T is large), one can also sample from x ∈ T at each update. This would require defining π η as the distribution on T ×R Z with p.d.f. ϕ η (x, z) = g(z; η x )/N , where N = |T |. One can now use

$Φ 1 (θ, x, z) = ∂ θ log f U (x, z ; θ) and Φ 2 (θ, η, z) = log f U (x, z ; θ) g(z; η) ∂ η log g(z; η), one can use            θ n+1 = θ n + γ n+1 ∂ θ log f U (X n+1 , Z n+1 ; θ n ) η n+1,X n+1 = η n,X n+1 + γ n+1 log f U (X n+1 , Z n+1 ; θ n ) g(Z n+1 ; η n,X n+1 ) ∂ η log g(Z n+1 ; η n,X n+1 ) (16.15)$with (X n+1 , Z n+1 ) ∼ π η n . Sampling from a single training sample at each step can be replaced by sampling from a minibatch with obvious modifications.

## Remarks

## Variations on the EM

Based on the formulation of the EM as the solution of (16.6), it should be clear that solving [(16.7)](#) at each step can be replaced by any update of the parameter that increases [(16.6)](#). For example, (16.7) can be replaced by a partial run of a gradient ascent algorithm, stopped before convergence. One can also use a coordinate ascent strategy. Assume that θ can be split into several components, say two, so that θ = (θ (1) , θ (2) ). Then, (16.7) may then be split into

$θ (1) n+1 = argmax$θ (1)  x∈T R Z log f U (x, z ; θ (1) , θ

(2)

$n ) f Z (z | x ; θ(n))µ Z (dz) θ (2) n+1 = argmax θ (2) x∈T R Z log f U (x, z ; θ (1)$n+1 , θ (2) )

$f Z (z | x ; θ(n))µ Z (dz).$Doing so is, in particular, useful when both these steps are explicit, but not (16.7).

## Direct minimization

While the EM algorithm is widely used in the context of partial observations, it is also possible to make explicit the derivative of

$log f X (x ; θ) = log R Z f U (x, z ; θ)µ Z (dz)$with respect to the parameter θ. Indeed, differentiating the integral and writing

$∂ θ f U = f U ∂ θ log f U , we have ∂ θ log f X (x ; θ) = R Z ∂ θ log f U (x, z ; θ) f U (x, z ; θ) f X (x ; θ) µ Z (dz) = R Z ∂ θ log f U (x, z ; θ)f Z (z | x, θ)µ Z (dz).$In other terms, the derivative of the log-likelihood of the observed data is the conditional expectation of the derivative of the log-likelihood of the full data given the observed data. When computable, this expression can be used with standard gradient-based optimization methods, such as those described in chapter 3. This expression is also amenable to a stochastic gradient ascent algorithm, namely

$θ n+1 = θ n + γ n+1 x∈T ∂ θ f U (x, Z n+1,x , θ n ) (16.16)$where Z n+1,x follows the distribution with density f Z (• | x, θ n ) with respect to µ Z . An alternative SGA implementation can use the discussion in section 16.4.4, with the density g η x replaces by f Z (• | x, η x ), which leads to

$         θ n+1 = θ n + γ n+1 x∈T ∂ θ log f U (x, Z n+1,x , θ n ) η n+1,x = η n,x -γ n+1 ∂ η x log f Z (Z n+1,x | x, η x ), x ∈ T$where Z n+1,x follows the distribution with density f Z (• | x, η n,x ).

## Product measure assumption

We have worked, in this chapter, under the assumption that π U was absolutely continuous with respect to a product measure µ U = µ X ⊗ µ Z . This is not a mild assumption, as it fails to include some important cases, for example when X and Z have some deterministic relationship, the simplest instance being when X = F(Z) for some function F. In many cases, however, one can make simple transformations on the model that will make it satisfy this working assumption. For example, if X = F(Z), one can generally split Z into Z = (Z (1) , Z (2) ) so that the equation X = F(Z) is equivalent to Z (2) = G(X, Z (1) ) for some function G. One can then apply the discussion above to U = (X, Z (1) ) instead of U = (X, Z).

If one is ready to step further into measure theoretic concepts, however, one can see that this product decomposition assumption was in fact unnecessary. Indeed, one can assume that the measure µ U can "disintegrate" in the following sense: there exists, a measure µ X on R X and, for all x ∈ R X , a measure µ Z ( • |x) on R Z such that, for all functions ψ defined on R U ,

$R U ψ(x, z)µ U (dx, dz) = R X R Z ψ(x, z)µ Z (dz|x)µ X (dx).$This is in fact a fairly general situation [[34]](#b52) as soon as one assumes that µ U (R) is finite (which is not a real loss of generality as one can reduce to this case by replacing if needed µ U by an equivalent probability distribution).

With this assumption, the marginal distribution of X had a p.d.f. with respect to µ X given by

$f X (x) = R Z f U (x, z)µ Z (dz|x)$and the conditional distributions

$P Z ( • | x) have a p.d.f. relative to µ Z ( • | x) given by f Z (z | x) = f U (x, z) f X (x) .$The computations and approximations made earlier in this chapter can then be applied with essentially no modification.

Chapter 17

## Learning Graphical Models

We discuss, in this chapter, several methods designed to learn parameters of graphical models, starting with the somewhat simpler case of Bayesian networks, than passing to Markov random fields on loopy graphs.

## Learning Bayesian networks 17.1.1 Learning a Single Probability

Since Bayesian networks are specified by probabilities and conditional probabilities of configurations of variables, we start with a discussion of the basic problem of estimating discrete probability distributions.

The obvious way to estimate the probability of an event A based on a series of N independent experiments is by using relative frequencies

$f A = #{A occurs} N .$This estimation is unbiased (E(f A ) = P(A)) and its variance is P(A)(1 -P(A))/N . This implies that the relative error δ A = f A /P(A) -1 has zero mean and variance

$σ 2 = 1 -P(A) N P(A)$.

This number can clearly become very large when P(A) ≃ 0. In particular, when P(A) is small compared to 1/N , the relative frequency will often be f A = 0, leading to the false conclusion that A is not just rare, but impossible. If there are reasons to expect beforehand that A is indeed possible, it is important to inject this prior belief in the procedure, which suggest using Bayesian estimation methods.

The main assumption for these methods is to consider the unknown probability, p = P(A), as a random variable, yielding a generative process in which a random probability is first obtained, and then N instances of A or not-A are generated using this probability.

Assume that the "prior distribution" of p (which determines a prior belief) has a p.d.f. q (with respect to Lebesgue's measure) on the unit interval. Given on N independent observations of occurrences of A, each following a Bernoulli distribution b(p), the joint likelihood of all involved variables is given by

$N k p k (1 -p) N -k q(p),$where k is the number of times the event A has been observed.

The conditional density of p given the observation (k occurrences of A) is called the posterior distribution. Here, it is given by

$q(p | k) = q(p) C k p k (1 -p) N -k$where C k is a normalizing constant. If there was no specific prior knowledge on p (so that q(p) = 1), the resulting distribution is a beta distribution with parameters k + 1 and Nk + 1, the beta distribution being defined as follows. 

$ρ(t) = Γ (a + b) Γ (a)Γ (b) t a-1 (1 -t) b-1 if t ∈ [0, 1]$and ρ(t) = 0 otherwise, with

$Γ (x) = ∞ 0 t x-1 e -t dt.$From the definition of a beta distribution, it is clear also that, if we choose the prior to be β(a + 1, νa + 1) then the posterior is

$β(k + a + 1, N + ν -(k + a) + 1).$The posterior therefore belongs to the same family of distributions as the prior, and one says that the beta distribution is a conjugate prior for the binomial distribution. The mode of the posterior distribution (which is the maximum a posteriori (MAP) estimator) is given by p = k + a N + ν .

This estimator now provides a positive value even if k = 0. By selecting a and ν, one therefore includes the prior belief that p is positive.

## Learning a Finite Probability Distribution

Now assume that F is a finite space and that we want to estimate a probability distribution p = (p(x), x ∈ F) using a Bayesian approach as above. We cannot use the previous approach to estimate each p(x) separately, since these probabilities are linked by the fact that they sum to 1. We can however come up with a good (conjugate) prior, identified, as done above, by computing the posterior associated to a uniform prior distribution.

Letting N x be the number of times x ∈ F is observed among N independent samples of a random variable X with distribution P X (•) = p(•), the joint distribution of (N x , x ∈ F) is multinomial, given by

$P(N x , x ∈ F | p(•)) = N ! x∈F N x ! x∈F p(x) N x .$The posterior distribution of p(•) given the observations with a uniform prior is proportional to x∈F p(x) N x . It belongs to the family of Dirichlet distributions, described in the following definition. Definition 17.2 Let F be a finite set and S F be the simplex defined by

$S F =        (p(x), x ∈ F) : p(x) ≥ 0, x ∈ F and x∈F p(x) = 1        .$The Dirichlet distribution with parameters a = (a(x), x ∈ F) (abbreviated Dir(a)) has density

$ρ(p(•)) = Γ (ν) x∈F Γ (a(x)) x∈F p(x) a(x)-1 , if x ∈ S F$and 0 otherwise, with ν = x∈F a(x).

Note that, if F has cardinality 2, the Dirichlet distribution coincides with the beta distribution. Similarly to the beta for the binomial, and almost by construction, the Dirichlet distribution is a conjugate prior for the multinomial. More precisely, if the prior distribution for p(•) is Dir(1+a(x), x ∈ F), then the posterior after N observations of X is Dir(1 + N x + a(x), x ∈ F), and the MAP estimator is given by

$p(x) = N x + a(x) N + ν with ν = x∈F a(x).$
## Conjugate Prior for Bayesian Networks

We now consider a Bayesian network on the set F (V ) containing configurations x = (x (s) , s ∈ V ) with x (s) ∈ F s . We want to estimate the conditional probabilities in the representation P(X = x) = s∈V p s (x (pa(s)) , x (s) ).

Assume that N independent observations of X have been made. Define the counts N s (x (s) , x (pa(s)) ) to be the number of times the observation x ({s}∪pa(s)) has been made. Then, it is straightforward to see that, assuming a uniform prior for the p s , their posterior distribution is proportional to

$s∈V x (pa(s)) ∈F pa(s) x (s) ∈F s p s (x (pa(s)) , x (s) ) N s (x (s) ,x (s -) ) .$This implies that, for the posterior distribution, the conditional probabilities p s (x (pa(s)) , •) are independent and follow a Dirichlet distribution with parameters 1 + N s (x (s) , x (pa(s)) ), x (s) ∈ F s . So, independent Dirichlet distributions indexed by configurations of parents of nodes provide a conjugate prior for the general Bayesian network model. This prior is specified by a family of positive numbers a s (x (s) , x (pa(s)) ), s ∈ V , x (s) ∈ F s , x (pa(s)) ∈ F (pa(s)) , [(17.1)](#) yielding a prior probability proportional to s∈V x (pa(s)) ∈F pa(s) x (s) ∈F s p s (x (pa(s)) , x (s) ) a s (x (s) ,x (s -) )-1 .

and a MAP estimator ps (x (pa(s)) , x (s) ) = N s (x (s) , x (pa(s)) ) + a s (x (s) , x (s -) )

$N s (x (s -) ) + ν s (x (s -) ) (17.2)$where N s (x (pa(s)) ) = x (s) ∈F s N s (x (s) , x (pa(s)) ) and ν s (x (pa(s)) ) = x (s) ∈F s a s (x (s) , x (pa(s)) ).

One can restrict the huge class of coefficients described by (17.1) to a smaller class by imposing the following condition. Definition 17.3 One says that the family of coefficients a = (a s (x (s) , x (pa(s)) ), s ∈ V , x (s) ∈ F s , x (pa(s)) ∈ F (pa(s))), is consistent if there exists a positive scalar ν and a probability distribution P ′ on F (V ) such that a s (x (s) , x (pa(s)) ) = νP ′ {s}∪pa(s) (x ({s}∪pa(s)) ).

The class of products of Dirichlet distributions with consistent families of coefficients still provides a conjugate prior for Bayesian networks (the proof being left to the reader). Within this class, the simplest choice (and most natural in the absence of additional information) is to assume that P ′ is uniform, so that

$a s (x (s) , x (pa(s)) ) = ν ′ |F ({s} ∪ pa(s))| . (17.3)$With this choice, ν ′ is the only parameter that needs to be specified. It is often called the equivalent sample size for the prior distribution.

We can see from (17.2) that using a prior distribution is quite important for Bayesian networks, since, when the number of parents increases, some configurations on F (pa(s)) may not be observed, resulting in an undetermined value for the ratio N s (x (s) , x (pa(s)) )/N s (x (s -) ), even though, for the estimated model, the probability of observing x (pa(s)) may not be zero.

## Structure Scoring

Given a prior defined as a family of Dirichlet distributions associated to a = (a s (x (s) , x (pa(s)) ) for s ∈ V , x (s) ∈ F s , x (pa(s)) ∈ F (pa(s)), the joint density of the observations and parameters is given by

$P (x, θ) = s,x (pa(s))$D(a s (•, x (pa(s)) ))

s,x (s) ,x (pa(s))

p(x (pa(s)) , x (s) ) N s (x (s) ,x (pa(s)) )+a s (x (s) ,x (pa(s)) )-1

with

$D(a(λ), λ ∈ F) = Γ (ν)$λ Γ (a(λ)) and ν = λ a(λ). Here, θ represents the parameters of the model, i.e., the conditional distributions that specify the Bayesian network. Note that P (x, θ) is a density over the product space F (V ) × Θ where Θ is the space of all these conditional distributions. The marginal of this likelihood over all possible parameters, i.e., P (x) = P (x, θ)dθ provides the expected likelihood of the sample relative to the distribution of the parameters, and only depends on the structure of the network. In our case, integrating with respect to θ yields

$log P (x) = s,x pa(s) log D(a s (•, x (pa(s)) )) D(a s (•, x (pa(s)) ) + N s (•, x (pa(s)) ))$.

## Letting γ(s, pa(s)) =

x (pa(s)) log D(a s (•, x (pa(s)) )) D(a s (•, x (pa(s)) ) + N s (•, x (pa(s)) ))

,

$the decomposition log P (x) = s∈V γ(s, pa(s))$expresses this likelihood as a sum of "scores" (associated to each node and its parents), which depends on the observed sample. The scores that are computed above are often called Bayesian scores because they derive from a Bayesian construction. One can also consider simpler scores, such as penalized likelihood:

$γ(s, pa(s)) = - x (pa(s)) Ĥ(X (s) | X (pa(s)) ) |F (pa(s))| -ρ|pa(s)|,$where Ĥ is the conditional entropy for the empirical distribution based on observed samples. Structure learning algorithms [[144,](#b162)[108]](#b126) are designed to optimize such scores.

## Reducing the Parametric Dimension

In the previous section, we estimated all conditional probabilities intervening in the network. This is obviously a lot of parameters and, even with a regularizing prior, the estimated values are likely to be be inaccurate for small sample sizes. It then becomes desirable to simplify the parametric complexity of the model.

When the sets F s are not too large, which is common in practice, the parametric explosion is due to the multiplicity of parents, since the number of conditional probabilities p s (x (pa(s)) , •) grows exponentially with |pa(s)|. One way to simplify this is to assume that the conditional probability at s only depends on x (pa(s)) via some "global-effect" statistic g s (x (pa(s)) ). The idea, of course, is that the number of values taken by g s should remain small, even if the number of parents is large.

Examples of some functions g s can be max(x (t) , t ∈ pa(s)), or the min, or some simple (quantized) function of the sum. With binary variables (F s = {0, 1}), logical operators are also available ("and", "or", "xor"), as well as combinations of them. The choice made for the functions g s is part of building the model, and would rely on the specific context and prior information on the process, which is always important to account for, in any statistical problem.

Once the g s 's are fixed, learning the network distribution, which is now given by π(x) = s∈V p s (g s (x (pa(s)) ), x (s) ) can be done exactly as before, the parameters being all p s (w, λ), λ ∈ F s , w ∈ W s , where W s is the range of g s , and Dirichlet priors can be associated to each p s (w, •) for s ∈ V and w ∈ W s . The counts provided in [(17.](#)3) now can be chosen as

$a s (x s , w) = ν ′ |F| |g -1 s (w)| . (17.4)$
## Learning Loopy Markov Random Fields

Like everything else, parameter estimation for loopy networks is much harder than with trees or Bayesian networks. There is usually no closed form expression for the estimators, and their computation relies on more or less tractable numerical procedures.

## Maximum Likelihood with Exponential Models

In this section, we consider a parametrized model for a Gibbs distribution

$π θ (x) = 1 Z θ exp(-θ T U (x)) (17.5)$where θ is a d-dimensional parameter and U is a function from F (V ) to R d . For example, if π is an Ising model with

$π(x) = 1 Z exp α s∈V x (s) + β s∼t x (s) x (t) ,$then θ = (α, β) and U (x) = -( s x (s) , s∼t x (s) x (t) ). Most of the Markov random fields models that are used in practice can be put in this form. The constant Z θ in (17.5) is

$Z θ = x∈F (V ) exp(-θ T U (x))$and is usually not computable. Now, assume that an N -sample, x 1 , . . . , x N , is observed for this distribution. The maximum likelihood estimator maximizes

$ℓ(θ) = 1 N N k=1 log π θ (x k ) = -θ T ŪN -log Z θ with ŪN = (U (x 1 ) + • • • + U (x N ))/N .$We have the following proposition, which is a well-known property of exponential families of probabilities. where E θ denotes the expectation with respect to π θ and Var θ the covariance matrix under the same distribution.

We skip the proof, which is just computation. This proposition implies that a local maximum of θ → ℓ(θ) must also be global. Any such maximum must be a solution of

$E θ (U ) = ŪN (x 0 )$and conversely. There are some situations in which the maximum does not exist, or is not unique. Let us first discuss the second case.

If several solutions exist, the log-likelihood cannot be strictly concave: there must exist at least one θ for which Var θ (U ) is not definite. This implies that there exists a nonzero vector u such that var θ (u T U ) = u T Var θ (U )u = 0. This is only possible when u T U (x) = cst for all x ∈ F V . Conversely, if this is true, Var θ (U ) is degenerate for all θ.

So, the non-uniqueness of the solutions is only possible when a deterministic affine relation exists between the components of U , i.e., when the model is overdimensioned. Such situations are usually easily dealt with by removing some parameters. In all other cases, there exists at most one maximum.

For a concave function like ℓ to have no maximum, there must exist what is called a direction of recession [[167]](#b185), which is a direction α ∈ R d such that, for all θ, the function t → ℓ(θ + tα) is increasing. In this case the maximum is attained "at infinity". Denoting U α (x) = α T U (x), the derivative in t of ℓ(θ + tα) is

$E θ+tα (U α ) -Ūα where Ūα = α T$ŪN . This derivative is positive for all t if and only if

$Ūα = U * α := min{U α (x), x ∈ F (V )} (17.8)$and U α is not constant. To prove this, assume that the derivative is positive. Then U α is not constant (otherwise, the derivative would be zero). Let F * α ⊂ F (V ) be the set of configurations x for which U α (x) = U * α . Then

$E θ+tα (U α ) = x∈F (V ) U α (x) exp(-θ T U (x) -tU α (x)) x∈F (V ) exp(-θ T U (x) -tU α (x)) = x∈F (V ) U α (x) exp(-θ T U (x) -t(U α (x) -U * α )) x∈F (V ) exp(-θ T U (x) -t(U α (x) -U * α )) = U * α x∈F * α exp(-θ T U (x)) + x F * α U α (x) exp(-θ T U (x) -t(U α (x) -U * α )) x∈F * α exp(-θ T U (x)) + x F * α exp(-θ T U (x) -t(U α (x) -U * α ))$.

When t tends to +∞, the sums over x F * α tend to 0, which implies that E θ+tα (U α ) tends to U * α . So, if E θ+tα (U α ) -Ūα > 0 for all t, then Ūα = U * α and U α is not constant. The converse statement is obvious.

As a conclusion, the function ℓ has a finite maximum if and only if there is no direction α ∈ R d such that α T (U (x)-ŪN ) ≤ 0 for all x ∈ F (V ). Equivalently, ŪN must belong to the interior of the convex hull of the finite set

${U (x), x ∈ F (V )} ⊂ R d .$In such a case, that we hereafter assume, computing the maximum likelihood estimator boils down to solving the equation

$E θ (U ) = ŪN .$Because the maximization problem is concave, we know that numerical algorithms such as gradient ascent,

$θ(t + 1) = θ(t) + ϵ(E θ(t) (U ) -ŪN ),(17.9)$converge to the optimal parameter. Unfortunately, the computation of the expectations and covariance matrices can only be made explicitly for acyclic models, for which parameter estimation is not a problem anyway. For general loopy graphical models, the expectation can be estimated iteratively using Monte-Carlo methods. It turns out that this estimation can be synchronized with gradient descent to obtain a consistent algorithm, which is described in the next section.

## Maximum likelihood with stochastic gradient ascent

As remarked above, for fixed θ, we have designed, in chapter 14, Markov chain Monte Carlo algorithms that asymptotically sample form π θ . Select one of these algorithms, and let p θ be the corresponding transition probabilities for a given θ, so that p θ (x, y) = P(X n+1 = y | X n = x) for the sampling chain. Then, define the iterative algorithm, initialized with arbitrary θ 0 and x 0 ∈ F (V ), that loops over the following two steps.

(SG1) Sample from the distribution p θ t (x t , •) to obtain a new configuration x t+1 .

(SG2) Update the parameter using

$θ t+1 = θ t + γ t+1 (U (x t+1 ) -ŪN ).(17.10)$This algorithm differs from the situation considered in section 3.3 in that the distribution of the sampled variable x t+1 depends on both the current parameter θ t and on the current variable x t . Convergence requires additional constraints on the size of the gains γ(t) and we have the following theorem [[206]](#b224).

Theorem 17.5 If p θ corresponds to the Gibbs sampler or Metropolis algorithm, and γ t+1 = ϵ/(t +1) for small enough ϵ, the algorithm that iterates (SG1) and (SG2) converges almost surely to the maximum likelihood estimator.

The speed of convergence of such algorithms depends both on the speed of convergence of the Monte-Carlo sampling and of the original gradient ascent. The latter can be improved somewhat with variants similar to those discussed in section 3.3, for example by choosing data-adaptive gains as in the ADAM algorithm.

## Relation with Maximum Entropy

The maximum likelihood estimator is closely related to what is called the maximum entropy extension of a set of constraints. Let the function U from F (V ) to R d be given. An element u ∈ R d is said to be a consistent assignment for U if there exists a probability distribution π on F (V ) such that E π (U ) = u. An example of consistent assignment is any empirical average Ū based on a sample (x (1) , . . . , x (N ) ), since Ū = E π (U ) for

$π = 1 N N k=1 δ x (k) .$Given U and a consistent assignment, u, the associated maximum entropy extension is defined as a probability distribution π maximizing the entropy, H(π), subject to the constraint E π (U ) = u. This is a convex optimization problem, with constraints

$                     x∈F (V ) π(x) = 1 x∈F (V ) U j (x)π(x) = u j , j = 1, . . . , d π(x) ≥ 0, x ∈ F (V ) (17.11)$Because the entropy is strictly convex, there is a unique solution to this problem. We first discuss non-positive solutions, i.e., solutions for which π(x) = 0 for some x. An important fact is that, if, for a given x, there exists π 1 such that E π 1 (U ) = u and π 1 (x) > 0, then the optimal π must also satisfy π(x) > 0. This is because, if π(x) = 0, then, letting π ϵ = (1ϵ)π + ϵπ 1 , we have E π ϵ (U ) = u since this constraint is linear, π ϵ (x) > 0 and

$H(π ϵ ) -H(π) = - y,π(y)>0 (π ϵ (y) log π ϵ (y) -π(y) log π(y)) - y,π(y)=0 ϵπ 1 (y)(log(ϵ) + log π 1 (y)) = -ϵ log ϵ y,π(y)=0 π 1 (y) + O(ϵ)$which is positive for small enough ϵ, contradicting the fact that π is a maximizer.

Introduce the set N u containing all configurations x ∈ F (V ) such that π(x) = 0 for all π such that E π (U ) = u. Then we know that the maximum entropy extension satisfies π(x) > 0 if x N u . Introduce Lagrange multipliers θ 0 , θ 1 , . . . , θ d for the d + 1 equality constraints in [(17.11)](#), and the Lagrangian

$L = H(π) + x∈F (V )\N u (θ 0 + θ T U (x))π(x)$in which we have set θ = (θ 1 , . . . , θ d ), we find that the optimal π must satisfy

$               log π(x) = -θ 0 -1 -θ T U (x) x π(x) = 1 E π (U ) = ū$In other terms, the maximum entropy extension is characterized by

$π(x) = 1 Z θ exp(-θ T U (x))1 N c u (x)$and E π (U ) = u.

In particular, if N u = ∅, then the maximum entropy extension is positive. If, in addition, u = Ū for some observed sample, then it coincides with the maximum likelihood estimator for [(17.5)](#). Notice that, in this case, the condition N u ∅ coincide with the condition that there exists α such that α T U (x) ≥ α T u for all x, with α T U (x) not constant. Indeed, assume that the latter condition is true. Then, if E π (U ) = u, then E π (α T U ) = α T u, which is only possible if π(x) = 0 for all x such that α T U (x) < α T u. Such x's exist by assumption, and therefore N u ∅. Conversely, assume N u ∅. If condition (17.8) is not satisfied, then we have shown when discussing maximum likelihood that an optimal parameter for the exponential model would exist, leading to a positive distribution for which E π (U ) = u, which is a contradiction.

## Iterative Scaling

Iterative scaling is a method that is well-adapted to learning distributions given by (17.5), when U can be interpreted as a random histogram, or a collection of them. More precisely, assume that for all x ∈ F (V ), one has

$U (x) = (U 1 (x), . . . , U q (x)) with q j=1 U j (x) = 1 and U j (x) ≥ 0.$Let the parameter be given by θ = (θ 1 , . . . , θ q ). Assume that x 1 , . . . , x N have been observed, and let u ∈ R d be a consistent assignment for U , with u j > 0 for j = 1, . . . , d and such that N u = ∅. Iterative scaling computes the maximum entropy extension of E π (U ) = u, that we will denote π * . It is supported by the following lemma. Lemma 17.6 Let π be a probability on F (V ) with π > 0 and define

$π ′ (x) = π(x) ζ d j=1 u j E π (U j ) U j (x)$where ζ is chosen so that π ′ is a probability. Then π ′ > 0 and KL(π * ∥π ′ ) -KL(π * ∥π) ≤ -KL(u∥E π (U )) ≤ 0 (17.12)

Proof Note that, since π > 0, E π (U j ) must also be positive for all j, since E π (U j ) = 0 would otherwise imply U j = 0 and u j = 0 for u to be consistent. So, π ′ is well defined and obviously positive.

We have

$KL(π * ∥π ′ ) -KL(π * ∥π) = log ζ - x∈F (V ) π * (x) d j=1 U j (x) log u j E π (U j ) = log ζ - d j=1 u j log u j E π (U j ) = log ζ -KL(u∥E π (U )).$(We have used the identity E π * (U ) = u.) So it suffices to prove that ζ ≤ 1. We have

$ζ = x∈F (V ) π(x) d j=1 u j E π (U j ) U j (x) ≤ d j=1 x∈F (V ) π(x)U j (x) u j E π (U j ) = d j=1 E π (U j ) u j E π (U j ) = 1,$which proves the lemma (we have used the fact that, for x i , w i positive numbers with

$i w i = 1, one has i x w i i ≤ i w i x i$, which is a consequence of the concavity of the logarithm).

## ■

Consider the iterative algorithm

$π n+1 (x) = π n (x) ζ n d j=1 u j E π n (U j ) U j (x)$initialized with a uniform distribution. Equivalently, using the exponential formulation, define, for j = 1, . . . , d, θ n+1,j = θ n,j + log E θ n (U j )

$u j + KL(u∥E θ n (U )),(17.13)$with π θ given by (17.5), initialized with θ 0 = 0. Note that adding a term that is independent of j to θ does not change the value of π θ , because the U j 's sum to 1. The model is in fact overparametrized, and the addition of the KL divergence in (17.13) ensures that d i=1 u i θ i = 0 at all steps.

This algorithm always reduces the Kullback-Leibler distance to the maximum entropy extension. This distance being always positive, it therefore converges to a limit, which, still according to lemma 17.6, is only possible if KL(u∥E π n (U )) also tends to 0, that is E π n (U ) → u. Since the space of probability distributions is compact, the Heine-Borel theorem implies that the sequence π θ n has at least one accumulation point, that we now identify. If π is such a point, one must have E π (U ) = u. Moreover, we have π > 0, since otherwise KL(π * ∥π) = +∞. To prove that π = π * (and therefore the limit of the sequence), it remains to show that it can be put in the form [(17.5)](#). For this, define the vector space V of functions v : F (V ) → R which can be written in the form

$v(x) = α 0 + g j=1 α j U j (x).$Since log π θ n ∈ V for all n, so is its limit, and this proves that log π belongs to V . We have obtained the following proposition.

Proposition 17.7 Assume that for all x ∈ F (V ), one has U (x) = (U 1 (x), . . . , U d (x)) with d j=1 U j (x) = 1 and U j (x) ≥ 0.

Let u be a consistent assignment for the expectation of U such thatN u = ∅. Then, the algorithm described in [(17.13](#)) converges to the maximum entropy extension of u. This is the iterative scaling algorithm. This method can be extended in a straightforward way to handle the maximum entropy extension for a family of functions U (1) , . . . , U (K) , such that, for all x and for all k, U (k) (x) is a d k -dimensional vector such that

$d k j=1 U (k) j (x) = 1.$The maximum entropy extension takes the form

$π θ (x) = 1 Z θ exp - K k=1 (θ (k) ) T U (k) (x) ,$where θ (k) is d k -dimensional, and iterative scaling can then be implemented by updating only one of these vectors at a time, using (17.13) with U = U (k) .

The restriction to U (x) providing a discrete probability distribution for all x is, in fact, no loss of generality. This is because adding a constant to U does not change the resulting exponential model in [(17.5)](#), and multiplying U by a constant can be also compensated by dividing θ by the same constant in the same model. So, if u -is a lower bound for min j,x U j (x), one can replace U by (Uu -), and therefore assume that U ≥ 0, and if u + is an upper bound for j U j (x), we can replace U by U /u + and therefore assume that j U j (x) ≤ 1. Define

$U d+1 (x) = 1 - d j=1 U j (x) ≥ 0.$Then, the maximum entropy extension for (U 1 , . . . , U d ) with assignment (u 1 , . . . , u d ) is obviously also the extension for (U 1 , . . . , U d+1 ), with assignment (u 1 , . . . , u d+1 ), where

$u d+1 = 1 - d j=1 u j ,$and the latter is in the form required in proposition 17.7. Note that iterative scaling requires to compute the expectation of U 1 , . . . , U d before each update. These are not necessarily available in closed form and may have to be estimated using Monte-Carlo sampling.

## Pseudo likelihood

Maximum likelihood estimation is a special case of minimal contrast estimators. These estimators are based on the definition of a measure of dissimilarity, say C(π∥ π), between two probability distributions π and π. The usual assumptions on C are that C(π∥ π) ≥ 0, with equality if and only if π = π, and that C is -at least -continuous in π and π. Minimal contrast estimators approximate the problem of minimizing θ → C(π true ∥π θ ) over a parameter θ ∈ Θ, (which is not feasible, since π true , the true distribution of the data, is unknown) by the minimization of θ → C( π∥π θ ) where π is the empirical distribution computed from observed data. Under mild conditions on C, these estimators are generally consistent when N tends to infinity, which means that the estimated parameter asymptotically (in the sample size N ) provides the best (according to C) approximation of π true by the family π θ , θ ∈ Θ.

The contrast that is associated with maximum likelihood is the Kullback-Leibler divergence. Indeed, given a sample x 1 , . . . , x N , we have

$KL( π∥π θ ) = E π log π -E π log π θ = E π log π - N k=1 log π θ (x k ).$Since E π log π does not depend on θ, minimizing KL( π∥π θ ) is equivalent to maximizing N k=1 log π θ (x k ) which is the log-likelihood.

Maximum pseudo-likelihood estimators form another class of minimal contrast estimators for graphical models. Given a distribution π on F (V ), define the local specifications π s (x (s) | x (t) , t s) to be conditional distributions at one vertex given the others, and the contrast

$C(π∥ π) = s∈V E π (log π s πs ).$Because we can write, using standard properties of conditional expectations,

$C(π∥ π) = s∈V E π E π s (log π s πs ) = s∈V E(KL(π s (• | X (t) , t s)∥ πs (• | X (t) , t s)),$we see that C(π, π) is always positive, and vanishes (under the assumption of positive π) only if all the local specifications for π and π coincide, and this can be shown to imply that π = π. Indeed, for any x, y ∈ F (V ), and choosing some order V = {s 1 , . . . , s n } on V , one can write

$π(x) π(y) = n k=1 π(x (s k ) |x (s 1 ) , . . . , x (s k-1$) , y (s k+1 ) , . . . , y (s n ) ) π(x (s k ) |x (s 1 ) , . . . , x (s k-1 ) , y (s k+1 ) , . . . , y (s n ) )

and the ratios π(x)/π(y), for x ∈ F (V ), combined with the constraint that x π(x) = 1 uniquely define π.

So C is a valid contrast and

$C( π∥π θ ) = s∈V E π log πs - s∈V N k=1 log π θ,s (x (s) k |x (t) k , t s).$This yields the maximum pseudo-likelihood estimator (or pseudo maximum likelihood) defined as a maximizer of the function (called log-pseudo-likelihood)

$θ → s∈V N k=1 log π θ,s (x (s) k |x (s) k , t s).$Although maximum likelihood is known to provide the most accurate approximations in many cases, maximum of pseudo likelihood has the important advantage to be, most of the time, computationally feasible. This is because, for a model like [(17.5)](#), local specifications are given by

$π θ,s (x (s) | x (t) , t s) = exp(-θ T U (x)) y (s) ∈F s exp(-θ T U (y (s) ∧ x (V \s) ))$. and therefore include no intractable normalizing constant. Maximum of pseudolikelihood estimators can be computed using standard maximization algorithms.

For exponential models such as (17.5), the log-pseudo-likelihood is, like the loglikelihood, a concave function.

## Continuous variables and score matching

The methods that were presented so far for discrete variables formally generalize to more general state spaces, even though consistency or convergence issues in noncompact cases can be significantly harder to address. Score matching is a parameter estimation method that was introduced in [[95]](#b113) and was designed, in its original version, to estimate parameters for statistical models taking the form

$π θ (x) = 1 C(θ) exp (-F(x, θ))$with x ∈ R d . We assume below suitable integrability and differentiability conditions, in order to justify differentiation under integrals whenever they are needed. The "score function" is defined as

$s(x, θ) = -∇ x log π θ (x) = ∇ x F(x, θ)$where ∇ x denotes the gradient with respect to the x variable. Letting π true denote the p.d.f. of the true data distribution (not necessarily part of the statistical model), score matching minimizes

$f (θ) = R d |s(x, θ) -s true (x)| 2 π true (x)dx$where s true = -∇ log π true . This integral can be restricted to the support of π true , if we don't want to assume that π true is non-vanishing. Note, however that f (θ) = 0 implies that log π θ (•, θ) = log π true π true -almost everywhere, so that π θ (x) = cπ true (x) for some constant c and x in the support of π true . Only if π true (x) > 0 for all x ∈ R d , can we conclude that this requires π θ = π true .

Expanding the squared norm and applying the divergence theorem yield

$f (θ) = R d |∇ x log π θ (x)| 2 π true (x)dx -2 R d ∇ x log π θ (x) T ∇π true (x)dx + R d |s true (x)| 2 π true (x)dx = R d |∇ x log π θ (x)| 2 π true (x)dx + 2 R d ∆ log π θ (x) T π true (x)dx + R d |s true (x)| 2 dx$To justify the use of the divergence theorem, one needs to assume two derivatives in the log-likelihoods with sufficient decay at infinity (see Hyvärinen and Dayan [[95]](#b113) for details). This shows that minimizing f is equivalent to minimizing

$g(θ) = R d |∇ x log π θ (x)| 2 π true (x)dx + 2 R d ∆ log π θ (x) T π true (x)dx = E(|∇ x log π θ (X)| 2 + 2∆ log π θ (X)).$In this form, the objective function can be approximated by a sample average, so that, given observed data x 1 , . . . , x N , one can define the score-matching estimator as a minimizer of

$N k=1 |∇ x log π θ (x k )| 2 + 2∆ log π θ (x k ) . (17.14)$Remark 17.8 The method can be adapted to deal with discrete variables replacing derivatives with differences. Let X take values in a finite set, R X , on which a graph structure can be defined, writing x ∼ y if x and y are connected by an edge. For example, if X is itself a Markov random field on a graph G = (V , E), so that R X = F (V ), one can define x ∼ y if and only if x (s) = y (s) for all but one s ∈ V . One can then define the score function

$s θ (x, y) = 1 - π θ (y) π θ (x)$defined over all x, y ∈ R X such that x ∼ y. Now the score matching functional is

$f (θ) = x∈R X         y∼x |s θ (x, y) -s * (x, y)| 2         π * (x),$whose minimization is, after reordering terms, equivalent to that of

$g(θ) = x∈R X y∼x 1 - π θ (y) π θ (x) 2 π * (x) + 2 x∈R X y∼x π θ (x) π θ (y) - π θ (y) π θ (x) π * (x).$Based on training data, a discrete score matching estimator is a minimizer of Missing variable sin the context of graphical models may correspond to real processes that cannot be measured, which is common, for example, with biological data. They may be more conceptual objects that are interpretable but are not parts of the data acquisition process, like phonemes in speech recognition, or edges and labels in image processing and object recognition. They may also be variables that have been added to the model to increase its parametric dimension without increasing the complexity of the graph. However, as we will see, dealing with incomplete or imperfect observations brings the parameter estimation problem to a new level of difficulty.

$N k=1 y∼x k 1 - π θ (y) π θ (x k ) 2 + 2 N k=1 y∼x k π θ (x k ) π θ (y) - π θ (y) π θ (x k ) . (17$Since it is the most common approach to address incomplete or noisy observations, we start with a description of how the EM algorithm (Algorithm 16.1) applies to graphical models, and of its limitations. We assume a graphical model on an undirected graph G = (V , E), in which we assume that V is separated in two nonintersecting subsets, V = S ∪ H. Letting X be a G-Markov random field, the part X (S) is assumed to be observable, and X (H) is hidden.

We assume that X takes values in F (V ), where we still denote by F s the sets in which X s takes values for s ∈ V . We let the model distribution belong to an exponential family, with

$π θ (x) = 1 Z(θ) exp -θ T U (x) , x ∈ F (V ).(17.16)$Assume that an N -sample x

$(S) 1 , . . . , x (S)$N is observed over S. Since

$log π θ (x) = -log Z(θ) -θ T U (x),$the transition from θ n to θ n+1 in Algorithm 16.1 is done by maximizing

$-log Z(θ) -θ T Ūn (17.17)$where

$Ūn = 1 N N k=1 E θ n (U (X) | X (S) = x (S) k ).(17.18)$So, the M-step of the EM, which maximizes (17.17), coincides with the completedata maximum-likelihood problem for which the empirical average of U is replaced by the average of its conditional expectations given the observations, as given in [(17.18)](#), which constitutes the E-step. As a consequence, a strict application of the EM algorithm for graphical models is unfeasible, since each step requires running an algorithm of similar complexity maximum likelihood for complete data, that we already identified as a challenging, computationally costly problem. The same remark holds for the SAEM algorithm of section 16.4.3, which also requires solving a maximum likelihood problem at each iteration.

## Stochastic gradient ascent

The stochastic gradient ascent described in section 17.2.2 can be extended to partial observations [[207]](#b225), even though it loses the global convergence guarantee that resulted from the concavity of the log-likelihood for complete observations. Indeed, applying the computation of section 16.5.2, to a model given by (17.16), we get using proposition 17.4,

$∂ θ log ψ θ = E θ (E θ (U ) -U | X (S) = x (S) ) = E θ (U ) -E θ (U | X (S) = x (S) )$where we ψ θ (x (S) ) denotes the marginal distribution of π θ on S.

Let π θ (x (H) | x (S) ) denotes the conditional probability P (X (H) = x (H) | X (S) = s (S) ) for the distribution π θ , therefore taking the form

$π θ (x (H) | x (S) ) = 1 Z(θ, x (S) ) exp -θ T U (x (S) ∧ x (H) ) .$Assume given an ergodic transition probability p θ on F (V ), and a family of ergodic transition probabilities p x (S) θ , x (S) ∈ F (S), such that the invariant distribution of p θ is π θ , and the one of p x (S)  θ is π θ (• | x (S) ). Then the following SGA algorithm can be used to estimate θ Algorithm 17.1 Start the algorithm with an initial parameter θ(0) and initial configurations x(0) and

$x (H) k (0), k = 1, . . . , N . Then, at step n, (SGH1) Sample from the distribution p θ(n) (x(n), •) to obtain new configurations x(n+ 1) ∈ F (V ). (SGH2) For k = 1, . . . , N , sample from the distribution p x (S) k θ(n) (x (H) k (n), •) to obtain a new configuration x ( )$k H(n + 1) over the hidden vertexes. (SGH3) Update the parameter using

$θ(n + 1) = θ(n) + γ(n + 1)        U (x(n + 1)) - 1 N N k=1 U (x (S) k ∧ x (H) k (n + 1))        . (17.19)$
## Pseudo-EM Algorithm

The EM update

$θ n+1 = argmax θ N k=1 E θ n log π θ (X) | X (S) = x (S) k .$being challenging for Markov random fields, it is tempting to replace the log-likelihood in the expectation by an other contrast, such as the log-pseudo-likelihood. A similar approach to that described here was introduced in Chalmond [[51]](#b69), for situations when the conditional distribution of X (S) given X (H) is "simple enough" (for example, if the variables X s , s ∈ S are conditionally independent given X (H) ) and when the cardinality of the sets F s , s ∈ H is small (binary, or ternary, variables).

The algorithm has the following variational interpretation. Fix x (S) ∈ F (S) and s ∈ H. Also denote

$µ s = 1/|F (H \ {s})|. If q is a transition probability from F (H \ {s}) to F s , let ∆ (s) θ (q, x (S) ) = y∈F (H) log π θ,s (y (s) ∧ x (S) | y (H\{s}) )$q(y (H\{s}) , y (s) )µ s q(y (H\{s}) , y (s) )µ s . [(17.20)](#) This function is concave in q, since its first partial derivative with respect to q(y (H\{s}) , y (s) ) (for each y ∈ F (H)) is given by µ s log π θ,s (y (s) ∧ x (S) | y (H\{s}) )µ s (y (H\{s}) )µ s log(q(y (H\{s}) , y (s) )µ s )µ s so that its Hessian is the diagonal matrix with negative entries -µ s /q(y (H\{s}) , y (s) ).

Using Lagrange multipliers to express the constraints y (s) ∈F s q(y (H\{s}) , y (s) ) = 1 for all y (H\{s}) , we find that ∆ (s) θ (q, x (S) ) is maximized when q(y (H\{s}) , y (s) ) is proportional to π θ,s (y (s) ∧ x (S) | y (H\{s}) ), yielding q(y (H\{s}) , y (s) ) = π θ,s (y (s) | x (S) ∧ y (H\{s}) ).

## Now, consider the problem of maximizing

$N k=1 s∈H ∆ (n) θ (q (s) k , x (s) k ) (17.21)$with respect to θ and q (s)

k , k = 1, . . . , N , s ∈ H. Consider an iterative maximization scheme in which, from a current parameter θ n , one first, maximizes (17.21) with respect to transition probabilities q (s) k , then with respect to θ to obtain θ n+1 . This scheme provides the iteration

$θ n+1 = argmax θ N k=1 s∈H y∈F (H) log π θ,s (y (s) ∧ x (S) k | y (H\{s}) ) π θ n ,s (y (s) | x (S) k ∧ y (H\{s}) )µ s .$
## Partially-observed Bayesian networks on trees

We now consider the situation in which the joint distribution of X = X (S) ∧ X (H) is a Bayesian network over a directed acyclic graph G = (V , E).

## Assume that x

$(S) 1 , . . . , x (S)$N are observed. The parameter θ is the collection of all p(x (pa(s)) , x (s) ) for s ∈ V . Define the random variables I s,x (y) equal to one if y ({s}∪pa(s)) = x ({s}∪pa(s)) and zero otherwise. We can write log π(y) = s∈S log p s (y (pa(s)) , y (s) ) = s∈S x ({s}∪pa(s)) ∈F ({s}∪pa(s)) log p s (x (pa(s)) , x (s) )I s,x (y) This implies that

$N k=1 E θ n log π(x (S) k , X (H) ) | X (S) = x (S) k = x ({s}∪pa(s)) ∈F ({s}∪pa(s)) log p s (x (pa(s)) , x (s) ) N k=1 E θ n (I s,x (X) | X (S) = x (S) k ) = x ({s}∪pa(s)) ∈F ({s}∪pa(s)) log p s (x (pa(s)) , x (s) ) N k=1 π θ n (x ({s}∪pa(s)) | X (S) = x (S) k ).$The EM iteration at step n then is

$p (n+1) s (x (pa(s)) , x (s) ) = 1 Z s (x (s -) ) N k=1 π θ n (x ({s}∪pa(s)) | X (S) = x (S) k ) with π θ n (x) = s∈V p (n) (x (pa(s)) , x (s) ),$Z s being a normalization constant.

If the estimation is solved with a Dirichlet prior Dir(1+a s (x (s) , x (pa(s)) )), the update formula becomes

$p (n+1) s (x (pa(s)) , x (s) ) = 1 Z s (x (s -) )        a s (x (s) , x (pa(s)) ) + N k=1 π θ n (x ({s}∪pa(s)) | X (S) = x (S) k )        . (17.22)$This algorithm is very simple when the conditional distributions π θ n (x (s∪pa(s)) | X (S) = x (S) k ) can be easily computed, which is not always the case for a general Bayesian network, since conditional distributions do not always have a structure of Bayesian network. The computation is simple enough for trees, however, since conditional tree distributions are still trees (or forests). More precisely, the conditional distribution given the observed variables can be written in the form

$π(y (H) | x (S) ) = 1 Z(x (S) ) s∈H ϕ s,x (y (s) )$t∼s,{s,t}⊂H ϕ st (y (s) , y (t) )

with ϕ s,pa(s) (y (s) , y (pa(s)) ) = p s (y (pa(s)) , y (s) ) and, letting ϕ s (y (s) ) = p s (y (s) ) if pa(s) = ∅ and 1 otherwise, ϕ s,x (y (s) ) = ϕ s (y (s) )

t∼s,t∈S ϕ st (y (s) , x (t) ).

So, the marginal joint distribution of a vertex and its parents are directly given by belief propagation, using the just defined interactions. This training algorithm is summarized below.

Algorithm 17.2 (Learning tree distributions with hidden variables) Start with some initial guess of the conditional probabilities (for example, those given by the prior). The iterate the following two steps providing the transition from θ n to step θ n+1 .

(1) For k = 1, . . . , N , use belief propagation (or sum-prod) to compute all π θ n (x ({s}∪pa(s)) |

$X (S) = x (S)$k ). Note that these probabilities can be 0 or 1 when s ∈ S and/or pa(s) ⊂ S. (2) Use [(17.22)](#) to compute the next set of parameters.

The tree case includes the important example of hidden Markov models, which are defined as follows. S and H are ordered, with same cardinality, say S = {s 1 , . . . , s q } and H = {h 1 , . . . , h q }. Edges are (h 1 , h 2 ), . . . , (h q-1 , h q ) and (h 1 , s 1 ), . . . , (h q , s q ). The interpretation generally is that the hidden variables, h s , are the variables of interest, and behave like a Markov chain, and that the observations, x s , are either noisy or transformed versions of them. A major application is in speech recognition, where the h s 's are labels that represent specific phonemes (little pieces of spoken words) and the x s 's are measured signals. The transitions between hidden variables then describe how phonemes are likely to appear in sequence for a given language, and those between hidden and observed variables describe how each phoneme is likely to be pronounced and heard.

## General Bayesian networks

The algorithm in the general case can move from tractable to intractable depending on the situation. This must generally be handled in a case by case basis, by analyzing the conditional structure, for a given model, knowing the observations. In practice, it is always possible to use loopy belief propagation to obtain some approximation of the conditional probabilities, even if it is not sure that the algorithm will converge to the correct marginals. When feasible, junction trees can be used, too. Monte-Carlo sampling is also an option, although quite computational.

## Chapter 18

Deep Generative Methods We develop, in this chapter, methods that model stochastic processes using a feedforward approach that generates complex random variables using non-linear transformations of simpler ones. Many of these methods can be seen as instances of structural equation models (SEMs), described in section 15.3, with, for deep-learning implementations, high-dimensional parametrizations of [(15.8)](#).

With start with the formally simple case where the modeled variable takes values in R d and is modeled as

$X = g(Z)$where Z also takes values in R d , with a known distribution and g is C 1 , invertible, with a C 1 inverse on R d , i.e., is a diffeomorphism of R d . Let us denote by h the inverse of g.

If Z has a p.d.f. f Z with respect to Lebesgue's measure, then, using the change of variable formula, the p.d.f. of X is

$f X (x) = f Z (h(x)) | det ∂ x h(x)|.$Now, given a training set T = (x 1 , . . . , x N ), the log-likelihood, considered as a function of h, is given by

$ℓ(h) = N k=1 log f Z (h(x k )) + N k=1 log | det ∂ x h(x k )| . (18.1)$This expression should then be maximized with respect to h, subject to some restrictions or constraints to avoid overfitting.

## A greedy computation

One can define a rich class of diffeomorphisms through iterative compositions of simple transformations. This framework was introduced in [[187]](#b205), where a greedy approach was suggested to build such compositions. The method was termed "normalizing flows," since it create a discrete flow of diffeomorphisms that transform the data into a sample of a normal distribution.

We quickly describe the basic principles of the algorithm. One starts with a parametrized family, say (ψ α , α ∈ A) of diffeomorphisms of R. Such families are relatively easy to design, one example proposed in [[187]](#b205) being a smoothed version of the piecewise linear function

$u → v 0 + (1 -σ )u + γ|(1 -σ )u -u 0 |$which is increasing as soon as 0 ≤ max(σ , γ) < 1. The smoothed version has an additional parameter, ϵ, and takes the form

$u → v 0 + (1 -σ )u + γ ϵ 2 + ((1 -σ )u -u 0 ) 2 .$This transformation is parametrized by α = (v 0 , σ , γ, u 0 , ϵ). Other families of parametrized transformations can be designed. A multivariate transformation ϕ α,U : R d → R d can then be associated to families α = (α 1 , . . . , α d ) and orthogonal matrices U by taking (1) )

$ϕ α,U (x) =           ψ α 1 (y$. . .

$ψ α d (y (d) )           with y = U x.$The algorithm in [[187]](#b205) is initialized with h 0 = id [d] and update the transformation at step n according to

$h n = ϕ α n ,U n • h n-1 .$In this update, U n is generated as a random rotation matrix, and α n is determined as a gradient ascent update (starting from α = 0) for the maximization of

$α → ℓ(ϕ α,U n • h n-1 ).$(Here, the current value h n-1 is not revisited, therefore providing a "greedy" optimization method.) Letting z n,k = h n (x k ), the chain rule implies that

$ℓ(ϕ α,U n • h n-1 )) = N k=1 log f Z (ϕ α,U n (z n-1,k )) + N k=1 log | det ϕ α,U n (z n-1,k )| + N k=1 log | det ∂ x h n-1 (x k )| .$Since the last term does not depend on α, we see that it suffices to keep track of the "particle" locations, z n-1,k to be able to compute α n . Note also that these locations are easily updated with z n,k = ϕ α n ,U n (z n-1,k ).

## Neural implementation

This iterated composition of diffeomorphisms obviously provides a neural architecture similar to those discussed in chapter 11. Fixing the number of iterations to be, say, m, one can consider families of diffeomorphisms (ϕ θ ) indexed by a parameter w (we had w = (α, U ) in the previous discussion), and optimize (18.1) over all functions h taking the form

$h = ϕ w m • • • • • ϕ w 1 . Letting z j,k = ϕ w j • • • • • ϕ w 1 (x k ) for j ≤ m (with z 0,k = x k ), we can write ℓ(h) = N k=1 log f Z (z m,k ) + N k=1 m j=1 log | det ∂ x ϕ w j (z j-1,k )|.$Normalizing flows in this form are described in [[161,](#b179)[107,](#b125)[148]](#b166). The gradient of ℓ with respect to the parameters w 1 , . . . , w m can be computed by backpropagation. We note however that, unlike typical neural implementations, the parameters may come with specific constraints, such as U ∈ O d (R) when w = (α, U ), so that the gradient and associated displacement may have to be adapted compared to standard gradient ascent implementations (see section 20.6.3 for a discussion of first-order implementations of gradient methods for functions of orthogonal matrices, and [[1]](#b19) for more general methods on optimization over matrix groups).

## Time-continuous version

In section 11.6, we described how diffeomorphisms could be generated as flows of differential equations, and this remark can be used to provide a time-continuous version of normalizing flows. Using [(11.3)](#), one generates trajectories z(•) by solving over, say, [0, T ]

$∂ t z(t) = ψ w(t) (z(t))$with z(0) = x for some function w : t → w(t). Letting z(t) = h w (t, x) (which defines h w ), we know that, under suitable assumptions on ψ, the mapping x → h w (t, x) is a diffeomorphism of R d . One can then maximize

$ℓ(h w (T , •)) = N k=1 log f Z (h w (T , x k )) + N k=1 log | det ∂ x h w (T , x k )| with respect to the function w. Let z k (t) = h w (t, x k ) and J k (t) = log | det ∂ x h w (t, x k )|. We have, by definition ∂ t z k (t) = ψ w(t) (z k (t))$with z k (0) = x k . One can also show that

$∂ t J k (t) = ∇ • ψ w(t) (z k (t))$with J k (0) = 0, where the r.h.s. is the divergence of ψ w(t) evaluated at z k (t). We provide a quick (and formal) justification of this fact. First note that differentiating

$∂ t h w (t, x) = ψ w(t) (h w (t, x)) with respect to x yields ∂ t ∂ x h w (t, x) = ∂ x ψ w(t) (h w (t, x))∂ x h w (t, x).$The mapping J : A → log | det(A)| is differentiable on the set of invertible matrices and is such that dJ (A)H = trace(A -1 H). Applying the chain rule, we find

$∂ t log | det ∂ x h w (t, x)| = trace(∂ x h w (t, x) -1 ∂ x ψ w(t) (h w (t, x))∂ x h w (t, x)) = trace(∂ x ψ w(t) (h w (t, x))) = ∇ • ψ w(t) (h w (t, x)).$From this, it follows that the time-continuous normalizing flow problem can be reformulated as maximizing

$N k=1 log f Z (z k (T )) + N k=1 J k (T ) subject to ∂ t z k (t) = ψ w(t) (z k (t)), ∂ t J k (t) = ∇ • ψ w(t) (z k (t)), z k (0) = x k , J k (0) = 0.$This is an optimal control problem, whose analysis can be done similarly to that made in section 11.6.1, provided that ∇ • ψ w(t) can be expressed in closed form.

Note that the inverse of h w (T , •), which provides the generative model going from Z to X can also be obtained as the solution of an ODE. Namely, if one solves the differential equation

$∂ t x(t) = -ψ w(T -t) (x(t))$with initial condition x(0) = z, then x(T ) solves the equation h w (T , •) = z.

## NON-DIFFEOMORPHIC MODELS AND VARIATIONAL AUTOENCODERS437

18.2 Non-diffeomorphic models and variational autoencoders

## General framework

The previous discussion addressed the situation X = g(Z) when g is a diffeomorphism, which required, in particular, that X and Z are real vectors with identical dimensions. This may not always be desirable, as one may prefer a small-dimensional variable Z (in the spirit of the factor analysis methods discussed in chapter 20), or a high-dimensional Z to increase, for example the modeling power. In addition, the observation variables may be discrete, which precludes the use of the change of variables formula. In such cases, Z has to be treated as a hidden variable using one of the methods discussed in chapter 16.

It will convenient to model the generative process in the form of a conditional distribution of X given Z rather than a deterministic function. We place ourselves in the framework of chapter 16 (with slightly modified notation) and let R X and R Z denote the measured spaces over where X and Z take their values, with measures µ X and µ Z , and assume that the conditional distribution of X given Z = z has density f X (x | z, θ) with respect to µ X , for some parameter θ. We also assume that Z has a distribution with density f Z with respect to µ Z , that we assume given and unparametrized. One can then directly apply the algorithms provided in chapter 16, and in particular the variational methods described in section 16.4.4 with an appropriate definition of the approximation of the conditional density of Z given X. An important example in this context is provided by variational autoencoders (VAEs) that we now present.

## Generative model for VAEs

VAEs [[103,](#b121)[104]](#b122) model X ∈ R d as X = g(Z, θ)+ϵ where ϵ is a centered Gaussian noise with covariance matrix Q. The function g is typically non-linear, and VAEs have been introduced with this function modeled as a deep neural network (see chapter 11). Letting ϕ N ( • ; 0, Q) denote the p.d.f. of the Gaussian distribution N (0, Q), the conditional distribution of X given Z = z has density

$f X (x | z, θ) = ϕ N (x -g(z, θ)) ; 0, Q) with respect to Lebesgue's measure on R d .$Following the procedure in section 16.4.4, we define an approximation of the conditional distribution of Z given X. Assuming that Z ∈ R p , we let this distribution be N (µ(x, w), Σ(x, w)) for some functions µ and Σ, w being a parameter. To ensure that Σ ⪰ 0, we will represent it in the form Σ(x, w) = S(x, w) 2 where S is a symmetric matrix. In [[103]](#b121), both functions µ and S are represented as neural networks parametrized by w. The joint density of X and Z is such that

$log f X,Z (x, z ; θ, Q) = log ϕ N (x -g(z, θ)) ; 0, Q) + log f Z (z) = - 1 2 (x -g(z, θ)) T Q -1 (x -g(z, θ)) - 1 2 log det Q - d 2 log 2π + log f Z (z)$We also have

$log ϕ N (z ; µ(x, w), S(x, w) 2 ) = - 1 2 (z-µ(x, w)) T S(x, w) -2 (z-µ(x, w))-log det S(x, w)- p 2 log 2π.$We can then rewrite the algorithm in [(16.15)](#) as

$                     θ n+1 = θ n + γ n+1 ∂ θ log f X,Z (X n+1 , Z n+1 ; θ n , Q n ) Q n+1 = Q n + γ n+1 ∂ Q log f X,Z (X n+1 , Z n+1 ; θ n , Q n ) w n+1 = w n + γ n+1 log f X,Z (X n+1 , Z n+1 ; θ n , Q n ) ϕ N (X n+1 ; µ(X n+1 , w n ), S(X n+1 , w n ) 2 ) × ∂ w log ϕ N (X n+1 ; µ(X n+1 , w n ), S(X n+1 , w n ) 2 ) (18.2)$where X n+1 is drawn uniformly from the training data and

$Z n+1 ∼ N (µ(X n+1 , w n ), S(X n+1 , w n ) 2 ).$The derivatives in this system can be computed from those of g, µ and S (typically involving back-propagation) and the expression of the derivatives of the determinant and inverse of a matrix provided in (1.4) and [(1.6)](#).

The computation can be simplified if one assumes that f Z is the p.d.f. of a standard Gaussian, i.e., f Z = ϕ N (•; 0, Id R p ). Indeed, in that case, the integral in [(16.11)](#), which is, using the current notation

$R p log ϕ N (x -g(z, θ) ; 0, Q)ϕ N (z ; 0, Id R p ) ϕ N (z ; µ(x, w), S(x, w) 2 ) ϕ N (z ; µ(x, w), S(x, w) 2 )dz,(18.3)$can be partially computed. For any two p-dimensional Gaussian p.d.f.'s, one has

$R p log ϕ N (z ; µ 1 , Σ 1 ) ϕ N (z ; µ 2 , Σ 2 ) dz = - 1 2 trace(Σ -1 1 Σ 2 ) - 1 2 (µ 2 -µ 1 ) T Σ -1 1 (µ 2 -µ 1 ) - 1 2 log det(Σ 1 ) - p 2 log(2π). (18.4)$As a consequence, (18.3) becomes

$- 1 2 E w (X -g(Z, θ)) T Q -1 (X -g(Z, θ)) - 1 2 log det Q - d 2 log 2π -E w 1 2 trace(S(X, w) 2 ) + 1 2 |µ(X, w)| 2 -log det(S(X, w)) + p 2 ,(18.5)$where E w denotes the expectation for the random variable (X, Z) where X follows a uniform distribution over training data and the conditional distribution of Z given X = x is N (µ(x, w) , S(x, w) 2 ).

The algorithm proposed in Kingma and Welling [[103]](#b121) introduces a change of variable Z = µ(X, w) + S(X, w)U where U ∼ N (0, Id R p ), rewriting (18.5) as

$- 1 2 E (X -g(µ(X, w) + S(X, w)U , θ)) T Q -1 (X -g(µ(X, w) + S(X, w)U , θ)) -E w 1 2 trace(S(x, w) 2 ) + 1 2 |µ(X, w)| 2 -log det(S(X, w)) - 1 2 log det Q - d 2 log 2π + p 2 ,(18.6)$with a modified version of (18.2). Letting

$F(θ, Q, w, x, u) = - 1 2 (x -g(µ(x, w) -S(x, w)U , θ)) T Q -1 (x -g(µ(x, w) -S(x, w)U , θ)) - 1 2 log det Q - 1 2 trace(S(x, w) 2 ) - 1 2 |µ(x, w)| 2 + log det(S(x, w))$the resulting algorithm is

$           θ n+1 = θ n + γ n+1 ∂ θ F(θ n , Q n , w n , X n+1 , U n+1 ) Q n+1 = Q n + γ n+1 ∂ Q F(θ n , Q n , w n , X n+1 , U n+1 ) w n+1 = w n -γ n+1 ∂ w F(θ n , Q n , w n , X n+1 , U n+1 ) (18.7)$where X n+1 is drawn uniformly from the training data and U n+1 ∼ N (0, Id R p ).

## Discrete data

This framework can be easily adapted to situations in which the observations are discrete. Consider, as an example, the situation in which X takes values in {0, 1} V , where V is a set of vertexes, i.e., X is a binary Markov random field on V . Assume, as a generative model, that conditionally to the latent variable Z ∈ R p , the variables X (s) , s ∈ V are independent and X (s) follows a Bernoulli distribution with parameter g s (z, θ), where g : R p → [0, 1] V . Assume also that Z ∼ N (0, Id R p ), and define, as above, an approximation of the conditional distribution of Z given X = x as a Gaussian with mean µ(x, w) and covariance matrix S(x, w) 2 . Then, the joint density of X and Z (with respect to the product of the counting measure on {0, 1} V and Lebesgue's measure on R p ) is

$log f X,Z (x, z ; θ) = x log g(z, θ) + (1 -x) log(1 -g(z, θ)) + log ϕ N (z ; 0, Id R p ) and (18.2) becomes                      θ n+1 = θ n + γ n+1 ∂ θ log f X,Z (X n+1 , Z n+1 ; θ n , Q n ) w n+1 = w n + γ n+1 log f X,Z (X n+1 , Z n+1 ; θ n ) ϕ N (X n+1 ; µ(X n+1 , w n ), S(X n+1 , w n ) 2 ) × ∂ w log ϕ N (X n+1 ; µ(X n+1 , w n ), S(X n+1 , w n ) 2 ) (18.8)$
## Generative Adversarial Networks (GAN) 18.3.1 Basic principles

Similarly to the methods discussed so far, GANs [[82]](#b100), use a one-step nonlinear generator X = g(Z, θ), with θ ∈ R K , to model observed data (we here switch back to a deterministic relation), where Z has a known distribution, with p.d.f. f Z , for example Z ∼ N (0, Id R p ). However, unlike the exact or approximate likelihood maximization that were discussed in sections 18.1 and 18.2, GANs us a different criterion for estimating the parameter θ by minimizing metrics that can be approximated by optimizing a classifier. The classifier is a function x → f (x, w), parametrized by w ∈ R M , whose goal is to separate simulated samples from real ones: it takes values in [0, 1] and estimates the (posterior) probability that its input x is real. GANs' adversarial paradigm consists in estimating θ and w together so that generated data, using θ, are indistinguishable from real ones using the optimal w. Their basic structure is summarized in Figure [18](#fig_135) "real data" vs. "simulation". Given W , θ is optimized to worsen the prediction.

## Objective function

Let P θ denote the distribution of g(Z, θ), and P true the target distribution of "real data." One can formalize the "real data" vs. "simulation" problem with a pair of random variables (X, Y ) where Y follows a Bernoulli distribution with parameter 1/2, and the conditional distribution of X given Y is P true when Y = 1 and P θ when Y = 0. Given a loss function r : {0, 1} × [0, 1] → [0, +∞), one can define

$U (θ, w) = E θ (r(Y , f (X, w))) and U * (θ) = min w∈R M U (θ, w).$We want to maximize U * or, equivalently, solve the optimization problem

$θ * = argmax θ min w∈R M U (θ, w). Note that 2U (θ, w) = E true (r(1, f (X, w))) + E θ (r(0, f (X, w)))$so that choosing the cost requires to specify the two functions t → r(1, t) and t → r(0, t). In Goodfellow et al. [[82]](#b100), they are:

$r(1, t) = -log t r(0, t) = -log(1 -t). (18.9) 18.3.3 Algorithm$Using costs in (18.9), one must compute

$θ * = argmin max w∈R M E true (log f (X, w)) + E θ (log(1 -f (X, w))) = argmin max w∈R M E true (log f (X, w)) + E(log(1 -f (g(Z, θ), w))) .$Such min-max, or saddle-point problem are numerically challenging. The following algorithm was proposed in Goodfellow et al. [[82]](#b100), and also includes a stochastic approximation component. Indeed, in practice, E true is only known through the observation of training data, say x 1 , . . . , x N . Moreover, E θ is only accessible through Monte-Carlo simulation, so that both expectations can only be approximated through finite-sample averaging.

## Algorithm 18.1 (GAN training algorithm)

1. Extract a batch of m examples from training data, simulate m samples according to P θ and run a few (stochastic) gradient ascent steps with fixed θ to update w, replacing expectations by averages.

2. Generate m new samples of Z and update θ with fixed w by iterating a few steps of (stochastic) gradient descent.

## Associated probability metric and Wasserstein GANs

Let F be the family of all measurable functions: f : R d → [0, 1]. Given two possible probability distributions P 1 , P 2 (with associated expectations denoted E 1 , E 2 ) of a random variable X taking values in R d , consider the function

$D(P 1 , P 2 ) = 2 log 2 + max f ∈F E 1 (log f (X)) + E 2 (log(1 -f (X)))$Assume that X under P 1 (resp. under P 2 ) has p.d.f. g 1 (resp. g 2 ) with respect to Lebesgue's measure (this assumption is not needed for the following to hold, but makes the discussion more elementary). Then

$E 1 (log f (X)) + E 2 (log(1 -f (X))) = R d (g 1 log f + g 2 log(1 -f ))dx which is maximal at f * = g 1 /(g 1 + g 2 ). For this f * , 2 log 2 + E 1 (log f * (X)) + E 2 (log(1 -f * (X))) = R d g 1 log 2g 1 g 1 + g 2 dx + R d g 2 log 2g 2 g 1 + g 2 dx = KL g 1 + g 2 2 , g 1 + KL g 1 + g 2 2 , g 2$This expression is called the Jensen-Shannon divergence between g 1 and g 2 . It is always non-negative, and vanishes only when g 1 = g 2 .

So, D : (P 1 , P 2 ) → D(P 1 , P 2 ) can be interpreted as a way to evaluate the difference between two probability distributions on R d . One can then define

$D(P 1 , P 2 ) = max w∈R M E 1 (log f (X, w)) + E 2 (log(1 -f (X, w)))$as an approximation of D in which the set of all possible functions with values in [0, 1] is replaced by those arising from the GAN classification network, parametrized by w. This approximation is useful when g 1 , g 2 are only observable through random sampling or simulation. With this interpretation, GANs minimize D(P true , P θ ). This discussion suggests that new types of GAN may be designed using other discrepancy functions between probability distributions, provided they can be expressed in terms of the maximization of some quantity over some space of functions. Consider, for example the norm in total variation, defined by (for discrete distributions)

$D var (P 1 , P 2 ) = 1 2 x |P 1 (x) -P 2 (x)|.$or, in the general case D var (P 1 , P 2 ) = max A (P 1 (A) -P 2 (A)).

If F is the space of continuous functions f : R d → [0, 1], then we also have (under mild assumptions on P 1 and P 2 )

$D var (P 1 , P 2 ) = max f ∈F (E 1 (f ) -E 2 (f )).$Since neural nets typically generate continuous functions with values in [0, 1], one could train GANs by maximizing

$Dvar (P 1 , P 2 ) = max w∈R M E 1 (f (X, w)) -E 2 (f (X, w))$However, the total variation distance is too crude to allow for meaningful comparisons between distributions. For example, the distance between two Dirac distributions at, say, x 1 and x 2 in R d is always 1, whatever the distance between x 1 and x 2 , unless x 1 = x 2 . A more sensitive distance can be defined based on the notion of optimal transport.

The Monge-Kantorovich, also called Wasserstein, and sometimes also called "earthmover", distance evaluates the minimal total distance along which "mass" needs to be transported to transform a distribution, P 1 , into another, P 2 . Its mathematical definition is

$D w (P 1 , P 2 ) = inf Q R d ×R d |x 1 -x 2 |Q(dx 1 , dx 2 )$where the inf is computed over all joint distributions on R d ×R d whose first marginal is P 1 and second marginal P 2 . Note that the distance D w between δ x 1 and δ x 2 now is

$|x 1 -x 2 |.$The Wasserstein distance can also be defined by

$D w (P 1 , P 2 ) = max f ∈F (E 1 (f ) -E 2 (f ))$where F is now the space of contractive (or 1-Lipschitz) functions, i.e., f ∈ F if and only if, for all

$x 1 , x 2 ∈ R d , |f (x 1 ) -f (x 2 )| ≤ |x 1 -x 2 |.$Using the fact that a neural network with all weights bounded by a constant K generates a function whose Lipschitz constant is controlled solely by K, one can then approximate (up to a multiplicative constant) the Wasserstein distance by

$Dw (P 1 , P 2 ) = max w∈W E 1 (f (X, w)) -E 2 (f (X, w))$where W is the set of all weights bounded by a fixed constant. Given the distribution P true and the model P θ , Wasserstein GANs (WGANs [[11]](#)) must then solve the saddlepoint problem

$U (θ, w) = max w∈W E true (f (X, w)) -E θ (f (X, w)) and U * (θ) = min w∈R M U (θ, w),$with an algorithm similar to that described earlier.

As a final reference, we note the improved WGAN algorithm introduced in Gulrajani et al. [[84]](#b102) in which the boundedness constraint in the weights is replaced by an explicit control of the derivative in x of the function f . More precisely, introduce a random variable Z with distribution Pθ equal (1 -U )X + U X ′ where U is uniformly distributed over [0, 1] and X and X ′ are independent respectively following the distribution P true and P θ . Then, the following approximation of the Wasserstein distance between P true and P θ that is used in Gulrajani et al. [[84]](#b102):

$Dw (P true , P θ ) = max w∈W E true (f (X, w)) -E θ (f (X, w)) -Ẽθ ((|∂ z f (Z, w)| -1) 2 ) .$
## Reversed Markov chain models 18.4.1 General principles

The discussions in sections 18.2 and 18.3 can be applied to sequences of structural equations (describing finite Markov chains) in the form

$           Z 0 = ξ 0 Z k+1 = g(Z k , ξ k ; θ k ), k = 0, . . . , m -1 X = Z m$where ξ 0 , . . . , ξ m-1 are random variables with fixed distribution.

Indeed, letting Z = (ξ 0 , . . . , ξ n-1 ) and θ = (θ 0 , . . . , θ m-1 ) the whole system can be considered as a function X = G( Z, θ) as considered in these sections. This representation, however, includes a large number of hidden variables, and it is unclear whether much improvement can be added to the case m = 1 to justify the additional computational load.

Reversed Markov chain models use a different generative approach in that they first model a forward Markov chain Z n , n ≥ 0 which is ergodic with known (and easy to sample from) limit distribution Q ∞ , and initial distribution Q true , the true distribution of the data. If one fixes a large enough number of steps, say, τ, then it is reasonable to assume that Z τ approximately follows the limit distribution, Q ∞ . One can then (approximately) sample from Q true by sampling Z0 according to Q ∞ and then applying τ steps of the time-reversed Markov chain.

Reversed chains were discussed in section 12.3.3. Assuming that Q true and P (z, •) have a density with respect to a fixed measure µ on R Z , we found that Zk = Z τ-k is a non-homogeneous Markov chain whose transition probability Pk (x, A) = P ( Zk+1

$∈ A | Zk = x) has density pk (x, y) = p(y, x)q τ-k-1 (y) q τ-k (x)$with respect to µ, where q n is the p.d.f. of Q n = Q true P n , the distribution of Z n .

The distributions Q n , n ≥ 0 are unknown, since they depend on the data distribution P true , and the transition probabilities above must be estimated from data to provide a sampling algorithm from the reversed Markov chain. While, at first glance, this does not seem like a simplification of the problem, because one now has to sample from a potentially large number (τ) of distributions instead of one, this leads, with proper modeling and some intensive learning, to efficient and accurate sampling algorithms.

Several factors can indeed make this approach achievable. First, the forward chain should be making small changes to the current configuration at each step (e.g., adding a small amount of noise). This ensures that the reversed transition probabilities pk (x, •) are close to Dirac distributions and are therefore likely to be well approximated by simple unimodal distributions such as Gaussians. Second, the estimation problem does not have hidden data: given an observed sample, one can simulate τ steps of the forward chain to obtain, after reversing the order, a full observation of the reversed chain. Third, in some cases, analytical considerations can lead to partial computations that facilitate the modeling of the reversed transitions.

## Binary model

We now take some examples, starting with a discrete one. Let Q true be the distribution of a binary random field with state space {0, 1} over a set of vertexes V , i.e., with the notation of section 13.2, R X = F (V ) with F = {0, 1}. Fix a small ϵ > 0 and define the transition probability p(x, y) for x, y ∈ F (V ) by

$p(x, y) = s∈V (1 -ϵ)1 y (s) =x (s) + ϵ1 y (s) =1-x (s) .$Since p(x, y) > 0 for all x and y, the chain converges (uniformly geometrically) to its invariant probability Q ∞ and one easily checks that this probability is such that all variables are independent Bernoulli random variables with success probability 1/2. Assuming that τ is large enough so that Q τ ≃ Q ∞ , the sampling algorithm initializes the reversed chain as independent Bernoulli(1/2) variables and runs τ steps using the transitions pk which must be learned from data.

For this model, we have

$q k (x) = y∈F (V ) q k-1 (y)p(y, x).$For this transition, the probabililty of flipping two or more values of y is

$1 -(1 -ϵ) N -N ϵ(1 -ϵ) N -1 = N (N -1) 2 ϵ 2 + o(ϵ 2 )$with N = |V |. We will write x ∼ s y if y (s) = 1x (s) and y (t) = x (t) for s t, and we will write x ∼ y if x ∼ s y for some s. With this notation, we have

$q k (x) = (1 -N ϵ)q k-1 (x) + ϵ y:y∼x q k-1 (y) + O(ϵ 2 )$Since it implies that q k (x) = q k-1 (x) + o(ϵ), this expression can be reversed as

$q k-1 (y) = (1 + N ϵ)q k (y) -ϵ x:x∼y q k (x) + O(ϵ 2 )$Similarly, we have

$p(y, x) = (1 -N ϵ)1 x=y + ϵ1 x∼y + O(ϵ 2 ).$This gives p(y, x)q k-1 (y) = q k (x)1 x=y -ϵ1 x=y

$x ′ :x ′ ∼y q k (x ′ ) + ϵq k (y)1 x∼y + O(ϵ 2 ),$and we finally get

$pk (x, y) =         1 -ϵ x ′ :x ′ ∼y q τ-k (x ′ ) q τ-k (x)         1 x=y + ϵ q τ-k (y) q τ-k (x) 1 x∼y + O(ϵ 2 ) If one lets σ (s) k (x) = q τ-k (y)$q τ-k (x) with y ∼ s x, and defines

$pk (x, y) = s∈V (1 -ϵσ (s) k (x))1 y (s) =x (s) + ϵσ (s) k (x)1 y (s) =1-x (s) ,$one checks easily that pk (x, y) = pk (x, y) + O(ϵ 2 ). This suggests modeling the reversed chain using transitions pk , for which the mapping x → (σ (s) k (x), s ∈ V ) needs to be learned from data (for example using a deep neural network). Note that 1σ k (x) is precisely the score function introduced for discrete distributions in remark 17.8.

## Model with continuous variables

We now switch to an example with vector-valued variables, R X = R d , and assume that the forward Markov chain is such that, conditionally to X n = x,

$X n+1 ∼ N (x + hf (x), √ hId R d ),$where f is C 1 . We saw in section 12.3.7 that, when f = -∇H/2 for a C[foot_17](#foot_17) function H such that exp(-H) is integrable, this chain converges (approximately for small h) to a limit distribution with p.d.f. (with respect to Lebesgue's measure) proportional to exp(-H). In the linear case, in which f (x) = -Ax/2 for some positive-definite symmetric matrix A, so that H(x) = 1 2 x T Ax, the limit distribution can be identified exactly as N (0, Σ h ) where Σ h satisfies the equation

$AΣ h + Σ h A - h 2 A 2 -2Id R d = 0$whose solution is Σ h = (A -hA 2 /4) -1 (details being left to the reader). This implies that this limit distribution can be easily sampled from for any choice of A.

We now return to general f 's and make, like in the discrete case, a first-order identification of the reversed chain. We note that, for any smooth function γ,

$E(γ(X n+1 ) | X n = x) = E(γ(x + hf (x) + √ hU ))$where U ∼ N (0, Id R d ). Making the second order expansion

$γ(x + hf (x) + hU ) = γ(x) + √ h∇γ(x) T U + h∇γ(x) T f (x) + h 2 U T ∇ 2 γ(x)U + o(h)$and taking the expectation gives

$E(γ(X n+1 ) | X n = x) = γ(x) + h∇γ(x) T f (x) + h 2 ∆γ(x) + o(h). (18.10)$Considering the reversed chain, and letting q k denote the p.d.f. of X k for the forward chain, we have

$E(γ(X k-1 ) | X k = x) = R d γ(y) pk (x, y)dy = R d γ(y)p(y, x) q k-1 (y) q k (x) dy = 1 (2πh) d/2 R d γ(y) q k-1 (y) q k (x) e -1 2h |x-y-hf (y)| 2 dy = 1 (2π) d/2 R d γ(x - √ hu) q k-1 (x - √ hu) q k (x) e -1$with the change of variable u = (xy)/ √ h. We make a first-order expansion of the terms in this integral, with

$γ(x - √ hu)q k-1 (x - √ hu) = γ(x)q k-1 (x) - √ h∇(γq k-1 )(x) T u + h 2 u T ∇ 2 (γq k-1 )(x)u + o(h) and e -1 2 |u- √ hf (x- √ hu)| 2 = e -1 2 |u| 2 e √ hu T f (x)-hu T df (x)u-1 2 |f (x)| 2 +o(h) = e -1 2 |u| 2 1 + √ hu T f (x) -hu T df (x)u - h 2 |f (x)| 2 + h 2 |u T f (x)| 2 + o(h) .$Taking products

$γ(x - √ hu)q k-1 (x - √ hu)e -1 2 |u- √ hf (x- √ hu)| 2 = e -1 2 |u| 2 γ(x)q k-1 (x) 1 + √ hu T f (x) -hu T df (x)u - h 2 |f (x)| 2 + h 2 |u T f (x)| 2 + e -1 2 |u| 2 - √ h∇(γq k-1 )(x) T u -h(∇(γq k-1 )(x) T u)(f (x) T u) + h 2 u T ∇ 2 (γq k-1 )(x)u + o(h)$We now take the integral with respect to u (recall that E(U T AU ) = trace(A) if A is any square matrix and U is standard Gaussian), so that

$1 (2π) d/2 R d γ(x - √ hu)q k-1 (x - √ hu)e -1 2 |u- √ hf (x- √ hu)| 2 du = γ(x)q k-1 (x) + h -γ(x)q k-1 (x)∇ • f (x) -∇(γq k-1 )(x) T f (x) + 1 2 ∆(γq k-1 )(x) + o(h) = q k-1 (x)       γ(x) + h       -γ(x)∇ • f (x) - ∇(γq k-1 )(x) q k-1 (x) T f (x) + 1 2 ∆(γq k-1 )(x) q k-1 (x)             + o(h)$To compute an expansion of q k (x), it suffices to take γ = 1 above, so that

$q k (x) = q k-1 (x)       1 + h       -∇ • f (x) - ∇q k-1 (x) q k-1 (x) T f (x) + 1 2 ∆q k-1 (x) q k-1 (x)             + o(h).$We now take the first-order expansion of the ratio, removing terms that cancel, and get

$E(γ(X k-1 ) | X k = x) = γ(x) -h∇γ(x) T f (x) + h∇γ(x) T ∇q k-1 (x) q k-1 (x) + h 2 ∆γ(x) + o(h)$Comparing with (18.10), we find that Xk = X τ-k behaves, for small h, like the non-homogeneous Markov chain such that the conditional distribution of Xk+1 given

$Xk = x is N (x -hf (x) -hs τ-k-1 (x), √ hId R d ), with s τ-k-1 (x) = -∇ log q τ-k-1$, the score function introduced in section 17.2.6, and score-matching methods from that section can be used to estimate it from observations of the forward chain initialized with training data.

## Continuous-time limit

The forward schemes described in the previous examples can be interpreted as continuous time processes over discrete or continuous variables. In the latter case, the example X k+1 ∼ N (x +hf (x),

$√ hId R d ) conditionally to X k = x is a discretization of the stochastic differential equation dx t = f (x t )dt + dw t$(see remark 12.5), where w t is a Brownian motion and the diffusion is initialized with Q true . We found that going backward meant (at first order and conditionally to

$X k = x) X k-1 ∼ N (x -hf (x) -hs k-1 (x), √ hId)$that we can rewrite as

$x τ -X k-1 ∼ N (x τ -x + hf (x) + hs k-1 (x), √ hId).$Following the definition in Anderson [[9]](#b27), this corresponds to a first-order discretization of the reverse diffusion

$dx t = (f (x t ) + s t (x t ))dt + d wt , t ≤ τ$where wt is also a Brownian motion. This reverse diffusion with X τ ∼ Q ∞ will therefore approximately sample from Q true . (With this terminology, forward and reverse diffusions have similar differential notation, but mean different things.) Note that, in the continuous-time limit, the reverse Markov process follows the distribution of the reversed diffusion exactly.

## Differential of neural functions

As we have seen in the previous two examples, estimating the reversed Markov chain requires computing the score functions of the forward probabilities. In the case of continuous variables, this score function is typically parametrized as a neural network, so that the function

$s k (x) = -∇ log q k (x) is computed as s k (x) = F(x; W k ), with the usual definition F(x, W k ) = z m+1 with z j+1 = ϕ j (z j , w jk ), z 0 = x and W k = (w 0k , . . . , w mk ).$Assume that a training set T is observed. Running the forward Markov chain initialized with elements of T generates a new training step at each time step, that we will denote T k at step k. We have seen in section 17.2.6 that the score function s k could be estimated by minimizing, with respect to

$W x∈T k |F(x, W )| 2 -2∇ • F(x, W ) .$This term involves the differential of F, which is defined recursively by (simply taking the derivative at each step)

$dF(x, W ) = ζ m+1 , ζ j+1 = dϕ j (z j , w j )ζ j , with ζ 0 = Id R d .$From this recursive definition, back-propagation can be applied, in principle, to compute the derivative of dF(x, W ) with respect to W . The feasibility of this computation, however, is limited when d is large (d could be tens of thousands if one models images) computing the d × d matrix dF(x, W ) is intractable.

We can note that, for any h ∈ R d , the vector dF(x, W )h also satisfies the recursion

$dF(x, W )h = ζ m+1 h, ζ j+1 h = dϕ j (z j , w j )ζ j h, with ζ 0 h = h and ∇ • F(x, W ) = d i=1 e T i dF(x, W )e i$where e 1 , . . . , e d is the canonical basis of R d . Putting the divergence of F in this form does not reduce the computation cost (which is, roughly d 2 m, assuming that all z j 's have the same dimension), but expresses the divergence term in a form that is amenable to stochastic gradient descent (which is typically already used to approximate the sum over x). Indeed, if U follows any distribution with zero mean and covariance matrix equal to the identity (such as a standard Gaussian, or the uniform distribution on the unit sphere), then

$∇ • F(x, W ) = E(U T dF(x, W )U )$so that U can be sampled from in minibatches in SGD implementations (see [[180]](#b198), where this approach is called "sliced score matching").

Chapter 19

Clustering

## Introduction

We now describe a collection of methods designed to divide a training set into homogeneous subsets, or clusters. This grouping operation is a key problem in many applications for which it is important to categorize the data in order to obtain improved understanding of the sampled phenomenon, and sometimes to be able to apply a different approach to subsequent processing or analysis adapted to each cluster.

We will assume that the variables of interest belong a set R = R X where R is equipped with a discrepancy function α : R × R → [0, +∞). Often, α is derived from a distance ρ on R, but this is not always the case. We will assume that the data results from a training set T = (x 1 , . . . , x N ). However, it may happen that only the discrepancy matrix A = (α(x, y), x, y ∈ T ) is observed, while a coordinate representation of the elements of T is not available.

## Let us consider a few examples.

(i) The simplest case is when R = R d with the standard Euclidean metric. Slightly more generally, a metric may be defined by

$ρ 2 (x, y) = ∥h(x) -h(y)∥ 2$H , where H is an inner-product space and the feature function h : R → H may be unknown, while its associated "kernel", K(x, y) = ⟨h(x) , h(y)⟩ H is known (this is a metric if h is one-toone). In this case ρ 2 (x, y) = K(x, x) -2K(x, y) + K(y, y).

Typically, one then takes α = ρ or α = ρ 2 .

(ii) Very often, however, the data is not Euclidean, and the distance does not correspond to a feature space representation. This is the case, for example, for data belonging to "curved spaces" (manifolds), for which one may use the intrinsic distance provided by the length of shortest paths linking two points (assuming of course that this notion can be given a rigorous meaning). The simplest example is data on the unit sphere, where the distance ρ(x, y) between two points x and y is the length of the shortest large circle that connects them, satisfying |x -y| 2 = 2 -2 cos ρ(x, y).

Once again, α = ρ or ρ 2 is a typical choice.

(iii) A more complex example is provided by R being the space of symmetric positive-definite matrices on R d , for which one defines the length of a differentiable curve (S(t

$), t ∈ [a, b]) in this space by b a trace((S(t) -1 ∂ t S)(S(t) -1 ∂ t S) T )dt$and for which

$ρ 2 (S 1 , S 2 ) = d i=1 (log λ i ) 2$where λ 1 , . . . , λ d are the eigenvalues of S -1/2 1 S 2 S -1/2 1 or, equivalently, solutions of the generalized eigenvalue problem S 2 u = λS 1 u (see, for example, [[72]](#b90)).

(iv) Another common assumption is that the elements of R are vertices of a weighted graph of which T is a subgraph; ρ may then be, e.g., the geodesic distance on the graph.

## Hierarchical clustering and dendograms 19.2.1 Partition trees

This method builds clusters by organizing them in a binary hierarchy in which the data is divided into subsets, starting with the full training set, and iteratively splitting each subset into two parts until reaching singletons. This results in a binary tree structure, called a dendogram, or partition tree, which is defined as follows.

## Definition 19.1 A partition tree of a finite set A is a finite collection of nodes T with the following properties. (i) Each node has either zero or exactly two children. (We will use the notation

$v → v ′ to indicate that v ′ is a child of v.$(ii) All nodes but one have exactly one parent. The node without parent is the root of the tree.

(iii) To each node v ∈ T is associated a subset A v ⊂ A. (iv

$) If v ′ and v ′′ are the children of v, then (A v ′ , A v ′′ ) forms a partition of A v .$Nodes without children are called leaves, or terminal nodes. We will say that the hierarchy is complete if

$A v = A if v is the root, and |A v | = 1 for all terminal nodes.$An example of partition tree is provided in fig.

The construction of the tree can follow two directions, the first one being bottomup, or agglomerative, in which the algorithm starts with the collection of all singletons and merges subsets one pair at a time until everything is merged into the full dataset. The second approach is top-down, or divisive, and initializes the algorithm with the full training set which is recursively split until singletons are reached. The first approach, on which we now focus, is more common, and computationally simpler.

We let T denote the training set and assume that a matrix of dissimilarities (α(x, y), x, y ∈ T ) is given. We will make the abuse of notation of considering that T is a set even though some of its elements may be repeated. This is no loss of generality, since T = (x 1 , . . . , x N ) can always be replaced by the subset

${(k, x k ), k = 1, . . . , N } of N × R.$
## Bottom-up construction

We will extend α to a dissimilarity measure between subsets A, A ′ ⊂ T that we will denote (A, A ′ ) → ϕ(A, A ′ ). Once ϕ is defined, agglomeration works along the following algorithm.

## Algorithm 19.1

1. Start with the collection T 1 , . . . , T N of all single-node trees associated to each element of T . Let n = 0 and m = N . 2. Assume that, at step n of the algorithm, one has a collection of partition trees T 1 , . . . , T m with root nodes r 1 , . . . , r m associated with subsets A r 1 , . . . , A r m of T . Let the total collection of nodes be indexed as V n = {v 1 , . . . , v N +n }, so that {r 1 , . . . , r m } ⊂ V n .

3. If m = 1, stop the algorithm.

4. Select indices i, j ∈ {1, . . . , m} such that ϕ(A r i , A r j ) is minimal, and merge the corresponding trees by creating a new node v n+1+N with the root nodes of T i and T j as children (so that v n+1+N is associated with A r i ∪ A r j ). Add v n+1+N to the collection of root nodes, and remove r i and r j .

5. Set n → n + 1 and m → m -1 and return to step 2.

Clearly, the specification of the extended dissimilarity measure (ϕ) is a key element of the method. Some of most commonly used extensions are:

$• Minimum gap: ϕ min (A, A ′ ) = min(α(x, x ′ ) : x ∈ A, x ′ ∈ A ′ ). • Maximum dissimilarity: ϕ max (A, A ′ ) = max(α(x, x ′ ) : x ∈ A, x ′ ∈ A ′ ).$• Sum of dissimilarities:

$ϕ sum (A, A ′ ) = x∈A x ′ ∈A ′ α(x, x ′ )$• Average dissimilarity:

$ϕ avg (A, A ′ ) = 1 |A| |A ′ | x∈A x ′ ∈A ′ α(x, x ′ ).$As shown in the next two propositions, the maximum distance favors clusters with small diameters, while using minimum gaps tends to favor connected clusters.

## Proposition 19.2 Let diam(A) = max(α(x, y), x, y ∈ A). The agglomerative algorithm using ϕ max is identical to that using ϕ(A, A

$′ ) = diam(A ∪ A ′ ).$Proof Call Algorithm 1 the agglomerative algorithm using ϕ max , and Algorithm 2 the one using ϕ. At initialization, we have (because all sets are singletons),

$ϕ max (A k , A l ) = diam(A k ∪ A l ) for all 1 ≤ k l ≤ m. (19.1)$We show that this property remains true at all steps of the algorithms. Proceeding by induction, assume that, up to the step n, Algorithms 1 and 2 have been identical and result in sets (A 1 , . . . , A m ) satisfy [(19.1)](#). Then the next steps of the two algorithms coincide and assume, without loss of generality, that this next step

$merges A m-1 with A m . Let A ′ m-1 = A m-1 ∪ A m so that diam(A ′ m-1 ) ≤ diam(A i ∪ A j ) for all 1 ≤ i j ≤ m.$We need to show that the new partition satisfies (19.1), which requires that

$ϕ max (A ′ m-1 , A k ) = diam(A ′ m-1 ∪ A k ) for k = 1, . . . , m -2. We have diam(A ′ m-1 ∪ A k ) = max(diam(A ′ m-1 ), diam(A k ), ϕ max (A ′ m-1 , A k )), so that we must show that max(diam(A ′ m-1 ), diam(A k )) ≤ ϕ max (A ′ m-1 , A k ). Write ϕ max (A ′ m-1 , A k ) = max(ϕ max (A m , A k ), ϕ max (A m-1 , A k )) = max(diam(A m ∪ A k ), diam(A m-1 ∪ A k ))$where the last identity results from the induction hypothesis.

The fact that

$diam(A k ) ≤ max(diam(A m ∪ A k ), diam(A m-1 ∪ A k ))$is obvious, and the inequality

$diam(A ′ m-1 ) ≤ max(diam(A m ∪ A k ), diam(A m-1 ∪ A k )$) results from the fact that A m and A m-1 was an optimal pair. This shows that the induction hypothesis remains true at the next step and concludes the proof of the proposition.

## ■

We now analyze ϕ min and, more specifically, the equivalence between the resulting algorithm and the one using the following measure of connectedness. For a given set A and x, y

$∈ A, let αA (x, y) = inf ϵ : ∃n > 0, ∃(x = x 0 , x 1 , . . . , x n-1 , x n = y) ∈ A n+1 : α(x i , x i-1 ) ≤ ϵ for 1 ≤ i ≤ n .$So αA is the smallest ϵ such that there exists a sequence of steps of size less than ϵ in A going from x to y. The function conn(A) = max{ αA (x, y) : x, y ∈ A} measures how well the set A is connected relative to the dissimilarity measure α. and we have: Proposition 19.3 The agglomerative algorithm using ϕ min is identical to that using ϕ(A, A ′ ) = conn(A ∪ A ′ ).

Proof The proof is similar to that of proposition 19.2. Indeed one can note that conn(A ∪ A ′ ) = max(conn(A), conn(A ′ ), ϕ min (A, A ′ )) .

Given this we can proceed by induction and prove that, if the current decomposition is A 1 , . . . , A m such that ψ(A k ∪ A l ) = ϕ min (A k , A l ) for all 1 ≤ k l ≤ m, then this property is still true after merging using ϕ min and ϕ.

Assuming again that A m-1 and A m are merged, and letting

$A ′ m-1 = A m ∪ A m-1 , we need to show that conn(A k ∪ A ′ m-1 ) = ϕ min (A k , A ′ m-1$) for all k = 1, . . . , m -2, which is the same as showing that:

$max(conn(A k ), conn(A ′ m-1 )) ≤ ϕ min (A k , A ′ m-1 ) = min(ϕ min (A k , A m-1 ), ϕ min (A k , A m )). From the induction hypothesis, we have min(ϕ min (A k , A m-1 ), ϕ min (A k , A m )) = min(conn(A k ∪ A m-1 ), conn(A k ∪ A m ))$and both terms in the right-hand side are larger than conn(A k ) and also larger than conn(A ′ m-1 ) which was a minimizer. 

## Top-down construction

The agglomerative method is the most common way to build dendograms, mostly because of the simplicity of the construction algorithm. The divisive approach is more complex, because the division step, which requires, given a set A, to optimize a splitting criterion over all two-partitions of A, may be significantly more expensive than the merging steps in the agglomerative algorithm. The top-down construction therefore requires the specification of a "splitting algorithm" σ : A → (A ′ , A ′′ ) such that (A ′ , A ′′ ) is a partition of A. We assume that, if |A| > 1, then the partition A, A ′′ is not trivial, i.e., neither set is empty.

Given σ , the top-down construction is as follows.

Algorithm 19.2 1. Start with the one-node partition tree T 0 = (T ).

2. Assume that at a given step of the algorithm, the current partition is T .

3. If T is complete, stop the algorithm.

## For each terminal node

$v in T such that |A v | > 1, compute (A ′ v , A ′′ v ) = σ (A v ) and add two children v ′ and v ′′ to v with A v ′ = A ′ v and A v ′′ = A ′′ v . 5. Return to step 2.$The division of a set into two parts is itself a clustering algorithm, and one may apply any of those described in the rest of this chapter.

## Thresholding

Once a complete hierarchy is built, it provides a complete binary partition tree T . This tree provides in turn a collection of partitions of V , each of them obtained through pruning. We now formalize this operation. Let V T denote the set of terminal nodes in T and V 0 = V \ V T contain the interior nodes. Define a pruning set to be a subset D ⊂ V 0 that contains no pair of nodes v, v ′ such that v ′ is a descendant of v. To any pruning set D, one can associate the pruned subtree T (D) of T consisting of T from which all the vertices that are descendants of elements of D are removed. From any such pruned subtree, one obtain a partition S(D) of T formed by the collection of sets A v for v in the terminal nodes of T (D). Between the extreme case S(v 0 ) = {V } (where v 0 is the root of T ) and S(∅) = ({x}, x ∈ V T ), there exists a huge number of possible partitions obtained in this way.

It is often convenient to organize these partitions according to the level sets of a well-chosen score function v → h(v) defined over V 0 . For D ⊂ V , we denote by max(D) the set of its deepest elements, i.e., the set formed by those v ∈ D that have no descendant in D. Then, for any λ ∈ R, one can define

$D + λ = max {v : h(v) ≥ λ} (resp. D - λ = max {v : h(v) ≤ λ}) and the associated partition S(D + λ ) (resp. S(D - λ ))$. The score function h can be linked to the construction algorithm. For example, if one uses a bottom-up construction using an extended dissimilarity ϕ, one can associate to each node v with v ∈ V 0 the value of ϕ(A v ′ , A v ′′ ) where v ′ and v ′′ are the children of v.

Another way to define such scores functions is by assigning weights to edges in T . Indeed, given a collection w of positive numbers w(v, v ′ ) for v → v ′ in T , one can define a score h w recursively by letting h w (v 0 ) = 0 and

$h w (v ′ ) = h w (v) + w(v, v ′ ) if v ′ is a child of v. The choice w(v, v ′ ) = 1 for all v, v ′$provide the usual notion of depth in the tree. Scores can also be built bottom-up, letting h(v) = 0 for terminal nodes and, for

$v ∈ V 0 , h w (v) = max(h w (v ′ ) + w(v, v ′ ), h w (v ′′ ) + w(v, v ′′ ))$where v ′ , v ′′ are the children of v Here, taking w = 1 provides the height of each node.

## K-medoids and K-mean 19.3.1 K-medoids

One of the limitation of hierarchical clustering is that it is a greedy approach that does not optimize a global quality measure associated to the partition. Such qual-ity measures can indeed be defined based on the heuristic that clusters should be homogeneous (for some criterion) and far apart from each other.

In centroid-based methods, the homogeneity criterion is the minimum, over all possible points in R, of the sum of dissimilarities between elements of the cluster and that point. More precisely, for any A ⊂ R, and any dissimilarity measure α, define the central dispersion index

$V α (A) = inf        x∈A α(x, c) : c ∈ R        . (19.2)$If c achieves the minimum in the definition of V α , it is called a centroid of A for the dissimilarity α.

The most common choice is α = ρ 2 , where ρ is a metric on R, and in this case, we will just use V in place of V ρ 2 . Note also that it is always possible to limit R to the training set T , in which case the optimization in (19.2) is over a finite number of centers. This makes centroid-based methods also applicable to the situation when the matrix of dissimilarities is the only input provided to the algorithm, or when the set R and the function α are too complex for the optimization in (19.2) to be feasible.

A centroid, c, in (19.2) may not always exists, and when it exists it may not always be unique. For α = ρ 2 , a point c such that

$V (A) = x∈A ρ 2 (x, c)$is called a Fréchet mean of the set A. Returning to the examples provided in the beginning of this chapter, two antipodal points on the sphere (whose distance is π) have an infinity of Fréchet means (or midpoints in this case) provided by every point in the equator between them. In contrast, the example provided with symmetric matrices provides a so-called Hadamard space [[44]](#b62) and the Fréchet mean in that case is unique. Of course, for Euclidean metrics, the Fréchet mean is just the usual one.

Returning to our general discussion, the K-medoids method optimizes the sum of central dispersions with a fixed number of clusters. Note that the letter K in Kmedoids originally refers to this number of clusters, but this notation conflicts with other notation in this book (e.g., reproducing kernels) and we shall denote by p this target number [1](#foot_18) . So the K-medoids method minimizes

$W α (A 1 , . . . , A p ) = p i=1 V α (A i ) over all partitions A 1 , . . . , A p of the training set T . Equivalently, it minimizes W α (A 1 , . . . , A p , c 1 , . . . , c p ) = p i=1 x∈A i α(x, c i ) (19.3)$over all partitions of T and c 1 , . . . , c p ∈ R. Finally, taking first the minimum with respect to A i , which corresponds to associating each x to the subset with closest center, K-medoids, an equivalent formulation minimizes

$Wα (c 1 , . . . , c p ) = x∈T min α(x, c i ), i = 1, . . . , p .$The standard implementation of K-medoids solves this problem using an alternate minimization, as defined in the following algorithm.

## Algorithm 19.3 (K-medoids)

Let T ⊂ R be the training set. Start with an initial choice of c 1 , . . . , c p ∈ R and iterate over the following two steps until stabilization:

(1) For i = 1, . . . , p, let A i contain points x ∈ T such that α(x, c i ) = min{α(x, c j ), j = 1, . . . , p}. In case of a tie in this minimum, assign x to only one of the tied sets (e.g., at random) to ensure that A 1 , . . . , A p is a partition.

(2) For i = 1, . . . , p, let c i be a minimizer of x∈A i α(x, c i ) if A i is not empty, or c i be a random point in T otherwise.

It should be clear that each step reduces the total cost W α and that this cost should stabilize at some point (which provides the stopping criterion) because there is only a finite number of possible partitions of T . However, there can be many possible limit points that are stable under the previous iterations, and some may correspond to poor "local minima" of the objective function. Since the end-point of the algorithm depends on the initialization, this step requires extra care. One may design ad-hoc heuristics in order to start the algorithm with a good initial point that is likely to provide a good solution at the end. These heuristics may depend on the problem at hand, or use a generic strategy. As a common example of the latter, one may ensure that the initial centers are sufficiently far apart by picking c 1 at random, c 2 as far as possible from c 1 , c 3 maximizing the sum of distances to c 1 and c 2 etc. One also typically runs the algorithm several times with random initial conditions and select the best solution over these multiple runs.

The second step of Algorithm 19.3 can be computationally challenging depending on the set R and the dissimilarity measure α. When R = R d and α = ρ[foot_19](#foot_19) is the square Euclidean distance, the solution is explicit and c i is simply the average of all points in A i . The resulting algorithm is the original incarnation of K-medoids, and called K-means [[182,](#b200)[121,](#b139)[124]](#b142). K-means is probably the most popular clustering method and is often a step in more advanced approaches, as we will discuss later. The two steps of Algorithm 19.3 are then simplified as follows. (1

$) For i = 1, . . . , p, let A i contain points x ∈ T such that |x -c i | 2 = min{|x -c j | 2 , j = 1, . . . , p}.$In case of tie in this minimum, assign x to only one of the tied sets (e.g., at random) to ensure that A 1 , . . . , A p is a partition.

(2) For i = 1, . . . , p, let

$c i = 1 |A i | x∈A i$x if A i is not empty, or c i be a random point in T otherwise.

## Mixtures of Gaussian and deterministic annealing

Mixtures of Gaussian (MoG) were discusssed in chapter 16 and in Algorithm 16.2. Recall that they model the observed data X together with a latent class variable Z ∈ {1, . . . , p} with joint distribution

$f (x, z; θ) = (2π) -d 2 (det Σ z ) -1 2 α z e -1$where θ contains the weights, α 1 , . . . , α p , the means, c 1 , . . . , c p and the covariance matrices Σ 1 , . . . , Σ p (we create, hopefully without risk of confusion, a short-lived conflict of notation between the weights and the dissimilarity function). The posterior class probabilities

$f Z (i|x ; θ) = (det Σ i ) -1 2 α i e -1 2 (x-c i ) T Σ -1 i (x-c i ) p j=1 (det Σ j ) -1 2 α j e -1 2 (x-c j ) T Σ -1 j (x-c j )$, i = 1, . . . , p, which are computed in step 3 of Algorithm 16.2 can be interpreted as a likelihood that observation x belongs to group i. As a consequence, the mixture of Gaussian algorithm can also be seen as a clustering method, in which one assigns each x ∈ T to cluster i when i = argmax{f Z (j|x, θ) : j = 1, . . . , p}, making an arbitrary decision in case of a tie.

In the special case in which all variances are fixed and equal to σ 2 Id R d , and all prior class probabilities are equal to 1/p (see remark 16.3), the EM algorithm for mixtures of Gaussian is also called "soft K-means", because it replaces the "hard" cluster assignments in K-means by "soft" ones represented by the update of the posterior distribution. We repeat its definition here for completeness (where θ = (c 1 , . . . , c p )).

## Algorithm 19.5 (Soft K-means)

1. Choose a number σ 2 > 0, a small constant ϵ and a maximal number of iterations M. Initialize the centers c = (c 1 , . . . , c p ).

2. At step n of the algorithm, let c be the current centers.

## Compute, for x ∈ T and i

$= 1, . . . , p f Z (i|x, θ) = e -1 2σ 2 |x-c i | 2 p j=1 e -1 2σ 2 |x-c j | 2$and let ζ i = N k=1 f Z (i|x, θ), i = 1, . . . , p. 4. For i = 1, . . . , p, let

$c ′ i = 1 ζ i x∈T xf Z (i|x, θ).$5. If |c ′ -c| < ϵ or n = M: stop the algorithm.

6. Replace c by c ′ and n by n + 1 and return to step 2.

When σ 2 → 0, f Z ( • |x k , θ) converges to the uniform probability on indexes j such that c j is closest to x k , which is a Dirac measure unless there are ties. Class allocation and center updating become then asymptotically identical to the K-means algorithm. A variant of soft K-means, called deterministic annealing [[169]](#b187), applies Algorithm 19.5 while letting σ slowly tend to 0. This new algorithm is experimentally more robust than K-means, in that it is less likely to be trapped in bad local minimums.

## Remark 19.4

The soft K-means algorithm can also be defined directly as an alternate minimization method for the objective function

$F(c, f Z ) = 1 2 x∈T p j=1 f Z (j|x)|x -c j | 2 + σ 2 x∈T p j=1 f Z (j|x) log f Z (j|x),$with the constraints f Z (j|x) ≥ 0 for all j and x and p j=1 f Z (j|x) = 1. One can check (we leave this as an exercise) that Step 3 in Algorithm 19.5 provides the optimal f Z for F when c is fixed, and that Step 4 gives the optimal c when f Z is fixed (see ??). ♦ Remark 19.5 We note that, if a K-means, soft K-means or MoG algorithm has been trained on a training set T , it is then easy to assign a new sample x to one of the clusters. Indeed, for K-means, it suffices to determine the center closest to x, and for the other methods to maximize f Z (j| x, θ), which is computable given the model parameters. In contrast, there was no direct way to do so using hierarchical clustering. ♦

## Kernel (soft) K-means

We now consider the soft K-means algorithm in feature space, and introduce features h k = h(x k ) in an inner product space H such that ⟨h k , h l ⟩ H = K(x k , x l ) for some positive definite kernel. As usual, the underlying assumption is that the computation of h(x) does not need to be feasible, while evaluations of K(x, y) are easy. Let us consider the minimization of

$1 2 x∈T p j=1 f Z (j|x)∥h(x) -c j ∥ 2 H + σ 2 x∈T p j=1 f Z (j|x) log f Z (j|x)$for some σ 2 > 0 (kernel K-means corresponds to taking the limit σ 2 → 0). Given f Z , the optimal centers are

$c j = 1 ζ j x∈T f Z (j|x)h(x)$with ζ = x∈T f Z (j|x). They belong to the feature space, H, and are therefore not computable in general. However, the distance between them and a point h(y) ∈ H is explicit and given by

$∥h(y) -c j ∥ 2 H = K(y, y) - 2 ζ j x∈T f Z (j|x)K(y, x) + 1 ζ 2 j x,x ′ ∈T f Z (j|x)f Z (j|x ′ )K(x, x ′ ).$The class probabilities at each iteration can therefore be updated using

$f Z (j|x) = e -∥h(x)-c j ∥ 2 H / 2σ 2 p j ′ =1 e -∥h(y)-c j ′ ∥ 2 H / 2σ 2 .$This yields the soft kernel K-means algorithm, that we repeat below.

## Algorithm 19.6 (Kernel soft K-means)

Let T ⊂ R d be the training set. Initialize the algorithm with some choice for f Z (j|x), j = 1, . . . , p, x ∈ T (for example: f Z (j|x) = 1/p for all j and x).

(1) For j = 1, . . . , p and x ∈ T compute

$∥h(x) -c j ∥ 2 H = K(x, x) - 2 ζ j x ′ ∈T f Z (j|x ′ )K(x, x ′ ) + 1 ζ 2 j x ′ ,x ′′ ∈T f Z (j|x ′ )f Z (j|x ′′ )K(x ′ , x ′′ ) with ζ j = x ′ ∈T f Z (j|x ′ ).$(2) Compute, for x ∈ T and j = 1, . . . , p,

$f Z (j|x) = e -∥h(x)-c j ∥ 2 H /2σ 2 p j ′ =1 e -∥h(y)-c j ′ ∥ 2 H /2σ 2 .$(3) If the variation of f Z compared to the previous iteration is small, or if a maximum number of iterations has been reached, exit the algorithm.

(4) Return to step 1.

After convergence, the clusters are computed by assigning x to A i when i = argmax{f Z (j|x) : j = 1, . . . , p}, making an arbitrary decision in case of a tie.

For "hard" K-means (with σ 2 → 0), step 2 simply updates f Z (j|x) as the uniform probability on the set of indexes j at which ∥h(x)c j ∥ 2 H is minimal.

## Convex relaxation

We return to the initial formulation of K-means for Euclidean data, as a minimization, over all partitions A = {A 1 , . . . , A K } of {1, . . . , N } of

$W (A) = K j=1 k∈A j |x k -c j | 2$where c j is the average of the points x j such that j ∈ A j . We start with a simple transformation expressing this function in terms of the matrix S α of square distances

$α(x k , x l ) = |x k -x l | 2$. Indeed, we have

$k∈A j |x k -c j | 2 = k∈A j |x k | 2 - 1 |A| k∈A j x k = k∈A j |x k | 2 - 1 |A| k,l∈A j x T k x l = 1 2|A j | k,l∈A j (|x k | 2 + |x l | 2 -2x T k x l ) = 1 2|A j | k,l∈A j |x k -x l | 2$Introduce the vector u j ∈ R N with coordinates u

$(k) j = 1/ |A j | for k ∈ A j and 0 other- wise. Then 1 2|A j | k,l∈A j |x k -x l | 2 = 1 2 u T j S α u J = 1 2 trace(S α u j u T j ) . (19.4) Let Z(A) = p j=1 u j u T j ,$so that Z(A) has entries Z (k,l) (A) = 1/|A j | for k, l ∈ A j , j = 1, . . . p and 0 for all other k, l. Summing (19.4) over j, we get

$W (A) = 1 2 trace(S α Z(A)).$The matrix Z(A) is symmetric, has non-negative entries. It moreover satisfies Z(A)1 N = 1 N and Z(A) 2 = Z(A). Interestingly, these properties characterize matrices Z associated with partitions, as stated in the next proposition [[153,](#b171)[152]](#b170).

Proposition 19.6 Let Z ∈ M N (R) be a symmetric matrix with non-negative entries satisfying Z1 N = 1 N and Z 2 = Z. The there exists a partition A of {1, . . . , N } such that

$Z = Z(A).$Proof Note that Z being symmetric and satisfying Z 2 = Z imply that it is an orthogonal projection with eigenvalues 0 and 1. In particular Z is positive semidefinite. This implies that, for all i, j ∈ {1, . . . , N }, one has

$Z(i, j) 2 ≤ Z(i, i), Z(j, j).$This inequality combined with N j=1 Z(k, j) = 1 (expressing Z1 N = 1 N ) shows that all diagonal entries of Z are positive.

Define on {1, . . . , N } the relation k ∼ j if and only if Z(j, k) > 0. The relation is symmetric and we just checked that k ∼ k for all k. It is also transitive, from the relation (deriving from

$Z 2 = Z) Z(k, j) = N i=1 Z(k, i)Z(i, j)$which shows (since all terms in the sum are non-negative) that k ∼ i and j ∼ i imply k ∼ j.

Let A = {A 1 , . . . , A q } be the partition of {1, . . . , N } formed by the equivalence classes for this relation. We now show that Z = Z(A).

We have, for all k, j ∈ {1, . . . , N

$} N i=1 Z(k, i)(Z(k, j) -Z(i, j)) = Z(k, j) N i=1 Z(k, i) - N i=1 Z(k, i)Z(i, j) = Z(k, j) - N i=1 Z(k, i)Z(i, j) = 0$Now, if k, j ∈ A s for some s, the identity reduces to i∈A s Z(k, i)(Z(k, j) -Z(i, j)) = 0.

(19.5)

Choose k such that Z(k, k) = max{Z(i, i) : i ∈ A s }. Then, for all i, j ∈ A s , Z(i, j) ≤ Z(i, i)Z(j, j) ≤ Z(k, k) and (19.5) for j = k yields

$i∈A s Z(k, i)(Z(k, k) -Z(k, i)) = 0, which is only possible (since all Z(k, i) are positive) if Z(k, i) = Z(k, k) for all i ∈ A s . From Z(k, i) ≤ Z(i, i)Z(k, k), we get Z(i, i) = Z(k, k$) for all i, and therefore (reapplying what we just found to i insteand of k) Z(i, j) = Z(i, i) = Z(k, k) for all i, j ∈ A s . Finally, we have 1 =

$i∈A s Z(k, i) = |A s |Z(k, k)$showing that Z(k, k) = 1/|A s | and completing the proof that Z = Z(A).

## ■

Note that the number of clusters, |A| is equal to the trace of Z(A). This shows that minimizing W (A) over partitions with p clusters is equivalent to the constrained optimization problem minimizing G(Z) = trace(S α Z) [(19.6)](#) over all matrices Z such that Z ≥ 0, Z T = Z, Z1 N = 1 N , trace(Z) = p and Z[foot_20](#foot_20) = Z.

This is still a difficult problem, since it is equivalent to K-means, which is NP hard.

Seeing the problem in this form, however, is more amenable to approximations and, in particular, convex relaxations.

In [[152]](#b170), it is proposed to use a semidefinite program (SDP) as a relaxation. The conditions Z = Z T and Z 2 = Z require that all eigenvalues of Z are either 0 or 1, and a direct relaxation is to replace these constraints by Z T = Z and 0 ⪯ Z ⪯ Id R N . The last inequality is however redundant if we add the conditions 2 Z ≥ 0 and Z1 = 1. This is a consequence of the Perron-Frobenius theorem which states that a matrix Z with positive entries has a largest (in modulus) real eigenvalue, which has multiplicity one and is associated with an eigenvector with positive coordinates, the latter eigenvector being (up to multiplication by a constant) the unique eigenvector of Z with positive coordinates. So, if a matrix Z is symmetric, satisfies Z > 0 and Z1 N = 1 N , then Z ⪯ Id R N . Applying this result to Z = (1ϵ)Z + (ϵ/N )1 N 1 T N and letting ϵ tend to 0 shows that any matrix Z with non-negative entries satisfying

$Z1 N = 1 N also satisfies Z ⪯ Id R N .$This provides the following SDP relaxation of K-means [[152]](#b170): minimize

$G(Z) = trace(S α Z) (19.7) subject to Z T = Z, Z1 N = 1 N , trace(Z) = p, Z ≥ 0, Z ⪰ 0.$Clusters can be immediately inferred from the columns of the matrix Z(A), since they are identical for two indices in the same cluster, and orthogonal to each other for two indices in different clusters. Let These properties will not necessarily be satisfied by a solution, say, Z * , of the SDP relaxation, but, assuming that the approximation is good enough, one may still consider the normalized columns of Z * and expect them to be similar for indices in the same cluster, and away from each other otherwise. Denoting by z * 1 , . . . , z * N these normalized columns, one can then run on them the standard K-means algorithm, or a spectral clustering method such as those described in the next sections, to infer clusters.

Remark 19.7 Clearly, one can use any symmetric matrix S in the definition of G in [(19.6)](#) and [(19.7)](#). The method is equivalent to, or to a relaxation of, K-means only when S is formed with squared norms in inner-product spaces, which does include kernel K-means, for which

$α(x k , x l ) = K(x k , x k ) -2K(x k , x l ) + K(x l , x l ).$If α is an arbitrary discrepancy measure, the minimization of G(Z) still makes sense, since it is equivalent to minimizing

$G(Z(A)) = p j=1 D α (A j ).$where

$D α (A) = 1 |A|$x,y∈A α(x, y) . [(19.8)](#) is a (normalized) measure of size, that we will call the α-dispersion of a finite set A.♦ Remark 19.8 Instead of using dissimilarities, some algorithms are more naturally defined in terms of similarities. Given such a similarity measure, say, β, one must maximize rather than minimize the index ∆ β (which becomes, rather than a measure of dispersion, a measure of concentration).

One passes from a dissimilarity α to a similarity β by applying a decreasing function to the former, a common choice being

$β(x, x ′ ) = exp(-α(x, x ′ )/τ)$for some τ > 0.

Alternatively, one can fix an element x 0 ∈ R and let β(x, y) = α(x, x 0 ) + α(y, x 0 )α(x, y)α(x 0 , x 0 ), (note that the last term, α(x 0 , x 0 ) is generally equal to 0). For example, if α(x, y) = |x -y| 2 , then β(x, y) = 2(xx 0 ) T (yx 0 ) (for which it is natural to take x 0 = 0). If α is a distance (not squared!), then β ≥ 0 by the triangular inequality. In this case, we have

$∆ β (A 1 , . . . , A p ) = n k=1 D β (A k ) = p k=1 1 |A p | x,y∈A k α(x, x 0 ) + p k=1 1 |A p | x,y∈A k α(y, x 0 ) - p k=1 1 |A p | x,y∈A k α(x 0 , x 0 ) - p k=1 1 |A p | x,y∈A k α(x, x 0 ) = 2 p k=1 x∈A k α(x, x 0 ) - p k=1 |A k |α(x 0 , x 0 ) -∆ α (A 1 , . . . , A p ) = 2 x∈T α(x, x 0 ) -|T |α(x 0 , x 0 ) -∆ α (A 1 , . . . , A p )$♦ so that minimizing ∆ α is equivalent to maximizing ∆ β .

## Spectral clustering 19.4.1 Spectral approximation of minimum discrepancy

One refers to spectral methods algorithms that rely on computing eigenvectors and eigenvalues (the spectrum) of data-dependent matrices. In the case of minimizing discrepancies, they can be obtained by further simplifying (19.7), essentially by removing constraints.

One indeed gets a simpler problem if the non-negativity constraint, Z ≥ 0, is removed. Doing so, one cannot guarantee anymore that Z ⪯ Id R N , so we need to reinstate this constraint. We will first make the further simplification to remove the constraint Z1 N = 1 N , the problem becoming minimizing trace(S α Z) over all Z ∈ S + N (R) such that 0 ⪯ Z ⪯ Id R N and trace(Z) = p. Decomposing Z in an eigenbasis, i.e., looking for it in the form

$Z = N j=1 ξ j e j e T j ,$this is equivalent to minimizing N j=1 ξ j e T j S α e j [(19.9)](#) subject to 0 ≤ ξ j ≤ 1, N j=1 ξ j = p and u 1 , . . . , u N orthonormal basis of R N . First consider minimization with respect to the basis, fixing ξ. There is obviously no loss of generality in requiring that ξ 1 ≤ ξ 2 ≤ • • • ≤ ξ N , and using corollary 2.4 (adapted to minimizing (19.9) rather than maximizing it) we know that an optimal basis is given by the eigenvectors of S α , ordered with non-decreasing eigenvalues. Letting λ 1 ≤ • • • ≤ λ N denote these eigenvalues, we find that ξ 1 , . . . , x N must be a nondecreasing sequence minimizing The following algorithm (similar to that discussed in [[64]](#b82)) summarizes this discussion.

## Algorithm 19.7 (Spectral clustering: version 1)

Let S α be an N × N discrepancy matrix. Let p denote the number of clusters.

(1) Compute the eigenvectors of S α associated with the p smallest eigenvalues.

(2) Denoting these eigenvectors by e 1 , . . . , e p , define y 1 , . . . , y N ∈ R p by y This algorithm needs to be slightly modified if one also wants Z to satisfy Z1 = 1. In that case, 1 is one of the eigenvectors (with eigenvalue 1), and the others are orthogonal to it. As a consequence, one now looks for Z in the form

$Z = N -1 k=1 ξ j e j e T j + 1 N 11 T$leading to the minimization of

$N -1 j=1$ξ j e T j S α e j + 1 N 1 T S α 1 over all ξ 1 , . . . , ξ N -1 such that 0 ≤ ξ j ≤ 1 and N j=1 ξ j = p -1, and over all e 1 , . . . , e N -1 such that e 1 , . . . , e N -1 , 1/ √ N form an orthonormal basis. The main difference with the previous problem is that we now need to ensure that all e j are perpendicular to 1.

To achieve this, introduce the projection matrix P = Id R N -11 T /N and let Sα = P S α P . Then, since u T 1 = 0 implies u T Sα u = u T S α u, it is equivalent to minimize N -1 j=1 ξ j e T j Sα e j over all ξ 1 , . . . , ξ N -1 such that 0 ≤ ξ j ≤ 1 and N j=1 ξ j = p -1, and over all e 1 , . . . , e N -1 such that e 1 , . . . , e N -1 , 1/ √ N form an orthonormal basis. Because Sα 1 = 0, we know that Sα can be diagonalized in an orthonormal basis (e 1 , . . . , e N -1 , 1/ √ N ), and we obtain an optimal solution by selecting the p -1 vectors associated with smallest eigenvalues, with associated ξ j = 1. We therefore get a modified version of the spectral clustering algorithm.  

## Graph partitioning

Similarity measures are often associated with graph structures, with a goal of finding a partition of their set of vertices. So, let T denote the set of these vertices and assume that to all pairs x, y ∈ T , one attribute a weight given by β(x, y), where β is assumed to be non-negative. We define β for all x, y ∈ T , but we interpret β(x, y) = 0 as marking the absence of an edge between x and y. Let V denote the vector space of all functions f : T → R (we have dim(V ) = |T |).

This space can be equipped with the standard Euclidean norm, that we will call in this section the L 2 norm (by analogy with general spaces of square integrable functions), letting,

$|f | 2 2 = x∈T f (x) 2 .$One can also associate a measure of smoothness for a function f ∈ V by computing the discrete "H 1 " semi-norm,

$|f | 2 H 1 = x,y∈T β(x, y)(f (x) -f (y)) 2 .$With this definition, "smooth functions" tend to have similar values at points x, y in T such that β(x, y) is large while there is less constraint when β(x, y) is small. In particular, |f | H 1 = 0 if and only if f is constant on connected components of the graph. [3](#foot_21)The notion of connected components, combined with thresholding, can be used to build a hierarchical family of partitions of the graph. Define, for all t > 0, the thresholded weights β (t) (x, y) = max(β(x, y)t, 0). The set of connected components associated with the pair (V , β (t) ) forms a partition, say, A (t) , of T . The resulting set of partitions is nested in the sense that, if s < t, the sets forming the partition A (s) are unions of sets forming A (t) . This thresholding procedure is not always satisfactory, however, because there does not always exist a fixed value of t that produces a good quality cluster decomposition.

If there exists p connected components, then the subspace of all functions f ∈ V such that |f | H 1 = 0 has dimension p. If C 1 , . . . , C p are the connected components, this space is generated by the functions δ C k , k = 1, . . . , p, with δ C k (x) = 1 if x ∈ C k and 0 otherwise. These functions form, in addition, an orthogonal system for the Euclidean inner product:

$⟨δ C k , δ C l ⟩ 2 = 0 if k l.$One can write 1  2 |f | 2 H 1 = f T Lf where L, called the Laplacian operator associated to the considered graph, is defined by

$Lf (x) = y∈T L(x, y)f (y) and L(x, y) =        z∈T β(x, z)        1 x=y -β(x, y). (19.10)$The vectors δ C k , k = 1, . . . , p are then an orthogonal basis of the null space of L. Conversely, let (e 1 , . . . , e p ) be any basis of this null space. Then, there exists an invertible matrix A = (a ij , i, j = 1, . . . , p) such that

$e i (x) = p j=1 a ij δ C j (x). Associate to each x ∈ T the vector e(x) =           e 1 (x) . . . e p (x)           ∈ R p .$Then, for any x, y ∈ T , we have e(x) = e(y) if and only if δ C j (x) = δ C j (y) for all j = 1, . . . , p (because A is invertible), that it, if and only if x and y belong to the same connected component. So, given any basis of the null space of L, the function x → e(x) determines these connected components. So, a-not very efficient-way of determining the connected components of the graph can be to diagonalize the operator L (written as an N by N matrix, where N = |T |), extract the p eigenvectors e 1 , . . . , e p associated with eigenvalue zero and deduce from the function e(x) above the set of connected components. Now, in practice, the graph associated to T and β will not separate nicely into connected components in order to cluster the training set. Most of the time, because of noise or some weak connections, there will be only one such component, or in any case much less than what one would expect when clustering the data. The previous discussion suggests, however, that in the presence of moderate noise in the connection weights, one may expect that the eigenvectors associated to the p smallest eigenvalues of L provide vectors e(x), x ∈ T such that e(x) and e(y) have similar values if x and y belong to the same cluster (see [19.2)](#). In such cases, these clusters should be easy to determine using, say, K-means on the transformed dataset T = (e(x), x ∈ T ). This is summarized in the following algorithm. Algorithm 19.9 (Spectral Graph Partitioning) Let T ⊂ R be the training set and (x, y) → β(x, y) a similarity measure defined on T × T . Let p be the desired number of clusters.

(1) Form the Laplacian operator described in [(19.10)](#) and let e 1 , . . . , e p be its eigenvectors associated to the p lowest eigenvalues. For x ∈ T , let e(x) ∈ R p be given by e(x) = (e 1 (x), . . . , e p (x)) T ∈ R p .

(2) Apply the K-means algorithm (or one of its variants) with p clusters to T = (e(x), x ∈ T ).

## Deciding the number of clusters 19.6.1 Detecting elbows

The number, p, of subsets with respect to which the population should be partitioned is rarely known a priori, and several methods have been introduced in the literature in order to assess the ideal number of clusters. We now review some of these methods, and denote, for this purpose, by L * (p) the minimized cost function obtained with p clusters, e.g., using (19.  in the case of K-medoids (this definition is algorithm dependent). It is clear that L * is a decreasing function of p. It is also natural to expect that L * should decrease significantly when p is smaller than the correct number of clusters, while the variation should be more marginal when p is overestimated, because the cost in putting together two sets of points that are far apart (which happens when p is too small) is typically larger than the gain in splitting a homogeneous region in two.

The simplest approach in this context is to visualize L * (p) as a function of p and try to locate at which value the resulting curve makes an "elbow," i.e., switches from a sharply decreasing slope to a milder one. Figure [19](#fig_82).3 provides an illustration of this visualization when the true number of clusters is three (the data in each cluster following a normal distribution). When the clusters are well separated, an elbow clearly appears on the graph of Γ * α , but this situation is harder to observe when clusters overlap with each other.

One can measure the "curvature" at the elbow using the distance between each point in the graph of (p, W * α (p)) and the line between its predecessor and successor. The result gives the criterion

$C(p) = L * (p + 1) + L * (p -1) -2L * (p) (L * (p + 1) -L * (p -1)) 2 + 4 ,$specifying the elbow point as the value of p at which C attains its maximum. For both examples in fig. [19](#fig_82).3, this method returns the correct number of clusters (3).

## The Cali ński and Harabasz index

Several other criteria have been introduced in the literature. Cali ński and Harabasz [[46]](#b64) propose to minimize the ratio of normalized between-group and within-groups sums of squares associated with K-means. For a given p, let c 1 , . . . , c p denote the optimal centers, and A 1 , . . . , A p the optimal partition, with N k = |A k |. The normalized between-group sum of squares is

$h α (p) = 1 p -1 p k=1 N k |c k -x| 2$and the normalized within-group sum of squares is

$w α (p) = 1 N -p p k=1 x∈A k |x -c k | 2$Cali ński and Harabasz [[46]](#b64) suggest to maximize γ CH (p) = h α (p)/w α (p). This criterion can be extended to other types of cluster analysis. We have seen in section 19.4 that, when α(x, y) = |x -y| 2 , 1 2

$p k=1 x,y∈A k α(x, y)/N k = p k=1 x∈A k |x -c k | 2 .$We also have

$x∈T |x -x| 2 = p k=1 x∈A k |x -c k | 2 + p k=1 N k |c k -x| 2$and the left-hand side is also equal to 1 2N

x,y∈T α(x, y).

It follows that, when α(x, y) = |x -y| 2 ,

$h α (p) = 1 2(p -1)         1 N x,y∈T α(x, y) - p k=1 x,y∈A k α(x, y)/N k         and w α (p) = 1 2(N -p) p k=1 x,y∈A k α(x, y)/N k .$These expressions can obviously be applied to any dissimilarity measure, extending γ CH to general clustering problems.

## The "silhouette" index

For x ∈ T , let This index measures how well x is classified in the partitioning. It is large when the mean distance between x and other objects in its class is small compared to the minimum mean distance between x and any other class. In order to estimate the best number of clusters with this criterion, one then can maximize the average index:

$d α (x, A k ) = 1 N k y∈A k α(x, y).$$γ R (p) = 1 N x∈T s α (x, p).$Remark 19.9 One can rewrite the Cali ński and Harabasz index using the notation introduced for the silhouette index. Indeed, let A(x) be the cluster A k to which x belongs. Then

$h α (p) = 1 2(p -1) x∈T p k=1 N k N (d α (x, A k ) -d α (x, A(x)))$and Let us assume that c 1 , . . . , c p are uniformly spaced, so that the sets Āk have similar volumes (close to 1/p) and have roughly spherical shapes (see fig. [19.4](#fig_101)). This implies that

$w α (p) = 1 2(N -p) p k=1 x∈A k d α (x, A k ). ♦$$Āk |x -c k | 2 dx ≃ vol(A k ) r 2 p d d + 2$where r p is the radius of a sphere of volume 1/p, i.e., pr d p ≃ d/Γ d-1 where Γ d-1 is the surface area of the unit sphere in R d . So, we should have, for some constant C that only depends on d,

$x∈A k |x -c k | 2 ≃ N k Āk |x -c k | 2 dx ≃ C(d)(pN )p -2/d-1 = C(d)N p -2/d .$This suggests that, for fixed N and d, p 2/d L * (p) should vary slowly when p overesti-mate the number of clusters (assuming that this operation divides an homogeneous cluster). Based on this analysis, Krzanowski and Lai [[111]](#b129) introduced the differenceratio criterion, namely, Another similar approach, introduced by Sugar and James [[185]](#b203), is based on an analysis of mixtures of Gaussian, namely assuming an underlying model with p 0 groups, where data in group k follow a Gaussian distribution N (µ k , Id) (possibly after standardizing the covariance matrix). In that work, the authors show that, if d (the dimension) tends to infinity, with the minimal distance between centers growing proportionally to √ d, then L * (p)/d tends to infinity when p < p 0 . They also show that, with similar assumptions, L * (p)/d behaves like p -2/d for p ≥ p 0 , still for large dimensions. Based on this, they suggest using the criterion

$γ KL (p) = (p -1)$$γ SJ (p) = L * (p) d -ν - L * (p -1) d -ν$(with the convention that L * (0) = 0) for some positive number ν and select the value of p that maximizes γ SJ . Indeed, in the case of Gaussian mixtures, the choice ν = d/2 ensures that, in large dimensions, γ SJ (p) is small for p < p 0 , that it is close to 1 for p > p 0 and close to p 0 for p = p 0 .

A more computational approach, based on Monte-Carlo simulations has been introduced in Tibshirani et al. [[191]](#b209), defining the gap index

$γ T W H (p) = E(L * (p, T ♯ )) -L * (p, T )$where the L * (p, T ) denotes the optimal value of the optimized cost with p clusters for a training set T . The notation T ♯ represent a random training set, with same size and dimension as T , generated using an unclustered probability distribution used as a reference. In Tibshirani et al. [[191]](#b209), this distribution is taken as uniform (over the smallest hypercube containing the observed data), or uniform on the coefficients of a principal component decomposition of the data (see chapter 20). The expectation E(L * (p, T ♯ )) is computed by Monte-Carlo simulation, by sampling many realizations of the training set T , running the clustering algorithm for each of them and averaging the optimal costs. One can expect L * (p, T ) (for observed data) to decrease much faster (when adding a cluster) than its expectation for homogeneous data when p < p 0 , and the decrease of both terms to be comparable when p ≥ p 0 . So the number of clusters can in principle be estimated by detecting an elbow in the graph of γ T W H (p) as a function of p. The procedure suggested in Tibshirani et al. [[191]](#b209) in order to detect this elbow if to look for the first index p such that The main parameters in this model were the number of classes, p, and the probabilities α j associated to each cluster, and the parameter of the conditional distribution (e.g., N (c j , σ 2 Id R d )) of X conditionally to being in the jth cluster. In the approach we described, these parameters were estimated from data using maximum likelihood (through the EM algorithm) and probabilities f Z (j|x) were then estimated in order to compute the most likely clustering.We interpreted f Z (j|x) as the conditional probability P (Z = z|X = x), where Z ∈ {1, . . . , p} represents the group variable. The natural generative order is Z → X: first decide to which group the observation belongs to, then sample the value of X conditional to this group. Clustering is in this case reversing the order, i.e., computing the posterior distribution of Z given X.

In a Bayesian approach, the parameters p, α, c and σ 2 are also considered as random variables, so that (letting θ denote the vector formed by these parameters), the generative random sequence becomes θ → Z → X. Importantly, θ is assumed to be generated once for all, even if several samples of X are observed, yielding the generative sequence for an N -sample, θ → (Z 1 , . . . , Z N ) → (X 1 , . . . , X N ).

We use below underlined letters to denote configurations of points, Z = (Z 1 , . . . , Z N ), X = (X 1 , . . . , X N ), etc. We also use capital letters or boldface letters (for Greek symbols) to differentiate random variable from realizations.

Clusters are still evaluated based on the conditional distribution of Z given X, but this distribution must be evaluated by averaging the conditional distribution of   In this expression, P (θ)dθ implies an integration with respect to the prior distribution of the parameters. This distribution is part of the design of the method, but one usually chooses it so that it leads to simple computations, using so-called conjugate priors, which are such that posterior distributions belong to the same parametric family as the prior. For example, the conjugate prior for the mean of a Gaussian distribution (such as c i in our model) is also a Gaussian distribution. The conjugate prior for a scalar variance is the inverse gamma distribution, with density

$v u Γ (u) s -u-1 exp(-v/s)$for some parameters u, v. A conjugate prior for the class probabilities α = (α 1 , . . . , α p ) is the Dirichlet distribution, with density

$D(α 1 , . . . , α p ) = Γ (a 1 + • • • + a p ) Γ (a 1 ) • • • Γ (a p ) p j=1 α a j -1 j$on the simplex

$S p = {(α 1 , . . . , α p ) ∈ R p : α i ≥ 0, α 1 + • • • + α p = 1}.$Note that these conjugate priors have the same form (up to normalization) as the parametric model densities when considered as functions of the parameters.

## Model with a bounded number of clusters

We first discuss the Bayesian approach assuming that the number of clusters is smaller than a fixed number, p. In this example, we assume that c 1 , . . . , c p are modeled as independent Gaussian variables N (0, τ 2 Id R d ), σ 2 with an inverse gamma distribution with parameters u and v and (α 1 , . . . , α p ) using a Dirichlet distribution with parameters (a, . . . , a). Analytical example. The joint probability density of (X, Z) and θ is proportional to

$(σ 2 ) -u-1 e -v/σ 2 e - p j=1 |c j | 2 /2τ 2 p j=1 α a-1 j N k=1 e -|x k -c z k | 2 /2σ 2 (σ 2 ) d/2 N k=1 α z k = (σ 2 ) -u-dN /2-1 exp        -(v + 1 2 N k=1 |x k -c z k | 2 )/σ 2        p j=1 α a+N j -1 j .$One can explicitly integrate this last expression with respect to σ 2 and α, using the expressions of the normalizing constants in the inverse gamma and Dirichlet distributions, yielding (after integration and ignoring constant terms)

$Γ (a + N 1 ) • • • Γ (a + N p ) (v + 1 2 N k=1 |x k -c z k | 2 ) u+dN /2 exp         - p j=1 |c j | 2 /2τ 2         = Γ (a + N 1 ) • • • Γ (a + N p ) (v + 1 2 S w + 1 2 p j=1 N j |c j -xj | 2 ) u+dN /2 exp         - p j=1 |c j | 2 /2τ 2        $where S w = N k=1 |x k -xz k | 2 is the within group sum of squares. Note that this sum of squares depends on x and z, and that (N 1 , . . . , N p ), the group sizes, depend on z.

Let us assume a "non-informative prior" on the centers, which corresponds to letting τ tend to infinity and neglecting the last exponential. The remaining expression can now be integrated with respect to c 1 , . . . , c p by making a change of variables µ j = N j /(2v + S k )(c jx j ) and using the fact that

$(R d ) p dc 1 . . . dc p (v + 1 2 S w + 1 2 p j=1 N j |c j -xj | 2 ) u+dN /2 = (2v + S w ) (p-N )d/2-u) p j=1 N -d/2 j (R d ) p dµ 1 . . . dµ p ( 1 2 + 1 2 p j=1 |µ j | 2 ) u+dN /2$and the final integral does not depend on x or z. It follows from this that the conditional distribution of Z given x takes the form

$P (z|x) = C(x) p j=1 Γ (a + N j ) (2v + S w ) (N -p)d/2+u) p j=1 N d/2 j$where C(x) is a normalization constant ensuring that the right-hand side is a probability distribution over configurations z = (z 1 , . . . , z N ) ∈ {1, . . . , p} N . In order to obtain the most likely configuration for this posterior distribution, one should therefore minimize in z the function

$((N -p) d 2 + u) log(2v + S w ) + d 2 p j=1 log N j - p j=1 log Γ (a + N j ).$This final optimization problem cannot be solved in closed form, but this can be performed numerically. One can simplify it a little by only keeping the main order terms in the last two sums (using Stirling formula for the Gamma function) and minimize

$((N -p) d 2 + u) log(2v + S w ) - p j=1 (a + N j ) log(a + N j ).$This expression has a nice interpretation, since the first term minimizes the withingroup sum of squares, the same objective function as in K-means, and the second one is an entropy term that favors clusters with similar sizes.

Monte-Carlo simulation. An alternative to this analytical approach is to use Monte-Carlo simulations to estimate some properties of the posterior distribution numerically. While they are often computationally demanding, Monte-Carlo methods are more flexible and can be used in situations when analytic computations are intractable . In order to sample from the distribution of Z given x, it is actually easier to sample from the joint distribution of (Z, θ) given x, because this distribution has a simpler form. Of course, if the pair (Z, θ) is sampled from the conditional distribution given x, the first component, Z will follow the posterior distribution we are interested in.

In the context of the discussed example, this reduces to sampling from a distribution proportional to [.11)](#) Sampling from all these variables at once is not tractable, but it is easy to sample from them in sub-groups, conditionally to the rest of the variables. We can, for example, deduce from the expression above the following conditional distributions.

$(σ 2 ) -u-1 e -v/σ 2 e - p j=1 |c j | 2 /2τ 2 p j=1 α a-1 j N k=1 e -|x k -c z k | 2 /2σ 2 (σ 2 ) d/2 N k=1 α z k . (19$(i) Given (α, c, z), σ 2 follows an inverse gamma distribution with parameters u + dN /2 and v + 1

$2 N k=1 |x k -c z k | 2 .$(ii) Given (z, z, σ 2 ), α follows a Dirichlet distribution with parameters a+N 1 , . . . , a+ N p .

(iii) Given (z, σ 2 , α), c 1 , . . . , c p are independent and follow a Gaussian distribution, respectively with mean (1 + σ 2 /(N j τ 2 )) -1 xj and variance (N j /σ 2 + 1/τ 2 ) -1 .

(iv) Given (σ 2 , α, c), z 1 , . . . , z N are independent and

$P (z k = j|σ 2 , α, c, x) ∝ α j e -|x k -c j | 2 /2σ 2 .$Algorithm 19.10 (Gibbs sampling for mixture of Gaussian (Bayesian case))

(1) Initialize with variables α, c, σ and z, for example generated according to the prior distribution.

(2) Loop a large number of times over the following steps.

(i) Simulate a new value of σ 2 according to an inverse gamma distribution with parameters u + dN /2 and v + 1

$2 N k=1 |x k -c z k | 2 .$(ii) Simulate new values for α 1 , . . . , α p according to a Dirichlet distribution with parameters a + N 1 , . . . , a + N p .

(iii) Simulate new values for c 1 , . . . , c p independently, sampling c i according to a Gaussian distribution with mean (1 + σ 2 /(N j τ 2 )) -1 xj and variance (N j /σ 2 + 1/τ 2 ) -1 .

(iv) Simulate new values of z 1 , . . . , z N independently such that

$P (z k = j|σ 2 , α, c, x) ∝ α j e -|x k -c j | 2 /2σ 2 .$Note that this algorithm is only asymptotically providing a sample of the posterior distribution (it has to be stopped at some point, of course). Note also that, at each step, the labels z 1 , . . . , z N provide a random partition of the set {1, . . . , N }, and this partition changes at every step.

To estimate one single partition out of this simulation, several strategies are possible. Using the simulation, one can estimate the probability w kl that x k and x l belong to the same cluster. This can be dome by averaging the number of times that z k = z l was observed along the Gibbs sampling iterations (from which one usually excludes a few early "burn-in" iterations). These weights, w kl can then be used as similarity measures in a clustering algorithm.

Alternatively, one can average for each k, the values of the class center c z k associated to k, still along the Gibbs sampling iterations. These average values can then be used as input of, say, a K-means algorithm to estimate final clusters.

Mean-field approximation. We conclude this section with a variational Bayes approximation of the posterior distribution. We will make a mean-field approximation, in which all parameters and latent variables are independent, therefore approximating the distribution in [(19.11](#)) by a product distribution taking the form

$g(σ 2 , α, c, z) = g (σ 2 ) (σ 2 )g (α) (α) p j=1 g (c) j (c j ) N k=1 g (z) k (z k ).$Here c = (c 1 , . . . , c p ), z = (z 1 , . . . , z N ) and α = (α 1 , . . . , α p ). We have σ 2 ∈ (0, +∞), c ∈ (R d ) p , α ∈ S, the set of all non-negative α 1 , . . . , α p that sum to one, and z ∈ {1, . . . , p} N   (so that g

$(x)$k is a p.m.f. on {1, . . . , p}. We will use the discussion in section 16.3.3 and lemma 16.1, and use the notation introduced in that section to denote as ϕ the expectation a variable ϕ of the variables above for the p.d.f. g.

The log-likelihood for a mixture of Gaussian takes the form (ignoring contant terms)

$ℓ(σ 2 , α, c, z) = -(u + 1) log σ 2 -vσ -2 - 1 2τ 2 p k=1 |c j | 2 + p j=1 (a -1) log α j - N d 2 log σ 2 - 1 2 σ -2 N k=1 |x k -c z k | 2 + N k=1 log α z k = -(u + 1) log σ 2 -vσ -2 - 1 2τ 2 p k=1 |c j | 2 + p k=1 (a -1) log α j - N d 2 log σ 2 - 1 2 σ -2 N k=1 p j=1 |x k -c j | 2 1 z k =j + N k=1 p j=1 log α j 1 z j =k$and can therefore be decomposed as a sum of products of functions of single variables, as assumed in section 16.3.3. Using lemma 16.1, we can identify each of the distributions composing g, namely:

• g (σ 2 ) is the p.d.f. of an inverse gamma with parameters ũ = u + N d/2 and ṽ = ν + 1 2

$N k=1 p j=1 |x k -C j | 2 Z k = j . • g (c)$j is the p.d.f. of a Gaussian, with parameters N ( mj , σ 2 j Id R d ), with, letting

$ζ(j) = N k=1 Z k = j = N k=1 g (z) k (j), σ 2 j = 1 τ 2 + σ -2 ζ(j) -1$and mi = σ -2 σ 2 j N k=1 Z k = j x k . • g (α) of a Dirichlet distribution, with parameters ã1 , . . . , ãk , with ãi = a + ζ(j).

## • Finally g (z)

k is a p.m.f. on {1, . . . , p} with

$g (z) k (j) ∝ exp - 1 2 σ -2 |x k -C j | 2 + log α j .$To complete the consistency equations, it now suffices to evaluate the expectations in the formula above as functions of the other parameters. We leave to the reader the verification of the following statements.

• If σ 2 follows an inverse gamma distribution with parameters ũ and ṽ, then σ -2 = ũ/ ṽ.

$• If C j ∼ N ( mj , σ 2 j Id R d ), then |x k -C j | 2 = |x k -mj | 2 + d σ 2 j .$• If α follows a Dirichlet distribution with parameters ã1 , . . . , ãp , then log

$α j = ψ( ãj ) -ψ( ã1 + • • • + ãp )$where ψ is the digamma function (derivative of the logarithm of the gamma function).

Combining these facts with the expression of the mean-field parameters, we can now formulate a mean-field estimation algorithm for mixtures of Gaussian that iteratively applies the consistency equations.  (8) Compare the updated variables with their previous values and stop if the difference is below a tolerance level. Otherwise, return to (3).

$= 1 u + N d/2         v + 1 2 N k=1 p j=1 gk (j)|x k -mj | 2 + d 2 p j=1 σ 2 j ζ(j)         . (5$
## After convergence g (z)

k provides the mean-field approximation of the posterior probability of classes for observation k and can be used to determine clusters.

## Non-parametric priors

The Polya urn In the previous model with p clusters or less, the joint distribution of Z 1 , . . . , Z N is given by

$π(z 1 , . . . , z N ) = Γ (pa) Γ (a) p S p p j=1 α a+N j -1 j dα = Γ (pa) Γ (pa + N ) p j=1 Γ (a + N j ) Γ (a)$.

Conditional to z 1 , . . . , z N , the data model was completed by sampling p sets of parameters, say, θ 1 , . . . , θ p , each belonging to a parameter space Θ and following a prior probability distribution with density, say, ψ and variables X 1 , . . . , X N , where X k ∈ R was drawn according to a law dependent on its cluster, that we will denote ϕ(

$• | θ z k ).$The complete likelihood of the data is now

$L(z, θ, x) = Γ (pa) Γ (pa + N ) p j=1 Γ (a + N j ) Γ (a) p j=1 ψ(θ j ) N k=1 ϕ(x k |θ z k ).$Note that the right-hand side does not change if one relabels the values of z 1 , . . . , z N , i.e., if one replaces each z k by s(z k ) where s is a permutation of {1, . . . , p}, creating a new configuration denoted s • z. Let [z] denote the equivalence class of z, containing all z ′ = s • z, s ∈ S N : all the labelings in [z] provide the same partition of {1, . . . , N } and can therefore be identified. One defines a probability distribution π over these equivalence classes by letting

$π([z]) = |[z]| Γ (pa) Γ (pa + N ) p j=1 Γ (a + N j ) Γ (a)$.

The first term on the right-hand side is the number of elements in the equivalence class of [z]. To compute it, let p 0 = p 0 (z) denote the number of different values taken by z 1 , . . . , z N , i.e., the "true" number of clusters (ignoring the empty ones), which now is a function of z. Let A 1 , . . . , A p 0 denote the partition associated with z.

New labelings equivalent to z can be obtained by assigning any index i 1 ∈ {1, . . . , p} to elements of A 1 , then any index i 2 i 1 to elements of A 2 , etc., so that there are |[z]| = p!/(pp 0 )! choices. We therefore find:

$π([z]) = p! (p -p 0 )! Γ (pa) Γ (pa + N ) p j=1 Γ (a + N j ) Γ (a)$.

Letting λ = pa and using the formula Γ (x + 1) = xΓ (x), this can be rewritten as

$π([z]) = p(p -1) • • • (p -p 0 + 1) λ(λ + 1) . . . (λ + N -1) p j=1 N j -1 i=0 (λ/p + i).$Now, the class [z] contains exactly one element ẑ with the following properties

$• ẑ1 = 1,$• ẑk ≤ max(z j , j < k) + 1 for all k > 1.

This means that the kth label is either one of those already appearing in ( ẑ1 , . . . , ẑk-1 ) or the next integer in the enumeration. We will call such a ẑ admissible. If we assume that z is admissible in the expression of π, we can write

$π([z]) = p 0 j=1 λ(1 -j/p) N j -1 i=1 (λ/p + i) λ(λ + 1) . . . (λ + N -1)$.

If one takes the limit p → ∞ in this expression, one still gets a probability distribution on admissible labelings, namely π([z]) = λ p 0 p 0 j=1 (N j -1)! λ(λ + 1) . . . (λ + N -1)

. [(19.12)](#) Recall that, in this equation, p 0 is a function of z, equal, for admissible labelings, to the largest j such that N j > 0.

The probability π is generated by the following sampling scheme, called the Polya urn process simulating admissible labelings. Algorithm 19.12 (Polya Urn)

$1 Initialize k = 1, z 1 = 1, j = 1. Let N 1 = 1$2 At step k, assume that z 1 , . . . , z k have been generated, with associated number of clusters equal to j and N 1 , . . . , N j elements per cluster. Generate z k+1 such that Using this prior, the complete model for the distribution of the observed data is

$z k+1 =            i with probability N i λ + k , for i =$$L(z, θ, x) = λ p 0 p 0 j=1 (N j -1)! λ(λ + 1) . . . (λ + N -1) p 0 j=1 ψ(θ j ) N k=1 ϕ(x k |θ z k )$Recall that, in this expression, z is restricted to the set of admissible labelings. We also note that admissible labelings are in one-to-one correspondence with the partitions of {1, . . . , N }, so that the latent variable z in this expression can also be interpreted as representing a random partition of this set.

Dirichlet processes. As we will see later, the expression of the global likelihood and the Polya urn model will suffice for us to develop non-parametric clustering methods for a set of observations x 1 , . . . , x N . However, this model is also associated to an important class of random probability distributions (i.e., random variables taking values in some set of probability distributions) called Dirichlet processes for which we provide a brief description.

The distribution in [(19.12)](#) was obtained by passing to the limit from a model that first generates p numbers α 1 , . . . , α p , then generates the labels z 1 , . . . , z N ∈ {1, . . . , p} identified modulo relabeling. This distribution can also be defined directly, by first defining an infinity of positive numbers (α j , j ≥ 1) such that ∞ i=1 α i = 1, followed by the generation of random labels Z 1 , . . . , Z N such that P (Z k = j) = α j , followed once again with an identification up to relabeling.

The distribution of α that leads to the Polya urn is called the stick breaking process. This process is such that

$α j = U j j-1 i=1 (1 -U i )$where U 1 , U 2 , . . . is a sequence of i.i.d. variables following a Beta(1, λ) distribution, i.e., with p.d.f. λ(1u) λ-1 for u ∈ [0, 1]. The stick breaking interpretation comes from the way α 1 , α 2 , . . . can be simulated: let α 1 ∼ Beta(1, λ); given α 1 , . . . , α j-1 , let α j = (1 -α 1 -• • • -α j-1 )U j where U j ∼ Beta(1, λ) and is independent from the past. Each step can be thought of as breaking the remaining length, (1 -α 1 -• • • -α j-1 ), of an original stick of length 1 using a beta-distributed variable, U j . This process leads to the distribution (19.12) over admissible distributions, i.e., if α is generated according to the stick breaking process, and Z 1 , . . . , Z N are independent, each such that P (Z k = j) = α j , then the probability that (Z 1 , . . . , Z N ) is identical, after relabeling, to the admissible configuration z is given by [(19.12)](#). (We skip the proof of this result, which is not straightforward.) Now, take a realization α = (α 1 , α 2 , . . .) of the stick-breaking process, and independent realizations η = (η 1 , η 1 , . . .) drawn according to the p.d.f. ψ. Define ρ = ∞ j=1 α j δ η j . [(19.14)](#) For any realization of α and of η, ρ is a probability distribution on the parameter space Θ (in which one chooses η i with probability α i ). Since α and η are both random variables, this defines a random variable ρ with values in the space of probability measures on Θ.

This process has the following characteristic property. For any family V 1 , . . . , V k ⊂ Θ forming a partition of that set, the random variable (ρ(U 1 ), . . . , ρ(U k )) follows a Dirichlet distribution with parameters

$λ U 1 ψ dη, . . . , λ U 1 ψ dη .$This is the definition of a Dirichlet process with parameters (λ, ψ), or, simply, with parameter λψ. Conversely, one can also show that any Dirichlet process can be decomposed as in [(19.14)](#) where α is a stick-breaking process and η independent realizations of ψ.

Monte-Carlo simulation. The joint distribution of labels, parameters and observed variables can also be deduced from [(19.12)](#), with a joint p.d.f. given by

$λ p 0 -1 p 0 j=1 (N j -1)! (λ + 1) • • • (λ + N -1) p 0 j=1 ψ(η j ) N k=1 ϕ(x k |η z k ). (19.15)$The forward simulation of this distribution is a straightforward extension of Algorithm 19.12, namely:

$Algorithm 19.13 1 Initialize k = 1, z 1 = 1, j = 1. Let N 1 = 1.$2 Sample η 1 ∼ ψ and x 1 ∼ ϕ(•|η 1 ).

3 At step k, assume that z 1 , . . . , z k has been generated, with associated number of clusters equal to j and N 1 , . . . , N j elements per cluster. Generate z k+1 such that

$z k+1 =            i with probability N i λ + k , for i = 1, . . . , j j + 1 with probability λ λ + k 4 If z k+1 = i ≤ j, sample x k+1 ∼ ϕ( • |η i ). Replace N i by N i + 1, k by k + 1.$5 If z k+1 = j + 1, let N j+1 = 1, sample η j+1 ∼ ψ and x k+1 ∼ ϕ( • |η j+1 ). Replace j by j + 1 and k by k + 1.

6 If k < N , return to step 2, otherwise, stop. This algorithm cannot be used, of course, to sample from the conditional distribution of Z and η given X = x, and Markov-chain Monte-Carlo must be used for this purpose. In order to describe how Gibbs sampling may be applied to this problem, we use the fact that, as previously remarked, using admissible labelings z is equivalent to using partitions A = (A 1 , . . . , A p 0 ) of {1, . . . , N }, and we will use the latter formalism to describe the algorithm. We will also use the notation η A to denote the parameter associated to A ∈ A so our new notation for the variables is (A, η) where A is a partition of {1, . . . , N } and η is a collection (η A , A ∈ A) with η A ∈ Θ. Given this, we want to sample from a conditional p.d.f.

$Φ(A, η|x) ∝ λ |A|-1 A∈A (|A| -1)! (λ + 1) • • • (λ + N -1) A∈A ψ(η A ) k∈A ϕ(x k |η A ).(19.16)$As an additional notation, given a partition A and an index k ∈ {1. . . . , N }, we let A k denote the set A in A that contains k.

The following points are relevant for the design of the sampling algorithm.

(1) The conditional distribution of η given A and the training data is proportional to

$A∈A        ψ(η A ) k∈A ϕ(x k |η A )       $This shows that the parameters η A , A ∈ A are independent of each other, with η A following a distribution proportional to

$η → ψ(η) k∈A j ϕ(x k |η).$Sampling from this distribution generally offers no special difficulty, especially if the prior ψ is conjugate to ϕ. Importantly, one does not need to sample exactly from η A , and it is often more convenient to separate η A into several components (such as mean and variance for mixtures of Gaussian) and sample from them alternatively, creating another level of Gibbs sampling.

(2) We now consider the issue of updating A. We will use for this purpose the formalism of Algorithm 12.2. In particular, for each k ∈ {1, . . . , N }, we associate to the variable (A, η) the pair (A (k) , η (k) ), where A (k) is the partition of {1, . . . , N } \ {k} formed by the sets A (k) = A \ {k} and η (k) A = η A , unless A = {k}, in which case the set and the corresponding η A are dropped.

We can write Φ(A, η|x) in the form

$Φ(A, η|x) ∝ q(A k , η A k )ϕ(x k |η A k ) λ |A (k) |-1 B∈A (k) (|B| -1)! (λ + 1) • • • (λ + N -1) B∈A (k) ψ(η B ) l∈B ϕ(x l |η B ) (19.17) with q(A, θ) = B∈A (k) |B|1 A=B∪{k} + λψ(θ)1 A={k}$Partitions A ′ that are consistent with A (k) allocate k to one of the clusters in A (k) or create a new cluster with a new parameter η ′ k . If one replaces (A, η) by (A ′ , η ′ ), only the first two terms in (19.17) will be affected, so that the conditional probability of

$A ′ given A (k) is proportional to q(A ′ k , η A ′ k )ϕ(x k |η A ′ k ) and given by              |B|ϕ(x k |η B ) C 1 + λC 2 if A ′ k = B ∪ {k}, η ′ B = η B , B ∈ A (k) λϕ(x k |η ′ k )ψ(η ′ k ) C 1 + λC 2 if A ′ k = {k},$where

$C 1 = B∈A k |B|ϕ(x k |η B )$and C 2 = Θ ϕ(x k |θ)ψ(θ)dθ. Concretely, this means that one first decides to allocate k to a set B in A (k) with probability |B|ϕ(x k |η B )/(C 1 +λC 2 ) and to create a new set with probability λC 2 /(C 1 + λC 2 ). If a new set is created, then the associated parameter η ′ {k} is sampled according to the p.d.f. ϕ(x k |θ)ψ(θ/C 2 .

(3) However, sampling using this conditional probability requires the computation of the integral C 2 , which can represent a significant computational burden, since this has to be done many times in a Gibbs sampling algorithm. A modification of this algorithm, introduced in Neal [[141]](#b159), avoids this computation by adding new auxiliary variables at each step of the computation. These variables are m parameters η * 1 , . . . , η * m ∈ Θ where m is a fixed integer. To define the joint distribution of A, η, η * , one lets the marginal distribution of (A, η) be given by [(19.16](#)) and conditionally to A, η, let η * 1 , . . . , η * m be: (i) independent with density ψ if |A k | > 1;

(ii) such that η * j = η A k and the other m -1 starred parameters are independent with distribution ψ, where j is randomly chosen in {1, . . . , m} if A k = {k}.

With this definition, the joint conditional distribution of (A, η, η * ) takes the form

$Φ(A, η, η * |x) ∝ q(A k , η A k , η * )ϕ(x k |η A k ) λ |A (k) |-1 B∈A (k) (|B| -1)! (λ + 1) • • • (λ + N -1) B∈A (k) ψ(η B ) l∈B ϕ(x l |η B ) (19.18) with q(A, θ, η * 1 , . . . , η * m ) = B∈A (k) |B|1 θ=η B ,A=B∪{k} m j=1 ψ(η * j ) + λ m m j=1 1 θ=η * j ,A={k} ψ(θ) m i=1,i j ψ(η * i ).$Note that Φ depends on k, so that the definition of the auxiliary variables will change at each step of Gibbs sampling. The conditional distribution, for Φ, of A ′ , η ′ given

$A (k) , η (k) , η * is such that • A ′ k = B ∪ {k} and η ′ A ′ k = η B with probability |B|ϕ(x k |η B )/C, for B ∈ A (k) . • A ′ k = {k} and η A ′ k = η * j with probability (λ/m)ϕ(x k |η * j )/C, j = 1, . . . , m.$The constant C is given by

$C = B∈A k |B|ϕ(x k |η B ) + λ m m j=1 ϕ(x k |η * j )$and is therefore easy to compute.

We can now summarize this discussion with Neal's version of the Gibbs sampling algorithm.

## Algorithm 19.14 (Neal)

Initialize the algorithm with some arbitrary partition and parameters (A, η) (for example, generated using the Dirichlet prior). Use the same notation to denote these variables at the end of the previous iteration of the algorithm. The next iteration is then run as follows.

(1) For k = 1, . . . , N , reallocate k to a cluster as follows.

(i) Form the new family of sets A (k) and labels η (k) by removing k from the partition A. (ii) If |A k | > 1, generate m variables η * 1 , . . . , η * m according to ψ. If A k = {k}, generate only m -1 such variables and let the last one be equal to η A k . (iii) Allocate k to a new cluster A ′ with parameter η ′ A ′ according to probabilities proportional to

$         |B|ϕ(x k |η (k) B ) if A ′ = B ∪ {k} and η ′ A ′ = η (k) B λ m ϕ(x k |η * j ) if A = {k} and η ′ A ′ = η * j , j = 1, . . . , m$(2) For A ∈ A, update η A , A ∈ A according to the distribution proportional to

$ψ(η) k∈A ϕ(x k |η)$either directly, or via one step of Gibbs sampling visiting each of the variables that constitute η A .

(3) Loop a sufficient number of times over the previous two steps.

After running this algorithm, the set of clusters should be finalized by using statistics computed along the simulation, as discussed after Algorithm 19.10.

Full example: Mixture of Gaussian. To conclude this section, we summarize the Monte-Carlo sampling algorithm for mixtures of Gaussian using a non-parametric Bayesian prior. Here, η ∈ Θ is the center c ∈ R d , with prior distribution ψ = N (0, τ 2 Id R d ).

The previous algorithm must be modified because an additional parameter σ 2 is shared by all classes, with prior given by an inverse gamma distribution with parameters u and v. The conditional distribution of the data is ϕ(x|c, σ ) ∼ N (c, σ 2 Id R d ).

Algorithm 19.15 (Gibbs sampling for non-parametric mixture of Gaussian) (1) Initialize the algorithm with some arbitrary partition and parameters (A, η).

(2) For k = 1, . . . , N , reallocate k to a cluster as follows.

(i) Form the new family of sets A (k) and labels η (k) by removing k from the partition A.

(ii (iii) Allocate k to a new cluster A ′ with parameter c ′ A ′ according to probabilities proportional to Factor analysis aims at representing potentially high-dimensional data as functions of a (generally) small number of "factors," with a representation taking the general form X = Φ(Y , θ) + residual, [(20.1)](#) where X is the observation, Y provide the factors and Φ is a function parametrized by θ. A factor analysis model must therefore specify Φ (often, a linear function of Y ), add hypotheses on Y (such as its dimension, or properties of its distribution) and on the residuals. The transformation Φ is estimated from training data, but, ideally, the method should also provide an algorithm that infers Y from a new observation of X. Most of the time, Y is small dimensional so that the model also implies a reduction of dimension.

$) If |A k | > 1, generate m variables c * i , i = 1, . . . , m independently with c * i ∼ N (0, τ 2 Id R d ). If A k = {k},$$             |B| exp - |x k -c (k) B | 2σ 2 if A ′ = B ∪ {k} and c ′ A ′ = c (k) B λ m exp - |x k -c * B | 2σ 2 if A = {k} and c ′ A ′ = c * j , j = 1, . . . , m.$We start our discussion with principal component analysis (or PCA). This methods can be characterized in multiple ways, and we introducing through the angle of data approximation. In the following, the random variable X takes values in a finiteor infinite-dimensional inner-product space H. We will denote, as usual, by ⟨. , .⟩ H the product in this space. Assume that N independent realization of X, denoted x 1 , . . . , x N , are observed, forming our training set T . Our goal is to obtain a small-dimensional representation of these data, while loosing a minimal amount of relevant information. PCA, is the simplest and most commonly used approach developed for this purpose.

If V is a finite-dimensional subspace of H, we denote by P V (y) the orthogonal projection of y ∈ H on V , i.e., the element ξ ∈ V such that ∥y -ξ∥ 2  H is minimal (see section 6.4). Recall that this orthogonal projection if characterized by the two properties: (i) P V (y) ∈ V and (ii) (y -P V (y)) ⊥ V .

Given a target dimension p, PCA determines a p-dimensional subspace of H, say, V and a point c ∈ H, such that, letting

$R k = x k -c -P V (x k -c)$for k = 1, . . . , N , the residual sum of squares

$S = N k=1 ∥R k ∥ 2 H (20.2)$is as small as possible.

An optimal choice for c is c = x = N k=1 x k /N . Indeed, using the linearity of the orthogonal projection, we have

$S = N k=1 ∥x k -P V (x k ) -(c -P V (c))∥ 2 H = N k=1 ∥x k -P V (x k ) -(x -P V (x))∥ 2 H + N ∥x -P V (x) -(c -P V (c))∥ 2 H .$Given this, there would be no loss of generality in assuming that all x k 's have been replaced by x kx and taking c = 0. While this is often done in the literature, there are some advantages (especially when discussing kernel methods) in keeping the average explicit in the notation, as we will continue to do.

Introducing an orthonormal basis (e 1 , . . . , e p ) of V , one has

$P V (x k -x) = p i=1 ρ k (i)e i$with ρ ki = ⟨x kx , e i ⟩ H . One can then reformulate the problem in terms of (e 1 , . . . , e p ), which must minimize

$S = N k=1 ∥x k -x - p i=1 ⟨x k -x , e i ⟩e i ∥ 2 H = N k=1 ∥x k -x∥ 2 H - p i=1 N k=1 ⟨x k -x , e i ⟩ 2 H . For u, v ∈ H, define ⟨u , v⟩ T = 1 N N k=1 ⟨x k -x , u⟩ H ⟨x k -x ,$v⟩ H and ∥u∥ T = ⟨u , u⟩[foot_23](#foot_23)/2 T (the index T refers to the fact that this norm is associated with the training set). This provides a new quadratic form on H. The formula above shows that minimizing S is equivalent to maximizing p i=1 ∥e i ∥ 2 T subject to the constraint that (e 1 , . . . , e p ) is orthonormal in H. Let us consider a slightly more general problem. If H is a separable Hilbert space 1 and µ is a square-integrable probability measure on H, such that H ∥x∥ 2 H dµ(x) < ∞, one can define m = H xdµ(x) and σ 2 µ = H ∥x -m∥ 2 H dµ. One can then define the covariance bilinear form Γ µ (u, v) = H ⟨u , x -m⟩ H ⟨v , x -m⟩ H dµ(x), which satisfies Γ µ (u, v) ≤ σ 2 µ ∥u∥ H ∥v∥ H . With this notation, we have ⟨u , v⟩ T = Γ μT (u, v), where μT = (1/N ) N k=1 δ x k is the empirical measure (and in that case m = x). We can therefore generalize the PCA problem by considering the maximization of p k=1 Γ µ (e k , e k ) (20.3) over all orthonormal families (e 1 , . . . , e p ) in H.

When µ is square integrable, the associated operator, A µ defined by ⟨u , A µ v⟩ H = Γ µ (u, v) [(20.4)](#) for all u, v ∈ H, is a Hilbert-Schmidt operator [[205]](#b223). Such an operator can, in particular, be diagonalized in an orthonormal basis of H, i.e., there exists an orthonormal basis, (f 1 , f 2 , . . .) of H such that A µ f i = λ 2 i f i for a non-increasing sequence of eigenvalues (with

$λ 1 ≥ λ 2 ≥ • • • ≥ 0) such that σ 2 µ = ∞ k=1 λ 2 i .$The main statement of the following result is in finite dimensions, a simple application of corollary 2.4. We here give a direct proof that also works in infinite dimensions.

Theorem 20.1 Let (f 1 , f 2 , . . .) be an orthonormal basis of eigenvectors of A µ with associated eigenvalues λ 2 1 ≥ λ

2 2 ≥ • • • ≥ 0. Then an orthonormal family (e 1 , . . . , e p ) in H maximizes (20.3) if and only if, span(f j : λ 2 j > λ 2 p ) ⊂ span(e 1 , . . . , e p ) ⊂ span(f j : λ 2 j ≥ λ 2 p ). (20.5) In particular f 1 , . . . , f p always provide a solution and span(e 1 , . . . , e p ) = span(f 1 , . . . , f p ) for any other solution as soon as λ 2 p > λ 2 p+1 . Definition 20.2 When µ = μT , the vectors (f 1 , . . . , f p ) are called (with some abuse when eigenvalues coincide) the first p principal components of the training set (x 1 , . . . , x N ). Proof If (e 1 , . . . , e p ) is an orthonormal family in H, let F(e 1 , . . . , e p ) = p k=1

Γ µ (e k , e k ) .

$Note that F(f 1 , . . . , f p ) = λ 2 1 + • • • + λ 2 p . Write e k = ∞ j=1 α (j) k f j (so that α (j) k = ⟨f j , e k ⟩ H ). These coefficients satisfy ∞ j=1 α (j) k α (j) l = 1 if k = l and 0 otherwise. Then Γ µ (e k , e k ) = ∞ j=1 λ 2 j (α (j) k ) 2 .$We have

$F(e 1 , . . . , e p ) = p k=1 ∞ j=1 λ 2 j (α (j) k ) 2 = p k=1 p j=1 λ 2 j (α (j) k ) 2 + p k=1 ∞ j=p+1 λ 2 j (α (j) k ) 2 ≤ p k=1 p j=1 λ 2 j (α (j) k ) 2 + p k=1 ∞ j=p+1 λ 2 p+1 (α (j) k ) 2 = p j=1 (λ 2 j -λ 2 p+1 ) p k=1 (α (j) k ) 2 +$pλ 2 p+1 . Let P denote the orthogonal projection operator from H to span(e 1 , . . . , e p ). We have, for any h ∈ H, ∥P h∥ 2 H ≤ ∥h∥ 2 H with equality if and only if h ∈ span(e 1 , . . . , e p ). Applying this to h = f j , with P (f j ) = p k=1 α (j) k e k , we get p k=1 (α (j) k ) 2 ≤ 1 with equality if and only if f j ∈ span(e 1 , . . . , e p ). As a consequence, the previous upper bound on F(e 1 , . . . , e p ) implies F(e 1 , . . . , e p ) ≤ p j=1 λ 2 j . This upper bound is attained at (e 1 , . . . , e p ) = (f 1 , . . . , f p ), which is therefore a maximizer. Also, inspecting the argument above, we see that F(e 1 , . . . , e p ) < λ 2 1 + • • • + λ 2 p unless (a) for all k ≤ p and j ≥ p + 1: α (j) k = 0 if λ 2 j > λ 2 p+1 , and (b) for all j ≤ p: p k=1 (α (j) k ) 2 = 1 unless λ 2 j = λ 2 p+1 . Condition (a) implies that span(e 1 , . . . , e p ) ⊂ span(f j : λ 2 j ≤ λ 2 p+1 ). If λ 2 p = λ 2 p+1 , the inclusion span(e 1 , . . . , e p ) ⊂ span(f j : λ 2 j ≤ λ 2 p ) therefore holds. If λ 2 p < λ 2 p+1 , condition (b) requires p k=1 (α (j)

k ) 2 = 1 for all j ≤ p, which implies f j ∈ span(e 1 , . . . , e p ) for j ≤ p, so that span(e 1 , . . . , e p ) = span(f 1 , . . . , f p ) and the inclusion also hold. k ) 2 = 1, hence f j ∈ span(f 1 , . . . , f p ), when λ j < λ p , showing that span(f j : λ 2 j < λ 2 p ) ⊂ span(e 1 , . . . , e p ). Equation (20.5) therefore always holds for (e 1 , . . . , e p ) such that F(e 1 , . . . , e p ) = λ 2  1 + • • • + λ 2 p . Furthermore, conditions (a) and (b) always hold for any orthonormal family that satisfy [(20.5)](#), showing that any such solution is optimal. ■ Remark 20.4 Sometimes, the metric is specified by giving Q -1 instead of Q (or Q -1 is easy to compute). Then, one can directly solve the generalized eigenvalue problem

## Condition (b) always requires

$Σ T f = λ 2 Q -1 f and set f = Q -1 f . The normalization f T Qf = 1 is then obtained by normalizing f so that f T Q -1 f = 1. ♦ Remark 20.5$The "standard" version of PCA applies this computation using the Euclidean inner product, with Q = Id R d , and the principal components are the eigenvectors of the covariance matrix of T associated with the largest eigenvalues. ♦ Large dimension. It often happens that the dimension of H is much larger than the number of observations, N . In such a case, the previous approach is quite inefficient (especially when the dimension of H is infinite!) and one should proceed as follows.

Returning to the original problem, one can remark that there is no loss of generality in assuming that V is a subspace of W := span{x 1x, . . . , x N -x}. Indeed, letting V ′ = P W (V ) (the projection of V on W ), we have, for ξ ∈ W ,

$∥ξ -P V ξ∥ 2 H = ∥ξ∥ 2 H -2⟨ξ , P V ξ⟩ H + ∥P V ξ∥ 2 H = ∥ξ∥ 2 H -2⟨P W ξ , P V ξ⟩ H + ∥P V ξ∥ 2 H = ∥ξ∥ 2 H -2⟨ξ , P W P V ξ⟩ H + ∥P V ξ∥ 2 H ≥ ∥ξ∥ 2 H -2⟨ξ , P W P V x⟩ H + ∥P W P V ξ∥ 2 H = ∥ξ -P W P V ξ∥ 2 H ≥ ∥ξ -P V ′ ξ∥ 2 H .$In this computation, we have used the facts that P W ξ = ξ (since ξ ∈ W ), that ∥P W P V ξ∥ H ≤ ∥P V ξ∥ H , that P W P V ξ ∈ V ′ and that P V ′ (ξ) is the best approximation of ξ by an element of V ′ . This shows that (since

$x k -x ∈ W for all k) N k=1 ∥x k -x -P V (x k -x)∥ 2 H ≥ N k=1 ∥x k -x -P V ′ (x k -x)∥ 2 H$with V ′ a subspace of W of dimension less than p, proving the result. This computation also shows that no improvement in PCA can be obtained by looking for spaces of dimension p ≥ dim(W ) (with dim(W ) ≤ N -1 because the data is centered).

It therefore suffices to look for f 1 , . . . , f p in the form

$f i = N k=1 α (i) k (x k -x).$for some α

$(i) k , 1 ≤ k ≤ N , 1 ≤ i ≤ p.$With this notation, we have ⟨f i , f j ⟩ H = N k,l=1 α

$(i) k α (j) l ⟨x k -x , x l -x⟩ H and ⟨f i , f j ⟩ T = 1 N N l=1 ⟨f i , x l -x⟩ H ⟨f j , x l -x⟩ H = 1 N N k,k ′ =1 α (i) k α (j) k ′ N l=1 ⟨x k -x , x l -x⟩ H ⟨x k ′ -x , x l -x⟩ H .$Let S be the Gram matrix of the centered data, formed by the inner products ⟨x kx , x l -x⟩ H , for k, l = 1, . . . , N . Let α (i) be the column vector with coordinates α

$(i)$k , k = 1, . . . , N . We have ⟨f i , f j ⟩ H = (α (i) ) T Sα (j) and ⟨f i , f j ⟩ T = (α (i) ) T S 2 α (j) /N , which implies that, in this representation, the operator A T is given by S/N . Thus, the previous simultaneous orthogonalization problem can be solved in terms of the α's by diagonalizing S and taking the first eigenvectors, normalized so that (α (i) ) T Sα (i) = 1. Let λ 2 j , j = 1, . . . , N be the eigenvalues of S/N (of which only the first min(d, N -1) may be non-zero). In this representation, the decomposition of the projection of x k on the PCA basis is given by

$x k = p j=1 β (j) k f j with β (j) k = ⟨x k -x , f j ⟩ H = N l=1 α (j) l ⟨x l -x , x k -x⟩ H = N λ 2 j α (j) k .$
## Kernel PCA

Since the previous computation only depended on the inner products ⟨x kx , x l -x⟩ H , PCA can be performed in reproducing kernel Hilbert spaces, and the resulting method is called kernel PCA. In this framework, X may take values in any set R with a representation h : R → H. The associated kernel, K(x, x ′ ) = ⟨h(x) , h(x ′ )⟩ H , provides a closed form expression of the inner products in terms of the original variables. The feature function itself is most of the time unnecessary.

The kernel version of PCA consists in replacing x kx with h(x k ) -h where h is the average feature. This leads to defining a "centered kernel:"

$K c (x, x ′ ) = ⟨h(x) -h , h(x ′ ) -h⟩ H = ⟨h(x) , h(x ′ )⟩ H -⟨h(x) + h(x ′ ) , h⟩ + h 2 H = K(x k , x l ) - 1 N N k=1 (K(x, x k ) + K(x ′ , x k )) + 1 N 2 N k,l=1 K(x k , x l ).$
## KERNEL PCA

Then the Gram matrix in feature space is S with s kl = K c (x k , x l ) and the computation described in the previous section can be applied. Note that, if one denotes, as usual K = K(x 1 , . . . , x N ) the matrix formed by kernel evaluations K(x k , x l ), and if one lets P = Id R N -1 N 1 N /N , then we have the simple matrix expression S = P KP .

Letting α (1) , . . . , α (p) ∈ R N be the first p eigenvectors of S, normalized so that (α (i) ) T Sα (i) = 1, the principal directions are vectors in feature space given by (using the notation in the previous section in which the kth coordinate of α (i) is α

$(i) k ) f i = N k=1 α (i) k (h(x k ) -h) ,$and they are not computable when the features not known explicitly. However, a few geometric features associated with these directions can be characterized using the kernel only.

Consider the line in feature space D i = h + λf i , λ ∈ R . Let Ω i denote the points

x ∈ R such that h(x) ∈ D i . Then x ∈ Ω i if and only if h(x) coincides with its orthogonal projection on D i , which is equivalent to

$⟨h(x) -h , f i ⟩ 2 H = h(x) -h 2$H , which can be expressed with the kernel as

$K c (x, x) -        N k=1 α (i) k K c (x, x k )        2 = 0 . (20.6)$This provides a nonlinear equation in x. In particular, Ω i is generally nonlinear, possibly with several connected components. Note that, by definition, the difference in [(20.6](#)) is always non-negative, so that a way to visualize Ω i is to compute its sublevel sets, i.e., the set of all x such that

$K c (x, x) -        N k=1 α (i) k K c (x, x k )        2 ≤ ϵ$for small ϵ.

Similarly, the feature vector h(x) -h belongs to the space generated by the first p components if and only if

$p i=1 ⟨h(x) -h , f i ⟩ 2 H = h(x) -h 2 H i.e., p i=1        N k=1 α (i) k K c (x, x k )        2 = K c (x, x).$One can also compute the finite-dimensional coordinates of h(x) in the PCA basis, and this computation is easier. The representation is

x → (u 1 (x), . . . , u p (x))

with

$u i = ⟨h(x) -h , f i ⟩ H = N k=1 α (i) k K c (x, x k ) .$This provides an explicit nonlinear transformation that maps each data point x into a p-dimensional point. This representation allows one to easily exploit the reduction of dimension.

## Statistical interpretation and probabilistic PCA

There is a simple probabilistic interpretation of linear PCA. Assume that H = R d with the standard inner product and that X is a centered random vector with covariance matrix Σ. Consider the problem that consists in finding a factor decomposition

$X = p i=1 Y (i) e i + R$where Y = (Y (1) , . . . , Y (p) ) T forms a p-dimensional centered vector, e 1 , . . . , e p is an orthonormal system, and R is a random vector, independent of Y and as small as possible, in the sense that E(|R| 2 ) is minimal.

One can see that, in an optimal decomposition, one needs R T e i = 0 for all i, because one can always write

$p i=1 Y (i) e i + R = p i=1 (Y (i) + R T e i )e i + R - p i=1 R T e i e i .$If R is centered, then so is R -p i=1 R T e i e i and the latter provides a better solution since |R -p i=1 R T e i e i | ≤ |R|. Also, there is no loss of generality in requiring that (Y (1) , . . . , Y (p) ) are uncorrelated, as this can always be obtained after a change of basis in span(e 1 , . . . , e p ).

Assuming this, we can write

$E(|X| 2 ) = p i=1 E((Y (i) ) 2 ) + E(|R| 2 )$with Y (i) = e T i X. So, to minimize E(|R| 2 ), one needs to maximize

$p i=1 E((e T i X) 2 )$which is equal to (letting Σ be the covariance matrix of X)

$p i=1 e T i Σe i .$The solution of this problem is given by the first p eigenvectors of Σ. PCA (with a Euclidean metric) exactly applies this procedure, with Σ replaced by the empirical covariance.

"Probabilistic PCA" is based on a slightly different statistical model in which it is assumed that X can be decomposed as

$X = p i=1 λ i Y (i) e i + σ R,$where R is a d dimensional standard Gaussian vector and Y = (Y (1) , . . . , Y (p) ) T a pdimensional standard Gaussian vector, independent of R. The main difference with standard PCA is that the total variance of the residual, here dσ 2 , is a model parameter and not a quantity to minimize. We can rewrite this model in the form

$X = W Y + σ 2 R$where the parameters are W and σ 2 , with the constraint that W T W is a diagonal matrix. As a linear combination of independent Gaussian random variables, X is Gaussian with covariance matrix W W T + σ 2 Id. The log-likelihood of the observations x 1 , . . . , x N therefore is

$L(W , σ ) = - N 2 d log 2π + log det(W W T + σ 2 Id) + trace((W W T + σ 2 Id) -1 Σ T ) (20.7)$where Σ Y is the empirical covariance matrix of x 1 , . . . , x N . This function can be maximized explicitly in W and σ , as stated in the following proposition.

Proposition 20.6 Assume that the matrix Σ T is invertible. The log-likelihood in (20.7) is maximized by taking (i) W = [λ 1 e 1 , . . . , λ p e p ] where e 1 , . . . , e p are the eigenvectors of Σ T associated to the p largest eigenvalues, and λ i = δ 2 iσ 2 , where δ 2 i is the eigenvalue of Σ associated to e i ; (ii) and

$σ 2 = 1 d -p d i=p+1 δ 2 i .$Proof We make the following change of variables: let ρ 2 = 1/σ 2 and

$µ 2 i = 1 σ 2 - 1 λ 2 i + σ 2 .$Let Q = [µ 1 e 1 , . . . , µ p e p ]. We have

$(W W T + σ 2 Id) -1 = ρ 2 Id -QQ T .$To see this, complete (e 1 , . . . , e p ) into an orthonormal basis of R d , letting e p+1 , . . . , e d denote the added vectors. Then

$W W T + σ 2 Id = p i=1 (λ 2 i + σ 2 )e i e T i + d i=p+1$σ 2 e i e T i so that

$(W W T + σ 2 Id) -1 = p i=1 (λ 2 i + σ 2 ) -1 e i e T i + d i=p+1 σ -2 e i e T i = ρ 2 Id -QQ T .$Using these variables, we can reformulate the problem as the minimization of

$- p i=1 log(ρ 2 -µ 2 i ) -(d -p) log ρ 2 + ρ 2 trace(Σ) - p j=1 µ$2 j e T j Σe j . From theorem 2.3, we have p j=1 µ 2 j e T j Σe j ≤ p j=1 µ 2 j δ 2 j and this upper bound is attained by letting e 1 , . . . , e p be the first p eigenvectors of Σ. Using this, we see that σ 2 , µ 2 1 , . . . , µ 2 p must minimize

$- p i=1 log(ρ 2 -µ 2 i ) -(d -p) log ρ 2 + ρ 2 d j=1 δ 2 j - p j=1 µ 2 j δ 2 j .$Computing the solution is elementary and left to the reader, and yields, when expressed as functions of σ 2 , λ 2 1 , . . . , λ 2 p , the expressions given in the statement of the theorem.

## ■

## Generalized PCA

We now discuss a dimension reduction method called generalized PCA (GPCA) [[200]](#b218) that, instead of looking for the best linear approximation of the training set by one specific subspace, provides an approximation by a finite union of such spaces.

As a motivation, consider the situation in fig. [20](#fig_158).1 in which part of the data is aligned along one direction in space, and another part along another direction. Then, the only information that PCA can retrieve (provided that the two directions intersect) is the plane generated by the two directions, which will be captured by the two principal components. PCA will not be able to determine the individual directions. GPCA addresses this type of situation as follows. For simplicity, assume that we are trying to decompose the data along unions of hyperplanes in R d . Such hyperplanes have equations of the form u T x = 0 where x is our notation for the vector (1, x T ) T . If we have two hyperplanes, specified by u 1 and u 2 and all the training samples approximately belong to one of them, then one has, for all k = 1, . . . , N :

$(u T 1 xk )(u T 2 xk ) = xT k u 1 u T 2 xk ≃ 0.$Similarly, for n hyperplanes, the identity is, for k = 1, . . . , N :

$n j=1 (u T j xk ) ≃ 0. Write n j=1 (u T j x) = 1≤i 1 ,...,i n ≤d u 1 (i 1 ) • • • u n (i n )x (i 1 ) • • • x (i n )$in the form (by regrouping the terms associated with the same powers of x)

$F(x) = p 1 +...+p d =n q p 1 ...p d (x (1) ) p 1 . . . (x (d) ) p d . (20.8)$The collection of n+d-1 n numbers Q = (q p 1 ...p n , p 1 + • • • + p d = n) takes a specific form (that we will not need to make explicit) as a function of the unknown u 1 , . . . , u n , but the first step of GPCA ignores this constraint and estimates Q minimizing 

$N k=1         p 1 +...+p d =n q p 1 ...p d (x (1) k ) p 1 . . . (x (d) k ) p d         2 under the constraint q 2 p 1 ...$$Σ = N k=1 V (x k )V (x k ) T .$The solution is given by the eigenvector associated with the smallest eigenvalue of Σ. If the model is exact, this eigenvalue should be zero, and if only one decomposition of the data in a set of distinct hyperplanes exists (i.e., if n is not chosen too large), then Q is the unique solution up to a multiplicative constant.

Once Q is found, it remains to identify the vectors u 1 , . . . , u n . This identification can be obtained by inspecting the gradient of F on the union of hyperplanes. Indeed, one has, for x ∈ R d ,

$∇F(x) = n j=1         j ′ j u T j ′ x         u j$However, if x belong in one and only one of the hyperplanes, say x T u j = 0, then all terms in the sum vanish but one and ∇F(x) is proportional to u j . So, if the model is exact, one has, for each k = 1, . . . , N , either ∇F(x k ) = 0 (if x k belongs to the intersection of two hyperplanes) or ∇F(x k )/|∇F(x k )| = ±u j for some j, and the sign ambiguity can be removed by ensuring, for example, that the first non-vanishing coordinate of u j is positive. (The gradient of F can be computed from Q using [(20.8)](#).) The computation of ∇F on training data therefore allows for an exact computation of the hyperplanes.

In practice, when noise is present, one cannot expect this computation to be exact. The vectors u 1 , . . . , u n can be estimated by clustering the collection of nonvanishing gradients ∇F(x k ), k = 1, . . . , N . For example, one can compute a dissimilarity matrix such as d kl = 1cos 2 (θ kl ), where θ kl is the angle between ∇F(x k ) and ∇F(x l ), and apply one of the methds discussed in section [19.4.1.](#) This analysis provides a decomposition of the training set into n (or fewer) hyperplanes. The computation can then be recursively refined in order to obtain smaller dimensional subspaces by applying the same method separately to each hyperplane.

## Nuclear norm minimization and robust PCA 20.5.1 Low-rank approximation

One can also interpret PCA in terms of low-rank matrix approximations. Let X c be the N by d matrix (x 1x, . . . , x Nx) T , which, in generic situations, has rank d -1. Then PCA with p components is equivalent to minimizing, over all N by d matrices Z of rank p, the norm of the difference

$|X c -Z| 2 = trace((X c -Z) T (X c -Z)) .$(20.9)

The quantity |A| 2 = trace(A T A) is the sum of square of the entries of A, which is often referred to as the (squared) Frobenius norm. We have

$|A| 2 = d k=1 σ 2 k$where σ 1 , . . . , σ d are the singular values of A, i.e., the square roots of the eigenvalues of A T A.

We first note the following characterization of rank-p matrices.

Proposition 20.7 A matrix Z has rank p if and only if it can be written in the form Z = AW T where A is N ×p, and W is d ×p with W T W = Id R p , i.e., W = [e 1 , . . . , e p ] where the columns form an orthonormal family of R d . Proof The "if" part is obvious and we prove the "only if" part. Assume that Z has rank p. Take W = [e 1 , . . . , e p ], where (e 1 , . . . , e p ) is an orthonormal family in Null(Z) ⊥ . Letting e p+1 , . . . , e d denote an orthonormal basis of Null(Z), we have d i=1 e i e T i = Id R d and Z = Z d i=1 e i e T i = Z p i=1

e i e T i = ZW W T so that one can take A = ZW .

## ■

Using this representation and letting z T k be the kth row vector of Z, we have

$|X x -Z| 2 = N k=1 |x k -x -z k | 2 = N k=1 x k -x - p j=1 a (j) k e j 2 .$With fixed e 1 , . . . , e p , the optimal matrix A has coefficients a (j) k = (x kx) T e j . In matrix form, this is:

$Z = X c         p j=1 e j e T j         .$We therefore retrieve the PCA formulation that we gave in section 20.1, in the special case of H = R d with the standard Euclidean product. The lowest value achieved by the PCA solution is

$|X c -Z| 2 = N d k=p+1 λ 2 k$where λ 2  1 , . . . , λ 2 d are the eigenvalues of the covariance matrix computed from x 1 , . . . , x N , who are also the squared singular values of the matrix X c divided by N .

In this section, we will explore variations on PCA in which the minimization of |X c -Z| 2 is completed with a penalty that depends on the singular values of the matrix Z. As a first example, one can modify PCA by adding a penalty on the rank (i.e., on the number of non-zero singular values), minimizing:

$γ|X c -Z| 2 + rank(Z)$for some parameter γ > 0. However, the solution to this problem is a small variation of that of standard PCA. It is indeed given by standard PCA with p components where p minimizes

$N γ d k=p+1 λ 2 k + p = N γ d k=p+1 (λ 2 k -(N γ) -1 ) + d,$i.e., p is the index of the last eigenvalue that is larger than (N γ) -1 .

## The nuclear norm

Based on the fact that rank(Z) is the number of non-zero singular values of Z, one can use the same heuristic as in the development of the lasso, and replace counting the non-zero values by the sum of the absolute values of the singular values, which is just the sum of singular values since they are non-negative. This provides the nuclear norm of A, defined in section 2.4 by

$|A| * = d k=1 σ k$where σ 1 , . . . , σ d are the singular values of A. We will consider below the problem of minimizing γ|X c -Z| 2 + |Z| * [(20.10)](#) and show that its solution is once again similar to PCA.

We recall the characterization of the nuclear norm proposition 2.6. If A is an N by d matrix,

$|A| * = max trace(U AV T ) : U is N × N and U T U = Id, V is d × d and V T V = Id .$In Cai et al. [[45]](#b63), the authors consider the minimization of (20.10) and prove the following result. Recall that we have defined the shrinkage function S τ : t → sign(t) max(|t|τ, 0) (with τ ≥ 0), using the same notation S τ (X) when applying S τ to every entry of a vector or matrix X. Following Cai et al. [[45]](#b63), we define the singular value thresholding operator A → S τ (A), where A is any rectangular matrix, by Proof Representing Z by its singular value decomposition, we have the equivalent formulation of minimizing

$F(U , V , D) = γ|X c -U DV T | 2 + |D| * = γ|X c | 2 -2γtrace(X T c U DV T ) + γ|D| 2 + |D| *$over all orthonormal matrices U and V and diagonal matrices with non-negative coefficients D. From theorem 2.1, we know that trace(X T c U DV T ) is less than the sum of the products of the non-increasingly ordered singular values of X c and D and this upper bound is attained by taking U = Ū and V = V where Ū and V are the matrices providing the SVD of X c , i.e., such that X c = Ū ∆ V T where ∆ is diagonal with non-decreasing coefficients along the diagonal. So, letting

$λ 1 ≥ • • • ≥ λ d ≥ 0 and µ 1 ≥ • • • ≥ µ d ≥ 0 be$the singular values of X c and Z, we have just proved that, for any D,

$F(U , V , D) ≥ F( Ū , V , D) = -2γ d i=1 µ i λ i + γ d i=1 µ 2 i + d i=1 µ i .$The lower bound is minimized when µ i = max(λ i -1/2γ, 0). This proves the proposition.

■

## Robust PCA

As a consequence, the nuclear norm penalty provides the same principal directions (after replacing γ by 2γ) as the rank penalty, but applies a shrinking operation rather than thresholding on the singular values. The difference is however more fundamental if, in addition to using the nuclear norm as a penalty, on replaces the squared Frobenius norm on the approximation error by the ℓ 1 norm, where, for an n by m matrix A with coefficients (a(i, j)),

$|A| ℓ 1 = i,j |a(i, j)| .$This is the formulation of robust PCA [[49]](#b67), which minimizes

$γ|X c -Z| ℓ 1 + |Z| * (20.11)$with respect to Z.

Robust PCA (which was initially named Principal Component Pursuit by the authors in Candès et al. [[49]](#b67)) is designed for situations in which X c can be decomposed as the sum of a low-rank matrix Z and of a sparse residual S. Some theoretical justification was provided in the original paper, stating that if X c = Z+S, with Z = U DV T (its singular value decomposition) such that U and V are sufficiently "diffuse" and rank(Z) is small enough, with the residual's sparsity pattern taken uniformly at random over the subsets of entries of S with a sufficiently small cardinality, then robust PCA is able to reconstruct the decomposition exactly with high probability (relative to the random selection of the sparsity pattern of S). We refer to Candès et al. [[49]](#b67) for the long proof that justifies this statement.

Robust PCA can be solved using the ADMM algorithm (section 3.5.5) after reformulating the problem as the minimization of

$γ|R| ℓ 1 + |Z| * subject to R + Z = X c .$The algorithm therefore iterates over the following steps.

$                   Z (k+1) = argmin Z |Z| * + 1 2α |Z + R (k) -X x + U (k) | 2 R (k+1) = argmin R γ|R| ℓ 1 + 1 2α |Z (k+1) + R -X c + U (k) | 2 U (k+1) = U (k) + Z (k+1) + R (k+1) -X c (20.12)$The first minimization is covered by proposition 2.6 and yields

$Z (k+1) = S α (X c -R (k) -U (k) ).$The second minimization is solved by a standard shrinking operation, i.e.,

$R (k+1) = S γα (X c -Z (k+1) -U (k) ).$Using this, we can rewrite the robust PCA algorithm as the sequence of fairly simple iterations.

Algorithm 20.1

(1) Choose a small enough constant α and a very small tolerance level ϵ.

(2) Initialize the algorithm with N by d matrices R (0) and U (0) (e.g., equal to zero).

(3) At step n, apply the iteration:

$             Z (k+1) = S α (X c -R (k) -U (k) ) R (k+1) = S γα (X c -Z (k+1) -U (k) ) U (k+1) = U (k) + Z (k+1) + R (k+1) -X c$(20.13) (4) Stop the algorithm is the variation compared to variables at the previous step is below the tolerance level. Otherwise, apply step n + 1.

## Independent component analysis

Independent component analysis (ICA) is a factor analysis method that represents a d-dimensional random variable X in the form X = AY where A is a fixed d ×d invertible matrix and Y is a d-dimensional random vector with independent components. There are two main approaches in this setting. The first one optimizes the matrix W = A -1 so that the components of W X are "as independent as possible" according to a suitable criterion. The second one is model-based, where a statistical model is assumed for Y , and its parameters, together with the entries of the matrix A, are estimated via maximum likelihood. Before describing each of these methods, we first discuss the extent to which the coefficients of A are identifiable.

## Identifiability

A statistical model is identifiable if its parameters (which could be finite-of infinitedimensional) are uniquely defined by the distribution of the observable variables. In the case of ICA, this question boils down to deciding whether AY ∼ A ′ Y ′ (i.e., they have the same probability distribution) implies that A = A ′ (where Y and Y ′ are two random vectors with independent components).

It should be clear that the answer to this question is negative, because there are trivial transformations of the matrix A that do not break the ICA model. One can, for example, take any invertible diagonal matrix, D, and let A ′ = AD -1 and Y ′ = DY . The same statement can be made if D is replaced by a permutation matrix, P , which reorders the components of Y . So we know that AY ∼ A ′ Y ′ is possible already when A ′ = ADP where D is diagonal and invertible and P is a permutation matrix. Note that iterating such matrices (i.e., letting A ′ = ADP D ′ P ′ ) does not extend the class of transformations because one has DP = P P -1 DP and one can easily check that P -1 DP is diagonal, so that one can rewrite any product of permutations and diagonal matrices as a single diagonal matrix multiplied by a single permutation.

It is interesting, and fundamental for the well-posedness of ICA, that, under one important additional assumption, the indeterminacy in the identification of A stops at these transformations. The additional assumption is that at most one of the components of Y follows a Gaussian distribution. That such a restriction is needed is clear from the fact that one can transform any Gaussian vector Y with independent components into another, BY , one as soon as BB T is diagonal. If two or more components of Y are Gaussian, one can restrict these matrices B to only affect those components. If only one of them is Gaussian, such an operation has no effect.

The following theorem is formally stated in Comon [[54]](#b72), and is a rephrasing of the Darmois-Skitovitch theorem [[57,](#b75)[179]](#b197). The proof of this theorem relies on complex analysis arguments on characteristic functions and is beyond the scope of these notes (see [Kagan et al. [101]](#) for more details). Theorem 20.9 Assume that Y is a random vector with independent components, such that at most one of its components is Gaussian. Let A be an invertible linear transformation and Ỹ = CY . Then the following statements are independent. (i) For all i j, the components Ỹ (i) , Ỹ (j) are independent.

(ii) Ỹ (1) , . . . , Ỹ (d) are mutually independent.

(iii) C = DP is the product on a diagonal matrix and of a permutation.

The equivalence of (ii) and (iii) implies that the ICA model is identifiable up to multiplication on the right by a permutation and a diagonal matrix. Indeed, if X = AY = A ′ Y ′ are two decompositions, then it suffices to apply the theorem to C = (A ′ ) -1 A to conclude. The equivalence of (i) and (ii) is striking, and has the important consequence that, if the data satisfies the ICA model, then, in order to identify A (up to the listed indeterminacy), it suffices to look for Y = A -1 X with pairwise independent components, which is a much lesser constraint than full mutual independence.

As a final remark on the Gaussian indeterminacy, we point out that, if the mean (m) and covariance matrix (Σ) of X are known (or estimated from data), the ICA problem can be reduced to looking for orthogonal transformations A. Indeed, assuming X = AY and letting X = Σ -1/2 (Xm) and Ỹ = D -1/2 (Y -A -1 m), where D is the (diagonal) covariance matrix of Y , we have

$X = Σ -1/2 (AY -m) = Σ -1/2 AD 1/2 Ỹ .$Letting Ã = Σ -1/2 AD 1/2 , we have Id R d = E( X XT ) = Ã ÃT so that Ã is orthogonal. This shows that the ICA problem for X in the form X = Ã Ỹ with the restriction that Ã is orthogonal has a solution, and also provides a solution of the original ICA problem by letting A = Σ 1/2 Ã and Y = Ỹ -Ã-1 Σ -1/2 m. Therefore, the indeterminacy associated with Gaussian vectors is as general as possible up to a normalization of first and second moments.

## Measuring independence and non-Gaussianity

Independence between d variables is a very strong property and its complete characterization is computationally challenging. The fact that the joint p.d.f.of the d variables (we will restrict, to simplify our discussion, to variables that are absolutely continuous) factorizes into the product of the marginal p.d.f.'s of each variable can be measured by computing the mutual information between the variables, defined by (letting ϕ Z denote the p.d.f. of a variable Z)

$I(Y ) = ϕ Y (y) d i=1 ϕ Y (i) (y (i) )$ϕ Y (y)dy.

The mutual information is always non-negative and vanishes only if the components of Y are mutually independent. Therefore, one can represent ICA as an optimization problem minimizing I(W X) with respect to all invertible matrices W (so that W = A -1 ). Letting

$h(Y ) = -log ϕ Y (y) ϕ Y (y)dy$denote the "differential entropy" of Y , we can write

$I(Y ) = d i=1 h(Y (i) ) -h(Y ). If Z = W X, then ϕ Z (z) = ϕ X (W -1 x)| det(W )| -1 .$
## Using this expression in h(Z) and making a change of variables yields h(

$W Z) = h(X) + log | det W | and I(W X) = d i=1 h(Z (i) ) -log | det(W )| -h(X).$This shows that the optimal W can be obtained by minimizing

$F(W ) = d i=1 h(W (i) X) -log | det(W )|$where W (i) is the ith row of W . This brings a notable simplification, since this expression only involves differential entropies of scalar variables, but still remains a challenging problem.

In Comon [[54]](#b72), it is proposed to use cumulant expansions of the entropy around that of a Gaussian with identical mean and variance to approximate the differential entropy. If ξ ∼ N (m, σ 2 ) , then

$h(ξ) = 1 2 + 1 2 log(2πσ 2 ).$Define, for a general random variable U with standard deviation σ U , the non-Gaussian entropy, or negentropy, defined by

$ν(U ) = 1 2 + 1 2 log(2πσ 2 U ) -h(U ) .$One can shows that ν(U ) ≥ 0 and is equal to 0 if and only if U is Gaussian. One can rewrite F(W ) as

$F(W ) = d 2 + d 2 log(2π) + d i=1 log(σ 2 W (i) X ) - d i=1 ν(W (i) X) -log | det(W )|$As we remarked earlier, if we replace X by Σ -1/2 (Xm) (after estimating the covariance matrix of X), there is no loss of generality in requiring that W is an orthogonal matrix, in which case both σ 2 W (i) X and | det W | are equal to 1. Assuming such a reduction is done, we see that the problem now requires to maximize

$d i=1 ν(W (i) X) (20.14)$among all orthogonal matrices W . Still in Comon [[54]](#b72), an approximation of the negentropy ν(U ) is provided as a function of the third and fourth cumulants of the distribution of U . These are given by

$κ 3 = E((U -E(U )) 3 )$and κ 4 = E((U -E(U )) 4 ) -3σ 4 U . In particular, when U is normalized, i.e., E(U ) = 0 and σ 2 U = 1, we have κ 3 = E(U 3 ) and κ 4 = E(U 4 ) -3. Under the same assumption, it is proposed in Comon [[54]](#b72) to use the approximation

$ν(U ) ∼ κ 2 3 12 + κ 2 4 48 + 7κ 4 3 48 - κ 2 3 κ 4 8 .$This approximation was derived from an Edgeworth expansion of the p.d.f. of U , which can be seen as a Taylor expansion around a Gaussian distribution. Plugging this expression into (20.14) provides an expression that can be maximized in W where the cumulants are replaced by their sample estimates. However, the maximized function involves high-degree polynomials in the unknown coefficients of W , and this simplified problem still presents numerical challenges.

An alternative approximation of the negentropy has been proposed in Hyvärinen [[94]](#b112) relying on the maximum entropy principle, described in the following theorem.

## Associate to any random variable

$Y : G → R the differential entropy h µ (Y ) = - G log ϕ Y (x)ϕ Y (x)dµ(x)$if the distribution of Y has a density, denoted ϕ Y , with respect to µ and h µ (Y ) = -∞ otherwise. Use also the same notation

$h µ (ϕ) = - G log ϕ(x)ϕ(x)dµ(x)$for a p.d.f. ϕ with respect to µ (i.e., such that ϕ is non-negative and has integral 1). Then, the following is true.

Theorem 20.10 Let g = (g (1) , . . . , g (p) ) T be a function defined on a measurable space G, taking values in R p , and let µ be a measure on G. Let Γ µ be the set of all λ = (λ (1) 

$, . . . , λ (p) ) ∈ R p such that G exp λ T g(y) dµ(y) < ∞. (20$
## .15)

Then

$h µ (Y ) ≤ inf -λ T E(g(Y )) + log G exp λ T g(y) dµ(y) : λ ∈ Γ µ . (20.16) Define, for λ ∈ Γ µ , ψ λ (x) = exp λ T g(x) dµ(x) G exp λ T g(y) dµ(y) . (20$
## .17)

Assume that the infimum in [(20.16](#)) is attained at an interior point λ * of Γ µ . Then

$h(ϕ λ * ) = max{h( Ỹ ) : E Ỹ (g) = E Y (g), i = 1, . . . , p}. (20$
## .18)

Proof Let Y be a random variable with p.d.f. ϕ Y with respect to µ (otherwise the lower bound in (20.16) is -∞). Then

$h µ (Y ) + λE(g(Y )) -log exp (λg(y)) dµ(y) = - G ϕ Y (x) log ϕ Y (x) ψ λ (x) dµ(x) ≤ 0 since G ϕ Y (x) log ϕ Y (x)$ψ λ (x) dµ(x) is a KL divergence and is always non-negative.

Assume that λ is in Γµ . Then, there exists ϵ > 0 such that, for any u ∈ R p , |u| = 1, λ + ϵu ∈ Γ µ . Using the fact that e β ≥ e α + (βα)e α , we can write ϵu T ge λ T g ≤ e (λ+ϵu) T ge λ T g -ϵu T ge λ T g ≤ e (λ-ϵu) T ge λ T g yielding ϵ|u T g|e λ T g ≤ max(e (λ+ϵu) T g , e (λ-ϵu) T g )e λ T g ≤ e (λ+ϵu) T g + e (λ-ϵu) T ge λ T g .

Since the upper-bound is integrable with respect to µ, so is the lower bound, showing that (taking u in the canonical basis of R p ) G |g (i) (y)|e λ T g(y) dµ(y) < ∞ for all i, or G |g(y)|e λ T g(y) dµ(y) < ∞.

Let c = E(g(Y )) and define

$Ψ c (λ) = -c T λ + log exp(λ T g(y))dy. (20.19)$Then

$∂ λ Ψ c = -c T + g(x) T exp(λ T g(x))dx exp(λ T g(y))dy = -c T + G g(x) T ψ λ (x)dx .$Since λ * is a minimizer, we find that, if Ỹ is a random variable with p.d.f. λ * , then

$E( Ỹ ) = c = E(Y ).$In that case, the upper-bound in [(20.16](#)) is h µ ( Ỹ ), proving [(20.18)](#). ■ Remark 20.11 The previous theorem is typically applied with µ equal to Lebesgue's measure on G = R d or to a counting measure with G finite. To rewrite the statement of theorem 20.10 in those cases, it suffices to replace dµ(x) by dx for the former, and integrals by sums over G for the latter. In the rest of the discussion, we restrict to the case when µ is Lebesgue's measure, using h(Y ) instead of h µ (Y ). ♦ Remark 20.12 This principle justifies, in particular, that the negentropy is always non-negative since it implies that a distribution that maximizes the entropy given its first and second moments must be Gaussian. ♦

The right-hand side of (20.16) provides a variational approximation of the entropy. If one uses this approximation when minimizing h(W (1) 

$X) + • • • + h(W (d) X),$the resulting problem can be expressed as a minimization, with respect to W and λ

$1 , . . . , λ d ∈ R p of - d j=1 λ T E(g(W (j) X)) + d j=1 log exp λ T g(y) dy .$While it would be possible to solve this optimization problem directly, a further approximation of the upper bound can be developed leading to a simpler procedure.

We have seen in the previous proof that, defining Ψ c by [(20.19)](#) and denoting by E λ the expectation with respect to ϕ λ , one has

$∇Ψ c (λ) = -c + E λ (g) .$Taking the second derivative, one finds

$∇ 2 Ψ c (λ) = E λ ((g -E λ (g))(g -E λ (g)) T ).$Now choose c 0 such that a maximizer of Ψ c 0 (λ), say, λ c 0 , is known. If c is close to c 0 , a first order expansion indicates that, for λ c maximizing Ψ c , one should have

$λ c ≃ λ c 0 + ∇ 2 Ψ c (λ c 0 ) -1 (c -c 0 ) with Ψ c (λ c ) ≃ Ψ c (λ c 0 ) -(c -c 0 ) T ∇ 2 Ψ c (λ c 0 ) -1 (c -c 0 ).$One can then use the right-hand side as an approximation of the optimal entropy. This leads to simple computations under the following assumptions. First, assume that the first two functions g (1) and g (2) are u and u 2 / √ 3. Let ϕ 0 be the p.d.f. of a standard Gaussian. Assume that the functions g (j) are chosen so that g (i) (u)g (j) (u)ϕ 0 (y)dy = δ ij for i, j = 1, . . . , p and such that g (i) (u)ϕ 0 (y)dy = 0 for i 2. Take

$c 0 = gϕ 0 (u)du so that c (1) 0 = 0, c (2) 0 = 1/ √ 3 and c (i) 0 = 0 for i ≥ 2.$Then λ c 0 provides, by construction, the distribution ϕ 0 and for any c, ∇ 2 Ψ c (λ c 0 ) = Id R p . With these assumptions, the approximation is

$Ψ c (λ) = h(ϕ 0 ) -|c -c 0 | 2 = 1 2 (1 + log 2π) - j≥3 (c (j) ) 2$(assuming that the data is centered and normalized so that c (1) = 0 and c (2) = 1/ √ 3). The ICA problem can then be solved by maximizing 2  (20.20) over orthogonal matrices W .

$d j=1 p i=1 E(g (i) (W (j) X))$Remark 20.13 Without the assumption made on the functions g (j) , one needs to compute S = Cov(g(U )) -1 where U ∼ N (0, 1) and maximize

$d j=1 (E(g(W (j) X)) -E(g(U ))) T S(E(g(W (j) X)) -E(g(U ))).$Clearly, this expression can be reduced to [(20.20)](#) by replacing g by S -1/2 (g-E(g(U ))).

Note also that we retrieve here a similar idea to the negentropy, maximizing a deviation to a Gaussian. ♦

## Maximization over orthogonal matrices

In the previous discussion, we reached a few times a formulation of ICA which required optimizing a function W → F(W ) over all orthogonal matrices. We now discuss how such a problem may be implemented.

In all the examples that were considered, there would have been no loss of generality in requiring that W is a rotation, i.e., det(W ) = 1. This is because one can change the sign of this determinant by simply changing the sign of one of the independent components, which is always possible. (In fact, the indeterminacy in W is by right multiplication by the product of a permutation matrix and a diagonal matrix with ±1 entries.) Let us assume that F(W ) is actually defined and differentiable over all invertible matrices, which form an open subset of the linear space M d (R) of d by d matrices.

Our optimization problem can therefore be considered as the minimization of F with the constraint that W W T = Id R d .

Gradient descent derives from the analysis that a direction of descent should be a matrix H such that F(W + ϵH) < F(W ) for small enough ϵ > 0 and on the remark that H = -∇F(W ) provides such a direction. This analysis does not apply to the constrained optimization setting because, unless the constraints are linear, W + ϵH will generally stop to satisfy the constraint when ϵ > 0, requiring the use of more complex procedures. In our case, however, one can take advantage of the fact that orthogonal matrices form a group to replace the perturbation W → W + ϵH by W → W e ϵH (using the matrix exponential) where H is moreover required to be skew symmetric (H + H T = 0), which guarantees that e ϵH is an orthogonal matrix with determinant 1. Now, using the fact that e ϵH = Id + ϵH + o(ϵ), we can write

$F(W e ϵH ) = F(W ) + ϵtrace(∇F(W ) T W H) + o(ϵ) .$Let ∇ s F(W ) be the skew symmetric part of W T ∇F(W ), i.e.,

$∇ s F(W ) = 1 2 (W T ∇F(W ) -∇F(W ) T W ). Then, if H is skew symmetric, trace(∇ s F(W ) T H) = 1 2 trace(∇F(W ) T W H) - 1 2 trace(W T ∇F(W )H) = 1 2 trace(∇F(W ) T W H) + 1 2 trace(W T ∇F(W )H T ) = trace(∇F(W ) T W H) so that F(W e ϵH ) = F(W ) + ϵtrace(∇ s F(W ) T H) + o(ϵ) .$This show that H = -∇ s F(W ) provides a direction of descent in the orthogonal group, in the sense that, if ∇ s F(W ) 0,

$F(W e -ϵ∇ s F(W ) ) < F(W )$for small enough ϵ > 0. As a consequence, the algorithm

$W n+1 = W n e -ϵ n ∇ s F(W n )$combined with a line search for ϵ n implements gradient descent in the group of orthogonal matrices, and therefore converges to a local minimizer of F.

If one linearizes the r.h.s. as a function of ϵ, one gets

$W n e -ϵ n ∇ s F(W n ) = W n + ϵ n 2 W n ((W n ) T ∇F(W n ) -∇F(W n ) T W n ) + o(ϵ) = W n + ϵ n 2 (∇F(W n ) -W n ∇F(W n ) T W n ) + o(ϵ).$As already argued, this linearized version cannot be used when optimizing over the orthogonal group. However, if one denotes by ω(A) the unitary part of the polar decomposition of A, i.e., ω(A) = (AA T ) -1/2 A, then the algorithm

$W n+1 = ω W n + ϵ n 2 (∇F(W n ) -W n ∇F(W n ) T W n )$also provides a valid gradient descent algorithm.

## Parametric ICA

We now describe a parametric version of ICA in which a model is chosen for the independent components of Y . The simplest version of to assume that all Y (j) are i.i.d. with some prescribed p.d.f., say, ψ. A typical example for ψ is a logistic distribution with ψ(t) = 2 (e t + e -t ) 2 .

If y is a vector in R d , we will use, as usual, the notation ψ(y) = (ψ(y (1) ), . . . , ψ(y (d) )) T for ψ applied to each component of y.

The model parameter is then the matrix A, or preferably W = A -1 , and it may be estimated using maximum likelihood. Indeed, the p.d.f. of X is

$f X (x) = | det W | d j=1 ψ(W (j) x)$where W (j) is the jth row of W , so that W can be estimated by maximizing

$ℓ(W ) = N log |det(W )| + N k=1 d j=1 log ψ(W (j) x k ) .$If we denote by Γ (W ) the matrix with coefficients

$γ ij (W ) = N k=1 x k (i) ψ ′ (W (j) x k ) ψ(W (j) x k )$and use the fact that the gradient of

$W → log | det W | is W -T (the inverse transpose of W ), we can write ∇ℓ(W ) = N W -T + Γ (W ).$We need however the maximization to operate on sets of invertible matrices, and it is more natural to move in this set through multiplication than through addition, because the product of two invertible matrices is always invertible, but not necessarily their sum. So, similarly to the previous section, we will look for small variations in the form W → W e ϵH , or simply, in this case, W → W (Id R d + ϵH). In both case, the first order expansion of the log-likelihood gives

$ℓ(W ) + ϵtrace((N W -T + Γ (W )) T W H) which suggests taking H = W T (N W -T + Γ (W )) = N Id + W T Γ (W ).$Dividing H by N , we obtain the following variant of gradient ascent for maximum likelihood

$W n+1 = (1 + ϵ n )W n + ϵ n W n W T n Γ (W n$) . This algorithm numerically performs much better than standard gradient ascent. It moreover presents the advantage of avoiding computing the inverse of W at each step.

## Probabilistic ICA

Note that the algorithms that we discussed concerning ICA were all formulated in terms of the matrix W = A -1 , which "filters" the data into independent components. As a result, ICA requires as many independent components as the dimension of X. Moreover, because the components are typically normalized to have equal variance, there is no obvious way to perform dimension reduction using this method. Indeed, ICA is typically run after the data is preprocessed using PCA, this preprocessing step providing the reduction of dimension.

It is however possible to define a model similar to probabilistic PCA, assuming a limited number of components to which a Gaussian noise is added, in the form (1) , . . . , Y (p) independent variables as before, and R ∼ N (0, Id R d ). This model is identifiable (up to permutation and scalar multiplication of the components) as soon as none of the variables Y (j) is Gaussian.

$X = p j=1 a j Y (j) + σ R with p < d, a 1 , . . . , a p ∈ R d , Y$Let us assume a parametric setting similar to that of the previous section, so that Y (1) , . . . , Y (p) are explicitly modeled as independent variables with p.d.f. ψ. Introduce the matrix A = [a 1 , . . . , a p ], so that the model can also be written X = AY + σ R, where A and σ 2 are unknown model parameters.

The p.d.f. of X is now given by (1) . . . dy (p) , which is definitely not a closed form. Since we are in a situation in which the pair of random variables is imperfectly observed through X, using the EM algorithm (chapter 16) is an option, but it may, as we shall see below, lead to heavy computation. The basic step of the EM is, given current parameters A 0 , σ 0 , to maximize the conditional expectation (knowing X, for the current parameters) of the joint log-likelihood of (X, Y ) with respect to the new parameters. In this context, the joint distribution of (X, Y ) has density

$f X (x; A, σ 2 ) = 1 (2πσ 2 ) d/2 R p e - |x-Ay| 2 2σ 2        p i=1 ψ(y (i) )        dy$$f X,Y (x, y; A, σ 2 ) = 1 (2πσ 2 ) d/2 e - |x-Ay| 2 2σ 2 p i=1 ψ(y (i) )$so that, the conditional joint likelihood over the training set is

$- N d 2 log(2πσ 2 )- 1 2σ 2 N k=1 E A 0 ,σ 0 (|x k -AY | 2 |X = x k )- N k=1 p j=1 E A 0 ,σ 2 0 (log ψ(Y (j) )|X = x k ).$Notice that the last term does not depend on A, σ 2 , and that, given A, the optimal value of σ 2 is given by

$σ 2 = 1 N d N k=1 E A 0 ,σ 0 (|x k -AY | 2 |X = x k )$The minimization of

$N k=1 E A 0 ,σ 0 (|x k -AY | 2 |X = x k ) with respect to A is a least square problem. Let b (j) k = E A 0 ,σ 0 (Y (j) |X = x k ) and s k (i, j) = E A 0 ,σ 0 (Y (i) Y (j) |X = x k ): the gradient of the previous term is -2 N k=1 E A 0 ,σ 2 0 ((x k -AY )Y T |X = x k ) = -2 N k=1 (x k b T k -AS k ),$b k being the column vector with coefficients b (j) k and S k the matrix with coefficients s k (i, j). The result therefore is

$A =        N k=1 x k b T k               N k=1 S k        -1 .$Unfortunately, the computation of the moments of the conditional distribution of

$Y given x k (needed in b k and S k ) is a difficult task. The conditional density of Y given X = x k is g(y|x k ) = ψ(y)e - |A 0 y-x| 2 2σ 2 0 /Z(A 0 , σ 0 )$from which moments cannot be computed analytically in general. Monte-Carlo sampling algorithms can be used however to approximate these moments, but they are computationally demanding. And they must be run at every step of the EM.

In place of the exact EM, one may use a mode approximation (section 16.3.1), which replaces the conditional likelihood of Y given X = x k by a Dirac distribution at the mode:

$ŷA 0 ,σ 0 (x k ) = argmax y        ψ(y)e - |A 0 y-x k | 2 2σ 2 0        .$The maximization step then reduces to maximizing in A, σ 2

$- N d 2 log(2πσ 2 ) - 1 2σ 2 N k=1 x k -A ŷA 0 ,σ 0 (x k ) 2 . (20.21)$This therefore provides a two-step procedure.

Algorithm 20.2 (Probabilistic ICA: mode approximation) (1) Initialize the algorithm with A 0 , σ 0 .

(2) At step n:

$(i) For k = 1, . . . , N , maximize p i=1 ψ(y (i) )e - |A n y-x k | 2 2σ 2 n$to obtain ŷA n ,σ n (x k ). This requires a numerical optimization procedure, such as gradient ascent. The problem is concave when log ψ is concave.

(ii) Minimize (20.21) with respect to A, σ 2 , yielding

$A n+1 =        N k=1 x k b T k               N k=1 S k        -1$with b k = ŷA n ,σ n (x k ), S k = ŷA n ,σ n (x k ) ŷA n ,σ n (x k ) T , and

$σ 2 n+1 = 1 N d N k=1 x k -A ŷA n ,σ n 2 .$(3) Stop if the variation of the parameter is below a tolerance level. Otherwise, iterate to the next step.

Once A and σ 2 have been estimated, the y components associated to a new observation x can be estimated by ŷA,σ (x), therefore minimizing

$1 2σ 2 x k -Ay 2 + p j=1 log ψ(y (j) ),$yielding the map estimate, the same convex optimization problem as in step (1) above. Now we can see how the method takes from both PCA and ICA: the columns of A, a 1 , . . . , a p can be considered as p principal directions, and are fixed after learning; they are not orthonormal, and do not satisfy the nesting properties of PCA (that those p contain those for p -1). The coordinates of x with respect to this basis is not a projection, as would be provided by PCA, but the result of a penalized estimation problem. The penalty associated to the logistic case is log ψ(y (j) ) = log 2 -2 log(e y (j) + e -y (j) ).

This distribution with "exponential tails" has the interest of allowing large values of y (j) , which generally entails sparse decompositions, in which y has a few large coefficients, and many zeros.

As an alternative to the mode approximation of the EM, which may lead to biased estimators, one may use the SAEM algorithm (section 16.4.3), as proposed in Allassonniere and Younes [[3]](#b21). Recall that the EM algorithm replaces the parameters A 0 , σ 2 0 by minimizers of

$N d 2 log(σ 2 ) + 1 2σ 2 N k=1 E A 0 ,σ 0 (|x k -AY | 2 |X = x k ) = N d 2 log(σ 2 ) + 1 2σ 2 N k=1 |x k | 2 - 1 σ 2 N k=1 x T k Ab k + 1 2σ 2 N k=1 trace(A T AS k ),$where the computation of b

$(j) k = E A 0 ,σ 0 (Y (j) |X = x k ) and s k (i, j) = E A 0 ,σ 0 (Y (i) Y (j) |X = x k )$was the challenging issue. In the SAEM algorithm, the statistics b k and S k are part of a stochastic approximation scheme, and are estimated in parallel with EM updates as follows. (1) For k = 1, . . . , N , sample y k according to the conditional distribution of Y given X = x k , using the current parameters A and σ 2 .

(2) Update b k and S k , letting (assuming step t of the algorithm)

$b k → b k + γ t (Y k -b k ) S k → S k + γ t (Y k Y T k -S k ) (3) Replace A and σ 2 by A =        N k=1 x k b T k               N k=1 S k        -1$and

$σ 2 = 1 N d N k=1 x k -A ŷA 0 ,σ 0 2 .$The parameter γ t should be decreasing with t, typically so that t γ t = +∞ and t γ 2 t < ∞ (e.g., γ t ∝ 1/t). One way to sample from Y k is to uses a rejection scheme, iterating the procedure which samples y according to the prior and accepts the result with probability M exp(-|x k -Ay| 2 /2σ 2 ) until acceptance. Here M must be chosen so that M max y exp(-|x k -Ay| 2 /2σ 2 ) ≤ 1 (e.g., M = 1). This method will work for small p, but for large p, the probability of acceptance may be very small. In such cases, Y k can be sampled changing one component at a time using a Metropolis-Hastings scheme. If component j is updated, this scheme samples a new value of y (call it y ′ ) by changing only y (j) according to the prior distribution ψ and accept the change with probability min 1, exp(-|x k -Ay ′ | 2 /2σ 2 ) exp(-|x k -Ay| 2 /2σ 2 ) .

## Non-negative matrix factorization

In this section, we consider factor analysis methods that approximate a random variable X in the form X = p j=1 a (j) Y (j) with the constraint that the scalars a (1) , . . . , a (p) ∈ R and the vectors Y (1) , . . . , Y (p) ∈ R d are respectively non-negative and with non-negative entries. This model makes sense, for example, when X represents the total multivariate production (e.g., in terms of number of molecules of various types) resulting of several chemical reactions that operate together. Another application is when X is a list of preference scores associated with a person for, say, books or movies, and each person is modeled as a positive linear combination of p "typical scorers," represented by the vector Y (j) for j = 1, . . . , p.

When training data (x 1 , . . . , x N ) is observed and stacked in an N by d matrix X , the decomposition can be summarized for all observations together in the matrix form X = AY T where A is N by p and provides the coefficients a (j) k associated with each observation and Y = [y (1) , . . . , y (p) ] is d by p and provides the p typical profiles. The matrices A and Y are unknown and their estimation subject to the constraint of having nonnegative components represent the non-negative matrix factorization (NMF) problem.

NMF is often implemented by solving the constrained optimization problem of minimizing |X -AY T | 2 subject to A and Y having non-negative entries. This problem is non-convex in general but the sub-problems of optimizing either A or Y when the other matrix is fixed are simple quadratic programs. This suggests using an alternating minimization method, iterating steps in which A is updated with Y fixed, followed by an update of Y with A fixed. However, solving a full quadratic program at each step would be computationally prohibitive with large datasets, and simpler update rules have been suggested, updating each matrix in turn with a guarantee of reducing the objective function.

If Y is considered as fixed and A is the free variable, we have

$|X -AY T | 2 = |X | 2 -2trace(X T AY T ) + trace(AY T Y A T ) = trace(A T AY T Y ) -2trace(A T (X Y )) + |X | 2 .$The next lemma will provide update steps for A. Lemma 20.14 Let M be an n by n symmetric matrix and b ∈ R n , both assumed to have non-negative entries. Let u ∈ R n , also with non-negative coefficients, and let

$v (i) = u (i)        b (i) d j=1 m(i, j)u (j)        . Then v T Mv -2b T v ≤ u T Mu -2b T u . Moreover, v = u if and only if u minimizes u T Mu -2b T u subject to u (i) = 0, i = 1, . . . , n. Proof Let F(u) = u T Mu -2b T u. We look for v (i) = β (i) u (i) with β (i) ≥ 0 such that F(v) ≤ F(u). We have F(v) = n i,j=1 β (i) β (j) u (i) u (j) m(i, j) -2 n i=1 b (i) β (i) ≤ 1 2 n i,j=1 ((β (i) ) 2 + (β (j) ) 2 )u (i) u (j) m(i, j) -2 n i=1 b (i) β (i) u (i) = n i,j=1 (β (i) ) 2 u (i) u (j) m(i, j) -2 n i=1 b (i) β (i) u (i)$When β = 1 n , this upper-bound is equal to F(u). So, if we choose β minimizing the upper-bound, we will indeed find v such that F(v) ≤ F(u). Rewriting the upperbound as

$n i=1 u (i)         (β (i) ) 2         n j=1 m(i, j)u (j)         -2b (i) β (i)        $we see that β (i) = b (i) / n j=1 m(i, j)u (j) provides such a minimizer, which proves the first statement of the lemma. For the second statement, we have v (i) = u (i) if and only if u (i) = 0 or n j=1 m(i, j)u (j) = b (i) , and one directly checks that these are exactly the KKT conditions for a minimizer of F over vectors with non-negative entries. ■ To apply the lemma to the minimization in A, let M : A → AY T Y and b = X Y (we are working in the linear space of N by p matrices). Then the update

$a (i) k → a (i) k (X Y )(i, k) (AY T Y )(i.k)$decreases the objective function.

Similarly, applying the lemma with the operator Y → Y A T A and b = X T A gives the update for Y , namely

$y (i) j → y (i) j (X T A)(i, j) (Y A T A)(i, j) .$We have therefore obtained the following algorithm.

Algorithm 20.4 (NMF, quadratic cost) 1. Fix p > 0 and let X be the N by d matrix containing the observed data. Initialize the procedure with matrices A and Y , respectively of size N by p and d by p, with positive coefficients.

2. At a given stage of the algorithm, let A and Y be the current matrices providing an approximate decomposition of X .

3. For the next step, let Ã be the matrix with coefficients

$ã(i) k = a (i) k (X Y )(i, k) (AY T Y )(i, k)$and Ỹ the matrix with coefficients An alternative version of the method has been proposed, where the objective function is Φ(AY T ), where, for an N by

$ỹ(i) j = y (i) j (X T Ã)(i, j) (Y ÃT Ã)(i, j) .$$d matrix Z = [z 1 , . . . , z N ] T , Φ(Z) = N k=1 d i=1 (z (i) k -x (i) k log z (i) k )$which is indeed minimal for Z = X . We state and prove a second lemma that will allow us to address this problem.

Lemma 20.15 Let M be an n by q matrix and x ∈ R n , b ∈ R q , all assumed to have positive entries. For u ∈ (0, +∞) q , define

$F(u) = q j=1 b (j) u (j) - n i=1 x (i) log q j=1 m(i, j)u (j) . Define v ∈ (0, +∞) q by v (j) = u (j) n i=1 m(i, j)x (i) /α (i) b (j)$with α (i) = q k=1 m(i, k)u (k) . Then F(v) ≤ F(u). Moreover, v = u if and only if u minimizes F subject to u (i) ≥ 0, i = 1, . . . , n.

Proof Introduce a variable β (j) > 0 for j = 1, . . . , q an let w (j) = u (j) β (j) . Then

$F(w) = q j=1 b (j) u (j) β (j) - n i=1 x (i) log q j=1 m(i, j)u (j) β (j) = q j=1 b (j) u (j) β (j) - n i=1 x (i) log q j=1 m(i, j)u (j) β (j) q j=1 m(i, j)u (j) - n i=1 x (i) log q j=1 m(i, j)u (j)$Let ρ(i, j) = m(i, j)u (j) /α (i) . Since the logarithm is concave, we have log q j=1 ρ(i, j)β (j) ≥ q j=1 ρ(i, j) log β (j)   so that

$F(w) ≤ q j=1 b (j) u (() jβ (j) - n i=1 q j=1 x (i) ρ(i, j) log β (j) - n i=1 x (i) log q j=1 m(i, j)u (j) .$The upper bound with β (j) ≡ 1 gives F(u), so minimizing this expression in β will give F(w) ≤ F(u). This minimization is straightforward and gives

$β (j) = n i=1 x (i) ρ(i, j) b (j) u (j) = n i=1 m(i, j)x (i) /α (i) b (j)$and the optimal w is the vector v provided in the lemma. Finally, one checks that v = u if and only if u satisfies the KKT conditions for the considered problem.

## ■

We can now apply this lemma to derive update rules for Y and A, where the objective is

$N k=1 d i=1 p j=1 y (i) j a (j) k - N k=1 d i=1 x (i) k log p j=1 y (i) j a (j) k .$Starting with the minimization in A, we apply the lemma to each index k separately, taking n = d and q = p, with b (j) = d i=1 y

(i) j and m(i, j) = y (j)

i . Then the update is

$a k (j) → a k (j) d i=1 x (i) k y (i) j /α (i) k d i=1 y (i) j with α (i) k = p j=1 y (i) j a (j) k .$For Y , we can work with fixed i and apply the lemma with n = N , q = p, b

$(j) = N k=1 a (j) k and m(k, j) = a (j)$k . This gives the update:

$y (i) j → y (i) j N k=1 x (i) k a (j) k /α (i) k N k=1 a (j) k , still with α (i) k = p j=1 y (i) j a (j) k .$We summarize this in our second algorithm for NMF. 

$(j) k = a (j) k d i=1 x (i) k y (i) j /α (i) k d i=1 y (i) j with α (i) k = p j=1 y (i) j a (j) k . 5. Let Ỹ the matrix with coefficients ỹ(i) j = y (i) j N k=1 x (i) k ã(j) k / α(i) k p j=1 ã(j) k with α(i) k = p j=1 y (i) j ã(j) k . 6.$Replace A by Ã and Y by Ỹ , iterating until numerical convergence.

## Variational Autoencoders

Variational autoencoders, which were described in section 18.2.2, can be ineterpreted as a non-linear factor model in which X = g(θ, Y ) + ϵ where ϵ is a centered Gaussian noise with covariance matrix Q and Y ∈ R p has a known probability distribution, such as Y ∼ N (0, Id R p ). In this framework, the conditional distribution of Y given X = x was approximated as a Gaussian distribution with mean µ(x, w) and covariance matrix S(x, w) 2 . The implementation in Kingma and Welling [[103,](#b121)[104]](#b122) use neural networks for the three functions g, µ and S.

## Bayesian factor analysis and Poisson point processes 20.9.1 A feature selection model

The expectation in many factor models is that individual observations are obtained by mixing pure categories, or topics, and represented as a weighted sum or linear combination of a small number of uncorrelated or independent variables. Denote p the number of possible categories, which, in this section, can be assumed to be quite large.

We will assume that each observation randomly selects a small number among these categories before combining them. Let us consider (as an example) the following model.

• The observations X 1 , . . . , X N take the form of a probabilistic ICA model

$X k = p j=1 a k (j)b k (j)Y (j) + σ R k ,$where:

• R k follows a standard Gaussian distribution,

• a k (1), . . . , a k (p) are independent with a k (j) ∼ N (m j , τ 2 j ), • b k (1), . . . , b k (p) are independent and follow a Bernoulli distribution with parameter π j ,

• Y (1) , . . . , Y (p) are independent standard Gaussian random variables.

• σ 2 follows an inverse gamma distribution with parameters α 0 , β 0 .

• τ 2 1 , . . . , τ 2 p follow independent inverse gamma distributions with parameters α 1 , β 1 . • m j follow a Gaussian N (0, ρ 2 ) and,

• π j follow a beta distribution with parameters (u, v).

The priors are, as usual, chosen so that the computation of posterior distributions is easy, i.e., they are conjugate priors. The observed data is therefore obtained by selecting components Y j with probability π j and weighted with a Gaussian random coefficient, then added before introducing noise.

Let n j = N k=1 b k (j). Ignoring constant factors, the joint likelihood of all variables together is proportional to:

$L ∝σ -N d exp         - 1 2σ 2 N k=1 |X k - p j=1 a k (j)b k (j)Y (j) | 2         p j=1        τ -N j exp        - 1 2τ 2 j N k=1 (a k (j) -m j ) 2               exp         - 1 2ρ 2 p j=1 m 2 j         p j=1 π n j j (1 -π j ) N -n j p j=1 (τ 2 j ) -α 1 -1 exp(-β 1 /τ 2 j ) (σ 2 ) α 0 -1 exp(-β 0 /σ 2 ) p j=1 π u-1 j (1 -π j ) v-1 exp        - 1 2 p i=1 |Y (i) | 2       $In spite of the complexity of this expression, it is relatively straightforward (by considering each variable in isolation) to see that

• The conditional distribution of σ 2 , τ 2  1 , . . . , τ 2 p given all other variables remains a product of inverse gamma distributions.

• The conditional distribution of Y (1) , . . . , Y (p) given the other variables is Gaussian.

• The conditional distribution of π 1 , . . . , π p given the other variables is a product of beta distributions.

• The conditional distribution of m 1 , . . . , m p given the other variables remain independent Gaussian.

• The posterior distribution of a 1 , . . . , a N (considered as p-dimensional vectors) given the other variables is a product of independent Gaussian (but the components a k (j), j = 1, . . . , p are correlated).

• For the posterior distribution given the other variables, b 1 , . . . , b N (considered as p-dimensional vectors) are independent. The components of each b k are not independent but each b k (j) being a binary variable follows a Bernoulli distribution given the other ones.

These remarks provide the basis of a Gibbs sampling algorithm for the simulation of the posterior distribution of all unobserved variables (the computation of the parameters of each of the conditional distribution above requires some work, of course, and these details are left to the reader). This simulation does not explicitly provide a matrix factorization of the data (in the sense of a single matrix A such that X = AY , as considered in the previous section), but a probability distribution on such matrices, expressed as A(k, j) = a k (j)b k (j). One can however use the average of the matrices obtained through the simulation for this purpose. Additional information can be obtained through this simulation. For example, the expectation of b k (j) provides a measure of proximity of observation k to category j.

## Non-negative and count variables

Poisson factor analysis. Many variations can be made on the previous construction. When the observations are non-negative, for example, an additive Gaussian noise may not be well adapted. Alternative models should model the conditional distribution of X given a, b and Y as a distribution over non-negative numbers with mean (a ⊙ b) T Y (for example a gamma distribution with appropriate parameters). The posterior sampling generally is more challenging in this case because simple conjugate priors are not always available.

An important special case is when X is a count variable taking values in the set of non-negative integers. In this case (starting with a model without feature selection), modeling X as a Poisson variable with mean a(1)Y (1) + • • • + a(p)Y (p) leads to tractable computations, once it is noticed that X can be seen as a sum of random variables Z [1] , . . . , Z [p] where Z [i] follows a Poisson distribution with parameter a(i)Y (i) . This suggests introducing new latent variables (Z [1] , . . . , Z [p] ), which are not observed but follow, conditionally to their sum, which is X and is observed, a multinomial distribution with parameters X, q 1 , . . . , q p , with q i = a(i)Y (i) /( p j=1 a(j)Y (j) ).

This provides what is referred to as a Poisson factor analysis (PFA). As an example, consider a Bayesian approach where, for the prior distribution, a(1), . . . , a(p) are independent and follow as a gamma distribution with parameters α 0 and β 0 , and Y (1) , . . . , Y (p) are independent, exponentially distributed with parameter 1. The joint likelihood of all data then is (up to constant factors):

$L ∝ exp        - N k=1 (a k (1)Y (1) + • • • + a k (p)Y (p) )                N k=1 p i=1 (a k (i)Y (i) ) z [i] k z [i] k !                N k=1 p i=1 a k (i) α-1        exp        -β N k=1 p i=1 a k (i)        exp        - p i=1 Y (i)        .$This is the GaP (For Gamma-Poisson) model introduced in Canny [[50]](#b68). The conditional distribution of the variables (a k (i)) given (Z [i] k ) and (Y i ) are independent and gamma-distributed, and so are (Y (i) ) given the other variables. Finally, for each k, the family (Z [[1]](#b19) k , . . . , Z

[p] k ) follows a multinomial distribution conditionally to their sum, X k , and the rest of the variables, and these variables are conditionally independent across k.

GaP with feature selection One can include a feature selection step in this model by introducing binary variables b(1), . . . , b(p), with selection probabilities π 1 , . . . , π p , with a Beta(u, v) prior distribution on π i . Doing so, the likelihood of the extended model is:

$L ∝ exp        - N k=1 (a k (1)b k (1)Y (1) + • • • + a k (p)b k (p)Y (p) )                N k=1 p i=1 (a k (i)b k (i)Y (i) ) z [i] k z [i] k !                N k=1 p i=1 a k (i) α-1        exp        -β N k=1 p i=1 a k (i)        exp        - p i=1 Y (i)        p j=1 π n j j (1 -π j ) N -n j p j=1 π u-1 j (1 -π j ) v-1 .$where, as before, n j = N k=1 b k (j). The conditional distribution of π 1 , . . . , π p given the other variables is therefore still that of a family of independent beta-distributed variables. The binary variables b k (1), . . . , b k (p) are also conditionally independent given the other variables, with b k (i) = 1 with probability one if z

$[i] k > 0 and with probability π j exp(-a k (j)Y (j) ) if z [j] k = 0.$
## Feature assignment model

The previous models assumed that p features were available, modeled as p random variables with some prior distribution, and that each observation picks a subset of them, drawing feature j with probability π j . We denoted by b k (j) the binary variable indicating whether feature j was selected for observation k, and n j was the number of times that feature was selected. Finally, we modeled π j as a beta variable with parameters u and v.

One can compute, using this model, the probability distribution of of the feature selection variables, b = (b k (j), j = 1, . . . , p, k = 1, . . . , N ). From the model definition, the probability of observing such a configuration is given by

$Q(b) = Γ (u + v) p Γ (u) p Γ (v) p p j=1 π n j +u-1 j (1 -π j ) N -n j +v-1 dπ 1 . . . dπ p = p j=1 Γ (u + v)Γ (u + n j )Γ (v + N -n j ) Γ (u)Γ (v)Γ (u + v + N ) = p j=1 u(u + 1) • • • (u + n j -1)v(v + 1) • • • (v + N -n j -1) (u + v)(u + v + 1) • • • (u + v + N -1)$Denote by n jk = k-1 l=1 b l (j) the number of observations with index less than k that pick feature j. Using this notation, we can write, using the fact that

$u(u + 1) • • • (u + n j -1) = N k=1 (u + n jk ) b k (j)$and a similar identity for v(v + 1)

$• • • (v + N -n j -1), Q(b) = p j=1 N k=1 (u + n jk ) b k (j) (v + k -1 -n jk ) 1-b k (j) u + v + k -1 = N k=1 p j=1 u + n jk u + v + k -1 b k (j) v + k -1 -n jk u + v + k -1 1-b k (j)$.

Using this last equation, we can interpret the probability Q as resulting from a progressive feature assignment process. The first observation, k = 1, for which n jk = 0 for all j, chooses each feature with probability u/(u + v). When reaching observation k, feature j is chosen with probability (u + n jk )/(u + v + k -1). At all steps, features are chosen independently from each other.

Let F k be the set of features assigned to observation k, i.e., F k = {j : b k (j) = 1} and

$G k = F k \ k-1 l=1 F l$be the set of features used in observation k but in no previous observation. Let 

$C k = F k \ G k and U k = G 1 ∩ • • • ∩ G k-1 .$$Q(S) = N k=1       u u + v + k -1 q k v + k -1 u + v + k -1 p-p k+1 j∈U k u + n jk u + v + k -1 1 j∈C k v + k -1 -n jk u + v + k -1 1 j C k       . Let S k = (G l , C l , l ≤ k).$Then the expression of Q shows that, conditionally to S k-1 , G k and C k are independent. Elements in C k are chosen independently for each feature j ∈ U k with probability (u + n jk )/(u + v + k -1). Moreover, the conditional distribution of q k given S k-1 is proportional to

$u u + v + k -1 q k v + k -1 u + v + k -1 p-p k -q k$i.e., it is a binomial distribution with parameters pp k and u/(u + v + k -1). Finally, given s k-1 and q k , the distribution of G k is uniform among all p-p k q k subsets of

${1, . . . , p} \ (G 1 ∪ • • • ∪ G k-1 ) with cardinality q k .$If there is no special meaning in the feature label, which is the case in our discussion of prior models in which all features are sampled independently with the same distribution, we may identify configurations that can be deduced from each other by relabeling (note that relabeling features does not change the value of Q).

Call a configuration normal if G k = {p k + 1, . . . , p k+1 }. Given S, it is always possible to relabel the features with a permutation σ so that, for each k, σ (G k ) = {p k + 1, . . . , p k+1 }. There are, in fact, q 1 ! . . . q N ! such permutations. We can complete the process generating S by adding at the end a transformation into a normal configuration (picking uniformly at random one of the possible ones). The probability of a normal configuration S obtained through this process is (using a simple counting argument)

$Q(S) = N k=1       p -p k q k u u + v + k -1 q k v + k -1 u + v + k -1 p-p k+1 j∈U k u + n jk u + v + k -1 1 j∈C k v + k -1 -n jk u + v + k -1 1 j C k       ,$This provides a new incremental procedure that directly samples normalized assignments. First let q 1 follow a binomial distribution bin(p, u/(u + v)) and assign the first observation to features 1 to q 1 . Assume that p k labels have been created before step k. Then select for observation k some of the already labeled features, label j being selected with probability (u + n jk )/(u + v + k -1) as above. Finally, add q k new features where q k follows a binomial distribution bin(pp k , u/(u

$+ v + k -1)).$This discussion is clearly reminiscent of the one that was made in section 19.7.3 leading to the Polya urn process, and we want here also to let p tend to infinity (with fixed N ) with proper choices of u and v as functions of p in the expression above. Choose two positive numbers c and γ and let u = cγ/p and v = cu. Note that, with the incremental simulation process that we just described, the conditional expectation of the next number of labels, p k+1 given the current one, p k is

$E(p k+1 |p k ) = (p -p k )u u + v + k -1 + p k = cγ c + k -1 + 1 - cγ p(c + k -1) p k ≤ cγ c + k -1 + p k$Taking expectations on both sides, we get

$E(p k+1 ) ≤ k l=1 cγ c + l -1 ≤ N l=1 cγ c + l -1$so that this expectation is bounded independently of k. This shows in particular that p k /p tends to 0 in probability (just applying Markov's inequality) and that the binomial distribution bin(pp k , u/(u + v + k -1)) can be approximated by a Poisson distribution with parameter cγ/(c + k -1). So, when p → ∞, we obtain the following incremental simulation process for the feature labels, that we combine with the actual simulation of the features, assumed to follow a prior distribution with p.d.f. ψ. This process is called the Indian buffet process in the literature, the analogy being that a buffet offers an infinite variety of dishes, and each observation is a customer who tastes a finite number of them. (i) Sample an integer q 1 according to a Poisson distribution with parameter γ.

(ii) Sample features y (1) , . . . , y (q 1 ) according to ψ.

(iii) Assign these features to observation 1, and let n 2,j = 1 for j = 1, . . . , q 1 .

2. Assume that observations 1 to k-1 have been obtained, with p k features y (1) , . . . , y (p k ) such that the jth feature has been chosen n k,j times.

(i) For j = 1, . . . , p k , assign feature j to sample k with probability

$n k,j c+k-1 . If j is selected, let n k+1,j = n k,j + 1, otherwise let n k+1,j = n k,j .$(ii) Sample an integer q k according to a Poisson distribution with parameter cγ c+k-1 and let p k+1 = p k + q k . (iii) Sample features y (p k +1) , . . . , y (p k+1 ) according to ψ.

(iv) Assign these features to observation k, and let n k+1,j = 1 for j = p k +1, . . . , p k .

3. If k = N , stop, otherwise replace k by k + 1 and return to Step 2.

## Point processes and random measures

This section assumes that the reader is familiar with measure theory. It can however safely be skipped as it is not reused in the rest of the book.

## Poisson processes

If Z is a set, we will denote by P c (Z) the set composed with all finite or countable subsets of Z. A point process over Z is a random variable S : Ω → P c (Z), i.e., a variable that provides a countable random subset of Z. If B ⊂ Z one can then define the counting function ν

$S (B) = |S ∩ B| ∈ Z ∪ {+∞}.$A proper definition of such point processes requires some measure theory. Equip Z with a σ -algebra A and consider the set N 0 of integer-valued measures µ on (Z, A) such that µ(Z) < ∞. Let N be the set formed with all countable sums of measures in N 0 . Then a general point process is a mapping ν : Ω → N such that for all k ∈ N ∪ {+∞} and all B ∈ A, the event {ν(B) = k} is measurable. Recall that, for each B ∈ A, ν(B) is itself a random variable, that we may denote ω → ν ω (B). One then define the intensity of the process as the the function µ : B → E(ν(B)).

The following proposition provides an important identity satisfied by such models.

Theorem 20.16 (Campbell identity) Let ν be a point process with intensity µ. For ω ∈ Ω, let X ω : Ω ′ → Z be a random variable with distribution ν ω (defined, if needed, on a different probability space (Ω ′ , P ′ )). Then, for any µ-integrable function f :

$E(f (X)) = Z f (z)dµ(z). (20.22)$Here, the expectation of f (X) is over both spaces Ω and Ω ′ and corresponds to the average of f . The identity is an immediate consequence of Fubini's theorem.

We will be mainly interested in the family of Poisson point processes. These processes are themselves parametrized by a measure, say µ, on Z such that µ is σfinite and µ(B) = 0 if B is a singleton. A Poisson process with intensity measure µ is a point process ν such that:

(i) If B 1 , . . . , B n are non-intersecting pairwise, then ν(B 1 ), . . . , ν(B n ) are mutually independent.

## (ii) for all B, ν(B) ∼ Poisson(µ(B)).

We take the convention that ν(B) = 0 (resp. = ∞) almost surely if µ(B) = 0 (resp. = ∞). Note that property (i) also implies that if g 1 , . . . , g n are measurable functions from Z to (0, +∞) such that g i g j = 0 for i j, then the variables ν(g i ) = Z g i (z)dν(z) are independent.

If µ(Z) < ∞ (i.e., µ is finite), one can represent the distribution of a Poisson point process as follows:

$ν = ν(Z) k=1 δ X k with ν(Z) ∼ Poisson(µ(Z))$and, conditional to ν(Z) = N , X 1 , . . . , X N are i.i.d. and follow the probability distribution μ = µ/µ(Z). This measure can also be identified with the random set S = {X 1 , . . . , X ν(Z) }. The assumption that µ({z}) = 0 for any singleton implies that ν({z}) = 0 almost surely. It also ensures that the points X 1 , . . . , X N are distinct with probability one.

If µ is σ -finite, then (by definition), it is a countable sum of finite measures µ 1 , µ 2 , . . .. Then ν can be generated as the sum of independent ν 1 , ν 2 , . . ., where ν i is a Poisson process with intensity µ i . It can moreover be identified with the countable random set S = ∞ i=1 S i , where S i is the random set associated with ν i . Note that, in this construction, one can always assume that the measures µ 1 , µ 2 , . . . are mutually singular (i.e., µ i (B) > 0 for some i implies that µ j (B) = 0 for j i).

If we consider a Poisson process on (0, +∞) × Z, we can define weighted random measures. Indeed, such a point process takes values in the collection of all sets of the form {(w k , z k ), k ∈ I} where I is finite or countable. These subsets can be represented as the sum of weighted Dirac masses,

$ξ = k∈I w k δ z k .$To ensure that the points (z k , k ∈ I) generated by this process are all different, we need to assume that the intensity µ of this random process is such that µ((0, +∞) × {z}) = 0 for all z ∈ Z. We will refer to ξ as a weighted Poisson process.

In the following, we will consider this class of random measures, with the small addition of allowing for an extra term including a measure supported by a fixed set. More precisely, given a (deterministic) countable subset I ⊂ Z, a family of independent random variables (ρ z , z ∈ I ) and a σ -finite measure µ o such that µ o ((0, +∞) × {z}) = 0 for all z ∈ Z, we can define the random measure

$ξ = ξ f + ξ o$where ξ o is a weighted Poisson process with intensity µ o , assumed independent of (ρ z , z ∈ I ) and

$ξ f = z∈I ρ z δ z .$The subscripts o and f come from the terminology introduced in Kingman [[105]](#b123), which studies "completely random measures," which are a random measures that satisfy point (i) in the definition of a Poisson process. Under mild assumptions, such measures can be decomposed as a sum of a weighted Poisson process (here, ξ o , the ordinary part), of a process with fixed support, (here, ξ f , the fixed part) and of a deterministic measure (which is here taken to be 0).

Let us rapidly check that ξ satisfies property (i). Let B 1 , . . . , B n be non-overlapping elements of A. Get g i (w, z) = w1 B i (z). Then

$ξ(B i ) = ξ f (B i ) + ν o (g i )$where ν o is a Poisson process with intensity µ o . Since the sets do not overlap, the variables (ξ f (B i ), i = 1, . . . , n) are independent, and so are (ν o (g i ), i = 1, . . . , n) since g i g j = 0 for i j. Since ξ f and ν o are, in addition independent, we see that (ξ(B i ), i = 1, . . . , n) are independent.

The intensity measure of such a process is still defined by

$η(B) = E(ξ(B)) = z∈I P ((ρ z , z) ∈ B) + (0,+∞)×B wdµ o (w, z)$where the last term is an application of Campbell's inequality to the Poisson process ν o and the function g(w, x) = w1 B (x).

## The gamma process

The main example of such processes in factor analysis is the beta process that will be discussed in the next section. We start, however, with a first example that is closely related with the Dirichlet process, called the gamma process.

In this process, one fixes a finite measure π 0 on Z and defines µ on (0, +∞) × Z by µ(dw, dz) = cw -1 e -cw π 0 (dz)dw.

Because µ is σ -finite but not finite (the integral over t diverges at t = 0), every realization of ξ is an infinite sum

$ξ = ∞ k=1 w k δ z k . The intensity measure of ξ is η(B) = cπ 0 (B) +∞ 0 e -cw dw = π 0 (B).$In particular,

$∞ k=1 w k = η(Z) = π 0 (Z) < ∞.$For fixed B, the variable ξ(B) follows a Gamma distribution. This can be proved by computing the Laplace transform of ξ, E(e -λξ(B) ), and identify it to that of a Gamma. To make this computation, consider the point process ν J restricted to a interval J ⊂ (0, +∞) with min(J) > 0, and ξ J the corresponding weighted process. Let m J (t) = J w -1 ce -(c+t)w dw. Then a realization of ν J can be obtained by first sampling N from a Poisson distribution with parameter µ(J × Z) = m J (0)π 0 (Z) and then sampling N points (w i , z i ) independently from the distribution µ/(m J (0)π 0 (Z)). This implies that

$E(e -tξ J (B) ) = ∞ n=0 e -m J (0)π 0 (Z) (m J (0)π 0 (Z)) n n!        ∞ 0 e -tw1 B (z) w -1 ce -cw dwdπ 0 (z) m J (0)π 0 (Z)        n = ∞ n=0 e -m J (0)π 0 (Z) n! π 0 (B)m J (t) + (π 0 (Z) -π 0 (B))m J (0) n = e π 0 (B)(m J (t)-m J (0)) . Now, m J (t) -m J (0) = c J$e cw e -tw -1 w dw is finite even when J = (0, +∞). With a little more work justifying passing to the limit, one finds that, for J = (0, +∞),

E(e -tξ J (B) ) = exp π 0 (B) +∞ 0 e -cw e -tw -1 w dw . ).

This shows that

$E(e -tξ J (B) ) = 1 + t c -cπ 0 (B)$which is the Laplace transform of a Gamma distribution with parameters cπ 0 (B) and c, i.e., with density proportional to w cπ 0 (B)-1 e -cw .

As a consequence, the normalized process δ = ξ/ξ(Z) is a Dirichlet process with intensity cπ 0 . Indeed, if B 1 , . . . , B n is a partition of Z the family (δ(B 1 ), . . . , δ(B n )) is the ratio of n independent gamma variables to their sum, which provides a Dirichlet distribution, and this property characterizes Dirichlet processes.

## The beta process

The definition of the beta process parallels that of the gamma process, with weights taking this time values in (0, 1). Fix again a finite measure π 0 on Z and let µ o on (0, +∞) × Z be defined by

$µ o (dw, dz) = cw -1 (1 -w) c-1 π 0 (dz)dw.$The associated weighted Poisson process can therefore be represented as a sum

$ξ o = ∞ k=1 w k δ z k ,$and its intensity measure is

$η o (B) = cπ 0 (B) 1 0 (1 -t) c-1 dw = π 0 (B).$In particular, since π 0 is finite, we have ∞ k=1 w k < ∞ almost surely. A beta process is the sum of the process ξ o and of a fixed set process

$ξ f = z∈I w z δ z$where I is a fixed finite set and (w z , z ∈ I ) are independent and follow a beta distribution with parameters (a(z), b(z)).

If Z is a space of features, such a process provides a prior distribution on feature selections. It indeed provides, in addition to the deterministic set I , a random countable set J ⊂ Z, with a set of random weights w z , z ∈ F := I ∪ J . Given this, one defines the feature process as the selection of a subset A ⊂ F where each feature z is selected with probability w z . Because E(|A|) = z∈F w z is finite, A is finite with probability 1.

In the same way the Polya urn could be used to sample from a realization of a Dirichlet process without actually sampling the whole process, there exists an algorithm that samples a sequence of feature sets (A 1 , . . . , A n ) from this feature selection process without needing the infinite collection of weights and features associated with a beta process. We assume in the following that the prior process has an empty fixed set. (Non-empty fixed sets will appear in the posterior.)

The first set of features, A 1 , is obtained as follows according to a Poisson process with intensity π 0 : choose the number N of features in A 1 according to a Poisson distribution with parameter π 0 (Z). Then sample N features independently according to the distribution π 0 /π 0 (Z). Now assume that n -1 sets of features A 1 , . . . , A n have been obtained and we want to sample a new set A n+1 conditionally to their observation. Let J n be the union of all random features obtained up to this point and n(z), for z ∈ J n the number of times this feature was observed in A 1 , . . . , A n . Then the conditional distribution of the beta process ξ given this observation is still a beta process, with fixed set given by I = J n , (a(z), b(z)) = (n(z), c + nn(z)) for z ∈ J n-1 and base measure π n = cπ 0 /(c + n). This implies that the next set A n+1 can be obtained by sampling from the associated feature process. To do this, one first selects features z ∈ J n with probability n(z)/(c + n), then selects additional features z 1 , . . . , z N independently with distribution π 0 /π 0 (Z) where N follows a Poisson distribution with parameter cπ 0 (Z)/(c + n). This is the Indian buffet process, described in Algorithm 20.6 (taking π 0 = γψ).

## Beta Process and feature selection

The beta process can be used as a prior for feature selection within a factor analysis model, as described in the previous paragraph. It is however easier to approximate it with a model with almost surely finite support. Indeed, letting, for ϵ > 0

$µ(dw, dz) = Γ (c + 1) Γ (ϵ + 1)Γ (c -ϵ) w ϵ-1 (1 -w) c-ϵ π 0 (dz)dw, one obtains a finite measure since +∞ 0 Z µ o (dw, dz) = cγ ϵ$where γ = π 0 (Z). Note that µ is normalized so that E(ξ(B)) = π 0 (B) for B ⊂ Z.

In this case, the prior generates features by first sampling their number, p, randomly according to a Poisson distribution with mean cγ/ϵ, then select p probabilities w 1 , . . . , w p independently using a beta distribution with parameters ϵ and cϵ, and finally attach to each i a feature z i with distribution π 0 /γ. The features associated with a given sample are then obtained by selecting each z i with probability w i .

We note also that the model described in section 20.9.3 provides an approximation of this prior using a finite number of features. With our notation here, this corresponds to taking p ≫ 1 and ϵ = cγ/p.

## Chapter 21

Data Visualization and Manifold Learning

## Multidimensional scaling

The methods described in this chapter aim at representing a dataset in low dimension, allowing for its visual exploration by summarizing its structure in a useraccessible interface. Unlike factor analysis methods, they do not necessarily attempt at providing a causal model expressing the data as a function of a small number of sources, and generally do not provide a direct mechanism for adding new data to the representation. In addition, all these methods take as input similarity dissimilarity matrices between data points and do not require, say, Euclidean coordinates.

Assuming that a dissimilarity matrix D = (d kl , k, l = 1, . . . , N ) is given, the goals of multidimensional scaling (or MDS) is to determine a small-dimensional Euclidean representation, say y 1 , . . . , y N ∈ R p , such that y ky l 2 ≃ d 2 kl . We review below two versions of this algorithm, referred to as "similarity" and "dissimilarity" matching.

## Similarity matching (Euclidean case)

We start with the standard hypotheses of MDS, assuming that the distances d kl derive from a representation in feature space, so that d 2 kl = ∥h kh l ∥ 2 H for some innerproduct space H and (possibly unknown) features h 1 , . . . , h N . Note that, since the Euclidean distance is invariant by translation, there is no loss of generality in assuming h 1 + • • • + h N = 0, which will be done in the following.

We look for a p-dimensional representation in the form y k = Φh k where Φ is a linear transformation (and we want y k to be computable directly from dissimilarities, since we do not assume that h k is known). Since we are only interested in a transformation of the h 1 , . . . , h N , it suffices to compute Φ in the vector space generated by them, so that we let Φ : span(h 1 , . . . , h N ) → R p , and we want Φ to (approximately) conserve the norm, i.e., be close to being an isometry.

Because isometries are one-to-one and onto, the existence of an exact isometry would require V ∆ = span(h 1 , . . . , h N ) to be p-dimensional. The mapping Φ could then be defined as Φ(h) = (⟨h , e 1 ⟩ H , . . . , ⟨h , e p ⟩ H ) where e 1 , . . . , e p is any orthonormal basis of V . In the general case, however, where V is not p-dimensional or less, one can replace it by a best p-dimensional approximation of the training data, leading to a problem similar to PCA in feature space.

Indeed, as we have seen in section 20.1.2, this best approximation can be obtained by diagonalizing the Gram matrix S of h 1 , . . . , h N , which is such that s kl = ⟨h k , h l ⟩ H . (Recall that we assume that h = 0, so we do not center the data here.) Using the notation in section 20.1.2, let α (1) , . . . , α (p) denote the eigenvectors associated with the p largest eigenvalues, normalized so that (α (i) ) T Sα (i) = 1 for i = 1, . . . , p. One can then take

$e i = N l=1 α (i) l h l$and, for k = 1, . . . , N , j = 1, . . . , p: y

$(i) k = λ 2 i α (i) k (21.1)$where λ 2 i is the eigenvalue associated with α (i) .

This does not entirely address the original problem, since the inner products s kl are not given, but only the distances d kl , which satisfy

$d 2 kl = -2s kl + s kk + s ll . (21.2)$This provides a linear system of equations in the unknown s kl . This system is underdetermined, because D is invariant by any transformation h k → h k + h 0 (for a fixed h 0 ), and S is not. However, the assumption h 1 +

• • • + h N = 0 provides the additional constraint needed to provide a unique solution. Summing (21.2) over l, we then get N l=1 d 2 kl = N s kk + N l=1 s ll . (21.3) Summing this equation over k, we find N k,l=1 d 2 kl = 2N N l=1 s ll . Using this in (21.3), we get s kk = 1 N N l=1 d 2 kl -1 2N 2 N k,l=1 d 2 kl , and, from (21.2)

$s kl = - 1 2         d 2 kl - 1 N N k ′ =1 d 2 k ′ l - 1 N N l ′ =1 d 2 kl ′ + 1 N 2 N k ′ ,l ′ =1 d 2 k ′ l ′         .$If we denote by D ⊙2 the matrix formed with the squared distances d 2 kl , this identity can we rewritten in the simpler form

$S = - 1 2 P D ⊙2 P (21.4) with P = Id R N -1 N 1 T N /N .$We now show that this PCA approach to MDS is equivalent to the problem of minimizing

$F(y) = N k,l=1 (y T k y l -s kl ) 2(21.5)$over all y 1 , . . . , y N ∈ R p such that y 1 +• • •+y N = 0, which can be interpreted as matching "similarities" s kl rather than distances. Indeed, letting Y denote the N by p matrix with rows y T 1 , . . . , y T N , we have

$F(y) = trace((Y Y T -S) 2 ).$Finding Y is equivalent to finding a symmetric matrix M of rank p minimizing trace((M -S) 2 ). We have, using the trace inequality (theorem 2.1), and letting λ

$2 1 ≥ • • • ≥ λ 2 N (resp. µ 2 1 ≥ • • • ≥ µ 2 p ) denote the eigenvalues of S (resp. M) trace((M -S) 2 ) = trace(M 2 ) -2trace(MS) + trace(S 2 ) = p k=1 µ 4 k -2trace(MS) + N k=1 λ 4 k ≥ p k=1 µ 4 k -2 p k=1 λ 2 k µ 2 k + N k=1 λ 2 k = p k=1 (λ 2 k -µ 2 k ) 2 + N k=p+1 λ 4 k ≥ N k=p+1 λ 4 k$for any connected component Γ of this graph. (If all weights are positive, then the only non-empty connected component is {1, . . . , N } and we retrieve our previous constraint N k=1 y k = 0.)

Standard nonlinear optimization methods, such as projected gradient descent, may be used to minimize G, but the preferred algorithm for MDS uses a stepwise procedure resulting from the addition of an auxiliary variable. Rewrite

$G(y) = N k,l=1 w kl |y k -y l | 2 -2 N k,l=1 w kl d kl |y k -y l | + N k,l=1 d 2 kl .$We have, for u ∈ R p : |u| = max{z T u : z ∈ R p , |z| = 1 u 0 } . Using this identity, we can introduce auxiliary variables z kl , k, l = 1, . . . , N in R p , with

$|z kl | = 1 if y k y l and define Ĝ(y, z) = N k,l=1 w kl |y k -y l | 2 -2 N k,l=1 w kl d kl (y k -y l ) T z kl + N k,l=1 d 2 kl .$We then have

$G(y) = min z:|z kl |=1 if y k y k Ĝ(y, z).$As a consequence, minimizing G in y can be achieved by minimizing Ĝ in y and z and discarding z when this is done. One can minimize Ĝ iteratively, alternating minimization in y given z and in z given y, both steps being elementary. In order to describe these steps, introduce some matrix notation.

Let L denote the Laplacian matrix of the weighted graph on {1, . . . , N } associated with the weight matrix W , namely L = (ℓ kl , k, l = 1, . . . , N ) with ℓ kk = N k=1 w klw kk and ℓ kl = -w kl when k l. Then,

$N k,l=1 w kl |y k -y l | 2 = 2trace(Y T LY ). Defining u k ∈ R p by u k = N l=1 w kl d kl (z kl -z lk ), and U =           u T 1 . . . u T N           , we have N k,l=1 w kl d kl (y k -y l ) T z kl = trace(U T Y ).$With this notation, the optimal matrix Y must minimize 2trace(Y T LY ) -2trace(U T Y ).

Let m be the number of connected components of the weighted graph. Recall that the matrix L is positive semi-definite and that an orthonormal basis of its null space is provided by vectors, say e 1 , . . . , e m , that are constant on each of the m connected components of the graph, so that the constraint on Y can be written as e where we have used the fact that L-1 e j = e j . We can now identify µ j since

$0 = e T j Y = 1 2 e T j L-1 U - 1 4 m j ′ =1 e T j e j ′ µ T j = 1 2 e T j U - 1 4 µ T j so that µ T j = 2e T j U and the optimal Y is Y = 1 2 L-1 U - 1 2 m j=1$e j e T j U .

Note that this expression can be rewritten as

$Y = 1 2 P L L-1 U$where P L = Id R N -N k=1 e j e T j is the projection onto the space perpendicular to the null space of L (i.e., the range of L). In the case where the graph has a single connected component, one has m = 1 and e 1 = 1 N / √ N yielding

$P L = Id R N - 1 N 1 N 1 T N .$The minimization in z given y is straightforward: if y k y k , then z kl = (y ky l )/|y ky l |. If y k = y l , then one can take any value for z kl and the simplest if of course z kl = 0. Using the previous computation, we can summarize a training algorithm for multi-dimensional scaling, called SMACOF for "Scaling by Maximizing a Convex Function" (see, e.g., Borg and Groenen [[36]](#b54) for more details and references).

## Algorithm 21.1 (SMACOF)

Assume that a symmetric matrix of dissimilarities (d kl , k, l = 1, . . . , N ) is given, together with a matrix of weights (w kl , k, l = 1, . . . , N ). Fix a target dimension, p. Fix a tolerance constant ϵ.

1. Compute the Laplacian matrix L of the graph associated with the weights, the projection matrix P L onto the range of L and the matrix M = (L + Id R N -P L ) -1 .

2. Initialize the algorithm with some family y 1 , . . . , y N ∈ R p and let Y =

$          y T 1 . . . y T N           .$3. At a given step of the algorithm, let Y be the current solution and compute, for k = 1, . . . , N :  

$u k = 2 N l=1 w kl d kl y k -y l |y k -y l | 1 y k y l to form the matrix U =           u T 1 . . . u T N           . 4. Compute Y ′ = 1 2 P L MU . 5. If |Y -Y ′ | ≤ ϵ,$
## Manifold learning

The goal of MDS is to map a full matrix of distances into a low-dimensional Euclidean space. Such a representation, however, cannot address the possibility that the data is supported by a low-dimensional, albeit nonlinear, space. For example, people leaving on Earth live, for all purposes, on a two-dimensional structure (a sphere), but any faithful Euclidean representation of the world population needs to use the three spatial dimensions. One may also argue that the relevant distance between points on Earth is not the Euclidean one either (because one would never travel through Earth to go from one place to another), but the distance associated to the shortest path on the sphere, which is measured along great circles.

To take another example, the left panel in fig. [21](#fig_5).1 provides the result of applying MDS to a ten-dimensional dataset obtained by applying a random ten-dimensional rotation to a curve supported by a three-dimensional torus. MDS indeed retrieves the correct curve structure in space, which is three dimensional. However, for a person "living" on the curve, the data is one-dimensional, a fact that is captured by the Isomap method that we now describe.

## Isomap

Let us return to the example of people living on the spherical Earth. One can define the distance between two points on Earth either as the shortest length a person would have to travel (say, by plane) to go from one point to the other (that we can call the intrinsic distance), or simply the chordal distance in 3D space between the two points. The first one is obviously the most relevant to the spherical structure of the Earth, but the second one is easier to compute given the locations of the points in space.

For typical datasets, the geometric structure of the data (e.g., that it is supported by a sphere) is unknown, and the only information that is available is their chordal distance in an ambient space (which can be very large). An important remark, however is that, when the points are close to each other, the two distances can be expected to be similar, if we assume that the geometry of the set supporting the data is locally linear (e.g., that it is, like the sphere, a "submanifold" of the ambient space, with small neighborhoods of any data point well approximated, at first order, by points on a tangent space). Isomap uses this property, only trusting small distances in the matrix D, and infers large distances by adding the costs resulting from traveling from data points to nearby data points.

Fix an integer c. Given D, the c-nearest neighbor graph on V = {1, . . . , N } places an edge between k and l if and only if d k,l is among the c smallest values in {d kl ′ , l ′ k} neighbors or x l among the c smallest values in {d k ′ l , k ′ l}. We will write k ∼ c l to indicate that there exists an edge between k and l in this graph. One then defines the geodesic distance on the graph as

$d ( * ) kl = min          m j=1 d k j-1 k j : k 0 , . . . , k m ∈ {1, . . . , N }, k 0 = k ∼ c k 1 ∼ c • • • ∼ c k m-1 ∼ c k m = l, m ≥ 0          .$This geodesic distance can be computed incrementally as follows. First define d 

$d (n) kl = min d (n-1) kl ′ + d (1)$ll ′ l ′ = 1, . . . , N until the entries stabilize, i.e., d (n+1) = d (n) , in which case one has d ( * ) = d (n) . The validity of the statement can be easily proved by checking that

$d (n) kl = min          n j=1 d (1) k j-1 k j : k 0 , . . . , k n ∈ {1, . . . , N }, k 0 = k, k n = l         $, which can be done by induction, the details being left to the reader. It should also be clear that the procedure will stabilize after no more than N steps.

Once the distance is computed, Isomap then applies standard MDS, resulting in a straightened representation of the data like in fig. 21.1. Another example is provided in fig. [21](#fig_5).2, where, this time, the input curve is closed and cannot therefore be represented as a one-dimensional structure. One can note, however, that, even in this case, Isomap still provides some simplification of the initial shape of the data. 

## Local Linear Embedding

Local linear embedding (LLE) exploits in a different way the fact that manifolds are locally well approximated by linear spaces. Like Isomap, it starts also with building a c-nearest-neighbor graph on {1, . . . , k}. Assume, for the sake of the discussion, that the distance matrix is computed for possibly unobserved data T = (x 1 , . . . , x N ). Letting N k denote the indices of the nearest neighbors of k (excluding k itself), the basic assumption is that x k should approximately lie in the affine space generated by x l , l ∈ N k . Expressed in barycentric coordinates, this space is defined by

$T k =          l∈N k ρ (l) x l : ρ ∈ R N k , l∈N k ρ (l) = 1         $, and T k can be interpreted as an approximation of the tangent space at x k to the data manifold. Optimal coefficients (ρ ( ) kl, k = 1, . . . , N , l ∈ N k ) providing the representation of x k in that space can be estimated by minimizing, for all k

$x k - l∈N k ρ (l) k x l 2 subject to l∈N k ρ (l) k = 1. This is a simple least-square program. Let c k = |N k | (c k = c$in the absence of ties). Order the elements of N k to represent ρ (l) k , l ∈ N k as a vector denoted ρ k ∈ R c k . Similarly, let S k be the Gram matrix associated with x l , l ∈ N k formed with all inner products x T l ′ x l , l, l ′ = 1, . . . , N and let r k be the vector composed with products x T k x l , l ∈ N k . Assume that S k is invertible, which is generally true if c < d, unless the neighbors are exactly linearly aligned. Then, the optimal ρ k and the Lagrange multiplier λ for the constraint are given by

$ρ k λ = S k 1 c k 1 T c k 0 -1 r k 1 . (21.6)$If S k is not invertible, the problem is under-constrained and one of its solutions can be obtained by replacing the inverse above by a pseudo-inverse.

The low-dimensional representation of the data, still denoted (y 1 , . . . , y N ) with y k ∈ R p is then estimated so that the relative position of y k to its neighbors is the same as that of x k , i.e., so that

$y k ≃ l∈N k ρ (l) k y l .$These vectors are estimated by minimizing

$F(y) = N k=1 y k - l∈N k ρ (l) k y l 2 .$Obviously, some additional constraints are needed to avoid the trivial solution y k = 0 for all k. Also, replacing all y k 's by y ′ k = Ry k + b where R is an orthogonal transformation in R p and b is a translation does not change the value of F, so there is no loss of generality in assuming that N k=1 y k = 0 and that N k=1 y k y T k = D 0 , a diagonal matrix. However, if one lets y ′ k = Dy k where D is diagonal, then

$F(y) = p i=1 D 2 ii N k=1         y (i) k - l∈N k ρ (l) k y (i) l         2 .$This shows that one should not allow the diagonal coefficients of D 0 to be chosen freely, since otherwise the optimal solution would require to take this coefficient to 0. So D 0 should be a fixed matrix, and by symmetry, it is natural to take D 0 = Id R p . (Any other solution-for a different D 0 -can then be obtained by rescaling independently the coordinates of y 1 , . . . , y N .)

## Extend ρ (l)

k to an N -dimensional vector by taking ρ

$(k) k = -1 and ρ (l) k = 0 if l k and l N k . We can write F(y) = N k=1 N l=1 ρ (l) k y l 2 .$Expanding the square, this is

$F(y) = N l,l ′ =1 w ll ′ y T l y l ′ with w ll ′ = N k=1 ρ (l) k ρ (l ′ )$k . Introducing the matrix W with entries w kl and the

$N × p matrix Y =           y T 1 . . . y T N          $, we have the simple expression

$F(y) = trace(Y T W Y ) .$Note that the constraints are Y T Y = Id R p and Y T 1 N = 0. Without this last constraint, we know that an optimal solution is provided by Y = [e 1 , . . . , e p ] where e 1 , . . . , e p provide an orthonormal family of eigenvectors associated to the p smallest eigenvalues of W (this is a consequence of corollary 2.4). To handle the additional constraint, it suffices to note that W 1 N = 0, so that 1 N is a zero eigenvector. Given this, it suffices to compute p + 1 eigenvectors associated to smallest eigenvalues of W , e 1 , . . . , e p+1 , with the condition that e 1 = ±1 N / √ N (which is automatically satisfied unless 0 is a multiple eigenvalue of W ) and let Y = [e 2 , . . . , e p+1 ].

Note that e 2 , . . . , e p+1 are also the p smallest eigenvectors of W + λ11 T for any large enough λ, e.g., λ > trace(W )/N . LLE is summarized in the following algorithm.  (3) For k = 1, . . . , N , let S k be the sub-matrix of S matrix associated with x l , l ∈ N k and compute coefficients ρ  Remark 21.1 We note that, for both Isomap and LLE, the c-nearest-neighbors graph can be replaced by the graph formed with edges between all pairs of points that are at distance less than ϵ from each other, for a chosen ϵ > 0, with no change in the algorithms.

These parameters (c or ϵ) must be chosen carefully and may have an important impact on the output of the algorithm. Choosing them too small would not allow for a correct estimation of distances in Isomap (with possibly some of them being infinite if the graph has more than one connected component), or of the linear approximations in LLE. However, choosing them too large may break the basic hypothesis that the data is locally Euclidean or linear that form the basic principles of these algorithms. ♦

## Graph Embedding

Both Isomap and LLE are based on the construction of a nearest-neighbor graph based on dissimilarity data and the conservation of some of its geometric features when deriving a small-dimensional representation. For LLE, a weight matrix W was first estimated based on optimal linear approximations of x k by its neighbors, and the representation was computed by estimating the eigenvectors associated with the smallest eigenvalues of W (excluding the eigenvector proportional to 1). However, both methods were motivated by the intuition that the dataset was supported by a continuous small-dimensional manifold. We now discuss methods that are solely motivated by the discrete geometry of a graph, for which we use tools that are similar to our discussion of graph clustering in section 19.5.

Adapting the notation in that section to the present one, we start with a graph with N vertices and weights β kl between these vertices (such that β ll = 0) and we form the Laplacian operator defined by, for any vector u ∈ R N :

$1 2 ∥u∥ H 1 = 1 2 N l,l ′ =1 β ll ′ (u (l) -u (l ′ ) ) 2 = u T Lu,$so that L is identified as the matrix with coefficients ℓ ll ′ = -β ll ′ for l l ′ and ℓ ll = N l ′ =1 β ll ′ . The matrix W that was obtained for LLE coincides with this graph Laplacian if one lets β ll ′ = -w ll ′ for l l ′ , since we have N l ′ =1 w ll ′ = 0. The usual requirement that weights are non-negative is no real loss of generality, because in LLE (and in the Graph embedding method above), one is only interested in eigenvectors of W (or L below) that are perpendicular to 1, and those remain the same if one replaces W by W -a1 N 1 T N + N aId R N which has negative off-diagonal coefficients wll ′ = w ll ′a for large enough a.

In graph (or Laplacian) embedding, the starting point is a weighted graph on {1, . . . , N } with edge weights β ll ′ interpreted as similarities between vertexes. These weights may or may not be deduced from measures of dissimilarity (d ll ′ , k, l = 1, . . . , N ) which themselves may or may not be computed as distances between training data x 1 , . . . , x N . If one starts with dissimilarities, it is typical to use simple transformations to compute edge weights, and one the most commonly used is

$β ll ′ = exp(-d 2 ll ′ /2τ 2 )$for some constant τ. These weights are usually truncated, replacing small values by zeros (or the computation is restricted to nearest neighbors), to ensure that the resulting graph is sparse, which speeds up the computation of eigenvectors for large datasets.

Given a target dimension p, the graph is then represented as a collection of points y 1 , . . . , y N ∈ R p , where y k is associated to vertex k. For this purpose, one needs to compute the first p + 1 eigenvectors, e 1 , . . . , e p+1 , of the graph Laplacian, with the requirement that e 1 = ±1 N / √ N . (This is always possible and can be achieved numerically by computing eigenvectors of L+c11 T for large enough c.) The graph representation is then given by y

$(|) k i = e (k)$i+1 for i = 1, . . . , p and k = 1, . . . , N . Note that these are exactly the same operations as those described in steps 4 and 5 of the LLE algorithm.

One way to interpret this construction is that e 2 , . . . , e p+1 (the coordinate functions for the representation y 1 , . . . , y N ) minimize

$p j=1 ∥e i ∥ 2 H 1$subject to e 2 , . . . , e p+1 being perpendicular to each other and perpendicular to the constant functions (these constraints being justified for the same reasons as those discussed for LLE). Small H 1 semi-norms being associated with smoothness on the graph, we see that we are looking for the smoothest zero-mean representation of the data.

Based on our discussion of LLE, we can make an alternative interpretation by introducing a symmetric square root R of the Laplacian matrix L or any matrix such that RR T  An alternate requirement that could have been made for LLE is that N l=1 (ρ (l) k ) 2 = 1 for all k. Instead of having to solve a linear system in step 2 of Algorithm 21.2, one would then compute an eigenvector with smallest eigenvalue of S k . For graph embedding, this constraint can be enforced by modifying the Laplacian matrix, since N l=1 (ρ (l) k ) 2 is just the (k, k) coefficient of RR T . Given this, let D be the diagonal matrix formed by the diagonal elements of L, and define the so-called "symmetric Laplacian" L = D -1/2 LD -1/2 . One obtain an alternative, and popular, graph embedding method by replacing e 1 , . . . , e p+1 above by the first p eigenvectors of L.

$= L. Writing R = [ρ 1 , . . . , ρ N ], one has L = N k=1 ρ k ρ T$If one introduces the eigenvectors ē1 , . . . , ēN of the normalized Laplacian, still associated with non-decreasing eigenvalues λ1 = 0, . . . , λN , and arranges without loss of generality that ē1 ∝ D 1/2 1 N , then

$P s = D -1/2        N i=1 (1 -λi ) s ēi ēT i        D 1/2 .$This shows that, for s large enough, the transitions of this Markov chain are well approximated by its first terms, suggesting using the alternative representation based on the normalized Laplacian: ȳk (i) = ēi+1 (k).

Both representations (using normalized or un-normalized Laplacians) are commonly used in practice.

## Stochastic neighbor embedding

## General algorithm

Stochastic neighbor embedding (SNE, Hinton and Roweis [[90]](#b108)), and its variant (t-SNE, Maaten and Hinton [[122]](#b140)) have become a popular tool for the visualization of high-dimensional data based on dissimilarity matrices. One of the key contributions of this algorithm is to introduce a local data rescaling step, that allows for visualization of more homogeneous point clouds.

Assume that dissimilarities D = (d kl , k, l = 1, . . . , N ) are observed. The basic principle in SNE is to deduce from the dissimilarities a family of N probability distributions on {1, . . . , N }, that we will denote π k , k = 1, . . . , N , with the property that π k (k) = 0. The computation of these probabilities include the local normalization step, and we will return to this later. Given the π k 's, one then estimate low-dimensional representations y = (y 1 , . . . , y N ) such that π k ≃ ψ k where ψ k is given by

$ψ k (l; y) = exp -β |y k -y l | 2 N l ′ =1,l ′ k exp -β |y k -y l ′ | 2 1 l k .$Here, β : [0, +∞) → [0, +∞) is an increasing differentiable function that tends to +∞ at infinity. The derivative is denoted ∂β. The original version of SNE [[90]](#b108) uses β(t) = t and t-SNE [[122]](#b140) takes β(t) = log(1 + t).

The determination of the representation can then be performed by minimizing a measure of discrepancy between the probabilities π k and ψ k . In Hinton and Roweis [[90]](#b108), it is suggested to minimize the sum of Kullback-Liebler divergences, namely

$N k=1 KL(π k ∥ψ k (•; y))$or, equivalently, to maximize

$F(y) = N k,l=1 π k (l) log ψ k (l; y) = - N k,l=1 β(|y k -y l | 2 )π k (l) + N k=1 log         N l=1,l k exp(-β(|y k -y l | 2 ))        $The gradient of this function can be computed by evaluating the derivative at ϵ = 0 of f : ϵ → F(y + ϵh). This computation gives

$f ′ (0) = -2 N k,l=1 ∂β(|y k -y l | 2 )(y k -y l ) T (h k -h l )π k (l) + 2 N k=1 N l=1 ∂β(|y k -y l | 2 )(y k -y l ) T (h k -h l )ψ k (l; y) = -2 N k=1 h T k N l=1 ∂β(|y k -y l | 2 )(y k -y l )(π k (l) + π l (k) -ψ k (l; y) -ψ l (k; y)) This shows that ∂ y k F(y) = -2 N l=1 β(|y k -y l | 2 )(y k -y l )(π k (l) + π l (k) -ψ k (l; y) -ψ l (k; y)).$This is a rather simple expression that can be used with any first-order optimization algorithm to maximize F. The algorithm in Hinton and Roweis [[90]](#b108) uses gradient ascent with momentum, namely iterating

$y (n+1) = y (n) + γ∇F(y (n) ) + α (n) (y (n) -y (n-1) )$Choosing α (n) = 0 provides standard gradient ascent with fixed gain γ (of course, other optimization methods may be used). The momentum can be interpreted, in a loose sense, as a "friction term".

A variant of the algorithm replaces the node-dependent probabilities π k by a single, symmetric, joint distribution π on {1, . . . , N } 2 , (k, l) → π(k, l), satisfying π(k, k) = 0 and π(k, l) = π(l, k). The target distribution ψ then becomes

$ψ(k, l; y) = exp(-β(|y k -y l | 2 )) N k ′ l ′ =1 exp(-β(|y k ′ -y l ′ | 2 ))$.

With such a choice, the objective function has a simpler form, namely minimizing KL( π∥ ψ(•, y)) or maximizing the expected likelihood

$F(y) = N k,l=1 π(k, l) log ψ(k, l; y) = - N k,l=1 β(|y k -y l | 2 ) π(k, l) + log        N k l=1 exp(-β(|y k -y l | 2 ))        .$The gradient of this symmetric version of F can be computed similarly to the previous one and is given by

$∂ y k F(y) = -4 N l=1 ∂β(|y k -y l | 2 )(y k -y l )( π( k, l) -ψ(k, l; y)).$
## Setting initial probabilities

The probabilities π k (l) or π(k, l) are deduced from the dissimilarities as

$π k (l) = e -d 2 kl /2σ 2 k N l ′ =1,l ′ k e -d 2 kl ′ /2σ 2 k for l k and π(k, l) = π k (l) + π l (k) 2n .$The coefficients σ 2 k , k = 1, . . . , N operate the local normalization, justifying, in particular, the parameter-free expression chosen for ψ and ψ. These coefficients are estimated so as to adjust the entropies of all π k to a fixed value, which is a parameter of the algorithm. Note that, letting t Using Schwartz inequality, we see that ∂ t H(π k ) ≤ 0 so that H(π k ) is decreasing as a function of t, i.e., increasing as a function of σ 2 k . When σ 2 k → 0, π k converges to the uniform distribution on the set of nearest neighbors of k (the indexes l k such that d 2 kl is minimal) and, letting ν k denote their number, which is typically equal to 1, H(π k ) converges to log ν k . When σ 2 k tends to infinity, π k converges to the uniform distribution over indexes l k, whose entropy is log(N -1). This shows that e H(π k ) , which is called the perplexity of π k can take any value between ν k and N -1. The common target value of the perplexity can therefore be taken anywhere between max k ν k and N -1. In Maaten and Hinton [[122]](#b140), it is recommended to choose a value between 5 and 50.

$= 1/2σ 2 k and H(π k ) = -N l=1 π k (l) log π k (l), ∂ t H(π k ) = - N l=1 ∂ t π k (l) log π k (l) - N l=1 ∂ t π k (l) = - N l=1 ∂ t π k (l) log π k (l) Now ∂ t log π k (l) = -d 2 kl + d2 k with d2 k = N l ′ =1 d 2 kl ′ π k (l ′ ). Writing$Remark 21.2 The complexity of the computation of the gradient of the objective function (either F or F ) scales like the square of the size of the training set, which may be prohibitive when N is large. In Van Der Maaten [[193]](#b211), an accelerated procedure, that involves an approximation of the gradient is proposed. (This procedure is however limited to representations in dimensions 2 or 3.) ♦

## Uniform manifold approximation and projection (UMAP)

UMAP is similar in spirit to t-SNE, with a few important differences that result in a simpler optimization problem and faster algorithms. Like Isomap, the approach is based on matching distances between the high-dimensional data and the lowdimensional representation. But while Isomap estimates a unique distance on the whole training set (the geodesic distance on the nearest-neighbor graph), UMAP estimates as many "local distances" as observations before "patching" them to form the final representation.

The goal of transporting possibly non-homogeneous locally defined objects on initial data to a homogeneous low-dimensional visualization is what makes UMAP similar to t-SNE. The difference is that t-SNE transports local probability distributions, while UMAP transports metric spaces. More precisely, given distances (d kl , k, l = 1, . . . , N ) and an integer m provided as input, the algorithm builds, for each k = 1, . . . , N a (pseudo-)metric δ k on the associated data graph by letting

$δ (k) (k, l) = δ (k) (l, k) = 1 σ k d kl -min l ′ k d kl ′$if l is among the m nearest neighbors of k, where m is a parameter of the algorithm, with all other distances being infinite. The normalization parameter σ k has a role similar to that of the same parameter in t-SNE in that it tends to make the representation homogeneous. Here, it is computed such that l exp(-δ (k) (l, l ′ )) = log 2 m .

Each such metric provides a weighted graph structure on {1, . . . , N } by defining weights w (k) ll ′ = exp(-δ (k) (l, l ′ )). In UMAP, these weights are interpreted in the framework of fuzzy sets, where a fuzzy set is defined by a pair (A, µ) where A is a set and µ a function µ : A → [0, 1] [[210]](#b228). The function µ is called the membership function and µ(x) for x ∈ A is the membership strength of x to A. Letting V = {1, . . . , N } and E = V × V , one then interprets the weights as defining the membership strength of edges to the graph, i.e., one defines the "fuzzy graph" G (k) = (V , E, µ (k) ) where µ (k) (l, l ′ ) = w (k) ll ′ is the membership strength of edge (l, l ′ ) to G (k) . This is, of course, just a reinterpretation of weighted graphs in terms of fuzzy sets, but it allows one to combine the collection (G (k) , k = 1, . . . , N ) using simple fuzzy sets operations, namely, defining the combined (fuzzy) graph G = (V , E, µ) with

$(E, µ) = N k=1 (E, µ (k) )$being the fuzzy union of the edge sets. There are, in fuzzy logic, multiple ways to define set unions [[85]](#b103), and the one selected for UMAP define (A, µ) ∪ (A ′ , µ ′ ) = (A∪A ′ , ν) with ν(x) = µ(x)+µ ′ (x)-µ(x)µ ′ (x) (µ(x) and µ ′ (x) being defined as 0 is x A or x A ′ respectively). In UMAP, each edge µ (k) (l, l ′ ) is non-zero only is k = l or l ′ so that µ(l, l ′ ) = w (l)

$ll ′ + w (l ′ ) ll ′ -w (l) ll ′ w (l ′ ) ll ′ .$This defines an input fuzzy graph structure on {1, . . . , N } that serves as target for an optimized similar structured associated with the representation y = (y 1 , . . . , y N ). This representation, since it is designed as a homogeneous representation of the data, provides a unique fuzzy graph H(y) = (V , E, ν(•; y)) and the edge membership function is defined by ν(l, l ′ ; y) = ϕ a,b (y l , y l ′ ) with where ρ 0 is an input parameter of the algorithm. This function ψ ρ 0 takes the same form as the membership function defined for local graphs G (k) , and its replacement by ϕ a,b makes possible the use of gradient-based methods for the determination of the optimal y (ψ ρ 0 is not differentiable everywhere). Note the important simplification compared to the similar function F is t-SNE, in that the logarithm of a potentially large sum is avoided. We have The optimization can be implemented using stochastic gradient ascent. Introduce random variables ξ kl and ξ ′ kl both taking value in {0, 1}, all independent of each other and such that P (ξ kl = 1) = µ kl and P (ξ ′ kl = 1) = ϵ. Define This corresponds to SGA iterations in which:

(1) Each edge (k, l) is selected with probability µ(k, l) (which are zero for unless k and l are neighbors);

(2) If (k, l) is selected, one selects an additional edges (k, l ′ ) each with probability ϵ.

Letting l 1 , . . . , l m be the number of edges selected, y k is updated according to 3 If one prefers using probability rather than fuzzy set theory, the graphs G (k) may also be interpreted as random graphs in which edges are added independently from each other and each edge (l, l ′ ) is drawn with probability µ (k) (l, l ′ ). The combined graph G is then the random graph in which (l, l ′ ) is present if and only if it is in at least one of the G (k) and the objective function C coincides with the KL divergence between this random graph and the random graph similarly defined for y.

$y k ← y k + 2γ         ∂$However, this fuzzy/random graph formulation of UMAP-which corresponds to current practical implementations-is only a special case of the theoretical construction made in McInnes et al. [[130]](#b148) which builds on the theory of (fuzzy) simplicial sets and their representation of metric spaces. We refer the interested reader to this reference, which requires a mathematical background beyond the scope of these notes. ♦

Chapter 22

## Generalization Bounds

We provide, in this chapter, an introduction to some theoretical aspects of statistical (or machine) learning, mostly focusing on the derivation of "generalization bounds" that provide high-probability guarantees on the generalization error of predictors using training data. While these bounds are not always of practical use, because making them small in realistic situations would require an enormous amount of training data, their derivations and the form they take for specific model classes bring important insight on the structure of the learning problem, and help understand why some methods may perform well while others do not.

## Notation

We here recall some notation introduced in chapter 5. We consider a pair of random variables (X, Y ), with X : Ω → R X and Y : Ω → G. Regression problems correspond to R Y = R (or R q if multivariate) and classification to R Y being a finite set. A predictor is a function f : R X → R Y . The general prediction problem is to find such a predictor within a class of functions, denoted F , minimizing the prediction (or generalization error) A good learning algorithm should be such that the generalization error R( fT ) is small, at least in average (i.e., E(R( fT )) is small). Our main goal in this chapter is to describe generalization bounds trying to find upper-bounds for R( fT ) based on E T and properties of the function class F . These bounds will reflect the bias-variance trade-off, in that, even though large function classes provide smaller in-sample errors, they will also induce a large additive term in the upper-bound, accounting for the "variance" associated to the class.

$R(f ) = E($Remark 22.1 Both variables X and Y are assumed to be random in the previous setting, but there are often situations when one of them is "more random" than the other. Randomness in Y is associated to measurement errors, or ambiguity in the decision. Randomness in X more generally relates to the issue of sampling a dataset in a large dimensional space. In some cases, Y is not random at all: for example, in object recognition, the question of assigning categories for images such as those depicted in fig. [22](#fig_136).1 has a quasi-deterministic answer. Sometimes, it is X who is not random, for example when observing noisy signals where X is a deterministic discretization of a time interval and Y is some function of X perturbed by noise. ♦

## Penalty-based Methods and Minimum Description Length

## Akaike's information criterion

We make a computation under the following assumptions. We assume a regression model Y = f θ (X) + ϵ where ϵ ∼ N (0, σ 2 ) and f is some function parametrized by θ ∈ R m . We also assume that the true distribution is actually covered by this model and represented by a parameter θ 0 . Let θT denote the parameter estimated by least squares using a training set T , and denote for short fT = f θT .

The in-sample error is

$E T = 1 N N k=1$(y k -fT (x k )) 2 .

with

$I = 1 2σ 2 E(∂ 2 θ (Y -f θ (X)) 2 | θ-θ 0 ).$As a consequence, we can write (taking expectations in both Taylor expansions) ∆ N = σ 2 E ( θTθ 0 ) T J T ( θTθ 0 ) + σ 2 E ( θTθ 0 ) T I( θTθ 0 ) + o(E(| θTθ 0 | 2 )).

(We skip hypotheses and justification for the analysis of the residual term.)

We now note that, because we are assuming a Gaussian noise, and that the true data distribution belongs to the parametrized family, the least-square estimator is also a maximum likelihood estimator. Indeed, the likelihood of the data is

$1 (2πσ 2 ) N /2 exp        - 1 2σ 2 N k=1 (Y k -f θ (X k )) 2        N k=1 ϕ X (X k )$where ϕ X is the p.d.f. of X and does not depend on the unknown parameter.

We can therefore apply classical results from mathematical statistics [[194]](#b212). Under some mild smoothness assumptions on the mapping θ → f θ , θT converges to θ 0 in probability when N tends to infinity, the matrix J T converges to I, which is the model's Fisher information matrix, and √ N ( θTθ 0 ) converges in distribution to a Gaussian N (0, I -1 ) . This implies that both N ( θTθ 0 ) T J T ( θTθ 0 ) and N ( θTθ 0 ) T I( θTθ 0 ) converge to a chi-square distribution with m degrees of freedom, whose expectation is m, which indicates that ∆ N has order 2σ 2 m/N . This analysis can be used to develop model selection rules, in which one chooses between models of dimensions k 1 < k 2 < • • • < k q = m (e.g., by truncating the last coordinates of X). The rule suggested by the previous computation is to select j minimizing E (j)

$T ( fT ) + 2σ 2 k j N ,$where E (j) is the in-sample error computed using the k j -dimensional model. This is an example of a penalty-based method, using the so-called Akaike's information criterion (AIC) [[2]](#b20).

## Bayesian information criterion and minimum description length

Other penalty-based methods are more size-averse and replace the constant, 2, in AIC by a function of N = |T |, for example log N . Such a change can be justified by a Bayesian analysis, yielding the Bayesian information criterion (BIC) [[174]](#b192). The approach in this case is not based on an evaluation of the error, but on an asymptotic estimation of the posterior distribution resulting from a Bayesian model selection principle. Like in the previous section, we content ourselves with a heuristic discussion.

Let us consider a statistical model parametrized by θ ∈ Θ, where Θ is an open convex subset of R m with p.d.f. given by f (z; θ) = exp(θ T U (z) -C(θ)) , with U : R d → R m and z = (x, y). We are given a family of sub-models represented by M 1 , . . . , M q , where, for each j, M j is the intersection of Θ with a k j -dimensional affine subspace of R m . We are also given a prior distribution for θ in which a submodel is first chosen, with probabilities α 1 , . . . , α q , and given that, say, M j is selected, θ ∈ M j is chosen with a probability distribution with density ϕ j with respect to Lebesgue's measure on M j (denoted dm j ). Given training data T = (z 1 , . . . , z N ), Bayesian model selection consists in choosing the model M j where j maximizes the posterior log-likelihood Note that the first derivative of ℓ is ∂ θ ℓ = Ū -E θ (U ) where E θ is the expectation for f (•, θ). The second derivative is -var θ (U ) (showing that ℓ is concave) and the third derivative involves third-order moments of U for E θ and (like the second derivative) does not depend on ŪT . In particular, we can assume that, for any M > 0, there exists a constant C M such that whenever max(|θ|, | θj |) ≤ M, we have R j (θ, θj ) ≤ C M .

$µ(M j |T ) = log$The law of large numbers implies that ŪT converges to a limit when N tends to infinity, and our assumptions imply that θj converges to the parameter providing the best approximation of the distribution of Z for the Kullback-Leibler divergence. In particular, with probability 1, there exists an N such that θj belongs to any large enough, but fixed, compact set. Moreover, the second derivative ℓ( θj ) will also converge to a limit, -Σ j .

For any ϵ > 0, write R m α j e N (θ T ŪT -C(θ)) ϕ j dm j (θ) = |θ-θj |≤ϵ e N (θ T ŪT -C(θ)) ϕ j dm j (θ) + |θ-θj |≥ϵ e N (θ T ŪT -C(θ)) ϕ j dm j (θ) .

The second integral converges to 0 exponentially fast when N tends to ∞. The first one behaves essentially like

$M j e -1$2 N (θ-θj ) T Σ -1 j (θ-θj )+log ϕ j (θ) dm j (θ) .

Neglecting log ϕ j (θ), this integral behaves like (2π det(Σ j /N )) -1/2 , whose logarithm is (-k j (log N )/2) plus constant terms. As a consequence, we find that µ(M j | T ) = max θ∈M j ℓ(θ) -k j 2 log N + bounded terms. Consider, as an example, linear regression with Y = β 0 + b T x + σ 2 ν where ν is a standard Gaussian random variable. Assume that the distribution of X is known, or, preferably, make the previous discussion conditional to X 1 , . . . , X N . Let sub-models M j correspond to the assumption that all but the first k j -1 coefficients of b vanish. Then, up to bounded terms, the Bayesian estimator must minimize (over such parameters b)

$1 2σ 2 N k=1 (y k -β 0 -b T x k ) 2 + k j 2 log N .$or

$E (j) T + k j σ 2 N log N .$We now turn to another interesting point of view, which provides the same penalty, based on maximum description length principle (MDL; Rissanen [[162]](#b180)) measuring the coding efficiency of a model. Let us fix some notation. We assume that one has q competing models for predicting Y from X, for example, linear regression models based on different subsets of the explanatory variables. Denote these models M 1 , . . . , M q . Each model will be seen, not as an assumption on the true joint distribution of X and Y , but rather as a tool to efficiently encode the training set ((x 1 , y 1 ), . . . , (x N , y N )). To describe MDL,

## PENALTY-BASED METHODS AND MINIMUM DESCRIPTION LENGTH579

which selects the model that provides the most efficient code, we need to reintroduce a few basic concepts of information theory.

The entropy of a discrete probability P over a set Ω is (The logarithm in base 2 is used because of the tradition of coding with bits in information theory.) For a discrete random variable X, the entropy H 2 (X) is H 2 (P X ) where P X is the probability distribution of X. The relation between the entropy and coding theory is as follows: a code is a function which associates to any element ω ∈ Ω a string of bits c(ω). The associated code-length is denoted l c (ω), which is simply the number of bits in c(ω). When P is a probability on Ω, the efficiency of a code is measured by the average code-length: E P (l c ) = ω∈Ω l c (ω)P (ω).

Shannon's theorem [[175,](#b193)[55]](#b73) states that, under some conditions on the code (ensuring that any sequence of words can be recognized as soon as it is observed: one says that it is instantaneously decodable) the average code length can never be larger than the entropy of P . Moreover, it states that there exists codes that achieve this lower bound with no more than one bit loss, such that for all ω, l c (ω) ≤ log 2 (P (ω))+1. These optimal codes, such as the Huffman code [[55]](#b73), can completely be determined from the knowledge of P . This allows one to interpret a probability P on Ω as a tool for designing codes with code-lengths essentially equal to (-log 2 P ). This statement can be generalized to continuous random variables (replacing the discrete probability P by a probability density function, say ϕ) if one introduces a coding precision level, denoted δ 0 , meaning that the decoded values may differ by no more than δ 0 from the encoded ones. The result is that the optimal code-length at precision δ 0 can be estimated (up to one extra bit) bylog 2 ϕlog 2 δ 0 .

In our context, each model of the conditional distribution of Y given X, with conditional density ϕ(y|x), provides a way to encode the training set with a total code length, for (y 1 , . . . , y N ), of -N k=1 log 2 ϕ(y k | x k ) -N log 2 δ 0 (working, as before, conditionally to x 1 , . . . , x N ). We assume that the precision at which the data is encoded is fixed, which implies that the last term does not affect the model choice. Now, assume a sequence of m parametrized model classes, M 1 , . . . , M m and let ϕ(y | x, θ, M j ) denote the conditional distribution with parameter θ in the class M j . Within model M j , the optimal code length corresponds to the maximum likelihood:

- If the models are nested, which is often the case, the most efficient will always be the largest model, since the maximization is on a larger set. However, the minimum description length (MDL) principle uses the fact that, in order to decode the compressed data, the model, including its optimal parameters, has to be known, so that the complete code needs to include a model description. The decoding algorithm will then be: decode the model, then use it to decode the data.

So assume that a model (one of the M j 's) has a k j -dimensional parameter θ. Also assume that a probability distribution, π(θ | M j ), is used to encode θ. Also choose a precision level, δ ij , for each coordinate in θ, i = 1, . . . , k j . (Previously, we could consider the precision of the y k , δ 0 , as fixed, but now, the precision level for parameters is a variable that will be optimized.) The total description length using this model now becomes If π is interpreted as a prior distribution of the parameters, θ(j) is the maximum a posteriori Bayes estimator. We now take the correction caused by (δ ij , i = 1, . . . , k j ) into account, by assuming that the ith coordinate in θ(j) is truncated tolog 2 δ i bits.

Let θ (j) denote this approximation. A second-order expansion of L(θ|M j ) around θ(j) yields (assuming sufficient differentiability)

L(θ (j) | M j ) = L( θ(j) | M j ) + 1 2 (θ (j) -θ(j) ) T S θ(j) (θ (j) -θ(j) ) + o(|θ

$(j) -θ(j) | 2 )$where S θ is the matrix of second derivatives of L(• | M j ) at θ. Approximating θ (j) -θ(j) by δ (j) (the k j -dimensional vector with coordinates δ ij , i = 1, . . . , k j ), we see that the precision should maximize 1 2 (δ (j) ) T S θ(j) δ (j) + k j i=1 log 2 δ ij .

Note that S θ(j) must be negative semi-definite, since θ is a local maximum. Assuming it is non-singular, the previous expression can be maximized and yields S θ(j) δ (j) = -1 log 2 1 δ (j) [(22.1)](#) where 1/δ (j) is the vector with coordinates (1/δ ij ).

Let us now make an asymptotic evaluation. Because L(θ | M j ) includes a sum over N independent terms, it is reasonable to assume that S θ(j) has order N , and more precisely, that S θ(j) /N has a limit. Rewrite [(22.1)](#) as (j)  .

$S θ(j) N √ N δ (j) = - 1 log 2 1 √ N δ$This implies that √ N δ (j) is the solution of an equation which stabilizes with N , and it is therefore reasonable to assume that the optimal δ ij takes the form δ ij = c i (N | M j )/ √ N , with c i (N | M j ) converging to some limit when N tends to infinity. The total cost can therefore be estimated by The last two terms are O(1), and can be neglected, at least when N is large compared to k j . The final criterion becomes the penalized likelihood

$l d (θ | M j ) = L(θ|M j ) - k j 2 log 2 N$in which we see that the dimension of the model appears with a factor log 2 N as announced (one needs to normalize both terms by N to compare with the previous paragraph).

## Concentration inequalities

The discussion of the AIC was a first attempt at evaluating a prediction error. It was however done under very specific parametric assumptions, including the fact that the true distribution of the data was within the considered model class. It was, in addition, a bias evaluation, i.e., we estimated how much, in average, the in-sample error was less than the generalization error. We would like to obtain upper bounds to the generalization error that hold with high probability, and rely as little as possible on assumptions on the true data distribution.

One of the main tools used in this context are concentration inequalities, which provide upper bounds on the various probabilities of events involving a large number of random variables. The current section provides a review of some of these inequalities.

## Cramér's theorem

If X 1 , X 2 , . . . are independent, integrable random variables with identical distributions (to that of a random variable X), the law of large numbers tells us that the empirical mean XN = (X 1 + • • • + X N )/N converges with probability one to m = E(X). When the variables are square integrable, Chebychev's inequality provides an easy proof of the weak law of large numbers. Indeed,

$P | Xn -m| > ϵ ≤ 1 ϵ 2 E ( XN -m) 2 1 | Xn -m|>ϵ ≤ var( XN ) ϵ 2 = var(X) N ϵ 2 .$A stronger assumption on the moments of X yields a stronger inequality. One says that X has exponential moments if there exists λ 0 > 0 such that E(e λ 0 |X| ) < ∞. In this case, the cumulant-generating function, defined, for λ ∈ R, by M X (λ) = log E(e λX ) ∈ [0, +∞], [(22.2)](#) is finite for λ ∈ [-λ 0 , λ 0 ].

Here are a few straightforward properties of the cumulant-generating function.

(i) One has M X (0) = 0.

(ii) For any a ∈ R, one has M aX (λ) = M X (aλ).

(iii) If X 1 and X 2 are independent variables, one also has

$M X 1 +X 2 (λ) = M X 1 (λ) + M X 2 (λ).$In particular, M X+a (λ) = M X (λ) + λa, so that M X-E(X) (λ) = M X (λ) -λE(X).

(iv) Finally, Markov's inequality (which states that, for any non-negative variable Y , P (Y > t) ≤ E(Y )/t) applied to Y = e λX for λ > 0 yields P(X > t) = P(e λX > e λt ) ≤ e M X (λ)-λt . [(22.3)](#) (Note that this inequality is trivially true for λ = 0.) the probability of a deviation by t at least of Xn from its mean decays exponentially fast. The derivation of the inequality above was quite easy: apply Markov's inequality in a parametrized form and optimize over the parameter. It is therefore surprising that this inequality is sharp, in the sense that a similar lower bound also holds. Even though we are not going to use it in the rest of this chapter, it is worth sketching the argument leading to this lower bound, which involves an interesting step making a change of measure.

Assume (without loss of generality) that m = 0 and consider P( Xn > t). Assume, to simplify the discussion, that the supremum of λ → ϵλ -M X (λ) is attained at some λ t . We have

$∂ λ M X (λ) = E(Xe λX ) E(e λX )$.

Let q λ (x) = e λx E(e λX ) and P λ (with expectation E λ ) the probability distribution on Ω with density q λ (X) with respect to P, so that ∂ λ M X (λ) = E λ (X). We have, since λ t is a maximizer, E λ t (X) = t. Moreover, fixing δ > 0,

## P( XN

$> t) = E(1 XN >t ) ≥ E(1 | Xn -t-δ|<δ )$≥ E 1 | Xn -t-δ|<δ e N λ XN -N t-2N δ = e -N (t+2δ) M X (λ) N P λ (| XNt -δ| < δ)

If one takes λ = λ t+δ , this implies that P( XN > t) ≥ e -N M * X (t+δ) e -N δ P λ t+δ (| XN -t -δ| < δ) . By the law of large numbers (applied to P λ t+δ ), P λ t+δ (| XN -t -δ| < δ) tends to 1 when N tends to infinity. This implies that the logarithmic rate of convergence to 0 of P( XN > t) is larger than N (M * X (t + δ) + δ), for any δ > 0, to be compared with the rate N M * X (t) for the upper bound. In Large Deviation theory, the upper and lower bounds are often simplified by considering the limit of log P( XN > t)/N , which, in this case, is M * X (t) (and this result is called Cramér's therorem).

While Cramér's upper bound is sharp, its computation requires an exact knowledge of the distribution of X, which is not a common situation. The following sections optimize the upper bound in situations where only partial information on the variable is known, such as its moments or its range. As a first example, we consider concentration of the mean for sub-Gaussian variables.

## Sub-Gaussian variables

If X has exponential moments, then, (applying again Markov's inequality)

$P (|X| > x) ≤ Ce -λx$for some positive constants C and λ. Reducing if needed the value of λ, one can assume that C takes some predetermined (larger than 1) value, say, C = 2, the simple argument being left to the reader. A random variable such that, for some λ > 0 P(|X| > x) ≤ 2e -λx is called sub-exponential (and this property is equivalent to X having exponential moments). Similarly, one says that X is sub-Gaussian if, some σ > 0, Proposition 22.3 Assume that X is sub-Gaussian, so that (22.5) holds for some σ 2 > 0.

## P(|X| > x) ≤ 2e

Then, for any t > 0, we have

$P( Xn -E(X) > t) ≤ 1 + 4t 2 σ 2 N e -N t 2 2σ 2 .$Proof Let us assume, without loss of generality, that E(X) = 0. For λ > 0, we then have E(e λX ) = 1 + E(e λX -λX -1) .

Let ϕ(t) = e tt -1. We have ϕ(t) ≥ 0 for all t, ϕ(0) = 0 and, for z > 0, the equation z = ϕ(t) has two solutions, one positive and one negative that we will denote g + (z) > 0 > g -(z). We have where Φ is the cumulative distribution function of the standard Gaussian and we have used Φ(-t) -Φ(t) ≤ 2t/ √ 2π. We therefore have M X (λ) ≤ log 1 + 4λ

## E(ϕ(λX)) =

2 σ 2 e λ 2 σ 2 2 ≤ λ 2 σ 2 2 + log(1 + 4λ 2 σ 2 ) . This implies M * X (t) = sup λ>0 (λt -M X (λ)) ≥ t 2 σ 2 -M X (t/σ 2 ) ≥ t 2 2σ 2log(1 + 4t 2 σ 2 ) so that P( Xn > t) ≤ 1 + 4t 2 σ 2 N e -N t 2 2σ 2 . ■ The following result allows one to control the expectation of a non-negative sub-Gaussian random variable. Proposition 22.4 Let X be a non-negative random variable such that P(X > t) ≤ Ce -t 2 /2σ 2 for some constants C and σ 2 . Then, E(X) ≤ 3σ log C. Proof For any α ∈ (1, C], one has min(1, Ce -t 2 /2σ 2 ) ≤ αe -t 2 log α 2σ 2 log C , which implies that E(X) = +∞ 0 P(X > t)dt ≤ α 2 log α √ 2πσ log C Taking α = √ e gives E(X) ≤ √ πeσ log C ≤ 3σ log C. ■ 22.3.3 Bennett's inequality

The following proposition (see [[24]](#b42)) provides an upper bound for M X (λ) as a function of E(X) and var(X) under the additional assumption that X is bounded from above. if X < b and E(X 2 ) ≤ σ 2 . Indeed, if this inequality is true for m = 0 and λ = 1, [(22.6)](#) in the general case will result from letting X = Y /λ + m and applying the special case to Y .

The right-hand side of (22.7) is exactly E(e X ) when X follows the discrete distribution P 0 supported by two points x 0 and b, and such that E(X) = 0 and E(X 2 ) = σ 2 , which requires x 0 = -σ 2 /b and P (X = x 0 ) = b 2 /(σ 2 + b 2 ). Now consider the quadratic function v(x) = αx 2 + βx + γ which intersects x → e x at x = x 0 and x = b, and is tangent to it at x = x 0 , i.e., v(b) = e b and v(x 0 ) = v ′ (x 0 ) = e x 0 (this uniquely defines v). Then e x ≤ v(x) for x < b, yielding E(e X ) ≤ ασ 2 + γ. However, since v(X) = e X almost surely when X ∼ P 0 , this upper bound is attained and equal to that provided in [(22.7)](#). Computing the derivative in µ and equating it to 0 gives

$µ = 1 1 + ρ log ρ + x ρ(1 -x) ,$which is non-negative since ρ + xρ(1x) = (1 + ρ)x. For this value of µ, we have e -µ(ρ+x) + ρe µ (1-x)  ρ + 1 = e -µ(ρ+x) 1 + ρe µ(1+ρ) ρ + 1 = e -µ(ρ+x)

1 + ρ ρ+x ρ [(1-x)](#) ρ + 1 = e -µ(ρ+x) 1x and log e -µ(ρ+x) + ρe µ (1-x)  ρ + 1 = µ(ρ + x) + log(1x)

$= ρ + x 1 + ρ log ρ + x ρ(1 -x) + log(1 -x) = ρ + x 1 + ρ log ρ + x ρ + 1 -x 1 + ρ log(1 -x) .$This provides a lower bound for M * X (m + (bm)x), and yields the following corollary. Corollary 22.6 Assume that X satisfy the conditions of proposition 22. Bennett's inequality is sometimes stated in a slightly weaker, but simpler form [[127]](#b145). Returning to the proof of proposition 22.5 and using the fact that log u ≤ u -1, equation ( [22](#formula_2063) We will use the following lemma.

Lemma 22.7 The function ϕ : u → (e uu -1)/u 2 is non-decreasing.

Proof We have ϕ ′ (u) = ψ(u)/u 3 where ψ(u) = ue u -2e u + u + 2, yielding ψ ′ (u) = ue ue u + 1, ψ ′′ (u) = ue u . Therefore, ψ ′ is has its minimum at u = 0 with ψ ′ (0) = 0 so that ψ is increasing. Since ψ(0) = 0, we have ψ(u)/u 3 ≥ 0. We summarize this in the following corollary. This estimate can be further simplified as follows. Let g be such that g ′′ (u) = (1 + u/3) -3 and g(0) = g ′ (0) = 0, which gives g(u) = u 2 /(2 + 2u/3). Noting that h ′′ (u) = (1 + u) -1 and that (1 + u) -1 ≥ (1 + u/3) -3 , for u ≥ 0 we find, integrating twice, that h(u) ≥ g(u) for u ≥ 0. This shows that the following upper-bound is also true: This upper bound is known as Bernstein's inequality.

$P( XN > m + t) ≤ exp - N t 2$Remark 22.9 It should be clear that, in the previous discussion, one may relax the assumption that X 1 , . . . , X N are identically distributed as long as there is a common function M such that M X k (λ) ≤ m k + M(λ) for all k, with m k = E(X k ). We have in this case P( XN > mN + t) ≤ exp(-N M * (t))

with mN = (m 1 + • • • + m N )/N and M * (t) = sup λ (λt -M(λ)). This remark can be, in particular, applied to the situation in which X 1 , . . . , X N satisfy the conditions of proposition 22.5 with the same constants b and σ 2 , yielding the same upper bound as in equation [(22.8)](#). ♦

## Hoeffding's inequality

We now consider the case in which the random variables X 1 , . . . , X N are bounded from above and from below, and start with the following consequence of proposition 22.5. This shows that, if λ ≥ 0, we can apply proposition 22.5 with σ 2 = (bm)(ma), which provides the first inequality in [(22.11)](#). To handle the case λ ≤ 0, it suffices to apply this inequality with λ = -λ, X = -X, ã = -b, b = -a and m = -m. Let f (α) denote the difference between the right-hand side and left-hand side. Then f (0) = 0,

$f ′ (α) = α 4$ue α 1u + ue α + u, (so that f ′ (0) = 0) and

$f ′′ (α) = 1 4 - u(1 -u)e α (1 -u + ue α ) 2 .$For positive numbers x = 1u and y = ue α , one has (x + y) 2 ≥ 4xy, which shows that f ′′ (α) ≥ 0. This proves that f ′ is non-decreasing with f ′ (0) = 0, proving that f is minimized at α = 0, so that f (α) ≥ 0 as needed.

## ■

We can then deduce the following theorem [[92]](#b110). The upper bound is minimized for λ = 4t/|c| 2 , yielding [(22.12)](#). Equation [(22.13](#)) is obtained by applying (22.12) to -X.

## ■

An important special case of this inequality is when X 1 , . . . , X N are i.i.d. taking values in an interval of length δ. Then P( XN > E(X) + t) ≤ exp -2N t 2 δ 2 . [(22.14)](#) This inequality is obtained after applying Hoeffding's inequality to X 1 /N , . . . , X N /N , therefore taking c 1 = • • • = c N = δ/N and |c| 2 = δ 2 /N .

## McDiarmid's inequality

One can relax the assumption that the random variables X 1 , . . . , X N are independent and only assume that these variables behave like "martingale increments," as stated in the following proposition [[59]](#b77). 

## ■

We will use this proposition to prove the "bounded difference," or McDiarmid's inequality.

Theorem 22.13 (McDiarmid's inequality) Let X 1 , . . . , X N be independent random variables and g : R N → R a function such that there exists c 1 , . . . , c N such that |g(x 1 , . . . , x k-1 , x k , x k+1 , . . . , x N )g(x 1 , . . . , x k-1 , xk , x k+1 , . . . , x N )| ≤ c k [(22.15)](#) for all k = 1, . . . , N and x 1 , . . . , x k-1 , x k , xk , x k+1 , . . . , x N . Then P (g(X 1 , . . . , X N ) > E(g(X 1 , . . . , X N )) + t) ≤ e -2t 2 /|c| 2

$with |c| 2 = c 2 1 + • • • + c 2 N .$Proof Let m = E(g(X 1 , . . . , X N )). Let Z 0 = 0,

$Y k = E(g(X 1 , . . . , X N ) | X 1 , . . . , X k ) -m and Z k = Y k -Y k-1 .$Note that Z k is a function of X 1 , . . . , X k and can therefore be omitted from the conditional expectation given (X 1 , Z 1 , . . . , X k-1 , Z k-1 ).

We have E(Y k ) = 0 and E(Y k | X 1 , . . . , X k-1 ) = Y k-1 so that E(Z k | X 1 , . . . , X k-1 ) = 0.

Because the variables are independent, we have, letting X1 , . . . , XN be independent copies of X 1 , . . . , X N , Z k = E(g(X 1 , . . . , X k-1 , X k , Xk+1 , . . . , XN ) | X 1 , . . . , X k ) -E(g(X 1 , . . . , X k-2 , Xk-1 , Xk , . . . , XN ) | X 1 , . . . , X k-1 ) .

For fixed X 1 , . . . , X k-1 , (22.15) implies that Z k varies in an interval of length c k at most (whose bounds depend on X 1 , . . . , X k-1 ) so that |Z k -E(Z k )| ≤ c k . Proposition 22.12 implies that P(Z 1 + • • • + Z N ≥ t) ≤ e -2t 2 /|c| 2 , which concludes the proof since Z 1 + • • • + Z N = g(X 1 , . . . , X N ) -E(g(X 1 , . . . , X N )).

■

## Boucheron-Lugosi-Massart inequality

The following result [[38]](#b56), that we state without proof, extends on the same idea.

Theorem 22.14 Let X 1 , . . . , X N be independent random variables. Let Z = g(X 1 , . . . , X N )

with g : R N → [0, +∞) and for k = 1, . . . , N , Z k = g k (X 1 , . . . , X k-1 , X k+1 , . . . , X N ) loss in a classification problem, Hoeffding's inequality implies, for training sets of size N , P(R(f ) -RT (f ) > t) ≤ e -2N t 2 . Now corollary 22.11 does not hold if we replace f by fT , i.e., if f is estimated from the training set T , which is, unfortunately, the situation we are interested in.

Before addressing this problem, we point out that this inequality does apply to the case in which f = fT 0 where T 0 is another training set, independent from T , so that P(R( fT 0 ) -RT ( fT 0 ) > t) ≤ e -2N t 2 , which is proved by writing P(R( fT 0 ) -RT ( fT 0 ) > t) = E(P(R( fT ) -RT ( fT ) |> t|T 0 = T )) .

In this situation, the empirical risk is computed on a test or validation set (T ) independent of the set used to estimate f (T 0 ).

If one does not have a test set, and fT is optimized over a set F of possible predictors, one can rarely do much better than starting from a variation of the trivial upper bound P(R( fT ) -E T > t) ≤ P sup f ∈F (R(f ) -E T (f )) > t

(with E T = RT ( fT )) and the concentration inequalities discussed in section 22.3 need to be extended to provide upper bounds to the right-hand side.

Remark 22.15 Computing supremums of functions over non countable sets may bring some issues regarding measurability. To avoid complications, we will always assume, when computing supremums over infinite sets, that such supremums can be reduced to maximizations over finite sets, i.e., when considering sup f ∈F Φ(f ) for some function Φ, we will assume that there exists a nested sequence of finite subsets F n ⊂ F such that sup{Φ(f ) : f ∈ F } = lim n→∞ sup{Φ(f ) : f ∈ F n } . [(22.17)](#) This is true, for example, when F has a topology that admits a countable dense subset, with respect to which Φ is continuous. ♦ When F is a finite set, one can use a "union bound" with

## P(sup

$f ∈F (R(f ) -E T (f )) > t) ≤ f ∈F P(R(f ) -E T (f ) > t) ≤ |F | max f ∈F P(R(f ) -E T (f ) > t).$Such bounds cannot be applied to the typical case in which F is infinite, and is likely to provide very poor estimates even when F is finite, but |F | is large. However, all proofs of concentration inequalities applied to such supremums require using a union bound at some point, often after considerable preparatory work. Union bounds will in particular appear in conjunction with the Vapnik-Chervonenkis dimension that we now discuss.

## Vapnik's theorem

We consider a classification problem with two classes, 0 and 1, and therefore let F be a set of binary functions, i.e., taking values in {0, 1}. We also assume that the risk function r takes values in the interval [0, 1] (using, for example, the 0-1 loss). Let

$U (t) = P sup f ∈F (R(f ) -E T (f )) > t . (22.18)$A fundamental theorem of Vapnik provides an estimate of U (t) based on the number of possible ways to split a training set of 2N points into two classes using functions in F . The rest of this section is devoted to a discussion of this result and related notions. The following theorem controls U in [(22.18)](#) in terms of S F . Theorem 22.16 (Vapnik) With the notation above, one has, for t ≥ √ 2/N : P sup f ∈F (R(f ) -E T (f )) > t ≤ 2S F (2N )e -N t 2 /8 , (22.19) which implies that, with probability at least 1δ, we have ∀f ∈ F : R(f ) ≤ E T (f )) + 8 N log S F (N ) + log 2 δ (22.20) (The requirement that t ≥ √ 2/N does not really reduce the range of applicability of (22.19), since, for t ≤ √ 2/N , the upper bound in that equation is typically much larger than 1.) Proof We first show that the problem can be symmetrized with the inequality, valid if N t 2 ≥ 2,

$P sup f ∈F (R(f ) -E T (f )) ≥ t ≤ 2P sup f ∈F (E T ′ (f ) -E T (f )) ≥ t 2 (22.21)$in which T ′ is a second training set (independent of T ) with N samples also. In view of assumption [(22.17)](#), there is no loss of generality in assuming that F is finite. Associate to any training set T , a classifier f T ∈ F maximizing R(f T ) -E(f T ). One then has

$P       sup f ∈F (E T ′ (f ) -E T (f )) ≥ t 2       ≥ P (E T ′ (f T ) -E T (f T )) ≥ t 2 ≥ P (R(f T ) -E T ′ (f T ) ≤ t 2 and R(f T ) -E T (f T )) ≥ t = E 1 R(f T )-E T (f T ))≥t P R(f T ) -E T ′ (f T ) ≤ t 2 T$Conditional to T , E T ′ (f T ) is the average of M i.i.d. Bernoulli random variables, with variance bounded from above by 1/4 and

$P R(f T ) -E T ′ (f T ) ≤ t 2 T ≥ 1 - 1/4 N t 2 /4 ≥ 1 2 .$It follows that

$P sup f ∈F (E T ′ (f ) -E T (f )) > t 2 ≥ 1 2 P R(f T ) -E T (f T )) ≥ t = 1 2 P sup f ∈F (R(f ) -E T (f )) ≥ t .$This justifies [(22.21)](#).

Now consider a family of independent Rademacher random variables ξ 1 , . . . , ξ N , also independent of T and T ′ , taking values -1 and +1 with equal probability. By Now, there are at most |F (X 1 , . . . , X N , X ′ 1 , . . . , X ′ N )| different sets of coefficients in front of ξ 1 , . . . , ξ N in the above sum when f varies in F , so that, conditioning on T , T ′ and taking a union bound , we have

$P sup f ∈F N k=1 ξ k (r(Y k , f (X k )) -r(Y ′ k , f (X ′ k )))/N ≥ t/2 T , T ′ ≤ |F (X 1 , . . . , X N , X ′ 1 , . . . , X ′ N )| sup f ∈F P N k=1 ξ k (r(Y k , f (X k )) -r(Y ′ k , f (X ′ k )))/N ≥ t/2 T , T ′ The variables ξ k (r(Y k , f (X k )) -r(Y ′ k , f (X ′ k )$) are centered and belong to the interval [-1, 1], which has length 2, so that Hoeffding's inequality implies with R(f ) ≤ E T (f ) + t for all f with probability 1δ or more.

■

## VC dimension

To obtain a practical bound, the quantity S F (2N ), or its upper-bound S * F (2N ), needs to be estimated. We prove below an important property of S * F , namely that, either S * F (M) = 2 M for all M, or there exists an M 0 for which S * F (M 0 ) < 2 M 0 , and taking M 0 to be the largest one for which an equality occurs, S * F (M) has order M M 0 for all M ≥ M 0 . This motivates the following definition of the VC-dimension of the model class.

From this lemma, it results that if VC-dim(F ) = D < ∞, then S * F (M) is bounded by the total number of subsets of cardinality D or less in a set of cardinality M. This provides the following result, which implies that the term in front of the exponential in [(22.18)](#) grows polynomially in N if F have finite VC-dimension. 

## Examples

The following result provides the VC-dimension of the collection of linear classifiers. Then there is no function f ∈ F (taking the form x → sign(β x)) that maps (x 1 , . . . , x d+2 ) to (sign(α 1 ), . . . , sign(α d+1 ), -1) (where the definition of sign(0) = ±1 is indifferent), since any such function satisfies

$β T xd+2 = d+1 k=1 α k β T xk > 0 .$This proves VC-dim(F ) < d + 2. To prove that VC-dim(F ) = d + 1, it suffices to exhibit a set of d + 1 vectors in R d that can be shattered by F . Choose x 1 , . . . , x d+1 such that x1 , . . . , xd+1 are linearly independent (for example x i = i-1 k=1 e i , where (e 1 , . . . , e d ) is the canonical basis of R d ). This linear independence implies that, for any vector α = (α 1 , . . . , α d+1 ) T ∈ R d+1 , there exists a vector β ∈ R d+1 such that xT i β = α i for all i = 1, . . . , d + 1. This shows that any combination of signs for xT i β can be achieved, so that (x 1 , . . . , x d+1 ) is shattered.

## ■

Upper-bounds on VC dimensions of more complex models have also been proposed in the literature. As an example, the following theorem, that we provide without proof, considers feed-forward neural networks with piecewise linear units (such as ReLU, see chapter 11). This theorem is a special case of Theorem 7 in Bartlett et al. [[21]](#), in which the more general case of networks with piecewise polynomial units is provided. Given integers L, U 1 , . . . , U L and W 1 , . . . , W L , define the function class F (L, (U i ), (W i ), p) that consists of feed-forward neural networks with L layers, U i piecewise linear computational units with less than p pieces in the ith layer, and such that the total number of parameters involved in layers 1, 2, . . . , j is less than W j . 

$       4ep L i=1 iU i log 2        L i=1 (2epiU i )               .$
## Data-based estimates

Approximations of the shattering numbers can be computed using training data. One can, in particular, prove a concentration inequality [[38]](#b56) on log S F (X 1 , . . . , X N ), which may in turn be used to estimate log(S F (2N )). In the following, we let H VC (N , F ) denote the expectation of log S F (X 1 , . . . , X N ). It is often referred to as the VC entropy of F .

Theorem 22.24 One has, letting H VC = H VC (N , F ): P(log S F (X 1 , . . . , X N ) ≥ H VC + t) ≤ exp -t 2 2H VC + 2t/3 and P(log S F (X 1 , . . . , X N ) ≤ H VCt) ≤ exp -t 2 2H VC

Proof We show that the random variable Z = log 2 S F (X 1 , . . . , X N ) satisfies the assumptions of theorem 22.14, with Z k = log 2 S F (X 1 , . . . , X k-1 , X k+1 , . . . , X N ).

Clearly, 0 ≤ Z, 0 ≤ Z -Z k ≤ 1, because one can do no more than double S F by adding one point. We need to show that N k=1 (Z -Z k ) ≤ Z. [(22.24)](#) Note that Z is the base-two entropy of the uniform distribution, π, on the set F (X 1 , . . . , X N ) ⊂ {-1, 1} N .

We will use the following lemma.

Lemma 22.25 Let A be a finite set and ψ a probability distribution on A N . Let ψ k be its marginal when the kth variable is removed. Then: This lemma is a special case of a collection of results on non-negative entropy measures developed in Han [[86]](#b104), and we provide a direct proof below for completeness.

Given the lemma, let π k denote the marginal distribution of π when the kth variable is removed, i.e., π k (ϵ 1 , . . . ϵ k-1 , ϵ k+1 , . . . , ϵ N ) = π(ϵ 1 , . . . ϵ k-1 , -1, ϵ k+1 , . . . , ϵ N ) + π(ϵ 1 , . . . ϵ k-1 , 1, ϵ k+1 , . . . , ϵ N ).

We have:

$N k=1 (H 2 (π) -H 2 (π k )) ≤ H(π)$from which [(22.24)](#) derives since Z = H 2 (π) and Z k ≥ H 2 (π k ). The result then follows from theorem 22.14.

We now prove lemma 22.25 by induction (this proof requires some basic notions of information theory). For convenience, introduce random variables (ξ 1 , . . . , ξ N ) such that ξ k ∈ A, with joint probability distribution given by ψ. Let Y = (ξ 1 , . . . , ξ N ), Y (k) the (N -1)-tuple formed from Y by removing ξ k , Y (k,l) the (N -2)-tuple obtained by removing ξ k and ξ l , etc. Inequality [(22.25)](#) can then be rewritten This inequality is obviously true for N = 1, and it is true also for N = 2 since it gives in this case the well-known inequality H 2 (Y 1 , Y 2 ) ≤ H 2 (Y 1 ) + H 2 (Y 2 ). Fix M > 2 and assume that the lemma is true for any N < M. To prove the statement for N = M, we will use the following inequality, which holds for any three random variables U 1 , U

2 , U 3 : H 2 (U 1 , U 3 ) + H 2 (U 2 , U 3 ) ≥ H 2 (U 1 , U 2 , U 3 ) + H 2 (U 3 ) . This inequality is equivalent to the statement on conditional entropies that H 2 (U 1 , U 2 | U 3 ) ≤ H 2 (U 1 | U 3 ) + H 2 (U 2 | U 3 ). We apply it, for given k l, to U 1 = Y l , U 2 = Y k , U 3 = Y (k,l) , yielding H 2 (Y (k) ) + H 2 (Y (l) ) ≥ H 2 (Y ) + H 2 (Y (k,l) ). We now sum over all pairs k l, yielding 2(N -1) N k=1 H 2 (Y (k) ) ≥ N (N -1)H 2 (Y ) + k l H 2 (Y (k,l) ). We finally use the induction hypothesis to write that, for all k l k H 2 (Y (k,l) ) ≥ (N -2)H 2 (Y (k) ) and obtain 2(N -1) N k=1 H 2 (Y (k) ) ≥ N (N -1)H 2 (Y ) + (N -2) N k=1 H 2 (Y (k) ), which provides the desired result after rearranging the terms. ■ Note that theorem 22.16 involves S F (2N ), with: log 2 (S F (2N )) = log 2 E(S F (X 1 , . . . , X 2N )) ≥ H VC (2N , F ) from Jensen's inequality. This implies that the high-probability upper bound on H VC (2N , F ) that results from the previous theorem is not necessarily an upper bound on log(S F (2N )). It is however proved in Boucheron et al. [38] that log 2 E(S F (X 1 , . . . , X 2N )) ≤ 1 log 2 H VC (2N , F ) also holds (as a consequence of (22.16)). A little more work (see Boucheron et al. [38]) combining theorem 22.16 and theorem 22.24 implies the following bound, which holds with probability 1δ at least: ∀f ∈ F : R(f ) ≤ E(f ) + 6 log S F (X 1 , . . . , X N ) N + 4 log(2/δ) N .

## Covering numbers and chaining

The upper bounds using the VC dimension relied on the number of different values taken by a set of functions when evaluated on a finite set, this number being used to apply a union bound. A different point of view may be applied when one relies on some notion of continuity of the family of functions on which a uniform concentration bound is needed, with respect to a given metric. This viewpoint is furthermore applicable when the sets F (X 1 , . . . , X N ) are infinite. To develop these tools, we will need some new concepts measuring the size of sets in a metric space.

Assume that N (G, ρ ∞ , ϵ) < ∞, for all ϵ > 0 (which requires the set G to be precompact for the ρ ∞ metric). Take t > 0, 0 < ϵ < t and choose a set G ⊂ G such that |G| = N (G, ρ ∞ , ϵ). Then, using a union bound, for some µ(g) > 0, then, assuming that µ(G) ∆ = max g∈G µ(g) is finite, we find that, for 0 < ϵ < t, We now apply this inequality to the case of binary classification, where a binary variable Y is predicted by an input variable X, with a model class of classifiers F and the 0-1 loss function. If A is a finite family of elements of R, we define, for f , f ′ ∈ F

## P(sup

## P(sup

$ρ A (f , f ′ ) = 1 |A| x∈A 1 f (x) f ′ (x) .$Let N (F , ϵ, N ) = E N (F , ρ {X 1 ,...,X N } , ϵ)

where X 1 , . . . , X N is an i.i.d. sample of X. We then have the following proposition. Proof A key step in the proof of theorem 22.16, was to show that [(22.30)](#) where ξ 1 , . . . , ξ N are Rademacher random variables and T , T ′ are two independent training sets of size N . We start from this inequality and bound the conditional expectation

$P sup f ∈F (R(f )-E T (f )) ≥ t ≤ 2P sup f ∈F N k=1 ξ k (r(Y ′ k , f (X ′ k ))-r(Y k , f (X k ))) ≥ N t/2 .$$P sup f ∈F N k=1 ξ k (r(Y ′ k , f (X ′ k )) -r(Y k , f (X k ))) ≥ N t/2 T , T ′ (22.31)$and therefore consider r(Y ′ k , f (X ′ k ))r(Y k , f (X k )) as constants that we will denote c k (f ). Since we are using a 0-1 loss, we have c k (f ) ∈ {-1, 0, 1} and, for f , f ′ ∈ F ,

$|c k (f ) -c k (f ′ )| ≤ 1 f (X k ) f ′ (X k ) + 1 f (X ′ k ) f ′ (X ′ k ) .$(22.32)

Consider the random variable Z = (ξ 1 , . . . , ξ N ), and let

$G = g f , f ∈ F with g f (ξ 1 , . . . , ξ N ) = 1 N N k=1 c k (f )ξ k .$We have Let A = (X 1 , . . . , X N , X ′ 1 , . . . , X ′ N ) so that

$ρ ∞ (g f , g f ′ ) = 1 N N k=1 |c k (f ) -c k (f ′ )| .$$ρ A (f , f ′ ) = 1 2N N k=1 1 f (X k ) f ′ (X k ) + 1 f (X ′ k ) f ′ (X ′ k ) .$Using [(22.32)](#), we have ρ ∞ (g f , g f ′ ) ≤ 2ρ A (f , f ′ ), which implies

$N (G, ϵ, ρ ∞ ) ≤ N (F , ϵ/2, ρ A ) .$Using this in [(22.33)](#) and taking the expectation in [(22.31)](#), we get So [(22.29)](#) provides a family of equations that depend on a parameter ϵ which, in the limit ϵ → 0, includes theorem 22.16 as a particular case. For a given N , optimizing (22.29) over ϵ may give a better upper bound, provided one has a good way to estimate N (F , ϵ/2, N ) (which is, of course, far from obvious).

## Evaluating covering numbers

Covering numbers can be evaluated in some simple situations. The following proposition provides an example in finite dimensions. 

## ■

One can also obtain entropy number estimates in infinite dimensions. Here, we quote a result applicable to spaces of smooth functions, referring to Van der Vaart and Wellner [[195]](#b213) for a proof. Let G be the unit ball for this norm,

$G = f ∈ C p (Z) : ∥f ∥ p,∞ ≤ 1 .$Let Z (1) be the set of all x ∈ R d at distance less than 1 from R.

Then there exists a constant K depending only on p and d such that log N (ϵ, G, ρ ∞ ) ≤ Kvolume(Z (1) ) 1 ϵ d/p

## Chaining

The distance ρ ∞ may not always be the best one to analyze the set of functions, G. For example, if G is a class of functions with values in {-1, 1}, then ρ ∞ (g, g ′ ) = 2 unless g = g ′ . In such contexts, it is often preferable to use distances that compute average discrepancies, such as ρ p (g, g ′ ) = E(|g(Z)g ′ (Z)| p ) 1/p , [(22.35)](#) for some random variable Z. Such distances, by definition, do not provide uniform bounds on differences between functions (that we used to write [(22.28)](#)), but can rather be used in upper-bounds on the probabilities of deviations from zero, which have to be handled somewhat differently. We here summarize a general approach called "chaining," following for this purpose the presentation made in Talagrand [[189]](#b207) (see also [Audibert and Bousquet [15]](#)). From now on, we assume that (G, ρ) is a (pseudo-)metric space of functions g : Z → R and Z a random variable taking values in Z. We will make the basic assumption that, for all g, g ′ ∈ G and t > 0, P(|g(Z)g ′ (Z)| > t) ≤ 2e

t 2 2ρ(g,g ′ ) 2 .

Note that this assumption includes cases in which P(|g(Z)g ′ (Z)| > t) ≤ 2e

t 2 2ρ(g,g ′ ) α .

for some α ∈ (0, 2], because, if ρ is a distance, then so is ρ α/2 if α ≤ 2. We will also assume that E(g(Z)) = 0 in order to avoid centering the variables at every step.

We are interested in upper bounds for P(sup g∈G g(Z) > t). To build a chaining argument, consider a family (G 0 , G 1 , . . .) of subsets of G. Assume that |G k | ≤ N k with N k chosen, for future simplicity, so that N k-1 N k ≤ N k+1 . For g ∈ G, let π k (g) denote a closest point to g in G k . Also assume that G 0 = {g 0 } is a singleton, so that π 0 (g) = g 0 for all g ∈ G. (One can generally assume without harm that 0 ∈ G, in which case one should choose g 0 = 0 in the following discussion.) For g ∈ G n , we therefore have

$g -g 0 = n k=1 (π k (g) -π k-1 (g)) .$Let (t 1 , t 2 , . . .) be a sequence of numbers that will be determined later. Let 2 2 k+1 e -2 k-1 t 2 . The upper bound converges (as a function of n) as soon as t > 2 log 2. Moreover, one has 2 n k=1 2 2 k+1 e -2 k-1 t 2 = 2e -t 2 2 n k=1 e -2 k-2 (t 2 -8 log 2) ≤ 2e -t 2 2 ∞ k=1 e -2 k-2 when t > 1 + 8 log 2. This provides a concentration bound for P(sup g∈G n g(Z)g 0 (Z) > tS n ), that we may rewrite as P(sup g∈G n g(Z)g 0 (Z) > t) ≤ Ce -t 2 2S 2 n (22.37) for t > 2S n log 2, C = 2 ∞ k=1 e -2 k-2 and S n given by (22.36), with t k = 2 k/2 . Moreover, we have S n = max and this simpler upper bound can be used in [(22.37)](#).

We haven't made many assumptions so far on the sequence G 0 , G 1 , . . ., beyond bounding their cardinality, but it is natural to require that they are built in order to behave like a dense subset of G, so that lim n→∞ max g∈G ρ(x, G n ) = 0. [(22.38)](#) Note that this requires that the set G is precompact for the distance ρ. We will also assume that lim The exponential rate of convergence in the right-hand side of [(22.41)](#) is the quantity S, and the upper bound will be improved when building the sequence (G 0 , G 1 , . . .) so that S is as small as possible. Such an optimization for a given family of functions is however a formidable problem. It is however interesting to see (still following [[188]](#b206)) that theorem 22.31 implies a classical inequality in terms of what is called the metric entropy of the metric space (G, ρ). ))

## Metric entropy

$≥ 1 - √ 2 2 ∞ n=1 2 n/2 e(2 2 n ).$is obvious. The same inequality will be true for some x 1 , . . . , x N with N = 2, except in the uninteresting case where f (x) = 1 (or -1) for every x ∈ R.

A similar inequality holds for entropy numbers with the ρ 1 distance (cf. [(22.35)](#)) because E(|r(Y , f (X))r(Y , f ′ (X))|) ≤ P(f (X) f ′ (X))

whenever r takes values in [0, 1], which implies that

$N (G, ρ 1 , ϵ) ≤ N (F , ρ 1 , ϵ)$for all ϵ > 0. Note however that evaluating this upper bound may still be challenging and would rely on strong assumptions on the distribution of X allowing to control P(f (X) f ′ (X)).

We now assume that functions in F define "posterior probabilities" on G. More precisely, given λ ∈ R we can define the probability π λ on {-1, 1} by so that entropy numbers in G can be estimated from entropy numbers in F . As an example, let F be a space of affine functions x → a 0 + b T x, x ∈ R d . Assume that the random variable X is bounded, so that one can take R to be an open ball centered at 0 with radius, say, U . For M > 0, let

$F M = {f : x → a 0 + b T x : |b| ≤ M, |a 0 | ≤ U M} .$The restriction |b| ≤ M is equivalent to using a penalty method, such as, for example, ridge logistic regression. Moreover, if |b| ≤ M, it is natural to assume that |a 0 | ≤ U M because otherwise f would have a constant sign on R. In this case, we get We have

$E T ′ (f ) -E γ,T (f ) = 1 N N k=1 (r 0 (Y ′ k , f (X ′ k )) -r γ (Y k , f (X k )))$and because (X k , Y k ) and (X ′ k , Y ′ k ) have the same distribution, sup f ∈F (E T ′ (f ) -E γ,T (f )) has the same distribution as

$∆ T ,T ′ (ξ 1 , . . . , ξ N ) = sup f ∈F 1 N N k=1 (r 0 (Y ′ k , f (X ′ k )) -r γ (Y k , f (X k )))ξ k + (r 0 (Y k , f (X k )) -r γ (Y ′ k , f (X ′ k )))(1 -ξ k )$where ξ 1 , . . . , ξ N is a sequence of Bernoulli random variables with parameter 1/2.

We now estimate P(∆ T ,T ′ (ξ 1 , . . . , ξ N ) > t/2 | T , T ′ ) and we therefore consider T and T ′ as fixed. Let F be a subset of F , with cardinality N ∞ (γ/2, 2N ), such that for all f ∈ F there exists an f ′ ∈ F such that |f (x)-f ′ (x)| ≤ γ/2 for all x ∈ {X 1 , . . . , X N , X ′ 1 , . . . , X ′ N }. Then we claim that ∆ T ,T ′ (ξ 1 , . . . , ξ N ) ≤ ∆ ′ T ,T ′ (ξ 1 , . . . , ξ N ) where

$∆ ′$T ,T ′ (ξ 1 , . . . , ξ N ) = max

$f ∈F 1 N N k=1 (2ξ k -1) r γ 2 (Y ′ k , f (X ′ k )) -r γ 2 (Y k , f (X k )) .$This is because, for any (x, y) ∈ R × {0, 1}, and f , f ′ such that |f (x)f ′ (x)| < γ/2, we have r 0 (y, f (x)) ≤ r γ/2 (y, f ′ (x)) and r γ/2 (y, f ′ (x)) ≤ r γ (y, f (x)): if an example is misclassified by f (resp. f ′ ) at a given margin, it must be misclassified by f ′ (resp. f ) at this margin plus γ/2.

## Now,

## P(∆ ′

T ,T ′ (ξ 1 , . . . , ξ N ) > t 2 )

$≤ |F| max f ∈F P 1 N N k=1 (2ξ k -1)(r γ 2 (Y ′ k , f (X ′ k )) -r γ 2 (Y k , f (X k ))) > t 2$to which we can apply Hoeffding's inequality, yielding In order to evaluate the covering numbers N ∞ (ϵ, N ) using quantities similar to VC-dimensions, a different type of set decomposition and shattering has been proposed. Following Alon et al. [[4]](#b22), we introduce the following notions. Recall that a family of functions F : R → {0, 1} shatters a finite set A ⊂ R if and only if |F (A)| = 2 |A| . The following definitions are adapted to functions taking values in a continuous set. (i) One says that F P -shatters A if there exists a function g A : R → R such that, for each B ⊂ A, there exists a function f ∈ F such that f (x) ≥ g A (x) if x ∈ B and f (x) < g A (x) if x ∈ A \ B.

$P ∆ ′$(ii) Let γ be a positive number. One says that F P γ -shatters A if there exists a function g A : R → R such that, for each B ⊂ A, there exists a function f ∈ F such that f (x) ≥

$g A (x) + γ if x ∈ B and f (x) ≤ g A (x) -γ if x ∈ A \ B.$Note that only the restriction of g A to A matters in this definition. This function acts as a threshold for binary classification. More precisely, given a function g : A → R, one can associate to every f ∈ F the binary function f g with f g (x) equal to 1 if f (x) ≥ g(x) and to 0 otherwise. Letting F g = {f g : f ∈ F } we see that F P-shatters A if there exists a function g A such that F g A shatters A. The definition of P γ -shattering introduces a margin in the definition of f g (with f g (x) equal to 1 if f (x) ≥ g(x) + γ, to 0 if f (x) ≤ g(x)γ and is ambiguous otherwise), and A is P γ -shattered by F if, for some g A , the corresponding F g A shatters A without ambiguities. Definition 22.36 One then defines the P -dimension of F by P-dim(F ) = max{|A| : A ⊂ R, F P -shatters A}, and similarly the P γ -dimension of F is P γ -dim(F ) = max{|A| : A ⊂ R, F P γ -shatters A}.

The P γ -dimension of F will replace the VC-dimension in order to control the covering numbers. More precisely, we have the following theorem [[4]](#b22). .

Proof The proof is quite technical and relies on a combinatorial argument in which F is first assumed to take integer values before addressing the continuous case.

Step 1. We first assume that functions in F take values in the finite set {1, . . . , r} where r is an integer. For the time of this proof, we introduce yet another notion of shattering called S-shattering (for strong shattering) which is essentially the same as P 1 -shattering, except that functions g are restricted to take values in {1, . . . , r}. Let A be a finite subset of R. Given a function g : R → {1, . . . , r}, we say that (F , g) Sshatters A if, for any B ⊂ A, there exist f ∈ F satisfying f (x) ≥ g(x) + 1 for x ∈ B and f (x) ≤ g(x) -1 if x ∈ A \ B. We say that F S-shatters A if (F , g) S-shatters A for some g. The S-dimension of F is the cardinality of the largest subset of R that can be S-shattered and will be denoted S-dim(F ). The first, and most difficult, part of the proof is to show that, if S-dim(F ) = D, then and ⌈i⌉ denotes the smallest integer larger than u ∈ R. Here, M is the packing number defined in section 22.5.1.

$M(F (A), ρ ∞ , 2) ≤ 2($To prove this, we can assume that r ≥ 3, since, for r ≤ 2, M(F (A), ρ ∞ , 2) = 1 (the diameter of F for the ρ ∞ distance is 0 or 1). Let G(A) = {1, . . . , r} A be the set of all functions f : A → {1, . . . , r} and let

$U A = F ⊂ G(A) : ∀f , f ′ ∈ F, ∃x ∈ A with |f (x) -f ′ (x)| ≥ 2 .$For F ∈ U A , let S A (F) = {(B, g) : B ⊂ A, B ∅, g : B → {1, . . . , r}, (F, g) S-shatters B}.

Let t A (h) = min{|S A (F)| : F ∈ U A , |F| = h} (where the minimum of the empty set is +∞). Since we are considering in U A all possible functions from A to {1, . . . , r}, it is clear that t A (h) only depends on |A|, and we will also denote it by t(h, |A|).

Note that, by definition, if (B, g) ∈ S A (F), and F ⊂ F , then |B| ≤ D. So, the number of elements in S A (F) for such an F is less or equal than the number of possible such pairs (B, g), which is strictly less than y = D k=1 |A| k r k . So, if t(h, |A|) ≥ y, then there cannot be any F ⊂ F in the set U A and M(F (A), ρ ∞ , 2) < h. The rest of the proof consists in showing that t(h, |A|) ≥ y.

For any n ≥ 1, we have t(2, n) = 1: fix x ∈ A, and F = {f 1 , f 2 } ∈ G such that f 1 (x) = 1, f 2 (x) = 3 and f 1 (y) = f 2 (y) if y x. Then only ({x}, g) is S-shattered by F, with g such that g(x) = 2. Now, assume that, for some integer m, t(2mnr 2 , n) < ∞, so that there exists F ∈ U A such that |F| = 2mnr 2 . Arrange the elements of F into mnr 2 pairs {f i , f ′ i }. For each such pair, there exists x i ∈ A such that |f i (x i )f ′ i (x i )| > 1. Since there are at most n selected x i , one of them must be appearing at least mr 2 times. Call it x and keep (and reindex) the corresponding mr 2 pairs, still denoted {f i , f ′ i }. Now, there are at most r(r -1)/2 possible distinct values for the unordered pairs {f i (x), f ′ i (x)}, so that one of them must be appearing at least 2mr 2 /r(r -1) > 2m times. Select these functions, reindex them and exchange the role of f i and f ′ i if needed to obtain 2m pairs {f i , f ′ i } such that f i (x) = k and f ′ i (x) = l for all i and fixed k, l ∈ {1, . . . , r} such that k + 1 < l. Let F 1 = {f 1 , . . . , f 2m } and F ′ 1 = {f ′ 1 , . . . , f ′ 2m }. Let A ′ = A \ {x}. Then both F 1 and F ′ 1 belong to U A ′ , which implies that both S A ′ (F 1 ) and S A ′ (F ′ 1 ) have cardinality at least t(2m, n-1). Moreover, both sets are included in S A (F), and if (B, g) ∈ S A ′ (F 1 )∩S A ′ (F ′ 1 ), then (B ∪ {x}, g ′ ) ∈ S A (F), with g ′ (y) = g(y) for y ∈ B and g ′ (x) = k + 1. This provides 2t(2m, n-1) elements in S A (F) and shows the key inequality (which is obviously true when the left-hand side is infinite) t(2mnr 2 , n) ≥ 2t(2m, n -1) .

This inequality can now be used to prove by induction that for all 0 ≤ k < n, one has t(2(nr 2 ) k , n) ≥ 2 k , since t(2((n + 1)r 2 ) k+1 , n + 1) ≥ 2t(2((n + 1)r 2 ) k , n) ≥ 2t(2(nr 2 ) k , n).

For k ≥ n, one has 2(nr 2 ) k > r n , where r n is the number of functions in G(A), so that t(2(nr 2 ) k , n) = +∞. So, t(2(nr 2 ) k , n) ≥ 2 k is valid for all k and it suffices to take k = ⌈log 2 y⌉ to obtain the desired result.  To prove (a), assume that F η S-shatters A, so that there exists g such that, for all B ⊂ A, there exists f ∈ F such that f η (x) ≥ g(x) + 1 for x ∈ B and f η (x) ≤ g(x) -1 for x ∈ A \ B. Using the fact that 2ηf η (x) -1 ≤ f (x) < 2ηf η (x) + 2η -1, we get f (x) ≥ 2ηg(x)+2η-1 for x ∈ B and f (x) ≤ 2ηg(x)-1 for x ∈ A\B. So taking g(x) = 2ηg(x)+η-1 as threshold function (which does not depend on B), we see that F P γ -shatters A if γ ≤ η. This shows, in particular, that (letting ξ 1 , . . . , ξ N be independent Rademacher random variables)

$P        Λ N k=1 ξ k + N k=1 ξ k x k ≥ N γ        = 1.$where U T (γ) depends on data and is increasing (as a function of γ), and C(γ) is a decreasing function of γ. Consider a decreasing sequence (γ k ) that converges to 0 (for example γ k = L2 -k ). Choose also an increasing function ϵ(γ). Then P R 0 ( fT ) > min{U T (γ) + t 2 + 2 log C(γ) + ϵ 2 (γ) : 0 ≤ γ ≤ L} ≤ P R 0 ( fT ) > min{U T (γ k ) + t 2 + 2 log C(γ k-1 ) + ϵ 2 (γ k ) : k ≥ 1} .

$Moreover P R 0 ( fT ) > min{U T (γ k ) + t 2 + 2 log C(γ k-1 ) + ϵ 2 (γ k ) : k ≥ 1} ≤ ∞ k=0 P R 0 ( fT ) > U T (γ k ) + t 2 + 2 log C(γ k-1 ) + ϵ(γ k ) ≤ ∞ k=0 C(γ k ) C(γ k-1 )$e -mϵ 2 (γ k )/2-mt 2 /2 .

So, it suffices to choose ϵ(γ) so that

$C 0 = ∞ k=1 C(γ k ) C(γ k-1 )$e -mϵ 2 (γ k )/2 < ∞ to ensure that P R 0 ( fT ) > min{U T (γ) + t 2 + 2 log C(γ) + ϵ 2 (γ) : γ 0 ≤ γ ≤ L} ≤ C 0 e -mt 2 /2 .

For example, if γ k = L2 -k , one can take

$ϵ(γ) = 2 m log C(γ) C(γ/2) + log γ -1$which yields C 0 ≤ L. ♦

## Maximum discrepancy

Let T be a training set and let T 1 and T 2 form a fixed partition of the training set in two equal parts. Assume, for simplicity, that N is even and that the method for selecting the two parts is deterministic, e.g., place the first half of T in T 1 and second one in T 2 . Following Bartlett et al. [[20]](#), one can then define the maximum discrepancy on T by C T = sup 

$ξ k r(Y k , f (X k )) T = T .$The mean Rademacher complexity is then the expectation of this quantity over the training set distribution. The Rademacher complexity can be computed with a-costly-Monte-Carlo simulation, in which the best estimator is computed with randomly flipped labels corresponding to the values of k such that ξ k = -1. This measure of complexity was introduced to the machine learning framework in Koltchinskii and Panchenko [[109]](#b127), [Bartlett and Mendelson [19]](#), and Rademacher sums have been extensively studied in relation to empirical processes (cf. Ledoux and Talagrand [[117]](#b135), chapter 4).

One can bound the Rademacher complexity in terms of VC dimension. 

## PAC-Bayesian bounds

Our final discussion of concentration bounds for the empirical error uses a slightly different paradigm from that discussed so far. The main difference is that, instead of computing one predictor fT from a training set T , it would return a random variable with values in F , or, equivalently, a probability distribution on F (therefore assuming that this space is measurable) that we will denote μT . The training set error is now defined by: ĒT (µ) = E T (f )dµ(f ) ,

for any probability distribution µ on F , while the generalization error is:

$R(µ) = F R(f )dµ(f ) .$Our goal is to obtain upper bounds on R(µ T )-ĒT (µ T ) that hold with high probability.

In this framework, we have the following result, in which Q denotes the space of probability distributions on F . The term log 2N is however superfluous in this simple context, because one can write, for any t > 0

$P sup f ∈F 0 R(f ) -E T (f ) ≥ t - log(π(f )) 2N ≤ f ∈F 0$e -2N (t log(π(f ))

## 2N

) = e -2N t so that, with probability 1δ (letting t = log(1/δ)/2N ), for all f ∈ F 0 : R(f ) -E T (f ) ≤ log δlog π(f ) 2N . ♦

## Application to model selection

We now describe how the previous results can, in principle, be applied to model selection [[20]](#). We assume that we have a countable family of nested models classes (F (j) , j ∈ J ). Denote, as usual, by E T (f ) the empirical prediction error in the training set for a given function f . We will denote by f (j) T a minimizer of the in-sample error for F (j) , such that E T ( f (j) T ) = min

$f ∈F (j)$E T (f ).

In the model selection problem, one would like to determine the best model class, j = j(T ), such that the prediction error R( f (j) T ) is minimal, or, more realistically, determine j * such that R( f (j * ) T ) is not too far from the optimal one.

We will consider penalty-based methods in which one minimizes ẼT (f ) = E T (f ) + C T (j) to determine j(T ). The penalty, C T , may also be data-dependent, and will therefore be a random variable. The previous concentration inequalities provided highly probable upper-bounds for R( f (j) T ), each exhibiting a random variable Γ

T that is larger than R( f (j) T ) with probability close to one. More precisely, we obtained inequalities taking the form (when applied to F (j) ) P(R T ( f (j) ) ≥ Γ (j) T + t) ≤ c j e -mt 2 [(22.48)](#) for some known constants c j and m. For example, the VC-dimension bounds have

$Γ (j)$T = E T ( f (j) T ), c j = 2S F (j) (2N ) and m = N /8.

Given such inequalities, one can develop a model selection strategy that relies on a priori weights, provided by a sequence π j of positive numbers such that j∈J π j = 1. Define The selected model class is then F (j * ) where j * minimizes Γ The same proof as that provided at the end of section 22.6.5 justifies this procedure. Indeed, for t > 0, P R( fT ) -ẼT ( fT ) ≥ t ≤ P max j (R( f (j) T ) -ẼT ( f (j) T )) ≥ t

$≤ P        max j (R( f (j) T ) ≥ R * j + t + - log πj m        ≤ c j π j e -mt 2$≤ ce -mt 2 with c = ∞ j=1 π j /c j .

![The set of m × d real matrices with real entries is denoted M m,d (R), or simply M m,d (M d,d will also be denoted M d ). The set of invertible d × d matrices will be denote GL d (R). Given m column vectors x 1 , . . . , x m ∈ R d , the notation [x 1 , . . . , x m ] refers to the d by m matrix with j th column equal to x j , so that, for example, Id R d = [e 1 , . . . , e d ].]()

![The space of d × d real symmetric matrices is denoted S d , and its subsets containing positive semi-definite (resp. positive definite) matrices is denotedS + d (resp. S ++ d ). If m ≤ d, O m,d denotes the set of m × d matrices A such that AA T = Id R m , and one writes O d for O d,d , the space of d-dimensional orthogonal matrices. Finally, SO d is the subset O d containing orthogonal matrices with determinant 1, i.e., rotation matrices.]()

![|a| = max{a(x 1 , . . . , x k ) : |x j | ≤ 1, j = 1, . . . , k} so that |a(x 1 , . . . , x k )| ≤ |a| k j=1 |x j | for all x 1 , . . . , x k ∈ R d .]()

![∀x, y ∈ B : ρ(x, y) = 0 ⇔ x = y, (1.2a) ∀x, y ∈ B : ρ(x, y) = ρ(y, x), (1.2b) ∀x, y, z ∈ B : ρ(x, z) ≤ ρ(x, y) + ρ(y, z). (1.2c) Equation (1.2c) is called the triangle inequality. The norm of the difference between two points: ρ(x, y) = |x -y|, is a distance on R d . The definition of open and closed]()

![Consider now the function I(A) =A → A -1 defined on GL d (R), which is an open subset of M d (R). Using AI(A) = Id R d andthe product rule, we getA(dI(A)H) + HI(A) = 0 or dI(A)H = -A -1 HA -1 . (1Higher-order partial derivatives ∂ i k • • • ∂ i 1 f : U → R m aredefined by iterating the definition of first-order derivatives, namely]()

![Von Neumann) Let A, B ∈ M n,d (R) have singular values (λ 1 , . . . , λ m ) and (µ 1 , . . . , µ m ), respectively, where m = min(n, d). Assume that these eigenvalues are listed in decreasing order so that λ 1 ≥ • • • ≥ λ m and µ 1 ≥ • • • ≥ µ m . Then, trace(A T B) ≤ m i=1 λ i µ i .]()

![Let us consider the first sum in the upper-bound. Let ξ d = λ d (resp. η d = µ d ) and ξ i = λ iλ i+1 (resp. η i = µ iµ i+1 ) for i = 1, . . . , d -1. Since singular values are nonincreasing, we have ξ i , η i ≥ 0 and i = 1, . . . , d. We have d i,j=1]()

![Let A ∈ S d (R) be a symmetric matrix with eigenvaluesλ 1 ≥ • • • ≥ λ d . For p ≤ d, let µ 1 ≥ • • • ≥ µ p >0 and define F(e 1 , . . . , e p ) = p i=1]()

![Let A ∈ M d,d (R) be a symmetric matrix with eigenvalues λ 1 ≥ • • • ≥ λ d . Then]()

![Let A be an n by d matrix. Then|A| * = max trace(U AV T ) : U ∈ M n,n and U T U = Id, V ∈ M d,d and V T V = Id .]()

![Conditions for optimality (general case) Consider a function F : Ω → R where Ω is an open subset of R d . We first discuss the unconstrained optimization problem of finding x * ∈ argmin Ω F.]()

![|∇F(0)|This shows that F(x) > F(0) if |x| > 2|∇F(0)|/m := r so that argmin F = argmin B(0,r)]()

![15 A function F : Ω → R m is L-C k , L being a positive number, if it is C k and |d k F(x)d k F(y)| ≤ L|x -y|.]()

![]()

![.16a) for some constant c 2 ∈ (c 1 , 1). The strong Wolfe conditions require (3.15) and|h T ∇F(x + αh)| ≤ c 2 |h T ∇F(x)|. (3.16b) (Since h is a direction of descent, (3.16b) requires (3.16a) and the fact that h T ∇F(x + αh) does not take too large positive values.) If F is L-C 1 , these conditions, with (3.12a) and (3.12b), imply (3.14). Indeed, (3.16a) and the L-C 1 condition imply -(1c 2 )h T ∇F(x) ≤ h T (∇F(x + αh) -∇F(x)) ≤ Lα|h| 2 and (3.12a) and (3.12b) give (1c 2 )ϵ|∇F(x)| 2 ≤ αLγ 2 2 |∇F(x)| 2]()

![Stochastic gradient descent 3.3.1 Stochastic approximation methods]()

![Finally, let∆(ρ, T ) = max s∈[ρ,ρ+T ] s ρ η c (u)du .]()

![30 A point x ∈ Ω satisfies the Mangasarian-Fromovitz constraint qualifications (MF-CQ) if the following two conditions are satisfied. (MF1) The vectors (∇γ i (x), i ∈ E) are linearly independent.]()

![General convex problems 3.5.1 Epigraphs Definition 3.39 Let F be a convex function. The epigraph of F is the set]()

![for all λ ∈ D and x ∈ Ω. Define d = sup{L * (λ) : λ ∈ D} and p = inf{F(x) : x ∈ Ω}, whose computations respectively represent the dual and primal problems. Then, we have d ≤ p.]()

![56)for all x ∈ R d and λ ∈ D. Such a pair (x * , λ * ) is called a saddle point of the function L. Conversely, any saddle point of L, i.e., any (x * , λ * ) ∈ R d × D satisfying(3.56), must be such that x * ∈ Ω (to ensure that L(x * , •) is bounded), and satisfies the KKT conditions.]()

![Proof of proposition 3.44 We start with a few general remarks. If x ∈ R d , the set {x} is convex and relint({x}) = {x}. If Ω is any convex set such that x relint(Ω), then theorem 3.63 implies that there exist b ∈ R d and β ∈ R such that b T y ≥ β ≥ b T x for all y ∈ Ω (with b T y > b T x for at least one y). If x is in Ω \ (relint(Ω)) (so that x is a point on the relative boundary of Ω), then, necessarily b T x = β and we can write b T y ≥ b T x]()

![65 Let F be a convex function with epigraph epi(F) = {(y, a) : y ∈ dom(F), F(y) ≤ a}. Then relint(epi(G)) = {(y, a) : y ∈ ridom(F), F(y) < a}. Proof Let Γ = {(y, a) : y ∈ ridom(F), F(y) < a}. Assume that (y, a) ∈ relint(epi(F)). Then (y, b) ∈ epi(F) for all b > a and there exists ϵ > 0 such that (y, a)ϵ((y, b) -(y, a)) ∈ epi(F) which requires that F(y) ≤ aϵ(b -1) < a. Now, take x ∈ dom(F).]()

![Proof of theorem 3.45]()

![Let µ and ν be two probability measures on Ω. Then KL(µ∥ν) ≥ 0 and vanishes if and only if µ = ν.]()

![4.1 and fig. 4.2).]()

![Figure 4.1: Kernel density estimators using a Gaussian kernel and various values of σ when the true distribution of the data is a standard Gaussian (Orange: true density; Blue: estimated density, Red dots: training data).]()

![set T , one can compute, for each λ, the cross-validation error e T (λ) = RCV,T (A λ ). Model selection is then performed by finding λ * (T ) = argmin λ e T (λ).]()

![A function K : R × R → R satisfying properties [K1] and [K2] is called a positive kernel.]()

![is attained at â0 = ȳ -xT b with the usual definitions]()

![This shows that there is no loss of generality in restricting the minimization of the residual sum of squares to b ∈ V . Such a b takes the form b]()

![The function λ → U (β λ ) is nondecreasing, and λ → ϕ(β λ ) is nonincreasing, with lim λ→∞ ϕ(β λ ) = inf(ϕ).]()

![terminate the algorithm without updating ζ and set b]()

![Figure 7.1: The function V defining the SVM risk function.]()

![]()

![Logistic lasso)(1) Input: (i) training data (x 1 , y 1 , . . . , x N , y N ) with x i ∈ R d and y i ∈ R Y ; (ii) coefficients ρ g , g ∈ R Y with non-zero sum and target value c ∈ R; (iii) algorithm step ϵ; (iv) penalty coefficient λ.]()

![Figure 8.1: Left: Original (training) data with three classes. Right: LDA scores, where the x axis provides γ 1 and the y axis γ 2 .]()

![8.2).]()

![Figure 8.2: The green line is preferable to the purple one in order to separate the data.]()

![Randomized node insertion: RNode(T , j)) (a) Given T and j, let T ν = T and L(ν) = j. (b) If σ (T ) = 1, let C(ν) = ∅, γ ν = "None" and f ν = fT ,CF .]()

![Figure 11.1: Linear net with increasing layer depths and decreasing layer width.]()

![11.5.1 Mini-batchesFix ℓ ≪ N . Consider the set of B ℓ of binary sequences ξ]()

![Rejection sampling with acceptance function a and base p.d.f. g) (1) Sample a realization z of a random variable with p.d.f. g.(2) Generate b ∈ {0, 1} with P(b = 1) = a(z).]()

![If b = 1, return Z = z and exit.(4) Otherwise, return to step 1.]()

![∆q(y) + ∇H(y) T ∇q(y) + q(y)∆H(y) = 0 (12.15) is satisfied by the function y → e -2H(y)]()

![Example: Ising modelWe will see several examples of applications of Gibbs sampling in the next few chapters. Here, we consider a special instance of Markov random field (see chapter 13) called the Ising model. For this example, B = {0, 1} L , and]()

![Denoting the Markov chain by (Z n , M n ), we assume that the next pair Z n+1 , M n+1 is computed by (i) sampling M ′ n ∼ N (0, Id R d ); (ii) solving (12.23), with initial conditions ζ(0) = Z n and µ(0) = M ′ n ; (iii) taking Z n+1 = ζ(θ) and sampling M n+1 ∼ N (0, Id R d ).]()

![with ρ = max(ρ(|x|), ρ(|y|)).]()

![With the notation above, π S|T (• | y (T ) ) is associated to the family of local interactions Φ |y T = (ϕ C|y (T ) , C ∈ C S ) with]()

![|C|-| B| = 0 (for B = B, the sum is obviously equal to 1). Indeed, if s ∈ B, s B, we have C⊃ B,C⊂B (-1) |C|-| B| = C⊃ B,C⊂B,s∈C (-1) |C|-| B| + C⊃ B,C⊂B,s C (-1) |C|-| B| = C⊃ B,C⊂B,s C ((-1) |C∪{s}|-| B| + (-1) |C|-| B| ) = 0.]()

![Figure 13.3: Graph forming a two-dimensional regular grid.]()

![24 Let G = (V , E) be an undirected graph, and C * G be the set of all maximum cliques in G. The following two properties are equivalent.(i) There exists a junction tree over C * G .]()

![JT1) Extend G by adding edges to obtain a triangulated graph G * . (JT2) Compute the set C * of maximal cliques in G * , which therefore extend C. (JT3) Build a junction tree over C * . (JT4) Assign interaction ϕ C to a clique C * ∈ C * such that C ⊂ C * . (JT5) Run the junction-tree belief propagation algorithm to compute the marginal of π (associated to Φ) over each set C * ∈ C * .]()

![Graph triangulation)Initialize the algorithm with k = n and E k = E. Given E k , determine E k-1 as follows:]()

![Prim's algorithm, adding a new edge e k+1 = {C k+1 , C ′ } to T k . Take as before the path in T linking C ′ to C k+1 in T , and select the edge e at which this path leaves C k . If e = (B, B ′ ), we must have w(e) = |B ∩ B ′ | ≤ w(e k ) = |C k+1 ∩ C ′ |, and the running intersection property in T implies that C k+1 ∩ C ′ ⊂ B ∩ B ′ , which implies that w(e) = w(e k+1 ).]()

![Bayesian networks are graphical models supported by directed acyclic graphs (DAG), which provide them with an ordered organization (directed graphs were introduced in definition 13.35).]()

![One says that two vertexes s and t in G are d-separated by a set U if and only if any path between s and t in G ♭ must either (D1) Pass at a vertex in U without a v-junction. (D2) Pass in V \ A U with a v-junction. Then we have: Theorem 15.11 Two vertexes s and t in G are separated by a set U in (G A {s,t}∪U ) ♯ if and only if they are d-separated by U .]()

![]()

![Figure 15.1: Example of causal graph.]()

![The beta distribution with parameters a and b (abbreviated β(a, b)) has density with respect to Lebesgue's measure]()

![The log-likelihood, ℓ, is a concave function of θ, with ∇ℓ(θ) = E θ (U ) -ŪN(17.6) and∇ 2 ℓ(θ) = -Var θ (U ) (17.7)]()

![Incomplete observations for graphical models 17.3.1 The EM Algorithm]()

![Normalizing flows 18.1.1 General concepts]()

![Figure 18.1: Basic structure of GANs: W is optimized to improve the prediction problem: "real data" vs. "simulation". Given W , θ is optimized to worsen the prediction.]()

![Figure 19.1: A partition tree of the set {a, b, c, d, e, f }.]()

![]()

![Algorithm 19.4 (K-means) Let T ⊂ R d be the training set. Start with an initial choice of c 1 , . . . , c p ∈ R d and iterate over the following two steps until stabilization:]()

![z 1 (A), . . . , z N (A) denote the columns of Z(A) and zk (A) = z k (A)/|z k (A)|. One has |z k (A) -zl (A)| = 0 if k and l belong to the same cluster and √ 2 otherwise.]()

![subject to 0 ≤ ξ k ≤ 1 and N j=1 ξ j = p. The optimal solution is obtained by takingξ 1 = • • • = ξ p = 1, since, for any other solution λ j )(1ξ j ) ≥ 0.]()

![Run K-means on (y 1 , . . . , y N ) to determine a partition.]()

![Spectral clustering: version 2)Let S α be an N × N discrepancy matrix. Let p denote the number of clusters. LetP = Id R N -1 N 1 T N /N .(1) Compute Sα = P S α P Compute the eigenvectors of Sα associated with the p -1 smallest eigenvalues.]()

![Denoting these eigenvectors by e 1 , . . . , e p-1 , define y 1 , . . . , y N ∈ R p-1 by y Run K-means on (y 1 , . . . , y N ) to determine a partition.]()

![3), L * (p) = min{W α (A 1 , . . . , A p , c 1 , . . . , c p ) : A 1 , . . . , A p partition of T , c 1 , . . . , c p ∈ R},]()

![Figure 19.2: Example of data transformed using the eigenvectors of the graph Laplacian. Left: Original data. Center: Result of a Kmeans algorithm with three clusters applied to the transformed data (2D projection). Right: Visualization of the cluster labels on the original data.]()

![Figure 19.3: Elbow graphs for K-means clustering for two populations generated as mixtures of Gaussian.]()

![Let a α (x, p) = d α (x, A(x)) and b(x, p) = min{d α (x, A k ) : A k A(x)}. Define the silhouette index of x in the segmentation [170]by s α (x, p) = b α (x, p)a α (x, p) max(b α (x, p), a α (x, p)) ∈ [-1, 1].]()

![Figure 19.4: Division of the unit square into clusters for uniformly distributed data.]()

![L * (p -1)p L * (p) -(p + 1) L * (p + 1), and estimate the number of clusters by taking p maximizing γ KL .]()

![Figures figs.19.5 to 19.7 provide a comparative illustration of some of these indexes.]()

![Figure 19.5: Comparison of cluster indices for Gaussian clusters. First row: original data and ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali ński and Harabasz; silhouette; Sugar and James)]()

![Figure 19.7: Comparison of cluster indices for Gaussian clusters. First row: original data and ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali ński and Harabasz; silhouette; Sugar and James).]()

![and θ given X with respect to θ, formally4 , P (z|x) = P (z, θ|x)P (θ)dθ ∝ N k=1 P (x k |z k , θ)P (z k |θ)P (θ)dθ.]()

![11 (Mean-field algorithm for mixtures of Gaussian) (1) : Input: training set (x 1 , . . . , x N ), number of clusters p, prior parameters u, v, τ 2 and a . (2) Initialize variables σ 2 1 , . . . , σ 2 p , m1 , . . . , mp , ã1 , . . . , ãp , gk (j), k = 1, . . . , N , j = 1, . . . , p. (3) Let ζ(j) = N k=1 gk (j), j = 1, . . . , p. (4) Let ρ2]()

![For j = 1, . . . , p,Let ãi = a + ζ(j), j = 1, . . . , p. For k = 1, . . . , N , j = 1, . . . , p, let gk (j) ∝ exp -1 2 ρ2 |x k -mj | 2 + d σ 2 j + ψ( ãj ) .]()

![1, . . . , j j + 1 with probability λ λ + k (19.13) 3 If z k+1 = i ≤ j, then replace N i by N i + 1, k by k + 1. 4 If z k+1 = j + 1, let N j+1 = 1, replace j by j + 1 and k by k + 1. 5 If k < N , return to step 2, otherwise, stop.]()

![generate only m -1 such pairs of variables and let the last one be equal to c A k .]()

![Simulate a new value of σ 2 according to an inverse gamma distribution with parameters u + dN /2 and v + 1k=1 |x kc A k | 2 .(4) Simulate new values for c A , A ∈ A independently, sampling c A according to a Gaussian distribution with mean (1 + σ 2 /(N j τ 2 )) -1 xA and variance (|A|/σ 2 + 1/τ 2 )]()

![the model is parametrized by the coordinates of e 1 , . . . , e p and the values of λ 1 , . . . , λ p . Introduce the d × p matrix W = [λ 1 e 1 , . . . , λ p e p ].]()

![Figure 20.1: PCA cannot distinguish between the situations depicted in the two datasets.]()

![p n = 1 (to avoid trivial solutions). Choosing an ordering on the set of indices (p 1 , . . . , p d ) such that p 1 + • • • p d = n, one can stack the coefficients in Q and the monomials (x (1) k ) p 1 . . . (x (d) k ) p d to form two vectors denoted Q (with some abuse of notation) and V (x k ). One can then rewrite the problem of determining Q as minimizing Q T ΣQ subject to |Q| 2 = 1, where]()

![τ (A) = U S τ (∆)V T when A = U ∆V T is a singular value decomposition of A.Proposition 20.8 Let us assume without loss of generality that N ≥ d. The function Z → γ|X c -Z| 2 + |Z| * is minimized by Z = S 1/2γ (X ).]()

![SAEM for probabilistic ICA)Initialize the algorithm with parameters A, σ 2 . Define a sequence of decreasing steps, γ t .Let, for k = 1, . . . , N , b k = 0 and S k = Id. Iterate the following steps.]()

![Replace A by Ã and Y by Ỹ , iterating until numerical convergence.]()

![NMF, logarithmic cost) 1. Fix p > 0 and let X be the N by d matrix containing the observed data.Initialize the procedure with matrices Y and A, respectively of size N by p and d by p, with positive coefficients.3. At a given stage of the algorithm, let A and Y be the current matrices decomposing X .4. Let Ã be the matrix with coefficients ã]()

![Instead of considering configurations b = (b k (j), i = 1, . . . , d, k = 1, . . . , N ) we may alternatively consider the family of sets S = (G k , C k , 1 ≤ k ≤ N ). Such a family must satisfy the property that the sets G k and C k are non-intersecting, C k ⊂ U k and G l ∩ G k = ∅ for l < k. It provide a unique configuration b by letting b k (j) = 1 if and only if j ∈ G k ∪ C k . We will let, in the following, q k = |G k | and p k = |U k |. The probability Q(b) can be re-expressed in terms of S, letting (with some abuse of notation)]()

![Indian buffet process) 1. Initialization:]()

![+ c) -1 ds = -c log(1 + t c]()

![T j Y = 0 for j = 1, . . . , m. Introduce the matrix L = L + m k=1 e k e T k which is positive definite. Our minimization problem is then equivalent to minimizing 2trace(Y T LY ) -2trace(U T Y ), subject to e T j Y = 0 for j = 1, . . . , m. The derivative of this function is 4 LY -2U so that an optimal Y must satisfy 4 LY -2U + Lagrange multipliers µ 1 , . . . , µ m ∈ R p . This shows that]()

![exit and return Y ′ .6. Return to step 3.]()

![Figure 21.1: Left: Multidimensional scaling applied to a 3D curve embedded in a 10dimensional space retrieves the Euclidean structure. Right: Isomap, in contrasts, identifies the one-dimensional nature of the data.]()

![kl = |x kx l | if k ∼ c l and d (1) kl = +∞ otherwise (and also let d (1) kk = 0). Then, given d (n-1) , define]()

![Figure 21.2: Left: Multidimensional scaling applied to a 3D curve embedded in a 10dimensional space retrieves the Euclidean structure. Right: Isomap, in contrasts, identifies the one-dimensional nature of the data.]()

![Local linear embedding)The input of the algorithm is (i) Either a training set T = (x 1 , . . . , x N ), or its Gram matrix S containing all inner products x T k x l (or more generally inner products in feature space), or a dissimilarity matrix D = (d kl ).(ii) An integer c for the graph construction.(iii) An integer p for the target dimension.If not provided in input, compute the Gram matrix S and distance matrix D (using (21.2) and (21.4)).]()

![Build the c-nearest-neighbor graph associated with the distances. Let N k be the set of neighbors of k, with c k = |N k |.]()

![∈ N k stacked in a vector ρ k ∈ R c k by solving(21.6).(4) Form the matrix W with entriesw ll ′ = N k=1 ρ (l) k ρ (l ′ )k with ρ extended so that ρ(k) k = -1 and ρ (l) k = 0 if l k and l N k .]()

![Figure 21.3: Local linear embedding with target dimension 3 applied to the data in fig. 21.1 and fig. 21.2.]()

![kand N k=1 ρ k = 0. With this notation, we can interpret Laplacian embedding as the minimization of subject to previous orthogonality constraints). In other terms, y 1 , . . . , y N are determined so that the linear relationships ρ kk y k =are satisfied, which is similar to the LLE condition, without the requirement that ρ k (k) = 1.]()

![∂ t π k (l) = π k (l)∂ t log π k (l), we have ∂ t H(π k ) = N l=1 (d kl log π k (l))π k (l) -dk N l=1 π k (l) log π k (l).]()

![ϕ a,b (y, y ′ ) = 1 1 + a|yy ′ | b .The parameters a and b are adjusted so that ϕ a,b provides a differentiable approximation of the functionψ ρ 0 (y, y ′ ) = exp(-max(0, |yy ′ |ρ 0 ))]()

![The representation y is optimized by minimizing the "fuzzy set cross-entropy"C(µ∥ν(•, y)) = (k,l)∈E µ(k, l) log µ(k, l) ν(k, l|y) + (1µ(k, l)) log 1µ(k, l) 1ν(k, l|y)or, equivalently, maximizing (using, for short, ϕ = ϕ a,b )F(y) = (k,l)∈E (µ(k, l) log ν(k, l|y) + (1µ(k, l)) log(1ν(k, l|y))) = (k,l)∈E(µ(k, l) log ϕ(y k , y l ) + (1µ(k, l)) log(1ϕ(y k , y l )))]()

![∂ y k F(y) =2 N l=1 µ(k, l)∂ y k log ϕ(y k , y l ) + 2 N l=1 (1µ(k, l))∂ y k log(1ϕ(y k , y l )) =2 N l=1 µ(k, l)∂ y k log ϕ(y k , y l ) 1ϕ(y k , y l ) + 2 N l=1∂ y k log(1ϕ(y k , y l )).]()

![H k (y, ξ, ξ ′ ) = 2 N l=1 ξ kl ∂ y k log ϕ(y k , y l ) 1ϕ(y k , y l ) ξ kl ξ ′ kl ′ ∂ y k log(1ϕ(y k , y l ′ )).Then, if one takes c k = 1/(ϵ l µ(k, l)) one hasE(H k (y, ξ, ξ ′ )) = ∂ y k F(y).]()

![y k log ϕ(y k , y l ) 1ϕ(y k , y l ) + c k m j=1 ∂ y k log(1ϕ(y k , y l ′ ))]()

![r(Y , f (X)))where r : R Y × R Y → [0. + ∞) is a risk function.A training set is a family T = ((x 1 , y 1 ), . . . , (x N , y N )) ∈ (R X × R Y ) N , the set T of all possible training sets therefore being the set of all finite sequences in R X × R Y . A training algorithm can then be seen as a function A : T → F which associates to each training set T a function A(T ) = fT .Given T ∈ T , The training set error associated to a functionf ∈ F is RT (f ) = 1 |T | (x,y)∈T r(y, f (x)))and the in-sample error associated to a learning algorithm is the function T → E T ∆ = RT ( fT ). Fixing the size (N ) of T , one also considers the random variable T with values in T distributed as an N -sample of the distribution of (X, Y ).]()

![N (θ T ŪT -C(θ)) ϕ j dm j(θ) where ŪT = (U (z 1 ) +• • • + U (z N ))/N .Consider the maximum likelihood estimator θj within M j , maximizing ℓ(θ, ŪT ) = θT ŪT -C(θ) over M j . Then one hasℓ(θ, ŪT ) = ℓ( θj , ŪT ) + 1 2 (θ -θj ) T ∂ 2 θ ℓ( θj , ŪT )(θ -θj ) + R j (θ, θj )|θ -θj | 3]()

![P ) = -x∈Ω p x log 2 p x .]()

![(x, y | θj , M j ) =max θ]()

![(y k | x k ; θ, M j )log 2 π(θ | M j ) -k j i=1 log 2 (δ ij ).Let θ(j) be the parameter that maximizesL(θ | M j ) = N k=1 log 2 ϕ(y k | x k ; θ, M j ) + log 2 π(θ | M j )]()

![c i (N | M j )]()

![random variables are such that M(λ) < ∞ for all λ ∈ R. Indeed, for(|X| > λ -1 log z)dz]()

![(λX > g + (z))dz + ∞ (λX < g -(z))dzThe change of variable u = g + (z) in the first integral is equivalent to u > 0, ϕ(u) = z with dz = (e u -1)du. Similarly, u = -g -(z) in the second integral gives u > 0, ϕ(-u) = z and dz = (1e -u )du so thatE(ϕ(λX)) = ∞ (λX > u)(e u -1)du + ∞ (λX < -u)(1e -u )du ≤ ∞ (λ|X| > u)(e ue -u )du.(Using the fact that max(P(λX > u), P(λX < -u)) ≤ P(λ|X| > u).) We have∞ (λ|X| > u)(e ue -u )du ≤ 2 +∞ 0 (e ue -u )eve -λσ v )e -v 2 2π(Φ(-σ λ) -Φ(σ λ))]()

![Let m = E(X) and assume that for some constant b, one has X ≤ b with probability one. Then, for any σ 2 > 0 such that var(X) ≤ σ 2 , one hasE(e λX ) ≤ e λm (bm) 2 (bm) 2 + σ 2 e -λσ 2 (b-m) + σ 2 (bm) 2 + σ 2 e λ(b-m) (22.6)for any λ ≥ 0.Proof There is no loss of generality in assuming that m = 0 and λ = 1, in which case one must show thatE(e X ) ≤ b 2 b 2 + σ 2 e -σ 2 b + σ 2 b 2 + σ 2 e b(22.7)]()

![If F(λ) denotes the right-hand side of(22.6), we have, for m ≤ u < b,M * X (t) ≥ sup λ≥0 (λulog F(λ))and we now estimate this lower bound. Maximizing λylog F(λ) is equivalent to minimizingλ → (bm) 2 e -λ(σ 2 +(u-m)) b-m + σ 2 e λ(b-u) (bm) 2 + σ Introduce the notation ρ = σ 2 /(bm) 2 , µ = λ(bm) and x = (um)/(bm), so that the function to minimize is µ →e -µ(ρ+x) + ρe µ(1-x) 1 + ρ .]()

![5. ThenP( XN> m + t) ≤ exp -N ρ + x 1 + ρ log ρ + x ρ + 1x 1 + ρ log(1x) (22.8) with x = t/(bm) and ρ = σ 2 /(bm) 2 .]()

![.7) implies log E(e X ) ≤ b 2 b 2 + σ 2 e -σ 2 b + σ 2 b 2 + σ 2 e bσ 2 (e bb -1).]()

![σ 2 (e bb -1) = b 2 b 2 + σ 2 σ 4 b 2 ϕ(-σ 2 /b) + σ 2 b 2 + σ 2 b 2 ϕ(b) ≤ σ 4 b 2 + σ 2 + σ 2 b 2 b 2 + σ 2 ϕ(b) = σ 2 b 2 (e bb -1)This shows that log E(e λX ) ≤σ 2 b 2 (e λbλbt/σ 2e λb + λb + 1) = σ 2 b 2 h(bt/σ 2 ) where h(u) = (1 + u) log(1 + u)u.]()

![Assume that X satisfy the conditions of proposition 22.5. Then, for t > 0,P( XN > m + t) ≤ exp -N σ 2 (bm) 2 h (bm)t σ 2 (22.9)where h(u) = (1 + u) log(1 + u)u.]()

![2σ 2 + 2t(bm)/3 . (22.10)]()

![10 Let X be a random variable taking values in the interval [a, b]. Let m = E(X). Then E(e λX ) ≤ bm ba e λa + ma ba e λb ≤ e λm e λ 2 (b-a) 2for all λ ∈ R.Proof We first note that, if X takes values in [a, b], then var(X) ≤ (b -m)(m-a) (using σ 2 = (bm)(ma) in (22.6)). To prove the upper bound on the variance, introduce the functiong(x) = (xa)(xb) so that g(x) ≤ 0 on [a, b]. Noting that one can write g(x) = (xm) 2 + (2mab)(xm) + (am)(bm), we have E(g(X)) = var(X) -(bm)(ma) ≤ 0,which proves the inequality.]()

![additional work. Letting u = (ma)/(ba), α = λ(ba) and taking logarithms, we need to prove that log(1u + ue α )uα ≤ α 2 8]()

![Hoeffding Inequality) If X 1 , . . . , X N are independent, taking values, respectively, in intervals of length, c 1 , . . . , c N andY = X 1 + • • • + X N , then P(Y > E(Y ) + t) ≤ exp -We have, by proposition 22.10, for any λ > 0P(Y > E(Y ) + t) ≤ e λt-N k=1 M X k (λ) ≤ e -(λt-λ |c| 2 )]()

![12 Let X 1 , . . . , X N , Z 1 , . . . , Z N be two sequences ofN random variables such that E(Z k | X 1 , Z 1 , . . . , X k-1 , Z k-1 ) = m k is constant and |Z km k | ≤ c k for some constants c 1 , . . . , c N . Then P(Y > E(Y ) + t) ≤ e -2t 2 /|c| 2 with Y = Z 1 + • • • + Z N and |c| 2 = N k=1 c 2 k .Proof Proposition 22.10 applied to the conditional distribution implies that, for λ ≥ 0:log E(e λ(Z k -m k ) | X 1 , Z 1 , . . . , X k-1 , Z k-1 ) ≤ log E(e λ|Z k -m k | | X 1 , Z 1 . . . , X k-1 , Z k-1 ) ≤ Let S k = k j=1 (Z jm j ). Then E(e λS k ) = E(e λS k-1 E(e λ(Z k -m k ) | X 1 , Z 1 , . . . , X k-1 , Z k-1 )) ≤ eand the result follows from Markov's inequality optimized over λ.]()

![A is a finite subset of R, we let F (A) denote the set {f | A : f ∈ F } of restrictions of elements of F to the set A. As a convention, we let F (∅) = {f ∅ }, containing the socalled empty function. Since F only contains binary functions, we have |F (A)| ≤ 2 |A| . If x 1 , . . . , x M ∈ R, we let, with a slight abuse of notation,F (x 1 , . . . , x M ) = F (A)where A = {x i , i = 1, . . . , M}. This provides the number of possible splits of a training set T = (x 1 , . . . , x M ) using classifiers in F . Fixing in this section a random variable X, we letS F (M) = E(|F (X 1 , . . . , X M )|)where the expectation is taken over all M i.i.d. realizations from X. We also let S * F (M) = max{|F (A)| : A ⊂ R, |A| ≤ M}.]()

![(E T ′ (f ) -E T (f )) = sup f ∈F N k=1 (r(Y k , f (X k ))r(Y ′ k , f (X ′ k )))/Nhas the same distribution as supf ∈F N k=1 ξ k (r(Y k , f (X k ))r(Y ′ k , f (X ′ k )))/N .]()

![r(Y k , f (X k ))r(Y ′ k , f (X ′ k )))/N ≥ t/2 T , T ′ ≤ e -2N (t/2) 2 /4 = e -N t 2 /8and taking expectation over T and T ′ yieldsP F (2N )e -Nt 2 /8 . Equation (22.20) is then obtained from letting δ = 2S F (2N )e -N t 2 /8 so that t = log 2S F (2N ) δ]()

![20 (Sauer-Shelah's lemma) If D is the VC-dimension of F , then, for N ≥ D, the statement of the proposition derives from the standard upper bound We can therefore state a corollary to theorem 22.16 for model classes with finite VC-dimension. Corollary 22.21 Assume that VC-dim(F ) = D < ∞. Then, for t ≥ √ 2/N and N ≥ D, f ) -E T (f )) ≤ 8 N D log eN D + log 2 δ ≥ 1δ. (22.23)]()

![22 Let R = R d and F = x → sign(a 0 + b T x) :β 0 ∈ R, b ∈ R d . Then VC-dim(F ) = d + 1.ProofLet us show that no set of d +2 points can be shattered by F . Use the notation x = (1, x T ) T and β = (a 0 , b T ) T , and consider d + 2 points x 1 , . . . , x d+2 . Then x1 , . . . , xd+2 are linearly dependent and one of them, say, xd+2 can be expressed as a linear combination of the others. Write xd+2 = d+1 k=1 α k xk .]()

![-dim(F (L, (U i ), (W i ), p)) = O( LW L log(pU )).whereU = U 1 + • • • + U L andNote that p = 2 for ReLU networks. Theorem 7 in Bartlett et al.[21] also provides a more explicit upper bound, namelyVC-dim(F (L, (U i ), (W ), p)) ≤ L + LW L log 2]()

![ψ k ) -(N -1)H 2 (ψ) ≥ 0. (22.25)]()

![Y (k) ) -(N -1)H 2 (Y ) ≥ 0.]()

![g∈G g(Z) ≥ t) ≤ P(sup g∈G g(Z) ≥ tϵ) (22.28) ≤ N (G, ρ ∞ , ϵ) sup g∈G P(g(Z) ≥ tϵ). Now, if each function in G satisfies a concentration inequality, say, P(g(Z) ≥ u) ≤ e]()

![g∈G g(Z) ≥ t) ≤ N (G, ρ ∞ , ϵ) e -(t-ϵ) 2 2µ(G) .]()

![For all ϵ > 0, one hasP sup f ∈F (R(f ) -E T (f )) ≥ t ≤ 2 N (F , ϵ/2, N )e -N (t/2-ϵ) 2(22.29)]()

![Applying Hoeffding's inequality, we have, for u > 0 and using the fact thatc k ∈ [-1, 1] P(g f (Z) > u | T , T ′ ) ≤ e -2N u 2the discussion preceding the theorem yields the fact that, for any ϵ > 0:P(supf ∈F g f (Z) > t/2 | T , T ′ ) ≤ N (G, ϵ, ρ ∞ )e -N (t/2-ϵ) 2]()

![f ) -E T (f )) ≥ t ≤ 2 N (F , ϵ/2, N )e -N (t/2-ϵ) 2which is valid for all ϵ > 0.■ One can retrieve the bound obtained in theorem 22.16 using the obvious fact thatN (F , ϵ, ρ A ) ≤ |F (A)|, for any A ⊂ R, so that P sup f ∈F (R(f ) -E T (f )) ≥ t ≤ 2S(F , 2N )e -N (t/2-ϵ) 2 2for any ϵ > 0, and letting ϵ go to zero,P sup f ∈F (R(f ) -E T (f )) ≥ t ≤ 2S(F , 2N )e -N t 2 8 .]()

![Assume that G is a parametric family of functions, so that G = {g θ , θ ∈ Θ} where Θ ⊂ R m . Assume also that, for some constantC, ρ ∞ (g θ , g θ ′ ) ≤ C|θ -θ ′ | for all θ, θ ′ ∈ Θ. Let G (M) = {g θ : θ ∈ Θ, |θ| ≤ M}. Then N (G, ρ ∞ , ϵ) ≤ 1 + 2CM ϵ mProof Letting ρ denote the Euclidean distance in R m , our hypotheses imply thatN (G (M) , ρ ∞ , ϵ) is bounded by N (B M , ρ, ϵ/C) where B M is the ball with radius M in R m . Now, if θ 1 , . . . , θ n is an α-covering of B M , then θ 1 /M, . . . , θ n /M is an (α/M)covering of B 1, which shows (together with a symmetric argument) that N (B M , ρ, α) = N (B 1 , ρ, α/M) and we getN (G (M) , ρ ∞ , ϵ) ≤ N (B 1 , ρ, ϵ/MC)and we only need to evaluate N (B 1 , ρ, α) for α > 0. Using proposition 22.27, one can instead evaluate M(B 1 , ρ, α). So let A be an α-net in B 1 . Then x∈A B ρ (x, α/2) ⊂ B ρ (0, 1 + α/2) and, since the sets in the union are disjoint, x∈A volume(B ρ (x, α/2)) = |A|volume(B ρ (0, α/2)) ≤ volume(B ρ (0, 1 + α/2)) .Letting C m denote the volume of the unit ball in R m , this shows]()

![30 Let Z be a bounded convex subset of R d with non-empty interior. Forp ≥ 1 and f ∈ C p (Z), let ∥f ∥ p,∞ = max |D k (f (x)| : k = 0, . . . , p, x ∈ Z .]()

![)g 0 (Z) > tS n ) ≤ P(∃g ∈ G n , ∃k ≤ n : π k (g)(Z)π k-1 (g)(Z) > tt k ρ(π k (g), π k-1 (g))) ≤ P(∃k ≤ n, ∃g ∈ G k , g ′ ∈ G k-1 : g(Z)g ′ (Z) > tt k ρ(g, g ′ )) g∈G k ,g ′ ∈G k-(g(Z)g ′ (Z) > tt k ρ(g, g ′ )) If one takes N k = 2 2 k , which satisfies N k N k-1 = 2 2 k +2 k-1 ≤ N k+1, and t k = 2 k/2 , one finds that P(sup g∈G n g(Z)g 0 (Z) > tS n ) ≤ 2 n k=1]()

![2 ρ(π k (g), π k-1 (g)) 2 (ρ(g, G k ) + ρ(g, G k-1 )) 2 ρ(g, G k )]()

![have proved the following result[188].Theorem 22.31 Let G 0 , G 1 , . . . be a family of subsets of G satisfying(22.38) and(22.39) and such that G 0 = {g 0 } and|G n | ≤ 2 2 n for n ≥ 0. Let S = 2 sup g∈G ∞ n=0 2 n/2 ρ(g, G n ) (22.40)Then, for t > S 1 + 8 log 2,)g 0 (Z) > t) ≤ Ce -t with C = 2 ∞ k=1 e -2 k-2 .]()

![If S is given by(22.40), we haveS = 2 sup ∞ n=0 2 n/2 ρ(g, G n ) : g ∈ G ≤ 2 ∞ n=0 2 n/2 sup{ρ(g, G n ) : g ∈ G}Take G n achieving the minimum in the entropy number e(G, ρ, 2 2 n ). Then,(22.41) holds with S replaced by G, ρ, ϵ)dϵ,(22.42) which is known as Dudley's metric entropy of the space (G, ρ). We haveh(G, ρ) = N (ϵ)dϵ. If ϵ ∈ [e(22 n-1 ), e(2 2 n )), we have N (ϵ) > 2 2 n so that h(G, ρ) ≥ e(2) log 3 + ∞ n=1 2 n/2 (e(2 2 n )e(2 2 n-1]()

![π λ (y) =e λy e -λ + e λ . Now, if F is a class of real-valued functions, we can define the risk functionr(y, f (x)) = log 1 π f (x) (y).Since |∂ λ log π λ (y)| = |ytanh λ| ≤ 2 for y ∈ {-1, 1}, we have|r(y, f (x))r(y, f ′ (x))| ≤ 2|f (x)f ′ (x)|]()

![∞ (r(y, f (x)), r(y, f ′ (x))) ≤ |a 0a ′ 0 | + U |bb ′ |and a small modification of the proof of proposition 22.29 shows thatN (F , ρ ∞ , ϵ) ≤ 1 + 4CU ϵwhich is proved exactly the same way as(22.21) in theorem 22.16, and we skip the argument.]()

![,T ′ (ξ 1 , . . . , ξ N ) > t 2 ≤ |F|e -N t 2 /8 , ■ which concludes the proof, since, by proposition 22.27, |F| ≤ N ∞ (γ/2, 2N ).]()

![35 Let F be a family of functions f : R → [-1, 1] and A a finite subset of R.]()

![Let γ > 0 and assume that F has P γ/4 -dimension D < ∞. Then,N ∞ (γ, N ) ≤ 2 16N γ log(4eN /(Dγ))]()

![]()

![The next step uses a discretization scheme to extend the previous result to functions with values in [-1, 1]. More precisely, given f : R → [0, 1], and η > 0, letf η (x) = max{k ∈ N : 2kη -1 ≤ f (x)} which takes values in {0, . . . , r} for r = ⌊η -1 ⌋. If F is a class of functions with values in [-1, 1], define F η = {f η : f ∈ F }.With this notation, the following holds.]()

![(a) For all γ ≤ η:S-dim(F η ) ≤ P γ -dim(F ) (b) For all ϵ ≥ 4η and A ⊂ R: M(F (A), ρ ∞ , ϵ) ≤ M ∞ (F η (A), ρ ∞ , 2).]()

![(b), we deduce from the definition of f η that |f η (x) -f η (x)| > (2η) -1 |f (x)f (x)| -1 so that, if ϵ = 4η, |f (x) -f (x)| ≥ ϵ implies |f η (x) -f η (x)| > 1, or, equivalently |f η (x) -f η (x)| ≥ 2.We can now conclude. Taking γ > 0, we have, if|A| = N N (F (A), ρ ∞ , γ) ≤ M(F (A), ρ ∞ , γ) ≤ M(F γ/4 (A), ρ ∞ , 2) Since the maximum of N (F (A), ρ ∞ , γ) over A with cardinality N is N ∞ (γ, N ), the proof is complete.■ One can use this result to evaluate margin bounds on linear classifiers with bounded data. Let R be the ball with radius Λ in R d and consider the model class containing all functions f (x) = a 0 + b T x with a 0 ∈ [-Λ, Λ] and b ∈ R d , |b| ≤ 1. Let A = {x 1 , . . . , x N } be a finite subset of R. Then, F P γ -shatters A if and only if there exists g 1 , . . . , g N ∈ R such that, for any sequences ξ = (ξ 1 , . . . , ξ N ) ∈ {-1, 1} N , there exists a ξ 0 ∈ [-Λ, Λ] and b ξ ∈ R d , |b ξ | ≤ 1 with ξ k (a ξ 0 + (b ξ ) T x kg k ) ≥ γ for k = 1, . . . , N . Summing over N , we find that This shows that, for any sequence ξ 1 , . . . , ξ N , Applying the same inequality after changing the signs of ξ 1 , . . . , ξ N yields N γ ≤ N γ +]()

![f ∈F (E T 1 (f ) -E T 2 (f ))One can then use McDiarmid's inequality (theorem 22.13) after noticing that, lettingz k = (x k , y k ) for k = 1, . . . , N , max z 1 ,...,z N ,z ′ k Φ(z 1 , . . . , z N ) -Φ(z 1 , . . . , z k-1 , z ′ k , z k+1 , . . . , z N ) f ) -E T (f )) ≥ C T + ϵ) ≤ e -2N ϵ 2Rademacher complexityWe now extend the previous definition by computing discrepancies over random two-set partitions of the training set, which have equal size in average. This leads to the empirical Rademacher complexity of the function class. Let ξ 1 , . . . , ξ N be a sequence of Rademacher random variables (equal to -1 and +1 with equal probability 1/2). Then, the (empirical) Rademacher complexity of the training set T for the model class F is rad(T ) = E sup]()

![40 Let F be a function class such that D = VC-dim(F ) < ∞. Then rad(T ) ≤ 3 √ N 2D log(eN /D) .Proof One has, using Hoeffding's inequalityP sup f ∈F 1 N N k=1 ξ k r(y k , f k ) > t ≤ |F (T )| sup f ∈F P 1 N N k=1 ξ k r(y k , f k ) > t ≤ |F (T )|e -N t 2 /2 .]()

![Assume that the loss function r takes its values in [0, 1]. Recall that KL(µ∥π) is the Kullback-Leibler divergence from µ to π, defined byKL(µ∥π) = F log(ϕ(f ))ϕ(f )dπ(f )if µ has a density ϕ with respect to π and +∞ otherwise. Then, the following theorem holds.Theorem 22.43 (McAllester[128]) With the notation above, for any fixed probability distribution π ∈ Q,P sup µ∈Q ( R(µ) -ĒT (µ)) > t + KL(µ∥π) 2N ≤ 2N e -Nt . (22.46) Taking t = log(2N /δ)/2N , the theorem is equivalent to the statement that, with probability 1δ, one has R(µ) -ĒT (µ) ≤ log 2N /δ + KL(µ∥π) 2N . (22.47)Proof We first show that, for any probability distributions π, µ on F , and any function H on F ,F H(f )dµlog F e H(f ) dπ ≤ KL(µ∥π) .Indeed, assume that µ has a density ϕ with respect to π (otherwise the upper bound is infinite) and letKL(µ∥ϕ H π) ≥ 0,which proves the result (and also shows that one can only have equality when ϕ = ϕ H π-almost surely.)Let χ(u) = max(u, 0) 2 . We can use this inequality to show that, for any probabilityQ ∈ Q and λ > 0, λχ( R(µ) -ĒT (µ)) ≤ λ F χ(R(f ) -E T (f ))dµ(f ) ≤ KL(µ∥π) + log F e λχ(R(f )-E T (f )) dπwhere we have applied Jensen's inequality to the convex function χ. This yields e λχ( R(µ)-ĒT (Q)) ≤ eKL(µ∥π) F e λχ(R(f )-E T (f )) dπ.Hoeffding's inequality implies that, for all f ∈ F and t ≥ 0P(χ(R(f ) -E T (f )) > t) = P(R(f ) -E T (f ) > √ t) ≤ e -2N tso thatE e λχ(R(f )-E T (f )) = ∞ (λχ(R(f ) -E T (f )) > log t)dtFrom this and Markov's inequality, we get, for any λ > 0:P(sup µ∈Q χ( R(µ) -ĒT (µ)) > t + KL(µ∥π)/λ) ≤ e -λt 1 + λ e λ-2N -1 λ -2N . Taking λ = 2N yields P(sup µ∈Q χ( R(µ) -ĒT (µ)) > t + KL(µ∥π)/2N ) ≤ 2N e -2N t ,which impliesP sup µ∈Q R(µ) -ĒT (µ) > t + KL(µ∥π)/2N ≤ 2N e -2N t ,concluding the proof.■Remark 22.44 Note that the proof, which follows that given in Audibert and Bousquet[15], provides a family of inequalities obtained by taking λ = 2N /c in the final step, with c > 1. In this case1 + λ e λ-2N -1 λ -2N ≤ ) -ĒT (µ) > t + cKL(µ∥π)/2N ≤ c c -1 2N e -2N t . ♦ Remark 22.45 One special case of theorem 22.43 is when π is a discrete probability measure supported by a subset F 0 of F and µ corresponds to a deterministic predictor optimized over F 0 , and is therefore a Dirac measure on some element f ∈ F 0 .Because δ f has density ϕ(g) = 1/π(g) if g = f and 0 otherwise with respect to π, we have KL(δ f ∥π) =log π(f ) and theorem 22.43 implies that, with probability larger than 1δ, R(f ) -E T (f ) ≤ log 2N /δlog π(f ) 2N .]()

![-based method that requires the minimization ofẼT (f ) = (E T (f ) -E T ( f]()

![]()

![]()

Unless mentioned otherwise, all matrices are assumed to be real.

Upper-semi continuous is sufficient.

∝ is the notation for "proportional to"

⌈x⌉ is the smallest integer larger than x (ceiling).

All vector spaces in these notes will be real, and will therefore only be referred as vector spaces.

Note that we are using double bars for the norm in H, which, in most applications, is infinite dimensional

The convolution between two absolutely integrable functions f and g is defined byf * g(u) = R d f (z)g(uz) dz

This part of the proof uses some measure theory.

Their computation is feasible unless N is very large, and the matrix inversion in Newton's iteration also requires d to be not too large.

In this section only, the notation x does not refer to (1, x T ) T .

The operator A + ρId H is invertible as soon as A is symmetric positive semi-definite.

Note that, even if the training data is linearly separable, there are generally samples that are on the right side of the hyperplane, but at a distance to the hyperplane strictly lower that the "nominal margin" C = 1/|b|. This is due to our relaxation of the original problem of finding a separating hyperplane with maximal margin.

If f and g are representable as trees, f + g can be represented as a tree whose depth is the sum as those of the original trees, simply by inserting copies of g below each leaf of f .

For every z and every ϵ > 0, there exists z ′ < z such that for allz ′′ ∈ [z ′ , z), |F(z ′′ ) -F(z -0)| < ϵ.Moreover, taking any y ∈ (z ′ , z), there exists y ′ < y such that for all y ′′ ∈ [y ′ , y), |F(y ′′ ) -F(y -0)| < ϵ.

We will assume in this chapter that B is a complete metric space with a dense countable subset, with the associated Borel σ -algebra.

The "almost everywhere" statement a priori depends on A, but can be made independent of it under the mild assumption (that we will always make) that B has a countable basis of open sets.

In the general case, R X , R Y , . . . are metric spaces with a countable dense subset with σ -algebras S X , S Y , . . .

|u-√ hf (x-√ hu)| 2 dy,

We still call the method K-medoids rather than p-medoids, to keep the name universally used in the literature.

(x-c z ) T Σ -1 z (x-c z )

Recall that Z ⪰ 0 means that Z is positive definite, while Z ≥ 0 indicates that all its entries are non-negative.

Two nodes x and y are connected in the graph if there is a sequence z 0 , . . . , z n in T such that z 0 = x, z n = y and β(z i , z i-1 ) > 0 for i = 1, . . . , n. This provides an equivalence relation and equivalent classes are called connected components.

The symbol ∝ means "equal up to a multiplicative constant".

A Hilbert space is an inner-product space which is complete for its norm. A separable Hilbert space must have a dense countable subset, which, in particular, implies that it has orthonormal bases.

