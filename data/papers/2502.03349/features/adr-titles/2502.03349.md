- Decision to utilize self-play as the primary training strategy
- Choice of GIGAFLOW as the simulation framework
- Design of the parameterized policy architecture
- Implementation of a minimalistic reward function
- Decision to train without human data
- Selection of urban environments for simulation
- Use of a single neural network for multiple traffic participant types
- Adoption of Proximal Policy Optimization (PPO) for training
- Implementation of advantage filtering in experience replay
- Decision to simulate diverse agent behaviors through conditioning
- Choice of evaluation benchmarks (CARLA, nuPlan, Waymo Open Motion Dataset)
- Design of the GIGAFLOW world with random perturbations
- Decision to model uncertainty in driving scenarios
- Use of batched simulation for high throughput
- Decision to optimize for cost-effectiveness in training
- Implementation of a compact policy architecture for fast inference
- Decision to allow for zero-shot evaluation across benchmarks
- Choice of training duration and scale (1.6 billion km driven)
- Decision to randomize reward weights for diverse agent behaviors
- Design of the training environment with variable waypoint navigation