- Decision to use Sparse Autoencoders (SAEs) for interpreting neuron activations
- Choice of using a higher-dimensional latent space for interpretability
- Selection of large language models (LLMs) for generating natural language interpretations
- Methodology for collecting activations from SAEs
- Criteria for selecting relevant contexts for interpretation generation
- Approach to scoring the quality of interpretations
- Introduction of intervention scoring as a new evaluation technique
- Guidelines for generating better interpretations across activating contexts
- Decision to automate the interpretability pipeline
- Choice of datasets for training and evaluation
- Strategy for sampling activating examples for interpretation
- Decision to use stratified sampling from activation distributions
- Approach to defining and scoring output latents
- Method for evaluating the robustness of interpretations against distribution shifts
- Decision to make the code and interpretations open-source
- Consideration of computational efficiency in the interpretability pipeline