- Decision to adopt KL-regularized reinforcement learning (KL-RL) for model alignment.
- Choice of inference-time algorithms to consider (e.g., Best-of-N, controlled decoding).
- Modification of the alignment objective to incorporate inference-time decoding procedures.
- Selection of reward transformation methods for inference-time strategies.
- Decision to focus on Best-of-N sampling and Best-of-N jailbreaking as key inference-time strategies.
- Choice of empirical benchmarks (Anthropic helpfulness and harmlessness datasets) for evaluation.
- Decision to implement the Calibrate-and-Transform RL (CTRL) algorithm.
- Assumptions regarding the continuity of the language model's output space.
- Decision to use a specific KL divergence constraint in the alignment framework.
- Choice of reward calibration techniques prior to applying KL-RL.
- Decision to evaluate win rates as the primary metric for model performance.
- Selection of the regularization parameter Î² in the KL-RL framework.
- Decision to derive theoretical conditions for optimal solutions in a toy setting.
- Choice of iterative optimization approach for reward transformation.
- Decision to compare performance against existing state-of-the-art methods.
- Assumptions about the independence of optimal solutions from the base policy and reward model.