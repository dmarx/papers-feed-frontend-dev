<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InfAlign: Inference-aware language model alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-30">30 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ananth</forename><surname>Balashankar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziteng</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Hutter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jong</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chirag</forename><surname>Nagpal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Flavien</forename><surname>Prost</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aradhana</forename><surname>Sinha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ananda</forename><forename type="middle">Theertha</forename><surname>Suresh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">InfAlign: Inference-aware language model alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-30">30 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">647CD3E3FBDE5D198C4A6EC1E4A61522</idno>
					<idno type="arXiv">arXiv:2412.19792v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the win rate of a sample from the aligned model over a sample from the reference model is high, subject to a KL divergence constraint. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. However, the alignment objective does not capture such inference-time decoding procedures. We show that the existing alignment framework is sub-optimal in view of such inferencetime methods. We then modify the alignment objective and propose a framework for inference-aware alignment (InfAlign). We prove that for any inference-time decoding algorithm, the optimal solution that optimizes the inference-time win rate of the aligned policy against the reference policy is the solution to the typical RLHF problem with a transformation of the reward. This motivates us to provide the KL-regularized calibrate-andtransform RL (CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. We particularize our study to two important inference-time strategies: best-of-N sampling and bestof-N jailbreaking, where N responses are sampled from the model and the one with the highest or lowest reward is selected. We propose specific transformations for these inference-time strategies and demonstrate that our framework offers significant improvements over existing state-of-the-art methods for language model alignment. Empirically, we outperform state-ofthe-art methods that are designed without taking inference-time decoding into consideration by 8-12% and 4-9% on inference-time win rates over the Anthropic helpfulness and harmlessness dialog benchmark datasets.</p><p>* Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aligning generative language models (LMs) through KL-regularized reinforcement learning (KL-RL) is a widely adopted framework for finetuning a generative language model to improve a reward (e.g., safety or quality). Solving KL-RL generally entails training a reward model, and then using an RL solver <ref type="bibr" target="#b12">(Christiano et al., 2017;</ref><ref type="bibr" target="#b45">Stiennon et al., 2020;</ref><ref type="bibr" target="#b31">Ouyang et al., 2022)</ref>. Other ways of solving KL-RL include variants of direct preference optimization <ref type="bibr" target="#b36">(Rafailov et al., 2023;</ref><ref type="bibr" target="#b3">Azar et al., 2023)</ref>, reward model distillation <ref type="bibr" target="#b16">(Fisch et al., 2024)</ref>, and Best-of-N distillation <ref type="bibr" target="#b51">(Yang et al., 2024;</ref><ref type="bibr" target="#b18">Gui et al., 2024;</ref><ref type="bibr" target="#b0">Amini et al., 2024;</ref><ref type="bibr" target="#b41">Sessa et al., 2024)</ref>. The success of the KL-RL framework is typically measured through the win rate of the aligned model over the reference model for a given task, which captures how often a sample drawn from the aligned model wins against a sample drawn from the base model using a judge for that task.</p><p>Pre-print However, rarely is the aligned model used as is at inference time; instead an inference-time procedure is typically used to accomplish a task. For example, it is customary to perform one or more of the following simple or complex procedures at decoding: best-of-N sampling <ref type="bibr" target="#b29">(Nakano et al., 2022;</ref><ref type="bibr" target="#b5">Beirami et al., 2024)</ref>, best-of-N jailbreaking <ref type="bibr">(Hughes et al., 2024b)</ref>, chain-of-thought reasoning <ref type="bibr" target="#b49">(Wei et al., 2022;</ref><ref type="bibr" target="#b30">OpenAI, 2024)</ref>, and self consistency <ref type="bibr">(Wang et al., 2022a)</ref> (see Section 7 for more). This creates a discrepancy between the inference-time decoding procedure and the training KL-RL objective, which is to maximize the expectation of a function of reward (e.g., win rate) of a sample from the aligned model against that of the base model.</p><p>In this paper, we address the following question: Given a known inference-time procedure, can we align a model to optimize for the inference-time win rate against the reference model, where inference-time win rate entails obtaining a response from each model through the inference-time procedure and counting which sample wins? While directly optimizing the inference-time win rate seems intractable, we show that the optimal solution is captured via a family of optimization objectives, which we call the inference-aware alignment (InfAlign) framework. We further prove that for a δ-bound language model, where the likelihood of all outcomes are upper-bounded by δ, as δ → 0, the solution could be obtained by solving KL-RL with a specific transformation of the reward (Lemma 1). Therefore, the challenge of optimizing for inference-time win rate can be captured by designing a reward transformation that is suited to the specific inference-time procedure, and solving KL-RL using existing optimization algorithms like PPO <ref type="bibr" target="#b39">(Schulman et al., 2017)</ref>.</p><p>We particularize the study of InfAlign to two simple yet popular inference-time strategies: Best-of-N (BoN) sampling <ref type="bibr" target="#b29">(Nakano et al., 2022;</ref><ref type="bibr" target="#b5">Beirami et al., 2024)</ref> and Best-of-N jailbreaking <ref type="bibr">(Hughes et al., 2024a)</ref>, which we call Worst-of-N (WoN) since the defender may assume that the attacker is choosing the worst outcome of N samples. Despite their simplicity, BoN is extremely effective, and even known to be an effective procedure for inference-time alignment <ref type="bibr" target="#b5">(Beirami et al., 2024;</ref><ref type="bibr" target="#b18">Gui et al., 2024;</ref><ref type="bibr" target="#b28">Mudgal et al., 2024)</ref>. WoN is a popular safety evaluation strategy often adopted to evaluate the ability to jailbreak foundation models <ref type="bibr" target="#b56">(Yohsua et al., 2024;</ref><ref type="bibr" target="#b9">Chao et al., 2023;</ref><ref type="bibr" target="#b44">Souly et al., 2024;</ref><ref type="bibr">Hughes et al., 2024b)</ref>. Hence, there is both theoretical and practical motivation for training to optimize inference-time BoN and WoN win rates. For these procedures, we solve InfAlign for different values of N in the limit of infinitely expressive language models. We also show that exponential reward transformation is almost optimal for these strategies. We further show that the win rate optimal solution to KL-RL without reward transformation leads to suboptimal policies for inference-time win rate. Motivated by our theoretical findings, we propose a practical solver for InfAlign, called Calibrate-and-Transform Reinforcement Learning (CTRL), which adopts a three-step approach:</p><p>(1) we first calibrate the scores of the reward model with respect to responses sampled perprompt by the reference model; (2) we then adopt a suitable transformation of the calibrated scores for a given inference-time procedure; and (3) we solve the KL-RL problem (e.g., using PPO).</p><p>We apply CTRL to the Anthropic helpfulness and harmlessness dataset <ref type="bibr" target="#b4">(Bai et al., 2022)</ref> for optimizing BoN helpfulness and WoN harmlessness with various values of N. We show that the (close-to-optimal) exponential reward transformations that were derived theoretically on idealized distributions transfer to real-world tasks -outperforming various KL-RL solvers at inference-time win rate. Finally, we also show CTRL (without any reward transformation) is a strong baseline for optimizing the standard win rate vs KL trade-off against various methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions are as follows:</head><p>• We propose a framework for inference-aware language model alignment (InfAlign) that captures the optimal solution to optimizing win rate in view of any wellbehaving inference-time procedure. Theoretically, we show that this framework satisfies a coupled-transformed reward/policy optimization objective which lends itself to iterative optimization for a large class of inference-time procedures (see Section 3).</p><p>• Motivated by our theoretical findings, we propose a practical solver, calibrate-andtransform RL (CTRL) for InfAlign, which consists of an offline reward calibration and transformation step prior to applying the regular KL-RL framework. For two popular inference-time procedures, namely optimizing Best-of-N (BoN) sampling and preventing Best-of-N jailbreaking (WoN), we derive the conditions for optimal theoretical solutions to alignment in a toy setting. Somewhat surprisingly, for BoN and WoN, the optimal solution is independent of the base policy and the reward model, which leads to an iterative optimization approach that can be computed offline (see Section 4 and Section 5).</p><p>• Empirically, we show on Anthropic dialog helpfulness and harmlessness datasets that CTRL with identity reward transformation achieves competitive performance compared to a variety of SOTA methods for optimizing standard win rate <ref type="bibr" target="#b18">(Gui et al., 2024;</ref><ref type="bibr" target="#b0">Amini et al., 2024;</ref><ref type="bibr" target="#b41">Sessa et al., 2024;</ref><ref type="bibr" target="#b3">Azar et al., 2023;</ref><ref type="bibr" target="#b36">Rafailov et al., 2023)</ref>, and produces better inference-time win rate vs KL tradeoffs by 8-12% for BoN and 4-9% for WoN inference-time procedures respectively (see Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem setup</head><p>We consider a generative language model that produces a response conditioned on an input prompt. Given a prompt x, e.g., x = What is a large language model?, a generative language model π ref returns a response y, which is sampled according to the distribution</p><formula xml:id="formula_0">π ref (• | x).</formula><p>We use X and Y to denote the space of possible inputs and outputs, respectively. Throughout the paper, we refer to the conditional distribution π(• | x) introduced by the language model as a policy, and assume π ref is a fixed base policy, e.g., obtained from supervised finetuning. We will often use the notation π(y</p><formula xml:id="formula_1">| x) ∝ f (y) for some f : Y → R + to denote the conditional distribution π(y | x) = f (y)/( y f (y)dy) obtained after normalization.</formula><p>Alignment of language models. Let r : X × Y → R be a reward function that assigns a scalar value to any (prompt, response) pair, e.g., a model trained side-by-side on human preferences. The reward determines the goodness of response y in context x. The goal of language model alignment is to construct an aligned distribution π that improves the reward of the response while being close to π ref .</p><p>Below we introduce KL-regularized reinforcement learning (RL), a popular training-time alignment framework in the literature.</p><p>Definition 1 (KL-regularized RL framework). Let β &gt; 0 be a regularization parameter, the KL-regularized RL problem aims to maximize the pointwise expected reward with a KL regularizer below: ∀x,</p><formula xml:id="formula_2">π * r,β (• | x) = arg max π E y∼π(•|x) {r(x, y)} -βD KL (π(• | x)∥π ref (• | x)) . 1<label>(1)</label></formula><p>When evaluating an aligned policy, a common measure to use is the win rate over the base policy π ref .</p><p>Let</p><formula xml:id="formula_3">w r (y, z | x) := 1 {r(x, y) &gt; r(x, z)} + 1 2 1 {r(x, y) = r(x, z)}</formula><p>be the win random variable under reward r.</p><p>Definition 2 (Calibrated reward). Let C r,π (x, y) be the calibrated reward 2 under policy π defined below C r,π (x, y)</p><formula xml:id="formula_4">:= E z∼π(•|x) w r (y, z | x).<label>(2)</label></formula><p>The win rate is defined as following:</p><p>Pre-print Definition 3 (Win rate). For any policy π 1 and π 2 , we define win rate of policy π 1 over policy π 2 given prompt x, as measured by reward r as follows:</p><formula xml:id="formula_5">W r (π 1 ≻ π 2 | x) := E y∼π1(•|x) C r,π2 (x, y).<label>(3)</label></formula><p>In this paper, for simplicity of the presentation and analysis, we assume that the language model is continuous, whose outcomes have infinitesimally small probabilities. In other words, we assume Y is a continuous set, which can be mapped to [0, 1] through a CDF inverse transformation <ref type="bibr" target="#b38">(Rosenblatt, 1952)</ref>, with the ordering determined by the reward of the outcomes from the smallest reward to the highest reward. We also assume that r assigns distinct rewards to different y's for a given x. These assumptions have been made in the past implicitly by <ref type="bibr" target="#b20">Hilton &amp; Gao (2022)</ref> to estimate the KL divergence of Best-of-n policy, and by <ref type="bibr" target="#b18">Gui et al. (2024)</ref> to estimate its win rate and characterize the KL divergence and win rate of optimal policy for win rate vs KL divergence tradeoffs. While these assumptions lead to approximations when analyzing real-world distributions, the results derived under them are reasonably tight when the actual likelihood of the language model outcomes are small <ref type="bibr" target="#b5">(Beirami et al., 2024)</ref>. Note that we don't make any such assumptions when providing our algorithmic developments, and the experimental results. Under these assumptions, we define the quantile mapping, which is the reverse of the calibrated reward mapping C -1 , satisfying ∀u ∈ [0, 1],</p><formula xml:id="formula_6">C -1 r,π,x (u) = y u,x where C r,π (y u,x | x) = u.</formula><p>Inference-time processing. As mentioned earlier, in many cases we do not simply just sample from the language model. When decoding is done through an inference-time procedure, the obtained sample follows a transformed distribution that depends on the aligned policy and the inference-time procedure. For example, when the inference-time procedure is BoN from the base policy π, it can be shown that the final obtained sample is distributed proportional to π(y | x)C r,π (x, y) N -1 <ref type="bibr">(Beirami et al., 2024, Lemma 1)</ref>. In this work, we model inference-time processing as a procedure T that maps π to a distribution T (π),</p><formula xml:id="formula_7">π T -→ T (π).</formula><p>In these cases, it is customary to compare the models by considering the following inferencetime win rate of the aligned policy π.</p><p>Definition 4 (Inference-time win rate). Under inference-time processing T , the inferencetime win rate of policy π 1 over π 2 is defined as</p><formula xml:id="formula_8">W T r (π 1 ≻ π 2 | x) := E y∼T (π1)(•|x),z∼T (π2)(•|x) {w r (y, z | x)} .<label>(4)</label></formula><p>With the above definitions at hand, the goal of the paper is solve the following KL-regularized inference-time win rate maximization problem.</p><p>Definition 5 (Inference-time win rate KL-regularized RL problem). Let T be a given inference-time procedure and β &gt; 0. Then, the optimization problem for maximizing inferencetime win rate is posed as</p><formula xml:id="formula_9">max π W T r (π ≻ π ref | x) -βD KL (π(• | x)∥π ref (•|x)) .<label>(5)</label></formula><p>Note that this formulation reduces to the IPO objective (Azar et al., 2023, Equation ( <ref type="formula">8</ref>)) when T is the identity transformation and extends it otherwise for an arbitrary inference-time procedure.<ref type="foot" target="#foot_2">foot_2</ref> </p><p>3 Reinforcement learning with reward transformation</p><p>In this section, we propose a general framework for solving the language model alignment problem defined in Definition 5. Our approach is based on designing a new reward function R based on the reward model r, the inference-time procedure T , and the base policy π ref , such that solving the KL-regularized RL problem (Definition 1) with the transformed reward R leads to an almost optimal solution to Eq. ( <ref type="formula" target="#formula_9">5</ref>). More precisely, the aligned policy is the solution to the following optimization problem:</p><formula xml:id="formula_10">max π E x∼µ,y∼π(•|x) {R r,π ref ,T (x, y) -βD KL (π(• | x)∥π ref (• | x))},<label>(6)</label></formula><p>where R r,π ref ,T (x, y) is a transformed reward function. At first, it might not be immediately clear how Eq. ( <ref type="formula" target="#formula_10">6</ref>) might help solve the problem in Eq. ( <ref type="formula" target="#formula_9">5</ref>), however, we will show that for any inference-time procedure T , there exists a transformed reward R, which solves Eq. ( <ref type="formula" target="#formula_9">5</ref>).</p><p>Lemma 1. For any base policy π ref , reward model r, inference-time procedure T , and β &gt; 0, there exists a reward function R r,π ref ,T such that the solution to Eq. (6) solves the optimization problem in Eq. (5) (Definition 5).</p><p>In general, such optimal reward transformation will depend on the base policy π ref , the post-hoc procedure T , and the reward function r. In the lemma below, we list the property that the reward transformation and the resulting optimal aligned policy must satisfy.</p><p>Theorem 1 (Characterization of InfAlign solution). Assuming that T is such that ∂T (π)(y 1 | x)/∂π(y 2 | x) exists for all x, y 1 , y 2 , then we have the optimal transformed reward R and the optimal policy π * in Eq. (5) must satisfy the following coupled equations ∀x, y</p><formula xml:id="formula_11">π * (y|x) ∝ π ref (y | x)e 1 β R(x,y) (7) R(x, y) = ∂ ∂π(y | x) W T r (π ≻ π ref | x) (8) = z C r,T (π ref ) (x, z) ∂ ∂π(y | x) T (π)(z | x),<label>(9)</label></formula><p>where C r,T (π ref ) (x, z) is the calibrated reward under the inference-time transformed policy.</p><p>Missing proofs are presented in Appendix A. Theorem 1 naturally leads to an iterative EM-style algorithm that (I) updates π with R fixed based on Eq. ( <ref type="formula">7</ref>) and (II) updates R with π fixed based on Eq. ( <ref type="formula" target="#formula_11">9</ref>) until convergence. However, such algorithm suffers from two drawbacks: first, for general language models, it is inefficient/intractable to evaluate Eq. ( <ref type="formula" target="#formula_11">9</ref>) since it involves evaluating the policy on a large, or even infinite output space; second, it is unclear whether such an algorithm could lead to the optimal solution.</p><p>To find more efficient ways to design reward transformations, we examine the case when no inference-time procedure is performed. In this case, T (π) = π and</p><formula xml:id="formula_12">∂ ∂π(y | x) T (π)(z | x) = 1 {z = y} .</formula><p>Eq. ( <ref type="formula" target="#formula_11">9</ref>) will reduce to R(x, y) = C r,π ref (x, y), the CDF or calibrated reward under π ref .</p><p>Corollary 1. When no inference-time procedure is performed, i.e. ∀π, T (π) = π, the solution to Eq. (6) with R(x, y) = C r,π ref (x, y) is the solution to Eq. (5). Note that the above corollary is also observed in <ref type="bibr" target="#b3">Azar et al. (2023)</ref>; <ref type="bibr" target="#b18">Gui et al. (2024)</ref>. Hence Theorem 1 can be viewed as a generalization of these results with general inference-time procedures. The observation motivates us to consider a family of reward transformations based on this calibrated reward, described in the next section. As we will see, for the class of calibrated inference-time procedures (Definition 6), different transformations in such family could be efficiently evaluated through a toy language model, which enables the search for good or even optimal transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-print 4 Towards solving InfAlign</head><p>We first state a few properties of reward calibration in Section 4.1. Then, in Section 4.2, we demonstrate how this approach enables efficient evaluations of different transformations for calibrated inference-time procedures. We will then use best-of-N and worst-of-N as examples of post-hoc procedures to demonstrate the effectiveness of such an approach in Section 4.3. Missing proofs in the section are presented in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reward calibration</head><p>Recall the definition of the calibrated reward in Definition 2. In this section, we assume that C r,π ref is already obtained and discuss methods to approximate C r,π ref in Section 5. We state a few properties of C r,π ref below. The first property states that reward calibration preserves the ordering of the reward. This implies that the win rate evaluated under the calibrated reward stays unchanged.</p><p>Lemma 2 (Calibration is a bounded monotone increasing transformation of reward). We have C r,π ref (x, y) ∈ [0, 1]. Furthermore, we have for any y and z</p><formula xml:id="formula_13">r(x, y) ≥ r(x, z) =⇒ C r,π ref (x, y) ≥ C r,π ref (x, z)<label>(10)</label></formula><p>Moreover, we have that C r,π ref is a canonical transformation of reward, and is invariant under all monotone increasing transformations of the reward function, stated below.</p><p>Lemma 3 (The calibrated reward is invariant under monotone increasing transformations of reward). Let m : R → R be any monotonic increasing function. Then,</p><formula xml:id="formula_14">C m(r),π ref = C r,π ref .<label>(11)</label></formula><p>In particular, this also immediately implies that</p><formula xml:id="formula_15">C Cr,π ref ,π ref = C r,π ref .</formula><p>This property is useful since as long as the learned reward model r can capture relative human preference between pairs, the calibration of r will be the same. Hence C is more robust to the learning process of r.</p><p>The next property shows that the calibration operation allows us to transform the distribution of the reward under the base policy to a uniform distribution over [0, 1] regardless of the base policy π ref and the reward model r.</p><p>Lemma 4. If π is a continuous language model, let y be sampled from</p><formula xml:id="formula_16">π ref (• | x), then we have ∀x, C r,π ref (x, y) ∼ Unif([0, 1]).</formula><p>The lemma provides us a simple, unified view of the output from a language model through the space of calibrated reward, which is independent from the base policy, and the reward model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">KL-regularized RL with the calibrated reward</head><p>Next we discuss how the calibrated reward can be used in KL-regularized reinforcement learning. As Lemma 4 suggests, after calibration, the reward distribution of the outputs from the base policy is independent from the reward model and the base policy itself. This allows us to design a transformation function Φ, focusing only on the inference-time procedure T , to be applied on top of the calibrated reward function, independent of π ref and r.</p><p>More precisely, let Φ : [0, 1] → R be a transformation function, we propose the following reward function</p><formula xml:id="formula_17">R Φ (x, y) = Φ(C r,π ref (y | x)),<label>(12)</label></formula><p>and we would like the aligned policy to be the solution to the KL-regularized RL problem defined in Definition 1 with reward R Φ (x, y),</p><formula xml:id="formula_18">π * RΦ,β (• | x) = arg max π E y∼π(•|x) {R Φ (x, y)} -βD KL (π(• | x)∥π ref (• | x)).<label>(13)</label></formula><p>Inference-aware reward transformation. For a given inference-time procedure T , our goal is to derive or design a suitable transformation Φ, such that the solution leads to a good or even optimal trade-off between the inference-time win rate W T and the KL divergence from the base policy.</p><p>Standard win rate (no inference-time procedure). When no inference-time procedure is employed (i.e., T is the identity mapping), W T reduces to the standard win rate. Setting Φ to be the identity transformation leads to the optimal win rate vs KL trade-off curve by noting that</p><formula xml:id="formula_19">E y∼π(•|x) {C r,π ref (x, y)} = W r (π ≻ π ref | x).</formula><p>In this case, Eq. ( <ref type="formula" target="#formula_18">13</ref>) will be the same as the alignment objective of Azar et al. ( <ref type="formula">2023</ref>). Moreover, it can also be shown that when Φ(•) = log(•), the solution π * RΦ,β recovers the popular best-of-N distillation objective, which has been studied in a recent line of works <ref type="bibr" target="#b18">(Gui et al., 2024;</ref><ref type="bibr" target="#b0">Amini et al., 2024;</ref><ref type="bibr" target="#b41">Sessa et al., 2024)</ref>, and shown to be nearly win rate optimal <ref type="bibr" target="#b51">(Yang et al., 2024;</ref><ref type="bibr" target="#b18">Gui et al., 2024)</ref>. We also note that while these methods lead to similar optimization objectives, the algorithmic approaches to solve the problems are different. In Section 5, we will discuss a unified algorithm to solve Eq. ( <ref type="formula" target="#formula_18">13</ref>) for general Φ. In Section 6, we compare the results for Φ set to the identity mapping with the abovementioned baseline approaches.</p><p>We consider a family of inference-time procedures that only depend on the calibrated reward of the outputs, which we term calibrated procedures, and discuss how to design a suitable Φ for this family of transformations. We first define calibrated procedures below. Definition 6 (Calibrated inference-time procedure). An inference-time procedure T is called a calibrated procedure if there exists a mapping function g T : [0, 1] → R such that for any π, r, and x, y, we have</p><formula xml:id="formula_20">T (π)(y | x) ∝ π(y | x) • g T (C r,π (x, y)).</formula><p>Our next result shows that for calibrated inference-time procedures, the aligned policy from solving Eq. ( <ref type="formula" target="#formula_18">13</ref>) has a win rate and KL divergence independent of the base policy and reward function.</p><p>Theorem 2 (Model-agnostic property of calibrated inference-time procedures, informal version of Theorem 4). If T is a calibrated inference-time procedure, for any continuous language model π, β &gt; 0 and reward transformation function Φ, we have that both</p><formula xml:id="formula_21">W T r (π * RΦ,β ≻ π ref | x) and D KL (π * RΦ,β ∥π ref ) are independent of r and π ref .</formula><p>The above theorem allows us to evaluate a transformation Φ by focusing on simple continuous language models that are easy to compute and simulate. In the next section, we will use two popular inference-time procedures, best-of-N and worst-of-N , as examples to demonstrate how the theorem enables us to efficiently evaluate the inference-time win rate vs KL divergence tradeoff curve for different Φ functions, which could be used to find a suitable transformation Φ in practical scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Finding better transformations for BoN and WoN</head><p>In this section, we focus on the following two inference-time procedures.</p><p>Best-of-N inference-time procedure (BoN</p><p>). During inference, N i.i.d. responses from a policy π are generated, i.e., y 1 , . . . y N ∼ π. The final output is the one with the highest reward, i.e., y BoN = arg max y∈{y1,...y N } r(x, y). Worst-of-N inference-time procedure (WoN). Let y 1 , . . . y N be N i.i.d. draws from policy π. The final output is the one with the lowest reward. i.e., y WoN = arg min y∈{y1,...y N } r(x, y).</p><p>The lemma below presents the distribution of outputs after the inference-time procedure is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-print</head><p>Lemma 5. For any N and continuous language model π,</p><formula xml:id="formula_22">BoN(π)(y | x) ∝ π(y | x) • C r,π (x, y) N -1 . WoN(π)(y | x) ∝ π(y | x) • (1 -C r,π (x, y)) N -1 .</formula><p>Note that the results for BoN have already been derived previously <ref type="bibr" target="#b5">(Beirami et al., 2024;</ref><ref type="bibr" target="#b18">Gui et al., 2024;</ref><ref type="bibr" target="#b0">Amini et al., 2024)</ref>. The lemma shows that these two inference-time procedures are calibrated procedures so that as claimed in Theorem 2, for the aligned policy, the inference-time win rate and KL divergence deviation from the base policy are independent of the base policy and reward model. Below we present the precise formula for these two procedures.</p><p>Theorem 3 (Properties of BoN and WoN procedures). For any transformation function Φ, the solution π * RΦ,β to the KL-regularized RL problem defined in Eq. ( <ref type="formula" target="#formula_18">13</ref>) satisfies the followings:</p><formula xml:id="formula_23">Let F Φ,β (u) = u 0 e Φ(u ′ )/β du ′ 1 0 e Φ(u ′ )/β du ′ .</formula><p>Then the following hold true:</p><p>• The best-of-N win rate over the base policy satisfies for any x,</p><formula xml:id="formula_24">W BoN r (π * RΦ,β ≻ π ref | x) = 1 -N 1 0 F Φ,β (u) N u N -1 du,</formula><p>• The worst-of-N win rate over the base policy satisfies for any x,</p><formula xml:id="formula_25">W WoN r (π * RΦ,β ≻ π ref | x) = N 1 0 (1 -F Φ,β (u)) N (1 -u) N -1 du,</formula><p>• The KL divergence between π * rΦ,β and π ref satisfies</p><formula xml:id="formula_26">D KL (π * RΦ,β ∥π ref ) = 1 β 1 0 Φ(u)e Φ(u)/β du 1 0 e Φ(u)/β du -log 1 0 e Φ(u)/β du . (<label>14</label></formula><formula xml:id="formula_27">)</formula><p>By varying β in Eq. ( <ref type="formula" target="#formula_18">13</ref>), we can obtain an alignment curve plotting the inference-time win rate and KL divergence deviation for different aligned policies. This allows us to compare the performance of different transformation functions Φ.</p><p>In the rest of the section, we will consider different types of transformations, and analytically compute the alignment curves using Theorem 3 by varying β, i.e., the plot of</p><formula xml:id="formula_28">(D KL (π * rΦ,β ∥π ref ), W T r (π * rΦ,β ) ≻ π ref ) for different β's.</formula><p>The transformations we consider include optimal transformations for standard win rate, exponential functions, and optimizationbased transformations, described below.</p><p>Optimal reward transformations for standard win rate. The identity mapping proposed by Azar et al. ( <ref type="formula">2023</ref>) and the logarithmic mapping as used by BoN distillation <ref type="bibr" target="#b5">(Beirami et al., 2024;</ref><ref type="bibr" target="#b51">Yang et al., 2024;</ref><ref type="bibr" target="#b0">Amini et al., 2024;</ref><ref type="bibr" target="#b41">Sessa et al., 2024)</ref> are shown to be (almost) optimal for the standard win rate. We would like to understand whether they are still good candidates when inference-time procedures are considered.</p><p>Deriving an optimized reward transformation function. For calibrated inferencetime procedures like BoN, and WoN, due to Theorem 2, we have that the optimal reward transformation Φ is independent of the base policy. One can optimize for good Φ's using simple toy language models, and we have the following corollary. Corollary 2. For any β &gt; 0, the Φ that achieves the optimal BoN win-rate vs. KL tradeoff must satisfy the following pair of equations: and for WoN, it satisfies that</p><formula xml:id="formula_29">Φ BoN (u) = -N 2 1 u F π (v) N -1 v N -1 dv, f (u) ∝ e Φ BoN (u) β , Pre-print</formula><formula xml:id="formula_30">Φ WoN (u) = -N 2 1 u (1 -v) N -1 (1 -F π (v)) N -1 dv, f (u) ∝ e Φ WoN (u) β .</formula><p>Hence one can find a transformation function based on finding the fixed point of the coupled equations in Corollary 2 through iterative updates.</p><p>Exponential tilting for reward transformation. In addition to deriving the optimized transformation, motivated by the exponential tilting of loss functions <ref type="bibr" target="#b24">(Li et al., 2021;</ref><ref type="bibr">2023)</ref>,</p><p>Pre-print we consider the following exponential transformation:</p><formula xml:id="formula_31">Φ t (u) = sign(t) • e tu ,<label>(15)</label></formula><p>where sign(t) = 1 for t ≥ 0 and sign(t) = -1 for t &lt; 0. These exponential transformations are essentially helping to optimize different quantiles of the reward for different values of t <ref type="bibr" target="#b25">(Li et al., 2023)</ref>. For a positive value of t, the exponential tilting transformation focuses on optimizing the high quantiles of the objective (calibrated reward) and indeed recovers the max function for a large positive t. Hence, we expect that positive values of t help with BoN inference-time procedure. On the other hand, for a negative value of t, the transformation is akin to optimizing the lower quantiles of the objective (calibrated reward), which makes it a suitable transformation for the WoN inference-time procedure.</p><p>Results. In In the first plot, we consider standard win rate. In this case, it is known that the IPO objective is win rate optimal. As can be seen, the logarithmic transformation (i.e., best-of-N distillation) also achieves a nearly optimal win rate, which was already observed by Yang et al. ( <ref type="formula">2024</ref>); <ref type="bibr" target="#b18">Gui et al. (2024)</ref>. All other transformations are sub-optimal for standard win rate.</p><p>Next, we consider Best-of-2 and Best-of-4 win rate. Here, additionally we include bon opt, which is obtained by deriving the fixed point of Corollary 2. As can be seen, the identity transformation is no longer optimal. The best tradeoffs are given by bon opt. We also observe that exp(5x) and exp(10x) are almost as good as bon opt for Best-of-2 and Best-of-4, respectively. Moreover,the identity transformation and logarithmic transformation are sub-optimal in these cases, which shows that considering standard win rate as the only metric is not optimal when inference-time procedure is concerned. We also observe that the behavior of identity transformation and logarithmic transformation is different in that the identity transformation gives better tradeoffs.</p><p>Finally, we consider Worst-of-2 and Worst-of-4 win rate. Again, it can be observed that bon opt gives the best tradeoffs for this inference-time procedure. Here, exp(-5x) and exp(-10x) are almost as good for Worst-of-2 and Worst-of-4, respectively. We also observe that identity transformation and logarithmic transformation are sub-optimal in these cases and the logarithmic transformation gives better tradeoffs for WoN compared to the identity transformation.</p><p>The above results demonstrate the importance of considering the inference-time procedure when performing alignment. We find that exponential transformation with different t's are good for different inference-time procedures, which will be our focus in practical experiments.</p><p>Next, we will examine whether a good transformation that we found on the idealized continuous language model generalizes in the wild to real-world scenarios. Before moving on to the experiments, we will have to offer a practical algorithm for solving the inference-time KL-regularized RL optimization problem, which is the subject of the next section.</p><p>Algorithm 1 CTRL Algorithm for BoN and WoN Require: Base policy π ref , (uncalibrated) reward model r, training prompts x ∈ X , number of offline rollouts per prompt m. 1: Compute approximate empirical calibration function C r,π ref using Algorithm 2 with m offline rollouts per prompt. 2: Transform calibrated reward using exponential function per Eq. ( <ref type="formula" target="#formula_31">15</ref>) with t &gt; 0 for BoN and t &lt; 0 for WoN. 3: Optimize KL-RL using calibrated and transformed reward per Eq. ( <ref type="formula" target="#formula_10">6</ref>).</p><p>Consider empirical calibration, where we draw K samples z 1 , z 2 , ..., z K from the reference model π ref for each prompt x in the RL training data. We then sort the rewards to all the responses {r(x, z 1 ), r(x, z 2 ), ...r(x, z K )}, and assign empirical calibrated reward scores during RLHF training for the prompt, response pair (x, y) as</p><formula xml:id="formula_32">C r,π ref (x, y) = 1 K K i=1,zi∼π ref 1[r(x, y) ≥ r(x, z i )].<label>(16)</label></formula><p>Ideally, as K → ∞, the empirical calibrated reward would converge to the true calibrated reward, and may be used in the RL training objective through PPO <ref type="bibr" target="#b39">(Schulman et al., 2017)</ref>. However, this could be costly as the exact computation of this calibrated reward requires us to sample and store K reward scores per-prompt and per roll-out in the KL-RL solvers.</p><p>Instead, we propose to approximate the calibration curve by scaling it with a step-wise function in the logarithmic domain. We do this by choosing p anchor points q 1 , q 2 , . . ., where at each of the quantiles q i ∈ (0, 1) we achieve zero calibration error. The algorithm for the simpler case (p = 1, median) is given in Algorithm 2. For larger values of p, the algorithm is given in Algorithm 3 (Appendix D).  6 Experiment Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation setup</head><p>Datasets. To train the reward models, we use the Anthropic Helpfulness and Harmlessness datasets <ref type="bibr" target="#b4">(Bai et al., 2022)</ref> which involve multi-turn dialogues between a human and a digital assistant. The preference datasets consist of two responses for one context, and a label for the human preference for the response. We use the train split of the two datasets (44K examples for helpfulness and 42K for harmlessness) to train the uncalibrated and calibrated reward models -separate reward models for each objective.</p><p>Model. The uncalibrated reward model is trained based on the Bradley-Terry pairwise objective <ref type="bibr" target="#b37">(Raffel et al., 2020)</ref>, and the calibration is done on the training-split of the RL training procedure by drawing samples from the reference model. The underlying model for both these rewards is the PaLM-2 S model <ref type="bibr" target="#b2">(Anil et al., 2023)</ref>. The base reference policy model is a PaLM-2 S model that is fine-tuned (SFT) on the Anthropic dialog preferred responses. We then train the aligned policy model through KL-regularized Reinforcement Learning <ref type="bibr" target="#b39">(Schulman et al., 2017;</ref><ref type="bibr" target="#b4">Bai et al., 2022)</ref>. We compare against uncalibrated (a model trained with KL-RL using PPO and no further processing), BoNBoN <ref type="bibr" target="#b18">(Gui et al., 2024)</ref>, Best-of-N <ref type="bibr" target="#b29">(Nakano et al., 2022;</ref><ref type="bibr" target="#b5">Beirami et al., 2024)</ref> True rewards. As evaluating using ground truth rewards in a pointwise manner based on human annotations can be expensive, we follow <ref type="bibr" target="#b15">Eisenstein et al. (2024)</ref>; <ref type="bibr" target="#b28">Mudgal et al. (2024)</ref> and perform automated evaluation using a larger PaLM-2 M model to compute true rewards.</p><p>Metrics. To measure improvement due to post-RL training, we report both the win rate and the BoN and WoN win rates, calculated after applying the respective inference-time procedures on the responses generated by the RL model against the base SFT model, along with the corresponding KL-divergence of the RL model with the SFT model. For each of the runs, we experiment with different KL-regularizer strengths (β ∈ {0.01, 0.02, . . . , 0.09}) and obtain the Pareto-curve of the KL divergence vs {standard, BoN, WoN } win rate curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Reward models are typically miscalibrated</head><p>We first validate our hypothesis that reward models used on real-world tasks are miscalibrated. We measure the miscalibration of the reward model trained on Anthropic helpfulness preference dataset by computing the scores of 100 reference-policy responses for 10 random prompts from training split. We then sort the scores and compute the ranks corresponding to each of the responses and plot these values as a scatter plot in Figure <ref type="figure" target="#fig_2">2</ref> (left). If the model were perfectly calibrated, the points for each prompt would lie on the line y = x. However, observe that for most prompts, the scatter plot deviates significantly from the y = x line, and the extent of this deviation varies depending on the prompt.</p><p>We then measure the Absolute Error (AE) between the reward scores and their corresponding ranks and plot the cumulative distribution function (CDF) of the AE of the various calibration approximations in Figure <ref type="figure" target="#fig_2">2</ref> (right). If the model is well-calibrated AE is zero always and hence the CDF reaches one at zero AE. We find that the reward scores (see legend named 'identity') are not calibrated (mean AE: 0.22) and using fixed reward polynomial transformation functions like square-root, cube, square -do not reduce the calibration error (mean AE &gt; 0.15). However, using a per-prompt quantile-based reward calibration, see legend named 'quantile') significantly reduces the calibration error (mean AE: 0.02).</p><p>Figure <ref type="figure" target="#fig_8">3</ref>: Calibrating the helpfulness and harmlessness reward improves average win rate (identity transformation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Calibrated rewards improve standard win rate</head><p>As a sanity check of InfAlign, we first measure the performance when there is no inferencetime procedure applied. Without any inference-time procedure, the optimization objective is standard win rate, and we compare the performance of CTRL (using an identity transform) against other relevant reward optimization baselines that are known to be (almost) win rate optimal, such as IPO <ref type="bibr" target="#b3">(Azar et al., 2023)</ref> and Best-of-N distillation <ref type="bibr" target="#b18">(Gui et al., 2024;</ref><ref type="bibr" target="#b0">Amini et al., 2024;</ref><ref type="bibr" target="#b41">Sessa et al., 2024)</ref>. Specifically, we calibrated the reward model for helpfulness and harmlessness of the base model fine-tuned on preferred responses using Algorithm 2 with p = 1. In Figure <ref type="figure" target="#fig_8">3</ref>, we find that compared to IPO and BoNBoN, the calibrated reward optimization achieves better win rate-KL trade-offs. We attribute this gain to a more efficient computation of win rate on training data using m samples from the base model, as opposed to relying of existing pairwise comparison data during KL-RL. Further, we find that the reward transformations applied on the calibrated reward perform similar to each other, thus validating the theoretical results of Section 4. We also find that reducing the calibration error with more anchors or by varying m, leads to better win rate vs KL tradeoffs (see Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">CTRL improves BoN</head><p>For the helpfulness objective in the Anthropic dialog dataset, we aim to optimize the Best-of-N performance of the aligned model through the exponential transformation of the calibrated rewards. We measure the win rate against the Best-of-N of the base policy model (N =4). In Figure <ref type="figure" target="#fig_6">4</ref> we see that calibration based on the median rewards per-prompt achieves 8 -12% higher Best-of-N win rates as compared to the uncalibrated model on helpfulness objective. The exponential transformation of the calibrated reward outperforms the rest of the models. We find that the exponential factor of t = 10 works best as simulated by our framework on a toy-setting (see Section 4). Further, we show that these gains hold for varying values of N (2, 32) (see Appendix D.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">CTRL improves WoN (BoN jailbreaks)</head><p>For the harmlessness objective in the Anthropic dialog dataset, we aim to improve the Worst-of-n performance of the aligned policy model to improve safety against adversarial actors <ref type="bibr">(Hughes et al., 2024b)</ref>. Here, we use the negative exponential transformation t &lt; 0.</p><p>In Figure <ref type="figure" target="#fig_6">4</ref> we see that calibration based on the median rewards per-prompt achieves 4 -9% higher Worst-of-N win rates as compared to the uncalibrated model. The negative transformation of the calibrated reward outperforms the rest of the models, with t = -10 performing the best: again identified as the optimal value per our simulation in a toy setting (see Section 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-print</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Inference-time compute. Test-time compute has been leveraged in recent work <ref type="bibr" target="#b43">(Snell et al., 2024;</ref><ref type="bibr" target="#b6">Brown et al., 2024;</ref><ref type="bibr">Wu et al., 2024a)</ref> to achieve better win rate vs KL tradeoffs from the aligned models including controlled decoding <ref type="bibr" target="#b28">(Mudgal et al., 2024;</ref><ref type="bibr" target="#b8">Chakraborty et al., 2024)</ref>, Monte Carlo tree search <ref type="bibr" target="#b7">(Chaffin et al., 2022;</ref><ref type="bibr" target="#b40">Scialom et al., 2021;</ref><ref type="bibr" target="#b57">Zhao et al., 2024)</ref>, iterative jailbreak query refinement <ref type="bibr" target="#b9">(Chao et al., 2023)</ref>, and model-chaining within agentic frameworks <ref type="bibr" target="#b19">(Gur et al., 2024)</ref>. Best-of-N (BoN) is also used as an evaluation metric in code and natural language generation benchmarks <ref type="bibr" target="#b45">(Stiennon et al., 2020;</ref><ref type="bibr" target="#b11">Chen et al., 2021)</ref>. Further, Worst-of-N (WoN) is a popular jailbreaking strategy for adversarial actors to elicit unsafe text from large language models <ref type="bibr">(Hughes et al., 2024b)</ref>. Prior work has largely focused on approximating inference-time solutions during training time through sampling <ref type="bibr" target="#b18">(Gui et al., 2024;</ref><ref type="bibr" target="#b0">Amini et al., 2024</ref><ref type="bibr">), distillation (Sessa et al., 2024)</ref>, and decoding <ref type="bibr" target="#b35">(Qiu et al., 2024)</ref>. Our work is orthogonal to this body of work as they assume that no inference-time procedure is applied, but rather attempt to approximate it during training. We show that our theoretical framework generalizes <ref type="bibr">IPO (Azar et al., 2023)</ref> and best-of-N distillation <ref type="bibr" target="#b18">(Gui et al., 2024;</ref><ref type="bibr" target="#b0">Amini et al., 2024;</ref><ref type="bibr" target="#b41">Sessa et al., 2024)</ref> as special cases.</p><p>We are motivated by recent work that apply meta-generation procedures <ref type="bibr" target="#b50">(Welleck et al., 2024)</ref> at inference-time such as chaining prompted models <ref type="bibr" target="#b6">(Brown et al., 2024)</ref>, problem decomposition through chain-of-thought <ref type="bibr" target="#b49">(Wei et al., 2022)</ref>, Best-of-N reranking <ref type="bibr" target="#b13">(Collins &amp; Koo, 2005;</ref><ref type="bibr" target="#b10">Charniak &amp; Johnson, 2005;</ref><ref type="bibr" target="#b34">Pauls &amp; Klein, 2009)</ref> applied on reasoning traces (OpenAI, 2024). Our InfAlign framework was also motivated by complex inference-time strategies that involve transformation techniques such as refinement <ref type="bibr" target="#b26">(Madaan et al., 2024)</ref>, majority voting <ref type="bibr">(Wang et al., 2022b)</ref>, or using the generator as input to other search algorithms <ref type="bibr" target="#b55">(Yao et al., 2024)</ref>, that have outperformed other models for harder tasks. In this spirit, our framework allows to get additional gains in aligning models with such inference-time procedures deployed in the future.</p><p>Reward miscalibration. Reward miscalibration or hacking has been studied extensively in recent work <ref type="bibr" target="#b1">(Amodei et al., 2016;</ref><ref type="bibr" target="#b33">Pang et al., 2022;</ref><ref type="bibr" target="#b17">Gao et al., 2023)</ref>. The hypotheses behind reward hacking can be broadly categorized into 3 themes: (1) reward underspecification, (2) training-serving skew between pairwise and pointwise reward models, (3) dominant reward due to adhoc transformations. Reward models suffer from underspecification due to under-specified training data <ref type="bibr" target="#b42">(Skalse et al., 2022)</ref> by capturing spurious correlations in the data <ref type="bibr" target="#b32">(Pan et al., 2022)</ref>. Methods to mitigate this often include training on non-overlapping splits during reward model fine-tuning <ref type="bibr" target="#b4">(Bai et al., 2022)</ref> and ensembling <ref type="bibr" target="#b14">(Coste et al., 2023;</ref><ref type="bibr" target="#b15">Eisenstein et al., 2024)</ref>. Our CTRL method can be easily augmented with such data interventions in reward learning.</p><p>Pre-print KL-RL solvers. Training reward models on pairwise preference data, and then using it as pointwise scorers during reinforcement learning poses problems of transitive inconsistency.</p><p>To mitigate this problem, optimization techniques that directly incorporate the pairwise preference data during offline reinforcement learning have been proposed <ref type="bibr" target="#b36">(Rafailov et al., 2023;</ref><ref type="bibr" target="#b3">Azar et al., 2023)</ref>. Further, calibrating model probabilities to reflect rank-order generated sequences by quality metrics have been proposed <ref type="bibr" target="#b58">(Zhao et al., 2022)</ref>. We share the motivation behind these methods, while additionally recognizing the need to calibrate the rewards against the base policy on which we are aligning.</p><p>When aligning language models for multiple objectives, aggregating the rewards via a weighted sum <ref type="bibr" target="#b4">(Bai et al., 2022;</ref><ref type="bibr">Wu et al., 2024b</ref>) is known to result in reward hacking of one of the dominant rewards. Thresholding the effect of individual rewards <ref type="bibr" target="#b27">(Moskovitz et al., 2023)</ref> or changing the weights of the training data <ref type="bibr" target="#b4">(Bai et al., 2022)</ref>, however requires costly hyperparameter fine-tuning and retraining without the ability to reason about the hyperparameters and their effects on the reward-tradeoffs. Reward transformation techniques that calibrate against a reference reward is effective at mitigating domination of one reward <ref type="bibr" target="#b48">(Wang et al., 2024)</ref>, but implicitly assumes that the reward aggregation function is a logical "AND" of all rewards, heavily penalizing under-performance on any of the rewards. Motivated by the success of exponential tilting for focusing on high/low quantiles <ref type="bibr" target="#b25">(Li et al., 2023)</ref>, we also show that CTRL with exponential reward transformation achieves near-optimal inference-time win rate vs KL divergence tradeoffs, surpassing the performance of methods such as <ref type="bibr">IPO (Azar et al., 2023)</ref> that target to optimize standard win rate vs KL divergence tradeoffs. In this paper, we show that calibration as a first-step can help ground reward transformations based on the final inference-time procedure applied. Further, we build on recent work that show the theoretical guarantees of Best-of-N sampling <ref type="bibr" target="#b5">(Beirami et al., 2024;</ref><ref type="bibr" target="#b17">Gao et al., 2023;</ref><ref type="bibr" target="#b28">Mudgal et al., 2024)</ref> over most reinforcement learning optimization techniques to ground our calibration and transformation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Concluding Remarks</head><p>In this paper, we have shown that existing win rate optimal alignment procedures are sub-optimal when inference-time procedures are applied on aligned language models. As models equipped with inference-time procedures such as reasoning, best-of-N , majority voting continue to outperform models which do not use such procedures, we study the question of how to learn inference-aware optimally aligned language models. While learning optimal solutions for general inference-time procedures is intractable, we propose InfAlign -a framework that optimizes for inference-time win rate, and provide theoretical guarantees of finding an optimal inference-aware aligned model. Our framework generalizes prior work on win rate optimal solutions <ref type="bibr" target="#b3">(Azar et al., 2023;</ref><ref type="bibr" target="#b18">Gui et al., 2024)</ref>. We further show that, for any inference time procedure, such an optimal model can be learned through KL-RL optimization using reward transformation. For a class of transformations that rely only on the rank of rewards, calibration of said rewards into a uniform distribution allows us to search efficiently for the optimal reward transformation through empirical simulation.</p><p>We demonstrate the efficacy of this framework, by transferring findings from empirical simulation to real-world tasks and propose CTRL -a calibrate-and-transform reinforcement learning solver for ranking based inference-time procedures -and particularize it to Best-of-N sampling (BoN) and jailbreaking (WoN). Empirically, we demonstrate on Anthropic dialog helpfulness and harmlessness datasets that, (1) in the standard setting when no inferencetime procedure is applied, CTRL with identity reward transformation achieves competitive or slightly better performance compared to a variety of SOTA methods for optimizing standard win rate (2) when inference-time procedures are applied, we outperform inference-time win rate vs KL tradeoffs compared to existing preference optimization methods by 8-12% for BoN and 4-9% for WoN inference-time procedures respectively.</p><p>Future work includes finding efficient solvers for complex inference-time procedures based on InfAlign that do not rely on the rank of rewards assigned to samples from the base policy model, e.g., reasoning <ref type="bibr" target="#b30">(OpenAI, 2024;</ref><ref type="bibr" target="#b53">Xie et al., 2024)</ref>. Further, practical solvers for reward aggregation of multiple competing objectives at inference-time could also be explored. From a theoretical standpoint, the generalizability of our inference-aware alignment approach to upstream tasks such as supervised fine-tuning or pre-training needs to be studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-print</head><p>A.8 Proof of Corollary 2</p><p>We will show that Corollary 2 is a special case of Theorem 1 with a simple continuous language model. And by Theorem 2, we have the Φ can be generalized to arbitrary continuous language models.</p><p>Let Y = [0, 1]. We assume the LMs and reward models are context-independent. We use u ∈ [0, 1] to denote y and set the reward model to be r(u) = u. The base policy is a simple uniform distribution over [0, 1], π ref = Unif([0, 1]). Let F π (u) be the CDF of π, then we have that the BoN win rate is</p><formula xml:id="formula_33">W BoN r (π ≻ π ref | x) = 1 -N 1 0 F π (u) N u N -1 du,</formula><p>and WoN win rate is</p><formula xml:id="formula_34">W WoN r (π ≻ π ref | x) = N 1 0 (1 -F π (u)) N (1 -u) N -1 du.</formula><p>Plugging these into Theorem 1, we have for BoN,</p><formula xml:id="formula_35">R(u) = ∂W BoN r (π ≻ π ref | x) ∂π(u) = -N 1 0 v N -1 ∂F π (v) N ∂π(u) dv = -N 1 0 v N -1 F π (v) N -1 ∂F π (v) ∂π(u) dv = -N 2 1 0 F π (v) N -1 1 {v ≥ u} v N -1 dv = -N 2 1 u F π (v) N -1 v N -1 dv.</formula><p>For WoN, we have</p><formula xml:id="formula_36">R(u) = ∂W WoN r (π ≻ π ref | x) ∂π(u) = N 1 0 (1 -v) N -1 ∂ (1 -F π (v)) N ∂π(u) dv = -N 2 1 0 (1 -v) N -1 (1 -F π (v)) N -1 ∂F π (v) ∂π(u) dv = -N 2 1 0 (1 -v) N -1 (1 -F π (v)) N -1 1 {v ≥ u} dv = -N 2 1 u (1 -v) N -1 (1 -F π (v)) N -1 dv.</formula><p>Pre-print</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The role of KL divergence in model alignment</head><p>One question that arises is the role of the KL divergence regularizer in Eq. ( <ref type="formula" target="#formula_9">5</ref>). In this section, we argue that the regularizer essentially enables multi-tasking between the SFT task and the RL task.</p><p>Let's consider a log-linear model such that π θ (y|x) = e θ T g(x,y)-A(θ;x) ,</p><p>where g(x, y) is a fixed encoding of (x, y), and A(θ; x) is the partition function normalizing the distribution.</p><p>Supervised finetuning (SFT). Let D sft (x, y) = µ(x) × p sft (y|x) be the SFT data distribution. Then, the SFT task is</p><formula xml:id="formula_38">θ * sft = arg min θ L sft (θ) where L sft (θ) := E (x,y)∼D sft {A(θ; x) -θ ⊤ g(x, y)},<label>(19)</label></formula><p>We further call p = π θ * sft . Lemma 8. The SFT solution satisfies</p><formula xml:id="formula_39">E x∼µ {∇ θ A(θ * sft )} = E (x,y)∼D sft g(x, y).<label>(20)</label></formula><p>Proof. This is a known property of exponential families. The proof follows by noticing Proof. Notice that</p><formula xml:id="formula_40">∇ θ L sft (θ * sft ) = 0.</formula><formula xml:id="formula_41">L bilevel,β (θ) = D KL (π θ ∥p) + 1 β L ro (θ) (23) = E x∼µ {A(θ; x) -A(θ * sft ; x) -(θ -θ * sft ) ⊤ ∇ θ A(θ * sft ; x)} + 1 β L ro (θ) (24) = E x∼µ {A(θ; x) -A(θ * sft ; x)} -(θ -θ * sft ) ⊤ E (x,y)∼D sft g(x, y) + 1 β L ro (θ) (25) = L multi-task,β (θ) + L sft (θ * sft ),<label>(26)</label></formula><p>where Eq. ( <ref type="formula">24</ref>) follows by noticing that KL divergence is a Bregman divergence in this setup, Eq. ( <ref type="formula">25</ref>) follows from Lemma 8, and Eq. ( <ref type="formula" target="#formula_41">26</ref>) follows from the definition of L sft (θ) applied to θ and θ * sft . Hence, the minimizers of the two objectives are the same given that L bilevel,β (θ) = L multi-task,β (θ) + C, completing the proof.</p><p>Pre-print Thus, effectively this proves that the KL-RL objective enables multi-tasking between the SFT stage and the reward optimization RL objective. One may wonder why we did not pose the KL divergence regularizer on the transformed distributions through D KL (T (π)(</p><formula xml:id="formula_42">• | x)∥T (π ref )(•|x)) instead.</formula><p>Consider the Best-of-N jailbreaking for example. While the adversary may be using the model to generate N responses and choose the least safe one for jailbreaking, the model should possess the core capabilities for other types of inference-time usage for other tasks that is different from that of jailbreaking (e.g., through chain-of-thought). Therefore, changing the KL divergence regularizer does not capture the fact that the model should remain suitable for all other tasks, and not just for the one for which it is getting aligned. We also note that if we used D KL (T (π)(• | x)∥T (π ref )(•|x)) instead, the problem would actually simplify to the standard alignment problem through a simple change of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The effect of anchor points on calibration</head><p>In this section, we analyze the effect of using more anchor points on results. First, we present the full algorithm for more than one anchor point in Algorithm 3. q ℓ = ℓ+1 p+1 5: r q ℓ (x) ← top q ℓ •m (r(x, z 1 ), . . . r(x, z m )) select (q ℓ • m) th smallest element , where a is the smallest number in [0, p -1) such that r qa (x) ≥ r(x, y), otherwise a = 0.</p><p>Similarly, b is the largest number in [0, p -1) such that r q b (x) ≤ r(x, y), otherwise b = p -1. 9: Output: Calibrated Reward Function: C r,π ref that assigns a calibrated reward for all prompts in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Better calibration leads to better gains</head><p>The effectiveness of calibration can be better understood when we approximate the calibration with fewer number of rollouts: n. When n = 1, we take one other rollout from the base policy model and use that as a reference reward, similar to <ref type="bibr" target="#b48">Wang et al. (2024)</ref>, and with n = 3, 5, 7, we approximate the median with this limited sample, and do the reward calibration. In Hence, we find that as the calibration error is reduced, we achieve better KL vs win rate tradeoffs, thus able to better implicitly approximate the best-of-n model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Calibration Reduces Reward Hacking</head><p>We demonstrate that calibrated reward models are less susceptible to reward hacking, a phenomenon where models exploit spurious correlations in training data to optimize for reward signals instead of true task objectives.</p><p>To induce reward hacking, we injected specific phrases into the start of preferred responses of our preference datasets: "Sorry, I can't help you with that" for Harmlessness and "Sure" for Helpfulness. We then evaluated the model's accuracy on a poisoned evaluation set where these phrases were inverted (added to the unpreferred responses). A significant drop in accuracy on this poisoned set would indicate reward hacking: a reliance on the spurious correlation.</p><p>Figure <ref type="figure">6</ref> shows that calibrated reward models are far more robust to these manipulated correlations, maintaining higher accuracy compared to uncalibrated models. Note that the calibration mechanism used here is different from the median-based calibration, and instead uses a pointwise loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-print</head><p>Figure <ref type="figure">5</ref>: (a) Calibration improves KL vs win rate tradeoff even when we approximate calibration of median approximated with n=7 samples, whereas with n=1,3,5, the gains in win rate for a fixed KL-budget is lower, but still outperforms the uncalibrated model at higher KL values. Using n=100 samples is (calibrate median pareto) provides the best tradeoff (b) Calibration with two or more anchor points per-prompt improves KL-winrate tradeoff, as compared to median-based approximation -however there is diminishing gains with ten anchor points as compared to using two anchor points. The win rate is measured using the PaLM-2 M reward model trained on the Anthropic helpfulness preference dataset.</p><p>Figure <ref type="figure">6</ref>: Calibrated reward models demonstrate robustness against reward hacking: We poisoned the training data by adding phrases to the preferred response to induce spurious correlations. When we evaluated against a test set where the correlations are inverted (phrase added to unpreferred models), calibrated models maintained higher accuracy than uncalibrated ones, demonstrating their reduced reliance on spurious correlations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Best-of-N and worst-of-N win rate vs KL tradeoff curves for N = 2, 4 with different transformation functions.</figDesc><graphic coords="9,108.00,387.97,190.07,152.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 ,</head><label>1</label><figDesc>we consider an ideal continuous language model and compare different alignment objectives on inference-time win rate vs KL divergence for three inference-time procedures: {standard, BoN, WoN }. The tradeoff curves are obtained by varying the strength of the regularizer, β. The different objectives correspond to different transformations of the calibrated reward, which include (1) identity mapping Φ = I(•) (IPO); (2) logarithmic mapping Φ = log (best-of-N distillation); (3) exponential tilting Φ t (•) as defined in Eq. (15); (4) optimized transformation function by solving for fixed points in Corollary 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2</head><label>2</label><figDesc>Offline Approximate Calibration Require: Base policy π ref , (uncalibrated) reward model r with range [0, 1], training prompts x ∈ X , number of samples per prompt m. 1: for all x ∈ X do 2: Roll out m i.i.d. samples from π ref per training prompt x: z j , j ∈ {1...m}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>r</head><figDesc>median (x) ← median(r(x, z 1 ), . . . , r(x, z m )). 4: end for 5: Define the calibrated reward function C r,π ref : X × Y → [0, 1] as follows: log C r,π ref (x, y) = log r(x, y) • log 0.5 log r median (x) . 6: Output: Calibrated reward function C r,π ref that assigns a calibrated reward for all prompts in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results on reward models trained on the Anthropic helpfulness preference dataset. (left) Scatter plot of reward scores and best-of-n ranks on a random sample of 10 prompts in the Anthropic helpfulness dataset. Note that the model shows miscalibration on most prompts, with the degree of miscalibration varying by prompt. (right) Plot of CDF of absolute error (AE). Observe that per-prompt quantile based methods have low AE with high probability, where as prompt-agnostic transformations have high AE typically.</figDesc><graphic coords="12,124.30,81.86,178.20,123.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>, BonD (Sessa et al., 2024), and IPO (Azar et al., 2023) as baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Calibration improves Best-of-N and Worst-of-N win rates for helpfulness (left) and harmlessness (right) objectives in the Anthropic dialog dataset respectively. We report win rate against the Best-of-N and Worst-of-N rewards of the base SFT model responses on the test split as measured by the PaLM-2 M reward model trained on the helpfulness and harmlessness data respectively.</figDesc><graphic coords="14,122.82,86.84,178.20,136.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>KL-regularized reward optimization (RO). Let r be a reward function that determines the reward for each (x, y). Let L ro (θ) := E x∼µ E y∼π θ r(x, y). Then,θ * bilevel,β = arg min θ L bilevel,β (θ) where L bilevel (θ) := D KL (π θ ∥p) + 1 β L ro (θ), (21)whereD KL (π θ ∥p) = E x∼µ D KL (π θ (•|x)∥p(•|x)).Multi-tasking SFT and RO. Now consider the following tasksθ * multi-task,β = arg min θ L multi-task,β (θ) where L multi-task (θ) := L sft (θ)+ 1 β L ro (θ). (22)Theorem 5. For all β ∈ R, we have θ * bilevel,β = θ * multi-task,β .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 3</head><label>3</label><figDesc>Offline Approximate Calibration with arbitrary anchor points Require: Base policy π ref , (uncalibrated) reward model r, training prompts x ∈ X , number of quantiles p, number of samples per prompt m. 1: for all x ∈ X do 2:Roll out m i.i.d. samples from π ref per training prompt x: z j , j ∈ {1...m}.3:for all ℓ ∈ {0...p -1} do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>Define the calibrated reward functionC r,π ref (x, y) : X × Y → [0, 1] as follows: log C r,π ref (x, y) = log r(x, y) • log qa+q b 2 log rq a (x)+rq b (x) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>Fig 5(a), we find that calibration using more number of samples improves the win rate, with n = 1 providing no gains in the KL vs win rate tradeoff. Additionally, in Fig 5(b), when we calibrate with more anchor points, we find that the KL vs win rate tradeoffs improves as compared to the median-based calibration. This is further supported by the fact that with more anchor points, the calibration error is significantly reduced as shown in Fig 2(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="28,177.30,369.83,257.39,276.27" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The solution to this optimization problem is unique(Korbak et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2022;<ref type="bibr" target="#b36">Rafailov et al., 2023;</ref><ref type="bibr" target="#b51">Yang et al., 2024)</ref>.2 The definition is similar to the cumulative density function of the reward r(x, y) under policy π except for how ties are decided.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>One question that arises is the role of the KL divergence regularizer in Eq. (5). We argue that the regularizer essentially enables multi-tasking between the SFT task and the RL task, which we formally prove for log-linear models in Appendix B. In other words, the KL divergence regularizer enables to preserve the core capabilities of the model while acquiring a new one through the KL-RL process.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>CTRL: Calibrate-and-Transform Reinforcement LearningIn this section, we propose the CTRL method, which is our proposed solver for the inferencetime win rate optimization problem. Recall from the previous section that the proposed solution could be decomposed into three stages: reward calibration and reward transformation followed by the standard KL-RL solver. In the previous section, we highlighted the reward transformation. In the rest of this section, we focus on approximate empirical calibration. Combining it with reward transformation yields the final CTRL algorithm, given in Algorithm 1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Below we add an additional 1 2 1 {r(x, y) = r(x, z)} to the win r.v for simplicity, which won't affect the result for continuous LMs.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Missing proofs A.1 Proof of Lemma 1</head><p>We start by stating the policy obtained by KL-regularized RL problem, which can be obtained by standard arguments in the literature (e.g., <ref type="bibr" target="#b23">(Korbak et al., 2022;</ref><ref type="bibr" target="#b36">Rafailov et al., 2023;</ref><ref type="bibr" target="#b51">Yang et al., 2024)</ref>). Lemma 6. The solution to the optimization problem in Definition 1 satisfies</p><p>Proof of Lemma 1. Let π * T be a solution to the optimization problem in Eq. ( <ref type="formula">5</ref>). Note that for all y such that π * T (y | x) &gt; 0, we must have π ref (y | x) &gt; 0 since otherwise the KL divergence would become infinite. <ref type="formula">6</ref>) leads to the solution π * T being its optimal solution as shown in Lemma 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 1</head><p>We first prove the following auxiliary lemma that we need in the proof. Lemma 7. For any inference-time procedure T , we have</p><p>Proof. The proof follows from the following identities:</p><p>Proof of Theorem 1. Note that Eq. ( <ref type="formula">5</ref>) has an implicit constraint that π(• | x) must be a valid distribution, i.e.</p><p>Hence adding the Lagrangian multiplier, we get the following Lagrangian form</p><p>By method of Lagrange multipliers, we have that the solution to Eq. ( <ref type="formula">5</ref>) must be a stationary point of L(π(• | x), α). And hence</p><p>And hence</p><p>It remains to prove Eq. ( <ref type="formula">9</ref>). Plugging in π 1 = π, π 2 = π ref to Lemma 7, and taking partial derivative with respect to π(y | x) on the right hand side completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Lemma 3</head><p>The proof follows from the fact that for all x and y</p><p>where (17) follows from monotone increasing property of g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof of Lemma 4</head><p>To show that C r,π ref (x, y) ∼ Unif([0, 1]), it would be enough to show ∀u ∈ [0, 1],</p><p>completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Proof of Theorem 2</head><p>In this section, we will prove an extended version of Theorem 2 below. Theorem 4 (Extended version of Theorem 2). If T is a calibrated inference-time procedure with mapping function g T , for any continuous language model π, β &gt; 0 and reward transformation function Φ, we have that</p><p>And hence they are independent of r and π ref .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-print</head><p>Proof. In the proof, we consider the two language models π * RΦ,β and π ref in the space of calibrated reward against the base policy π ref . For any policy π, let C r,π ref • π(• | x) be a distribution over [0, 1] that outputs the calibrated reward C r,π ref (x, y) of the sample y sampled from π(• | x). Then by Lemma 4,</p><p>Similarly, using Lemma 6, it can be shown that C r,π ref • π * RΦ,β follows the distribution with density ∀u ∈ [0, 1],</p><p>Note that since r assigns distinct rewards to different y's and C r,π ref is a monotone transformation of r, we have that</p><p>After the inference-time procedure T is applied, we have that inference-time base policy satisfies</p><p>For the inference-time aligned policy, we have that:</p><p>which is defined as F Φ,β (u). And hence we have</p><p>.</p><p>Thus, the inference-time win rate satisfies 4</p><p>, completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Proof of Theorem 3</head><p>The KL divergence is the same as the KL divergence in Theorem 4. The win rate can be obtained by plugging g BoN (u) = u N -1 and g WoN (u) = (1 -u) N -1 (as shown in Lemma 5) into Theorem 4.</p><p>Pre-print</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Zero-shot prompt</head><p>We also evaluate the win rate using a prompted PaLM-XL model with the following instructions:</p><p>You are a helpful assistant, that ranks AI assistants' responses by the quality of their answers.</p><p>The AI assistants try to be helpful, sophisticated, emotionally aware, and humble-butknowledgeable. Below are a series of dialogues between various people and an AI assistant, and the assistant tries to reply to the dialogue. I want you to rank the responses of assistants. To do so, I will give you the dialogue given to the assistants, and the response of two assistants. Please rank the assistants based on which response would be more helpful, sophisticated, emotionally aware, and humble-but-knowledgeable. All inputs are python dictionaries.</p><p>Here is the prompt: { "dialogue": {dialogue}, } Here are the outputs of the assistants: [ { "assistant": "assistant 1", "answer": {output 1} }, { "assistant": "assistant 2", "answer": {output 2} } ] Respond 1 or 2 to indicate the better output. Please provide the ranking that the majority of humans would give.</p><p>Pre-print As can be seen, in Fig. <ref type="figure">7</ref>, for N = 32, we see better gains as compared to N = 2 in both BoN and WoN win-rates on Anthropic helpfulness and harmlessness datasets. Thus, as more inference-time compute becomes available in the future, we find that InfAlign can scale and further augment the performance gains.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Variational best-of-n alignment</title>
		<author>
			<persName><forename type="first">Afra</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.06057" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in ai safety</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<title level="m">Palm 2 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A general theoretical paradigm to understand learning from human preferences</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Calandriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12036</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01879</idno>
		<title level="m">Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large language monkeys: Scaling inference compute with repeated sampling</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Juravsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><surname>Mirhoseini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21787</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PPL-MCTS: Constrained textual generation through discriminator-guided MCTS decoding</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Claveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewa</forename><surname>Kijak</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.215</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.215" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>
			<persName><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ivan</forename><surname>Vladimir</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Meza</forename><surname>Ruiz</surname></persName>
		</editor>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="2953" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Souradip</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Suvra Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrit</forename><forename type="middle">Singh</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20495</idno>
		<title level="m">Transfer q star: Principled decoding for llm alignment</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Robey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Dobriban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">J</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08419</idno>
		<title level="m">Jailbreaking black box large language models in twenty queries</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coarse-to-fine n-best parsing and MaxEnt discriminative reranking</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219862</idno>
		<ptr target="https://aclanthology.org/P05-1022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</editor>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning from human preferences</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<idno type="DOI">10.1162/0891201053630273</idno>
		<ptr target="https://aclanthology.org/J05-1003" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reward model ensembles help mitigate overoptimization</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Coste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usman</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.02743</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chirag</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">D</forename><surname>'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><surname>Berant</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.09244" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicky</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chirag</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.19316</idno>
		<title level="m">Robust preference optimization through reward model distillation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling laws for reward model overoptimization</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10835" to="10866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bonbon alignment for large language models and the sweetness of best-of-n sampling</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Gârbacea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Veitch</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.00832" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A real-world webagent with planning, long context understanding, and program synthesis</title>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">V</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Safdari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Faust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Measuring Goodhart&apos;s law</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://openai.com/research/measuring-goodharts-law" />
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">John</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aengus</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rylan</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fazl</forename><surname>Barez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Sleight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinank</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03556</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Best-of-n jailbreaking. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Best-of-n jailbreaking</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aengus</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rylan</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fazl</forename><surname>Barez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Sleight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinank</forename><surname>Sharma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.03556" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">L</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11275</idno>
		<title level="m">RL with KL penalties is better viewed as Bayesian inference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tilted empirical risk minimization</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On tilted losses in machine learning: Theory and applications</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">142</biblScope>
			<biblScope unit="page" from="1" to="79" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-refine: Iterative refinement with self-feedback</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ted</forename><surname>Moskovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aaditya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Strouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Sandholm</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Anca D Dragan</surname></persName>
		</author>
		<author>
			<persName><surname>Mcaleer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04373</idno>
		<title level="m">Confronting reward model overoptimization with constrained rlhf</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Sidharth</forename><surname>Mudgal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Alex Beutel, and Ahmad Beirami. Controlled decoding from language models. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Webgpt: Browser-assisted question-answering with human feedback</title>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.09332" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to reason with llms</title>
		<author>
			<persName><forename type="first">Pre-Print</forename><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/index/learning-to-reason-with-llms/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03544</idno>
		<title level="m">The effects of reward misspecification: Mapping and mitigating misaligned models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><surname>Yuanzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.08714</idno>
		<title level="m">Reward gaming in conditional text generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">K-best A* parsing</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P09-1108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<editor>
			<persName><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jian</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</editor>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08">August 2009</date>
			<biblScope unit="page" from="958" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.16033</idno>
		<title level="m">Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Remarks on a multivariate transformation</title>
		<author>
			<persName><forename type="first">Murray</forename><surname>Rosenblatt</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2236692" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<idno type="ISSN">00034851</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="470" to="472" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">To beam or not to beam: That is a question of cooperation for language gans</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Alexis</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="26585" to="26597" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léonard</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nino</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Vieillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Ramé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Shariari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sertan</forename><surname>Cideron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Stanczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danila</forename><surname>Michi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabela</forename><surname>Sinopalnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amélie</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Héliou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Momchev</surname></persName>
		</author>
		<author>
			<persName><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><surname>Bond</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.14622" />
		<title level="m">Aligning llms with best-of-n distillation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Defining and characterizing reward gaming</title>
		<author>
			<persName><forename type="first">Joar</forename><surname>Skalse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Krasheninnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9460" to="9471" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scaling LLM test-time compute optimally can be more effective than scaling model parameters</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03314</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillon</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvis</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sana</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Svegliato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Watkins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10260</idno>
		<title level="m">A strongreject for empty jailbreaks</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transforming and combining rewards for aligning large language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chirag</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">D</forename><surname>'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Veitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">From decoding to meta-generation: Inferencetime algorithms for large language models</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Bertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.16838</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">An empirical analysis of compute-optimal inference for problem-solving with language models</title>
		<author>
			<persName><forename type="first">Yangzhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00724</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fine-grained human feedback gives better rewards for language model training</title>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Zhihui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18711</idno>
		<title level="m">Calibrating reasoning in language models with internal consistency</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Joy</forename><surname>Qiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Salamatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziteng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.01730</idno>
		<title level="m">Asymptotics of language model alignment</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Bengio</forename><surname>Yohsua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Privitera</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besiroglu</forename><surname>Tamay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bommasani</forename><surname>Rishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choi</forename><surname>Yejin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goldfarb</forename><surname>Danielle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidari</forename><surname>Hoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalatbari</forename><surname>Leila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longpre</forename><surname>Shayne</surname></persName>
		</author>
		<title level="m">International Scientific Report on the Safety of Advanced AI</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
		<respStmt>
			<orgName>Department for Science, Innovation and Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Probabilistic inference in language models via twisted sequential monte carlo</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Brekelmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grosse</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Calibrating sequence likelihood improves conditional language generation</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
