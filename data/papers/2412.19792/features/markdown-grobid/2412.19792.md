# InfAlign: Inference-aware language model alignment

## Abstract

## 

Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the win rate of a sample from the aligned model over a sample from the reference model is high, subject to a KL divergence constraint. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. However, the alignment objective does not capture such inference-time decoding procedures. We show that the existing alignment framework is sub-optimal in view of such inferencetime methods. We then modify the alignment objective and propose a framework for inference-aware alignment (InfAlign). We prove that for any inference-time decoding algorithm, the optimal solution that optimizes the inference-time win rate of the aligned policy against the reference policy is the solution to the typical RLHF problem with a transformation of the reward. This motivates us to provide the KL-regularized calibrate-andtransform RL (CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. We particularize our study to two important inference-time strategies: best-of-N sampling and bestof-N jailbreaking, where N responses are sampled from the model and the one with the highest or lowest reward is selected. We propose specific transformations for these inference-time strategies and demonstrate that our framework offers significant improvements over existing state-of-the-art methods for language model alignment. Empirically, we outperform state-ofthe-art methods that are designed without taking inference-time decoding into consideration by 8-12% and 4-9% on inference-time win rates over the Anthropic helpfulness and harmlessness dialog benchmark datasets.

* Equal contribution.

## Introduction

Aligning generative language models (LMs) through KL-regularized reinforcement learning (KL-RL) is a widely adopted framework for finetuning a generative language model to improve a reward (e.g., safety or quality). Solving KL-RL generally entails training a reward model, and then using an RL solver [(Christiano et al., 2017;](#b12)[Stiennon et al., 2020;](#b45)[Ouyang et al., 2022)](#b31). Other ways of solving KL-RL include variants of direct preference optimization [(Rafailov et al., 2023;](#b36)[Azar et al., 2023)](#b3), reward model distillation [(Fisch et al., 2024)](#b16), and Best-of-N distillation [(Yang et al., 2024;](#b51)[Gui et al., 2024;](#b18)[Amini et al., 2024;](#b0)[Sessa et al., 2024)](#b41). The success of the KL-RL framework is typically measured through the win rate of the aligned model over the reference model for a given task, which captures how often a sample drawn from the aligned model wins against a sample drawn from the base model using a judge for that task.

Pre-print However, rarely is the aligned model used as is at inference time; instead an inference-time procedure is typically used to accomplish a task. For example, it is customary to perform one or more of the following simple or complex procedures at decoding: best-of-N sampling [(Nakano et al., 2022;](#b29)[Beirami et al., 2024)](#b5), best-of-N jailbreaking [(Hughes et al., 2024b)](#), chain-of-thought reasoning [(Wei et al., 2022;](#b49)[OpenAI, 2024)](#b30), and self consistency [(Wang et al., 2022a)](#) (see Section 7 for more). This creates a discrepancy between the inference-time decoding procedure and the training KL-RL objective, which is to maximize the expectation of a function of reward (e.g., win rate) of a sample from the aligned model against that of the base model.

In this paper, we address the following question: Given a known inference-time procedure, can we align a model to optimize for the inference-time win rate against the reference model, where inference-time win rate entails obtaining a response from each model through the inference-time procedure and counting which sample wins? While directly optimizing the inference-time win rate seems intractable, we show that the optimal solution is captured via a family of optimization objectives, which we call the inference-aware alignment (InfAlign) framework. We further prove that for a δ-bound language model, where the likelihood of all outcomes are upper-bounded by δ, as δ → 0, the solution could be obtained by solving KL-RL with a specific transformation of the reward (Lemma 1). Therefore, the challenge of optimizing for inference-time win rate can be captured by designing a reward transformation that is suited to the specific inference-time procedure, and solving KL-RL using existing optimization algorithms like PPO [(Schulman et al., 2017)](#b39).

We particularize the study of InfAlign to two simple yet popular inference-time strategies: Best-of-N (BoN) sampling [(Nakano et al., 2022;](#b29)[Beirami et al., 2024)](#b5) and Best-of-N jailbreaking [(Hughes et al., 2024a)](#), which we call Worst-of-N (WoN) since the defender may assume that the attacker is choosing the worst outcome of N samples. Despite their simplicity, BoN is extremely effective, and even known to be an effective procedure for inference-time alignment [(Beirami et al., 2024;](#b5)[Gui et al., 2024;](#b18)[Mudgal et al., 2024)](#b28). WoN is a popular safety evaluation strategy often adopted to evaluate the ability to jailbreak foundation models [(Yohsua et al., 2024;](#b56)[Chao et al., 2023;](#b9)[Souly et al., 2024;](#b44)[Hughes et al., 2024b)](#). Hence, there is both theoretical and practical motivation for training to optimize inference-time BoN and WoN win rates. For these procedures, we solve InfAlign for different values of N in the limit of infinitely expressive language models. We also show that exponential reward transformation is almost optimal for these strategies. We further show that the win rate optimal solution to KL-RL without reward transformation leads to suboptimal policies for inference-time win rate. Motivated by our theoretical findings, we propose a practical solver for InfAlign, called Calibrate-and-Transform Reinforcement Learning (CTRL), which adopts a three-step approach:

(1) we first calibrate the scores of the reward model with respect to responses sampled perprompt by the reference model; (2) we then adopt a suitable transformation of the calibrated scores for a given inference-time procedure; and (3) we solve the KL-RL problem (e.g., using PPO).

We apply CTRL to the Anthropic helpfulness and harmlessness dataset [(Bai et al., 2022)](#b4) for optimizing BoN helpfulness and WoN harmlessness with various values of N. We show that the (close-to-optimal) exponential reward transformations that were derived theoretically on idealized distributions transfer to real-world tasks -outperforming various KL-RL solvers at inference-time win rate. Finally, we also show CTRL (without any reward transformation) is a strong baseline for optimizing the standard win rate vs KL trade-off against various methods.

## Our contributions are as follows:

• We propose a framework for inference-aware language model alignment (InfAlign) that captures the optimal solution to optimizing win rate in view of any wellbehaving inference-time procedure. Theoretically, we show that this framework satisfies a coupled-transformed reward/policy optimization objective which lends itself to iterative optimization for a large class of inference-time procedures (see Section 3).

• Motivated by our theoretical findings, we propose a practical solver, calibrate-andtransform RL (CTRL) for InfAlign, which consists of an offline reward calibration and transformation step prior to applying the regular KL-RL framework. For two popular inference-time procedures, namely optimizing Best-of-N (BoN) sampling and preventing Best-of-N jailbreaking (WoN), we derive the conditions for optimal theoretical solutions to alignment in a toy setting. Somewhat surprisingly, for BoN and WoN, the optimal solution is independent of the base policy and the reward model, which leads to an iterative optimization approach that can be computed offline (see Section 4 and Section 5).

• Empirically, we show on Anthropic dialog helpfulness and harmlessness datasets that CTRL with identity reward transformation achieves competitive performance compared to a variety of SOTA methods for optimizing standard win rate [(Gui et al., 2024;](#b18)[Amini et al., 2024;](#b0)[Sessa et al., 2024;](#b41)[Azar et al., 2023;](#b3)[Rafailov et al., 2023)](#b36), and produces better inference-time win rate vs KL tradeoffs by 8-12% for BoN and 4-9% for WoN inference-time procedures respectively (see Section 6).

## Problem setup

We consider a generative language model that produces a response conditioned on an input prompt. Given a prompt x, e.g., x = What is a large language model?, a generative language model π ref returns a response y, which is sampled according to the distribution

$π ref (• | x).$We use X and Y to denote the space of possible inputs and outputs, respectively. Throughout the paper, we refer to the conditional distribution π(• | x) introduced by the language model as a policy, and assume π ref is a fixed base policy, e.g., obtained from supervised finetuning. We will often use the notation π(y

$| x) ∝ f (y) for some f : Y → R + to denote the conditional distribution π(y | x) = f (y)/( y f (y)dy) obtained after normalization.$Alignment of language models. Let r : X × Y → R be a reward function that assigns a scalar value to any (prompt, response) pair, e.g., a model trained side-by-side on human preferences. The reward determines the goodness of response y in context x. The goal of language model alignment is to construct an aligned distribution π that improves the reward of the response while being close to π ref .

Below we introduce KL-regularized reinforcement learning (RL), a popular training-time alignment framework in the literature.

Definition 1 (KL-regularized RL framework). Let β > 0 be a regularization parameter, the KL-regularized RL problem aims to maximize the pointwise expected reward with a KL regularizer below: ∀x,

$π * r,β (• | x) = arg max π E y∼π(•|x) {r(x, y)} -βD KL (π(• | x)∥π ref (• | x)) . 1(1)$When evaluating an aligned policy, a common measure to use is the win rate over the base policy π ref .

Let

$w r (y, z | x) := 1 {r(x, y) > r(x, z)} + 1 2 1 {r(x, y) = r(x, z)}$be the win random variable under reward r.

Definition 2 (Calibrated reward). Let C r,π (x, y) be the calibrated reward 2 under policy π defined below C r,π (x, y)

$:= E z∼π(•|x) w r (y, z | x).(2)$The win rate is defined as following:

Pre-print Definition 3 (Win rate). For any policy π 1 and π 2 , we define win rate of policy π 1 over policy π 2 given prompt x, as measured by reward r as follows:

$W r (π 1 ≻ π 2 | x) := E y∼π1(•|x) C r,π2 (x, y).(3)$In this paper, for simplicity of the presentation and analysis, we assume that the language model is continuous, whose outcomes have infinitesimally small probabilities. In other words, we assume Y is a continuous set, which can be mapped to [0, 1] through a CDF inverse transformation [(Rosenblatt, 1952)](#b38), with the ordering determined by the reward of the outcomes from the smallest reward to the highest reward. We also assume that r assigns distinct rewards to different y's for a given x. These assumptions have been made in the past implicitly by [Hilton & Gao (2022)](#b20) to estimate the KL divergence of Best-of-n policy, and by [Gui et al. (2024)](#b18) to estimate its win rate and characterize the KL divergence and win rate of optimal policy for win rate vs KL divergence tradeoffs. While these assumptions lead to approximations when analyzing real-world distributions, the results derived under them are reasonably tight when the actual likelihood of the language model outcomes are small [(Beirami et al., 2024)](#b5). Note that we don't make any such assumptions when providing our algorithmic developments, and the experimental results. Under these assumptions, we define the quantile mapping, which is the reverse of the calibrated reward mapping C -1 , satisfying ∀u ∈ [0, 1],

$C -1 r,π,x (u) = y u,x where C r,π (y u,x | x) = u.$Inference-time processing. As mentioned earlier, in many cases we do not simply just sample from the language model. When decoding is done through an inference-time procedure, the obtained sample follows a transformed distribution that depends on the aligned policy and the inference-time procedure. For example, when the inference-time procedure is BoN from the base policy π, it can be shown that the final obtained sample is distributed proportional to π(y | x)C r,π (x, y) N -1 [(Beirami et al., 2024, Lemma 1)](#). In this work, we model inference-time processing as a procedure T that maps π to a distribution T (π),

$π T -→ T (π).$In these cases, it is customary to compare the models by considering the following inferencetime win rate of the aligned policy π.

Definition 4 (Inference-time win rate). Under inference-time processing T , the inferencetime win rate of policy π 1 over π 2 is defined as

$W T r (π 1 ≻ π 2 | x) := E y∼T (π1)(•|x),z∼T (π2)(•|x) {w r (y, z | x)} .(4)$With the above definitions at hand, the goal of the paper is solve the following KL-regularized inference-time win rate maximization problem.

Definition 5 (Inference-time win rate KL-regularized RL problem). Let T be a given inference-time procedure and β > 0. Then, the optimization problem for maximizing inferencetime win rate is posed as

$max π W T r (π ≻ π ref | x) -βD KL (π(• | x)∥π ref (•|x)) .(5)$Note that this formulation reduces to the IPO objective (Azar et al., 2023, Equation ( [8](#))) when T is the identity transformation and extends it otherwise for an arbitrary inference-time procedure.[foot_2](#foot_2)

3 Reinforcement learning with reward transformation

In this section, we propose a general framework for solving the language model alignment problem defined in Definition 5. Our approach is based on designing a new reward function R based on the reward model r, the inference-time procedure T , and the base policy π ref , such that solving the KL-regularized RL problem (Definition 1) with the transformed reward R leads to an almost optimal solution to Eq. ( [5](#formula_9)). More precisely, the aligned policy is the solution to the following optimization problem:

$max π E x∼µ,y∼π(•|x) {R r,π ref ,T (x, y) -βD KL (π(• | x)∥π ref (• | x))},(6)$where R r,π ref ,T (x, y) is a transformed reward function. At first, it might not be immediately clear how Eq. ( [6](#formula_10)) might help solve the problem in Eq. ( [5](#formula_9)), however, we will show that for any inference-time procedure T , there exists a transformed reward R, which solves Eq. ( [5](#formula_9)).

Lemma 1. For any base policy π ref , reward model r, inference-time procedure T , and β > 0, there exists a reward function R r,π ref ,T such that the solution to Eq. (6) solves the optimization problem in Eq. (5) (Definition 5).

In general, such optimal reward transformation will depend on the base policy π ref , the post-hoc procedure T , and the reward function r. In the lemma below, we list the property that the reward transformation and the resulting optimal aligned policy must satisfy.

Theorem 1 (Characterization of InfAlign solution). Assuming that T is such that ∂T (π)(y 1 | x)/∂π(y 2 | x) exists for all x, y 1 , y 2 , then we have the optimal transformed reward R and the optimal policy π * in Eq. (5) must satisfy the following coupled equations ∀x, y

$π * (y|x) ∝ π ref (y | x)e 1 β R(x,y) (7) R(x, y) = ∂ ∂π(y | x) W T r (π ≻ π ref | x) (8) = z C r,T (π ref ) (x, z) ∂ ∂π(y | x) T (π)(z | x),(9)$where C r,T (π ref ) (x, z) is the calibrated reward under the inference-time transformed policy.

Missing proofs are presented in Appendix A. Theorem 1 naturally leads to an iterative EM-style algorithm that (I) updates π with R fixed based on Eq. ( [7](#)) and (II) updates R with π fixed based on Eq. ( [9](#formula_11)) until convergence. However, such algorithm suffers from two drawbacks: first, for general language models, it is inefficient/intractable to evaluate Eq. ( [9](#formula_11)) since it involves evaluating the policy on a large, or even infinite output space; second, it is unclear whether such an algorithm could lead to the optimal solution.

To find more efficient ways to design reward transformations, we examine the case when no inference-time procedure is performed. In this case, T (π) = π and

$∂ ∂π(y | x) T (π)(z | x) = 1 {z = y} .$Eq. ( [9](#formula_11)) will reduce to R(x, y) = C r,π ref (x, y), the CDF or calibrated reward under π ref .

Corollary 1. When no inference-time procedure is performed, i.e. ∀π, T (π) = π, the solution to Eq. (6) with R(x, y) = C r,π ref (x, y) is the solution to Eq. (5). Note that the above corollary is also observed in [Azar et al. (2023)](#b3); [Gui et al. (2024)](#b18). Hence Theorem 1 can be viewed as a generalization of these results with general inference-time procedures. The observation motivates us to consider a family of reward transformations based on this calibrated reward, described in the next section. As we will see, for the class of calibrated inference-time procedures (Definition 6), different transformations in such family could be efficiently evaluated through a toy language model, which enables the search for good or even optimal transformations.

## Pre-print 4 Towards solving InfAlign

We first state a few properties of reward calibration in Section 4.1. Then, in Section 4.2, we demonstrate how this approach enables efficient evaluations of different transformations for calibrated inference-time procedures. We will then use best-of-N and worst-of-N as examples of post-hoc procedures to demonstrate the effectiveness of such an approach in Section 4.3. Missing proofs in the section are presented in Appendix A.

## Reward calibration

Recall the definition of the calibrated reward in Definition 2. In this section, we assume that C r,π ref is already obtained and discuss methods to approximate C r,π ref in Section 5. We state a few properties of C r,π ref below. The first property states that reward calibration preserves the ordering of the reward. This implies that the win rate evaluated under the calibrated reward stays unchanged.

Lemma 2 (Calibration is a bounded monotone increasing transformation of reward). We have C r,π ref (x, y) ∈ [0, 1]. Furthermore, we have for any y and z

$r(x, y) ≥ r(x, z) =⇒ C r,π ref (x, y) ≥ C r,π ref (x, z)(10)$Moreover, we have that C r,π ref is a canonical transformation of reward, and is invariant under all monotone increasing transformations of the reward function, stated below.

Lemma 3 (The calibrated reward is invariant under monotone increasing transformations of reward). Let m : R → R be any monotonic increasing function. Then,

$C m(r),π ref = C r,π ref .(11)$In particular, this also immediately implies that

$C Cr,π ref ,π ref = C r,π ref .$This property is useful since as long as the learned reward model r can capture relative human preference between pairs, the calibration of r will be the same. Hence C is more robust to the learning process of r.

The next property shows that the calibration operation allows us to transform the distribution of the reward under the base policy to a uniform distribution over [0, 1] regardless of the base policy π ref and the reward model r.

Lemma 4. If π is a continuous language model, let y be sampled from

$π ref (• | x), then we have ∀x, C r,π ref (x, y) ∼ Unif([0, 1]).$The lemma provides us a simple, unified view of the output from a language model through the space of calibrated reward, which is independent from the base policy, and the reward model.

## KL-regularized RL with the calibrated reward

Next we discuss how the calibrated reward can be used in KL-regularized reinforcement learning. As Lemma 4 suggests, after calibration, the reward distribution of the outputs from the base policy is independent from the reward model and the base policy itself. This allows us to design a transformation function Φ, focusing only on the inference-time procedure T , to be applied on top of the calibrated reward function, independent of π ref and r.

More precisely, let Φ : [0, 1] → R be a transformation function, we propose the following reward function

$R Φ (x, y) = Φ(C r,π ref (y | x)),(12)$and we would like the aligned policy to be the solution to the KL-regularized RL problem defined in Definition 1 with reward R Φ (x, y),

$π * RΦ,β (• | x) = arg max π E y∼π(•|x) {R Φ (x, y)} -βD KL (π(• | x)∥π ref (• | x)).(13)$Inference-aware reward transformation. For a given inference-time procedure T , our goal is to derive or design a suitable transformation Φ, such that the solution leads to a good or even optimal trade-off between the inference-time win rate W T and the KL divergence from the base policy.

Standard win rate (no inference-time procedure). When no inference-time procedure is employed (i.e., T is the identity mapping), W T reduces to the standard win rate. Setting Φ to be the identity transformation leads to the optimal win rate vs KL trade-off curve by noting that

$E y∼π(•|x) {C r,π ref (x, y)} = W r (π ≻ π ref | x).$In this case, Eq. ( [13](#formula_18)) will be the same as the alignment objective of Azar et al. ( [2023](#)). Moreover, it can also be shown that when Φ(•) = log(•), the solution π * RΦ,β recovers the popular best-of-N distillation objective, which has been studied in a recent line of works [(Gui et al., 2024;](#b18)[Amini et al., 2024;](#b0)[Sessa et al., 2024)](#b41), and shown to be nearly win rate optimal [(Yang et al., 2024;](#b51)[Gui et al., 2024)](#b18). We also note that while these methods lead to similar optimization objectives, the algorithmic approaches to solve the problems are different. In Section 5, we will discuss a unified algorithm to solve Eq. ( [13](#formula_18)) for general Φ. In Section 6, we compare the results for Φ set to the identity mapping with the abovementioned baseline approaches.

We consider a family of inference-time procedures that only depend on the calibrated reward of the outputs, which we term calibrated procedures, and discuss how to design a suitable Φ for this family of transformations. We first define calibrated procedures below. Definition 6 (Calibrated inference-time procedure). An inference-time procedure T is called a calibrated procedure if there exists a mapping function g T : [0, 1] → R such that for any π, r, and x, y, we have

$T (π)(y | x) ∝ π(y | x) • g T (C r,π (x, y)).$Our next result shows that for calibrated inference-time procedures, the aligned policy from solving Eq. ( [13](#formula_18)) has a win rate and KL divergence independent of the base policy and reward function.

Theorem 2 (Model-agnostic property of calibrated inference-time procedures, informal version of Theorem 4). If T is a calibrated inference-time procedure, for any continuous language model π, β > 0 and reward transformation function Φ, we have that both

$W T r (π * RΦ,β ≻ π ref | x) and D KL (π * RΦ,β ∥π ref ) are independent of r and π ref .$The above theorem allows us to evaluate a transformation Φ by focusing on simple continuous language models that are easy to compute and simulate. In the next section, we will use two popular inference-time procedures, best-of-N and worst-of-N , as examples to demonstrate how the theorem enables us to efficiently evaluate the inference-time win rate vs KL divergence tradeoff curve for different Φ functions, which could be used to find a suitable transformation Φ in practical scenarios.

## Finding better transformations for BoN and WoN

In this section, we focus on the following two inference-time procedures.

Best-of-N inference-time procedure (BoN

). During inference, N i.i.d. responses from a policy π are generated, i.e., y 1 , . . . y N ∼ π. The final output is the one with the highest reward, i.e., y BoN = arg max y∈{y1,...y N } r(x, y). Worst-of-N inference-time procedure (WoN). Let y 1 , . . . y N be N i.i.d. draws from policy π. The final output is the one with the lowest reward. i.e., y WoN = arg min y∈{y1,...y N } r(x, y).

The lemma below presents the distribution of outputs after the inference-time procedure is performed.

## Pre-print

Lemma 5. For any N and continuous language model π,

$BoN(π)(y | x) ∝ π(y | x) • C r,π (x, y) N -1 . WoN(π)(y | x) ∝ π(y | x) • (1 -C r,π (x, y)) N -1 .$Note that the results for BoN have already been derived previously [(Beirami et al., 2024;](#b5)[Gui et al., 2024;](#b18)[Amini et al., 2024)](#b0). The lemma shows that these two inference-time procedures are calibrated procedures so that as claimed in Theorem 2, for the aligned policy, the inference-time win rate and KL divergence deviation from the base policy are independent of the base policy and reward model. Below we present the precise formula for these two procedures.

Theorem 3 (Properties of BoN and WoN procedures). For any transformation function Φ, the solution π * RΦ,β to the KL-regularized RL problem defined in Eq. ( [13](#formula_18)) satisfies the followings:

$Let F Φ,β (u) = u 0 e Φ(u ′ )/β du ′ 1 0 e Φ(u ′ )/β du ′ .$Then the following hold true:

• The best-of-N win rate over the base policy satisfies for any x,

$W BoN r (π * RΦ,β ≻ π ref | x) = 1 -N 1 0 F Φ,β (u) N u N -1 du,$• The worst-of-N win rate over the base policy satisfies for any x,

$W WoN r (π * RΦ,β ≻ π ref | x) = N 1 0 (1 -F Φ,β (u)) N (1 -u) N -1 du,$• The KL divergence between π * rΦ,β and π ref satisfies

$D KL (π * RΦ,β ∥π ref ) = 1 β 1 0 Φ(u)e Φ(u)/β du 1 0 e Φ(u)/β du -log 1 0 e Φ(u)/β du . (14$$)$By varying β in Eq. ( [13](#formula_18)), we can obtain an alignment curve plotting the inference-time win rate and KL divergence deviation for different aligned policies. This allows us to compare the performance of different transformation functions Φ.

In the rest of the section, we will consider different types of transformations, and analytically compute the alignment curves using Theorem 3 by varying β, i.e., the plot of

$(D KL (π * rΦ,β ∥π ref ), W T r (π * rΦ,β ) ≻ π ref ) for different β's.$The transformations we consider include optimal transformations for standard win rate, exponential functions, and optimizationbased transformations, described below.

Optimal reward transformations for standard win rate. The identity mapping proposed by Azar et al. ( [2023](#)) and the logarithmic mapping as used by BoN distillation [(Beirami et al., 2024;](#b5)[Yang et al., 2024;](#b51)[Amini et al., 2024;](#b0)[Sessa et al., 2024)](#b41) are shown to be (almost) optimal for the standard win rate. We would like to understand whether they are still good candidates when inference-time procedures are considered.

Deriving an optimized reward transformation function. For calibrated inferencetime procedures like BoN, and WoN, due to Theorem 2, we have that the optimal reward transformation Φ is independent of the base policy. One can optimize for good Φ's using simple toy language models, and we have the following corollary. Corollary 2. For any β > 0, the Φ that achieves the optimal BoN win-rate vs. KL tradeoff must satisfy the following pair of equations: and for WoN, it satisfies that

$Φ BoN (u) = -N 2 1 u F π (v) N -1 v N -1 dv, f (u) ∝ e Φ BoN (u) β , Pre-print$$Φ WoN (u) = -N 2 1 u (1 -v) N -1 (1 -F π (v)) N -1 dv, f (u) ∝ e Φ WoN (u) β .$Hence one can find a transformation function based on finding the fixed point of the coupled equations in Corollary 2 through iterative updates.

Exponential tilting for reward transformation. In addition to deriving the optimized transformation, motivated by the exponential tilting of loss functions [(Li et al., 2021;](#b24)[2023)](#),

Pre-print we consider the following exponential transformation:

$Φ t (u) = sign(t) • e tu ,(15)$where sign(t) = 1 for t ≥ 0 and sign(t) = -1 for t < 0. These exponential transformations are essentially helping to optimize different quantiles of the reward for different values of t [(Li et al., 2023)](#b25). For a positive value of t, the exponential tilting transformation focuses on optimizing the high quantiles of the objective (calibrated reward) and indeed recovers the max function for a large positive t. Hence, we expect that positive values of t help with BoN inference-time procedure. On the other hand, for a negative value of t, the transformation is akin to optimizing the lower quantiles of the objective (calibrated reward), which makes it a suitable transformation for the WoN inference-time procedure.

Results. In In the first plot, we consider standard win rate. In this case, it is known that the IPO objective is win rate optimal. As can be seen, the logarithmic transformation (i.e., best-of-N distillation) also achieves a nearly optimal win rate, which was already observed by Yang et al. ( [2024](#)); [Gui et al. (2024)](#b18). All other transformations are sub-optimal for standard win rate.

Next, we consider Best-of-2 and Best-of-4 win rate. Here, additionally we include bon opt, which is obtained by deriving the fixed point of Corollary 2. As can be seen, the identity transformation is no longer optimal. The best tradeoffs are given by bon opt. We also observe that exp(5x) and exp(10x) are almost as good as bon opt for Best-of-2 and Best-of-4, respectively. Moreover,the identity transformation and logarithmic transformation are sub-optimal in these cases, which shows that considering standard win rate as the only metric is not optimal when inference-time procedure is concerned. We also observe that the behavior of identity transformation and logarithmic transformation is different in that the identity transformation gives better tradeoffs.

Finally, we consider Worst-of-2 and Worst-of-4 win rate. Again, it can be observed that bon opt gives the best tradeoffs for this inference-time procedure. Here, exp(-5x) and exp(-10x) are almost as good for Worst-of-2 and Worst-of-4, respectively. We also observe that identity transformation and logarithmic transformation are sub-optimal in these cases and the logarithmic transformation gives better tradeoffs for WoN compared to the identity transformation.

The above results demonstrate the importance of considering the inference-time procedure when performing alignment. We find that exponential transformation with different t's are good for different inference-time procedures, which will be our focus in practical experiments.

Next, we will examine whether a good transformation that we found on the idealized continuous language model generalizes in the wild to real-world scenarios. Before moving on to the experiments, we will have to offer a practical algorithm for solving the inference-time KL-regularized RL optimization problem, which is the subject of the next section.

Algorithm 1 CTRL Algorithm for BoN and WoN Require: Base policy π ref , (uncalibrated) reward model r, training prompts x ∈ X , number of offline rollouts per prompt m. 1: Compute approximate empirical calibration function C r,π ref using Algorithm 2 with m offline rollouts per prompt. 2: Transform calibrated reward using exponential function per Eq. ( [15](#formula_31)) with t > 0 for BoN and t < 0 for WoN. 3: Optimize KL-RL using calibrated and transformed reward per Eq. ( [6](#formula_10)).

Consider empirical calibration, where we draw K samples z 1 , z 2 , ..., z K from the reference model π ref for each prompt x in the RL training data. We then sort the rewards to all the responses {r(x, z 1 ), r(x, z 2 ), ...r(x, z K )}, and assign empirical calibrated reward scores during RLHF training for the prompt, response pair (x, y) as

$C r,π ref (x, y) = 1 K K i=1,zi∼π ref 1[r(x, y) ≥ r(x, z i )].(16)$Ideally, as K → ∞, the empirical calibrated reward would converge to the true calibrated reward, and may be used in the RL training objective through PPO [(Schulman et al., 2017)](#b39). However, this could be costly as the exact computation of this calibrated reward requires us to sample and store K reward scores per-prompt and per roll-out in the KL-RL solvers.

Instead, we propose to approximate the calibration curve by scaling it with a step-wise function in the logarithmic domain. We do this by choosing p anchor points q 1 , q 2 , . . ., where at each of the quantiles q i ∈ (0, 1) we achieve zero calibration error. The algorithm for the simpler case (p = 1, median) is given in Algorithm 2. For larger values of p, the algorithm is given in Algorithm 3 (Appendix D).  6 Experiment Results

## Evaluation setup

Datasets. To train the reward models, we use the Anthropic Helpfulness and Harmlessness datasets [(Bai et al., 2022)](#b4) which involve multi-turn dialogues between a human and a digital assistant. The preference datasets consist of two responses for one context, and a label for the human preference for the response. We use the train split of the two datasets (44K examples for helpfulness and 42K for harmlessness) to train the uncalibrated and calibrated reward models -separate reward models for each objective.

Model. The uncalibrated reward model is trained based on the Bradley-Terry pairwise objective [(Raffel et al., 2020)](#b37), and the calibration is done on the training-split of the RL training procedure by drawing samples from the reference model. The underlying model for both these rewards is the PaLM-2 S model [(Anil et al., 2023)](#b2). The base reference policy model is a PaLM-2 S model that is fine-tuned (SFT) on the Anthropic dialog preferred responses. We then train the aligned policy model through KL-regularized Reinforcement Learning [(Schulman et al., 2017;](#b39)[Bai et al., 2022)](#b4). We compare against uncalibrated (a model trained with KL-RL using PPO and no further processing), BoNBoN [(Gui et al., 2024)](#b18), Best-of-N [(Nakano et al., 2022;](#b29)[Beirami et al., 2024)](#b5) True rewards. As evaluating using ground truth rewards in a pointwise manner based on human annotations can be expensive, we follow [Eisenstein et al. (2024)](#b15); [Mudgal et al. (2024)](#b28) and perform automated evaluation using a larger PaLM-2 M model to compute true rewards.

Metrics. To measure improvement due to post-RL training, we report both the win rate and the BoN and WoN win rates, calculated after applying the respective inference-time procedures on the responses generated by the RL model against the base SFT model, along with the corresponding KL-divergence of the RL model with the SFT model. For each of the runs, we experiment with different KL-regularizer strengths (β ∈ {0.01, 0.02, . . . , 0.09}) and obtain the Pareto-curve of the KL divergence vs {standard, BoN, WoN } win rate curves.

## Reward models are typically miscalibrated

We first validate our hypothesis that reward models used on real-world tasks are miscalibrated. We measure the miscalibration of the reward model trained on Anthropic helpfulness preference dataset by computing the scores of 100 reference-policy responses for 10 random prompts from training split. We then sort the scores and compute the ranks corresponding to each of the responses and plot these values as a scatter plot in Figure [2](#fig_2) (left). If the model were perfectly calibrated, the points for each prompt would lie on the line y = x. However, observe that for most prompts, the scatter plot deviates significantly from the y = x line, and the extent of this deviation varies depending on the prompt.

We then measure the Absolute Error (AE) between the reward scores and their corresponding ranks and plot the cumulative distribution function (CDF) of the AE of the various calibration approximations in Figure [2](#fig_2) (right). If the model is well-calibrated AE is zero always and hence the CDF reaches one at zero AE. We find that the reward scores (see legend named 'identity') are not calibrated (mean AE: 0.22) and using fixed reward polynomial transformation functions like square-root, cube, square -do not reduce the calibration error (mean AE > 0.15). However, using a per-prompt quantile-based reward calibration, see legend named 'quantile') significantly reduces the calibration error (mean AE: 0.02).

Figure [3](#fig_8): Calibrating the helpfulness and harmlessness reward improves average win rate (identity transformation).

## Calibrated rewards improve standard win rate

As a sanity check of InfAlign, we first measure the performance when there is no inferencetime procedure applied. Without any inference-time procedure, the optimization objective is standard win rate, and we compare the performance of CTRL (using an identity transform) against other relevant reward optimization baselines that are known to be (almost) win rate optimal, such as IPO [(Azar et al., 2023)](#b3) and Best-of-N distillation [(Gui et al., 2024;](#b18)[Amini et al., 2024;](#b0)[Sessa et al., 2024)](#b41). Specifically, we calibrated the reward model for helpfulness and harmlessness of the base model fine-tuned on preferred responses using Algorithm 2 with p = 1. In Figure [3](#fig_8), we find that compared to IPO and BoNBoN, the calibrated reward optimization achieves better win rate-KL trade-offs. We attribute this gain to a more efficient computation of win rate on training data using m samples from the base model, as opposed to relying of existing pairwise comparison data during KL-RL. Further, we find that the reward transformations applied on the calibrated reward perform similar to each other, thus validating the theoretical results of Section 4. We also find that reducing the calibration error with more anchors or by varying m, leads to better win rate vs KL tradeoffs (see Appendix D).

## CTRL improves BoN

For the helpfulness objective in the Anthropic dialog dataset, we aim to optimize the Best-of-N performance of the aligned model through the exponential transformation of the calibrated rewards. We measure the win rate against the Best-of-N of the base policy model (N =4). In Figure [4](#fig_6) we see that calibration based on the median rewards per-prompt achieves 8 -12% higher Best-of-N win rates as compared to the uncalibrated model on helpfulness objective. The exponential transformation of the calibrated reward outperforms the rest of the models. We find that the exponential factor of t = 10 works best as simulated by our framework on a toy-setting (see Section 4). Further, we show that these gains hold for varying values of N (2, 32) (see Appendix D.3).

## CTRL improves WoN (BoN jailbreaks)

For the harmlessness objective in the Anthropic dialog dataset, we aim to improve the Worst-of-n performance of the aligned policy model to improve safety against adversarial actors [(Hughes et al., 2024b)](#). Here, we use the negative exponential transformation t < 0.

In Figure [4](#fig_6) we see that calibration based on the median rewards per-prompt achieves 4 -9% higher Worst-of-N win rates as compared to the uncalibrated model. The negative transformation of the calibrated reward outperforms the rest of the models, with t = -10 performing the best: again identified as the optimal value per our simulation in a toy setting (see Section 4). 

## Pre-print

## Related work

Inference-time compute. Test-time compute has been leveraged in recent work [(Snell et al., 2024;](#b43)[Brown et al., 2024;](#b6)[Wu et al., 2024a)](#) to achieve better win rate vs KL tradeoffs from the aligned models including controlled decoding [(Mudgal et al., 2024;](#b28)[Chakraborty et al., 2024)](#b8), Monte Carlo tree search [(Chaffin et al., 2022;](#b7)[Scialom et al., 2021;](#b40)[Zhao et al., 2024)](#b57), iterative jailbreak query refinement [(Chao et al., 2023)](#b9), and model-chaining within agentic frameworks [(Gur et al., 2024)](#b19). Best-of-N (BoN) is also used as an evaluation metric in code and natural language generation benchmarks [(Stiennon et al., 2020;](#b45)[Chen et al., 2021)](#b11). Further, Worst-of-N (WoN) is a popular jailbreaking strategy for adversarial actors to elicit unsafe text from large language models [(Hughes et al., 2024b)](#). Prior work has largely focused on approximating inference-time solutions during training time through sampling [(Gui et al., 2024;](#b18)[Amini et al., 2024](#b0)[), distillation (Sessa et al., 2024)](#), and decoding [(Qiu et al., 2024)](#b35). Our work is orthogonal to this body of work as they assume that no inference-time procedure is applied, but rather attempt to approximate it during training. We show that our theoretical framework generalizes [IPO (Azar et al., 2023)](#) and best-of-N distillation [(Gui et al., 2024;](#b18)[Amini et al., 2024;](#b0)[Sessa et al., 2024)](#b41) as special cases.

We are motivated by recent work that apply meta-generation procedures [(Welleck et al., 2024)](#b50) at inference-time such as chaining prompted models [(Brown et al., 2024)](#b6), problem decomposition through chain-of-thought [(Wei et al., 2022)](#b49), Best-of-N reranking [(Collins & Koo, 2005;](#b13)[Charniak & Johnson, 2005;](#b10)[Pauls & Klein, 2009)](#b34) applied on reasoning traces (OpenAI, 2024). Our InfAlign framework was also motivated by complex inference-time strategies that involve transformation techniques such as refinement [(Madaan et al., 2024)](#b26), majority voting [(Wang et al., 2022b)](#), or using the generator as input to other search algorithms [(Yao et al., 2024)](#b55), that have outperformed other models for harder tasks. In this spirit, our framework allows to get additional gains in aligning models with such inference-time procedures deployed in the future.

Reward miscalibration. Reward miscalibration or hacking has been studied extensively in recent work [(Amodei et al., 2016;](#b1)[Pang et al., 2022;](#b33)[Gao et al., 2023)](#b17). The hypotheses behind reward hacking can be broadly categorized into 3 themes: (1) reward underspecification, (2) training-serving skew between pairwise and pointwise reward models, (3) dominant reward due to adhoc transformations. Reward models suffer from underspecification due to under-specified training data [(Skalse et al., 2022)](#b42) by capturing spurious correlations in the data [(Pan et al., 2022)](#b32). Methods to mitigate this often include training on non-overlapping splits during reward model fine-tuning [(Bai et al., 2022)](#b4) and ensembling [(Coste et al., 2023;](#b14)[Eisenstein et al., 2024)](#b15). Our CTRL method can be easily augmented with such data interventions in reward learning.

Pre-print KL-RL solvers. Training reward models on pairwise preference data, and then using it as pointwise scorers during reinforcement learning poses problems of transitive inconsistency.

To mitigate this problem, optimization techniques that directly incorporate the pairwise preference data during offline reinforcement learning have been proposed [(Rafailov et al., 2023;](#b36)[Azar et al., 2023)](#b3). Further, calibrating model probabilities to reflect rank-order generated sequences by quality metrics have been proposed [(Zhao et al., 2022)](#b58). We share the motivation behind these methods, while additionally recognizing the need to calibrate the rewards against the base policy on which we are aligning.

When aligning language models for multiple objectives, aggregating the rewards via a weighted sum [(Bai et al., 2022;](#b4)[Wu et al., 2024b](#)) is known to result in reward hacking of one of the dominant rewards. Thresholding the effect of individual rewards [(Moskovitz et al., 2023)](#b27) or changing the weights of the training data [(Bai et al., 2022)](#b4), however requires costly hyperparameter fine-tuning and retraining without the ability to reason about the hyperparameters and their effects on the reward-tradeoffs. Reward transformation techniques that calibrate against a reference reward is effective at mitigating domination of one reward [(Wang et al., 2024)](#b48), but implicitly assumes that the reward aggregation function is a logical "AND" of all rewards, heavily penalizing under-performance on any of the rewards. Motivated by the success of exponential tilting for focusing on high/low quantiles [(Li et al., 2023)](#b25), we also show that CTRL with exponential reward transformation achieves near-optimal inference-time win rate vs KL divergence tradeoffs, surpassing the performance of methods such as [IPO (Azar et al., 2023)](#) that target to optimize standard win rate vs KL divergence tradeoffs. In this paper, we show that calibration as a first-step can help ground reward transformations based on the final inference-time procedure applied. Further, we build on recent work that show the theoretical guarantees of Best-of-N sampling [(Beirami et al., 2024;](#b5)[Gao et al., 2023;](#b17)[Mudgal et al., 2024)](#b28) over most reinforcement learning optimization techniques to ground our calibration and transformation method.

## Concluding Remarks

In this paper, we have shown that existing win rate optimal alignment procedures are sub-optimal when inference-time procedures are applied on aligned language models. As models equipped with inference-time procedures such as reasoning, best-of-N , majority voting continue to outperform models which do not use such procedures, we study the question of how to learn inference-aware optimally aligned language models. While learning optimal solutions for general inference-time procedures is intractable, we propose InfAlign -a framework that optimizes for inference-time win rate, and provide theoretical guarantees of finding an optimal inference-aware aligned model. Our framework generalizes prior work on win rate optimal solutions [(Azar et al., 2023;](#b3)[Gui et al., 2024)](#b18). We further show that, for any inference time procedure, such an optimal model can be learned through KL-RL optimization using reward transformation. For a class of transformations that rely only on the rank of rewards, calibration of said rewards into a uniform distribution allows us to search efficiently for the optimal reward transformation through empirical simulation.

We demonstrate the efficacy of this framework, by transferring findings from empirical simulation to real-world tasks and propose CTRL -a calibrate-and-transform reinforcement learning solver for ranking based inference-time procedures -and particularize it to Best-of-N sampling (BoN) and jailbreaking (WoN). Empirically, we demonstrate on Anthropic dialog helpfulness and harmlessness datasets that, (1) in the standard setting when no inferencetime procedure is applied, CTRL with identity reward transformation achieves competitive or slightly better performance compared to a variety of SOTA methods for optimizing standard win rate (2) when inference-time procedures are applied, we outperform inference-time win rate vs KL tradeoffs compared to existing preference optimization methods by 8-12% for BoN and 4-9% for WoN inference-time procedures respectively.

Future work includes finding efficient solvers for complex inference-time procedures based on InfAlign that do not rely on the rank of rewards assigned to samples from the base policy model, e.g., reasoning [(OpenAI, 2024;](#b30)[Xie et al., 2024)](#b53). Further, practical solvers for reward aggregation of multiple competing objectives at inference-time could also be explored. From a theoretical standpoint, the generalizability of our inference-aware alignment approach to upstream tasks such as supervised fine-tuning or pre-training needs to be studied.

## Pre-print

A.8 Proof of Corollary 2

We will show that Corollary 2 is a special case of Theorem 1 with a simple continuous language model. And by Theorem 2, we have the Φ can be generalized to arbitrary continuous language models.

Let Y = [0, 1]. We assume the LMs and reward models are context-independent. We use u ∈ [0, 1] to denote y and set the reward model to be r(u) = u. The base policy is a simple uniform distribution over [0, 1], π ref = Unif([0, 1]). Let F π (u) be the CDF of π, then we have that the BoN win rate is

$W BoN r (π ≻ π ref | x) = 1 -N 1 0 F π (u) N u N -1 du,$and WoN win rate is

$W WoN r (π ≻ π ref | x) = N 1 0 (1 -F π (u)) N (1 -u) N -1 du.$Plugging these into Theorem 1, we have for BoN,

$R(u) = ∂W BoN r (π ≻ π ref | x) ∂π(u) = -N 1 0 v N -1 ∂F π (v) N ∂π(u) dv = -N 1 0 v N -1 F π (v) N -1 ∂F π (v) ∂π(u) dv = -N 2 1 0 F π (v) N -1 1 {v ≥ u} v N -1 dv = -N 2 1 u F π (v) N -1 v N -1 dv.$For WoN, we have

$R(u) = ∂W WoN r (π ≻ π ref | x) ∂π(u) = N 1 0 (1 -v) N -1 ∂ (1 -F π (v)) N ∂π(u) dv = -N 2 1 0 (1 -v) N -1 (1 -F π (v)) N -1 ∂F π (v) ∂π(u) dv = -N 2 1 0 (1 -v) N -1 (1 -F π (v)) N -1 1 {v ≥ u} dv = -N 2 1 u (1 -v) N -1 (1 -F π (v)) N -1 dv.$Pre-print

## B The role of KL divergence in model alignment

One question that arises is the role of the KL divergence regularizer in Eq. ( [5](#formula_9)). In this section, we argue that the regularizer essentially enables multi-tasking between the SFT task and the RL task.

Let's consider a log-linear model such that π θ (y|x) = e θ T g(x,y)-A(θ;x) ,

where g(x, y) is a fixed encoding of (x, y), and A(θ; x) is the partition function normalizing the distribution.

Supervised finetuning (SFT). Let D sft (x, y) = µ(x) × p sft (y|x) be the SFT data distribution. Then, the SFT task is

$θ * sft = arg min θ L sft (θ) where L sft (θ) := E (x,y)∼D sft {A(θ; x) -θ ⊤ g(x, y)},(19)$We further call p = π θ * sft . Lemma 8. The SFT solution satisfies

$E x∼µ {∇ θ A(θ * sft )} = E (x,y)∼D sft g(x, y).(20)$Proof. This is a known property of exponential families. The proof follows by noticing Proof. Notice that

$∇ θ L sft (θ * sft ) = 0.$$L bilevel,β (θ) = D KL (π θ ∥p) + 1 β L ro (θ) (23) = E x∼µ {A(θ; x) -A(θ * sft ; x) -(θ -θ * sft ) ⊤ ∇ θ A(θ * sft ; x)} + 1 β L ro (θ) (24) = E x∼µ {A(θ; x) -A(θ * sft ; x)} -(θ -θ * sft ) ⊤ E (x,y)∼D sft g(x, y) + 1 β L ro (θ) (25) = L multi-task,β (θ) + L sft (θ * sft ),(26)$where Eq. ( [24](#)) follows by noticing that KL divergence is a Bregman divergence in this setup, Eq. ( [25](#)) follows from Lemma 8, and Eq. ( [26](#formula_41)) follows from the definition of L sft (θ) applied to θ and θ * sft . Hence, the minimizers of the two objectives are the same given that L bilevel,β (θ) = L multi-task,β (θ) + C, completing the proof.

Pre-print Thus, effectively this proves that the KL-RL objective enables multi-tasking between the SFT stage and the reward optimization RL objective. One may wonder why we did not pose the KL divergence regularizer on the transformed distributions through D KL (T (π)(

$• | x)∥T (π ref )(•|x)) instead.$Consider the Best-of-N jailbreaking for example. While the adversary may be using the model to generate N responses and choose the least safe one for jailbreaking, the model should possess the core capabilities for other types of inference-time usage for other tasks that is different from that of jailbreaking (e.g., through chain-of-thought). Therefore, changing the KL divergence regularizer does not capture the fact that the model should remain suitable for all other tasks, and not just for the one for which it is getting aligned. We also note that if we used D KL (T (π)(• | x)∥T (π ref )(•|x)) instead, the problem would actually simplify to the standard alignment problem through a simple change of variables.

## D The effect of anchor points on calibration

In this section, we analyze the effect of using more anchor points on results. First, we present the full algorithm for more than one anchor point in Algorithm 3. q ℓ = ℓ+1 p+1 5: r q ℓ (x) ← top q ℓ •m (r(x, z 1 ), . . . r(x, z m )) select (q ℓ • m) th smallest element , where a is the smallest number in [0, p -1) such that r qa (x) ≥ r(x, y), otherwise a = 0.

Similarly, b is the largest number in [0, p -1) such that r q b (x) ≤ r(x, y), otherwise b = p -1. 9: Output: Calibrated Reward Function: C r,π ref that assigns a calibrated reward for all prompts in the training set.

## D.1 Better calibration leads to better gains

The effectiveness of calibration can be better understood when we approximate the calibration with fewer number of rollouts: n. When n = 1, we take one other rollout from the base policy model and use that as a reference reward, similar to [Wang et al. (2024)](#b48), and with n = 3, 5, 7, we approximate the median with this limited sample, and do the reward calibration. In Hence, we find that as the calibration error is reduced, we achieve better KL vs win rate tradeoffs, thus able to better implicitly approximate the best-of-n model.

## D.2 Calibration Reduces Reward Hacking

We demonstrate that calibrated reward models are less susceptible to reward hacking, a phenomenon where models exploit spurious correlations in training data to optimize for reward signals instead of true task objectives.

To induce reward hacking, we injected specific phrases into the start of preferred responses of our preference datasets: "Sorry, I can't help you with that" for Harmlessness and "Sure" for Helpfulness. We then evaluated the model's accuracy on a poisoned evaluation set where these phrases were inverted (added to the unpreferred responses). A significant drop in accuracy on this poisoned set would indicate reward hacking: a reliance on the spurious correlation.

Figure [6](#) shows that calibrated reward models are far more robust to these manipulated correlations, maintaining higher accuracy compared to uncalibrated models. Note that the calibration mechanism used here is different from the median-based calibration, and instead uses a pointwise loss.

## Pre-print

Figure [5](#): (a) Calibration improves KL vs win rate tradeoff even when we approximate calibration of median approximated with n=7 samples, whereas with n=1,3,5, the gains in win rate for a fixed KL-budget is lower, but still outperforms the uncalibrated model at higher KL values. Using n=100 samples is (calibrate median pareto) provides the best tradeoff (b) Calibration with two or more anchor points per-prompt improves KL-winrate tradeoff, as compared to median-based approximation -however there is diminishing gains with ten anchor points as compared to using two anchor points. The win rate is measured using the PaLM-2 M reward model trained on the Anthropic helpfulness preference dataset.

Figure [6](#): Calibrated reward models demonstrate robustness against reward hacking: We poisoned the training data by adding phrases to the preferred response to induce spurious correlations. When we evaluated against a test set where the correlations are inverted (phrase added to unpreferred models), calibrated models maintained higher accuracy than uncalibrated ones, demonstrating their reduced reliance on spurious correlations.

![Figure 1: Best-of-N and worst-of-N win rate vs KL tradeoff curves for N = 2, 4 with different transformation functions.]()

![we consider an ideal continuous language model and compare different alignment objectives on inference-time win rate vs KL divergence for three inference-time procedures: {standard, BoN, WoN }. The tradeoff curves are obtained by varying the strength of the regularizer, β. The different objectives correspond to different transformations of the calibrated reward, which include (1) identity mapping Φ = I(•) (IPO); (2) logarithmic mapping Φ = log (best-of-N distillation); (3) exponential tilting Φ t (•) as defined in Eq. (15); (4) optimized transformation function by solving for fixed points in Corollary 2.]()

![Offline Approximate Calibration Require: Base policy π ref , (uncalibrated) reward model r with range [0, 1], training prompts x ∈ X , number of samples per prompt m. 1: for all x ∈ X do 2: Roll out m i.i.d. samples from π ref per training prompt x: z j , j ∈ {1...m}.]()

![median (x) ← median(r(x, z 1 ), . . . , r(x, z m )). 4: end for 5: Define the calibrated reward function C r,π ref : X × Y → [0, 1] as follows: log C r,π ref (x, y) = log r(x, y) • log 0.5 log r median (x) . 6: Output: Calibrated reward function C r,π ref that assigns a calibrated reward for all prompts in the training set.]()

![Figure 2: Results on reward models trained on the Anthropic helpfulness preference dataset. (left) Scatter plot of reward scores and best-of-n ranks on a random sample of 10 prompts in the Anthropic helpfulness dataset. Note that the model shows miscalibration on most prompts, with the degree of miscalibration varying by prompt. (right) Plot of CDF of absolute error (AE). Observe that per-prompt quantile based methods have low AE with high probability, where as prompt-agnostic transformations have high AE typically.]()

![, BonD (Sessa et al., 2024), and IPO (Azar et al., 2023) as baselines.]()

![Figure 4: Calibration improves Best-of-N and Worst-of-N win rates for helpfulness (left) and harmlessness (right) objectives in the Anthropic dialog dataset respectively. We report win rate against the Best-of-N and Worst-of-N rewards of the base SFT model responses on the test split as measured by the PaLM-2 M reward model trained on the helpfulness and harmlessness data respectively.]()

![KL-regularized reward optimization (RO). Let r be a reward function that determines the reward for each (x, y). Let L ro (θ) := E x∼µ E y∼π θ r(x, y). Then,θ * bilevel,β = arg min θ L bilevel,β (θ) where L bilevel (θ) := D KL (π θ ∥p) + 1 β L ro (θ), (21)whereD KL (π θ ∥p) = E x∼µ D KL (π θ (•|x)∥p(•|x)).Multi-tasking SFT and RO. Now consider the following tasksθ * multi-task,β = arg min θ L multi-task,β (θ) where L multi-task (θ) := L sft (θ)+ 1 β L ro (θ). (22)Theorem 5. For all β ∈ R, we have θ * bilevel,β = θ * multi-task,β .]()

![Offline Approximate Calibration with arbitrary anchor points Require: Base policy π ref , (uncalibrated) reward model r, training prompts x ∈ X , number of quantiles p, number of samples per prompt m. 1: for all x ∈ X do 2:Roll out m i.i.d. samples from π ref per training prompt x: z j , j ∈ {1...m}.3:for all ℓ ∈ {0...p -1} do 4:]()

![Define the calibrated reward functionC r,π ref (x, y) : X × Y → [0, 1] as follows: log C r,π ref (x, y) = log r(x, y) • log qa+q b 2 log rq a (x)+rq b (x) 2]()

![Fig 5(a), we find that calibration using more number of samples improves the win rate, with n = 1 providing no gains in the KL vs win rate tradeoff. Additionally, in Fig 5(b), when we calibrate with more anchor points, we find that the KL vs win rate tradeoffs improves as compared to the median-based calibration. This is further supported by the fact that with more anchor points, the calibration error is significantly reduced as shown in Fig 2(b).]()

![]()

The solution to this optimization problem is unique(Korbak et al.,  

2022;[Rafailov et al., 2023;](#b36)[Yang et al., 2024)](#b51).2 The definition is similar to the cumulative density function of the reward r(x, y) under policy π except for how ties are decided.

One question that arises is the role of the KL divergence regularizer in Eq. (5). We argue that the regularizer essentially enables multi-tasking between the SFT task and the RL task, which we formally prove for log-linear models in Appendix B. In other words, the KL divergence regularizer enables to preserve the core capabilities of the model while acquiring a new one through the KL-RL process.

CTRL: Calibrate-and-Transform Reinforcement LearningIn this section, we propose the CTRL method, which is our proposed solver for the inferencetime win rate optimization problem. Recall from the previous section that the proposed solution could be decomposed into three stages: reward calibration and reward transformation followed by the standard KL-RL solver. In the previous section, we highlighted the reward transformation. In the rest of this section, we focus on approximate empirical calibration. Combining it with reward transformation yields the final CTRL algorithm, given in Algorithm 1.

Below we add an additional 1 2 1 {r(x, y) = r(x, z)} to the win r.v for simplicity, which won't affect the result for continuous LMs.

