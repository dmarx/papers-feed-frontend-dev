{
  "arxivId": "2412.19792",
  "title": "InfAlign: Inference-aware language model alignment",
  "authors": "Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, Ananda Theertha Suresh, Ahmad Beirami",
  "abstract": "Language model alignment has become a critical step in training modern\ngenerative language models. The goal of alignment is to finetune a reference\nmodel such that the win rate of a sample from the aligned model over a sample\nfrom the reference model is high, subject to a KL divergence constraint. Today,\nwe are increasingly using inference-time algorithms (e.g., Best-of-N,\ncontrolled decoding, tree search) to decode from language models rather than\nstandard sampling. However, the alignment objective does not capture such\ninference-time decoding procedures. We show that the existing alignment\nframework is sub-optimal in view of such inference-time methods. We then modify\nthe alignment objective and propose a framework for inference-aware alignment\n(IAPO). We prove that for any inference-time decoding algorithm, the optimal\nsolution that optimizes the inference-time win rate of the aligned policy\nagainst the reference policy is the solution to the typical RLHF problem with a\ntransformation of the reward. This motivates us to provide the KL-regularized\ncalibrate-and-transform RL (CTRL) algorithm to solve this problem, which\ninvolves a reward calibration step and a KL-regularized reward maximization\nstep with a transformation of the calibrated reward. We particularize our study\nto two important inference-time strategies: best-of-N sampling and best-of-N\njailbreaking, where N responses are sampled from the model and the one with the\nhighest or lowest reward is selected. We propose specific transformations for\nthese strategies and demonstrate that our framework offers significant\nimprovements over existing state-of-the-art methods for language model\nalignment. Empirically, we outperform baselines that are designed without\ntaking inference-time decoding into consideration by 8-12% and 4-9% on\ninference-time win rates over the Anthropic helpfulness and harmlessness dialog\nbenchmark datasets.",
  "url": "https://arxiv.org/abs/2412.19792",
  "issue_number": 763,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/763",
  "created_at": "2025-01-04T06:51:54.729533",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 49,
  "last_read": "2025-01-04T06:52:00.645791",
  "last_visited": "2025-01-03T08:59:10.679Z",
  "main_tex_file": null,
  "published_date": "2024-12-27T18:45:36Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CL",
    "cs.IT",
    "math.IT"
  ]
}