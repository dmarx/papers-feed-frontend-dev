# YOLOE: Real-Time Seeing Anything

## Abstract

## 

Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a builtin large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3× less training cost and 1.4× inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP b and 0.4 AP m gains over closed-set YOLOv8-L with nearly 4× less training time. Code and models are available at [https: //github.com/THU-MIG/yoloe](https://github.com/THU-MIG/yoloe).

## Introduction

Object detection and segmentation are foundational tasks in computer vision [[15,](#b14)[48]](#b49), with widespread applications spanning autonomous driving [[2]](#b1), medical analyses [[55](#b56)], * Equal contribution. 7UDLQLQJ7LPHK /9,6$3 ×/HVV7 LPH <2/2:RUOGY <2/2(2XUV )36Z7HQVRU57 × 6 S H H G X S <2/2:RUOGY <2/2(2XUV )36Z&RUH0/ × 6 S H H G X S <2/2:RUOGY <2/2(2XUV Figure 1. Comparison of performance, training cost, and inference efficiency between YOLOE (Ours) and advanced YOLO-Worldv2 in terms of open text prompts. LVIS AP is evaluated on minival set and FPS w/ TensorRT and w/ CoreML is measured on T4 GPU and iPhone 12, respectively. The results highlight our superiority.

and robotics [[8]](#b7), etc. Traditional approaches like YOLO series [[1,](#b0)[3,](#b2)[21,](#b20)[47]](#b48), have leveraged convolutional neural networks to achieve real-time remarkable performance. However, their dependence on predefined object categories constrains flexibility in practical open scenarios. Such scenarios increasingly demand models capable of detecting and segmenting arbitrary objects guided by diverse prompt mechanisms, such as texts, visual cues, or without prompt. Given this, recent efforts have shifted towards enabling models to generalize for open prompts [[5,](#b4)[20,](#b19)[49,](#b50)[80]](#b81). They target single prompt type, e.g., GLIP [[32]](#b32), or multiple prompt types in a unified way, e.g., DINO-X [[49]](#b50). Specifically, with region-level vision-language pretraining [[32,](#b32)[37,](#b37)[65]](#b66), text prompts are usually processed by text encoder to serve as contrastive objectives for region features [[20,](#b19)[49]](#b50), achieving recognition for arbitrary categories, e.g., YOLO-World [[5]](#b4). For visual prompts, they are often encoded as class embeddings tied to specified regions for identifying similar objects, by the interaction with image features or language-aligned visual encoder [[5,](#b4)[19,](#b18)[30,](#b29)[49]](#b50), e.g., T-Rex2 [[20]](#b19). In prompt-free scenario, existing methods typically integrate language models, finding all objects and generating the corresponding category names conditioned on region features sequentially [[49,](#b50)[62]](#b63), e.g., GenerateU [[33]](#b33).

Despite notable advancements, a single model that supports diverse open prompts for arbitrary objects with high efficiency and accuracy is still lacking. For example, DINO-X [[49]](#b50) features a unified architecture, which, however, incurs resource-intensive training and inference overhead. Additionally, individual designs for different prompts in separate works exhibit suboptimal trade-offs between performance and efficiency, making it difficult to directly combine them into one model. For example, text-prompted approaches often incur substantial computational overhead when incorporating large vocabularies, due to complexity of cross-modality fusion [[5,](#b4)[32,](#b32)[37,](#b37)[49]](#b50). Visual-prompted methods usually compromise deployability on edge devices owing to the transformer-heavy design or reliance on additional visual encoder [[20,](#b19)[30,](#b29)[67]](#b68). Prompt-free ways, meanwhile, depend on large language models, introducing considerable memory and latency costs [[33,](#b33)[49]](#b50).

In light of these, in this paper, we introduce YOLOE(ye), a highly efficient, unified, and open object detection and segmentation model, like human eye, under different prompt mechanisms, like texts, visual inputs, and promptfree paradigm. We begin with YOLO models with widely proven efficacy. For text prompts, we propose a Reparameterizable Region-Text Alignment (RepRTA) strategy, which employs a lightweight auxiliary network to improve pretrained textual embeddings for better visualsemantic alignment. During training, pre-cached textual embeddings require only the auxiliary network to process text prompts, incurring low additional cost compared with closed-set training. At inference and transferring, auxiliary network is seamlessly re-parameterized into the classification head, yielding an architecture identical to YOLOs with zero overhead. For visual prompts, we design a Semantic-Activated Visual Prompt Encoder (SAVPE). By formalizing regions of interest as masks, SAVPE fuses them with multi-scale features from PAN to produce grouped promptaware weights in low dimension in an activation branch and extract prompt-agnostic semantic features in a semantic branch. Prompt embeddings are derived through aggregation of them, resulting in favorable performance with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. Without relying on costly language models, LRPC leverages a specialized prompt embedding to find all objects and a built-in large vocabulary for category retrieval. By matching only anchor points with identified objects against the vocabulary, LRPC ensures high performance with low overhead.

Thanks to them, YOLOE excels in detection and segmentation across diverse open prompt mechanisms within one model, enjoying high inference efficiency and low training cost. Notably, as shown in Fig. [1](#), under 3× less training cost, YOLOE-v8-S significantly outperforms YOLO-Worldv2-S [5] by 3.5 AP on LVIS [[14]](#b13), with 1.4× and 1.3× inference speedups on T4 and iPhone 12, respectively. In visual-prompted and prompt-free settings, YOLOE-v8-L outperforms T-Rex2 by 3.3 AP r and GenerateU by 0.4 AP with 2× less training data and 6.3× fewer parameters, respectively. For transferring to COCO [[34]](#b34), YOLOE-v8-M / L outperforms YOLOv8-M / L by 0.4 / 0.6 AP b and 0.4 / 0.4 AP m with nearly 4× less training time. We hope that YOLOE can establish a strong baseline and inspire further advancements in real-time open prompt-driven vision tasks.

## Related Work

Traditional detection and segmentation. Traditional approaches for object detection and segmentation primarily operate under closed-set paradigms. Early two-stage frameworks [[4,](#b3)[12,](#b11)[15,](#b14)[48]](#b49), exemplified by Faster R-CNN [[48]](#b49), introduce region proposal networks (RPNs) followed by region-of-interest (ROI) classification and regression. Meanwhile, single-stage detectors [[10,](#b9)[35,](#b35)[38,](#b39)[56,](#b57)[72]](#b73) prioritizes speed through grid-based predictions within a single network. The YOLO series [[1,](#b0)[21,](#b20)[27,](#b26)[47,](#b48)[59,](#b60)[60]](#b61) plays a significant role in this paradigm and are widely used in real world. Moreover, DETR [[28]](#b27) and its variants [[28,](#b27)[69,](#b70)[77]](#b78) mark a major shift by removing heuristicdriven components with transformer-based architectures. To achieve finer-grained results, existing instance segmentation methods predict pixel-level masks rather than bounding box coordinates [[15]](#b14). For this, YOLACT [[3]](#b2) facilitates real-time instance segmentation through integration of prototype masks and mask coefficients. Based on DINO [[69]](#b70), MaskDINO [[29]](#b28) utilizes query embeddings and a highresolution pixel embedding map to produce binary masks.

Text-prompted detection and segmentation. Recent advancements in open-vocabulary object detection [[13,](#b12)[25,](#b24)[61,](#b62)[68,](#b69)[[74]](#b75)[[75]](#b76)[[76]](#b77) have focused on detecting novel categories by aligning visual features with textual embeddings. Specifically, GLIP [[32]](#b32) unifies object detection and phrase grounding through grounded pre-training on largescale image-text pairs, demonstrating robust zero-shot performance. DetCLIP [[65]](#b66) facilitates open-vocabulary learning by enriching the concepts with descriptions. Besides, Grounding DINO [[37]](#b37) enhances this by integrating crossmodality fusion into DINO, improving alignment between text prompts and visual representations. YOLO-World [[5]](#b4) further shows the potential of pretraining small detectors with open recognition capabilities based on the YOLO architecture. YOLO-UniOW [[36]](#b36) builds upon YOLO-World by leveraging the adaptive decision-learning strategy. Similarly, several open-vocabulary instance segmentation models [[11,](#b10)[18,](#b17)[26,](#b25)[45,](#b46)[63]](#b64) learn rich visual-semantic knowledge from advanced foundation models to perform segmentation on novel object categories. For example, X-Decoder [[79]](#b80) and OpenSeeD [[71]](#b72) explore both the open-vocabulary detection and segmentation tasks. APE [[54]](#b55) introduces a universal visual perception model that aligns and prompts all objects in image using various text prompts. Visual-prompted detection and segmentation. While Backbone PAN Segmentation Regression Image Embedding P5 P4 P3 man, horse, dog, cat, … Text prompt Text Encoder man horse dog Textual Embedding 𝑃 Auxiliary Network 𝑓 𝜃 Visual prompt Activation Branch Semantic Branch Prompt-aware weight Aggregation Prompt Embedding Semantic-Activated Vis. Prompt Encoder Re-parameterizable Region-Text Alignment Label Re-parameterization Inference Transferring Classification Offline for training/inference Lazy Region-Prompt Contrast Specialized Embedding Anchor Points Prompt free Built-in Vocabulary Retrieval Semantic feature We design a re-parameterizable region-text alignment strategy to improve performance with zero inference and transferring overhead. For visual prompts, SAVPE is employed to encode visual cues with enhanced prompt embedding under minimal cost. For prompt-free setting, we introduce lazy region-prompt contrast strategy to provide category names for all identified objects efficiently by retrieval.

text prompts offer a generic description, certain objects can be challenging to describe with language alone, such as those requiring specialized domain knowledge. In such cases, visual prompts can guide detection and segmentation more flexibly and specifically, complementing text prompts [[19,](#b18)[20]](#b19). OV-DETR [[67]](#b68) and OWL-ViT [[41]](#b42) leverage CLIP encoders to process text and image prompts. MQ-Det [[64]](#b65) augments text queries with class-specific visual information from query images. DINOv [[30]](#b29) explores visual prompts as in-context examples for generic and referring vision tasks. T-Rex2 [[20]](#b19) integrates visual and text prompts by region-level contrastive alignment. For segmentation, based on large-scale data, SAM [[23]](#b22) presents a flexible and strong model that can be prompted interactively and iteratively. SEEM [[80]](#b81) further explores segmenting objects with more various prompt types. Semantic-SAM [[31]](#b30) excels in semantic comprehension and granularity detection, handling both panoptic and part segmentation tasks.

Prompt-free detection and segmentation. Existing approaches still depend on explicit prompts during inference for open-set detection and segmentation. To address this limitation, several works [[33,](#b33)[40,](#b41)[49,](#b50)[62,](#b63)[66]](#b67) explore integrating with generative language models to produce object descriptions for all found objects. For instance, GRiT [[62]](#b63) employs a text decoder for both dense captioning and object detection tasks. DetCLIPv3 [[66]](#b67) trains an object captioner on large-scale data, enabling model to generate rich label information. GenerateU [[33]](#b33) leverages the language model to generate object names in a free-form way.

Closing remarks. To the best of our knowledge, aside from DINO-X [[49]](#b50), few efforts have achieved object detection and segmentation across various open prompt mechanisms within a single architecture. However, DINO-X entails extensive training cost and notable inference overhead, severely constraining the practicality for real-world edge deployments. In contrast, our YOLOE aims to deliver an efficient and unified model that enjoys real-time performance and efficiency with easy deployability.

## Methodology

In this section, we detail designs of YOLOE. Building upon YOLOs (Sec. 3.1), YOLOE supports text prompts through RepRTA (Sec. 3.2), visual prompts via SAVPE (Sec. 3.3), and prompt-free scenario with LRPC (Sec. 3.4).

## Model architecture

As shown in Fig. [2](#fig_0), YOLOE adopts the typical YOLOs' architecture [[1,](#b0)[21,](#b20)[47]](#b48), consisting of backbone, PAN, regression head, segmentation head, and object embedding head. The backbone and PAN extracts multi-scale features for the image. For each anchor point, the regression head predicts the bounding box for detection, and the segmentation head produces the prototype and mask coefficients for segmentation [[3]](#b2). The object embedding head follows the structure of classification head in YOLOs, except that the output channel number of last 1× convolution layer is changed from the class number in closed-set scenario to the embedding dimension. Meanwhile, given text and visual prompts, we employ RepRTA and SAVPE to encode them as normalized prompt embeddings P, respectively. They serve as the classification weights and contrast with the anchor points' object embeddings O to obtain category labels. The process can be formalized as

$Label = O • P T : R N ×D × R D×C → R N ×C ,(1)$where N denotes the number of anchor points, C indicates the number of prompts, and D means the feature dimension of embeddings, respectively.

## Re-parameterizable region-text alignment

In open-set scenarios, the alignment between textual and object embeddings determines the accuracy of identified categories. Prior works usually introduce complex crossmodality fusion to improve the visual-textual representation for better alignment [[5,](#b4)[37]](#b37). However, these ways incur notable computational overhead, especially with large number of texts. Given this, we present Re-parameterizable Region-Text Alignment (RepRTA) strategy, which improves pretrained textual embeddings during training through the reparameterizable lightweight auxiliary network. The alignment between textual and anchor points' object embeddings can be enhanced with zero inference and transferring cost. Specifically, with the text prompts of T with length of C, we first employ the CLIP text encoder [[44,](#b45)[57]](#b58) to obtain pretrained textual embedding P = TextEncoder(T ). Before training, we cache all embeddings of texts in datasets in advance and the text encoder can be removed with no extra training cost. Meanwhile, as shown in Fig. [3](#).(a), we introduce a lightweight auxiliary network f θ with only one feed forward block [[53,](#b54)[58]](#b59), where θ indicates the trainable parameters and introduces low overhead compared with closed-set training. It derives the enhanced textual embedding P = f θ (P ) ∈ R C×D for contrasting with the anchor points' object embedding during training, leading to improved visual-semantic alignment. Let K ∈ R D×D ′ ×1×1 be the kernel parameters of last convolution layer with input features I ∈ R D ′ ×H×W in the object embedding head, ⊛ be the convolution operator, and R be the reshape function, we have

$Label = RD×H×W →HW ×D (I ⊛ K) • (f θ (P )) T .(2)$Moreover, after training, the auxiliary network can be reparameterized with the object embedding head into the identical classification head of YOLOs. The new kernel parameters K ′ ∈ R C×D ′ ×1×1 for last convolution layer after re-parameterization can be derived by

$K ′ = RC×D→C×D×1×1(f θ (P )) ⊛ K T .(3)$The final predication can be obtained by Label = I ⊛ K ′ , which is identical to the original YOLO architecture, leading to zero overhead for deployment and transferring to downstream closed-set tasks.

## Semantic-activated visual prompt encoder

Visual prompts are designed to indicate the object category of interest through visual cues, e.g., box and mask.

To produce the visual prompt embedding, prior works often employ transformer-heavy design [[20,](#b19)[30]](#b29), e.g., deformable attention [[78]](#b79), or additional CLIP vision encoder [[44,](#b45)[67]](#b68). These ways, however, introduce challenges in deployment and efficiency due to complex operators or high computational demands. Considering this, we intro-

$Linear (a) Linear Linear Axuiliary Network 𝑓 𝜃 U U C 3 × 3 3 × 3 3 × 3 3 × 3 3 × 3 1 × 1 P5 P4 P3 3 × 3 Semantic Branch P5 P4 P3 U U C Visual Prompt C Activation Branch 1 × 1 1 × 1 1 × 1 3 × 3 3 × 3 3 × 3 3 × 3 (b)$Textual Embedding 𝑃 Prompt Embedding Text Prompt U Upsample C Aggregation Concatenation 𝐹 𝑉 𝐹 𝐼 Figure 3. (a) The structure of lightweight auxiliary network in RepRTA, which consists of one SwiGLU FFN block [53]. (b) The structure of SAVPE, which consists of semantic branch to generate prompt-agnostic semantic features and activation branch to provide grouped prompt-aware weights. Visual prompt embedding can thus be efficiently derived by their aggregation. duce Semantic-Activated Visual Prompt Encoder (SAVPE) for efficiently processing visual cues. It features two decoupled lightweight branches: (1) Semantic branch outputs prompt-agnostic semantic features in D channels without overhead of fusing visual cues, and (2) Activation branch produces grouped prompt-aware weights by interacting visual cues with image features in much fewer channels under low costs. Their aggregation then leads to informative prompt embedding under minimal complexity.

As shown in Fig. [3](#).(b), in the semantic branch, we adopt the similar structure as object embedding head. With multiscale features {P 3 , P 4 , P 5 } from PAN, we employ two 3×3 convs for each scale, respectively. After upsampling, features are concatenated and projected to derive semantic features S ∈ R D×H×W . In the activation branch, we formalize visual prompt as mask with 1 for indicated region and 0 for others. We downsample it and leverage 3×3 conv to derive prompt feature F V ∈ R A×H×W . Besides, we obtain image features F I ∈ R A×H×W for fusion with it from {P 3 , P 4 , P 5 } by convs. F V and F I are then concatenated and utilized to output prompt-aware weights W ∈ R A×H×W , which is normalized using softmax within prompt-indicated region. Moreover, we divide the channels of S into A groups with D A channels in each. The channels in the i-th group share the weight W i:i+1 from the ith channel of W. With A ≪ D, we can process visual cues with image features in low dimension, bringing minimal cost. Furthermore, prompt embedding can be derived with aggregation of two branches by

$P = Concat(G1, ..., GA); Gi = Wi:i+1 • S T D A * i: D A * (i+1) . (4)$It can thus contrast with anchor points' object embeddings to identify objects with category of interest.

## Lazy region-prompt contrast

In prompt-free scenario without explicit guidance, models are expected to identity all objects with names in the image. Prior works usually formulate such setting as a generative problem, where language model is employed to generate categories for dense found objects [[33,](#b33)[49,](#b50)[62]](#b63). However, this introduces notable overhead, where language models, e.g., FlanT5-base [[6]](#b5) with 250M parameters in Genera-teU [[33]](#b33) and OPT-125M [[73]](#b74) in DINO-X [[49]](#b50), are far from meeting high efficiency requirement. Given this, we reformulate such setting as a retrieval problem and present Lazy Region-Prompt Contrast (LRPC) strategy. It lazily retrieves category names from a built-in large vocabulary for anchor points with objects in the cost-effective way. Such paradigm enjoys zero dependency on language models, meanwhile with favorable efficiency and performance.

Specifically, with pretrained YOLOE, we introduce a specialized prompt embedding and train it exclusively to find all objects, where objects are treated as one category. Meanwhile, we follow [[16]](#b15) to collect a large vocabulary which covers various categories and serve as the built-in data source for retrieval. One may directly leverage the large vocabulary as text prompts for YOLOE to identify all objects, which, however, incurs notable computational cost by contrasting abundant anchor points' object embeddings with numerous textual embeddings. Instead, we employ the specialized prompt embedding P s to find the set O ′ of anchor points corresponding to objects by

$O ′ = {o ∈ O | o • P T s > δ},(5)$where O denotes all anchor points and δ is the threshold hyperparameter for filtering. Then, only anchor points in O ′ are lazily matched against the built-in vocabulary to retrieve category names, bypassing the cost for irrelevant anchor points. This further improves efficiency without performance drop, facilitating the real world application.

## Training objective

During training, we follow [[5]](#b4) to obtain an online vocabulary for each mosaic sample with the texts involved in the images as positive labels. Following [[21]](#b20), we leverage taskaligned label assignment to match predictions with ground truths. The binary cross entropy loss is employed for classification, with IoU loss and distributed focal loss adopted for regression. For segmentation, we follow [[3]](#b2) to utilize binary cross-entropy loss for optimizing masks.

## Experiments

## Implementation details

Model. For fair comparison with [[5]](#b4), we employ the same YOLOv8 architecture [[21]](#b20) for YOLOE. Besides, to verify its good generalizability on other YOLOs, we also experiment with YOLO11 architecture [[21]](#b20). For both of them, we provide three model scales, i.e., small (S), medium (M), and large (L), to suit various application needs. Text prompts are encoded using the pretrained MobileCLIP-B(LT) [[57]](#b58) text encoder. We empirically use A = 16 in SAVPE, by default.

Data. We follow [[5]](#b4) to utilize detection and grounding datasets, including Objects365 (V1) [[52]](#b53), GoldG [[22]](#b21) (includes GQA [[17]](#b16) and Flickr30k [[43]](#b44)), where images from COCO [[34]](#b34) are excluded. Beside, we leverage advanced SAM-2.1 [[46]](#b47) model to generate pseudo instance masks using ground truth bounding boxes from the detection and grounding datasets for segmentation data. These masks undergo filtering and simplification to eliminate noise [[9]](#b8). For visual prompt data, we follow [[20]](#b19) to leverage ground truth bounding boxes for visual cues. In prompt-free tasks, we reuse the same datasets, but annotate all objects as a single category to learn a specialized prompt embedding.

Training. Due to limited computational resource, unlike YOLO-World's training for 100 epochs, we first train YOLOE with text prompts for 30 epochs. Then, we only train the SAVPE for merely 2 epochs with visual prompts, which avoids additional significant training cost that comes with supporting visual prompts. At last, we train the specialized prompt embedding for only 1 epoch for promptfree scenarios. During the text prompt training stage, we adopt the same settings as [[5]](#b4). Notably, YOLOE-v8-S / M / L can be trained on 8 Nvidia RTX4090 GPUs in 12.0 / 17.0 / 22.5 hours, with 3× less cost compared with YOLO-World. For visual prompt training, we freeze all other parts and adopt the same setting as in text prompt training. To enable prompt-free capability, we leverage the same data to train a specialized embedding. We can see that YOLOE not only enjoys low training costs but also show exceptional zero-shot performance. Besides, to verify YOLOE's good transferability on downstream tasks, we fine-tune our YOLOE on COCO [[34]](#b34) for closed-set detection and segmentation. We experiment with two distinct practical finetuning strategies: (1) Linear probing: Only the classification head is learnable and (2) Full tuning: All parameters are trainable. For Linear probing, we train all models for only 10 epochs. For Full tuning, we train small scale models including YOLOE-v8-S / 11-S for 160 epochs, and medium and large scale models including YOLOE-v8-M / L and YOLOE-11-M / L for 80 epochs, respectively.

Metric. For text prompt evaluation, we utilize all category names from the benchmark as inputs, adhering to the standard protocol for open-vocabulary object detection tasks. For visual prompt evaluation, following [[20]](#b19), for each category, we randomly sample N training images (N =16 by default), extract visual embeddings using their ground truth bounding boxes, and compute the average prompt embedding. For prompt-free evaluation, we employ the same protocol as [[33]](#b33). A pretrained text encoder [[57]](#b58) is employed to map open-ended predictions to semantically similar category names within the benchmark. In contrast to [[33]](#b33), we streamline the mapping process by selecting the most confident prediction, and eliminating the need for top-k selection and beam search. We use the tag list from [[16]](#b15) as the Table [1](#). Zero-shot detection evaluation on LVIS. For fair comparisons, Fixed AP is reported on LVIS minival set in a zero-shot manner. The training time is for text prompts, based on 8 Nvidia V100 GPUs for [[32,](#b32)[65]](#b66) and 8 RTX4090 GPUs for YOLO-World and YOLOE. The FPS is measured on Nvidia T4 GPU using TensorRT and on iPhone 12 using CoreML, respectively. Results are provided with text prompt (T) and visual prompt (V) type. For training data, OI, HT, and CH indicates OpenImages [[24]](#b23), HierText [[39]](#b40), and CrowdHuman [[51]](#b52), respectively. OG indicates Objects365 [[52]](#b53) and GoldG [[22]](#b21), and G-20M represents Grounding-20M [[50]](#b51).

## Model

Prompt Type Params Training Data Training Time FPS T4 / iPhone AP APr APc AP f GLIP-T [32] T 232M OG,Cap4M 1337.6h -/ -26.0 20.8 21.4 31.0 GLIPv2-T [70] T 232M OG,Cap4M --/ -29.0 ---GDINO-T [37] T 172M OG,Cap4M --/ -27.4 18.1 23.3 32.7 DetCLIP-T [65] T 155M OG 250.0h -/ -34.4 26.9 33.9 36.3 G-1.5 Edge [50] T -G-20M --/ -33.5 28.0 34.3 33.9 T-Rex2 [20] V -O365,OI,HT CH,SA-1B --/ -37.4 29.9 33.9 41.8 YWorldv2-S [5] T 13M OG 41.7h 216.4 / 48.9 24.4 17.1 22.5 27.3 YWorldv2-M [5] T 29M OG 60.0h 117.9 / 34.2 32.4 28.4 29.6 35.5 YWorldv2-L [5] T 48M OG 80.0h 80.0 / 22.1 35.5 25.6 34.6 38.1 YOLOE-v8-S T / V 12M / 13M OG 12.0h 305.8 / 64.3 27.9 / 26.2 22.3 / 21.3 27.8 / 27.7 29.0 / 25.7 YOLOE-v8-M T / V 27M / 30M OG 17.0h 156.7 / 41.7 32.6 / 31.0 26.9 / 27.0 31.9 / 31.7 34.4 / 31.1 YOLOE-v8-L T / V 45M / 50M OG 22.5h 102.5 / 27.2 35.9 / 34.2 33.2 / 33.2 34.8 / 34.6 37.3 / 34.1 YOLOE-11-S T / V 10M / 12M OG 13.0h 301.2 / 73.3 27.5 / 26.3 21.4 / 22.5 26.8 / 27.1 29.3 / 26.4 YOLOE-11-M T / V 21M / 27M OG 18.5h 168.3 / 39.2 33.0 / 31.4 26.9 / 27.1 32.5 / 31.9 34.5 / 31.7 YOLOE-11-L T / V 26M / 32M OG 23.5h 130.5 / 35.1 35.2 / 33.7 29.1 / 28.1 35.0 / 34.6 36.5 / 33.8

built-in large vocabulary with total 4585 category names, and empirically use δ = 0.001 for LRPC, by default. For all three prompt types, following [[5,](#b4)[20,](#b19)[33]](#b33), evaluations are conducted on LVIS [[14]](#b13) in a zero-shot manner, which contains 1,203 categories. By default, Fixed AP [[7]](#b6) on LVIS minival subset is reported. For transferring to COCO, standard AP is evaluated, following [[1,](#b0)[21]](#b20). Besides, we measure the FPS for all models on Nvidia T4 GPU with TensorRT and mobile device iPhone 12 with CoreML.

## Text and visual prompt evaluation

As shown in Tab. 1, for detection on LVIS, YOLOE exhibits favorable trade-offs between efficiency and zero-shot performance across different model scales. We also note that such results are achieved under much less training time, e.g., 3× faster than YOLO-Worldv2. Specifically, YOLOE-v8-S / M / L outperforms YOLOv8-Worldv2-S / M / L by 3.5 / 0.2 / 0.4 AP, along with 1.4× / 1.3× / 1.3× and 1.3× / 1.2× / 1.2× inference speedups on T4 and iPhone 12, respectively. Besides, for rare category which is challenging, our YOLOE-v8-S and YOLOE-v8-L obtains significant improvements of 5.2% and 7.6% AP r . Besides, compared with YOLO-Worldv2, while YOLOE-v8-M / L achieves lower AP f , this performance gap primarily stems from YOLOE's integration of both detection and segmentation in one model. Such multi-task learning introduces a trade-off that adversely impact detection performance on frequent categories, as shown in Tab. 5. Besides, YOLOE with YOLO11 architecture also exhibits favorable perfor-mance and efficiency. For example, YOLOE-11-L achieves comparable AP with YOLO-Worldv2-L, but with notably 1.6× inference speedups on T4 and iPhone 12, highlighting the strong generalizability of our YOLOE. Moreover, the inclusion of visual prompts further amplifies YOLOE's versatility. Compared with T-Rex2, YOLOE-v8-L yield the improvements of 3.3 AP r and 0.9 AP c , with 2× less training data (3.1 M vs. Our: 1.4 M) and much lower training resource (16 Nvidia A100 GPUs vs. Our: 8 Nvidia RTX4090 GPUs). Besides, for visual prompts, while we only train SAVPE with other parts frozen for 2 epochs, we note that it can achieve comparable AP r and AP c with the text prompts for various model scales. This shows the efficacy of visual prompts in less frequent objects that text prompts often struggle to accurately describe, which is similar to the observation in [[20]](#b19).

Furthermore, for segmentation, we present the evaluation results on the LVIS val set with the standard AP m reported in Tab. 2. It shows that YOLOE exhibits strong performance by leveraging both text prompts and visual prompts. Specifically, YOLOE-v8-M / L achieves 20.8 and 23.5 AP m in the zero-shot manner, significantly outperforming YOLO-Worldv2-M / L that is fine-tuned on LVIS-Base dataset, by 3.0 and 3.7 AP m , respectively. These results well show the superiority of YOLOE.

## Prompt-free evaluation

As shown in Tab. 3, for prompt-free scenario, YOLOE also exhibits superior performance and efficiency. Specifically, Table [3](#). Prompt-free evaluation on LVIS. Fixed AP is reported on the LVIS minival set, following the protocol in [[33]](#b33). The FPS is measured on Nvidia T4 GPU with Pytorch [[42]](#b43). YOLO-v8-L achieves 27.2 AP and 23.5 AP r , outperforming GenerateU with Swin-T backbone by 0.4 AP and 3.5 AP r , along with 6.3× fewer parameters and 53× inference speedups. It shows the effectiveness of YOLOE by reformulating the open-ended problem as the retrieval task for a built-in large vocabulary and underscores its potential in generalizing across a wide range of categories without replying on explicit prompts. Such functionality also enhances YOLOE's practicality, enabling its application in a broader range of real-world scenarios.

## Model

## Downstream transferring

As shown in Tab. 4, when transferring to COCO for downstream closed-set detection and segmentation, YOLOE exhibits favorable performance under limited training epochs in both two fine-tuning strategies. Specifically, for Linear probing, with less than 2% of the training time, YOLOE-11-M / L can achieve over 80% of the performance of YOLO11-M / L, respectively. This highlights the strong transferability of YOLOE. For Full tuning, YOLOE can further enhance the performance under limited training cost. For example, with nearly 4× less training epochs, YOLOE- v8-M / L outperforms YOLOv8-M / L by 0.4 AP m and 0.6 AP b , respectively. Under 3× less training time, YOLO-v8-S also obtains better performance compared with YOLOv8-S for both detection and segmentation. These results well demonstrate that YOLOE can serve as a strong starting point for transferring to downstream task.

## Ablation study

We further provide extensive analyses for the effectiveness of designs in our YOLOE. Experiments are conducted on YOLOE-v8-L and standard AP is reported on LVIS minival set for zero-shot evaluation, by default.

Roadmap to YOLOE. We outline the stepwise progression from the baseline model YOLOv8-Worldv2-L to our  Table 7. Effective. of LRPC. YOLOE-v8-L in terms of text prompts in Tab. 5. With the initial baseline metric of 33.0% AP, due to limited computational resource, we first reduce the training epochs to 30, leading to 31.0% AP. Besides, instead of using empty string as negative texts for grounding data, we follow [[65]](#b66) by maintaining a global dictionary to sample more diverse negative prompts. The global dictionary is constructed by selecting category names that appear more than 100 times in the training data. This leads to 0.9% AP improvement. Next, we remove the cross-modality fusion to avoid costly visual-textual feature interaction, which results in 1.9% AP degradation but with 1.28× and 1.23× inference speedups on T4 and iPhone 12, respectively. To address this drop, we utilize stronger MobileCLIP-B(LT) text encoder [[57]](#b58) to obtain better pretrained textual embeddings, which recovers AP to 31.5%. Furthermore, we employ RepRTA to enhance the alignment between anchor points' object and textual embeddings, which leads to notable 2.3% AP enhancement with zero inference overhead, showing its effectiveness. At last, we introduce the segmentation head and train YOLOE for detection and segmentation simultaneously. Although this leads to 0.2% AP and 0.9 AP f drop due to multi-task learning, YOLOE gains ability to segment arbitrary objects.

Effectiveness of SAVPE. To verify the effectiveness of SAVPE for visual inputs, we remove the activation branch and simply leverage mask pooling to aggregate semantic features with the formulated visual prompt mask. As shown in Tab. 6, SAVPE significantly outperforms "Mask pool" by 1.5 AP. This is because "Mask pool" neglects the varying semantic importance at different positions within promptindicated region, while our activation branch effectively models such difference, leading to improved aggregation of semantic features and better prompt embedding for contrast. We also examine the impact of different group numbers, i.e., A, in the activation branch. As shown in Tab. 6, performance can also be enhanced with only a group, i.e., A = 1. Besides, we can achieve the strong performance of 31.9 AP under A = 16, obtaining the favorable balance, where more groups lead to marginal performance difference.

Effectiveness of LRPC. To verify the effectiveness of LRPC for prompt-free setting, we introduce the baseline that directly leverage the built-in large vocabulary as text prompts for YOLOE to identify all objects. Tab. 7 presents the comparison results. We observe that with the same performance, our LRPC obtains notably 1.7× / 1.3× inference speedups for YOLOE-v8-S / L, respectively, by lazily retrieving the categories for anchor points with found objects and skipping the numerous irrelevant ones. These results well highlight its efficacy and practicality. Besides, with different threshold δ for filtering, LRPC can achieve different performance and efficiency trade-offs, e.g., enabling 1.9× speedup for YOLOE-v8-S with only 0.2 AP drop.

## Visualization analyses

We conduct visualization analyses for YOLOE in four scenarios: (1) Zero-shot inference on LVIS in where model identifies all objects. We can see that YOLOE performs well and can accurately detect and segment various objects in these diverse scenarios, further showing its efficacy and practicality in various applications.

## Conclusion

In this paper, we present YOLOE, a single highly efficient model that seamlessly integrates object detection and segmentation across diverse open prompt mechanisms. Specifically, we introduce RepRTA, SAVPE, and LRPC to enable YOLOs to process textual prompt, visual cues, and prompt-free paradigm with favorable performance and low cost. Thanks to them, YOLOE enjoys strong capabilities and high efficiency for various prompt ways, enabling realtime seeing anything. We hope that it can serve as a strong baseline to inspire further advancements.

## A. More Implementation Details

Data. We employ Objects365 [[52]](#b53), GoldG [[22]](#b21) (including GQA [[17]](#b16) and Flickr30k [[43]](#b44)) for training YOLOE. Tab. 8 present their details. We utilize SAM-2.1-Hiera-Large [[46]](#b47) to generate high-quality pseudo labeling of segmentation masks with ground truth bounding boxes as prompts. We filter out ones with too few areas. To enhance the smoothness of mask edges, we apply Gaussian kernel to masks, using 3×3 and 7×7 kernels for small and large ones, respectively. Besides, we refine the masks following [[9]](#b8), which iteratively simplifies the mask contours. This reduces noise pixels while preserving overall structure. Objects365 [[52]](#b53) Detection ✓ ✓ 609k 8,530k GQA [[17]](#b16) Grounding ✓ ✓ 621k 3,662k Flickr [[43]](#b44) Grounding ✓ ✓ 149k 638k

Training. For all models, we adopt AdamW optimizer with an initial learning rate of 0.002. The batch size and weight decay are set to 128 and 0.025, respectively. The data augmentation includes color jittering, random affine transformations, random horizontal flipping, and mosaic augmentation. During transferring to COCO, for both Linear probing and Full tuning, we utilize the AdamW optimizer with an initial learning rate of 0.001, setting the batch size and weight decay to 128 and 0.025, respectively.

## B. More Analyses for LRPC

To qualitatively show the efficacy of LRPC strategy, we visualize the number of anchor points retained for category retrieval after filtering. We present their average count under varying filtering threshold δ on the LVIS minival set in Fig. [5](#). It reveals that as δ increases, the number of retained anchor points decreases substantially across different models. This reduction significantly lowers computational overhead compared with the baseline scenario, which employs a total of 8400 anchor points. For example, for YOLOE-v8-S, with δ = 0.001, the number of valid anchor points is reduced by 80%, enjoying 1.7× inference speedup with the same performance (see Tab. 7 in the paper). The results further confirm the notably redundancy of anchor points for category retrieval and verify the efficacy of LRPC.

## C. More Visualization Results

To qualitatively show the efficacy of YOLOE, we present more visualization results for it in various scenarios.

Zero-shot inference on LVIS. In Fig. [6](#fig_6), we present the zero-shot inference capabilities of YOLOE on the LVIS. By leveraging the 1203 category names as text prompts, the model demonstrates its ability to detect and segment diverse objects across various images.

Prompt with customized texts. Fig. [7](#fig_7) presents the results with customized text prompts. We can see that YOLOE can interpret both generic and specific textual inputs, enabling precise object detection and fine-grained segmentation. Such capability allows users to tailor the model's behavior to meet specific requirements by defining input prompts at varying levels of granularity.

Prompt with visual inputs. In Fig. [8](#fig_8), we present the results of YOLOE with visual inputs as prompt. The visual inputs can take various forms, such as bounding box, point, or handcrafted shape. It can also be provided across the images. We can see that with visual prompt indicating the target object, YOLOE can accurately find other instances of the same category. Beside, it performs well across different objects and images, exhibiting robust capability.

Prompt-free inference. Fig. [9](#) shows the results of YOLOE with the prompt-free paradigm. We can see that in such setting, YOLOE achieves effective identification for diverse objects. This highlights its practicality in scenarios where predefined inputs are unavailable or impractical.   

![Figure 2. The overview of YOLOE, which supports detection and segmentation for diverse open prompt mechanisms. For text prompts,We design a re-parameterizable region-text alignment strategy to improve performance with zero inference and transferring overhead. For visual prompts, SAVPE is employed to encode visual cues with enhanced prompt embedding under minimal cost. For prompt-free setting, we introduce lazy region-prompt contrast strategy to provide category names for all identified objects efficiently by retrieval.]()

![Backbone Params AP APr APc AP f FPS GenerateU [33] Swin-T 297M 26.8 20.0 24.9 29.8 0.48 GenerateU [33] Swin-L 467M 27.9 22.3 25.2 31.4 0.40 YOLOE-v8-S YOLOv8-S 13M 21.0 19.1 21.3 21.0 95.8 YOLOE-v8-M YOLOv8-M 29M 24.7 22.2 24.5 25.3 45.9 YOLOE-v8-L YOLOv8-L 47M 27.2 23.5 27.0 28.0 25.3 YOLOE-11-S YOLO11-S 11M 20.6 18.4 20.2 21.3 93.0 YOLOE-11-M YOLO11-M 24M 25.5 21.6 25.5 26.1 42.5 YOLOE-11-L YOLO11-L 29M 26.3 22.7 25.8 27.5 34.9]()

![Figure 4. (a) Zero-shot inference on LVIS. (b) Results with customized text prompt, where "white hat, red hat, white car, sunglasses, mustache, tie" are provided as text prompts. (c) Results with visual prompt, where the red dashed bounding box serves as the visual cues. (d) Results in prompt-free scenario, where no explicit prompt is provided. Please refer to the supplementary for more examples.]()

![.1 21.4 21.0 56.5 δ = 1e -3 21.0 19.1 21.3 21.0 95.8 δ = 1e -4 21.0 19.1 21.3 21.0 66.1 δ = 1e -2 20.8 19.1 21.2 20.8 106 v8-L ✗ 27.2 23.5 27.0 28.0 19.9 δ = 1e -3 27.2 23.5 27.0 28.0 25.3]()

![(a), where its category names are text prompts, (2) Text prompts in Fig. 4.(b), where arbitrary texts can be input as prompts, (3) Visual prompts in Fig. 4.(c), where visual cues can be drawn as prompts, and (4) No explicit prompt in Fig. 4.(d),]()

![Figure 5. The count of retained anchor points under different filtering thresholds in LRPC. The dashed line means the total number.]()

![Figure 6. Zero-Shot inference on LVIS. The categories of LVIS are provided as text prompts.]()

![Figure 7. Prompt with customized texts. YOLOE adapts to both generic and specific text prompts for flexible usage.]()

![Figure 8. Prompt with visual inputs. YOLOE demonstrates the ability to identify objects guided by various visual prompts, like bounding box (top left), point (top right), handcrafted shape (bottom left). The visual prompt can also be applied across images (bottom right).]()

![Segmentation evaluation on LVIS. We evaluate all models on LVIS val set with the standard AP m reported. YOLOE supports both text (T) and visual cues (V) as inputs. † indicates that the pretrained models are fine-tuned on LVIS-Base data for segmentation head. In contrast, we evaluate YOLOE in a zero-shot manner without utilizing any images from LVIS during training. YOLOE-v8-S T / V 17.7 / 16.8 15.5 / 13.5 16.3 / 16.7 20.3 / 18.2 YOLOE-v8-M T / V 20.8 / 20.3 17.2 / 17.0 19.2 / 20.1 24.2 / 22.0 YOLOE-v8-L T / V 23.5 / 22.0 21.9 / 16.5 21.6 / 22.1 26.4 / 24.3 YOLOE-11-S T / V 17.6 / 17.1 16.1 / 14.4 15.6 / 16.8 20.5 / 18.6 YOLOE-11-M T / V 21.1 / 21.0 17.2 / 18.3 19.6 / 20.6 24.4 / 22.6 YOLOE-11-L T / V 22.6 / 22.5 19.3 / 20.5 20.9 / 21.7 26.0 / 24.1]()

![Downstream transfer on COCO. We fine-tune YOLOE on COCO and report the standard AP for both detection and segmentation. We experiment with two practical fine-tuning strategies, i.e., Linear probing and Full tuning.]()

![Roadmap to YOLOE in terms of text prompts. The standard AP is reported on LVIS minival set in the zero-shot manner. The FPS is is measured on Nvidia T4 GPU and iPhone 12 with TensorRT (T) and CoreML (C), respectively.]()

![Effective. of SAVPE.]()

![Data details for YOLOE training.]()

Figure 9. Prompt-free inference (omitting segmentation masks for clearer visualization). YOLOE can find diverse objects without prompt.

