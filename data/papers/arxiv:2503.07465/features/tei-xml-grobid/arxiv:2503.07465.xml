<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YOLOE: Real-Time Seeing Anything</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-10">10 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zijia</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">YOLOE: Real-Time Seeing Anything</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-10">10 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">631239DFC340591F97E818089B72169D</idno>
					<idno type="arXiv">arXiv:2503.07465v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-03-18T18:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a builtin large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3× less training cost and 1.4× inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP b and 0.4 AP m gains over closed-set YOLOv8-L with nearly 4× less training time. Code and models are available at <ref type="url" target="https://github.com/THU-MIG/yoloe">https: //github.com/THU-MIG/yoloe</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection and segmentation are foundational tasks in computer vision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">48]</ref>, with widespread applications spanning autonomous driving <ref type="bibr" target="#b1">[2]</ref>, medical analyses <ref type="bibr" target="#b56">[55</ref>], * Equal contribution. 7UDLQLQJ7LPHK /9,6$3 ×/HVV7 LPH &lt;2/2:RUOGY &lt;2/2(2XUV )36Z7HQVRU57 × 6 S H H G X S &lt;2/2:RUOGY &lt;2/2(2XUV )36Z&amp;RUH0/ × 6 S H H G X S &lt;2/2:RUOGY &lt;2/2(2XUV Figure 1. Comparison of performance, training cost, and inference efficiency between YOLOE (Ours) and advanced YOLO-Worldv2 in terms of open text prompts. LVIS AP is evaluated on minival set and FPS w/ TensorRT and w/ CoreML is measured on T4 GPU and iPhone 12, respectively. The results highlight our superiority.</p><p>and robotics <ref type="bibr" target="#b7">[8]</ref>, etc. Traditional approaches like YOLO series <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b48">47]</ref>, have leveraged convolutional neural networks to achieve real-time remarkable performance. However, their dependence on predefined object categories constrains flexibility in practical open scenarios. Such scenarios increasingly demand models capable of detecting and segmenting arbitrary objects guided by diverse prompt mechanisms, such as texts, visual cues, or without prompt. Given this, recent efforts have shifted towards enabling models to generalize for open prompts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b81">80]</ref>. They target single prompt type, e.g., GLIP <ref type="bibr" target="#b32">[32]</ref>, or multiple prompt types in a unified way, e.g., DINO-X <ref type="bibr" target="#b50">[49]</ref>. Specifically, with region-level vision-language pretraining <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b66">65]</ref>, text prompts are usually processed by text encoder to serve as contrastive objectives for region features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b50">49]</ref>, achieving recognition for arbitrary categories, e.g., YOLO-World <ref type="bibr" target="#b4">[5]</ref>. For visual prompts, they are often encoded as class embeddings tied to specified regions for identifying similar objects, by the interaction with image features or language-aligned visual encoder <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">49]</ref>, e.g., T-Rex2 <ref type="bibr" target="#b19">[20]</ref>. In prompt-free scenario, existing methods typically integrate language models, finding all objects and generating the corresponding category names conditioned on region features sequentially <ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b63">62]</ref>, e.g., GenerateU <ref type="bibr" target="#b33">[33]</ref>.</p><p>Despite notable advancements, a single model that supports diverse open prompts for arbitrary objects with high efficiency and accuracy is still lacking. For example, DINO-X <ref type="bibr" target="#b50">[49]</ref> features a unified architecture, which, however, incurs resource-intensive training and inference overhead. Additionally, individual designs for different prompts in separate works exhibit suboptimal trade-offs between performance and efficiency, making it difficult to directly combine them into one model. For example, text-prompted approaches often incur substantial computational overhead when incorporating large vocabularies, due to complexity of cross-modality fusion <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b50">49]</ref>. Visual-prompted methods usually compromise deployability on edge devices owing to the transformer-heavy design or reliance on additional visual encoder <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b68">67]</ref>. Prompt-free ways, meanwhile, depend on large language models, introducing considerable memory and latency costs <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b50">49]</ref>.</p><p>In light of these, in this paper, we introduce YOLOE(ye), a highly efficient, unified, and open object detection and segmentation model, like human eye, under different prompt mechanisms, like texts, visual inputs, and promptfree paradigm. We begin with YOLO models with widely proven efficacy. For text prompts, we propose a Reparameterizable Region-Text Alignment (RepRTA) strategy, which employs a lightweight auxiliary network to improve pretrained textual embeddings for better visualsemantic alignment. During training, pre-cached textual embeddings require only the auxiliary network to process text prompts, incurring low additional cost compared with closed-set training. At inference and transferring, auxiliary network is seamlessly re-parameterized into the classification head, yielding an architecture identical to YOLOs with zero overhead. For visual prompts, we design a Semantic-Activated Visual Prompt Encoder (SAVPE). By formalizing regions of interest as masks, SAVPE fuses them with multi-scale features from PAN to produce grouped promptaware weights in low dimension in an activation branch and extract prompt-agnostic semantic features in a semantic branch. Prompt embeddings are derived through aggregation of them, resulting in favorable performance with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. Without relying on costly language models, LRPC leverages a specialized prompt embedding to find all objects and a built-in large vocabulary for category retrieval. By matching only anchor points with identified objects against the vocabulary, LRPC ensures high performance with low overhead.</p><p>Thanks to them, YOLOE excels in detection and segmentation across diverse open prompt mechanisms within one model, enjoying high inference efficiency and low training cost. Notably, as shown in Fig. <ref type="figure">1</ref>, under 3× less training cost, YOLOE-v8-S significantly outperforms YOLO-Worldv2-S [5] by 3.5 AP on LVIS <ref type="bibr" target="#b13">[14]</ref>, with 1.4× and 1.3× inference speedups on T4 and iPhone 12, respectively. In visual-prompted and prompt-free settings, YOLOE-v8-L outperforms T-Rex2 by 3.3 AP r and GenerateU by 0.4 AP with 2× less training data and 6.3× fewer parameters, respectively. For transferring to COCO <ref type="bibr" target="#b34">[34]</ref>, YOLOE-v8-M / L outperforms YOLOv8-M / L by 0.4 / 0.6 AP b and 0.4 / 0.4 AP m with nearly 4× less training time. We hope that YOLOE can establish a strong baseline and inspire further advancements in real-time open prompt-driven vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditional detection and segmentation. Traditional approaches for object detection and segmentation primarily operate under closed-set paradigms. Early two-stage frameworks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">48]</ref>, exemplified by Faster R-CNN <ref type="bibr" target="#b49">[48]</ref>, introduce region proposal networks (RPNs) followed by region-of-interest (ROI) classification and regression. Meanwhile, single-stage detectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b73">72]</ref> prioritizes speed through grid-based predictions within a single network. The YOLO series <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b60">59,</ref><ref type="bibr" target="#b61">60]</ref> plays a significant role in this paradigm and are widely used in real world. Moreover, DETR <ref type="bibr" target="#b27">[28]</ref> and its variants <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr" target="#b78">77]</ref> mark a major shift by removing heuristicdriven components with transformer-based architectures. To achieve finer-grained results, existing instance segmentation methods predict pixel-level masks rather than bounding box coordinates <ref type="bibr" target="#b14">[15]</ref>. For this, YOLACT <ref type="bibr" target="#b2">[3]</ref> facilitates real-time instance segmentation through integration of prototype masks and mask coefficients. Based on DINO <ref type="bibr" target="#b70">[69]</ref>, MaskDINO <ref type="bibr" target="#b28">[29]</ref> utilizes query embeddings and a highresolution pixel embedding map to produce binary masks.</p><p>Text-prompted detection and segmentation. Recent advancements in open-vocabulary object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b75">[74]</ref><ref type="bibr" target="#b76">[75]</ref><ref type="bibr" target="#b77">[76]</ref> have focused on detecting novel categories by aligning visual features with textual embeddings. Specifically, GLIP <ref type="bibr" target="#b32">[32]</ref> unifies object detection and phrase grounding through grounded pre-training on largescale image-text pairs, demonstrating robust zero-shot performance. DetCLIP <ref type="bibr" target="#b66">[65]</ref> facilitates open-vocabulary learning by enriching the concepts with descriptions. Besides, Grounding DINO <ref type="bibr" target="#b37">[37]</ref> enhances this by integrating crossmodality fusion into DINO, improving alignment between text prompts and visual representations. YOLO-World <ref type="bibr" target="#b4">[5]</ref> further shows the potential of pretraining small detectors with open recognition capabilities based on the YOLO architecture. YOLO-UniOW <ref type="bibr" target="#b36">[36]</ref> builds upon YOLO-World by leveraging the adaptive decision-learning strategy. Similarly, several open-vocabulary instance segmentation models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b64">63]</ref> learn rich visual-semantic knowledge from advanced foundation models to perform segmentation on novel object categories. For example, X-Decoder <ref type="bibr" target="#b80">[79]</ref> and OpenSeeD <ref type="bibr" target="#b72">[71]</ref> explore both the open-vocabulary detection and segmentation tasks. APE <ref type="bibr" target="#b55">[54]</ref> introduces a universal visual perception model that aligns and prompts all objects in image using various text prompts. Visual-prompted detection and segmentation. While Backbone PAN Segmentation Regression Image Embedding P5 P4 P3 man, horse, dog, cat, … Text prompt Text Encoder man horse dog Textual Embedding 𝑃 Auxiliary Network 𝑓 𝜃 Visual prompt Activation Branch Semantic Branch Prompt-aware weight Aggregation Prompt Embedding Semantic-Activated Vis. Prompt Encoder Re-parameterizable Region-Text Alignment Label Re-parameterization Inference Transferring Classification Offline for training/inference Lazy Region-Prompt Contrast Specialized Embedding Anchor Points Prompt free Built-in Vocabulary Retrieval Semantic feature We design a re-parameterizable region-text alignment strategy to improve performance with zero inference and transferring overhead. For visual prompts, SAVPE is employed to encode visual cues with enhanced prompt embedding under minimal cost. For prompt-free setting, we introduce lazy region-prompt contrast strategy to provide category names for all identified objects efficiently by retrieval.</p><p>text prompts offer a generic description, certain objects can be challenging to describe with language alone, such as those requiring specialized domain knowledge. In such cases, visual prompts can guide detection and segmentation more flexibly and specifically, complementing text prompts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. OV-DETR <ref type="bibr" target="#b68">[67]</ref> and OWL-ViT <ref type="bibr" target="#b42">[41]</ref> leverage CLIP encoders to process text and image prompts. MQ-Det <ref type="bibr" target="#b65">[64]</ref> augments text queries with class-specific visual information from query images. DINOv <ref type="bibr" target="#b29">[30]</ref> explores visual prompts as in-context examples for generic and referring vision tasks. T-Rex2 <ref type="bibr" target="#b19">[20]</ref> integrates visual and text prompts by region-level contrastive alignment. For segmentation, based on large-scale data, SAM <ref type="bibr" target="#b22">[23]</ref> presents a flexible and strong model that can be prompted interactively and iteratively. SEEM <ref type="bibr" target="#b81">[80]</ref> further explores segmenting objects with more various prompt types. Semantic-SAM <ref type="bibr" target="#b30">[31]</ref> excels in semantic comprehension and granularity detection, handling both panoptic and part segmentation tasks.</p><p>Prompt-free detection and segmentation. Existing approaches still depend on explicit prompts during inference for open-set detection and segmentation. To address this limitation, several works <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b67">66]</ref> explore integrating with generative language models to produce object descriptions for all found objects. For instance, GRiT <ref type="bibr" target="#b63">[62]</ref> employs a text decoder for both dense captioning and object detection tasks. DetCLIPv3 <ref type="bibr" target="#b67">[66]</ref> trains an object captioner on large-scale data, enabling model to generate rich label information. GenerateU <ref type="bibr" target="#b33">[33]</ref> leverages the language model to generate object names in a free-form way.</p><p>Closing remarks. To the best of our knowledge, aside from DINO-X <ref type="bibr" target="#b50">[49]</ref>, few efforts have achieved object detection and segmentation across various open prompt mechanisms within a single architecture. However, DINO-X entails extensive training cost and notable inference overhead, severely constraining the practicality for real-world edge deployments. In contrast, our YOLOE aims to deliver an efficient and unified model that enjoys real-time performance and efficiency with easy deployability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we detail designs of YOLOE. Building upon YOLOs (Sec. 3.1), YOLOE supports text prompts through RepRTA (Sec. 3.2), visual prompts via SAVPE (Sec. 3.3), and prompt-free scenario with LRPC (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model architecture</head><p>As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, YOLOE adopts the typical YOLOs' architecture <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b48">47]</ref>, consisting of backbone, PAN, regression head, segmentation head, and object embedding head. The backbone and PAN extracts multi-scale features for the image. For each anchor point, the regression head predicts the bounding box for detection, and the segmentation head produces the prototype and mask coefficients for segmentation <ref type="bibr" target="#b2">[3]</ref>. The object embedding head follows the structure of classification head in YOLOs, except that the output channel number of last 1× convolution layer is changed from the class number in closed-set scenario to the embedding dimension. Meanwhile, given text and visual prompts, we employ RepRTA and SAVPE to encode them as normalized prompt embeddings P, respectively. They serve as the classification weights and contrast with the anchor points' object embeddings O to obtain category labels. The process can be formalized as</p><formula xml:id="formula_0">Label = O • P T : R N ×D × R D×C → R N ×C ,<label>(1)</label></formula><p>where N denotes the number of anchor points, C indicates the number of prompts, and D means the feature dimension of embeddings, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Re-parameterizable region-text alignment</head><p>In open-set scenarios, the alignment between textual and object embeddings determines the accuracy of identified categories. Prior works usually introduce complex crossmodality fusion to improve the visual-textual representation for better alignment <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">37]</ref>. However, these ways incur notable computational overhead, especially with large number of texts. Given this, we present Re-parameterizable Region-Text Alignment (RepRTA) strategy, which improves pretrained textual embeddings during training through the reparameterizable lightweight auxiliary network. The alignment between textual and anchor points' object embeddings can be enhanced with zero inference and transferring cost. Specifically, with the text prompts of T with length of C, we first employ the CLIP text encoder <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b58">57]</ref> to obtain pretrained textual embedding P = TextEncoder(T ). Before training, we cache all embeddings of texts in datasets in advance and the text encoder can be removed with no extra training cost. Meanwhile, as shown in Fig. <ref type="figure">3</ref>.(a), we introduce a lightweight auxiliary network f θ with only one feed forward block <ref type="bibr" target="#b54">[53,</ref><ref type="bibr" target="#b59">58]</ref>, where θ indicates the trainable parameters and introduces low overhead compared with closed-set training. It derives the enhanced textual embedding P = f θ (P ) ∈ R C×D for contrasting with the anchor points' object embedding during training, leading to improved visual-semantic alignment. Let K ∈ R D×D ′ ×1×1 be the kernel parameters of last convolution layer with input features I ∈ R D ′ ×H×W in the object embedding head, ⊛ be the convolution operator, and R be the reshape function, we have</p><formula xml:id="formula_1">Label = RD×H×W →HW ×D (I ⊛ K) • (f θ (P )) T .<label>(2)</label></formula><p>Moreover, after training, the auxiliary network can be reparameterized with the object embedding head into the identical classification head of YOLOs. The new kernel parameters K ′ ∈ R C×D ′ ×1×1 for last convolution layer after re-parameterization can be derived by</p><formula xml:id="formula_2">K ′ = RC×D→C×D×1×1(f θ (P )) ⊛ K T .<label>(3)</label></formula><p>The final predication can be obtained by Label = I ⊛ K ′ , which is identical to the original YOLO architecture, leading to zero overhead for deployment and transferring to downstream closed-set tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic-activated visual prompt encoder</head><p>Visual prompts are designed to indicate the object category of interest through visual cues, e.g., box and mask.</p><p>To produce the visual prompt embedding, prior works often employ transformer-heavy design <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>, e.g., deformable attention <ref type="bibr" target="#b79">[78]</ref>, or additional CLIP vision encoder <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b68">67]</ref>. These ways, however, introduce challenges in deployment and efficiency due to complex operators or high computational demands. Considering this, we intro-</p><formula xml:id="formula_3">Linear (a) Linear Linear Axuiliary Network 𝑓 𝜃 U U C 3 × 3 3 × 3 3 × 3 3 × 3 3 × 3 1 × 1 P5 P4 P3 3 × 3 Semantic Branch P5 P4 P3 U U C Visual Prompt C Activation Branch 1 × 1 1 × 1 1 × 1 3 × 3 3 × 3 3 × 3 3 × 3 (b)</formula><p>Textual Embedding 𝑃 Prompt Embedding Text Prompt U Upsample C Aggregation Concatenation 𝐹 𝑉 𝐹 𝐼 Figure 3. (a) The structure of lightweight auxiliary network in RepRTA, which consists of one SwiGLU FFN block [53]. (b) The structure of SAVPE, which consists of semantic branch to generate prompt-agnostic semantic features and activation branch to provide grouped prompt-aware weights. Visual prompt embedding can thus be efficiently derived by their aggregation. duce Semantic-Activated Visual Prompt Encoder (SAVPE) for efficiently processing visual cues. It features two decoupled lightweight branches: (1) Semantic branch outputs prompt-agnostic semantic features in D channels without overhead of fusing visual cues, and (2) Activation branch produces grouped prompt-aware weights by interacting visual cues with image features in much fewer channels under low costs. Their aggregation then leads to informative prompt embedding under minimal complexity.</p><p>As shown in Fig. <ref type="figure">3</ref>.(b), in the semantic branch, we adopt the similar structure as object embedding head. With multiscale features {P 3 , P 4 , P 5 } from PAN, we employ two 3×3 convs for each scale, respectively. After upsampling, features are concatenated and projected to derive semantic features S ∈ R D×H×W . In the activation branch, we formalize visual prompt as mask with 1 for indicated region and 0 for others. We downsample it and leverage 3×3 conv to derive prompt feature F V ∈ R A×H×W . Besides, we obtain image features F I ∈ R A×H×W for fusion with it from {P 3 , P 4 , P 5 } by convs. F V and F I are then concatenated and utilized to output prompt-aware weights W ∈ R A×H×W , which is normalized using softmax within prompt-indicated region. Moreover, we divide the channels of S into A groups with D A channels in each. The channels in the i-th group share the weight W i:i+1 from the ith channel of W. With A ≪ D, we can process visual cues with image features in low dimension, bringing minimal cost. Furthermore, prompt embedding can be derived with aggregation of two branches by</p><formula xml:id="formula_4">P = Concat(G1, ..., GA); Gi = Wi:i+1 • S T D A * i: D A * (i+1) . (4)</formula><p>It can thus contrast with anchor points' object embeddings to identify objects with category of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Lazy region-prompt contrast</head><p>In prompt-free scenario without explicit guidance, models are expected to identity all objects with names in the image. Prior works usually formulate such setting as a generative problem, where language model is employed to generate categories for dense found objects <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b63">62]</ref>. However, this introduces notable overhead, where language models, e.g., FlanT5-base <ref type="bibr" target="#b5">[6]</ref> with 250M parameters in Genera-teU <ref type="bibr" target="#b33">[33]</ref> and OPT-125M <ref type="bibr" target="#b74">[73]</ref> in DINO-X <ref type="bibr" target="#b50">[49]</ref>, are far from meeting high efficiency requirement. Given this, we reformulate such setting as a retrieval problem and present Lazy Region-Prompt Contrast (LRPC) strategy. It lazily retrieves category names from a built-in large vocabulary for anchor points with objects in the cost-effective way. Such paradigm enjoys zero dependency on language models, meanwhile with favorable efficiency and performance.</p><p>Specifically, with pretrained YOLOE, we introduce a specialized prompt embedding and train it exclusively to find all objects, where objects are treated as one category. Meanwhile, we follow <ref type="bibr" target="#b15">[16]</ref> to collect a large vocabulary which covers various categories and serve as the built-in data source for retrieval. One may directly leverage the large vocabulary as text prompts for YOLOE to identify all objects, which, however, incurs notable computational cost by contrasting abundant anchor points' object embeddings with numerous textual embeddings. Instead, we employ the specialized prompt embedding P s to find the set O ′ of anchor points corresponding to objects by</p><formula xml:id="formula_5">O ′ = {o ∈ O | o • P T s &gt; δ},<label>(5)</label></formula><p>where O denotes all anchor points and δ is the threshold hyperparameter for filtering. Then, only anchor points in O ′ are lazily matched against the built-in vocabulary to retrieve category names, bypassing the cost for irrelevant anchor points. This further improves efficiency without performance drop, facilitating the real world application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training objective</head><p>During training, we follow <ref type="bibr" target="#b4">[5]</ref> to obtain an online vocabulary for each mosaic sample with the texts involved in the images as positive labels. Following <ref type="bibr" target="#b20">[21]</ref>, we leverage taskaligned label assignment to match predictions with ground truths. The binary cross entropy loss is employed for classification, with IoU loss and distributed focal loss adopted for regression. For segmentation, we follow <ref type="bibr" target="#b2">[3]</ref> to utilize binary cross-entropy loss for optimizing masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Model. For fair comparison with <ref type="bibr" target="#b4">[5]</ref>, we employ the same YOLOv8 architecture <ref type="bibr" target="#b20">[21]</ref> for YOLOE. Besides, to verify its good generalizability on other YOLOs, we also experiment with YOLO11 architecture <ref type="bibr" target="#b20">[21]</ref>. For both of them, we provide three model scales, i.e., small (S), medium (M), and large (L), to suit various application needs. Text prompts are encoded using the pretrained MobileCLIP-B(LT) <ref type="bibr" target="#b58">[57]</ref> text encoder. We empirically use A = 16 in SAVPE, by default.</p><p>Data. We follow <ref type="bibr" target="#b4">[5]</ref> to utilize detection and grounding datasets, including Objects365 (V1) <ref type="bibr" target="#b53">[52]</ref>, GoldG <ref type="bibr" target="#b21">[22]</ref> (includes GQA <ref type="bibr" target="#b16">[17]</ref> and Flickr30k <ref type="bibr" target="#b44">[43]</ref>), where images from COCO <ref type="bibr" target="#b34">[34]</ref> are excluded. Beside, we leverage advanced SAM-2.1 <ref type="bibr" target="#b47">[46]</ref> model to generate pseudo instance masks using ground truth bounding boxes from the detection and grounding datasets for segmentation data. These masks undergo filtering and simplification to eliminate noise <ref type="bibr" target="#b8">[9]</ref>. For visual prompt data, we follow <ref type="bibr" target="#b19">[20]</ref> to leverage ground truth bounding boxes for visual cues. In prompt-free tasks, we reuse the same datasets, but annotate all objects as a single category to learn a specialized prompt embedding.</p><p>Training. Due to limited computational resource, unlike YOLO-World's training for 100 epochs, we first train YOLOE with text prompts for 30 epochs. Then, we only train the SAVPE for merely 2 epochs with visual prompts, which avoids additional significant training cost that comes with supporting visual prompts. At last, we train the specialized prompt embedding for only 1 epoch for promptfree scenarios. During the text prompt training stage, we adopt the same settings as <ref type="bibr" target="#b4">[5]</ref>. Notably, YOLOE-v8-S / M / L can be trained on 8 Nvidia RTX4090 GPUs in 12.0 / 17.0 / 22.5 hours, with 3× less cost compared with YOLO-World. For visual prompt training, we freeze all other parts and adopt the same setting as in text prompt training. To enable prompt-free capability, we leverage the same data to train a specialized embedding. We can see that YOLOE not only enjoys low training costs but also show exceptional zero-shot performance. Besides, to verify YOLOE's good transferability on downstream tasks, we fine-tune our YOLOE on COCO <ref type="bibr" target="#b34">[34]</ref> for closed-set detection and segmentation. We experiment with two distinct practical finetuning strategies: (1) Linear probing: Only the classification head is learnable and (2) Full tuning: All parameters are trainable. For Linear probing, we train all models for only 10 epochs. For Full tuning, we train small scale models including YOLOE-v8-S / 11-S for 160 epochs, and medium and large scale models including YOLOE-v8-M / L and YOLOE-11-M / L for 80 epochs, respectively.</p><p>Metric. For text prompt evaluation, we utilize all category names from the benchmark as inputs, adhering to the standard protocol for open-vocabulary object detection tasks. For visual prompt evaluation, following <ref type="bibr" target="#b19">[20]</ref>, for each category, we randomly sample N training images (N =16 by default), extract visual embeddings using their ground truth bounding boxes, and compute the average prompt embedding. For prompt-free evaluation, we employ the same protocol as <ref type="bibr" target="#b33">[33]</ref>. A pretrained text encoder <ref type="bibr" target="#b58">[57]</ref> is employed to map open-ended predictions to semantically similar category names within the benchmark. In contrast to <ref type="bibr" target="#b33">[33]</ref>, we streamline the mapping process by selecting the most confident prediction, and eliminating the need for top-k selection and beam search. We use the tag list from <ref type="bibr" target="#b15">[16]</ref> as the Table <ref type="table">1</ref>. Zero-shot detection evaluation on LVIS. For fair comparisons, Fixed AP is reported on LVIS minival set in a zero-shot manner. The training time is for text prompts, based on 8 Nvidia V100 GPUs for <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b66">65]</ref> and 8 RTX4090 GPUs for YOLO-World and YOLOE. The FPS is measured on Nvidia T4 GPU using TensorRT and on iPhone 12 using CoreML, respectively. Results are provided with text prompt (T) and visual prompt (V) type. For training data, OI, HT, and CH indicates OpenImages <ref type="bibr" target="#b23">[24]</ref>, HierText <ref type="bibr" target="#b40">[39]</ref>, and CrowdHuman <ref type="bibr" target="#b52">[51]</ref>, respectively. OG indicates Objects365 <ref type="bibr" target="#b53">[52]</ref> and GoldG <ref type="bibr" target="#b21">[22]</ref>, and G-20M represents Grounding-20M <ref type="bibr" target="#b51">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Prompt Type Params Training Data Training Time FPS T4 / iPhone AP APr APc AP f GLIP-T [32] T 232M OG,Cap4M 1337.6h -/ -26.0 20.8 21.4 31.0 GLIPv2-T [70] T 232M OG,Cap4M --/ -29.0 ---GDINO-T [37] T 172M OG,Cap4M --/ -27.4 18.1 23.3 32.7 DetCLIP-T [65] T 155M OG 250.0h -/ -34.4 26.9 33.9 36.3 G-1.5 Edge [50] T -G-20M --/ -33.5 28.0 34.3 33.9 T-Rex2 [20] V -O365,OI,HT CH,SA-1B --/ -37.4 29.9 33.9 41.8 YWorldv2-S [5] T 13M OG 41.7h 216.4 / 48.9 24.4 17.1 22.5 27.3 YWorldv2-M [5] T 29M OG 60.0h 117.9 / 34.2 32.4 28.4 29.6 35.5 YWorldv2-L [5] T 48M OG 80.0h 80.0 / 22.1 35.5 25.6 34.6 38.1 YOLOE-v8-S T / V 12M / 13M OG 12.0h 305.8 / 64.3 27.9 / 26.2 22.3 / 21.3 27.8 / 27.7 29.0 / 25.7 YOLOE-v8-M T / V 27M / 30M OG 17.0h 156.7 / 41.7 32.6 / 31.0 26.9 / 27.0 31.9 / 31.7 34.4 / 31.1 YOLOE-v8-L T / V 45M / 50M OG 22.5h 102.5 / 27.2 35.9 / 34.2 33.2 / 33.2 34.8 / 34.6 37.3 / 34.1 YOLOE-11-S T / V 10M / 12M OG 13.0h 301.2 / 73.3 27.5 / 26.3 21.4 / 22.5 26.8 / 27.1 29.3 / 26.4 YOLOE-11-M T / V 21M / 27M OG 18.5h 168.3 / 39.2 33.0 / 31.4 26.9 / 27.1 32.5 / 31.9 34.5 / 31.7 YOLOE-11-L T / V 26M / 32M OG 23.5h 130.5 / 35.1 35.2 / 33.7 29.1 / 28.1 35.0 / 34.6 36.5 / 33.8</p><p>built-in large vocabulary with total 4585 category names, and empirically use δ = 0.001 for LRPC, by default. For all three prompt types, following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">33]</ref>, evaluations are conducted on LVIS <ref type="bibr" target="#b13">[14]</ref> in a zero-shot manner, which contains 1,203 categories. By default, Fixed AP <ref type="bibr" target="#b6">[7]</ref> on LVIS minival subset is reported. For transferring to COCO, standard AP is evaluated, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>. Besides, we measure the FPS for all models on Nvidia T4 GPU with TensorRT and mobile device iPhone 12 with CoreML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Text and visual prompt evaluation</head><p>As shown in Tab. 1, for detection on LVIS, YOLOE exhibits favorable trade-offs between efficiency and zero-shot performance across different model scales. We also note that such results are achieved under much less training time, e.g., 3× faster than YOLO-Worldv2. Specifically, YOLOE-v8-S / M / L outperforms YOLOv8-Worldv2-S / M / L by 3.5 / 0.2 / 0.4 AP, along with 1.4× / 1.3× / 1.3× and 1.3× / 1.2× / 1.2× inference speedups on T4 and iPhone 12, respectively. Besides, for rare category which is challenging, our YOLOE-v8-S and YOLOE-v8-L obtains significant improvements of 5.2% and 7.6% AP r . Besides, compared with YOLO-Worldv2, while YOLOE-v8-M / L achieves lower AP f , this performance gap primarily stems from YOLOE's integration of both detection and segmentation in one model. Such multi-task learning introduces a trade-off that adversely impact detection performance on frequent categories, as shown in Tab. 5. Besides, YOLOE with YOLO11 architecture also exhibits favorable perfor-mance and efficiency. For example, YOLOE-11-L achieves comparable AP with YOLO-Worldv2-L, but with notably 1.6× inference speedups on T4 and iPhone 12, highlighting the strong generalizability of our YOLOE. Moreover, the inclusion of visual prompts further amplifies YOLOE's versatility. Compared with T-Rex2, YOLOE-v8-L yield the improvements of 3.3 AP r and 0.9 AP c , with 2× less training data (3.1 M vs. Our: 1.4 M) and much lower training resource (16 Nvidia A100 GPUs vs. Our: 8 Nvidia RTX4090 GPUs). Besides, for visual prompts, while we only train SAVPE with other parts frozen for 2 epochs, we note that it can achieve comparable AP r and AP c with the text prompts for various model scales. This shows the efficacy of visual prompts in less frequent objects that text prompts often struggle to accurately describe, which is similar to the observation in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Furthermore, for segmentation, we present the evaluation results on the LVIS val set with the standard AP m reported in Tab. 2. It shows that YOLOE exhibits strong performance by leveraging both text prompts and visual prompts. Specifically, YOLOE-v8-M / L achieves 20.8 and 23.5 AP m in the zero-shot manner, significantly outperforming YOLO-Worldv2-M / L that is fine-tuned on LVIS-Base dataset, by 3.0 and 3.7 AP m , respectively. These results well show the superiority of YOLOE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Prompt-free evaluation</head><p>As shown in Tab. 3, for prompt-free scenario, YOLOE also exhibits superior performance and efficiency. Specifically, Table <ref type="table">3</ref>. Prompt-free evaluation on LVIS. Fixed AP is reported on the LVIS minival set, following the protocol in <ref type="bibr" target="#b33">[33]</ref>. The FPS is measured on Nvidia T4 GPU with Pytorch <ref type="bibr" target="#b43">[42]</ref>. YOLO-v8-L achieves 27.2 AP and 23.5 AP r , outperforming GenerateU with Swin-T backbone by 0.4 AP and 3.5 AP r , along with 6.3× fewer parameters and 53× inference speedups. It shows the effectiveness of YOLOE by reformulating the open-ended problem as the retrieval task for a built-in large vocabulary and underscores its potential in generalizing across a wide range of categories without replying on explicit prompts. Such functionality also enhances YOLOE's practicality, enabling its application in a broader range of real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Downstream transferring</head><p>As shown in Tab. 4, when transferring to COCO for downstream closed-set detection and segmentation, YOLOE exhibits favorable performance under limited training epochs in both two fine-tuning strategies. Specifically, for Linear probing, with less than 2% of the training time, YOLOE-11-M / L can achieve over 80% of the performance of YOLO11-M / L, respectively. This highlights the strong transferability of YOLOE. For Full tuning, YOLOE can further enhance the performance under limited training cost. For example, with nearly 4× less training epochs, YOLOE- v8-M / L outperforms YOLOv8-M / L by 0.4 AP m and 0.6 AP b , respectively. Under 3× less training time, YOLO-v8-S also obtains better performance compared with YOLOv8-S for both detection and segmentation. These results well demonstrate that YOLOE can serve as a strong starting point for transferring to downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>We further provide extensive analyses for the effectiveness of designs in our YOLOE. Experiments are conducted on YOLOE-v8-L and standard AP is reported on LVIS minival set for zero-shot evaluation, by default.</p><p>Roadmap to YOLOE. We outline the stepwise progression from the baseline model YOLOv8-Worldv2-L to our  Table 7. Effective. of LRPC. YOLOE-v8-L in terms of text prompts in Tab. 5. With the initial baseline metric of 33.0% AP, due to limited computational resource, we first reduce the training epochs to 30, leading to 31.0% AP. Besides, instead of using empty string as negative texts for grounding data, we follow <ref type="bibr" target="#b66">[65]</ref> by maintaining a global dictionary to sample more diverse negative prompts. The global dictionary is constructed by selecting category names that appear more than 100 times in the training data. This leads to 0.9% AP improvement. Next, we remove the cross-modality fusion to avoid costly visual-textual feature interaction, which results in 1.9% AP degradation but with 1.28× and 1.23× inference speedups on T4 and iPhone 12, respectively. To address this drop, we utilize stronger MobileCLIP-B(LT) text encoder <ref type="bibr" target="#b58">[57]</ref> to obtain better pretrained textual embeddings, which recovers AP to 31.5%. Furthermore, we employ RepRTA to enhance the alignment between anchor points' object and textual embeddings, which leads to notable 2.3% AP enhancement with zero inference overhead, showing its effectiveness. At last, we introduce the segmentation head and train YOLOE for detection and segmentation simultaneously. Although this leads to 0.2% AP and 0.9 AP f drop due to multi-task learning, YOLOE gains ability to segment arbitrary objects.</p><p>Effectiveness of SAVPE. To verify the effectiveness of SAVPE for visual inputs, we remove the activation branch and simply leverage mask pooling to aggregate semantic features with the formulated visual prompt mask. As shown in Tab. 6, SAVPE significantly outperforms "Mask pool" by 1.5 AP. This is because "Mask pool" neglects the varying semantic importance at different positions within promptindicated region, while our activation branch effectively models such difference, leading to improved aggregation of semantic features and better prompt embedding for contrast. We also examine the impact of different group numbers, i.e., A, in the activation branch. As shown in Tab. 6, performance can also be enhanced with only a group, i.e., A = 1. Besides, we can achieve the strong performance of 31.9 AP under A = 16, obtaining the favorable balance, where more groups lead to marginal performance difference.</p><p>Effectiveness of LRPC. To verify the effectiveness of LRPC for prompt-free setting, we introduce the baseline that directly leverage the built-in large vocabulary as text prompts for YOLOE to identify all objects. Tab. 7 presents the comparison results. We observe that with the same performance, our LRPC obtains notably 1.7× / 1.3× inference speedups for YOLOE-v8-S / L, respectively, by lazily retrieving the categories for anchor points with found objects and skipping the numerous irrelevant ones. These results well highlight its efficacy and practicality. Besides, with different threshold δ for filtering, LRPC can achieve different performance and efficiency trade-offs, e.g., enabling 1.9× speedup for YOLOE-v8-S with only 0.2 AP drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization analyses</head><p>We conduct visualization analyses for YOLOE in four scenarios: (1) Zero-shot inference on LVIS in where model identifies all objects. We can see that YOLOE performs well and can accurately detect and segment various objects in these diverse scenarios, further showing its efficacy and practicality in various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present YOLOE, a single highly efficient model that seamlessly integrates object detection and segmentation across diverse open prompt mechanisms. Specifically, we introduce RepRTA, SAVPE, and LRPC to enable YOLOs to process textual prompt, visual cues, and prompt-free paradigm with favorable performance and low cost. Thanks to them, YOLOE enjoys strong capabilities and high efficiency for various prompt ways, enabling realtime seeing anything. We hope that it can serve as a strong baseline to inspire further advancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Implementation Details</head><p>Data. We employ Objects365 <ref type="bibr" target="#b53">[52]</ref>, GoldG <ref type="bibr" target="#b21">[22]</ref> (including GQA <ref type="bibr" target="#b16">[17]</ref> and Flickr30k <ref type="bibr" target="#b44">[43]</ref>) for training YOLOE. Tab. 8 present their details. We utilize SAM-2.1-Hiera-Large <ref type="bibr" target="#b47">[46]</ref> to generate high-quality pseudo labeling of segmentation masks with ground truth bounding boxes as prompts. We filter out ones with too few areas. To enhance the smoothness of mask edges, we apply Gaussian kernel to masks, using 3×3 and 7×7 kernels for small and large ones, respectively. Besides, we refine the masks following <ref type="bibr" target="#b8">[9]</ref>, which iteratively simplifies the mask contours. This reduces noise pixels while preserving overall structure. Objects365 <ref type="bibr" target="#b53">[52]</ref> Detection ✓ ✓ 609k 8,530k GQA <ref type="bibr" target="#b16">[17]</ref> Grounding ✓ ✓ 621k 3,662k Flickr <ref type="bibr" target="#b44">[43]</ref> Grounding ✓ ✓ 149k 638k</p><p>Training. For all models, we adopt AdamW optimizer with an initial learning rate of 0.002. The batch size and weight decay are set to 128 and 0.025, respectively. The data augmentation includes color jittering, random affine transformations, random horizontal flipping, and mosaic augmentation. During transferring to COCO, for both Linear probing and Full tuning, we utilize the AdamW optimizer with an initial learning rate of 0.001, setting the batch size and weight decay to 128 and 0.025, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Analyses for LRPC</head><p>To qualitatively show the efficacy of LRPC strategy, we visualize the number of anchor points retained for category retrieval after filtering. We present their average count under varying filtering threshold δ on the LVIS minival set in Fig. <ref type="figure">5</ref>. It reveals that as δ increases, the number of retained anchor points decreases substantially across different models. This reduction significantly lowers computational overhead compared with the baseline scenario, which employs a total of 8400 anchor points. For example, for YOLOE-v8-S, with δ = 0.001, the number of valid anchor points is reduced by 80%, enjoying 1.7× inference speedup with the same performance (see Tab. 7 in the paper). The results further confirm the notably redundancy of anchor points for category retrieval and verify the efficacy of LRPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Visualization Results</head><p>To qualitatively show the efficacy of YOLOE, we present more visualization results for it in various scenarios.</p><p>Zero-shot inference on LVIS. In Fig. <ref type="figure" target="#fig_6">6</ref>, we present the zero-shot inference capabilities of YOLOE on the LVIS. By leveraging the 1203 category names as text prompts, the model demonstrates its ability to detect and segment diverse objects across various images.</p><p>Prompt with customized texts. Fig. <ref type="figure" target="#fig_7">7</ref> presents the results with customized text prompts. We can see that YOLOE can interpret both generic and specific textual inputs, enabling precise object detection and fine-grained segmentation. Such capability allows users to tailor the model's behavior to meet specific requirements by defining input prompts at varying levels of granularity.</p><p>Prompt with visual inputs. In Fig. <ref type="figure" target="#fig_8">8</ref>, we present the results of YOLOE with visual inputs as prompt. The visual inputs can take various forms, such as bounding box, point, or handcrafted shape. It can also be provided across the images. We can see that with visual prompt indicating the target object, YOLOE can accurately find other instances of the same category. Beside, it performs well across different objects and images, exhibiting robust capability.</p><p>Prompt-free inference. Fig. <ref type="figure">9</ref> shows the results of YOLOE with the prompt-free paradigm. We can see that in such setting, YOLOE achieves effective identification for diverse objects. This highlights its practicality in scenarios where predefined inputs are unavailable or impractical.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The overview of YOLOE, which supports detection and segmentation for diverse open prompt mechanisms. For text prompts,We design a re-parameterizable region-text alignment strategy to improve performance with zero inference and transferring overhead. For visual prompts, SAVPE is employed to encode visual cues with enhanced prompt embedding under minimal cost. For prompt-free setting, we introduce lazy region-prompt contrast strategy to provide category names for all identified objects efficiently by retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Backbone Params AP APr APc AP f FPS GenerateU [33] Swin-T 297M 26.8 20.0 24.9 29.8 0.48 GenerateU [33] Swin-L 467M 27.9 22.3 25.2 31.4 0.40 YOLOE-v8-S YOLOv8-S 13M 21.0 19.1 21.3 21.0 95.8 YOLOE-v8-M YOLOv8-M 29M 24.7 22.2 24.5 25.3 45.9 YOLOE-v8-L YOLOv8-L 47M 27.2 23.5 27.0 28.0 25.3 YOLOE-11-S YOLO11-S 11M 20.6 18.4 20.2 21.3 93.0 YOLOE-11-M YOLO11-M 24M 25.5 21.6 25.5 26.1 42.5 YOLOE-11-L YOLO11-L 29M 26.3 22.7 25.8 27.5 34.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) Zero-shot inference on LVIS. (b) Results with customized text prompt, where "white hat, red hat, white car, sunglasses, mustache, tie" are provided as text prompts. (c) Results with visual prompt, where the red dashed bounding box serves as the visual cues. (d) Results in prompt-free scenario, where no explicit prompt is provided. Please refer to the supplementary for more examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>.1 21.4 21.0 56.5 δ = 1e -3 21.0 19.1 21.3 21.0 95.8 δ = 1e -4 21.0 19.1 21.3 21.0 66.1 δ = 1e -2 20.8 19.1 21.2 20.8 106 v8-L ✗ 27.2 23.5 27.0 28.0 19.9 δ = 1e -3 27.2 23.5 27.0 28.0 25.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>(a), where its category names are text prompts, (2) Text prompts in Fig. 4.(b), where arbitrary texts can be input as prompts, (3) Visual prompts in Fig. 4.(c), where visual cues can be drawn as prompts, and (4) No explicit prompt in Fig. 4.(d),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>&lt;2/ 2 (Figure 5 .</head><label>25</label><figDesc>Figure 5. The count of retained anchor points under different filtering thresholds in LRPC. The dashed line means the total number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Zero-Shot inference on LVIS. The categories of LVIS are provided as text prompts.</figDesc><graphic coords="14,96.49,243.24,136.71,94.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Prompt with customized texts. YOLOE adapts to both generic and specific text prompts for flexible usage.</figDesc><graphic coords="14,96.73,377.74,206.52,142.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Prompt with visual inputs. YOLOE demonstrates the ability to identify objects guided by various visual prompts, like bounding box (top left), point (top right), handcrafted shape (bottom left). The visual prompt can also be applied across images (bottom right).</figDesc><graphic coords="14,425.85,525.85,88.22,137.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Segmentation evaluation on LVIS. We evaluate all models on LVIS val set with the standard AP m reported. YOLOE supports both text (T) and visual cues (V) as inputs. † indicates that the pretrained models are fine-tuned on LVIS-Base data for segmentation head. In contrast, we evaluate YOLOE in a zero-shot manner without utilizing any images from LVIS during training. YOLOE-v8-S T / V 17.7 / 16.8 15.5 / 13.5 16.3 / 16.7 20.3 / 18.2 YOLOE-v8-M T / V 20.8 / 20.3 17.2 / 17.0 19.2 / 20.1 24.2 / 22.0 YOLOE-v8-L T / V 23.5 / 22.0 21.9 / 16.5 21.6 / 22.1 26.4 / 24.3 YOLOE-11-S T / V 17.6 / 17.1 16.1 / 14.4 15.6 / 16.8 20.5 / 18.6 YOLOE-11-M T / V 21.1 / 21.0 17.2 / 18.3 19.6 / 20.6 24.4 / 22.6 YOLOE-11-L T / V 22.6 / 22.5 19.3 / 20.5 20.9 / 21.7 26.0 / 24.1</figDesc><table><row><cell>Model</cell><cell>Prompt</cell><cell>AP m</cell><cell>AP m r</cell><cell>AP m c</cell><cell>AP m f</cell></row><row><cell>YWorld-M  †</cell><cell>T</cell><cell>16.7</cell><cell>12.6</cell><cell>14.6</cell><cell>20.8</cell></row><row><cell>YWorld-L  †</cell><cell>T</cell><cell>19.1</cell><cell>14.2</cell><cell>17.2</cell><cell>23.5</cell></row><row><cell>YWorldv2-M  †</cell><cell>T</cell><cell>17.8</cell><cell>13.9</cell><cell>15.5</cell><cell>22.0</cell></row><row><cell>YWorldv2-L  †</cell><cell>T</cell><cell>19.8</cell><cell>17.2</cell><cell>17.5</cell><cell>23.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Downstream transfer on COCO. We fine-tune YOLOE on COCO and report the standard AP for both detection and segmentation. We experiment with two practical fine-tuning strategies, i.e., Linear probing and Full tuning.</figDesc><table><row><cell>Model</cell><cell cols="2">Epochs AP b AP b 50 AP b 75 AP m AP m 50 AP m 75</cell></row><row><cell></cell><cell cols="2">Training from scratch</cell></row><row><cell>YOLOv8-S</cell><cell cols="2">500 44.7 61.4 48.7 36.6 58.0 38.6</cell></row><row><cell>YOLOv8-M</cell><cell cols="2">300 50.0 66.8 54.8 40.5 63.4 43.3</cell></row><row><cell>YOLOv8-L</cell><cell cols="2">300 52.4 69.3 57.2 42.3 66.0 44.9</cell></row><row><cell>YOLO11-S</cell><cell cols="2">500 46.6 63.3 50.6 37.8 59.7 40.0</cell></row><row><cell>YOLO11-M</cell><cell cols="2">600 51.5 68.5 55.7 41.5 65.0 43.9</cell></row><row><cell>YOLO11-L</cell><cell cols="2">600 53.3 70.1 58.2 42.8 66.8 45.5</cell></row><row><cell></cell><cell cols="2">Linear probing</cell></row><row><cell>YOLOE-v8-S</cell><cell>10</cell><cell>35.6 51.5 38.9 30.3 48.2 32.0</cell></row><row><cell>YOLOE-v8-M</cell><cell>10</cell><cell>42.2 59.2 46.3 35.5 55.6 37.7</cell></row><row><cell>YOLOE-v8-L</cell><cell>10</cell><cell>45.4 63.3 50.0 38.3 59.6 40.8</cell></row><row><cell>YOLOE-11-S</cell><cell>10</cell><cell>37.0 52.9 40.4 31.5 49.7 33.5</cell></row><row><cell>YOLOE-11-M</cell><cell>10</cell><cell>43.1 60.6 47.4 36.5 56.9 39.0</cell></row><row><cell>YOLOE-11-L</cell><cell>10</cell><cell>45.1 62.8 49.5 38.0 59.2 40.6</cell></row><row><cell></cell><cell></cell><cell>Full tuning</cell></row><row><cell>YOLOE-v8-S</cell><cell cols="2">160 45.0 61.6 49.1 36.7 58.3 39.1</cell></row><row><cell>YOLOE-v8-M</cell><cell>80</cell><cell>50.4 67.0 55.2 40.9 63.7 43.5</cell></row><row><cell>YOLOE-v8-L</cell><cell>80</cell><cell>53.0 69.8 57.9 42.7 66.5 45.6</cell></row><row><cell>YOLOE-11-S</cell><cell cols="2">160 46.2 62.9 50.0 37.6 59.3 40.1</cell></row><row><cell>YOLOE-11-M</cell><cell>80</cell><cell>51.3 68.3 56.0 41.5 64.8 44.3</cell></row><row><cell>YOLOE-11-L</cell><cell>80</cell><cell>52.6 69.7 57.5 42.4 66.2 45.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Roadmap to YOLOE in terms of text prompts. The standard AP is reported on LVIS minival set in the zero-shot manner. The FPS is is measured on Nvidia T4 GPU and iPhone 12 with TensorRT (T) and CoreML (C), respectively.</figDesc><table><row><cell>Model</cell><cell>Epochs AP APr APc AP f FPS (T / C)</cell></row><row><cell>YOLO-Worldv2-L</cell><cell>100 33.0 22.6 32.0 35.8 80.0 / 22.1</cell></row><row><cell cols="2">+ Fewer train. epochs 30 31.0 22.6 28.8 34.2 80.0 / 22.1</cell></row><row><cell cols="2">+ Global negative dict. 30 31.9 22.8 31.0 34.4 80.0 / 22.1</cell></row><row><cell cols="2">-Cross-modal. fusion 30 30.0 19.1 28.0 33.9 102.5 / 27.2</cell></row><row><cell cols="2">+ MobileCLIP encoder 30 31.5 20.2 30.5 34.4 102.5 / 27.2</cell></row><row><cell>+ RepRTA</cell><cell>30 33.5 29.5 32.0 35.5 102.5 / 27.2</cell></row><row><cell cols="2">+ Segment. (YOLOE) 30 33.3 30.8 32.2 34.6 102.5 / 27.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Effective. of SAVPE.</figDesc><table><row><cell>Model</cell><cell>AP APr APc APf</cell></row><row><cell cols="2">Mask pool 30.4 27.6 31.3 30.2</cell></row><row><cell>SAVPE</cell><cell>31.9 29.4 32.5 31.7</cell></row><row><cell>A = 1</cell><cell>30.9 28.2 31.9 30.4</cell></row><row><cell>A = 16</cell><cell>31.9 29.4 32.5 31.7</cell></row><row><cell>A = 32</cell><cell>31.9 28.2 33.0 31.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Data details for YOLOE training.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell>Box Mask Images Anno.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Figure 9. Prompt-free inference (omitting segmentation masks for clearer visualization). YOLOE can find diverse objects without prompt.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
	</analytic>
	<monogr>
		<title level="m">Optimal speed and accuracy of object detection</title>
		<imprint>
			<date type="published" when="2006">2020. 1, 2, 3, 6</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Anomaly detection in autonomous driving: A survey</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bogdoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nitsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Zöllner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4488" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2005">2019. 1, 2, 3, 5</date>
			<biblScope unit="page" from="9157" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yolo-world: Real-time open-vocabulary object detection</title>
		<author>
			<persName><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2024. 1, 2, 4, 5, 6</date>
			<biblScope unit="page" from="16901" to="16911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling instructionfinetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Evaluating large-vocabulary object detectors: The devil is in the details</title>
		<author>
			<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01066</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm</title>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Henke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dos</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Welfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Antonio De Souza Leite Cuadros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tello</forename><surname>Gamarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1290" to="1305" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms for the reduction of the number of points required to represent a digitized line or its caricature</title>
		<author>
			<persName><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><surname>Peucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cartographica: the international journal for geographic information and geovisualization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tood: Task-aligned one-stage object detection</title>
		<author>
			<persName><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3490" to="3499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling open-vocabulary image segmentation with image-level labels</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="540" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006">2019. 2, 6</date>
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Open-set image tagging with multi-grained text supervision</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youcai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15200</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling</title>
		<author>
			<persName><forename type="first">Dat</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7020" to="7031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>T-Rex</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13596</idno>
		<title level="m">Counting by visual prompting</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">T-rex2: Towards generic object detection via text-visual prompt synergy</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2024. 1, 2, 3, 4, 5, 6</date>
			<biblScope unit="page" from="38" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Chaurasia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultralytics YOLO</title>
		<imprint>
			<date type="published" when="2006">2023. 1, 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mdetrmodulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segment anything</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4015" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">F-vlm: Open-vocabulary object detection upon frozen vision and language models</title>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representation</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language-driven semantic segmentation</title>
		<author>
			<persName><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><surname>Ranftl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Yolov6: A single-stage object detection framework for industrial applications</title>
		<author>
			<persName><forename type="first">Chuyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lulu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiheng</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaidan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqiang</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.02976</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dn-detr: Accelerate detr training by introducing query denoising</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13619" to="13627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask dino: Towards a unified transformer-based framework for object detection and segmentation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaizhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3041" to="3050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual in-context prompting</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaizhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2024. 1, 2, 3, 4</date>
			<biblScope unit="page" from="12861" to="12871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segment and recognize anything at any granularity</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="467" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-03">2024. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grounded language-image pre-training</title>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006">2022. 1, 2, 6</date>
			<biblScope unit="page" from="10965" to="10975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative region-language pretraining for open-ended object detection</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2024. 1, 2, 3, 5, 6, 7</date>
			<biblScope unit="page" from="13958" to="13968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision-ECCV 2014: 13th European conference</title>
		<meeting><address><addrLine>zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Yolo-uniow: Efficient universal open-world object detection</title>
		<author>
			<persName><forename type="first">Lihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juexiao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.20645</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grounding dino: Marrying dino with grounded pre-training for open-set object detection</title>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="38" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2024. 1, 2, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016: 14th European Conference, Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 14</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards end-to-end unified scene text detection and layout analysis</title>
		<author>
			<persName><forename type="first">Shangbang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Panteleev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1049" to="1059" />
		</imprint>
	</monogr>
	<note>Alessandro Bissacco, Yasuhisa Fujii, and Michalis Raptis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Capdet: Unifying dense captioning and open-world detection pretraining</title>
		<author>
			<persName><forename type="first">Yanxin</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youpeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengzhen</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15233" to="15243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple open-vocabulary object detection</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="728" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Bryan A Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PmLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Denseclip: Language-guided dense prediction with contextaware prompting</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18082" to="18091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ryali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Khedr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Rädle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00714</idno>
		<title level="m">Segment anything in images and videos</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016. 1, 2, 3</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuda</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoke</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.14347</idno>
		<title level="m">Dino-x: A unified vision model for openworld object detection and understanding</title>
		<imprint>
			<date type="published" when="2005">2024. 1, 2, 3, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoke</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuda</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Grounding dino 1.5: Advance the &quot;edge&quot; of open-set object detection</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Crowdhuman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<title level="m">A benchmark for detecting human in a crowd</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aligning and prompting everything all at once for universal visual perception</title>
		<author>
			<persName><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="13193" to="13203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Medyolo: a medical image object detection framework</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Sobek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose R Medina</forename><surname>Inojosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Betsy</forename><forename type="middle">J</forename><surname>Medina Inojosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Rassoulinejad-Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gian</forename><forename type="middle">Marco</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Lopez-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><forename type="middle">J</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging Informatics in Medicine</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Zhi Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mobileclip: Fast image-text models through multi-modal reinforced training</title>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anasosalu</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Pouransari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2024. 4, 5, 8</date>
			<biblScope unit="page" from="15963" to="15974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Real-time end-to-end object detection</title>
		<author>
			<persName><forename type="first">Ao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107984" to="108011" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
		<author>
			<persName><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7464" to="7475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Ovlw-detr: Open-vocabulary light-weighted detection transformer</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangbo</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10655</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Grit: A generative region-to-text transformer for object understanding</title>
		<author>
			<persName><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Groupvit: Semantic segmentation emerges from text supervision</title>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalini</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18134" to="18144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-modal queried object detection in the wild</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4452" to="4469" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Detclip: Dictionary-enriched visual-concept paralleled pretraining for open-world detection</title>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youpeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2022. 1, 2, 6, 8</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9125" to="9138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Detclipv3: Towards versatile generative open-vocabulary object detection</title>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="27391" to="27401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Open-vocabulary detr with conditional matching</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">Dela</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Hao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14393" to="14402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><surname>Dino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03605</idno>
		<title level="m">Detr with improved denoising anchor boxes for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Glipv2: Unifying localization and vision-language understanding</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A simple framework for open-vocabulary segmentation and detection</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1020" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Real-time transformer-based open-vocabulary detection with efficient fusion head</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyusong</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.06892</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Regionclip: Regionbased language-image pretraining</title>
		<author>
			<persName><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16793" to="16803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Detecting twenty-thousand classes using image-level supervision</title>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="350" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Generalized decoding for pixel, image, and language</title>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harkirat</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15116" to="15127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Segment everything everywhere all at once. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">19769-19782, 2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
