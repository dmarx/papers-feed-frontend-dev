- Decision to use a neural model for real-time game simulation
- Choice of diffusion model architecture for frame prediction
- Selection of DOOM as the target game for simulation
- Training methodology: two-phase training (RL-agent followed by diffusion model)
- Use of teacher forcing for training the generative model
- Implementation of conditioning augmentations for stable auto-regressive generation
- Design of the reward function for the RL-agent
- Decision to fine-tune the latent decoder of the auto-encoder
- Choice of DDIM sampling for inference
- Use of Classifier-Free Guidance for past observations
- Strategy for mitigating auto-regressive drift using noise augmentation
- Decision to limit the number of denoising steps during inference
- Choice of PPO for training the RL-agent
- Decision to record agent's training trajectories for generative model training
- Implementation of parallel frame generation for increased performance
- Decision to use a single TPU for real-time simulation
- Choice of PSNR as a quality metric for frame prediction
- Decision to evaluate human raters' ability to distinguish between real and simulated clips
- Use of Gaussian noise for training to improve stability
- Decision to explore model distillation for single-step inference in future work