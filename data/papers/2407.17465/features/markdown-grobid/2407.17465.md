# u-µP: The Unit-Scaled Maximal Update Parametrization

## Abstract

## 

The Maximal Update Parametrization (µP) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-µP, which improves upon µP by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: µP ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-µP models reaching a loss that is equal to or lower than comparable µP models and working out-of-the-box in FP8.

## 

10 0 10 1 10 2 10 3 Run Count 3.2 3.3 3.4 3.5 3.6 Best Validation Loss u-µP 256 µP 256 (a) More Efficient HP Sweeps HP Sweep Strategy u-µP µP Random Search Independent Search → LR → Mults → Combined Mults 2 -11 | 2 -2 2 -9 | 2 0 2 -7 | 2 2 2 -5 | 2 4 µP | u-µP Learning Rate 2.8 3.0 3.2 3.4 3.6 u-µP 256 u-µP 4096 µP 256 (b) Better HP Transfer Widths 128 256 512 1024 2048 4096 0 2500 5000 7500 Training Step 2 4 6 8 10 (c) Simple FP8 Training u-µP 4096 µP 4096 Precision FP32 FP8 µP 4096

Figure [1](#): (a) Two different HP sweeping processes used for µP and u-µP proxy models. Unlike µP, u-µP admits independent (1D) search due to careful HP design. The first part of independent search is an LR sweep, which alone reaches near-optimal loss for u-µP. (b) Using the best proxy HPs from (a), we train many models at different widths and LRs. The best LR for width 256 is ~optimal for 4096, showing LR transfer along with lower loss. (c) We re-train with a simple un-scaled .to(float8) cast on matmul inputs. This would fail for other models, but u-µP trains with minimal degradation.

## Introduction

The challenges of large-model training extend beyond the domain of engineering; they are also algorithmic in nature. Effective approaches for training smaller models are not guaranteed to work at the multi-billion-parameter scale used for today's large language models (LLMs). These difficulties can be framed in terms of stability, which we consider in three forms:

1. feature learning stability, which ensures that parts of the model do not learn too fast or slow relative to each other. 2. hyperparameter stability, which ensures that the optimal HPs for small models remain unchanged as the model size grows. 3. numerical stability, which ensures that floating-point representations during training stay within the range of a given number format.

The Maximal Update Parametrization (µP) [[1,](#b0)[2]](#b1) targets the first two sources of instability. µP defines a set of scaling rules that in principle make a model's optimal HP values consistent across model sizes and ensure 'maximal feature learning' in the infinite-width limit. The practical benefits of this are that models continue to improve as they get larger, and that practitioners can re-use a set of HP values (especially the learning rate) found for a small proxy version of their model, on a larger target model. This is vital for modern LLM training, where the cost of sweeping over candidate HP values for the target model is prohibitive. Consequently, µP has been adopted by several open LLM training efforts [[3,](#b2)[4,](#b3)[5,](#b4)[6]](#b5) and there are indications of its use in state-of-the-art LLMs[foot_0](#foot_0) .

However, there exists a gap between the extensive theory underpinning µP and its effective use in practice. This relates to issues surrounding efficient HP search, HP transfer, interpretability, ease-of-use and low-precision training. Some of these problems have been observed in the literature [[9,](#b7)[10,](#b8)[2]](#b1); others we outline here for the first time. As a result, µP does not necessarily provide the kind of simple, stable scaling for which a user might hope.

To address this, we propose the Unit-Scaled Maximal Update Parametrization (u-µP). u-µP combines µP with another closely-related training innovation, Unit Scaling [[11]](#b9). µP ideally provides consistent training dynamics across model sizes, but says little about what those dynamics should be. Unit Scaling addresses this by proposing an ideal principle for dynamics: unit variance for all activations, weights and gradients. Unit Scaling was initially designed to ensure stable numerics, but in the context of µP the principle of unit-scale brings many additional benefits. We show that it provides a foundation upon which the broader range of drawbacks identified for µP can be addressed.

## Contributions

We focus on LLMs in this work as this is the domain where µP has primarily been used in the literature (though u-µP's principles should extend to other architectures). We contribute the following:

1. Drawbacks of standard µP: We show that the way µP is typically applied has several limitations, and does not give effective transfer for Llama-style models (Section 3). 2. Simpler scaling rules: u-µP is easier to implement in practice than µP, and removes the unnecessary 'base shape' and initialization-scale HPs (Section 4.1; Table [2](#)). 3. Out-of-the-box FP8 training: u-µP models generate tensors that lie close to the center of a floating point format's range, meaning that most matrix multiplications can be performed in FP8 via a simple .to(float8) cast without dynamic rescaling. 4. A principled, interpretable & independent set of HPs: The set of transferable HPs used in the µP literature is chosen in an inconsistent and arbitrary way. We provide concrete recommendations for a good set of transferable HPs to use with u-µP (Section 4.3). 5. Improved HP transfer: We identify a problem with the scaling of the embedding layer's LR under µP. Fixing this for u-µP gives us better scaling with width (Section 4.4). [6](#b5). A more efficient approach to HP search: We show that u-µP facilitates a cheaper independent search method, attaining near-optimal loss when only sweeping the LR (Section 4.5).

We provide a guide for using u-µP in Appendix C, and a library [[12]](#b10) implementing u-µP functions, layers and optimizers, outlined in Appendix D.

2 Background

## The Maximal Update Parametrization

Tensor Programs V [[2]](#b1) defines a parametrization as 'a rule for how to change hyperparameters when the widths of a neural network change'. They show that µP is the only parametrization that gives 'maximal feature learning' in the limit, whereas standard parametrization (SP) has imbalanced learning (parts of the network blow up or cease to learn).

One consequence of this improved stability is that learning dynamics under µP are ideally independent of model-size, as are optimal HPs. This facilitates a method known as µTransfer, which describes the process of training many smaller proxy models to evaluate candidate HP values, then using the best-performing ones to train a larger target model. An HP is said to be µTransferable if its optimal value is the same across model-sizes.

ABC-parametrizations µP, SP, and the Neural Tangent Kernel (NTK) [[13]](#b11) are all instances of abc-parametrizations. This assumes a model under training where weights are defined as:

$w 0 ∼ N (0, B 2 W ),(1)$$W t = A W • w t , w t+1 = w t + C W • Φ t (∇L 0 , ..., ∇L t ),$with t a time-step and Φ t (∇L 0 , ..., ∇L t ) the weight update based on previous loss gradients.

A parametrization scheme such as µP is then defined specifying how scalars A W , B W , C W change with model width. This can be expressed in terms of width-dependent factors a W , b W , c W , such that

$A W ∝ a W , B W ∝ b W , C W ∝ c W .$The values these factors take are what characterize a particular scheme. For µP these are given in Table [1](#). For depth a similar result has been proved using depth-µP [[14]](#b12), albeit in a restricted setting. When we refer to µP in the paper we assume the depth-µP scaling rules (Table [2](#), 'Residual' column).

A key property of the abc-parametrization is that one can shift scales between A W , B W , C W in a way that preserves learning dynamics (i.e. the activations computed during training are unchanged). We term this abc-symmetry. For a fixed θ > 0, the behavior of a network trained with Adam is invariant to changes of the kind:

$A W ← A W • θ, B W ← B W /θ, C W ← C W /θ(2)$(reproduced from Tensor Programs V, Section J.2.1). This means that parametrizations like µP can be presented in different but equivalent ways. ABC-symmetry is a key component in developing u-µP.

Transferable HPs µP focuses on the subset of HPs whose optimal values we expect to transfer across axes such as width and depth. We term these µTransferable HPs. All µTransferable HPs function as multipliers and can be split into three kinds, which contribute to the three (non-HP) multipliers given by the abc-parametrization: α W , σ W , η W where

$A W ∝ α W , B W ∝ σ W , C W ∝$Table [1](#): The scaling rules defining µP. The type of a weight is determined by whether fan-in & fan-out both depend on width (hidden), only fan-out does (input), or only fan-in (output). Hence fan-in is always a multiple of width here.

## ABC-multiplier Weight (W ) Type Input

Hidden Output µP parameter (a W ) η W . The difference between these multipliers and the ones that define a parametrization is that they are specified by the user, rather than being a function of width. α W and η W are rarely introduced outside of the µP literature, but can be valuable to tune for both µP and SP models. In the µP literature the term 'HPs' often implicitly refers to µTransferable HPs. We adopt this convention here, unless specified otherwise.

Base shape Two additional (non-µTransferable) HPs introduced by µP are the base-width and base-depth. This refers to a mechanism where a user specifies a particular shape for the model, where its behavior under µP and SP are the same. The µP model still scales according to the abc-rules, so for all other shapes the two models will be different. This is implemented by dividing the µP scaling rules for the given model by those of a fixed-shape model at the base-width and base-depth.

Putting this together with our abc-parametrization given in Equation ( [1](#formula_0)), and the µTransferable HPs outlined above, we now derive our final, absolute expressions for A W , B W , C W :

$A W ← α W a W a Wbase , B W ← σ W b W b Wbase , C W ← η W c W c Wbase(3)$Though base shapes are necessary for µP, they are not typically swept. Rather, they are considered a preference of the user, who may wish to retain the behavior of an existing SP model at a given shape.

Choosing HPs to sweep In theory, the search space of µTransferable HPs includes α W , σ W , η W for every parameter tensor W in the model. In practice far fewer HPs are swept, with global grouping often used for σ W and η W , and many α W s dropped or grouped across layers.

The sets of HPs chosen for sweeps in the µP literature is explored in Appendix E.1. Tensor Programs V uses a random search to identify the best HP values, which has become the standard approach to sweeping. The number of runs in a sweep is typically in the low 100s, incurring a non-negligible cost (though usually less than a single training run of the target model). This high number partly owes to dependencies between HPs (shown in Section 5.2), making the search space hard to explore.

## Low-precision training

All the major potential bottlenecks of model training-compute, communication and storage-see roughly linear improvements as the bit-width of their number format is reduced. In modern LLM training, the compute cost of large matrix multiplications (matmuls) means that substantial gains are available if these can be done in low-precision (< 32 bit) formats. With the ending of Dennard scaling and Moore's law [[15,](#b13)[16]](#b14), the use of low-precision formats represents one of the most promising avenues towards increased efficiency in deep learning.

Recent AI hardware offers substantial acceleration for the 8-bit FP8 E4 and E5 formats. However the reduced range of these formats means that they cannot directly represent some values generated during training. Various methods have been introduced to address this, such as the per-tensor dynamic re-scaling in Transformer Engine [[17]](#b15). However, this comes at the cost of added complexity and potential overheads. For a more in-depth treatment of low-precision formats, see Appendix J.

## Unit Scaling

An alternative approach to low-precision training is Unit Scaling [[11]](#b9), which also uses fine-grained scaling factors to control range, but instead finds these factors via an analysis of expected tensor statistics at initialization. These are fixed factors, calculated independently of the contents of a tensor, at the beginning of training. As such, the method is easy to use and only adds the overhead of applying static scaling factors (which we show to be negligible in Appendix K).

These factors are chosen to ensure the unit variance of activations, weights and gradients at initialization. This is a useful criterion as it places values around the center of floating-point formats' absolute range. This applies to all tensors, meaning every operation in the network requires a scaling factor that ensures unit-scaled outputs, assuming unit-scaled inputs. Unit Scaling does not provide a mechanism for re-scaling tensors dynamically during training, but due to its ideal starting scale for gradients, activations and weights this may not be required. Empirically this is shown to be true across multiple architectures, though it is not guaranteed.

6 8 10 12 14 16 Validation Loss (a) Tensor Programs V settings (on GPT, Wikitext-2) 2.8 3.0 3.2 3.4 3.6 3.8 4.0 (b) Standard Llama settings (on Llama, Wikitext-103) 2.8 3.0 3.2 3.4 3.6 3.8 4.0 (c) Standard Llama settings + stability fixes (on Llama, Wikitext-103) 2 -12 2 -10 2 -8 2 -6 Learning Rate 0 1 2 3 4 5 Training Loss 2 -11 2 -9 2 -7 2 -5 2 -3 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 4.0 2 -11 2 -9 2 -7 2 -5 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 4.0 Width 128 256 512 1024 2048 4096 Min We provide an example of deriving the Unit Scaling rule for a matmul op in Appendix E.2, resulting in the scaling factor: 1/ √ d fan-in . We accompany this example with a full recipe for applying Unit Scaling to an arbitrary model. [3](#b2) The challenges with µP in practice 3.1 Not all training setups give µTransfer Lingle [[9]](#b7) shows that directly applying µP to a decoder LM fails to provide LR transfer across width. Given that the primary use of µP in the literature has been LM training of this kind, this result suggests a significant limitation. How do we reconcile this with the strong LR transfer across width shown for language models in Tensor Programs V?

We answer this in Figure [2](#fig_0). The first training setup (a) is aligned with that used in Tensor Programs V (their Figure [4](#fig_2)). There are several atypical aspects to their training setup, primarily the use of a constant LR schedule and a high number of epochs; we outline the precise differences between setup (a) and (b) in Table [6](#tab_13). This overfitting regime makes validation loss unusable, and transfer misleadingly good. When we remove these and shift to a standard Llama training setup (b), optimal HPs begin to drift with width (see Figure [9](#fig_7) for an ablation). This confirms Lingle's findings that standard µP is in fact a poor fit for modern LM training. We fix this (c) by the removal of parameters from LayerNorms/RMSNorms, as suggested by Lingle, and the introduction of independent weight decay for AdamW, as suggested by Wortsman et al. [[18]](#b16)[foot_2](#foot_2) (see [[19]](#b17) for further analysis). With these changes adopted, we recover the strong transfer shown in Tensor Programs V's experiments.

## It's not clear which hyperparameters to sweep

The problem of selecting HPs to sweep can be framed as choosing a subset of the per-tensor α W , σ W , η W HPs outlined in Section 2.1, and grouping across/within layers. As shown in Table [9](#), µTransfer experiments in the literature have done this in a variety ways. Practitioners have not justified these choices, appearing to rely on a mixture of precedent and intuition. We outline two major downsides to the lack of a principled approach.

Firstly, not all groupings of HPs are suitable. Consider the commonly-used global σ init HP. At initialization the activations going into the FFN swish function have std(x swish ) ∝ σ Wgate , whereas the self-attention softmax activations have std(x attn ) ∝ σ WQ σ WK . A global σ HP thus has a linear effect on the FFN and a quadratic effect on attention, suggesting that this grouping may be unideal.

Secondly, not all HPs are independent of one another. The key example of this is the interaction between σ W and η W . The relative size of a weight update is determined by the ratio η W /σ W , not by either HP individually. Because of this, the optimal values for σ and η depend on each other, which we demonstrate empirically in Section 5.2. This can make the problem of HP search much harder, and may be why hundreds of random-search runs have been required for sweeps in the literature.

## Base shape complicates usage

Most practitioners are unlikely to require alignment with an SP model, in which case it is unclear what base-width (and base-depth) should be used. The literature has aligned on a standard base-width of 256 (see Table [9](#)), but this appears to lacking a principled motivation-though the fact that they are not dropped entirely suggests they may be beneficial under u-µP.

Implementing base-shape HPs (see Equation ( [3](#formula_5))) can also add complications from an engineering perspective. The proposed implementation in the mup library [[20]](#b18) reflects this, requiring an extra 'base' model to be created and the original model to be re-initialized. This can interact awkwardly with other model-transforms for features like quantization, compilation, etc:

$import mup proxy_model = MupModel(d_model=128, ...)$# proxy width base_model = MupModel(d_model=256, ...) # base width mup.set_base_shapes(proxy_model, base_model) # re-initialize proxy_model

## µP appears to struggle with low-precision

Finally, we note an interesting contradiction observed in the relationship between µP and lowprecision. One of the stated aims for µP is that its activations have Θ(1)-sized coordinates in the limit [2, Desiderata J.1]. This desideratum is specifically given in order that values can be represented using finite-range floating-point numbers [1, [Section 3]](#). Yet despite numerical stability being central to the theory underlying µP, this is not leveraged to ensure that µP models can actually be trained in low-precision. Indeed, for the LLM runs in Tensor Programs V the SP model trains successfully in FP16, while the µP model diverges (attributed to underflow of gradients). We remedy this with u-µP.

## The Unit-Scaled Maximal Update Parametrization

In this section we show how µP can be adapted to satisfy Unit Scaling, and provide a new set of HPs which-thanks to Unit Scaling-are more interpretable and separable than those commonly used for µP, unlocking several practical benefits. For those wishing to apply u-µP to their own models, we provide a user-guide in Appendix C and an overview of our library implementing u-µP in Appendix D.

## Combining µP with Unit Scaling

Whereas Unit Scaling provides rules for scaling all operations, µP only does so for parametrized ones. It's these operations we need to address to arrive at a unified scheme, resolving differences in the scaling rules each recommends. We begin with the expressions for the A W , B W , C W scaling factors in Equation ( [3](#formula_5)), and substitute in the µP scaling rules defined in Table [1](#). This results in a complete implementation of µP, which is shown in the top half of Table [2](#) (using the extended set of µP HPs given in Table [3](#)). We set out to turn this into a valid Unit Scaling scheme, which requires unit initializations (B W ← 1) and matmuls with the Unit Scaling factor we identified in Section 2.3

$(A W ← 1/ √ fan-in).$Table [2](#): The definition of u-µP along with an implementation of µP (assuming the extended HP set in Table [3](#)). u-µP aims to simplify µP and provide the benefits of Unit Scaling.

## ABC-multiplier

Weight Type Residual Input Hidden Output parameter (A W ) α emb 1 (or α attn ) α out base-fan-in fan-in base-depth depth * µP initialization (B W ) σ init σ init base-fan-in fan-in

$σ init - Adam LR (C W ) η ηemb η base-fan-in fan-in η base-depth depth parameter † (A W ) 1 1 √ fan-in 1 fan-in ‡ 1 √ depth * u-µP initialization (B W ) 1 1 1 - Adam LR (C W ) η 1 √ fan-out η 1 √ fan-in η 1 √ depth * Residual$multipliers are applied to the end of each branch, rather than the output of linear layers. † u-µP's α HPs are associated with operations, not weights, so are not included here (see Section 4.3). ‡ To maintain unit scale we apply 1/ √ fan-out scaling in the backward pass (see Appendix H).

Our first step is to drop the σ W and base-fan-in HPs entirely, and associate the α W HPs with certain functions instead of weights-decisions we justify in the rest of this section (this results in the simplified intermediate implementation in Table [11](#tab_22)). Our input weights now have unit initializations as desired, and a unit parameter multiplier, which is also the appropriate scaling factor (as input layers here are embedding lookups, not matmuls).

Hidden weights now have the implementation:

$A W ← 1, B W ← 1 √ fan-in , C W ← η 1 fan-in ,(4)$which differs from our Unit Scaling criteria. However, using the abc-symmetry outlined in Equation (2) we can shift scales by a factor of √ fan-in, arriving at a unit-scaled scheme:

$A W ← 1 √ fan-in , B W ← 1, C W ← η 1 √ fan-in .(5)$Finally, our output layers also have unit initialization, but a parameter multiplier of A W ← 1/fan-in. This differs from the Unit Scaling rule, but in the forward pass this is permissible as there are no subsequent matmuls of a transformer. In the backward pass this mis-scaling would propagate, so we apply the desired ← 1/ √ fan-in factor. Using different forward and backward scales in this way is usually not allowed, but is valid for output layers due to the cut-edge rule (Appendix H).

The final change we make is to the input LR scaling rule, which we show in Section 4.4 is more effective if c W ← 1 is replaced with c W ← 1/ √ fan-out [3](#foot_3) . With these changes made, we arrive at our final u-µP scheme, given in Table [2](#). It's important to note that the scaling rules in this table must be combined with the standard Unit Scaling rules for other non-matmul operations. These are covered in Appendix B, and implemented in our library (see Appendix D).

## Out-of-the-box low-precision training

By applying the principles of Unit Scaling to µP, u-µP gains a key feature: the easy use of lowprecision number formats during training. We can attribute the difficulties µP has with low precision to the fact that it ignores constant factors (along with weight and gradient-scaling), only ensuring that activations are of order Θ(1). The stricter condition of unit scale across all tensors at initialization provides a way of leveraging µP's rules in order to make low-precision training work.

When training a transformer model with u-µP most scales in the model stabilize while certain tensors exhibit scale growth that potentially pushes them out of FP8 range. We empirically identify these critical tensors to be the inputs to the attention dense projection and final FFN matmul as well as the weight of the decoder head (for details see Appendix A.8). The latter becomes negligible in terms of model flops as width and depth of the model increase, so we generally keep this operation in higher precision.

Following these observations, we propose the following FP8 mixed precision scheme for u-µP transformer models:

• For non-critical matmul operations, we cast the input and weight to E4M3, and the gradient with respect to the output to E5M2. This is done in the forward computation, as well as the two backward computations (for the gradient w.r.t. the weight, respectively the input). Non-critical layers are query, key, value as well as the input layer(s) to the FFN. • All layers involving critical tensors, as well as embedding layer, residual addition and nonlinear functions are performed in higher precision. This also means that we directly aggregate into higher precision in each FP8 matmul. We keep optimizer states in FP32, as is usually the case in mixed precision training.

We note that in some cases one can deal with the critical tensors by casting them to E5M2 instead of E4M3, however we observed some instabilities applying this in a large scale setting, possibly due to loss of precision. In small scale scenarios we also empirically find that applying the E4M3 format instead of E5M2 for the gradients is possible, but becomes problematic in a more realistic setting where gradients require a higher dynamic range.

With our proposed mixed precision scheme, about 70% of the matmul computations in the transformer block are performed natively in FP8 (assuming a standard architecture, e.g. Llama). If desired, a dynamic per-tensor scaling could still be applied to the critical tensors.

## A principled approach to hyperparameters

We saw in Section 3.2 that approaches for selecting which HPs to sweep are poorly motivated in the literature. Our objective in u-µP is to find a simple, well-justified and effective alternative. To this end, we propose the following ideal criteria:

1. Minimal cardinality: the use of as few HPs as possible.

2. Maximal expressivity: the ability to still express any model defined using the per-tensor α W , σ W , η W HPs outlined in Section 2.1 (in practice, we relax this slightly).

3. Minimal interdependency: the optimal value of each HP should not depend on the value of other HPs, simplifying the search space. 4. Interpretability: there should be a clear explanation for what an HP's value 'means' in the context of the model.

Table 3: Typical transformer HPs used under different schemes. Basic HPs in bold are considered most impactful and are commonly swept. Extended HPs in non-bold are not always swept, often set heuristically or dropped. SP µP u-µP η η η σ-scheme σ init α emb |η emb α ffn-act α attn α attn-softmax α out α res base-width α res-attn-ratio base-depth α loss-softmax

The u-µP HPs given in Table [3](#) are designed to satisfy these criteria, to the fullest extent possible. The placement of these HPs in the model is given in Table [8](#).

Cardinality & expressivity We arrive at our set of HPs in three steps, starting with the full α W , σ W , η W for each weight tensor W . Firstly, we can choose to 'drop' any one of these three HPs by permuting under abc-symmetry, such that one HP = 1. As we want our weights to begin with unit scale, we choose σ W (i.e. θ = σ W in Equation ( [2](#formula_3))), leaving just α W , η W .

Secondly, we observe that several of the α W HPs combine linearly with other α W HPs, providing an opportunity to re-parametrize with a single HP. For instance, we noted in Section 3 that the scale of selfattention softmax activations is proportional to the product of σ W multipliers, and the same is true for α W multipliers: std(x attn ) ∝ α WQ α WK . In this instance it appears more natural to use a single α parameter and associate it with the attention operation, rather than the weights. We term this α attn-softmax .

We apply the same principle to the rest of the model, associating α HPs with operations instead of weights. This applies to all operations, unless they are unary and k-homogeneous for k ≥ 0, in which case they propagate scale and don't require an HP (see Appendix G.1). This results in the set of HPs shown, with their placement in the model given in Table [8](#).

Thirdly, we use a single global η and group α HPs across layers. This breaks our expressivity criterion, but we argue represents the best trade-off between expressivity and cardinality. We show in Appendix A.4 that having tuned a global η HP and our extended α HPs, the further benefits of tuning per-tensor ηW HPs (which modify the global η) is minimal, justifying our decision to only use one global η.

Interdependency The second stage above, moving α HPs from weights into subsequent operations, not only reduces the number of HPs, but also minimizes the interdependence between those that remain. Interactions between HPs are complex and unlikely to be entirely separable, but we find that u-µP's optimal HP values depend less on each other than under µP (see Section 5.2).

Interpretability The combination of unit scale and reduced dependencies between HPs means that each α can be interpreted as determining some fundamental property of the model at initialization. For example, the α loss-softmax HP defines the (inverse of) the softmax's temperature for a unit-scaled input. We also introduce a new scaling scheme (defined in Appendix G.2.2) for residual connections, designed to give HPs independence and a clear interpretation: α res defines the contribution of the residual connections to the output scale, and α res-attn-ratio defines the relative contribution of attention versus FFN branches. Finally, we choose not to include base shape HPs in u-µP. They do not add to expressivity, lack a clear interpretation (besides alignment to a base model at a particular shape), break the interpretations of other HPs (as given above), and complicate implementation.

## A new embedding LR rule

Although theoretical transfer properties have been proved for µP, not all its HPs have had µTransfer shown empirically. We do so for the extended µP transformer HPs in Figure [17](#fig_5), where we observe poor transfer across width for the embedding LR multiplier ηemb . The associated scaling rule for the embedding LR is constant in width (c emb = 1), but this poor multiplier transfer suggests the rule is mis-specified. We show in Figure [3](#fig_1) (left) that a more effective rule is

$c emb = 1/ √ fan-out.$This keeps the optimal value of ηemb the same regardless of width. Figure [3](#fig_1) (right) shows that a constant scaling rule leads to diminishing returns as width increases, whereas our new rule continues to work well at scale, attaining the same loss at 2048-width that constant scaling attains at 4096-width. Our adoption of this change is a key factor in the improved performance of u-µP over µP in Figure [1](#).

We offer no theoretical justification for our rule, which we leave to further work.

## Hyperparameter search

As shown in section Section 2.1, the standard approach to HP search for µTransfer is via a random sweep over all HPs simultaneously. Sweeping individual HPs separately is challenging due to the dependencies between them. In contrast, u-µP's HPs are designed to admit such a strategy due to our interdependence criterion. Because of this, we propose a simpler sweeping strategy for u-µP which we term independent search (outlined in detail in Appendix A.6).

Independent search involves a sweep of the LR, followed by a set of one-dimensional sweeps of the other HPs (which can be run in parallel). The best results from the individual sweeps are combined to form the final set of HP values. We also consider an even simpler scheme, which only sweeps the LR, leaving other HP values at 1 (i.e. dropping them). For caution, we recommend the full approach, but in practice we find that only sweeping the LR is surprisingly effective, as shown in Figure [1](#) (a). This indicates that not only is the principle of unit scale good for numerics, but also for learning dynamics where it provides near-optimal scaling.

2 4 2 6 2 8 C emb 2.8 3.0 3.2 3.4 3.6 Validation Loss 2 -12 2 -10 2 -8 2 -6 2 -4 Learning Rate Proxy Optimum Proposed Transfer Rule cemb ← 1/ √ fan-out µP Transfer Rule cemb ← 1 Recall: wt+1 = wt + C emb • Φt(∇L0, . . . , ∇Lt), C emb ← η emb • c emb /c emb-base Improved Optimum Transfer Width 128 (BaseWidth) 256 512 1024 2048 4096 Embedding LR scale 

$c emb ← 1/ √ fan-out c emb ← 1$
## Experiments

## Experimental setup

Our experiments all use the Llama [[21]](#b19) architecture trained on WikiText-103 [[22]](#b20) (excepting the large-scale runs in Section 5.5). We apply current best-practice LLM training techniques from the literature (full settings are given in Table [5](#tab_12)). In accordance with our analysis of settings for µTransfer in Section 3.1, we remove parameters from norm layers, use independent AdamW, and avoid training on too many epochs for both u-µP and µP for the sake of fair comparison.

## Quantifying hyperparameter interdependence

Our principled approach to HPs (Section 4.3) contains the requirement that their optimal values should depend minimally on the value of other HPs. We now investigate this empirically, conducting a 2D sweep over every pair of HPs for µP and u-µP, shown in Figures [14](#fig_2) and [15](#fig_14) respectively.

To derive an empirical measure of HP dependency, we introduce the notion of transfer error (see Algorithm 1). This considers a pair of HPs, with one 'fixed' and the other for 'transfer'. We take the best value of the transfer HP for each non-optimal value of the fixed HP, and use it with the optimal value of the fixed HP. The transfer error is the difference between the losses obtained and the minimum loss. Figure [4](#fig_2) shows this measure for each pair of HPs under µP and u-µP, reflecting the improvement in HP dependency as a result of our scheme. This gives u-µP a reduced risk of small transfer errors leading to large degradations, and the potential to sweep HPs in a more separable way.

## Hyperparameter search

We now leverage this improved separability of HPs for the purpose of efficient sweeping. In Figure [1](#) (a) we conduct a standard random search for µP and u-µP, along with the independent search outlined in Section 4.5 (and Appendix A.6). We observe the following:

1. For u-µP the LR-sweep phase of independent search alone is sufficient to reach near-optimal loss (totaling 9 runs). During this phase other HPs are fixed at 1, which for u-µP means that the inputs to operations are generally unit-scaled.

2. Consequently, we conclude that unit scale at initialization is close to the ideal scaling for effective learning here. This is not a property we asserted a priori, nor do we argue that it necessarily holds for other training setups and models; hence why we still provide a set of extended HPs to be swept. 3. In contrast µP still requires non-LR HPs to be swept to attain a reasonable loss. Unlike u-µP, fixing HPs at 1 results in arbitrarily-scaled inputs, which appear to result in worse training. 4. The 'combined mults' phase causes the loss to spike for µP. This is due to the HP dependencies shown in Figure [4](#fig_2), which mean HPs cannot be swept independently and used together.

Conversely, lower dependence means this can be done for u-µP, making random search unnecessary.

## Hyperparameter transfer

We train many models and plot transfer of LR across width (Figure [1](#) (b)), steps, batch size and depth (Figure [5](#fig_3)), and transfer of other HPs across width (Figure [17](#fig_5)). Note that u-µP (building on µP) is designed to give transfer over width [4](#foot_4) ; the other axes we report for practical purposes. We find that:

1. The optimal LR is constant across width under u-µP. There is a small drift for training steps and batch size, and a larger one with depth. Hence we recommend proxy models which primarily differ in width, moderately in steps and batch size, and least in depth. 2. The optimal LR is also approximately constant for training steps, batch size and depth. This means we can scale our proxy model down across all these axes and maintain LR transfer. Of these, width appears the most stable and depth the least. 3. Whereas µP sees diminishing returns for larger widths, u-µP continues to benefit from width, with the 2048 u-µP model matching the 4096 µP model. We attribute this primarily to our improved embedding LR rule. 4. Non-LR HPs also have approximately constant optima across width under u-µP. This is not true for µP, where ηemb has poor transfer due to the embedding scaling rule issue identified in Section 4.4, along with σ init which in Section 3.2 we argue should not be grouped across all weights (and drop from the u-µP HP scheme). 5. The optimal values found for non-LR HPs are all close to 1. In practice this means that dropping these HPs entirely is potentially viable for similar models and training setups.

## FP8 training

In this section we justify the simple mixed-precision scheme described in Section 4.2 and demonstrate that it can be used to train u-µP models out-of-the-box.

2 -10 2 -8 2 -6 Learning Rate 3.0 3.2 3.4 3.6 3.8 Validation Loss Training Steps 4096 8192 16384 32768 2 -10 2 -8 2 -6 Learning Rate µP Batch Size 32 64 128 256 2 -10 2 -8 2 -6 Learning Rate Depth 1 2 4 8 16 2 -2 2 0 2 2 2 4 Learning Rate 3.0 3.2 3.4 3.6 3.8 Validation Loss Training Steps 4096 8192 16384 32768 2 -2 2 0 2 2 2 4 Learning Rate u-µP Batch Size 32 64 128 256 2 -2 2 0 2 2 2 4 Learning Rate Depth 1 2 4 8 16 activation E4M3 E5M2 E4M3 weight grad 2 -24 2 -16 2 -8 2 0 2 8 RMS µP u-µP activation E4M3 E5M2 E4M3 weight grad 2 -24 2 -16 2 -8 2 0 2 8 RMS µP u-µP Proof-of-concept Figure [6](#fig_4) shows the RMS of all linear layer inputs for a moderately sized transformer. RMS captures the larger of the mean and scale of a distribution, and as such is a good test of whether a tensor is likely to suffer over/underflow in low-precision. We observe that u-µP tensors largely have RMS starting close to 1 and remaining so at the end of training, supporting our scheme.

Figure [19](#fig_16) demonstrates the scale-growth of critical tensors which our scheme is designed to accommodate, showing RMS on a per-tensor basis over steps. Figure [20](#fig_17) provides further insight into this issue, showing the effect of LR, width, depth, steps and batch size on the RMS of critical tensors.

As an initial proof-of-concept we train a u-µP model using our FP8 scheme over 8k steps, using HPs from a proxy model, as shown in Figure [1](#) (c). We see only a small degradation versus FP32, and at this scale critical tensors can still be cast to FP8 using E5M2, while gradients can even use E4M3.

0K 20K 40K 60K Training Step 1.9 2.0 2.1 2.2 2.3 2.4 Training Loss 1B 3B 7B BF16 FP8 0K 20K 40K 60K Training Step 1.9 2.0 2.1 2.2 2.3 2.4 Training Loss 1B 3B 7B u-µP SP Table 4: 0-shot benchmark results at 7B scale. Scheme Format MMLU HellaSwag OpenBook QA PIQA TriviaQA WinoGr SP BF16 29.6 52.4 27.8 76.5 22.2 63.3 u-µP BF16 29.0 53.4 31.6 77.1 23.4 63.7 u-µP FP8 31.2 53.4 29.6 77.6 21.3 65.7

Larger scale Next we consider a more realistic training scenario [5](#foot_5) . Using the same architecture, and following the steps set out in our u-µP user-guide (Appendix C), we train our target models on 300B tokens of the SlimPajama dataset [[23]](#b21) (see Appendix A.9 for training details).

We begin with an independent search (Section 4.5) over our u-µP proxy model's HPs. Here we make the following observations:

1. When using a relatively small proxy model (8 layers and 512 width), the HP-loss landscape is rather noisy. By doubling the width we can discern optimal HP values more clearly. 2. The most important HPs are η and α res-attn-ratio . All others can be left at the default of 1.

3. The optimal values of these HPs are η = 2 3.5 and α res-attn-ratio = 2 -2.0 and thus differ non-trivially from the observed HPs in our smaller-scale experiments.

We then train u-µP models of approximately 1B, 3B and 7B parameters, using our FP8 mixedprecision scheme (see Section 4.2). We also train two baselines at each size: the first is a BF16 version of our u-µP models, and the second is a set of SP models using the weight init scheme from the Pythia model family [[24]](#b22) and the LR scheme from Llama 3 [[25]](#b23), scaling inversely with width and using a LR of 3e-4 at 7B scale. The loss curves are shown in Figure [7](#fig_5). All FP8 runs converge and show no significant loss degradation. In comparison to SP, the u-µP models have a qualitatively different training curve with a higher loss for most of training that catches up in latter stages, hinting at a fundamentally different optimization trajectory. In terms of downstream performance, both of the u-µP 7B models are competitive with SP. In particular, the scores of the FP8 model are mostly on par with the BF16 models (see Table [4](#)).

## Related Work

Low-precision training Techniques introduced to facilitate FP8 training include those covered in Appendix J and more [[26,](#b24)[27,](#b25)[28]](#b26). These largely concern the quantizing of activations, weights and gradients, though [[29]](#b27) also explore FP8 optimizer states and cross-device communication, which we consider interesting avenues of further exploration. Recently, stable training has been demonstrated for the MX family of formats which use a shared block-exponent [[30,](#b28)[31]](#b29), and even for the ternary BitNet format [[32,](#b30)[33,](#b31)[34]](#b32). Again, we consider these formats for follow-up work.

Stability features Another recent research trend has been the analysis of features that contribute to (or resolve) numerical and algorithmic instability. [[18]](#b16) show that unstable training dynamics can result from attention logit growth (fixed by QK-norm [[35]](#b33)) and from divergent output logits (fixed by z-loss [[36]](#b34)). [[37]](#b35) find large feature magnitudes can be avoided by zero-initialization, and loss spikes avoided via a modified AdamW, specifically for low-precision training. [[38]](#b36) investigate how pre-training settings affect instabilities revealed during post-training quantization. [[39]](#b37) apply a similar philosophy to Unit Scaling for the training of diffusion models, to address uncontrolled magnitude changes. Extreme activation values seen in large models [[40,](#b38)[41]](#b39) have been addressed by softmax-clipping [[42]](#b40), and by the addition of extra terms [[43]](#b41) or tokens [[44]](#b42) to bias the attention computation. We do not adopt these features in our experiments to avoid confounding effects, but we expect them to benefit u-µP and hope to explore their usage.

Learning dynamics Several recent efforts have tried to improve µP from different angles. [[45]](#b43) introduces the notion of the modular norm over the full weight-space, which like µP aims to ensure stable updates that provide LR transfer, and like u-µP is implemented via modules designed to ensure stable training. Challenging the assumptions underpinning µP, [[46]](#b44) explores the notion of alignment between parameters and data, demonstrating that other parametrizations with per-layer learning rates can outperform standard µP. We consider comparing these parametrizations against u-µP and trying unit-scaled versions valuable future work. Recent applications of µP to the problems of weight sparsity [[47]](#b45) and structured matrices [[48]](#b46) are also interesting candidates for u-µP.

## Conclusions

We introduce u-µP, a modified and improved version of µP that satisfies Unit Scaling. Through careful analysis guided by first principles we identify an interpretable set of HPs that has minimal interdependencies and facilitates an efficient independent sweeping strategy. We show that the stability properties of µP combined with Unit Scaling enable a simple and robust FP8 mixed precision scheme that works in a realistic large scale training scenario. u-µP provides further evidence that the principle of Unit Scaling is beneficial for model design.

Limitations and future work Some choices like the modified embedding LR rule are only justified by empirical observations, and currently lack a theoretical explanation. Additionally, neither µP nor Unit Scaling give guarantees for network quantities to be well-behaved over the course of training.

In particular we would like to understand the issue (or feature) of scale growth in the critical layers better and look into possible mitigations. We also believe that low-precision training techniques can be pushed further, with u-µP offering an ideal starting point for future optimizations.

Contents 1 Introduction 1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Background 2.1 The Maximal Update Parametrization . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Low-precision training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Unit Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 The challenges with µP in practice 3.1 Not all training setups give µTransfer . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 It's not clear which hyperparameters to sweep . . . . . . . . . . . . . . . . . . . . 3.3 Base shape complicates usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 µP appears to struggle with low-precision . . . . . . . . . . . . . . . . . . . . . . 4 The Unit-Scaled Maximal Update Parametrization 4.1 Combining µP with Unit Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Out-of-the-box low-precision training . . . . . . . . . . . . . . . . . . . . . . . . 4.3 A principled approach to hyperparameters . . . . . . . . . . . . . . . . . . . . . . 4.4 A new embedding LR rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Experiments 5.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Quantifying hyperparameter interdependence . . . . . . . . . . . . . . . . . . . . 5.3 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Hyperparameter transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 FP8 training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Related Work 7 Conclusions 8 Acknowledgments A Additional experimental details A.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Further analysis of µTransfer failure modes . . . . . . . . . . . . . . . . . . . . . A.3 Validating our experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1 Repeated data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.2 Warmup duration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3 Learning rate decay target . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Per-tensor learning rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Hyperparameter independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Hyperparameter transfer experiments . . . . . . . . . . . . . . . . . . . . . . . . . A.8 Numerical properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.9 Large-scale training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B Unit-scaled op definitions C A guide to using u-µP D A guide to the unit scaling library D.1 Standard usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Extending the library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 As a reference implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . E Additional background material E.1 The Maximal Update Parametrization . . . . . . . . . . . . . . . . . . . . . . . . E.2 Unit Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F Unit-scaled pre-norm residual layers F.1 Scale growth in pre-norm residual networks . . . . . . . . . . . . . . . . . . . . . F.2 Residual symmetry in pre-norm architectures . . . . . . . . . . . . . . . . . . . . F.3 Proof of Lemma F.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Unit Scaling for transformer residuals . . . . . . . . . . . . . . . . . . . . . . . . G Justifying the u-µP hyperparameter scheme G.1 Multipliers for non-homogeneous ops: α attn-softmax , α ffn-act , α loss-softmax . . . . G.2 Residual branch multipliers: α res , α res-attn-ratio . . . . . . . . . . . . . . . . . . . G.2.1 Improved hyperparameters for transformer residuals . . . . . . . . . . . . G.2.2 The full u-µP residual scheme . . . . . . . . . . . . . . . . . . . . . . . . H The cut-edge rule I From µP to u-µP J Low-precision and its trade-offs K Benchmarking scaled matrix multiplication implementation in PyTorch L Attention output RMS grows with model depth A Additional experimental details A.1 Experimental Setup

Our experimental analysis of u-µP was conducted by adapting the codebase used for Tensor Programs V, allowing us to compare µP and u-µP in the same setting. We change various experimental settings from the original paper to make our experiments better reflect standard training procedures, particularly the dataset which we switch from WikiText-2 to the larger WikiText-103 [[22]](#b20). Where not specified otherwise, the default setting used in our experiments are given in Table [5](#tab_12). These also represent the settings of our proxy model.

Dataset WikiText-103 [22] Sequence length 256 Vocab size 32000 Training set tokens 138M Architecture Llama [21] (Transformer, PreNorm, RMSNorm, SwiGLU, RoPE, "untied" embeddings), non-trainable RMSNorm parameters. Width 256 (scaled up to 4096) Depth 4 Number of heads 4 (scaled up to 64) Head dimension 64 Total parameters 19.5M (scaled up to 1.07B) Batch size 64 Training steps 8192 (0.97 epochs) LR schedule Cosine to 10%, 2000 steps warm-up

Optimizer AdamW (β 1 , β 2 , ϵ) = (0.9, 0.999, 10 -8 ) Weight decay 2 -13 , independent [[49]](#b47) Dropout 0.0

$µP HP search range η ∈ [2 -10 , 2 -6 ] ηemb ∈ [2 0 , 2 8 ] σ init , α emb , α attn , α output ∈ [2 -2 , 2 2 ] u-µP HP search range η ∈ [2 -1 , 2 3 ] α attn ∈ [2 -2 , 2 2 ] α residual , α residual-attn-ratio , α ffn-act , α output ∈ [2 -3 , 2 3 ]$µP HP defaults σ init = α emb = α attn = α output = ηemb = 1 u-µP HP defaults α residual = α residual-attn-ratio = α ffn-act = α output = α attn = 1  

## A.2 Further analysis of µTransfer failure modes

In Table [6](#tab_13) we provide exact details of the experimental differences between setups (a) and (b) from Figure [2](#fig_0), for readers wishing to understand and reproduce this result. We also provide a step-by-step ablation of the various changes made between these setups in Figure [9](#fig_7).

For setup (c), which shows how our two combined stability fixes mitigate the problem of poor transfer, both changes are evaluated independently in Figure [8](#fig_6), which shows that the dominant effect is a narrowing of the learning basin due to non-parametric RMSNorms, leading to better learning rate transfer. 

## A.3 Validating our experimental setup

In this section we run a series of ablations to validate decisions relating to our experimental setup given above. In particular, we examine the effect of using repeated data, the effect of using a shorter warmup duration, and the effect of different final learning rates at the end of decay. 

## A.3.1 Repeated data

As outlined in Table [5](#tab_12), our standard training setup uses 0.97 epochs of the WikiText-103 dataset (50x larger than the WikiText-2 dataset used in Tensor Programs V). However on our batch size and training steps scaling experiments in Figure [5](#fig_3) we train on up to 4× the amount of data than in our standard setup, and hence use up to 4 epochs.

Though this is still a small level of repeated data, this moves our training slightly into the over-fitting regime. Based on this change, we here investigate the hypothesis that this regime has better or worse transfer of the optimal LR than the non-overfitting regime, and hence our results could be misleading.

To do so, we repeated these experiments with the same number of tokens, but using the much larger SlimPajama dataset [[23]](#b21) where we use < 1 epoch.

The results for this experiment are seen in Figure [10](#fig_8). The shape of curves is very similar across the two datasets, for both batch size and training steps (albeit with a higher loss, due to the more varied nature of SlimPajama). From this we conclude that the effect of repeated data from our use of WikiText-103 is not significant.

## A.3.2 Warmup duration

For our experimental setup (Table [5](#tab_12)) we use a longer duration of warmup than in our large-scale setup (Table [7](#)). We do so out of caution, as we use fewer tokens-per-batch for the smaller-scale experiments and so may require longer warmup. However, doing so also creates the risk of spending too large a proportion of training doing warmup, which could affect transfer.

To investigate this effect, we run two experiments. Firstly, we re-run the experiment for LR transfer over training steps, shown in Figure [5](#fig_3), on a quarter of the warmup steps. This is shown in Figure [11](#) (left). The main effect appears to be higher loss for larger learning rates, but the optima are largely unchanged. The only exception is the 4096-step run, where the optimum shifts left and the loss improves slightly. This appears to now align the optimum better with the other training durations, but leads to narrower basins as a result, suggesting a trade-off for this particular experiment.

However, all our other experimental runs use the 8192-step configuration, which has a consistent optimum regardless of warmup duration here. To investigate the effect of reduced warmup on width transfer at this particular step-count, we re-run our experiment in Figure [1](#) (b) under the shorter warmup duration, shown in Figure [11](#) (right). The only significant impact of this change is to narrow the basins, inducing no significant change in the optimal LR. As such, we conclude that using 2000

2 -1 2 1 2 3 2 5 Learning Rate 3.00 3.25 3.50 3.75 4.00 4.25 4.50 Validation Loss wikitext-103 2 -1 2 1 2 3 2 5 Learning Rate 3.75 4.00 4.25 4.50 4.75 5.00 SlimPajama Batch Size 32 64 128 256 2 -1 2 1 2 3 2 5 Learning Rate 3.00 3.25 3.50 3.75 4.00 4.25 4.50 Validation Loss wikitext-103 2 -1 2 1 2 3 2 5 Learning Rate 3.75 4.00 4.25 4.50 4.75 5.00 SlimPajama Training Steps 4096 8192 16384 32768  [5](#fig_3), but using the larger SlimPajama dataset where no data is repeated. In both settings our validation loss basins take the same shape, indicating that our analysis using the WikiText-103 dataset holds.

steps of warmup in our experimental setup is a reasonable choice, and both give the same width transfer.

$2 -2 2 -1 2 0 2 1 2 2 2 3 2 4 2 5$Learning Rate

3.0 3.2 3.4 3.6 3.8 Validation Loss Training Steps 4096 8192 16384 32768 Warmup Steps 2000 500 2 -3 2 -2 2 -1 2 0 2 1 2 2 2 3 2 4 2 5 Learning Rate 2.8 3.0 3.2 3.4 3.6 Validation Loss Width 128 256 512 1024 2048 4096 Warmup Steps 2000 500 Figure 11: (Left) Learning rate transfer across training steps under different numbers of warmup steps. (Right) Learning rate transfer across width under different numbers of warmup steps. In this setting (training steps = 8192) the optimal LR is consistent, meaning either warmup regime can be used, though the longer gives wider basins.

## A.3.3 Learning rate decay target

In all our experiments we use a cosine decay of our learning rate down to 10% of the maximum. This follows the standard approach taken by most LLM training projects [[50,](#b48)[24,](#b22)[51,](#b49)[10,](#b8)[52]](#b50). However, recent research has indicated that this may not be the optimal decay target, with implications for LR transfer. [[53]](#b51) show that the choice of target percentage can alter the shape of transfer curves and potentially shift the optimum value (Figure [21](#fig_19), right). They also suggest that using a fixed target value may work better than a percentage (Figure [22](#fig_20), right), which could be swept separately. [[54]](#b52) separately suggest that linear decay to zero is the most effective scheme.

Though using the optimal decay scheme is not necessarily essential to the validity of our method, any implications of different schemes on transfer properties should be investigated. To do so, we run two experiments. The first sweeps the learning rate for our standard model at various percentages and fixed values of cosine decay target, including zero, in Figure [12](#fig_11) (left). Lower decay targets perform better here, including zero, suggesting that this simple rule may be ideal.

We then re-run our width transfer experiment from Figure [1](#) (b) but with our LR decaying to 0, and plot the result in Figure [12](#fig_11) (right). This leads to slightly better results for large learning rates, though for large models this difference diminishes. Fortunately the effect this decay target has on the shape of curves (and hence optimal LR transfer) is minimal, indicating that our conclusions are not effected significantly by the choice of decay target.

$2 -2 2 -1 2 0 2 1 2 2 2 3 2 4 2 5$Learning Rate

3.0 3.2 3.4 3.6 3.8 Validation Loss Min LR 0 10 -7 10 -4 0.1% 1% 10% 100% 2 -3 2 -2 2 -1 2 0 2 1 2 2 2 3 2 4 2 5 Learning Rate 2.8 3.0 3.2 3.4 3.6 Validation Loss Width 128 256 512 1024 2048 4096 Final LR ratio 0.1 0.0 Figure 12: (Left) A learning rate sweep over LR targets of different types (percentage, fixed and zero) on our standard model. (Right) Using the zero and 10% learning rate targets, LR transfer over width.

## A.4 Per-tensor learning rates

In Section 4.3 we relax the requirement for each weight tensor in the u-µP model to have an associated tuneable learning-rate multiplier on top of the global learning rate. Whilst this does reduce the theoretical expressivity of the u-µP scheme, Figure [13](#fig_1) shows that using a single globally optimized learning rate is already at or close to the optimal choice for all weight tensors, and therefore it is reasonable to drop these multipliers in favor of reducing the number of HPs. However, a practitioner attempting to absolutely maximize the task performance of their model could experiment with tuning a few key per-tensor LRs, in particular the embedding table. 

## Validation Loss

Self Attention Out FFN Linear 1 FFN Linear 2

$2 -4 2 -2 2 0 2 2 2 4$3.20

3.25

## 3.30

FFN SwiGLU

$2 -4 2 -2 2 0 2 2 2 4$Per-Tensor LR Multipliers η W Embedding

$2 -4 2 -2 2 0 2 2 2 4$Unembedding Figure [13](#fig_1): Independently varying per-tensor learning rate multipliers η W , using the u-µP model of width 256 from Figure [1](#) with optimized global learning rate 2 1.5 as the starting point. Where applicable, the same multiplier is used for tensors of the same name across transformer layers. Each subplot fixes all but one multiplier at 1, therefore the midpoint of each subplot is precisely the u-µP 256 model from Figure [1](#).

## A.5 Hyperparameter independence

In Section 5.2 we explore the question of HP independence under µP and u-µP. The following plots in Figures [14](#fig_2) and [15](#fig_14) show the result of a 2D sweep over every pair of HPs under each scheme. All other HPs are held at 1 when not swept, except the η which is held at 2 -7.5 for µP and 2 1.5 for u-µP, and ηemb which is held at 2 4 for µP.

These results show visual dependence between µP hyperparameters as a diagonal structure in the grids, such as (η emb , σ init ) and (η, α attn ). We quantify this in the plot in Figure [4](#fig_2), where we use a measure of HP dependence termed transfer error. This is explained verbally in Section 5.2, and we provide an algorithmic description in Algorithm 1. We note that differences in transfer error between the two methods may also be influenced by the flatness of the optimum. The HP and loss values used for our transfer error calculations are those in Figures 14 and 15 2 -5.5

$2 -3.5 η 2 0 2 2 2 4 2 6 2 8 ηemb 2 -4 2 -2 2 0 2 2 2 4 σinit 2 -4 2 -2 2 0 2 2 2 4 αemb 2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4 αoutput -12$-9.5 -7.5  

$η 2 -4 2 -2 2 0 2 2 2 4 αoutput 2 0 2 2 2 4 2 6 2 8 ηemb 2 -4 2 -2 2 0 2 2 2 4 σinit 2 -4 2 -2 2 0 2 2 2 4 αemb 2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4 αoutput -4$$err ← 0 f * , t * ← argmin(L) for f in F do if f ̸ = f * then t ← argmin(L(f )) err += L(f * , t) -L(f * , t * ) end if end for return err/(n -1) 2 -2.5 2 -0.5 2 1.5 2 3.5 2 5.5 η 2 -2.5 2 -0.5 2 1.5 2 3.5 2 5.5 η 2 -4 2 -2 2 0 2 2 2 4 αres 2 -4 2 -2 2 0 2 2 2 4$αres-attn-ratio

$2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4$αffn-act

$2 -4 2 -2 2 0 2 2 2 4$αoutput -2.5 -0.5

1.5

3.5

$5.5 η 2 -4 2 -2 2 0 2 2 2 4 αres -4 -2 0 2 4 αres 2 -4 2 -2 2 0 2 2 2 4$αres-attn-ratio αffn-act

$2 -2.5 2 -0.5 2 1.5 2 3.5 2 5.5 η 2 -4 2 -2 2 0 2 2 2 4 αoutput 2 -4 2 -2 2 0 2 2 2 4 αres 2 -4 2 -2 2 0 2 2 2 4$αres-attn-ratio

$2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4$αffn-act 

$2 -4 2 -2 2 0 2 2 2 4 αoutput -4 -20$
## A.6 Hyperparameter search

Here we outline the particular search processes used for our µP and u-µP HP sweeps in Figure [1 (a)](#). The random search samples uniformly from a grid defined over all extended HPs (extended HP sets are defined in Table [3](#), with grid values defined in Table [5](#tab_12)). We perform the random search over 339 runs, each of which is a full training of the width-256 proxy model. We then simulate the effect of shorter searches at various run-counts by taking a random sample of the results, resulting in the smooth curve over run-count shown.

The independent search consists of the following phases:

1. Perform a 1D line search for an optimal learning rate, with other hyperparameters set to their default values (9 runs). 2. For each hyperparameter in parallel, perform a 1D line search (330 runs). 3. Combine the best settings from step 2, and re-evaluate (6 runs).

The number of runs in the 1D line search is an order of magnitude higher than is required in practice. We do so to form a fair comparison with the random search, which benefits from this large number of runs. The number of runs for the 1D line search could be reduced further by using binary search, though this would require sequential runs and limit the extent of parallelism.

## A.7 Hyperparameter transfer experiments LR transfer over width

The transfer experiments shown in Figure [1](#) (b) use the non-LR HPs found in Figure [1](#) (a) (indicated by the circled points), rather than using default HP values. For the u-µP sweep we take the HPs at the end of the LR portion of the independent search, as these are already close-to-optimal, and means only 9 runs were required in the sweep. In contrast, for µP it is necessary to use the results of the random search over a large number of runs.

LR transfer over other axes For the training steps, batch size and depth transfer experiments in Figure [5](#fig_3), all HP values are fixed to 1 except LR which is swept. As with width transfer, u-µP outperforms µP here using these default HP values. Reducing training steps is done by fixing the number of warm-up steps (at 2000) and still cosine-decaying the learning rate to 10%; all that changes is the number of post-warm-up steps. We found this to be more effective than cutting-short the decay schedule. For both Figure [1](#) (b) and Figure [5](#fig_3) we sweep the LR over a logarithmically-spaced grid of step 2 1/2 ×, with 3 runs for each point.

Additionally, in Figure [16](#fig_4) we show learning rate transfer over sequence length for both µP and u-µP fixing either tokens per batch or sequences per batch. In both scenarios u-µP shows not only better absolute training performance, but also better transfer behaviour as sequence length increases. Since our default proxy sequence length is 256, using µP to transfer to sequence length 2048 would result in minimal improvements or even a degradation in validation loss, whereas the u-µP shows much greater and more consistent improvements.

Other HP transfer over width For our non-LR HP transfer results in Figure [17](#fig_5), we note that good transfer under µP has not been demonstrated for all HPs used in the literature. This is particularly true for the ηemb HP, which has poor transfer under µP. Our investigation here led to our identification of the need to adjust the embedding LR scaling rule outlined in Section 4.4. In many cases users have not swept this HP, but instead swept the corresponding parameter multiplier α emb . How this HP interacts with the embedding LR scaling problem identified (and our proposed fix) remains to be explored, though we note in Figure [17](#fig_5) that it also appears to have poor transfer.

Combined HP transfer Whilst Figure [17](#fig_5) demonstrates the transfer of individual hyperparameters over width, Figure [18](#fig_6) instead demonstrates the simultaneous transfer of all hyperparameters when co-optimized on the small-scale proxy model, as is done for µTransfer. The µP and u-µP points are taken from Figure [1](#), with hyperparameters swept on a model of width 256 using a full random HP search and a simple learning rate sweep for µP and u-µP respectively. The Standard Parametrization scheme, as shown in Table [3](#) requires choosing a learning rate and a weight-initialization scheme. We follow the initialization scheme of Pythia [[24]](#b22), and transfer learning rate using a heuristic scaling factor of base-width /width, as is done in [[25]](#b23). 2 -8 2 -6 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 Validation Loss 2 -2 2 0 2 2 2 4 Learning Rate Sequence Length 64 128 256 512 1024 2048 Fix tokens/batch = 16K Fix sequences/batch = 64 Min Figure 16: Transfer of learning rate over sequence length for µP (left) and u-µP (right). As sequence length varies, we can fix the number of tokens per batch by inversely varying the number of sequences per batch (top). Alternatively we can fix the sequences per batch and allow the number of tokens per batch to vary with sequence length (bottom). In the latter case, larger sequence lengths mean the model sees more tokens during training, though as per Table 5 this translates to >1 epoch on WikiText-103 when sequence length goes above 256. 2 -12 2 -10 2 -8 2 -6 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 Training Loss 2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4 αoutput 2 -4 2 -2 2 0 2 2 2 4 σinit 2.8 3.0 3.2 3.4 3.6 3.8 Training Loss 2 -4 2 -2 2 0 2 2 2 4 αemb 2 3 2 5 2 7 2 9 ηemb µP Width 128 256 512 1024 2048 4096 2 -3 2 -1 2 1 2 3 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 Training Loss 2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4 αoutput 2 -4 2 -2 2 0 2 2 2 4 αresidual 2.8 3.0 3.2 3.4 3.6 3.8 Training Loss 2 -4 2 -2 2 0 2 2 2 4 αresidual-attn-ratio 2 -4 2 -2 2 0 2 2 2 4 αffn-act u-µP Width 128 256 512 1024 2048 4096 Figure 17: Transfer of model hyperparameters over width for µP (top) and u-µP (bottom). When one hyperparameter is being swept, all others are fixed at 1, with the exception of Learning Rate η = (2 1.5 , 2 -7.5 ) for (u-µP, µP). 2 8 2 9 2 10 2 11 2 12 Width 2.7 2.8 2.9 3.0 3.1 3.2 Validation Loss SP µP u-µP

Figure [18](#fig_6): Transferring hyperparameters from width 256 up to 4096 using three different hyperparametrization schemes. µP and u-µP results are as seen in Figure [1](#), whilst Standard Parametrization follows the initialization approach of Pythia [[24]](#b22).

## A.8 Numerical properties

Our analysis of the numerical properties of u-µP focuses on the RMS of tensors that we wish to cast to FP8: linear module input activations, weights and output gradients. From the RMS training statistics plots in Figure [6](#fig_4) and Figure [19](#fig_16) we note that

1. µP has gradients and weights with low RMS, at risk of FP8 underflow, whereas u-µP starts with RMS ≈ 1. 2. Many input activations do not grow RMS during training (due to a preceding non-trainable RMSNorm), however the attention out projection and FFN down projection have unconstrained input activations that grow considerably during training. 3. The decoder weight grows during training. Since it is preceded by a RMSNorm, the model may require scale growth in order to increase the scale of softmax inputs. Other weights grow slightly during training. 4. Gradients grow quickly but stabilize, except for attention out projection and FFN down projection, whose gradients shrink as the inputs grow. We also evaluate how RMS growth is affected by model and training hyperparameters in the tensors that showed the highest end-training RMS, shown in Figure 20. This shows that the main parameter affecting scale growth is learning rate, with end-training RMS increasing to the right of the optimal LR basin, as training becomes unstable. End-training RMS is remarkably stable as width, depth, training steps and batch size are independently increased.  

## B Unit-scaled op definitions

Table 8: Implementations of unit-scaled ops, building on Table A.2. from the Unit Scaling paper [[11]](#b9). These are considered part of u-µP and should be used in the place of standard operations.

## Op

Unit Scaling factors

$matmul(x, w) = xw α = 1 √ fan-in , β x = 1 √ fan-out , β w = 1 √ batch-size attention(q, k, v) = α = β q = β k = β v = softmax α attn d -1 head (qk ⊤ ) ⊙ c mask v 1/ log_interpolate 1 1+ 4d head α 2 attn , 1, log(s) s gated_silu(x in , x gate ) = α = β xin = β xgate = x in ⊙ x gate ⊙ sigmoid(α ffn-act x gate ) 1/ log_interpolate 1 1+ 1 α 2 ffn-act , 1 √ 2 , 1 2 residual_add(x resid. , x skip ) = a = τ √ τ 2 +1 , b = 1 √ τ 2 +1 a x resid. + b x skip (see G.2.$2 for full details, inc. values for τ , which depends on α res and α res-attn-ratio .)

$softmax_xent(x, t) = log_softmax(α loss-softmax x) t α = 1, β = s/ √ s -1 RoPE(x) α = β = 1 (i.e. no scaling)$RMSNorm(x) (non-trainable, see [[9]](#b7)) α = β = 1 (i.e. no scaling)

The original Unit Scaling paper provides scaling factors for various ops, in order to make them unit-scaled. However, these ops do not cover every case required for the Llama architecture used in our experiments, nor do they cover our updated residual layer implementation. To address this, in this section we outline a series of new unit-scaled ops for each of our required architectural features, as well as existing unit-scaled ops, as given in Table [8](#).

The presentation here is derived from that of the Unit Scaling Compendium given in [[11,](#b9)[Table A.2]](#). This makes reference to the factors α, β 1 , . . . , β k . α is the output scaling factor in the forward pass, and β i are the scaling factors for the gradient of the op's inputs in the backward pass. For each op, a value or rule is provided for determining the required mult to ensure unit-scale. The correct value for these multipliers is derived by analyzing the scaling behavior of each op, given some reasonable distributional assumptions about the input and incoming gradient tensors (see Appendix E.2 for an example). Below we provide an in-depth overview of each new or modified unit-scaled op we introduce here.

Unit-scaled dot-product attention The Unit Scaling paper considers the attention layer scaling in terms of its separate components: the various matmul operations and the internal softmax. Linear operations are scaled using the standard rule, and the softmax scaling is given a α = β = s factor.

From an implementation perspective, the self-attention layer is more typically broken down into weight-matmuls and a fused scaled-dot-product attention operation. This is the case we handle here, accounting for three complicating factors not considered in the Unit Scaling paper:

2. We follow the µP guidance of using 1/d head scaling in our self-attention layer, rather than the usual 1/ √ d head .

3. We place a α attn multiplier immediately before the softmax, which is an HP that users may tune.

As a result our dot-product attention takes the form:

$attention(q, k, v) = softmax α attn-softmax • d -1 head • (q • k ⊤ ) ⊙ c mask • v$The addition of an HP before the softmax introduces an additional challenge for Unit Scaling, as our scaling multipliers will need to account for this value when preserving unit scale. This operation is sufficiently complex that we found an empirical model of its scale to be more accurate than any mathematically-derived rule (future work may consider justifying our model mathematically). We find that the scale of dot-product attention is approximately The corresponding scaling rule is therefore to divide by this factor in both the forward and backward pass, as outlined in Table [8](#).

$σ(attention(q, k, v)) = log_interpolate 1 1 + 4d head α 2 attn , 1, log(s)$SwiGLU FFN Llama uses a SwiGLU [[57]](#b55) layer for its FFN, which introduces two new operations for us to unit-scale: a SiLU [[58]](#b56) (a.k.a. swish [[59]](#b57)) operation and an element-wise multiplication. We take a similar approach to our dot-product attention, and consider unit-scaling the following fused operation:

$gated_silu(x in , x gate ) = x in ⊙ x gate ⊙ sigmoid(α ffn-act x gate )$For the surrounding weight-matmuls we follow the standard Unit Scaling rules.

Again, we use an empirical model of the scale of this op, which is surprisingly similar to the dot-product attention model:

$σ(gated_silu(x in , x gate )) = log_interpolate 1 1 + 1 α 2 ffn-act , 1 √ 2 ,1 2 ,$dividing through by this factor to get our scaling rule.

Residual layers Our implementation of residual layers for u-µP is more complex than other operations, as adjustments are required to:

1. Make pre-norm residual networks support Unit Scaling (see Appendix F). 2. Introduce our new, principled residual HPs (see Appendix G).

Our residual layer scheme is presented in full in G.2.2. For readers interested in our justification for this, see the sections noted above. We also follow the example of Unit Scaling and delay the application of our residual multiplier in the backward pass to the base of the branch (see [[11]](#b9), Figure [3c](#fig_1)). This does not change the model, and enables unit-scale to be maintained on the residual branch regardless of the value of the multiplier.

## RoPE embeddings

We also require a unit-scaled implementation of Rotary Position Embeddings (RoPE [[60]](#b58)), which are applied just before the scaled dot-product attention operation. As RoPE essentially consists of pair-wise rotations of elements by different degrees, we observe no meaningful scale-change as a result of it's application, and hence leave it unchanged.

RMSNorm Following [[9]](#b7) we opt to use a non-trainable version of RMSNorm [[61]](#b59), in order to facilitate better transfer. As a result, we also leave this operation unchanged. Were a trainable RMSNorm to be used, the recipe would follow closely that of the LayerNorm presented in the original Unit Scaling Compendium.

## 5.

Train the target model: This can be done in FP8 simply by placing casts on matmul inputs (though for our large-scale experiments we found the scales of two operations drifted enough over time that some lightweight dynamic re-scaling was required).

The above functionality is provided in the Unit Scaling library, to avoid users having to implement it themselves, and to provide a reference implementation. We provide a guide to using this library in the following section.

D A guide to the unit scaling library

Our PyTorch [[62]](#b60) extension library, released under an open source license at [https://github.com/ graphcore-research/unit-scaling](https://github.com/graphcore-research/unit-scaling), accompanies this paper to provide standard and reference implementations of u-µP operations and optimizers.

This section provides an overview of the functionality of the library; please consult the repository documentation for details. A good place to start is our demo of a simple u-µP training implementation: [https://github.com/graphcore-research/unit-scaling/blob/main/examples/ demo.ipynb](https://github.com/graphcore-research/unit-scaling/blob/main/examples/demo.ipynb).

## D.1 Standard usage

Compared with SP, u-µP requires the insertion of appropriate scaling factors in the forward and backward pass, a different parameter initialization scheme and the application of learning rate scaling rules based on the role and shape of each parameter.

The library provides implementations of ops in unit_scaling.functional with appropriate scaling rules (Table [8](#)). Non-homogeneous ops (Appendix G.1) have optional mults, which are hyperparameters controlling shape of non-linear operations and the interpolation between mutiple inputs. Ops may also specify constraints, which are used to satisfy the cut-edge rule (Appendix H).

Although this rule could be automated as a global graph transformation, the library makes constraint selection an explicit step for the user, while providing sensible defaults. For example, weight gradients are generally cut-edges, so are unconstrained.

Parameters are tagged with their role in the model (as a "bias", "norm" parameter, "weight" or "output"). The library achieves this by extending torch.nn.Parameter with an additional property mup_type. This property is required for every parameter in a u-µP model. Given this, and information on the overall depth of the model, the library applies the learning rate rules of Table [2](#) as a preoptimizer transformation that modifies the learning rate for each parameter. This allows standard PyTorch optimizers to be used without modification.

PyTorch uses modules to encapsulate parameter declaration, initialization and the calling of ops. The library makes available u-µP versions of common modules, which declare tagged parameters, apply unit-scale initialization, and call unit-scaled ops, with appropriate default settings.

With these components, user code for training using u-µP is very close to that of vanilla PyTorch (see an example in Figure [21](#fig_19)). 

## D.2 Extending the library

As the set of deep learning ops of interest is always growing, the unit-scaling library is open for extension. For example, consider the possible implementation of unit-scaled hardtanh(x) = clip(x, -1, 1) in Figure [22](#fig_20).

$import$torch from math import e, erf, pi, sqrt from unit_scaling.constraints import apply_constraint from unit_scaling.scale import scale_fwd, scale_bwd def hardtanh(x, constraint="to_output_scale"): y_scale = 1 / sqrt(1 -sqrt(2/(pi*e))) grad_scale = 1 / sqrt(erf(1/sqrt(2))) y_scale, grad_scale = apply_constraint(constraint, y_scale, grad_scale) x = scale_bwd(x, grad_scale) y = torch.nn.functional.hardtanh(x) return scale_fwd(y, y_scale) The implementation follows a standard pattern:

1. Calculate the theoretical or empirical scaling factors for each forward and backward pass independently, based on independent unit-scaled Gaussian inputs. 2. Apply the optional constraint to select or combine these scaling factors, using the helper function apply_constraint. 3. Call scale_bwd on the inputs, and scale_fwd on the outputs to compensate for scaling after the op or grad-op is executed.

It can be checked empirically using random inputs and gradients (example in Figure [23](#fig_11)).

x = torch.randn(2**20, requires_grad=True) y = hardtanh(x, None) y.backward(torch.randn_like(y)) assert abs(y.std() -1) < 0.01 assert abs(x.grad.std() -1) < 0.01

Figure [23](#fig_11): Testing unit-scaled operations, using constraint=None to allow independent fwd and bwd scaling. The default constraint "to_output_scale" preserves forward-pass scale while constraining the forward and backward scales to be equal.

## D.3 As a reference implementation

The core technique of u-µP is readily implementable in most deep learning frameworks; the primary requirement is for custom gradient operations in order to provide equivalents of scale_fwd and scale_bwd. We hope that the library provides a useful reference, as well as a set of tools and techniques for developing custom u-µP support in other libraries and projects.

## E Additional background material E.1 The Maximal Update Parametrization

Theoretical background We do not cover the theory underpinning µP in this paper, presenting only its resulting scaling rules (Table [1](#)). For readers interested in this theory, the extensive Tensor Programs series [[63,](#b61)[64,](#b62)[65,](#b63)[66,](#b64)[67]](#b65) builds up a framework from which µP is derived [[1]](#b0). For those requiring a more accessible introduction, [[68]](#b66) show that µP can be derived in a simpler and more general way by placing a spectral scaling condition on the norm of weights and their updates.

Applying unit scaling To apply Unit Scaling to a model and train in low-precision, the following steps are required:

1. Scale parameter initializations to have zero-mean and unit variance.

2. Replace operations with their unit-scaled equivalents (including and especially the loss, matmuls and residual-adds).

3. Constrain the scales of operations which are required to have the same forward and backward factors.

4. Place a simple .to(fp8) cast on the inputs to matmuls.

Step 3 relates to the problem of conflicting scales in the forward and backward passes. A single linear layer in a differentiated model requires 3 matmul ops in the forward and backward passes, each requiring a different scaling factor ( 1

$√ d fan-in , 1 √ d fan-out , 1 √ d batch-size$). However, using these directly would give invalid gradients. The compromise here is that the activations and activation gradients have their scaling factors constrained such that they are equal (the original Unit Scaling paper recommends taking the geometric mean; we modify this for u-µP in Appendix B to simply use the forward scale everywhere). Weight gradients can still be given their own scaling factor due to the cut-edge rule (as explained in Appendix H).

Step 4 reflects the key benefit of Unit Scaling. Unlike other methods it changes the learning dynamics of a model, but the advantage is that unit-scaled models then 'naturally' generate well-scaled tensors. This means that low-precision arithmetic ideally becomes as simple as placing a cast operation before matmuls as outlined.

## F Unit-scaled pre-norm residual layers

The popular pre-norm residual network architecture is simple to implement, but problematic to combine with Unit Scaling. It exhibits scale-growth in the skip-stream at initialization, due to the repeated addition of residual connections without subsequent normalization. Here we present a surprising and useful finding: that for any pre-norm model there exists a mathematically-equivalent model where this scale-growth is eliminated, through the careful re-scaling of residual connections.

Note that this section focuses on applying Unit Scaling to standard pre-norm models. Only once we have addressed this problem are we able to do the same for u-µP models, as shown in Appendix G.2. Readers only interested in our final u-µP residual implementation may skip ahead to Appendix G.2.2.

## F.1 Scale growth in pre-norm residual networks

Let's consider a pre-norm residual network of depth L:

$R 0 (x) = r 0 x, (6) R l (x) = r l f l (R l-1 (x)) + R l-1 (x), l = 1, .., L (7) R L+1 (x) = f L+1 (R L (x))(8)$with embedding multiplier r 0 and residual branch multipliers r l for l = 1, .., L. To satisfy pre-norm, all f l are zero-homogeneous functions, i.e. f l (λx) = f l (x).

The scale of the skip-stream at initialization as a result of Equation ( [7](#)) is

$σ(R l ) = r 2 l σ(f l ) 2 + σ(R l-1 ) 2 > σ(R l-1 ), l = 1, .., L(9)$assuming r 2 l σ(f l ) 2 > 0. This shows that scale inevitably grows with the addition of each residual layer.

This scale-growth is clearly incompatible with unit scaling, which aims for σ(R l ) = 1 for all l = 0, .., L + 1. In the following we present an elegant solution to this problem making use of a symmetry transformation available in pre-norm residual architectures. 

## Residual add

This operation is non-unary and hence receives our second (and third) multipliers: α res , α res-attn-ratio . The manner and motivation for using two multipliers here is justified in the next section.

## FFN RMSNorm

This operation is 0-homogeneous and thus we start over.

## FFN input scale

The input layer is linear, hence it propagates scale.

## Sigmoid input

This function is non-homogeneous and thus we have our fourth multiplier: α ffn-act .

## SiLU weight

This layer is also linear and propagates scale.

## Product

The entry-wise multiplication of the outputs of sigmoid, input layer and SiLU weight is homogeneous and thus propagates scale.

## FFN output

This layer is linear and at the end of the residual path. Hence there are no more multipliers in the FFN residual block. Residual add See above.

## Output RMSNorm

This operation is 0-homogeneous and thus we start over.

## Output head

This layer is linear, hence it propagates scale.

## Loss

The cross-entropy loss is non-homogeneous and leads to our final multiplier: α loss-softmax .

To facilitate our analysis, we can view the transformer residual output as the sum of three terms:

$R L = R (e) L + R (a) L + R (f ) L , R (e) L := α e x, R(a)$$L := L/2 l=1 α a L/2 f 2l-1 (R 2l-1 (x)), R(f )$$L := L/2 l=1 α f L/2 f 2l (R 2l (x)),$and define the average residual scale,

$σ(R (a,f ) L ) 2 := σ(R (a) L ) 2 + σ(R (f ) L ) 22$.

Note that we have added in the depth-µP multipliers here, though a similar analysis can be performed for non-depth-µP models. As above, f l functions alternate between self-attention layers and feedforward layers.

With respect to our interpretability criterion, we propose two new multipliers that correspond to dynamics in the network which we suggest are important to control at initialization. The first is the ratio of the average scale of the residuals' contributions to those of the embedding, α r = σ(R

$(a,f ) L )/σ(R(e)$L ). The second is the ratio of the scale of the attention-residuals' contributions to those of the feed-forward-residuals, α ρ = σ(R (a) L )/σ(R (f ) L ). Not only do these two ratios control key dynamics of our model, but we can use them to replace our existing (α e , α a , α f ) multipliers.

Let us first examine these two quantities for a standard (non-unit-scaled model). Residual functions of the same kind have the same expected output scale at initialization in pre-norm networks, meaning we can denote the output scale σ(f l (R l )) of all self-attention functions as σ a , and of all feed-forward functions as σ f . We thus have the following scales at the output:

$σ(R (e) L ) = α e σ(x), σ(R (a) L ) = α a L/2 σ   L/2 i=1 f 2l-1 (R 2l-1 )   = α a σ a , σ(R (f ) L ) = α f L/2 σ   L/2 i=1 f 2l (R 2l )   = α f σ f , σ(R (a,f ) L ) = (α a σ a ) 2 + (α f σ f ) 2 2 .$Recalling our definitions of α r , α ρ above, this gives us:

$α ρ = α a α f σ a σ f , α r = (α a σ a ) 2 + (α f σ f ) 2 2 (α e σ(x)) 2 , = α 2 ρ + 1 2 σ f σ(x) α f α e .$The original α a , α f multipliers can then be written in terms of α r , α ρ :

$α a = α ρ α f σ f σ a α f = α r α e σ(x) σ f 2 α 2 ρ + 1$We have replaced two of the three original multipliers, but still have a dependence on α e here in our expressions for α f and R

L , which we now remove by dividing it out of our residual branches and embedding. We use the hat (•) symbol to denote terms that have been divided-through by α e . This new system of equations is equivalent to our old one thanks to the zero-homogeneity of the final post-residual layer:

$R L+1 (x) = f L+1 (R (e) L + R (a) L + R (f ) L ) = f L+1 ((R (e) L + R (a) L + R (f ) L )/α e ) = f L+1 ( R(e) L + R(a) L + R(f) L )$which we implement (assuming depth-µP branch scaling) as:

$a l =        α attn-residual L/2 l is odd (self-attention) α ffn-residual L/2 l is even (feed-forward) b l = 1 c = α emb .$The corresponding u-µP set of residual HPs is (α res , α res-attn-ratio ), which we implement as:

$a 2 l = τ 2 l τ 2 l + 1 (25) b 2 l = 1 τ 2 l + 1 (26) c = 1,(27) (28)$$τ 2 l =            α2 a L 2 + ℓα 2 a + ℓα 2 f l is odd α2 f L 2 + (ℓ + 1) α2 a + ℓα 2 f l is even , ℓ = l -1 2(29)$$α2 a = α 2 res-attn-ratio α2 f(30)$$α2 f = 2 α 2 res-attn-ratio + 1 α 2 res .(31)$This is the u-µP residual scheme. It satisfies the three properties that we initially set out to achieve: the variance at initialization of our R l (x) is always 1, our HPs have a clear and useful interpretation, and our scheme is as expressive as the baseline (which is neither unit-scaled or has interpretable HPs).

## H The cut-edge rule

In the section we review the notion of constraints used for scaling operations in a computational graph. For a more thorough, generalized treatment, please refer to Section 5.1 and Appendix E.4 of the original Unit Scaling paper [[11]](#b9).

For simplicity, we will only discuss the cut-edge rule in the context of a typical neural network. For each operation f , parametrized by θ taking input x and emitting output y, a user must choose how to scale y, ∇ x and ∇ θ (gradient of loss w.r.t. x and θ respectively). In the simplest case, where there are no further data dependencies, we can simply choose factors that preserve unit scale. In more complex scenarios, we must balance the need for each tensor to be unit-scaled and for gradients to be correct up to a constant factor.

In particular, a problem emerges in the presence of residual blocks in which y = x + f (x; θ). In these circumstances, ∇ x is computed as the sum of residual gradient ∇ f ∂f ∂x and skip gradient ∇ y . If we choose not to insert scaling factors into our graph, ∇ f ∂f ∂x and ∇ y will have some ratio of scale r. However, if we have chosen to rescale the gradient of operations in f , then ∇ f ∂f ∂x will have been rescaled by some s. This means the new ratio of ∇ f ∂f ∂x and ∇ y will be r • s. Therefore, when adding these together, ∇ x is no longer a correct gradient up to a constant factor.

How do you remedy this? If we can ensure that the scaling factors are the same for both the input gradients and outputs of an op, we will have s = 1. This ensures that gradients for inputs to residual blocks are correct up to a constant factor. How do you decide when you are free to preserve unit scale, and when to constrain scaling factors to be the same? We previously define the cut-edge rule [[11]](#b9) for computational graphs where nodes represent forward pass operations and edges represent operation outputs. If an input edge is a cut-edge, i.e., the number of connected components in the graph would increase upon deletion (examples in a typical transformer model: output of embedding gather, output of a residual add, output of final norm, output token logits, weights), there is no need to constrain the scales of the operation's output edge and the input edge gradient. For all other input edges (e.g., inputs to a residual add, intermediates computed along a residual branch), the scales of gradients and outputs should be constrained.

## I From µP to u-µP

Here we outline additional details to help readers follow the process of deriving u-µP from the combination of Unit Scaling and µP. Our first step of dropping σ W and base-fan-in, and moving α W s to functions, results in Table [11](#tab_22). This intermediate scheme does not yet satisfy Unit Scaling, but simplifies the HP rules in preparation for further changes. Note that we have also removed ηemb as we don't include this HP in our u-µP extended HP set. We have included residual scaling rules here, in accordance with depth-µP, which we intend u-µP to satisfy, though our standard µP implementation doesn't use it. 

$(A W ) 1 1 1 fan-in 1 √ depth * initialization (B W ) 1 1 √ fan-in 1 - Adam LR (C W ) η η 1 fan-in η 1 √ depth$
## J Low-precision and its trade-offs

Number formats for deep learning The standard numerical representations used in deep learning are the set of formats defined by the IEEE 754 floating-point standard [[70]](#b68). IEEE floats comprise three elements: a sign bit, exponent bits, and mantissa bits. The number of exponent bits determines the range of a format, while the mantissa determines the precision[foot_7](#foot_7) . We refer readers to the original Unit Scaling paper ( [[11]](#b9), Section 3.1) for a comprehensive overview of floating-point representations.

The default format used for training is the single-precision floating-point format, commonly known as FP32, with some hardware providers automatically casting it to the smaller TF32 compute mode for accelerated arithmetic. The 16-bit FP16 and BF16 formats were later introduced, and more recently the FP8 E5 & E4 formats [[71,](#b69)[72,](#b70)[73]](#b71). The higher range of E5 has typically been used for gradients, while the higher precision of E4 has been seen as necessary for weights and activations.

Our particular implementation of FP8 training is covered in Section 4.2. Other aspects of training such as the optimizer state and cross-device communication have also been put into FP8 [[29]](#b27), though not all tensors are amenable to being run in the lowest precision [[40]](#b38) without degradation. The use of multiple formats is known as mixed precision [[74]](#b72). A comparison of these formats is given in Table [12](#tab_23).

The benefits of low-precision Using numerical representations with fewer bits facilitates the design of more efficient arithmetic in hardware, typically leading to a linear increase in peak FLOPS (as shown in Table [12](#tab_23)). As large-scale training efforts are typically compute-bound due to the size of matmuls [[75]](#b73), putting the inputs to these operations in low-precision formats has a substantial impact on training efficiency. Low-precision formats also reduce the other two common performance constraints: for memory-bandwidth-bound models they require fewer bits to be transmitted, and for memory-size-bound models they require fewer bits to be stored. The challenges of low-precision Unfortunately, moving to low-precision formats also increases quantization error. For values within the representable range this takes the form of rounding error, and for values outside it, clipping error (both overflow and underflow). Rounding error tends to be an intrinsic problem: the number of mantissa bits dictates the expected accuracy of representations and this cannot easily be changed. In contrast, clipping error is often eliminated by scaling a tensor so that its values lie within the range of a format. Note that a multiplicative change in values of this kind doesn't affect the (relative) rounding error, due to the exponential spacing of values. Most research into making low-precision work has focused on the problem of scaling tensors in this way.

Simply casting all tensors to FP16 or FP8 tends to impair training, largely due to clipping error. For FP16, this primarily affects gradients. [[74]](#b72) address this by introducing a fixed global loss-scale HP, which multiplies the loss value in the backward pass, artificially up-scaling gradients to lie within FP16 range [[74]](#b72). Automatic loss scaling [[76]](#b74) builds upon this idea, making the loss-scale a dynamic value that is tuned during training.

The later BF16 format has the same range as FP32, making loss scaling unnecessary. For FP8 no such range-equivalent format can exist, so the problem of clipping error must be addressed. Most FP8 implementations have done so by moving from a global loss-scale to a local scale for each FP8 tensor. In pseudo-code, this takes the form: where we assume that matmul takes inputs in FP8 and directly produces the output in higher precision.

The result of the scale() operation can either be a fixed scale determined before training [[72]](#b70), or in the case of Transformer Engine [[77]](#b75), computed dynamically as a function of the 'absmax' of the input tensor (though they introduce a delay across time-steps, to facilitate an efficient fused kernel).

Increasing granularity and computing scales dynamically using this kind of method inevitably adds complexity (from both a logical and implementation perspective), as well the potential for computational overhead. Unit Scaling generally avoids the need for matmul input scaling.

## K Benchmarking scaled matrix multiplication implementation in PyTorch

Given that the end-goal of leveraging u-mup's low-precision properties is to speed up training and reduce memory usage, it's reasonable to ask why we don't investigate this experimentally. The answer relates to the relative immaturity of the FP8 training software stack -a lack of open, efficient FP8 kernels for compute and communication mean significant additional engineering effort is required to attain expected speedups over the full model.

Here we show that u-µP's static scaling factors add no overhead to matmuls in FP8, and hence ought to be able to reach close to the maximal FP8 throughput attainable for the full model. Standard strategies for FP8 training require expensive statistics gathering (e.g., amax) per tensor. A key benefit of u-µP for FP8 training is that it instead provides us with static scaling factors to rescale operation outputs. Even a naive implementation in pytorch can achieve a minimal drop in hardware utilization.

Figure [24](#fig_22) demonstrates hardware utilization for FP8, FP16, and FP32 matrix multiplications on a single NVIDIA H100 PCIe card. For FP16 and FP32, torch.matmul is used, whereas torch._scaled_mm is used for FP8. Comparing "scaled" to "unscaled" matrix multiplication demonstrates a 30%, 20%, and 10% drop in hardware utilization for each data type respectively. In the case of FP8, where the drop in utilization is most pronounced, utilization can be recovered by passing the scaling factor as a scale associated with one of the two input tensors.

It should be noted that as of PyTorch version 2.3, torch._scaled_mm always computes amax as well as the matrix multiplication. The performance of FP8 matrix multiplications could be higher without this overhead.

The above analysis focuses on throughput; significant memory savings are also possible through the use of FP8, though how this affects the total memory footprint depends on various additional variables and the overall distributed training setup. The following factors are play a significant role: typically the main memory bottlenecks are the optimizer states, which are kept in full precision. This footprint can be reduced by applying ZeRO sharding [[55]](#b53), though for significant gains the number of data parallel processes needs to be sufficiently large and ZeRO stage 2 or 3 are required. In these settings the memory footprint of activations and gradients becomes significant, and quantizing these to lower precision promises further memory savings, though may be non-trivial [[29]](#b27).

## L Attention output RMS grows with model depth

A core assumption in deriving per-op scaling factors is that each input to an operation has zero mean, unit-variance, and uncorrelated elements at initialization. This is trivially true for weights and by extension the token embeddings taken as input to the transformer trunk. However, this is not guaranteed for intermediate results and gradients if an operation in the computational graph induces correlation in the elements. In such a scenario our scaling factors will not return unit-variance outputs as we will not have corrected for these correlations in the inputs. As we then increase the depth of the network, where the same operation is left to amplify correlations, we can end up with variance in intermediate results and gradients scaling with depth Figure [25](#fig_24) illustrates this phenomenon in a unit-scaled four-layer Llama model with width=256. All activation tensors in the residual branches are unit-scaled, except for the output of the attention layers. We also see that the variance of attention outputs grows with depth. Since Llama models use pre-norm on the residual-branch, residual-branch inputs will revert to unit-scale again until they reach another instance of the correlation-inducing operation. As we add under-scaled attention layer results back to the skip-branch, our skip tensor variances grow with depth as our residual-add assumes unit-variance inputs. This has a knock-on effect on the global scaling of the gradients since the Jacobian of the final norm will scale the gradient by the inverse of the final skip tensor variance.  So which operation induces correlation in the attention output at initialization? For the default case where all multipliers are set to 1, our 1/d scaling of attention logits results in a sufficiently high temperature that attention probabilities are effectively uniform. With causal masking, we effectively take a running mean across the value tensor along the sequence dimension. As a result, each subsequent token representation is correlated with the last. Since we derive appropriate scaling factors for the first layer, we do not see scale growth emerging until the second layer, where correlations accumulate during the next effective running mean.

We leave it to future work to offer a solution to scale growth created by correlation in intermediate tensors. We note that this is scale growth emergent at initialization, but we also see scale growth in other intermediate tensors during training. Whether scale growth during training is related to the phenomenon outlined here remains to be seen.

![Figure 2: Effective µTransfer does not hold across all training setups. (a) We show strong transfer for the unrealistic setup used in Tensor Programs V (too many epochs; constant LR). (b) Moving to a more standard Llama training setup, transfer breaks down. (c) This is restored by the introduction of two stability fixes: non-parametric norms and independent weight decay.]()

![Figure 3: (Left) holding the embedding LR (η emb ) constant, vs. scaling with base-width /width, both with a fixed global LR. This suggests the µP embedding LR rule (c emb ) should follow the latter scaling. (Right) we test this by sweeping the global LR under the two scaling rules. The new rule leads to lower loss on large models. (Dot/cross markers represent the same runs across both graphs).]()

![Figure4: A visualization of the dependencies between pairs of HPs under each scheme. Transfer error measures the extent to which the optimal value of the transfer HP depends on the fixed HP (see Algorithm 1). On average, µP has a transfer error of 0.03, whereas u-µP has 0.005.]()

![Figure 5: Learning rate transfer for µP (top) and u-µP (bottom), over training steps, batch size and depth. See Figure 1 (b) for transfer over width. The default shape parameter for other panels is shown in bold. The shaded area shows the 95% confidence interval for the mean.]()

![Figure 6: Per-tensor RMS = σ 2 + µ 2 across u-µP and µP models at initialization (left) and after training (right). u-µP tensors have RMS that starts close to 1 and remains within E4M3 range at the end of training. Dashed and solid red lines show each format's min. normal and subnormal values.]()

![Figure 7: Large-scale training runs. (Left) u-µP BF16 vs u-µP FP8. (Right) u-µP BF16 vs SP BF16.]()

![Figure 8: The effect of the individual transfer stability fixes from Figure 2. (a) In this setting switching from non-independent to independent weight decay has only a minor effect, though [18] Figure 6 suggests it may be highly valuable in other settings. (b) Non-parametric norms give a narrower learning rate basin, leading to better transfer. (c) The combination of these, for comparison, matching Figure 2 (c).]()

![Figure 9: An ablation of the more standard Llama training settings against the Tensor Programs V settings from Figure 2. This shows that the flat basins with poor transfer are not due to a single change, but the combination of a larger dataset (training < 1 epoch) and the stronger Llama model are largely responsible. Note that 'Llama model' here indicates a group of changes: rms norm, rotary embeddings & swiglu FFN.]()

![Figure 10: A repeat of the batch size and training steps experiments in Figure5, but using the larger SlimPajama dataset where no data is repeated. In both settings our validation loss basins take the same shape, indicating that our analysis using the WikiText-103 dataset holds.]()

![]()

![12 2 -9.5 2 -7.5 2 -5.5 2 -3.5]()

![Figure14: Hyperparameter coupling sweep for µP. Note strong coupling between optima, e.g. in the cases of (η emb , σ init ) and (η, α attn ). See also: u-µP, Figure15.]()

![Figure 15: Hyperparameter coupling sweep for u-µP. Note less coupling than with µP, see Figure 14.]()

![Figure 19: RMS during training, for all parametrized matmul inputs, for µP (top) and u-µP (bottom). Model width 256, default hyperparameters, η = (2 1 , 2 -8 ) for (u-µP, µP).]()

![Figure 20: The effect of hyperparameters on FP8 training loss and on the end-training RMS of critical tensors: (a) decoder weight, (b) last-layer FFN down-projection input and (c) last-layer FFN down-projection output gradient. Only learning rate has a substantial effect on the end-training RMS. Vertical lines show the default setting of that hyperparameter, as used for all other plots.]()

![s where log_interpolate(α, b upper , b lower ) = e α log(bupper)+(1-α) log(b lower ) .]()

![Figure 21: Using the unit scaling library given the tensors input_ & target.]()

![Figure 22: Implementing new unit-scaled operations.]()

![= scale(A) b = scale(B) A = to_fp8(A / a) B = to_fp8(B / b) C = (a * b) * matmul(A, B)]()

![Figure 24: Square matrix multiplication throughput in TFLOPs with and without scaling factors applied to the output across 32-, 16-, and 8-bit float dtypes on NVIDIA H100 PCIe. Naive implementation in PyTorch.]()

![Figure 25: Scale of intermediate tensors grows with depth at initialization. Top left: Intermediate activation tensor RMS along the residual branch. Only the attention outputs after the first layer are not unit-scaled. Bottom left: Skip activation tensor RMS. Scale growth in attention outputs drives growth in skip activation scales. Note that layer_idx= 0 corresponds to the embedding output, and layer_idx= 4 corresponds to the final layer outputs. Top right: Intermediate gradient tensor RMS along the residual branch. Growth in the attention output scale drives growth in attention qkv gradient scales. Bottom Right: Skip gradient tensor RMS. The scale of output activations induces a global rescaling of the gradients.]()

![Default hyperparameters and training settings.]()

![Comparison of Tensor Programs V's standard settings (as best we can tell) and our Standard Llama setup, corresponding to (a) and (b) in Figure2.]()

![A walkthrough of the Llama architecture, showing how our α attn-softmax , α ffn-act and α loss-softmax multipliers are derived via an analysis of scale-propagation.linear and lies at the end of the attention residual path. Hence there are no more multipliers in the attention block.]()

![An intermediate scheme resulting from dropping those HPs from µP which are not needed under u-µP.]()

![A comparison of deep learning formats. E indicates exponent bits, and M mantissa bits. The smaller formats typically give more FLOPS, at the expense of reduced range and/or precision.]()

The GPT-4 technical report[[7]](#b6) hints at the use of µP by including[[2]](#b1) in its references, without citing it directly. The multipliers present in the Grok [8] codebase also suggest the use of µP.

As in other work, we use µP as a shorthand for the method outlined in Tensor Programs V, including µTransfer. Strictly speaking, µP ought only to refer to the parametrization outlined in Tensor Programs IV.

Lingle suggests independent weight decay is unstable, but we find it to be more so than Adam or standard AdamW.

This represents a slight deviation from the Maximal Update Parametrization, though we still refer to our scheme as a form of µP as it conforms in all other aspects.

As we use depth-µP this could be said about depth as well, but as[[14]](#b12) show that transformers don't attain depth-transfer under depth-µP we do not expect strong transfer across depth.

The training codebase used for our larger-scale experiments can be found at the following url https://github.com/Aleph-Alpha/scaling. We have also released model checkpoints, which are available at https://huggingface.co/Aleph-Alpha.

As we use a decoder-style transformer in our experiments, our softmax operation has a causal mask applied to its input.

Confusingly, the term low-precision tends to indicate using <32 bit-width formats, so in this context precision also reflects the number of exponent bits as well as the usual mantissa bits.

