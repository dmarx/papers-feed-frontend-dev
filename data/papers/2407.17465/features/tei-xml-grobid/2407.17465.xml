<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">u-µP: The Unit-Scaled Maximal Update Parametrization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-10">10 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Charlie</forename><surname>Blake</surname></persName>
							<email>charlieb@graphcore.ai</email>
							<affiliation key="aff0">
								<address>
									<settlement>Aleph Alpha</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Graphcore</forename><forename type="middle">Constantin</forename><surname>Eichenberg</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Aleph Alpha</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aleph</forename><surname>Alpha</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Aleph Alpha</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carlo</forename><surname>Luschi</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Aleph Alpha</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Graphcore</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Aleph Alpha</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Weinbach</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Aleph Alpha</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><forename type="middle">Orr</forename><surname>Graphcore</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Aleph Alpha</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">u-µP: The Unit-Scaled Maximal Update Parametrization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-10">10 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">40368627A9EE6139567C2F380FAAB502</idno>
					<idno type="arXiv">arXiv:2407.17465v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Maximal Update Parametrization (µP) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-µP, which improves upon µP by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: µP ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-µP models reaching a loss that is equal to or lower than comparable µP models and working out-of-the-box in FP8.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>10 0 10 1 10 2 10 3 Run Count 3.2 3.3 3.4 3.5 3.6 Best Validation Loss u-µP 256 µP 256 (a) More Efficient HP Sweeps HP Sweep Strategy u-µP µP Random Search Independent Search → LR → Mults → Combined Mults 2 -11 | 2 -2 2 -9 | 2 0 2 -7 | 2 2 2 -5 | 2 4 µP | u-µP Learning Rate 2.8 3.0 3.2 3.4 3.6 u-µP 256 u-µP 4096 µP 256 (b) Better HP Transfer Widths 128 256 512 1024 2048 4096 0 2500 5000 7500 Training Step 2 4 6 8 10 (c) Simple FP8 Training u-µP 4096 µP 4096 Precision FP32 FP8 µP 4096</p><p>Figure <ref type="figure">1</ref>: (a) Two different HP sweeping processes used for µP and u-µP proxy models. Unlike µP, u-µP admits independent (1D) search due to careful HP design. The first part of independent search is an LR sweep, which alone reaches near-optimal loss for u-µP. (b) Using the best proxy HPs from (a), we train many models at different widths and LRs. The best LR for width 256 is ~optimal for 4096, showing LR transfer along with lower loss. (c) We re-train with a simple un-scaled .to(float8) cast on matmul inputs. This would fail for other models, but u-µP trains with minimal degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The challenges of large-model training extend beyond the domain of engineering; they are also algorithmic in nature. Effective approaches for training smaller models are not guaranteed to work at the multi-billion-parameter scale used for today's large language models (LLMs). These difficulties can be framed in terms of stability, which we consider in three forms:</p><p>1. feature learning stability, which ensures that parts of the model do not learn too fast or slow relative to each other. 2. hyperparameter stability, which ensures that the optimal HPs for small models remain unchanged as the model size grows. 3. numerical stability, which ensures that floating-point representations during training stay within the range of a given number format.</p><p>The Maximal Update Parametrization (µP) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> targets the first two sources of instability. µP defines a set of scaling rules that in principle make a model's optimal HP values consistent across model sizes and ensure 'maximal feature learning' in the infinite-width limit. The practical benefits of this are that models continue to improve as they get larger, and that practitioners can re-use a set of HP values (especially the learning rate) found for a small proxy version of their model, on a larger target model. This is vital for modern LLM training, where the cost of sweeping over candidate HP values for the target model is prohibitive. Consequently, µP has been adopted by several open LLM training efforts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> and there are indications of its use in state-of-the-art LLMs<ref type="foot" target="#foot_0">foot_0</ref> .</p><p>However, there exists a gap between the extensive theory underpinning µP and its effective use in practice. This relates to issues surrounding efficient HP search, HP transfer, interpretability, ease-of-use and low-precision training. Some of these problems have been observed in the literature <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b1">2]</ref>; others we outline here for the first time. As a result, µP does not necessarily provide the kind of simple, stable scaling for which a user might hope.</p><p>To address this, we propose the Unit-Scaled Maximal Update Parametrization (u-µP). u-µP combines µP with another closely-related training innovation, Unit Scaling <ref type="bibr" target="#b9">[11]</ref>. µP ideally provides consistent training dynamics across model sizes, but says little about what those dynamics should be. Unit Scaling addresses this by proposing an ideal principle for dynamics: unit variance for all activations, weights and gradients. Unit Scaling was initially designed to ensure stable numerics, but in the context of µP the principle of unit-scale brings many additional benefits. We show that it provides a foundation upon which the broader range of drawbacks identified for µP can be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>We focus on LLMs in this work as this is the domain where µP has primarily been used in the literature (though u-µP's principles should extend to other architectures). We contribute the following:</p><p>1. Drawbacks of standard µP: We show that the way µP is typically applied has several limitations, and does not give effective transfer for Llama-style models (Section 3). 2. Simpler scaling rules: u-µP is easier to implement in practice than µP, and removes the unnecessary 'base shape' and initialization-scale HPs (Section 4.1; Table <ref type="table">2</ref>). 3. Out-of-the-box FP8 training: u-µP models generate tensors that lie close to the center of a floating point format's range, meaning that most matrix multiplications can be performed in FP8 via a simple .to(float8) cast without dynamic rescaling. 4. A principled, interpretable &amp; independent set of HPs: The set of transferable HPs used in the µP literature is chosen in an inconsistent and arbitrary way. We provide concrete recommendations for a good set of transferable HPs to use with u-µP (Section 4.3). 5. Improved HP transfer: We identify a problem with the scaling of the embedding layer's LR under µP. Fixing this for u-µP gives us better scaling with width (Section 4.4). <ref type="bibr" target="#b5">6</ref>. A more efficient approach to HP search: We show that u-µP facilitates a cheaper independent search method, attaining near-optimal loss when only sweeping the LR (Section 4.5).</p><p>We provide a guide for using u-µP in Appendix C, and a library <ref type="bibr" target="#b10">[12]</ref> implementing u-µP functions, layers and optimizers, outlined in Appendix D.</p><p>2 Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Maximal Update Parametrization</head><p>Tensor Programs V <ref type="bibr" target="#b1">[2]</ref> defines a parametrization as 'a rule for how to change hyperparameters when the widths of a neural network change'. They show that µP is the only parametrization that gives 'maximal feature learning' in the limit, whereas standard parametrization (SP) has imbalanced learning (parts of the network blow up or cease to learn).</p><p>One consequence of this improved stability is that learning dynamics under µP are ideally independent of model-size, as are optimal HPs. This facilitates a method known as µTransfer, which describes the process of training many smaller proxy models to evaluate candidate HP values, then using the best-performing ones to train a larger target model. An HP is said to be µTransferable if its optimal value is the same across model-sizes.</p><p>ABC-parametrizations µP, SP, and the Neural Tangent Kernel (NTK) <ref type="bibr" target="#b11">[13]</ref> are all instances of abc-parametrizations. This assumes a model under training where weights are defined as:</p><formula xml:id="formula_0">w 0 ∼ N (0, B 2 W ),<label>(1)</label></formula><formula xml:id="formula_1">W t = A W • w t , w t+1 = w t + C W • Φ t (∇L 0 , ..., ∇L t ),</formula><p>with t a time-step and Φ t (∇L 0 , ..., ∇L t ) the weight update based on previous loss gradients.</p><p>A parametrization scheme such as µP is then defined specifying how scalars A W , B W , C W change with model width. This can be expressed in terms of width-dependent factors a W , b W , c W , such that</p><formula xml:id="formula_2">A W ∝ a W , B W ∝ b W , C W ∝ c W .</formula><p>The values these factors take are what characterize a particular scheme. For µP these are given in Table <ref type="table">1</ref>. For depth a similar result has been proved using depth-µP <ref type="bibr" target="#b12">[14]</ref>, albeit in a restricted setting. When we refer to µP in the paper we assume the depth-µP scaling rules (Table <ref type="table">2</ref>, 'Residual' column).</p><p>A key property of the abc-parametrization is that one can shift scales between A W , B W , C W in a way that preserves learning dynamics (i.e. the activations computed during training are unchanged). We term this abc-symmetry. For a fixed θ &gt; 0, the behavior of a network trained with Adam is invariant to changes of the kind:</p><formula xml:id="formula_3">A W ← A W • θ, B W ← B W /θ, C W ← C W /θ<label>(2)</label></formula><p>(reproduced from Tensor Programs V, Section J.2.1). This means that parametrizations like µP can be presented in different but equivalent ways. ABC-symmetry is a key component in developing u-µP.</p><p>Transferable HPs µP focuses on the subset of HPs whose optimal values we expect to transfer across axes such as width and depth. We term these µTransferable HPs. All µTransferable HPs function as multipliers and can be split into three kinds, which contribute to the three (non-HP) multipliers given by the abc-parametrization: α W , σ W , η W where</p><formula xml:id="formula_4">A W ∝ α W , B W ∝ σ W , C W ∝</formula><p>Table <ref type="table">1</ref>: The scaling rules defining µP. The type of a weight is determined by whether fan-in &amp; fan-out both depend on width (hidden), only fan-out does (input), or only fan-in (output). Hence fan-in is always a multiple of width here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABC-multiplier Weight (W ) Type Input</head><p>Hidden Output µP parameter (a W ) η W . The difference between these multipliers and the ones that define a parametrization is that they are specified by the user, rather than being a function of width. α W and η W are rarely introduced outside of the µP literature, but can be valuable to tune for both µP and SP models. In the µP literature the term 'HPs' often implicitly refers to µTransferable HPs. We adopt this convention here, unless specified otherwise.</p><p>Base shape Two additional (non-µTransferable) HPs introduced by µP are the base-width and base-depth. This refers to a mechanism where a user specifies a particular shape for the model, where its behavior under µP and SP are the same. The µP model still scales according to the abc-rules, so for all other shapes the two models will be different. This is implemented by dividing the µP scaling rules for the given model by those of a fixed-shape model at the base-width and base-depth.</p><p>Putting this together with our abc-parametrization given in Equation ( <ref type="formula" target="#formula_0">1</ref>), and the µTransferable HPs outlined above, we now derive our final, absolute expressions for A W , B W , C W :</p><formula xml:id="formula_5">A W ← α W a W a Wbase , B W ← σ W b W b Wbase , C W ← η W c W c Wbase<label>(3)</label></formula><p>Though base shapes are necessary for µP, they are not typically swept. Rather, they are considered a preference of the user, who may wish to retain the behavior of an existing SP model at a given shape.</p><p>Choosing HPs to sweep In theory, the search space of µTransferable HPs includes α W , σ W , η W for every parameter tensor W in the model. In practice far fewer HPs are swept, with global grouping often used for σ W and η W , and many α W s dropped or grouped across layers.</p><p>The sets of HPs chosen for sweeps in the µP literature is explored in Appendix E.1. Tensor Programs V uses a random search to identify the best HP values, which has become the standard approach to sweeping. The number of runs in a sweep is typically in the low 100s, incurring a non-negligible cost (though usually less than a single training run of the target model). This high number partly owes to dependencies between HPs (shown in Section 5.2), making the search space hard to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Low-precision training</head><p>All the major potential bottlenecks of model training-compute, communication and storage-see roughly linear improvements as the bit-width of their number format is reduced. In modern LLM training, the compute cost of large matrix multiplications (matmuls) means that substantial gains are available if these can be done in low-precision (&lt; 32 bit) formats. With the ending of Dennard scaling and Moore's law <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16]</ref>, the use of low-precision formats represents one of the most promising avenues towards increased efficiency in deep learning.</p><p>Recent AI hardware offers substantial acceleration for the 8-bit FP8 E4 and E5 formats. However the reduced range of these formats means that they cannot directly represent some values generated during training. Various methods have been introduced to address this, such as the per-tensor dynamic re-scaling in Transformer Engine <ref type="bibr" target="#b15">[17]</ref>. However, this comes at the cost of added complexity and potential overheads. For a more in-depth treatment of low-precision formats, see Appendix J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unit Scaling</head><p>An alternative approach to low-precision training is Unit Scaling <ref type="bibr" target="#b9">[11]</ref>, which also uses fine-grained scaling factors to control range, but instead finds these factors via an analysis of expected tensor statistics at initialization. These are fixed factors, calculated independently of the contents of a tensor, at the beginning of training. As such, the method is easy to use and only adds the overhead of applying static scaling factors (which we show to be negligible in Appendix K).</p><p>These factors are chosen to ensure the unit variance of activations, weights and gradients at initialization. This is a useful criterion as it places values around the center of floating-point formats' absolute range. This applies to all tensors, meaning every operation in the network requires a scaling factor that ensures unit-scaled outputs, assuming unit-scaled inputs. Unit Scaling does not provide a mechanism for re-scaling tensors dynamically during training, but due to its ideal starting scale for gradients, activations and weights this may not be required. Empirically this is shown to be true across multiple architectures, though it is not guaranteed.</p><p>6 8 10 12 14 16 Validation Loss (a) Tensor Programs V settings (on GPT, Wikitext-2) 2.8 3.0 3.2 3.4 3.6 3.8 4.0 (b) Standard Llama settings (on Llama, Wikitext-103) 2.8 3.0 3.2 3.4 3.6 3.8 4.0 (c) Standard Llama settings + stability fixes (on Llama, Wikitext-103) 2 -12 2 -10 2 -8 2 -6 Learning Rate 0 1 2 3 4 5 Training Loss 2 -11 2 -9 2 -7 2 -5 2 -3 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 4.0 2 -11 2 -9 2 -7 2 -5 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 4.0 Width 128 256 512 1024 2048 4096 Min We provide an example of deriving the Unit Scaling rule for a matmul op in Appendix E.2, resulting in the scaling factor: 1/ √ d fan-in . We accompany this example with a full recipe for applying Unit Scaling to an arbitrary model. <ref type="bibr" target="#b2">3</ref> The challenges with µP in practice 3.1 Not all training setups give µTransfer Lingle <ref type="bibr" target="#b7">[9]</ref> shows that directly applying µP to a decoder LM fails to provide LR transfer across width. Given that the primary use of µP in the literature has been LM training of this kind, this result suggests a significant limitation. How do we reconcile this with the strong LR transfer across width shown for language models in Tensor Programs V?</p><p>We answer this in Figure <ref type="figure" target="#fig_0">2</ref>. The first training setup (a) is aligned with that used in Tensor Programs V (their Figure <ref type="figure" target="#fig_2">4</ref>). There are several atypical aspects to their training setup, primarily the use of a constant LR schedule and a high number of epochs; we outline the precise differences between setup (a) and (b) in Table <ref type="table" target="#tab_13">6</ref>. This overfitting regime makes validation loss unusable, and transfer misleadingly good. When we remove these and shift to a standard Llama training setup (b), optimal HPs begin to drift with width (see Figure <ref type="figure" target="#fig_7">9</ref> for an ablation). This confirms Lingle's findings that standard µP is in fact a poor fit for modern LM training. We fix this (c) by the removal of parameters from LayerNorms/RMSNorms, as suggested by Lingle, and the introduction of independent weight decay for AdamW, as suggested by Wortsman et al. <ref type="bibr" target="#b16">[18]</ref> <ref type="foot" target="#foot_2">foot_2</ref> (see <ref type="bibr" target="#b17">[19]</ref> for further analysis). With these changes adopted, we recover the strong transfer shown in Tensor Programs V's experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">It's not clear which hyperparameters to sweep</head><p>The problem of selecting HPs to sweep can be framed as choosing a subset of the per-tensor α W , σ W , η W HPs outlined in Section 2.1, and grouping across/within layers. As shown in Table <ref type="table">9</ref>, µTransfer experiments in the literature have done this in a variety ways. Practitioners have not justified these choices, appearing to rely on a mixture of precedent and intuition. We outline two major downsides to the lack of a principled approach.</p><p>Firstly, not all groupings of HPs are suitable. Consider the commonly-used global σ init HP. At initialization the activations going into the FFN swish function have std(x swish ) ∝ σ Wgate , whereas the self-attention softmax activations have std(x attn ) ∝ σ WQ σ WK . A global σ HP thus has a linear effect on the FFN and a quadratic effect on attention, suggesting that this grouping may be unideal.</p><p>Secondly, not all HPs are independent of one another. The key example of this is the interaction between σ W and η W . The relative size of a weight update is determined by the ratio η W /σ W , not by either HP individually. Because of this, the optimal values for σ and η depend on each other, which we demonstrate empirically in Section 5.2. This can make the problem of HP search much harder, and may be why hundreds of random-search runs have been required for sweeps in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Base shape complicates usage</head><p>Most practitioners are unlikely to require alignment with an SP model, in which case it is unclear what base-width (and base-depth) should be used. The literature has aligned on a standard base-width of 256 (see Table <ref type="table">9</ref>), but this appears to lacking a principled motivation-though the fact that they are not dropped entirely suggests they may be beneficial under u-µP.</p><p>Implementing base-shape HPs (see Equation ( <ref type="formula" target="#formula_5">3</ref>)) can also add complications from an engineering perspective. The proposed implementation in the mup library <ref type="bibr" target="#b18">[20]</ref> reflects this, requiring an extra 'base' model to be created and the original model to be re-initialized. This can interact awkwardly with other model-transforms for features like quantization, compilation, etc:</p><formula xml:id="formula_6">import mup proxy_model = MupModel(d_model=128, ...)</formula><p># proxy width base_model = MupModel(d_model=256, ...) # base width mup.set_base_shapes(proxy_model, base_model) # re-initialize proxy_model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">µP appears to struggle with low-precision</head><p>Finally, we note an interesting contradiction observed in the relationship between µP and lowprecision. One of the stated aims for µP is that its activations have Θ(1)-sized coordinates in the limit [2, Desiderata J.1]. This desideratum is specifically given in order that values can be represented using finite-range floating-point numbers [1, <ref type="bibr">Section 3]</ref>. Yet despite numerical stability being central to the theory underlying µP, this is not leveraged to ensure that µP models can actually be trained in low-precision. Indeed, for the LLM runs in Tensor Programs V the SP model trains successfully in FP16, while the µP model diverges (attributed to underflow of gradients). We remedy this with u-µP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Unit-Scaled Maximal Update Parametrization</head><p>In this section we show how µP can be adapted to satisfy Unit Scaling, and provide a new set of HPs which-thanks to Unit Scaling-are more interpretable and separable than those commonly used for µP, unlocking several practical benefits. For those wishing to apply u-µP to their own models, we provide a user-guide in Appendix C and an overview of our library implementing u-µP in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Combining µP with Unit Scaling</head><p>Whereas Unit Scaling provides rules for scaling all operations, µP only does so for parametrized ones. It's these operations we need to address to arrive at a unified scheme, resolving differences in the scaling rules each recommends. We begin with the expressions for the A W , B W , C W scaling factors in Equation ( <ref type="formula" target="#formula_5">3</ref>), and substitute in the µP scaling rules defined in Table <ref type="table">1</ref>. This results in a complete implementation of µP, which is shown in the top half of Table <ref type="table">2</ref> (using the extended set of µP HPs given in Table <ref type="table">3</ref>). We set out to turn this into a valid Unit Scaling scheme, which requires unit initializations (B W ← 1) and matmuls with the Unit Scaling factor we identified in Section 2.3</p><formula xml:id="formula_7">(A W ← 1/ √ fan-in).</formula><p>Table <ref type="table">2</ref>: The definition of u-µP along with an implementation of µP (assuming the extended HP set in Table <ref type="table">3</ref>). u-µP aims to simplify µP and provide the benefits of Unit Scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABC-multiplier</head><p>Weight Type Residual Input Hidden Output parameter (A W ) α emb 1 (or α attn ) α out base-fan-in fan-in base-depth depth * µP initialization (B W ) σ init σ init base-fan-in fan-in</p><formula xml:id="formula_8">σ init - Adam LR (C W ) η ηemb η base-fan-in fan-in η base-depth depth parameter † (A W ) 1 1 √ fan-in 1 fan-in ‡ 1 √ depth * u-µP initialization (B W ) 1 1 1 - Adam LR (C W ) η 1 √ fan-out η 1 √ fan-in η 1 √ depth * Residual</formula><p>multipliers are applied to the end of each branch, rather than the output of linear layers. † u-µP's α HPs are associated with operations, not weights, so are not included here (see Section 4.3). ‡ To maintain unit scale we apply 1/ √ fan-out scaling in the backward pass (see Appendix H).</p><p>Our first step is to drop the σ W and base-fan-in HPs entirely, and associate the α W HPs with certain functions instead of weights-decisions we justify in the rest of this section (this results in the simplified intermediate implementation in Table <ref type="table" target="#tab_22">11</ref>). Our input weights now have unit initializations as desired, and a unit parameter multiplier, which is also the appropriate scaling factor (as input layers here are embedding lookups, not matmuls).</p><p>Hidden weights now have the implementation:</p><formula xml:id="formula_9">A W ← 1, B W ← 1 √ fan-in , C W ← η 1 fan-in ,<label>(4)</label></formula><p>which differs from our Unit Scaling criteria. However, using the abc-symmetry outlined in Equation (2) we can shift scales by a factor of √ fan-in, arriving at a unit-scaled scheme:</p><formula xml:id="formula_10">A W ← 1 √ fan-in , B W ← 1, C W ← η 1 √ fan-in .<label>(5)</label></formula><p>Finally, our output layers also have unit initialization, but a parameter multiplier of A W ← 1/fan-in. This differs from the Unit Scaling rule, but in the forward pass this is permissible as there are no subsequent matmuls of a transformer. In the backward pass this mis-scaling would propagate, so we apply the desired ← 1/ √ fan-in factor. Using different forward and backward scales in this way is usually not allowed, but is valid for output layers due to the cut-edge rule (Appendix H).</p><p>The final change we make is to the input LR scaling rule, which we show in Section 4.4 is more effective if c W ← 1 is replaced with c W ← 1/ √ fan-out <ref type="foot" target="#foot_3">3</ref> . With these changes made, we arrive at our final u-µP scheme, given in Table <ref type="table">2</ref>. It's important to note that the scaling rules in this table must be combined with the standard Unit Scaling rules for other non-matmul operations. These are covered in Appendix B, and implemented in our library (see Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Out-of-the-box low-precision training</head><p>By applying the principles of Unit Scaling to µP, u-µP gains a key feature: the easy use of lowprecision number formats during training. We can attribute the difficulties µP has with low precision to the fact that it ignores constant factors (along with weight and gradient-scaling), only ensuring that activations are of order Θ(1). The stricter condition of unit scale across all tensors at initialization provides a way of leveraging µP's rules in order to make low-precision training work.</p><p>When training a transformer model with u-µP most scales in the model stabilize while certain tensors exhibit scale growth that potentially pushes them out of FP8 range. We empirically identify these critical tensors to be the inputs to the attention dense projection and final FFN matmul as well as the weight of the decoder head (for details see Appendix A.8). The latter becomes negligible in terms of model flops as width and depth of the model increase, so we generally keep this operation in higher precision.</p><p>Following these observations, we propose the following FP8 mixed precision scheme for u-µP transformer models:</p><p>• For non-critical matmul operations, we cast the input and weight to E4M3, and the gradient with respect to the output to E5M2. This is done in the forward computation, as well as the two backward computations (for the gradient w.r.t. the weight, respectively the input). Non-critical layers are query, key, value as well as the input layer(s) to the FFN. • All layers involving critical tensors, as well as embedding layer, residual addition and nonlinear functions are performed in higher precision. This also means that we directly aggregate into higher precision in each FP8 matmul. We keep optimizer states in FP32, as is usually the case in mixed precision training.</p><p>We note that in some cases one can deal with the critical tensors by casting them to E5M2 instead of E4M3, however we observed some instabilities applying this in a large scale setting, possibly due to loss of precision. In small scale scenarios we also empirically find that applying the E4M3 format instead of E5M2 for the gradients is possible, but becomes problematic in a more realistic setting where gradients require a higher dynamic range.</p><p>With our proposed mixed precision scheme, about 70% of the matmul computations in the transformer block are performed natively in FP8 (assuming a standard architecture, e.g. Llama). If desired, a dynamic per-tensor scaling could still be applied to the critical tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A principled approach to hyperparameters</head><p>We saw in Section 3.2 that approaches for selecting which HPs to sweep are poorly motivated in the literature. Our objective in u-µP is to find a simple, well-justified and effective alternative. To this end, we propose the following ideal criteria:</p><p>1. Minimal cardinality: the use of as few HPs as possible.</p><p>2. Maximal expressivity: the ability to still express any model defined using the per-tensor α W , σ W , η W HPs outlined in Section 2.1 (in practice, we relax this slightly).</p><p>3. Minimal interdependency: the optimal value of each HP should not depend on the value of other HPs, simplifying the search space. 4. Interpretability: there should be a clear explanation for what an HP's value 'means' in the context of the model.</p><p>Table 3: Typical transformer HPs used under different schemes. Basic HPs in bold are considered most impactful and are commonly swept. Extended HPs in non-bold are not always swept, often set heuristically or dropped. SP µP u-µP η η η σ-scheme σ init α emb |η emb α ffn-act α attn α attn-softmax α out α res base-width α res-attn-ratio base-depth α loss-softmax</p><p>The u-µP HPs given in Table <ref type="table">3</ref> are designed to satisfy these criteria, to the fullest extent possible. The placement of these HPs in the model is given in Table <ref type="table">8</ref>.</p><p>Cardinality &amp; expressivity We arrive at our set of HPs in three steps, starting with the full α W , σ W , η W for each weight tensor W . Firstly, we can choose to 'drop' any one of these three HPs by permuting under abc-symmetry, such that one HP = 1. As we want our weights to begin with unit scale, we choose σ W (i.e. θ = σ W in Equation ( <ref type="formula" target="#formula_3">2</ref>)), leaving just α W , η W .</p><p>Secondly, we observe that several of the α W HPs combine linearly with other α W HPs, providing an opportunity to re-parametrize with a single HP. For instance, we noted in Section 3 that the scale of selfattention softmax activations is proportional to the product of σ W multipliers, and the same is true for α W multipliers: std(x attn ) ∝ α WQ α WK . In this instance it appears more natural to use a single α parameter and associate it with the attention operation, rather than the weights. We term this α attn-softmax .</p><p>We apply the same principle to the rest of the model, associating α HPs with operations instead of weights. This applies to all operations, unless they are unary and k-homogeneous for k ≥ 0, in which case they propagate scale and don't require an HP (see Appendix G.1). This results in the set of HPs shown, with their placement in the model given in Table <ref type="table">8</ref>.</p><p>Thirdly, we use a single global η and group α HPs across layers. This breaks our expressivity criterion, but we argue represents the best trade-off between expressivity and cardinality. We show in Appendix A.4 that having tuned a global η HP and our extended α HPs, the further benefits of tuning per-tensor ηW HPs (which modify the global η) is minimal, justifying our decision to only use one global η.</p><p>Interdependency The second stage above, moving α HPs from weights into subsequent operations, not only reduces the number of HPs, but also minimizes the interdependence between those that remain. Interactions between HPs are complex and unlikely to be entirely separable, but we find that u-µP's optimal HP values depend less on each other than under µP (see Section 5.2).</p><p>Interpretability The combination of unit scale and reduced dependencies between HPs means that each α can be interpreted as determining some fundamental property of the model at initialization. For example, the α loss-softmax HP defines the (inverse of) the softmax's temperature for a unit-scaled input. We also introduce a new scaling scheme (defined in Appendix G.2.2) for residual connections, designed to give HPs independence and a clear interpretation: α res defines the contribution of the residual connections to the output scale, and α res-attn-ratio defines the relative contribution of attention versus FFN branches. Finally, we choose not to include base shape HPs in u-µP. They do not add to expressivity, lack a clear interpretation (besides alignment to a base model at a particular shape), break the interpretations of other HPs (as given above), and complicate implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">A new embedding LR rule</head><p>Although theoretical transfer properties have been proved for µP, not all its HPs have had µTransfer shown empirically. We do so for the extended µP transformer HPs in Figure <ref type="figure" target="#fig_5">17</ref>, where we observe poor transfer across width for the embedding LR multiplier ηemb . The associated scaling rule for the embedding LR is constant in width (c emb = 1), but this poor multiplier transfer suggests the rule is mis-specified. We show in Figure <ref type="figure" target="#fig_1">3</ref> (left) that a more effective rule is</p><formula xml:id="formula_11">c emb = 1/ √ fan-out.</formula><p>This keeps the optimal value of ηemb the same regardless of width. Figure <ref type="figure" target="#fig_1">3</ref> (right) shows that a constant scaling rule leads to diminishing returns as width increases, whereas our new rule continues to work well at scale, attaining the same loss at 2048-width that constant scaling attains at 4096-width. Our adoption of this change is a key factor in the improved performance of u-µP over µP in Figure <ref type="figure">1</ref>.</p><p>We offer no theoretical justification for our rule, which we leave to further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyperparameter search</head><p>As shown in section Section 2.1, the standard approach to HP search for µTransfer is via a random sweep over all HPs simultaneously. Sweeping individual HPs separately is challenging due to the dependencies between them. In contrast, u-µP's HPs are designed to admit such a strategy due to our interdependence criterion. Because of this, we propose a simpler sweeping strategy for u-µP which we term independent search (outlined in detail in Appendix A.6).</p><p>Independent search involves a sweep of the LR, followed by a set of one-dimensional sweeps of the other HPs (which can be run in parallel). The best results from the individual sweeps are combined to form the final set of HP values. We also consider an even simpler scheme, which only sweeps the LR, leaving other HP values at 1 (i.e. dropping them). For caution, we recommend the full approach, but in practice we find that only sweeping the LR is surprisingly effective, as shown in Figure <ref type="figure">1</ref> (a). This indicates that not only is the principle of unit scale good for numerics, but also for learning dynamics where it provides near-optimal scaling.</p><p>2 4 2 6 2 8 C emb 2.8 3.0 3.2 3.4 3.6 Validation Loss 2 -12 2 -10 2 -8 2 -6 2 -4 Learning Rate Proxy Optimum Proposed Transfer Rule cemb ← 1/ √ fan-out µP Transfer Rule cemb ← 1 Recall: wt+1 = wt + C emb • Φt(∇L0, . . . , ∇Lt), C emb ← η emb • c emb /c emb-base Improved Optimum Transfer Width 128 (BaseWidth) 256 512 1024 2048 4096 Embedding LR scale </p><formula xml:id="formula_12">c emb ← 1/ √ fan-out c emb ← 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Our experiments all use the Llama <ref type="bibr" target="#b19">[21]</ref> architecture trained on WikiText-103 <ref type="bibr" target="#b20">[22]</ref> (excepting the large-scale runs in Section 5.5). We apply current best-practice LLM training techniques from the literature (full settings are given in Table <ref type="table" target="#tab_12">5</ref>). In accordance with our analysis of settings for µTransfer in Section 3.1, we remove parameters from norm layers, use independent AdamW, and avoid training on too many epochs for both u-µP and µP for the sake of fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantifying hyperparameter interdependence</head><p>Our principled approach to HPs (Section 4.3) contains the requirement that their optimal values should depend minimally on the value of other HPs. We now investigate this empirically, conducting a 2D sweep over every pair of HPs for µP and u-µP, shown in Figures <ref type="figure" target="#fig_2">14</ref> and <ref type="figure" target="#fig_14">15</ref> respectively.</p><p>To derive an empirical measure of HP dependency, we introduce the notion of transfer error (see Algorithm 1). This considers a pair of HPs, with one 'fixed' and the other for 'transfer'. We take the best value of the transfer HP for each non-optimal value of the fixed HP, and use it with the optimal value of the fixed HP. The transfer error is the difference between the losses obtained and the minimum loss. Figure <ref type="figure" target="#fig_2">4</ref> shows this measure for each pair of HPs under µP and u-µP, reflecting the improvement in HP dependency as a result of our scheme. This gives u-µP a reduced risk of small transfer errors leading to large degradations, and the potential to sweep HPs in a more separable way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyperparameter search</head><p>We now leverage this improved separability of HPs for the purpose of efficient sweeping. In Figure <ref type="figure">1</ref> (a) we conduct a standard random search for µP and u-µP, along with the independent search outlined in Section 4.5 (and Appendix A.6). We observe the following:</p><p>1. For u-µP the LR-sweep phase of independent search alone is sufficient to reach near-optimal loss (totaling 9 runs). During this phase other HPs are fixed at 1, which for u-µP means that the inputs to operations are generally unit-scaled.</p><p>2. Consequently, we conclude that unit scale at initialization is close to the ideal scaling for effective learning here. This is not a property we asserted a priori, nor do we argue that it necessarily holds for other training setups and models; hence why we still provide a set of extended HPs to be swept. 3. In contrast µP still requires non-LR HPs to be swept to attain a reasonable loss. Unlike u-µP, fixing HPs at 1 results in arbitrarily-scaled inputs, which appear to result in worse training. 4. The 'combined mults' phase causes the loss to spike for µP. This is due to the HP dependencies shown in Figure <ref type="figure" target="#fig_2">4</ref>, which mean HPs cannot be swept independently and used together.</p><p>Conversely, lower dependence means this can be done for u-µP, making random search unnecessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Hyperparameter transfer</head><p>We train many models and plot transfer of LR across width (Figure <ref type="figure">1</ref> (b)), steps, batch size and depth (Figure <ref type="figure" target="#fig_3">5</ref>), and transfer of other HPs across width (Figure <ref type="figure" target="#fig_5">17</ref>). Note that u-µP (building on µP) is designed to give transfer over width <ref type="foot" target="#foot_4">4</ref> ; the other axes we report for practical purposes. We find that:</p><p>1. The optimal LR is constant across width under u-µP. There is a small drift for training steps and batch size, and a larger one with depth. Hence we recommend proxy models which primarily differ in width, moderately in steps and batch size, and least in depth. 2. The optimal LR is also approximately constant for training steps, batch size and depth. This means we can scale our proxy model down across all these axes and maintain LR transfer. Of these, width appears the most stable and depth the least. 3. Whereas µP sees diminishing returns for larger widths, u-µP continues to benefit from width, with the 2048 u-µP model matching the 4096 µP model. We attribute this primarily to our improved embedding LR rule. 4. Non-LR HPs also have approximately constant optima across width under u-µP. This is not true for µP, where ηemb has poor transfer due to the embedding scaling rule issue identified in Section 4.4, along with σ init which in Section 3.2 we argue should not be grouped across all weights (and drop from the u-µP HP scheme). 5. The optimal values found for non-LR HPs are all close to 1. In practice this means that dropping these HPs entirely is potentially viable for similar models and training setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">FP8 training</head><p>In this section we justify the simple mixed-precision scheme described in Section 4.2 and demonstrate that it can be used to train u-µP models out-of-the-box.</p><p>2 -10 2 -8 2 -6 Learning Rate 3.0 3.2 3.4 3.6 3.8 Validation Loss Training Steps 4096 8192 16384 32768 2 -10 2 -8 2 -6 Learning Rate µP Batch Size 32 64 128 256 2 -10 2 -8 2 -6 Learning Rate Depth 1 2 4 8 16 2 -2 2 0 2 2 2 4 Learning Rate 3.0 3.2 3.4 3.6 3.8 Validation Loss Training Steps 4096 8192 16384 32768 2 -2 2 0 2 2 2 4 Learning Rate u-µP Batch Size 32 64 128 256 2 -2 2 0 2 2 2 4 Learning Rate Depth 1 2 4 8 16 activation E4M3 E5M2 E4M3 weight grad 2 -24 2 -16 2 -8 2 0 2 8 RMS µP u-µP activation E4M3 E5M2 E4M3 weight grad 2 -24 2 -16 2 -8 2 0 2 8 RMS µP u-µP Proof-of-concept Figure <ref type="figure" target="#fig_4">6</ref> shows the RMS of all linear layer inputs for a moderately sized transformer. RMS captures the larger of the mean and scale of a distribution, and as such is a good test of whether a tensor is likely to suffer over/underflow in low-precision. We observe that u-µP tensors largely have RMS starting close to 1 and remaining so at the end of training, supporting our scheme.</p><p>Figure <ref type="figure" target="#fig_16">19</ref> demonstrates the scale-growth of critical tensors which our scheme is designed to accommodate, showing RMS on a per-tensor basis over steps. Figure <ref type="figure" target="#fig_17">20</ref> provides further insight into this issue, showing the effect of LR, width, depth, steps and batch size on the RMS of critical tensors.</p><p>As an initial proof-of-concept we train a u-µP model using our FP8 scheme over 8k steps, using HPs from a proxy model, as shown in Figure <ref type="figure">1</ref> (c). We see only a small degradation versus FP32, and at this scale critical tensors can still be cast to FP8 using E5M2, while gradients can even use E4M3.</p><p>0K 20K 40K 60K Training Step 1.9 2.0 2.1 2.2 2.3 2.4 Training Loss 1B 3B 7B BF16 FP8 0K 20K 40K 60K Training Step 1.9 2.0 2.1 2.2 2.3 2.4 Training Loss 1B 3B 7B u-µP SP Table 4: 0-shot benchmark results at 7B scale. Scheme Format MMLU HellaSwag OpenBook QA PIQA TriviaQA WinoGr SP BF16 29.6 52.4 27.8 76.5 22.2 63.3 u-µP BF16 29.0 53.4 31.6 77.1 23.4 63.7 u-µP FP8 31.2 53.4 29.6 77.6 21.3 65.7</p><p>Larger scale Next we consider a more realistic training scenario <ref type="foot" target="#foot_5">5</ref> . Using the same architecture, and following the steps set out in our u-µP user-guide (Appendix C), we train our target models on 300B tokens of the SlimPajama dataset <ref type="bibr" target="#b21">[23]</ref> (see Appendix A.9 for training details).</p><p>We begin with an independent search (Section 4.5) over our u-µP proxy model's HPs. Here we make the following observations:</p><p>1. When using a relatively small proxy model (8 layers and 512 width), the HP-loss landscape is rather noisy. By doubling the width we can discern optimal HP values more clearly. 2. The most important HPs are η and α res-attn-ratio . All others can be left at the default of 1.</p><p>3. The optimal values of these HPs are η = 2 3.5 and α res-attn-ratio = 2 -2.0 and thus differ non-trivially from the observed HPs in our smaller-scale experiments.</p><p>We then train u-µP models of approximately 1B, 3B and 7B parameters, using our FP8 mixedprecision scheme (see Section 4.2). We also train two baselines at each size: the first is a BF16 version of our u-µP models, and the second is a set of SP models using the weight init scheme from the Pythia model family <ref type="bibr" target="#b22">[24]</ref> and the LR scheme from Llama 3 <ref type="bibr" target="#b23">[25]</ref>, scaling inversely with width and using a LR of 3e-4 at 7B scale. The loss curves are shown in Figure <ref type="figure" target="#fig_5">7</ref>. All FP8 runs converge and show no significant loss degradation. In comparison to SP, the u-µP models have a qualitatively different training curve with a higher loss for most of training that catches up in latter stages, hinting at a fundamentally different optimization trajectory. In terms of downstream performance, both of the u-µP 7B models are competitive with SP. In particular, the scores of the FP8 model are mostly on par with the BF16 models (see Table <ref type="table">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Low-precision training Techniques introduced to facilitate FP8 training include those covered in Appendix J and more <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28]</ref>. These largely concern the quantizing of activations, weights and gradients, though <ref type="bibr" target="#b27">[29]</ref> also explore FP8 optimizer states and cross-device communication, which we consider interesting avenues of further exploration. Recently, stable training has been demonstrated for the MX family of formats which use a shared block-exponent <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b29">31]</ref>, and even for the ternary BitNet format <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b32">34]</ref>. Again, we consider these formats for follow-up work.</p><p>Stability features Another recent research trend has been the analysis of features that contribute to (or resolve) numerical and algorithmic instability. <ref type="bibr" target="#b16">[18]</ref> show that unstable training dynamics can result from attention logit growth (fixed by QK-norm <ref type="bibr" target="#b33">[35]</ref>) and from divergent output logits (fixed by z-loss <ref type="bibr" target="#b34">[36]</ref>). <ref type="bibr" target="#b35">[37]</ref> find large feature magnitudes can be avoided by zero-initialization, and loss spikes avoided via a modified AdamW, specifically for low-precision training. <ref type="bibr" target="#b36">[38]</ref> investigate how pre-training settings affect instabilities revealed during post-training quantization. <ref type="bibr" target="#b37">[39]</ref> apply a similar philosophy to Unit Scaling for the training of diffusion models, to address uncontrolled magnitude changes. Extreme activation values seen in large models <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b39">41]</ref> have been addressed by softmax-clipping <ref type="bibr" target="#b40">[42]</ref>, and by the addition of extra terms <ref type="bibr" target="#b41">[43]</ref> or tokens <ref type="bibr" target="#b42">[44]</ref> to bias the attention computation. We do not adopt these features in our experiments to avoid confounding effects, but we expect them to benefit u-µP and hope to explore their usage.</p><p>Learning dynamics Several recent efforts have tried to improve µP from different angles. <ref type="bibr" target="#b43">[45]</ref> introduces the notion of the modular norm over the full weight-space, which like µP aims to ensure stable updates that provide LR transfer, and like u-µP is implemented via modules designed to ensure stable training. Challenging the assumptions underpinning µP, <ref type="bibr" target="#b44">[46]</ref> explores the notion of alignment between parameters and data, demonstrating that other parametrizations with per-layer learning rates can outperform standard µP. We consider comparing these parametrizations against u-µP and trying unit-scaled versions valuable future work. Recent applications of µP to the problems of weight sparsity <ref type="bibr" target="#b45">[47]</ref> and structured matrices <ref type="bibr" target="#b46">[48]</ref> are also interesting candidates for u-µP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We introduce u-µP, a modified and improved version of µP that satisfies Unit Scaling. Through careful analysis guided by first principles we identify an interpretable set of HPs that has minimal interdependencies and facilitates an efficient independent sweeping strategy. We show that the stability properties of µP combined with Unit Scaling enable a simple and robust FP8 mixed precision scheme that works in a realistic large scale training scenario. u-µP provides further evidence that the principle of Unit Scaling is beneficial for model design.</p><p>Limitations and future work Some choices like the modified embedding LR rule are only justified by empirical observations, and currently lack a theoretical explanation. Additionally, neither µP nor Unit Scaling give guarantees for network quantities to be well-behaved over the course of training.</p><p>In particular we would like to understand the issue (or feature) of scale growth in the critical layers better and look into possible mitigations. We also believe that low-precision training techniques can be pushed further, with u-µP offering an ideal starting point for future optimizations.</p><p>Contents 1 Introduction 1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Background 2.1 The Maximal Update Parametrization . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Low-precision training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Unit Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 The challenges with µP in practice 3.1 Not all training setups give µTransfer . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 It's not clear which hyperparameters to sweep . . . . . . . . . . . . . . . . . . . . 3.3 Base shape complicates usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 µP appears to struggle with low-precision . . . . . . . . . . . . . . . . . . . . . . 4 The Unit-Scaled Maximal Update Parametrization 4.1 Combining µP with Unit Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Out-of-the-box low-precision training . . . . . . . . . . . . . . . . . . . . . . . . 4.3 A principled approach to hyperparameters . . . . . . . . . . . . . . . . . . . . . . 4.4 A new embedding LR rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Experiments 5.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Quantifying hyperparameter interdependence . . . . . . . . . . . . . . . . . . . . 5.3 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Hyperparameter transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 FP8 training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Related Work 7 Conclusions 8 Acknowledgments A Additional experimental details A.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Further analysis of µTransfer failure modes . . . . . . . . . . . . . . . . . . . . . A.3 Validating our experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1 Repeated data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.2 Warmup duration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3 Learning rate decay target . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Per-tensor learning rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Hyperparameter independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Hyperparameter transfer experiments . . . . . . . . . . . . . . . . . . . . . . . . . A.8 Numerical properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.9 Large-scale training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B Unit-scaled op definitions C A guide to using u-µP D A guide to the unit scaling library D.1 Standard usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Extending the library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 As a reference implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . E Additional background material E.1 The Maximal Update Parametrization . . . . . . . . . . . . . . . . . . . . . . . . E.2 Unit Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F Unit-scaled pre-norm residual layers F.1 Scale growth in pre-norm residual networks . . . . . . . . . . . . . . . . . . . . . F.2 Residual symmetry in pre-norm architectures . . . . . . . . . . . . . . . . . . . . F.3 Proof of Lemma F.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Unit Scaling for transformer residuals . . . . . . . . . . . . . . . . . . . . . . . . G Justifying the u-µP hyperparameter scheme G.1 Multipliers for non-homogeneous ops: α attn-softmax , α ffn-act , α loss-softmax . . . . G.2 Residual branch multipliers: α res , α res-attn-ratio . . . . . . . . . . . . . . . . . . . G.2.1 Improved hyperparameters for transformer residuals . . . . . . . . . . . . G.2.2 The full u-µP residual scheme . . . . . . . . . . . . . . . . . . . . . . . . H The cut-edge rule I From µP to u-µP J Low-precision and its trade-offs K Benchmarking scaled matrix multiplication implementation in PyTorch L Attention output RMS grows with model depth A Additional experimental details A.1 Experimental Setup</p><p>Our experimental analysis of u-µP was conducted by adapting the codebase used for Tensor Programs V, allowing us to compare µP and u-µP in the same setting. We change various experimental settings from the original paper to make our experiments better reflect standard training procedures, particularly the dataset which we switch from WikiText-2 to the larger WikiText-103 <ref type="bibr" target="#b20">[22]</ref>. Where not specified otherwise, the default setting used in our experiments are given in Table <ref type="table" target="#tab_12">5</ref>. These also represent the settings of our proxy model.</p><p>Dataset WikiText-103 [22] Sequence length 256 Vocab size 32000 Training set tokens 138M Architecture Llama [21] (Transformer, PreNorm, RMSNorm, SwiGLU, RoPE, "untied" embeddings), non-trainable RMSNorm parameters. Width 256 (scaled up to 4096) Depth 4 Number of heads 4 (scaled up to 64) Head dimension 64 Total parameters 19.5M (scaled up to 1.07B) Batch size 64 Training steps 8192 (0.97 epochs) LR schedule Cosine to 10%, 2000 steps warm-up</p><p>Optimizer AdamW (β 1 , β 2 , ϵ) = (0.9, 0.999, 10 -8 ) Weight decay 2 -13 , independent <ref type="bibr" target="#b47">[49]</ref> Dropout 0.0</p><formula xml:id="formula_13">µP HP search range η ∈ [2 -10 , 2 -6 ] ηemb ∈ [2 0 , 2 8 ] σ init , α emb , α attn , α output ∈ [2 -2 , 2 2 ] u-µP HP search range η ∈ [2 -1 , 2 3 ] α attn ∈ [2 -2 , 2 2 ] α residual , α residual-attn-ratio , α ffn-act , α output ∈ [2 -3 , 2 3 ]</formula><p>µP HP defaults σ init = α emb = α attn = α output = ηemb = 1 u-µP HP defaults α residual = α residual-attn-ratio = α ffn-act = α output = α attn = 1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Further analysis of µTransfer failure modes</head><p>In Table <ref type="table" target="#tab_13">6</ref> we provide exact details of the experimental differences between setups (a) and (b) from Figure <ref type="figure" target="#fig_0">2</ref>, for readers wishing to understand and reproduce this result. We also provide a step-by-step ablation of the various changes made between these setups in Figure <ref type="figure" target="#fig_7">9</ref>.</p><p>For setup (c), which shows how our two combined stability fixes mitigate the problem of poor transfer, both changes are evaluated independently in Figure <ref type="figure" target="#fig_6">8</ref>, which shows that the dominant effect is a narrowing of the learning basin due to non-parametric RMSNorms, leading to better learning rate transfer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Validating our experimental setup</head><p>In this section we run a series of ablations to validate decisions relating to our experimental setup given above. In particular, we examine the effect of using repeated data, the effect of using a shorter warmup duration, and the effect of different final learning rates at the end of decay. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Repeated data</head><p>As outlined in Table <ref type="table" target="#tab_12">5</ref>, our standard training setup uses 0.97 epochs of the WikiText-103 dataset (50x larger than the WikiText-2 dataset used in Tensor Programs V). However on our batch size and training steps scaling experiments in Figure <ref type="figure" target="#fig_3">5</ref> we train on up to 4× the amount of data than in our standard setup, and hence use up to 4 epochs.</p><p>Though this is still a small level of repeated data, this moves our training slightly into the over-fitting regime. Based on this change, we here investigate the hypothesis that this regime has better or worse transfer of the optimal LR than the non-overfitting regime, and hence our results could be misleading.</p><p>To do so, we repeated these experiments with the same number of tokens, but using the much larger SlimPajama dataset <ref type="bibr" target="#b21">[23]</ref> where we use &lt; 1 epoch.</p><p>The results for this experiment are seen in Figure <ref type="figure" target="#fig_8">10</ref>. The shape of curves is very similar across the two datasets, for both batch size and training steps (albeit with a higher loss, due to the more varied nature of SlimPajama). From this we conclude that the effect of repeated data from our use of WikiText-103 is not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Warmup duration</head><p>For our experimental setup (Table <ref type="table" target="#tab_12">5</ref>) we use a longer duration of warmup than in our large-scale setup (Table <ref type="table">7</ref>). We do so out of caution, as we use fewer tokens-per-batch for the smaller-scale experiments and so may require longer warmup. However, doing so also creates the risk of spending too large a proportion of training doing warmup, which could affect transfer.</p><p>To investigate this effect, we run two experiments. Firstly, we re-run the experiment for LR transfer over training steps, shown in Figure <ref type="figure" target="#fig_3">5</ref>, on a quarter of the warmup steps. This is shown in Figure <ref type="figure">11</ref> (left). The main effect appears to be higher loss for larger learning rates, but the optima are largely unchanged. The only exception is the 4096-step run, where the optimum shifts left and the loss improves slightly. This appears to now align the optimum better with the other training durations, but leads to narrower basins as a result, suggesting a trade-off for this particular experiment.</p><p>However, all our other experimental runs use the 8192-step configuration, which has a consistent optimum regardless of warmup duration here. To investigate the effect of reduced warmup on width transfer at this particular step-count, we re-run our experiment in Figure <ref type="figure">1</ref> (b) under the shorter warmup duration, shown in Figure <ref type="figure">11</ref> (right). The only significant impact of this change is to narrow the basins, inducing no significant change in the optimal LR. As such, we conclude that using 2000</p><p>2 -1 2 1 2 3 2 5 Learning Rate 3.00 3.25 3.50 3.75 4.00 4.25 4.50 Validation Loss wikitext-103 2 -1 2 1 2 3 2 5 Learning Rate 3.75 4.00 4.25 4.50 4.75 5.00 SlimPajama Batch Size 32 64 128 256 2 -1 2 1 2 3 2 5 Learning Rate 3.00 3.25 3.50 3.75 4.00 4.25 4.50 Validation Loss wikitext-103 2 -1 2 1 2 3 2 5 Learning Rate 3.75 4.00 4.25 4.50 4.75 5.00 SlimPajama Training Steps 4096 8192 16384 32768  <ref type="figure" target="#fig_3">5</ref>, but using the larger SlimPajama dataset where no data is repeated. In both settings our validation loss basins take the same shape, indicating that our analysis using the WikiText-103 dataset holds.</p><p>steps of warmup in our experimental setup is a reasonable choice, and both give the same width transfer.</p><formula xml:id="formula_14">2 -2 2 -1 2 0 2 1 2 2 2 3 2 4 2 5</formula><p>Learning Rate</p><p>3.0 3.2 3.4 3.6 3.8 Validation Loss Training Steps 4096 8192 16384 32768 Warmup Steps 2000 500 2 -3 2 -2 2 -1 2 0 2 1 2 2 2 3 2 4 2 5 Learning Rate 2.8 3.0 3.2 3.4 3.6 Validation Loss Width 128 256 512 1024 2048 4096 Warmup Steps 2000 500 Figure 11: (Left) Learning rate transfer across training steps under different numbers of warmup steps. (Right) Learning rate transfer across width under different numbers of warmup steps. In this setting (training steps = 8192) the optimal LR is consistent, meaning either warmup regime can be used, though the longer gives wider basins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Learning rate decay target</head><p>In all our experiments we use a cosine decay of our learning rate down to 10% of the maximum. This follows the standard approach taken by most LLM training projects <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b50">52]</ref>. However, recent research has indicated that this may not be the optimal decay target, with implications for LR transfer. <ref type="bibr" target="#b51">[53]</ref> show that the choice of target percentage can alter the shape of transfer curves and potentially shift the optimum value (Figure <ref type="figure" target="#fig_19">21</ref>, right). They also suggest that using a fixed target value may work better than a percentage (Figure <ref type="figure" target="#fig_20">22</ref>, right), which could be swept separately. <ref type="bibr" target="#b52">[54]</ref> separately suggest that linear decay to zero is the most effective scheme.</p><p>Though using the optimal decay scheme is not necessarily essential to the validity of our method, any implications of different schemes on transfer properties should be investigated. To do so, we run two experiments. The first sweeps the learning rate for our standard model at various percentages and fixed values of cosine decay target, including zero, in Figure <ref type="figure" target="#fig_11">12</ref> (left). Lower decay targets perform better here, including zero, suggesting that this simple rule may be ideal.</p><p>We then re-run our width transfer experiment from Figure <ref type="figure">1</ref> (b) but with our LR decaying to 0, and plot the result in Figure <ref type="figure" target="#fig_11">12</ref> (right). This leads to slightly better results for large learning rates, though for large models this difference diminishes. Fortunately the effect this decay target has on the shape of curves (and hence optimal LR transfer) is minimal, indicating that our conclusions are not effected significantly by the choice of decay target.</p><formula xml:id="formula_15">2 -2 2 -1 2 0 2 1 2 2 2 3 2 4 2 5</formula><p>Learning Rate</p><p>3.0 3.2 3.4 3.6 3.8 Validation Loss Min LR 0 10 -7 10 -4 0.1% 1% 10% 100% 2 -3 2 -2 2 -1 2 0 2 1 2 2 2 3 2 4 2 5 Learning Rate 2.8 3.0 3.2 3.4 3.6 Validation Loss Width 128 256 512 1024 2048 4096 Final LR ratio 0.1 0.0 Figure 12: (Left) A learning rate sweep over LR targets of different types (percentage, fixed and zero) on our standard model. (Right) Using the zero and 10% learning rate targets, LR transfer over width.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Per-tensor learning rates</head><p>In Section 4.3 we relax the requirement for each weight tensor in the u-µP model to have an associated tuneable learning-rate multiplier on top of the global learning rate. Whilst this does reduce the theoretical expressivity of the u-µP scheme, Figure <ref type="figure" target="#fig_1">13</ref> shows that using a single globally optimized learning rate is already at or close to the optimal choice for all weight tensors, and therefore it is reasonable to drop these multipliers in favor of reducing the number of HPs. However, a practitioner attempting to absolutely maximize the task performance of their model could experiment with tuning a few key per-tensor LRs, in particular the embedding table. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation Loss</head><p>Self Attention Out FFN Linear 1 FFN Linear 2</p><formula xml:id="formula_16">2 -4 2 -2 2 0 2 2 2 4</formula><p>3.20</p><p>3.25</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.30</head><p>FFN SwiGLU</p><formula xml:id="formula_17">2 -4 2 -2 2 0 2 2 2 4</formula><p>Per-Tensor LR Multipliers η W Embedding</p><formula xml:id="formula_18">2 -4 2 -2 2 0 2 2 2 4</formula><p>Unembedding Figure <ref type="figure" target="#fig_1">13</ref>: Independently varying per-tensor learning rate multipliers η W , using the u-µP model of width 256 from Figure <ref type="figure">1</ref> with optimized global learning rate 2 1.5 as the starting point. Where applicable, the same multiplier is used for tensors of the same name across transformer layers. Each subplot fixes all but one multiplier at 1, therefore the midpoint of each subplot is precisely the u-µP 256 model from Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Hyperparameter independence</head><p>In Section 5.2 we explore the question of HP independence under µP and u-µP. The following plots in Figures <ref type="figure" target="#fig_2">14</ref> and <ref type="figure" target="#fig_14">15</ref> show the result of a 2D sweep over every pair of HPs under each scheme. All other HPs are held at 1 when not swept, except the η which is held at 2 -7.5 for µP and 2 1.5 for u-µP, and ηemb which is held at 2 4 for µP.</p><p>These results show visual dependence between µP hyperparameters as a diagonal structure in the grids, such as (η emb , σ init ) and (η, α attn ). We quantify this in the plot in Figure <ref type="figure" target="#fig_2">4</ref>, where we use a measure of HP dependence termed transfer error. This is explained verbally in Section 5.2, and we provide an algorithmic description in Algorithm 1. We note that differences in transfer error between the two methods may also be influenced by the flatness of the optimum. The HP and loss values used for our transfer error calculations are those in Figures 14 and 15 2 -5.5</p><formula xml:id="formula_19">2 -3.5 η 2 0 2 2 2 4 2 6 2 8 ηemb 2 -4 2 -2 2 0 2 2 2 4 σinit 2 -4 2 -2 2 0 2 2 2 4 αemb 2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4 αoutput -12</formula><p>-9.5 -7.5  </p><formula xml:id="formula_20">η 2 -4 2 -2 2 0 2 2 2 4 αoutput 2 0 2 2 2 4 2 6 2 8 ηemb 2 -4 2 -2 2 0 2 2 2 4 σinit 2 -4 2 -2 2 0 2 2 2 4 αemb 2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4 αoutput -4</formula><formula xml:id="formula_21">err ← 0 f * , t * ← argmin(L) for f in F do if f ̸ = f * then t ← argmin(L(f )) err += L(f * , t) -L(f * , t * ) end if end for return err/(n -1) 2 -2.5 2 -0.5 2 1.5 2 3.5 2 5.5 η 2 -2.5 2 -0.5 2 1.5 2 3.5 2 5.5 η 2 -4 2 -2 2 0 2 2 2 4 αres 2 -4 2 -2 2 0 2 2 2 4</formula><p>αres-attn-ratio</p><formula xml:id="formula_22">2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4</formula><p>αffn-act</p><formula xml:id="formula_23">2 -4 2 -2 2 0 2 2 2 4</formula><p>αoutput -2.5 -0.5</p><p>1.5</p><p>3.5</p><formula xml:id="formula_24">5.5 η 2 -4 2 -2 2 0 2 2 2 4 αres -4 -2 0 2 4 αres 2 -4 2 -2 2 0 2 2 2 4</formula><p>αres-attn-ratio αffn-act</p><formula xml:id="formula_25">2 -2.5 2 -0.5 2 1.5 2 3.5 2 5.5 η 2 -4 2 -2 2 0 2 2 2 4 αoutput 2 -4 2 -2 2 0 2 2 2 4 αres 2 -4 2 -2 2 0 2 2 2 4</formula><p>αres-attn-ratio</p><formula xml:id="formula_26">2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4</formula><p>αffn-act </p><formula xml:id="formula_27">2 -4 2 -2 2 0 2 2 2 4 αoutput -4 -2<label>0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Hyperparameter search</head><p>Here we outline the particular search processes used for our µP and u-µP HP sweeps in Figure <ref type="figure">1 (a)</ref>. The random search samples uniformly from a grid defined over all extended HPs (extended HP sets are defined in Table <ref type="table">3</ref>, with grid values defined in Table <ref type="table" target="#tab_12">5</ref>). We perform the random search over 339 runs, each of which is a full training of the width-256 proxy model. We then simulate the effect of shorter searches at various run-counts by taking a random sample of the results, resulting in the smooth curve over run-count shown.</p><p>The independent search consists of the following phases:</p><p>1. Perform a 1D line search for an optimal learning rate, with other hyperparameters set to their default values (9 runs). 2. For each hyperparameter in parallel, perform a 1D line search (330 runs). 3. Combine the best settings from step 2, and re-evaluate (6 runs).</p><p>The number of runs in the 1D line search is an order of magnitude higher than is required in practice. We do so to form a fair comparison with the random search, which benefits from this large number of runs. The number of runs for the 1D line search could be reduced further by using binary search, though this would require sequential runs and limit the extent of parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Hyperparameter transfer experiments LR transfer over width</head><p>The transfer experiments shown in Figure <ref type="figure">1</ref> (b) use the non-LR HPs found in Figure <ref type="figure">1</ref> (a) (indicated by the circled points), rather than using default HP values. For the u-µP sweep we take the HPs at the end of the LR portion of the independent search, as these are already close-to-optimal, and means only 9 runs were required in the sweep. In contrast, for µP it is necessary to use the results of the random search over a large number of runs.</p><p>LR transfer over other axes For the training steps, batch size and depth transfer experiments in Figure <ref type="figure" target="#fig_3">5</ref>, all HP values are fixed to 1 except LR which is swept. As with width transfer, u-µP outperforms µP here using these default HP values. Reducing training steps is done by fixing the number of warm-up steps (at 2000) and still cosine-decaying the learning rate to 10%; all that changes is the number of post-warm-up steps. We found this to be more effective than cutting-short the decay schedule. For both Figure <ref type="figure">1</ref> (b) and Figure <ref type="figure" target="#fig_3">5</ref> we sweep the LR over a logarithmically-spaced grid of step 2 1/2 ×, with 3 runs for each point.</p><p>Additionally, in Figure <ref type="figure" target="#fig_4">16</ref> we show learning rate transfer over sequence length for both µP and u-µP fixing either tokens per batch or sequences per batch. In both scenarios u-µP shows not only better absolute training performance, but also better transfer behaviour as sequence length increases. Since our default proxy sequence length is 256, using µP to transfer to sequence length 2048 would result in minimal improvements or even a degradation in validation loss, whereas the u-µP shows much greater and more consistent improvements.</p><p>Other HP transfer over width For our non-LR HP transfer results in Figure <ref type="figure" target="#fig_5">17</ref>, we note that good transfer under µP has not been demonstrated for all HPs used in the literature. This is particularly true for the ηemb HP, which has poor transfer under µP. Our investigation here led to our identification of the need to adjust the embedding LR scaling rule outlined in Section 4.4. In many cases users have not swept this HP, but instead swept the corresponding parameter multiplier α emb . How this HP interacts with the embedding LR scaling problem identified (and our proposed fix) remains to be explored, though we note in Figure <ref type="figure" target="#fig_5">17</ref> that it also appears to have poor transfer.</p><p>Combined HP transfer Whilst Figure <ref type="figure" target="#fig_5">17</ref> demonstrates the transfer of individual hyperparameters over width, Figure <ref type="figure" target="#fig_6">18</ref> instead demonstrates the simultaneous transfer of all hyperparameters when co-optimized on the small-scale proxy model, as is done for µTransfer. The µP and u-µP points are taken from Figure <ref type="figure">1</ref>, with hyperparameters swept on a model of width 256 using a full random HP search and a simple learning rate sweep for µP and u-µP respectively. The Standard Parametrization scheme, as shown in Table <ref type="table">3</ref> requires choosing a learning rate and a weight-initialization scheme. We follow the initialization scheme of Pythia <ref type="bibr" target="#b22">[24]</ref>, and transfer learning rate using a heuristic scaling factor of base-width /width, as is done in <ref type="bibr" target="#b23">[25]</ref>. 2 -8 2 -6 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 Validation Loss 2 -2 2 0 2 2 2 4 Learning Rate Sequence Length 64 128 256 512 1024 2048 Fix tokens/batch = 16K Fix sequences/batch = 64 Min Figure 16: Transfer of learning rate over sequence length for µP (left) and u-µP (right). As sequence length varies, we can fix the number of tokens per batch by inversely varying the number of sequences per batch (top). Alternatively we can fix the sequences per batch and allow the number of tokens per batch to vary with sequence length (bottom). In the latter case, larger sequence lengths mean the model sees more tokens during training, though as per Table 5 this translates to &gt;1 epoch on WikiText-103 when sequence length goes above 256. 2 -12 2 -10 2 -8 2 -6 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 Training Loss 2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4 αoutput 2 -4 2 -2 2 0 2 2 2 4 σinit 2.8 3.0 3.2 3.4 3.6 3.8 Training Loss 2 -4 2 -2 2 0 2 2 2 4 αemb 2 3 2 5 2 7 2 9 ηemb µP Width 128 256 512 1024 2048 4096 2 -3 2 -1 2 1 2 3 Learning Rate 2.8 3.0 3.2 3.4 3.6 3.8 Training Loss 2 -4 2 -2 2 0 2 2 2 4 αattn 2 -4 2 -2 2 0 2 2 2 4 αoutput 2 -4 2 -2 2 0 2 2 2 4 αresidual 2.8 3.0 3.2 3.4 3.6 3.8 Training Loss 2 -4 2 -2 2 0 2 2 2 4 αresidual-attn-ratio 2 -4 2 -2 2 0 2 2 2 4 αffn-act u-µP Width 128 256 512 1024 2048 4096 Figure 17: Transfer of model hyperparameters over width for µP (top) and u-µP (bottom). When one hyperparameter is being swept, all others are fixed at 1, with the exception of Learning Rate η = (2 1.5 , 2 -7.5 ) for (u-µP, µP). 2 8 2 9 2 10 2 11 2 12 Width 2.7 2.8 2.9 3.0 3.1 3.2 Validation Loss SP µP u-µP</p><p>Figure <ref type="figure" target="#fig_6">18</ref>: Transferring hyperparameters from width 256 up to 4096 using three different hyperparametrization schemes. µP and u-µP results are as seen in Figure <ref type="figure">1</ref>, whilst Standard Parametrization follows the initialization approach of Pythia <ref type="bibr" target="#b22">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Numerical properties</head><p>Our analysis of the numerical properties of u-µP focuses on the RMS of tensors that we wish to cast to FP8: linear module input activations, weights and output gradients. From the RMS training statistics plots in Figure <ref type="figure" target="#fig_4">6</ref> and Figure <ref type="figure" target="#fig_16">19</ref> we note that</p><p>1. µP has gradients and weights with low RMS, at risk of FP8 underflow, whereas u-µP starts with RMS ≈ 1. 2. Many input activations do not grow RMS during training (due to a preceding non-trainable RMSNorm), however the attention out projection and FFN down projection have unconstrained input activations that grow considerably during training. 3. The decoder weight grows during training. Since it is preceded by a RMSNorm, the model may require scale growth in order to increase the scale of softmax inputs. Other weights grow slightly during training. 4. Gradients grow quickly but stabilize, except for attention out projection and FFN down projection, whose gradients shrink as the inputs grow. We also evaluate how RMS growth is affected by model and training hyperparameters in the tensors that showed the highest end-training RMS, shown in Figure 20. This shows that the main parameter affecting scale growth is learning rate, with end-training RMS increasing to the right of the optimal LR basin, as training becomes unstable. End-training RMS is remarkably stable as width, depth, training steps and batch size are independently increased.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Unit-scaled op definitions</head><p>Table 8: Implementations of unit-scaled ops, building on Table A.2. from the Unit Scaling paper <ref type="bibr" target="#b9">[11]</ref>. These are considered part of u-µP and should be used in the place of standard operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Op</head><p>Unit Scaling factors</p><formula xml:id="formula_28">matmul(x, w) = xw α = 1 √ fan-in , β x = 1 √ fan-out , β w = 1 √ batch-size attention(q, k, v) = α = β q = β k = β v = softmax α attn d -1 head (qk ⊤ ) ⊙ c mask v 1/ log_interpolate 1 1+ 4d head α 2 attn , 1, log(s) s gated_silu(x in , x gate ) = α = β xin = β xgate = x in ⊙ x gate ⊙ sigmoid(α ffn-act x gate ) 1/ log_interpolate 1 1+ 1 α 2 ffn-act , 1 √ 2 , 1 2 residual_add(x resid. , x skip ) = a = τ √ τ 2 +1 , b = 1 √ τ 2 +1 a x resid. + b x skip (see G.2.</formula><p>2 for full details, inc. values for τ , which depends on α res and α res-attn-ratio .)</p><formula xml:id="formula_29">softmax_xent(x, t) = log_softmax(α loss-softmax x) t α = 1, β = s/ √ s -1 RoPE(x) α = β = 1 (i.e. no scaling)</formula><p>RMSNorm(x) (non-trainable, see <ref type="bibr" target="#b7">[9]</ref>) α = β = 1 (i.e. no scaling)</p><p>The original Unit Scaling paper provides scaling factors for various ops, in order to make them unit-scaled. However, these ops do not cover every case required for the Llama architecture used in our experiments, nor do they cover our updated residual layer implementation. To address this, in this section we outline a series of new unit-scaled ops for each of our required architectural features, as well as existing unit-scaled ops, as given in Table <ref type="table">8</ref>.</p><p>The presentation here is derived from that of the Unit Scaling Compendium given in <ref type="bibr" target="#b9">[11,</ref><ref type="bibr">Table A.2]</ref>. This makes reference to the factors α, β 1 , . . . , β k . α is the output scaling factor in the forward pass, and β i are the scaling factors for the gradient of the op's inputs in the backward pass. For each op, a value or rule is provided for determining the required mult to ensure unit-scale. The correct value for these multipliers is derived by analyzing the scaling behavior of each op, given some reasonable distributional assumptions about the input and incoming gradient tensors (see Appendix E.2 for an example). Below we provide an in-depth overview of each new or modified unit-scaled op we introduce here.</p><p>Unit-scaled dot-product attention The Unit Scaling paper considers the attention layer scaling in terms of its separate components: the various matmul operations and the internal softmax. Linear operations are scaled using the standard rule, and the softmax scaling is given a α = β = s factor.</p><p>From an implementation perspective, the self-attention layer is more typically broken down into weight-matmuls and a fused scaled-dot-product attention operation. This is the case we handle here, accounting for three complicating factors not considered in the Unit Scaling paper:</p><p>2. We follow the µP guidance of using 1/d head scaling in our self-attention layer, rather than the usual 1/ √ d head .</p><p>3. We place a α attn multiplier immediately before the softmax, which is an HP that users may tune.</p><p>As a result our dot-product attention takes the form:</p><formula xml:id="formula_30">attention(q, k, v) = softmax α attn-softmax • d -1 head • (q • k ⊤ ) ⊙ c mask • v</formula><p>The addition of an HP before the softmax introduces an additional challenge for Unit Scaling, as our scaling multipliers will need to account for this value when preserving unit scale. This operation is sufficiently complex that we found an empirical model of its scale to be more accurate than any mathematically-derived rule (future work may consider justifying our model mathematically). We find that the scale of dot-product attention is approximately The corresponding scaling rule is therefore to divide by this factor in both the forward and backward pass, as outlined in Table <ref type="table">8</ref>.</p><formula xml:id="formula_31">σ(attention(q, k, v)) = log_interpolate 1 1 + 4d head α 2 attn , 1, log<label>(s)</label></formula><p>SwiGLU FFN Llama uses a SwiGLU <ref type="bibr" target="#b55">[57]</ref> layer for its FFN, which introduces two new operations for us to unit-scale: a SiLU <ref type="bibr" target="#b56">[58]</ref> (a.k.a. swish <ref type="bibr" target="#b57">[59]</ref>) operation and an element-wise multiplication. We take a similar approach to our dot-product attention, and consider unit-scaling the following fused operation:</p><formula xml:id="formula_32">gated_silu(x in , x gate ) = x in ⊙ x gate ⊙ sigmoid(α ffn-act x gate )</formula><p>For the surrounding weight-matmuls we follow the standard Unit Scaling rules.</p><p>Again, we use an empirical model of the scale of this op, which is surprisingly similar to the dot-product attention model:</p><formula xml:id="formula_33">σ(gated_silu(x in , x gate )) = log_interpolate 1 1 + 1 α 2 ffn-act , 1 √ 2 ,<label>1 2 ,</label></formula><p>dividing through by this factor to get our scaling rule.</p><p>Residual layers Our implementation of residual layers for u-µP is more complex than other operations, as adjustments are required to:</p><p>1. Make pre-norm residual networks support Unit Scaling (see Appendix F). 2. Introduce our new, principled residual HPs (see Appendix G).</p><p>Our residual layer scheme is presented in full in G.2.2. For readers interested in our justification for this, see the sections noted above. We also follow the example of Unit Scaling and delay the application of our residual multiplier in the backward pass to the base of the branch (see <ref type="bibr" target="#b9">[11]</ref>, Figure <ref type="figure" target="#fig_1">3c</ref>). This does not change the model, and enables unit-scale to be maintained on the residual branch regardless of the value of the multiplier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoPE embeddings</head><p>We also require a unit-scaled implementation of Rotary Position Embeddings (RoPE <ref type="bibr" target="#b58">[60]</ref>), which are applied just before the scaled dot-product attention operation. As RoPE essentially consists of pair-wise rotations of elements by different degrees, we observe no meaningful scale-change as a result of it's application, and hence leave it unchanged.</p><p>RMSNorm Following <ref type="bibr" target="#b7">[9]</ref> we opt to use a non-trainable version of RMSNorm <ref type="bibr" target="#b59">[61]</ref>, in order to facilitate better transfer. As a result, we also leave this operation unchanged. Were a trainable RMSNorm to be used, the recipe would follow closely that of the LayerNorm presented in the original Unit Scaling Compendium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Train the target model: This can be done in FP8 simply by placing casts on matmul inputs (though for our large-scale experiments we found the scales of two operations drifted enough over time that some lightweight dynamic re-scaling was required).</p><p>The above functionality is provided in the Unit Scaling library, to avoid users having to implement it themselves, and to provide a reference implementation. We provide a guide to using this library in the following section.</p><p>D A guide to the unit scaling library</p><p>Our PyTorch <ref type="bibr" target="#b60">[62]</ref> extension library, released under an open source license at <ref type="url" target="https://github.com/graphcore-research/unit-scaling">https://github.com/ graphcore-research/unit-scaling</ref>, accompanies this paper to provide standard and reference implementations of u-µP operations and optimizers.</p><p>This section provides an overview of the functionality of the library; please consult the repository documentation for details. A good place to start is our demo of a simple u-µP training implementation: <ref type="url" target="https://github.com/graphcore-research/unit-scaling/blob/main/examples/demo.ipynb">https://github.com/graphcore-research/unit-scaling/blob/main/examples/ demo.ipynb</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Standard usage</head><p>Compared with SP, u-µP requires the insertion of appropriate scaling factors in the forward and backward pass, a different parameter initialization scheme and the application of learning rate scaling rules based on the role and shape of each parameter.</p><p>The library provides implementations of ops in unit_scaling.functional with appropriate scaling rules (Table <ref type="table">8</ref>). Non-homogeneous ops (Appendix G.1) have optional mults, which are hyperparameters controlling shape of non-linear operations and the interpolation between mutiple inputs. Ops may also specify constraints, which are used to satisfy the cut-edge rule (Appendix H).</p><p>Although this rule could be automated as a global graph transformation, the library makes constraint selection an explicit step for the user, while providing sensible defaults. For example, weight gradients are generally cut-edges, so are unconstrained.</p><p>Parameters are tagged with their role in the model (as a "bias", "norm" parameter, "weight" or "output"). The library achieves this by extending torch.nn.Parameter with an additional property mup_type. This property is required for every parameter in a u-µP model. Given this, and information on the overall depth of the model, the library applies the learning rate rules of Table <ref type="table">2</ref> as a preoptimizer transformation that modifies the learning rate for each parameter. This allows standard PyTorch optimizers to be used without modification.</p><p>PyTorch uses modules to encapsulate parameter declaration, initialization and the calling of ops. The library makes available u-µP versions of common modules, which declare tagged parameters, apply unit-scale initialization, and call unit-scaled ops, with appropriate default settings.</p><p>With these components, user code for training using u-µP is very close to that of vanilla PyTorch (see an example in Figure <ref type="figure" target="#fig_19">21</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Extending the library</head><p>As the set of deep learning ops of interest is always growing, the unit-scaling library is open for extension. For example, consider the possible implementation of unit-scaled hardtanh(x) = clip(x, -1, 1) in Figure <ref type="figure" target="#fig_20">22</ref>.</p><formula xml:id="formula_34">import</formula><p>torch from math import e, erf, pi, sqrt from unit_scaling.constraints import apply_constraint from unit_scaling.scale import scale_fwd, scale_bwd def hardtanh(x, constraint="to_output_scale"): y_scale = 1 / sqrt(1 -sqrt(2/(pi*e))) grad_scale = 1 / sqrt(erf(1/sqrt(2))) y_scale, grad_scale = apply_constraint(constraint, y_scale, grad_scale) x = scale_bwd(x, grad_scale) y = torch.nn.functional.hardtanh(x) return scale_fwd(y, y_scale) The implementation follows a standard pattern:</p><p>1. Calculate the theoretical or empirical scaling factors for each forward and backward pass independently, based on independent unit-scaled Gaussian inputs. 2. Apply the optional constraint to select or combine these scaling factors, using the helper function apply_constraint. 3. Call scale_bwd on the inputs, and scale_fwd on the outputs to compensate for scaling after the op or grad-op is executed.</p><p>It can be checked empirically using random inputs and gradients (example in Figure <ref type="figure" target="#fig_11">23</ref>).</p><p>x = torch.randn(2**20, requires_grad=True) y = hardtanh(x, None) y.backward(torch.randn_like(y)) assert abs(y.std() -1) &lt; 0.01 assert abs(x.grad.std() -1) &lt; 0.01</p><p>Figure <ref type="figure" target="#fig_11">23</ref>: Testing unit-scaled operations, using constraint=None to allow independent fwd and bwd scaling. The default constraint "to_output_scale" preserves forward-pass scale while constraining the forward and backward scales to be equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 As a reference implementation</head><p>The core technique of u-µP is readily implementable in most deep learning frameworks; the primary requirement is for custom gradient operations in order to provide equivalents of scale_fwd and scale_bwd. We hope that the library provides a useful reference, as well as a set of tools and techniques for developing custom u-µP support in other libraries and projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional background material E.1 The Maximal Update Parametrization</head><p>Theoretical background We do not cover the theory underpinning µP in this paper, presenting only its resulting scaling rules (Table <ref type="table">1</ref>). For readers interested in this theory, the extensive Tensor Programs series <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b65">67]</ref> builds up a framework from which µP is derived <ref type="bibr" target="#b0">[1]</ref>. For those requiring a more accessible introduction, <ref type="bibr" target="#b66">[68]</ref> show that µP can be derived in a simpler and more general way by placing a spectral scaling condition on the norm of weights and their updates.</p><p>Applying unit scaling To apply Unit Scaling to a model and train in low-precision, the following steps are required:</p><p>1. Scale parameter initializations to have zero-mean and unit variance.</p><p>2. Replace operations with their unit-scaled equivalents (including and especially the loss, matmuls and residual-adds).</p><p>3. Constrain the scales of operations which are required to have the same forward and backward factors.</p><p>4. Place a simple .to(fp8) cast on the inputs to matmuls.</p><p>Step 3 relates to the problem of conflicting scales in the forward and backward passes. A single linear layer in a differentiated model requires 3 matmul ops in the forward and backward passes, each requiring a different scaling factor ( 1</p><formula xml:id="formula_35">√ d fan-in , 1 √ d fan-out , 1 √ d batch-size</formula><p>). However, using these directly would give invalid gradients. The compromise here is that the activations and activation gradients have their scaling factors constrained such that they are equal (the original Unit Scaling paper recommends taking the geometric mean; we modify this for u-µP in Appendix B to simply use the forward scale everywhere). Weight gradients can still be given their own scaling factor due to the cut-edge rule (as explained in Appendix H).</p><p>Step 4 reflects the key benefit of Unit Scaling. Unlike other methods it changes the learning dynamics of a model, but the advantage is that unit-scaled models then 'naturally' generate well-scaled tensors. This means that low-precision arithmetic ideally becomes as simple as placing a cast operation before matmuls as outlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Unit-scaled pre-norm residual layers</head><p>The popular pre-norm residual network architecture is simple to implement, but problematic to combine with Unit Scaling. It exhibits scale-growth in the skip-stream at initialization, due to the repeated addition of residual connections without subsequent normalization. Here we present a surprising and useful finding: that for any pre-norm model there exists a mathematically-equivalent model where this scale-growth is eliminated, through the careful re-scaling of residual connections.</p><p>Note that this section focuses on applying Unit Scaling to standard pre-norm models. Only once we have addressed this problem are we able to do the same for u-µP models, as shown in Appendix G.2. Readers only interested in our final u-µP residual implementation may skip ahead to Appendix G.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Scale growth in pre-norm residual networks</head><p>Let's consider a pre-norm residual network of depth L:</p><formula xml:id="formula_36">R 0 (x) = r 0 x, (6) R l (x) = r l f l (R l-1 (x)) + R l-1 (x), l = 1, .., L (7) R L+1 (x) = f L+1 (R L (x))<label>(8)</label></formula><p>with embedding multiplier r 0 and residual branch multipliers r l for l = 1, .., L. To satisfy pre-norm, all f l are zero-homogeneous functions, i.e. f l (λx) = f l (x).</p><p>The scale of the skip-stream at initialization as a result of Equation ( <ref type="formula">7</ref>) is</p><formula xml:id="formula_37">σ(R l ) = r 2 l σ(f l ) 2 + σ(R l-1 ) 2 &gt; σ(R l-1 ), l = 1, .., L<label>(9)</label></formula><p>assuming r 2 l σ(f l ) 2 &gt; 0. This shows that scale inevitably grows with the addition of each residual layer.</p><p>This scale-growth is clearly incompatible with unit scaling, which aims for σ(R l ) = 1 for all l = 0, .., L + 1. In the following we present an elegant solution to this problem making use of a symmetry transformation available in pre-norm residual architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual add</head><p>This operation is non-unary and hence receives our second (and third) multipliers: α res , α res-attn-ratio . The manner and motivation for using two multipliers here is justified in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFN RMSNorm</head><p>This operation is 0-homogeneous and thus we start over.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFN input scale</head><p>The input layer is linear, hence it propagates scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sigmoid input</head><p>This function is non-homogeneous and thus we have our fourth multiplier: α ffn-act .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SiLU weight</head><p>This layer is also linear and propagates scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product</head><p>The entry-wise multiplication of the outputs of sigmoid, input layer and SiLU weight is homogeneous and thus propagates scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFN output</head><p>This layer is linear and at the end of the residual path. Hence there are no more multipliers in the FFN residual block. Residual add See above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output RMSNorm</head><p>This operation is 0-homogeneous and thus we start over.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output head</head><p>This layer is linear, hence it propagates scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>The cross-entropy loss is non-homogeneous and leads to our final multiplier: α loss-softmax .</p><p>To facilitate our analysis, we can view the transformer residual output as the sum of three terms:</p><formula xml:id="formula_38">R L = R (e) L + R (a) L + R (f ) L , R (e) L := α e x, R<label>(a)</label></formula><formula xml:id="formula_39">L := L/2 l=1 α a L/2 f 2l-1 (R 2l-1 (x)), R<label>(f )</label></formula><formula xml:id="formula_40">L := L/2 l=1 α f L/2 f 2l (R 2l (x)),</formula><p>and define the average residual scale,</p><formula xml:id="formula_41">σ(R (a,f ) L ) 2 := σ(R (a) L ) 2 + σ(R (f ) L ) 2<label>2</label></formula><p>.</p><p>Note that we have added in the depth-µP multipliers here, though a similar analysis can be performed for non-depth-µP models. As above, f l functions alternate between self-attention layers and feedforward layers.</p><p>With respect to our interpretability criterion, we propose two new multipliers that correspond to dynamics in the network which we suggest are important to control at initialization. The first is the ratio of the average scale of the residuals' contributions to those of the embedding, α r = σ(R</p><formula xml:id="formula_42">(a,f ) L )/σ(R<label>(e)</label></formula><p>L ). The second is the ratio of the scale of the attention-residuals' contributions to those of the feed-forward-residuals, α ρ = σ(R (a) L )/σ(R (f ) L ). Not only do these two ratios control key dynamics of our model, but we can use them to replace our existing (α e , α a , α f ) multipliers.</p><p>Let us first examine these two quantities for a standard (non-unit-scaled model). Residual functions of the same kind have the same expected output scale at initialization in pre-norm networks, meaning we can denote the output scale σ(f l (R l )) of all self-attention functions as σ a , and of all feed-forward functions as σ f . We thus have the following scales at the output:</p><formula xml:id="formula_43">σ(R (e) L ) = α e σ(x), σ(R (a) L ) = α a L/2 σ   L/2 i=1 f 2l-1 (R 2l-1 )   = α a σ a , σ(R (f ) L ) = α f L/2 σ   L/2 i=1 f 2l (R 2l )   = α f σ f , σ(R (a,f ) L ) = (α a σ a ) 2 + (α f σ f ) 2 2 .</formula><p>Recalling our definitions of α r , α ρ above, this gives us:</p><formula xml:id="formula_44">α ρ = α a α f σ a σ f , α r = (α a σ a ) 2 + (α f σ f ) 2 2 (α e σ(x)) 2 , = α 2 ρ + 1 2 σ f σ(x) α f α e .</formula><p>The original α a , α f multipliers can then be written in terms of α r , α ρ :</p><formula xml:id="formula_45">α a = α ρ α f σ f σ a α f = α r α e σ(x) σ f 2 α 2 ρ + 1</formula><p>We have replaced two of the three original multipliers, but still have a dependence on α e here in our expressions for α f and R</p><p>L , which we now remove by dividing it out of our residual branches and embedding. We use the hat (•) symbol to denote terms that have been divided-through by α e . This new system of equations is equivalent to our old one thanks to the zero-homogeneity of the final post-residual layer:</p><formula xml:id="formula_47">R L+1 (x) = f L+1 (R (e) L + R (a) L + R (f ) L ) = f L+1 ((R (e) L + R (a) L + R (f ) L )/α e ) = f L+1 ( R(e) L + R(a) L + R(f) L )</formula><p>which we implement (assuming depth-µP branch scaling) as:</p><formula xml:id="formula_48">a l =        α attn-residual L/2 l is odd (self-attention) α ffn-residual L/2 l is even (feed-forward) b l = 1 c = α emb .</formula><p>The corresponding u-µP set of residual HPs is (α res , α res-attn-ratio ), which we implement as:</p><formula xml:id="formula_49">a 2 l = τ 2 l τ 2 l + 1 (25) b 2 l = 1 τ 2 l + 1 (26) c = 1,<label>(27) (28)</label></formula><formula xml:id="formula_50">τ 2 l =            α2 a L 2 + ℓα 2 a + ℓα 2 f l is odd α2 f L 2 + (ℓ + 1) α2 a + ℓα 2 f l is even , ℓ = l -1 2<label>(29)</label></formula><formula xml:id="formula_51">α2 a = α 2 res-attn-ratio α2 f<label>(30)</label></formula><formula xml:id="formula_52">α2 f = 2 α 2 res-attn-ratio + 1 α 2 res .<label>(31)</label></formula><p>This is the u-µP residual scheme. It satisfies the three properties that we initially set out to achieve: the variance at initialization of our R l (x) is always 1, our HPs have a clear and useful interpretation, and our scheme is as expressive as the baseline (which is neither unit-scaled or has interpretable HPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H The cut-edge rule</head><p>In the section we review the notion of constraints used for scaling operations in a computational graph. For a more thorough, generalized treatment, please refer to Section 5.1 and Appendix E.4 of the original Unit Scaling paper <ref type="bibr" target="#b9">[11]</ref>.</p><p>For simplicity, we will only discuss the cut-edge rule in the context of a typical neural network. For each operation f , parametrized by θ taking input x and emitting output y, a user must choose how to scale y, ∇ x and ∇ θ (gradient of loss w.r.t. x and θ respectively). In the simplest case, where there are no further data dependencies, we can simply choose factors that preserve unit scale. In more complex scenarios, we must balance the need for each tensor to be unit-scaled and for gradients to be correct up to a constant factor.</p><p>In particular, a problem emerges in the presence of residual blocks in which y = x + f (x; θ). In these circumstances, ∇ x is computed as the sum of residual gradient ∇ f ∂f ∂x and skip gradient ∇ y . If we choose not to insert scaling factors into our graph, ∇ f ∂f ∂x and ∇ y will have some ratio of scale r. However, if we have chosen to rescale the gradient of operations in f , then ∇ f ∂f ∂x will have been rescaled by some s. This means the new ratio of ∇ f ∂f ∂x and ∇ y will be r • s. Therefore, when adding these together, ∇ x is no longer a correct gradient up to a constant factor.</p><p>How do you remedy this? If we can ensure that the scaling factors are the same for both the input gradients and outputs of an op, we will have s = 1. This ensures that gradients for inputs to residual blocks are correct up to a constant factor. How do you decide when you are free to preserve unit scale, and when to constrain scaling factors to be the same? We previously define the cut-edge rule <ref type="bibr" target="#b9">[11]</ref> for computational graphs where nodes represent forward pass operations and edges represent operation outputs. If an input edge is a cut-edge, i.e., the number of connected components in the graph would increase upon deletion (examples in a typical transformer model: output of embedding gather, output of a residual add, output of final norm, output token logits, weights), there is no need to constrain the scales of the operation's output edge and the input edge gradient. For all other input edges (e.g., inputs to a residual add, intermediates computed along a residual branch), the scales of gradients and outputs should be constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I From µP to u-µP</head><p>Here we outline additional details to help readers follow the process of deriving u-µP from the combination of Unit Scaling and µP. Our first step of dropping σ W and base-fan-in, and moving α W s to functions, results in Table <ref type="table" target="#tab_22">11</ref>. This intermediate scheme does not yet satisfy Unit Scaling, but simplifies the HP rules in preparation for further changes. Note that we have also removed ηemb as we don't include this HP in our u-µP extended HP set. We have included residual scaling rules here, in accordance with depth-µP, which we intend u-µP to satisfy, though our standard µP implementation doesn't use it. </p><formula xml:id="formula_53">(A W ) 1 1 1 fan-in 1 √ depth * initialization (B W ) 1 1 √ fan-in 1 - Adam LR (C W ) η η 1 fan-in η 1 √ depth</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Low-precision and its trade-offs</head><p>Number formats for deep learning The standard numerical representations used in deep learning are the set of formats defined by the IEEE 754 floating-point standard <ref type="bibr" target="#b68">[70]</ref>. IEEE floats comprise three elements: a sign bit, exponent bits, and mantissa bits. The number of exponent bits determines the range of a format, while the mantissa determines the precision<ref type="foot" target="#foot_7">foot_7</ref> . We refer readers to the original Unit Scaling paper ( <ref type="bibr" target="#b9">[11]</ref>, Section 3.1) for a comprehensive overview of floating-point representations.</p><p>The default format used for training is the single-precision floating-point format, commonly known as FP32, with some hardware providers automatically casting it to the smaller TF32 compute mode for accelerated arithmetic. The 16-bit FP16 and BF16 formats were later introduced, and more recently the FP8 E5 &amp; E4 formats <ref type="bibr" target="#b69">[71,</ref><ref type="bibr" target="#b70">72,</ref><ref type="bibr" target="#b71">73]</ref>. The higher range of E5 has typically been used for gradients, while the higher precision of E4 has been seen as necessary for weights and activations.</p><p>Our particular implementation of FP8 training is covered in Section 4.2. Other aspects of training such as the optimizer state and cross-device communication have also been put into FP8 <ref type="bibr" target="#b27">[29]</ref>, though not all tensors are amenable to being run in the lowest precision <ref type="bibr" target="#b38">[40]</ref> without degradation. The use of multiple formats is known as mixed precision <ref type="bibr" target="#b72">[74]</ref>. A comparison of these formats is given in Table <ref type="table" target="#tab_23">12</ref>.</p><p>The benefits of low-precision Using numerical representations with fewer bits facilitates the design of more efficient arithmetic in hardware, typically leading to a linear increase in peak FLOPS (as shown in Table <ref type="table" target="#tab_23">12</ref>). As large-scale training efforts are typically compute-bound due to the size of matmuls <ref type="bibr" target="#b73">[75]</ref>, putting the inputs to these operations in low-precision formats has a substantial impact on training efficiency. Low-precision formats also reduce the other two common performance constraints: for memory-bandwidth-bound models they require fewer bits to be transmitted, and for memory-size-bound models they require fewer bits to be stored. The challenges of low-precision Unfortunately, moving to low-precision formats also increases quantization error. For values within the representable range this takes the form of rounding error, and for values outside it, clipping error (both overflow and underflow). Rounding error tends to be an intrinsic problem: the number of mantissa bits dictates the expected accuracy of representations and this cannot easily be changed. In contrast, clipping error is often eliminated by scaling a tensor so that its values lie within the range of a format. Note that a multiplicative change in values of this kind doesn't affect the (relative) rounding error, due to the exponential spacing of values. Most research into making low-precision work has focused on the problem of scaling tensors in this way.</p><p>Simply casting all tensors to FP16 or FP8 tends to impair training, largely due to clipping error. For FP16, this primarily affects gradients. <ref type="bibr" target="#b72">[74]</ref> address this by introducing a fixed global loss-scale HP, which multiplies the loss value in the backward pass, artificially up-scaling gradients to lie within FP16 range <ref type="bibr" target="#b72">[74]</ref>. Automatic loss scaling <ref type="bibr" target="#b74">[76]</ref> builds upon this idea, making the loss-scale a dynamic value that is tuned during training.</p><p>The later BF16 format has the same range as FP32, making loss scaling unnecessary. For FP8 no such range-equivalent format can exist, so the problem of clipping error must be addressed. Most FP8 implementations have done so by moving from a global loss-scale to a local scale for each FP8 tensor. In pseudo-code, this takes the form: where we assume that matmul takes inputs in FP8 and directly produces the output in higher precision.</p><p>The result of the scale() operation can either be a fixed scale determined before training <ref type="bibr" target="#b70">[72]</ref>, or in the case of Transformer Engine <ref type="bibr" target="#b75">[77]</ref>, computed dynamically as a function of the 'absmax' of the input tensor (though they introduce a delay across time-steps, to facilitate an efficient fused kernel).</p><p>Increasing granularity and computing scales dynamically using this kind of method inevitably adds complexity (from both a logical and implementation perspective), as well the potential for computational overhead. Unit Scaling generally avoids the need for matmul input scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Benchmarking scaled matrix multiplication implementation in PyTorch</head><p>Given that the end-goal of leveraging u-mup's low-precision properties is to speed up training and reduce memory usage, it's reasonable to ask why we don't investigate this experimentally. The answer relates to the relative immaturity of the FP8 training software stack -a lack of open, efficient FP8 kernels for compute and communication mean significant additional engineering effort is required to attain expected speedups over the full model.</p><p>Here we show that u-µP's static scaling factors add no overhead to matmuls in FP8, and hence ought to be able to reach close to the maximal FP8 throughput attainable for the full model. Standard strategies for FP8 training require expensive statistics gathering (e.g., amax) per tensor. A key benefit of u-µP for FP8 training is that it instead provides us with static scaling factors to rescale operation outputs. Even a naive implementation in pytorch can achieve a minimal drop in hardware utilization.</p><p>Figure <ref type="figure" target="#fig_22">24</ref> demonstrates hardware utilization for FP8, FP16, and FP32 matrix multiplications on a single NVIDIA H100 PCIe card. For FP16 and FP32, torch.matmul is used, whereas torch._scaled_mm is used for FP8. Comparing "scaled" to "unscaled" matrix multiplication demonstrates a 30%, 20%, and 10% drop in hardware utilization for each data type respectively. In the case of FP8, where the drop in utilization is most pronounced, utilization can be recovered by passing the scaling factor as a scale associated with one of the two input tensors.</p><p>It should be noted that as of PyTorch version 2.3, torch._scaled_mm always computes amax as well as the matrix multiplication. The performance of FP8 matrix multiplications could be higher without this overhead.</p><p>The above analysis focuses on throughput; significant memory savings are also possible through the use of FP8, though how this affects the total memory footprint depends on various additional variables and the overall distributed training setup. The following factors are play a significant role: typically the main memory bottlenecks are the optimizer states, which are kept in full precision. This footprint can be reduced by applying ZeRO sharding <ref type="bibr" target="#b53">[55]</ref>, though for significant gains the number of data parallel processes needs to be sufficiently large and ZeRO stage 2 or 3 are required. In these settings the memory footprint of activations and gradients becomes significant, and quantizing these to lower precision promises further memory savings, though may be non-trivial <ref type="bibr" target="#b27">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Attention output RMS grows with model depth</head><p>A core assumption in deriving per-op scaling factors is that each input to an operation has zero mean, unit-variance, and uncorrelated elements at initialization. This is trivially true for weights and by extension the token embeddings taken as input to the transformer trunk. However, this is not guaranteed for intermediate results and gradients if an operation in the computational graph induces correlation in the elements. In such a scenario our scaling factors will not return unit-variance outputs as we will not have corrected for these correlations in the inputs. As we then increase the depth of the network, where the same operation is left to amplify correlations, we can end up with variance in intermediate results and gradients scaling with depth Figure <ref type="figure" target="#fig_24">25</ref> illustrates this phenomenon in a unit-scaled four-layer Llama model with width=256. All activation tensors in the residual branches are unit-scaled, except for the output of the attention layers. We also see that the variance of attention outputs grows with depth. Since Llama models use pre-norm on the residual-branch, residual-branch inputs will revert to unit-scale again until they reach another instance of the correlation-inducing operation. As we add under-scaled attention layer results back to the skip-branch, our skip tensor variances grow with depth as our residual-add assumes unit-variance inputs. This has a knock-on effect on the global scaling of the gradients since the Jacobian of the final norm will scale the gradient by the inverse of the final skip tensor variance.  So which operation induces correlation in the attention output at initialization? For the default case where all multipliers are set to 1, our 1/d scaling of attention logits results in a sufficiently high temperature that attention probabilities are effectively uniform. With causal masking, we effectively take a running mean across the value tensor along the sequence dimension. As a result, each subsequent token representation is correlated with the last. Since we derive appropriate scaling factors for the first layer, we do not see scale growth emerging until the second layer, where correlations accumulate during the next effective running mean.</p><p>We leave it to future work to offer a solution to scale growth created by correlation in intermediate tensors. We note that this is scale growth emergent at initialization, but we also see scale growth in other intermediate tensors during training. Whether scale growth during training is related to the phenomenon outlined here remains to be seen.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effective µTransfer does not hold across all training setups. (a) We show strong transfer for the unrealistic setup used in Tensor Programs V (too many epochs; constant LR). (b) Moving to a more standard Llama training setup, transfer breaks down. (c) This is restored by the introduction of two stability fixes: non-parametric norms and independent weight decay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (Left) holding the embedding LR (η emb ) constant, vs. scaling with base-width /width, both with a fixed global LR. This suggests the µP embedding LR rule (c emb ) should follow the latter scaling. (Right) we test this by sweeping the global LR under the two scaling rules. The new rule leads to lower loss on large models. (Dot/cross markers represent the same runs across both graphs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>η ηe m b σ i n i t α e m b α a t t n α o u t p u t η ηemb σ init α emb α attn α output µP η α r eFigure 4 :</head><label>4</label><figDesc>Figure4: A visualization of the dependencies between pairs of HPs under each scheme. Transfer error measures the extent to which the optimal value of the transfer HP depends on the fixed HP (see Algorithm 1). On average, µP has a transfer error of 0.03, whereas u-µP has 0.005.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Learning rate transfer for µP (top) and u-µP (bottom), over training steps, batch size and depth. See Figure 1 (b) for transfer over width. The default shape parameter for other panels is shown in bold. The shaded area shows the 95% confidence interval for the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Per-tensor RMS = σ 2 + µ 2 across u-µP and µP models at initialization (left) and after training (right). u-µP tensors have RMS that starts close to 1 and remains within E4M3 range at the end of training. Dashed and solid red lines show each format's min. normal and subnormal values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Large-scale training runs. (Left) u-µP BF16 vs u-µP FP8. (Right) u-µP BF16 vs SP BF16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The effect of the individual transfer stability fixes from Figure 2. (a) In this setting switching from non-independent to independent weight decay has only a minor effect, though [18] Figure 6 suggests it may be highly valuable in other settings. (b) Non-parametric norms give a narrower learning rate basin, leading to better transfer. (c) The combination of these, for comparison, matching Figure 2 (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An ablation of the more standard Llama training settings against the Tensor Programs V settings from Figure 2. This shows that the flat basins with poor transfer are not due to a single change, but the combination of a larger dataset (training &lt; 1 epoch) and the stronger Llama model are largely responsible. Note that 'Llama model' here indicates a group of changes: rms norm, rotary embeddings &amp; swiglu FFN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: A repeat of the batch size and training steps experiments in Figure5, but using the larger SlimPajama dataset where no data is repeated. In both settings our validation loss basins take the same shape, indicating that our analysis using the WikiText-103 dataset holds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>. 2 -12 2 - 9 .5 2 - 7 .5 2 - 5 .5 2 - 3</head><label>229272523</label></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 -</head><label>2</label><figDesc>12 2 -9.5 2 -7.5 2 -5.5 2 -3.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :Algorithm 1</head><label>141</label><figDesc>Figure14: Hyperparameter coupling sweep for µP. Note strong coupling between optima, e.g. in the cases of (η emb , σ init ) and (η, α attn ). See also: u-µP, Figure15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Hyperparameter coupling sweep for u-µP. Note less coupling than with µP, see Figure 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: RMS during training, for all parametrized matmul inputs, for µP (top) and u-µP (bottom). Model width 256, default hyperparameters, η = (2 1 , 2 -8 ) for (u-µP, µP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: The effect of hyperparameters on FP8 training loss and on the end-training RMS of critical tensors: (a) decoder weight, (b) last-layer FFN down-projection input and (c) last-layer FFN down-projection output gradient. Only learning rate has a substantial effect on the end-training RMS. Vertical lines show the default setting of that hyperparameter, as used for all other plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><figDesc>s where log_interpolate(α, b upper , b lower ) = e α log(bupper)+(1-α) log(b lower ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Using the unit scaling library given the tensors input_ &amp; target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Implementing new unit-scaled operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>a</head><figDesc>= scale(A) b = scale(B) A = to_fp8(A / a) B = to_fp8(B / b) C = (a * b) * matmul(A, B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Square matrix multiplication throughput in TFLOPs with and without scaling factors applied to the output across 32-, 16-, and 8-bit float dtypes on NVIDIA H100 PCIe. Naive implementation in PyTorch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Scale of intermediate tensors grows with depth at initialization. Top left: Intermediate activation tensor RMS along the residual branch. Only the attention outputs after the first layer are not unit-scaled. Bottom left: Skip activation tensor RMS. Scale growth in attention outputs drives growth in skip activation scales. Note that layer_idx= 0 corresponds to the embedding output, and layer_idx= 4 corresponds to the final layer outputs. Top right: Intermediate gradient tensor RMS along the residual branch. Growth in the attention output scale drives growth in attention qkv gradient scales. Bottom Right: Skip gradient tensor RMS. The scale of output activations induces a global rescaling of the gradients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Default hyperparameters and training settings.</figDesc><table><row><cell></cell><cell cols="4">(a) Independent weight decay</cell><cell></cell><cell cols="4">(b) Non-parametric norms</cell><cell></cell><cell cols="4">(c) Independent weight decay</cell></row><row><cell></cell><cell>4.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">+ Non-parametric norms</cell></row><row><cell></cell><cell>3.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Width 128</cell></row><row><cell>Validation Loss</cell><cell>3.2 3.4 3.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>256 512 1024 2048 4096 Standard</cell></row><row><cell></cell><cell>3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Llama settings</cell></row><row><cell></cell><cell>2.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Min</cell></row><row><cell></cell><cell>2 -11</cell><cell>2 -9</cell><cell>2 -7</cell><cell>2 -5</cell><cell>2 -3</cell><cell>2 -11</cell><cell>2 -9</cell><cell>2 -7</cell><cell>2 -5</cell><cell>2 -3</cell><cell>2 -11</cell><cell>2 -9</cell><cell>2 -7</cell><cell>2 -5</cell><cell>2 -3</cell></row><row><cell></cell><cell></cell><cell cols="2">Learning Rate</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Learning Rate</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Learning Rate</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Comparison of Tensor Programs V's standard settings (as best we can tell) and our Standard Llama setup, corresponding to (a) and (b) in Figure2.</figDesc><table><row><cell>Feature</cell><cell cols="2">Tensor Programs V Standard Llama</cell></row><row><cell>Dataset</cell><cell>wikitext-2</cell><cell>wikitext-103</cell></row><row><cell>Vocab Size</cell><cell>33278</cell><cell>32000</cell></row><row><cell>Nsteps</cell><cell>10000</cell><cell>8192</cell></row><row><cell>Batch Size</cell><cell>20</cell><cell>64</cell></row><row><cell>Optimizer</cell><cell>adam</cell><cell>adamw</cell></row><row><cell>LR Schedule</cell><cell>constant</cell><cell>cosine</cell></row><row><cell>Weight Decay</cell><cell>0</cell><cell>0.00012</cell></row><row><cell>Positional Encoding</cell><cell>absolute</cell><cell>rotary</cell></row><row><cell>Norm</cell><cell>layer_norm</cell><cell>rms_norm</cell></row><row><cell>Dropout</cell><cell>0.2</cell><cell>0</cell></row><row><cell>NLayers</cell><cell>2</cell><cell>4</cell></row><row><cell>Use Gated FFN</cell><cell>False</cell><cell>True</cell></row><row><cell>Activation FN</cell><cell>relu</cell><cell>swish</cell></row><row><cell>FFN Ratio</cell><cell>4</cell><cell>2.75</cell></row><row><cell>Final Norm</cell><cell>False</cell><cell>True</cell></row><row><cell>Base Depth</cell><cell>1</cell><cell>4</cell></row><row><cell>Zero Init Readout</cell><cell>True</cell><cell>False</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 10 :</head><label>10</label><figDesc>A walkthrough of the Llama architecture, showing how our α attn-softmax , α ffn-act and α loss-softmax multipliers are derived via an analysis of scale-propagation.linear and lies at the end of the attention residual path. Hence there are no more multipliers in the attention block.</figDesc><table><row><cell>Op</cell><cell>Scale propagation behavior</cell></row><row><cell>Embedding</cell><cell>We show in Appendix G.2.1 that the embedding multiplier can be</cell></row><row><cell></cell><cell>absorbed in the residual multipliers, meaning one is not required here.</cell></row><row><cell>Attention RMSNorm</cell><cell>This operation is 0-homogeneous and thus we start over.</cell></row><row><cell>Query &amp; key projection</cell><cell>Both are linear, meaning their scale is propagated. Multipliers are</cell></row><row><cell></cell><cell>therefore not required.</cell></row><row><cell>Query-key matmul</cell><cell>Again linear. As query &amp; key are both generated from the same</cell></row><row><cell></cell><cell>input, this operation is 2-homogeneous wrt. that input. Hence it also</cell></row><row><cell></cell><cell>propagates scale.</cell></row><row><cell>Softmax</cell><cell>The softmax operation is non-homogeneous. Thus the pre-op scale of</cell></row><row><cell></cell><cell>the softmax becomes our first multiplier: α attn-softmax .</cell></row><row><cell>Value</cell><cell>The value layer is linear and hence propagates scale.</cell></row><row><cell>Softmax-value matmul</cell><cell>Again linear and hence propagates scale.</cell></row><row><cell>Attention projection</cell><cell>This operation is</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 11 :</head><label>11</label><figDesc>An intermediate scheme resulting from dropping those HPs from µP which are not needed under u-µP.</figDesc><table><row><cell>ABC-multiplier</cell><cell>Input</cell><cell>Weight Type Hidden</cell><cell>Output</cell><cell>Residual</cell></row><row><cell>parameter</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 12 :</head><label>12</label><figDesc>A comparison of deep learning formats. E indicates exponent bits, and M mantissa bits. The smaller formats typically give more FLOPS, at the expense of reduced range and/or precision.</figDesc><table><row><cell>Format E M</cell><cell>| max |</cell><cell cols="3">| min normal | | min subnormal | FLOPS (vs TF32)</cell></row><row><cell cols="2">FP32 TF32 BF16 FP16 FP8 E5 5 8 23 3.4 × 10 38 8 10 3.4 × 10 38 8 7 3.4 × 10 38 5 10 65504 2 57344 FP8 E4 4 3 448</cell><cell>1.2 × 10 -38 1.2 × 10 -38 1.2 × 10 -38 6.1 × 10 -5 6.1 × 10 -5 1.6 × 10 -2</cell><cell>1.4 × 10 -45 1.1 × 10 -41 9.2 × 10 -41 6.0 × 10 -8 1.5 × 10 -5 2.0 × 10 -3</cell><cell>&lt; 1 × 1× 2× 2× 4× 4×</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The GPT-4 technical report<ref type="bibr" target="#b6">[7]</ref> hints at the use of µP by including<ref type="bibr" target="#b1">[2]</ref> in its references, without citing it directly. The multipliers present in the Grok [8] codebase also suggest the use of µP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>As in other work, we use µP as a shorthand for the method outlined in Tensor Programs V, including µTransfer. Strictly speaking, µP ought only to refer to the parametrization outlined in Tensor Programs IV.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Lingle suggests independent weight decay is unstable, but we find it to be more so than Adam or standard AdamW.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>This represents a slight deviation from the Maximal Update Parametrization, though we still refer to our scheme as a form of µP as it conforms in all other aspects.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>As we use depth-µP this could be said about depth as well, but as<ref type="bibr" target="#b12">[14]</ref> show that transformers don't attain depth-transfer under depth-µP we do not expect strong transfer across depth.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>The training codebase used for our larger-scale experiments can be found at the following url https://github.com/Aleph-Alpha/scaling. We have also released model checkpoints, which are available at https://huggingface.co/Aleph-Alpha.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_6"><p>As we use a decoder-style transformer in our experiments, our softmax operation has a causal mask applied to its input.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p>Confusingly, the term low-precision tends to indicate using &lt;32 bit-width formats, so in this context precision also reflects the number of exponent bits as well as the usual mantissa bits.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>We would like to thank <rs type="person">Paul Balança</rs>, <rs type="person">Andrew Fitzgibbon</rs>, <rs type="person">Steve Barlow</rs>, <rs type="person">Mark Pupilli</rs>, <rs type="person">Jeremy Bernstein</rs>, <rs type="person">Tim Large</rs> and <rs type="person">Lucas Lingle</rs> for their feedback and discussions around u-µP at the various stages of its development.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Large-scale training details</head><p>Our large-scale training settings are given in Table <ref type="table">7</ref>. These are largely the same as our standard experiments (Table <ref type="table">5</ref>), but with many more tokens used for training and scaling up to a larger model-size.</p><p>Dataset SlimPajama <ref type="bibr" target="#b21">[23]</ref> Sequence length 4096</p><p>Vocab size 65536 Training set tokens 600B</p><p>Architecture Llama <ref type="bibr" target="#b19">[21]</ref> (Transformer, PreNorm, RMSNorm, SwiGLU, RoPE, "untied" embeddings), non-trainable RMSNorm parameters. Width <ref type="bibr">[2048,</ref><ref type="bibr">3072,</ref><ref type="bibr">4096]</ref> (1024 for proxy model) Depth <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b30">32]</ref> (8 for proxy model) Number of heads <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b30">32]</ref> (8 for proxy model) Head dimension 128 Total parameters [1.07B, 3.12B, 6.98B] Batch size 1024 Training steps 72000 (∼ 300B tokens; 20000 for proxy model) LR schedule Cosine to 10%, 500 steps warm-up Optimizer AdamW (β 1 , β 2 , ϵ) = (0.9, 0.95, 10 -8 ) Weight decay 2 -13 , independent <ref type="bibr" target="#b47">[49]</ref> Dropout 0.0 Table <ref type="table">7</ref>: Large-scale training settings.</p><p>We use mixed-precision during training with optimizer states in FP32 that are sharded via ZeRO stage 1 <ref type="bibr" target="#b53">[55]</ref>. We retain the model weights in BF16 and apply our FP8 scheme as described in Section 4.2 to the tensors participating in matmul operations throughout the transformer block. All other tensors remain either in BF16 (embedding, readout layer, norm, activation function) or FP32 (Flash Attention <ref type="bibr" target="#b54">[56]</ref>).</p><p>Each model was trained on several Nvidia A100 (80GB) or H100 GPUs, with all FP8 experiments conducted on the H100 chips utilizing their native FP8 support. For the FP8 operations we use PyTorch's torch._scaled_mm function as a backbone.</p><p>Scale constraints One final, minor deviation from the scheme outlined in the Unit Scaling paper is the way in which we apply scale constraints (see their Section 5.2). The essence of scale constraints is that for perfect unit scaling, sometimes the ideal scale for the forward pass differs from those in the backward pass. In some special cases (e.g. at the ends of the network) the use of different scales can be valid, but in the general case a single scale must be agreed upon. The solution in the Unit Scaling paper is to use the geometric mean of the forward and backward scales.</p><p>We propose instead to simply use the forward scale over the backward scale(s) in these cases. We do so for the following reasons:</p><p>1. For these architectures we find empirically that where there is a disparity in ideal forward and backward scales, it is not large.</p><p>2. By taking the forward scale, we can ensure strict unit-scale in the forward pass.</p><p>The value of the latter point is in terms of what it means for the interpretation of our u-µP multiplier HPs. Consider the α ffn-act multiplier; with strict unit scale we can say that the standard deviation of activations immediately before this multiplier is 1. Therefore the standard deviation immediately after is α ffn-act . As this multiplier is (by design) the last operation before the ffn activation function, we can say that the interpretation of α ffn-act is simply to set the input standard deviation to the FFN's activation function. Similar arguments can be made for other u-µP multiplier HPs. This interpretation only holds because we use the forward-scale in our constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C A guide to using u-µP</head><p>We bring together our u-µP scheme presented in Section 4 to form a simple recipe for applying it to a model. The u-µP scheme is designed and validated on a Llama-style architecture, so it may not be applicable or effective on other models, particularly those with substantially different architectures. Exploring this question is an important avenue for future work.</p><p>Before applying our scheme, users are encouraged to apply the following pre-requisites to their training setup, based on our analysis of effective µTransfer in Section 3.1:</p><p>• Remove trainable parameters from normalization layers</p><p>• Use the independent form of AdamW</p><p>• Ensure training is in the under-fitting regime (i.e. avoid excessive data repetition)</p><p>Having done this, our recipe for using u-µP is as follows:</p><p>1. Replace operations &amp; optimizers with u-µP versions: Each operation should be replaced by a unit-scaled version (these wrap the existing operations, with added static scales in the forward and backward passes). We have pre-calculated scales for common operations in Appendix B. Parameters should be initialized with unit variance, and Adam(W) adjusted to use the scaling rules defined in Section 4.4 (we refer to the optimizer as Adam in this section, but AdamW should be used if weight decay is required. Other optimizer scaling rules can be determined by the same process we outline). These features are all implemented in our library (see Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Choose a set of HPs to sweep: From the set of HPs outlined in Table <ref type="table">3</ref>, select those to be swept. We recommend the extended set, though a basic LR sweep can be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Decide on proxy model config:</head><p>The cost of proxy model training should be such that the sweeping process is much less than target model training, while still being as representative as possible. We base our recommendations on the results in Figure <ref type="figure">5</ref>. In general, width is the most reliable feature to transfer. Training steps and batch size also give good transfer, so moderate changes here are permissible. Depth is the least reliable feature for transfer, so we only recommend modest changes in depth. We keep the number of warmup steps constant, but always decay to the same final LR when varying the number of steps.</p><p>4. Perform independent HP search: Following the process outlined in Section 5.2 and Appendix A.6.</p><p>Approaches to HP sweeping in the literature Table <ref type="table">9</ref> outlines the ways in which users of µP in the literature have approached HP sweeping. These all follow the approach used in Tensor Programs V of a random sweep, sampling combinations from the joint space of all HPs. The authors of Tensor Programs V note that other more complex methods may be more efficient, but these are considered beyond the scope of their work and have not been used widely. A Bayesian search method was used for the development of MiniCPM <ref type="bibr" target="#b5">[6]</ref>, but the authors give no further details-as they use 400 runs in their sweep it is not clear that this approach makes HP search easier.</p><p>Table <ref type="table">9</ref>: Sweeping configurations used for a selection of µP models from the literature. The sweeping process is similar across models, the only differences being the choice of discrete or continuous distributions and their ranges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Unit Scaling</head><p>An example: the unit-scaled matmul op Here we outline the procedure for calculating the scaling factor of a matmul op, which practitioners can use as a guide for scaling new ops that we do not cover in this paper (see Appendix B).</p><p>There are two potential approaches here. The first is to derive scaling factors from an analysis of an op's dynamics. Specifically, given the assumption of unit-scaled inputs, the appropriate scaling factor is the reciprocal of the expected output scale. For a basic matrix-matrix matmul we have,</p><p>where weights and activations are sampled i.i.d. from a centered Gaussian:</p><p>From this we can derive the expected output scale (i.e. σ(matmul)):</p><p>Under Unit Scaling we have σ W = σ X = 1, and hence the scaling factor required to ensure a unit-scaled output is 1/ √ d fan-in . This gives our final unit-scaled matmul:</p><p>The distributional assumptions made here hold at initialization, but do not over training. A more precise model for the asymptotic behavior of neural networks under training is given by the Tensor Programs framework, but for the purposes of numerics this precise treatment of scale at initialization appears to be sufficient.</p><p>The second, less ideal approach to calculating scaling factors is to use experimentation to infer this relationship empirically. In this case, one would sample random initializations and compute the output scale over a range of d fan-in values (or whatever HPs one expects the output scale to depend on), fitting a curve to the observed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Residual symmetry in pre-norm architectures</head><p>To resolve the problem of scale shift in residual networks demonstrated by Equation ( <ref type="formula">9</ref>), we try a slightly more general ansatz:</p><p>with coefficients a l , b l . We want to choose these coefficients so that the outputs of Rl are unit-scaled if the outputs f l , Rl-1 are. A similar calculation as in Equation ( <ref type="formula">9</ref>) leads to the sufficient condition</p><p>which can be easily satisfied. Having restored Unit Scale, we are faced with another issue. It seems that Equations <ref type="bibr" target="#b8">(10)</ref> to <ref type="bibr" target="#b10">(12)</ref> describe a different network than Equations ( <ref type="formula">6</ref>) to (8), whereas ideally the relation from input to final output should be unchanged when converting the network to Unit Scaling.</p><p>Note that the coefficients a l , b l are not uniquely defined yet, so our mathematical intuition tells us that we should find an additional constraint to get a unique solution. To find this constraint, let us consider our original residual network in Equations ( <ref type="formula">6</ref>) to ( <ref type="formula">8</ref>) and analyze how the variance propagates through the network if we assume all the f l satisfy Unit Scaling and σ(x) = 1. Let σ 2 l-1 denote the variance of R l-1 . Then a simple inductive calculation shows that</p><p>By Equation ( <ref type="formula">7</ref>) the output of R l adds a quantity of scale r l from the residual connection and a quantity of scale σ l-1 from the skip connection. Intuitively, the ratio of these scales should be more important for the overall network dynamics than their absolute values. Thus our constraint becomes preserving the ratio of scales from the original model, through our choice of a l , b l :</p><p>which, recalling Equation ( <ref type="formula">13</ref>), (up to sign) uniquely defines our multipliers a l , b l as</p><p>In summary, we propose the modified residual network</p><p>Our main result of this section is that this network is indeed mathematically equivalent to the network defined in Equations ( <ref type="formula">6</ref>) to (8), under a simple additional structural assumption:</p><p>Lemma F.1. Consider R l , Rl defined as in Equations ( <ref type="formula">7</ref>) and ( <ref type="formula">16</ref>) respectively. Then Rl = R l / l i=0 r 2 i for all l = 0, .., L.</p><p>Remarkably, this result does not assume the individual network operations f l actually satisfy Unit Scaling. It is purely a consequence of the pre-norm residual structure. However, only under Unit Scaling can the factors τ l be interpreted as the ratio of scales between skip and residual branch.</p><p>As a consequence of the lemma, the final residual output R L (x) is the same as in our original network up to a fixed multiplier. Due to the zero-homogeneity of the final output function f L+1 this</p><p>, proving the mathematical equivalence of our residual scheme. Modern LLM architectures like Llama <ref type="bibr" target="#b19">[21]</ref> are pre-norm residual networks of this kind. Hence they admit a faithful unit-scaled reparametrization.</p><p>Proof. This is proved by induction. For the base-case l = 1, we have τ 1 = r 1 /r 0 , giving</p><p>Then if the statement holds for l -1 we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unit Scaling for transformer residuals</head><p>The above scheme describes Unit Scaling for arbitrary pre-norm residual networks. We now apply it to the case of pre-norm transformer residual layers.</p><p>We can describe a transformer in terms of the residual network given in Equations ( <ref type="formula">6</ref>) to (8). Our f l functions alternate between self-attention layers and feed-forward layers. Implementations differ in the handling of how residual multipliers r l correspond to HPs. In many cases practitioners simply ignore these r l , but for the sake of expressivity we assume the two types of residual layer each have their own HP, as well as the embedding. In other words,</p><p>l is even, and l &gt; 0.</p><p>To convert this to a Unit Scaled network we apply Equations ( <ref type="formula">15</ref>) to <ref type="bibr" target="#b16">(18)</ref>, from which can derive the following closed-form expression for τ l :</p><p>where ℓ = ⌊ l-1 2 ⌋. This gives us a unit-scaled pre-norm residual implementation for a standard transformer, which is mathematically equivalent to a non-unit-scaled version. In the next section we augment this by adding in two HPs, in a carefully-designed manner that satisfies our criteria for u-µP HPs, giving us our full residual implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Justifying the u-µP hyperparameter scheme</head><p>Here we justify our particular choice of u-µP HP, as given in Table <ref type="table">3</ref> (with their placement defined in Table <ref type="table">8</ref>). We discuss this topic briefly in Section 4.3, stating that all our HPs (excepting the LR) are α HPs, and under u-µP they are now associated with operations instead of weights. All operations have an α HPs, unless they are unary and k-homogeneous for k ≥ 0.</p><p>We begin this section by explaining why we apply this rule to the model and how it results in three of our u-µP HPs. We then consider how best to hyperparametrize our residual layers, building on our criteria for HPs given in Section 4.3 and the unit-scaled pre-norm residual scheme in Appendix F.</p><p>G.1 Multipliers for non-homogeneous ops: α attn-softmax , α ffn-act , α loss-softmax</p><p>In this section we derive the rest of our u-µP multipliers. We want to identify the minimal set that can still express all different choices of pre-op scales in the model. The crucial observation is that every pre-scale multiplier α of a unary operation h → f (αh) can be propagated through the network if f is k-homogeneous for some k &gt; 0, i.e. f (αx) = α k f (x), leaving the model and its optimization unchanged. We can iterate this along the computational path until either the next operation is non-homogeneous, non-unary (we are at the end of a residual path), or the next operation is 0-homogeneous (e.g. a norm).</p><p>In the first case the accumulated scales are absorbed in the pre-op scale of the non-homogeneous operation (where we introduce a multiplier), in the second case they are absorbed in the residual addition for that branch (where we again introduce a multiplier), and in the final case the scale disappears (so we start over). We now go through the Llama forward computation and follow this paradigm to identify our multipliers in Table <ref type="table">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Residual branch multipliers: α res , α res-attn-ratio</head><p>In this section we derive our two u-µP residual HPs. We start with the basic, non-unit scaled model we began with in the previous section, outlined in Equations ( <ref type="formula">6</ref>) to (8). We described a set of α emb , α attn-residual , α ffn-residual HPs associated with this model in Appendix F.4. However these HPs poorly satisfy our cardinality, independence and interpretability criteria from Section 4.3, so in the Appendix G.2.1 we present a re-parametrization of these HPs designed to better satisfy these points. In Appendix G.2.2 we then combine these HPs with the final unit-scaled pre-norm residual scheme we derived in Appendix F, resulting in our complete u-µP residual scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.1 Improved hyperparameters for transformer residuals</head><p>To avoid cluttered notation, in this section we rename α res = α r , α res-attn-ratio = α ρ α emb = α e , α attn-residual = α a α ffn-residual = α f .</p><p>To make the presentation more clear, we derive our new HPs using the standard residual scheme from Equations ( <ref type="formula">6</ref>) to (8). For the actual unit scaled implementation one needs to transform the multipliers following Equations ( <ref type="formula">15</ref>) to <ref type="bibr" target="#b16">(18)</ref>, which we do in Section G.2.2. This gives R(e) L = α e x/α e = x, removing our first occurrence of α e . Following the division through R(a) L and R(f) L , we obtain:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(a)</head><p>This system of equations is the same as the original, but with the two α e terms dropped, meaning our model's multipliers can be expressed in terms of only α r and α ρ . Using the above equations, any pair of values for (α r , α ρ ) can be translated back into an equivalent set of values for (α e , α a , α f ) such that the output R L+1 (x) is the same, meaning that our multipliers are no less expressive than the original set. This satisfies our desired criteria of minimizing the number of multipliers while maintaining expressivity.</p><p>We can simplify further in the case of unit-scaled models, which are designed such that σ(x), σ a , σ f are all 1 at initialization. In this case our re-parametrization becomes:</p><p>This is the basis of our claim that Unit Scaling is what enables a more intuitive set of multipliers. Not only do the multipliers α r and α ρ represent important dynamics in the network at initialization (the ratio of residual-to-embedding scales, and the ratio of attention-to-feed-forward scales), but it's only via unit scaling that these equations become simple enough to implement in practice. Using equations Equations ( <ref type="formula">19</ref>) to <ref type="bibr" target="#b19">(21)</ref> for a non-unit scaled network may still be effective, but the interpretation we've given to α r and α ρ no longer hold.</p><p>Our final desired property is an empirical one: that the most effective choice of one multiplier depends as little as possible on the choice of the other multiplier(s). We demonstrate that our multipliers satisfy this property better than the standard set of residual multipliers in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.2 The full u-µP residual scheme</head><p>Here we give the full definition of our u-µP residual scheme, summarizing the results of previous sections. A general pre-norm transformer is implemented as: R 0 (x) = c x, (22) R l (x) = a l f l (R l-1 (x)) + b l R l-1 (x), l = 1, .., L</p><p>where a l , b l and c are scalar multipliers, and the f l alternate between self-attention and feed-forward layers. We consider our baseline set of µP residual HPs here to be (α emb , α attn-residual , α ffn-residual ),</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor programs IV: Feature learning in infinite-width neural networks</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11727" to="11737" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tensor programs V: Tuning large neural networks via zero-shot hyperparameter transfer</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>CoRR, abs/2203.03466</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cerebras-GPT: Open compute-optimal language models trained on the cerebras wafer-scale cluster</title>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurpreet</forename><surname>Gosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemant</forename><surname>Khachane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ribhu</forename><surname>Pathria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<idno>CoRR, abs/2304.03208</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BTLM-3B-8K: 7B parameter performance in a 3B parameter model</title>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Al-Khateeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ribhu</forename><surname>Pathria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemant</forename><surname>Khachane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaheer</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">Robert</forename><surname>Steeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Vassilieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<idno>CoRR, abs/2309.11568</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards fully transparent open-source LLMs</title>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Pangarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajri</forename><surname>Koto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuguang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Iriondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cun</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>CoRR, abs/2312.06550</idno>
	</analytic>
	<monogr>
		<title level="j">LLM</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unveiling the potential of small language models with scalable training strategies</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuge</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoqun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Leng Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Minicpm</surname></persName>
		</author>
		<idno>CoRR, abs/2404.06395</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">GPT-4 technical report</title>
		<idno>CoRR, abs/2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A large-scale exploration of µ-transfer</title>
		<author>
			<persName><forename type="first">Lucas</forename><forename type="middle">D</forename><surname>Lingle</surname></persName>
		</author>
		<idno>CoRR, abs/2404.05728</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The Falcon series of open language models</title>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulaziz</forename><surname>Alshamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mérouane</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Étienne</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno>CoRR, abs/2311.16867</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unit scaling: Out-of-the-box low-precision training</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Luschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jonathan</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023-07-29">23-29 July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="2548" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unit scaling</title>
		<author>
			<persName><surname>Graphcore</surname></persName>
		</author>
		<ptr target="https://github.com/graphcore-research/unit-scaling" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="8580" to="8589" />
		</imprint>
	</monogr>
	<note>Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tensor programs VI: Feature learning in infinite-depth neural networks</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soufiane</forename><surname>Hayou</surname></persName>
		</author>
		<idno>CoRR, abs/2310.02244</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The end of Moore&apos;s law: A new beginning for information technology</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S. Philip</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dark silicon and the end of multicore scaling</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">R</forename><surname>Hadi Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renée</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Amant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th International Symposium on Computer Architecture (ISCA 2011)</title>
		<editor>
			<persName><forename type="first">Ravi</forename><forename type="middle">R</forename><surname>Iyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Antonio</forename><surname>González</surname></persName>
		</editor>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">June 4-8, 2011. 2011</date>
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nvidia</forename><forename type="middle">Transformer</forename><surname>Engine</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/TransformerEngine" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<idno>CoRR, abs/2309.14322</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How to set adamw&apos;s weight decay as you scale model and dataset size</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Aitchison</surname></persName>
		</author>
		<idno>CoRR, abs/2405.13698</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Maximal update parametrization (µP) and hyperparameter transfer (µTransfer)</title>
		<ptr target="https://github.com/microsoft/mup" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno>CoRR, abs/2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Vassilieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>CoRR, abs/2309.10818</idno>
		<title level="m">SlimPajama-DC: Understanding data combinations for LLM training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<title level="m">Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aobo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archi</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archie</forename><surname>Sravankumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Hinsvark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austen</forename><surname>Gregerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ava</forename><surname>Spataru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Roziere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethany</forename><surname>Biron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobbie</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Caucheteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaya</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Marra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Touret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinne</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Allonsius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Pintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Livshits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Perino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egor</forename><surname>Lakomkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehab</forename><surname>Albadawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elina</forename><surname>Lobanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Eric Michael Smith, Filip Radenovic</publisher>
		</imprint>
	</monogr>
	<note>and Frank Zhang et al. The llama 3 herd of models</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Training and inference of large language models using 8-bit floating point</title>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levy-Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Balanca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Luschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<idno>CoRR, abs/2309.17224</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training deep neural networks with 8-bit floating point numbers</title>
		<author>
			<persName><forename type="first">Naigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><surname>Editors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="7686" to="7695" />
		</imprint>
	</monogr>
	<note>Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mixed precision training with 8-bit floating point</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Mellempudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudarshan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905">1905.12334, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingcheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaosen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>training FP8 large language models. CoRR, abs/2310.18313</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">With shared microexponents, a little shifting goes a long way</title>
		<author>
			<persName><forename type="first">Darvish</forename><surname>Bita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritchie</forename><surname>Rouhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venmugil</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasoul</forename><surname>Elango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Shafipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maral</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Mesmakhosroshahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levi</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Melnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai</forename><surname>Varatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitry</forename><surname>Kolhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Melts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renee L'</forename><surname>Klar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Heureux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Zhaoxia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Summer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Naghshineh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><surname>Naumov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual International Symposium on Computer Architecture, ISCA 2023</title>
		<editor>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Heinrich</surname></persName>
		</editor>
		<meeting>the 50th Annual International Symposium on Computer Architecture, ISCA 2023<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">June 17-21, 2023. 2023</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Microscaling data formats for deep learning</title>
		<author>
			<persName><forename type="first">Darvish</forename><surname>Bita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritchie</forename><surname>Rouhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Summer</forename><surname>Khodamoradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Cornea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Dellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dusan</forename><surname>Denolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venmugil</forename><surname>Stosic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Elango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharmesh</forename><surname>James-Roxby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Jani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kolhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Langhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maral</forename><surname>Melnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Mesmakhosroshahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasoul</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shafipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Verilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Wittig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><surname>Chung</surname></persName>
		</author>
		<idno>CoRR, abs/2310.10537</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bitnet: Scaling 1-bit transformers for large language models</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2310.11453</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The era of 1-bit LLMs: All large language models are in 1.58 bits</title>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2402.17764</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scalable matmul-free language modeling</title>
		<author>
			<persName><forename type="first">Rui-Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Sifferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Sheaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">K</forename><surname>Eshraghian</surname></persName>
		</author>
		<idno>CoRR, abs/2406.02528</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to 22 billion parameters</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">Peter</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">Riquelme</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gamaleldin</forename><surname>Fathy Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fantine</forename><surname>Huot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmijn</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><forename type="middle">Nader</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Pavetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><forename type="middle">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jonathan</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="7480" to="7512" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="2023">2023</date>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stable and low-precision training for large-scale vision-language models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intriguing properties of quantization at scale</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Ahmadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Venkitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Zhen Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. December 10 -16, 2023, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Analyzing and improving the training dynamics of diffusion models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<idno>CoRR, abs/2312.02696</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">8-bit matrix multiplication for transformers at scale</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sanmi Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Danielle</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-11-28">2022. 2022. November 28 -December 9, 2022, 2022</date>
		</imprint>
	</monogr>
	<note>Gpt3.int8(</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Smoothquant: Accurate and efficient post-training quantization for large language models</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mickaël</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jonathan</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023-07-29">23-29 July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="38087" to="38099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Quantizable transformers: Removing outliers by helping attention heads do nothing</title>
		<author>
			<persName><forename type="first">Yelysei</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. December 10 -16, 2023, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Massive activations in large language models</title>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Xinlei Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2402.17762</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vision transformers need registers</title>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. OpenReview.net, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scalable optimization in the modular norm</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyojin</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<idno>CoRR, abs/2405.14813</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Katie</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Scaling exponents across parameterizations and optimizers</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sparse maximal update parameterization: A holistic approach to sparse training dynamics</title>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<idno>CoRR, abs/2405.15743</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Compute better spent: Replacing dense layers with structured matrices</title>
		<author>
			<persName><forename type="first">Shikai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Potapczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno>CoRR, abs/2406.06248</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</author>
		<editor>Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Olmo: Accelerating the science of language models</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuling</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15789" to="15809" />
		</imprint>
	</monogr>
	<note>ACL 2024</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinzheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfeng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ru</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Scaling laws and compute-optimal training beyond fixed training durations</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Bakouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atli</forename><surname>Kosson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno>CoRR, abs/2405.18392</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Straight to zero: Why linearly decaying the learning rate to zero works best for LLMs</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to The Thirteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>under review</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ZeRO: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<editor>
			<persName><forename type="first">Christine</forename><surname>Cuicchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Irene</forename><surname>Qualters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Kramer</surname></persName>
		</editor>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis<address><addrLine>SC; Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-09">2020. November 9-19, 2020. 2020</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sanmi Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Danielle</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-11-28">2022. 2022. November 28 -December 9, 2022, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">GLU variants improve transformer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR, abs/2002.05202</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Symmetrical Gaussian error linear units (SGELUs)</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Su</surname></persName>
		</author>
		<idno>CoRR, abs/1911.03925</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Searching for activation functions</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="12360" to="12371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Tensor programs I: Wide feedforward or recurrent neural networks of any architecture are Gaussian processes</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910">1910.12478, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Tensor programs II: Neural tangent kernel for any architecture</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006.14548, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tensor programs IIb: Architectural universality of neural tangent kernel training dynamics</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etai</forename><surname>Littwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11762" to="11772" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Tensor programs III: Neural matrix laws</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009.10685, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Tensor programs IVb: Adaptive optimization in the infinite-width limit</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etai</forename><surname>Littwin</surname></persName>
		</author>
		<idno>CoRR, abs/2308.01814</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A spectral condition for feature learning</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<idno>CoRR, abs/2310.17813</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Sparse maximal update parameterization: A holistic approach to sparse training dynamics</title>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<idno>CoRR, abs/2405.15743</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m">IEEE standard for floating-point arithmetic</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="1" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="4901" to="4910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">8-bit numerical formats for deep neural networks</title>
		<author>
			<persName><forename type="first">Badreddine</forename><surname>Noune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Justus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Luschi</surname></persName>
		</author>
		<idno>CoRR, abs/2206.02915</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dusan</forename><surname>Stosic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cornea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Grisenthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwon</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kamalu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Mellempudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><forename type="middle">F</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Y</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR, abs/2209.05433</idno>
	</analytic>
	<monogr>
		<title level="j">FP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficient large-scale language model training on GPU clusters using Megatron-LM</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Bronis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mary</forename><forename type="middle">W</forename><surname>De Supinski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Todd</forename><surname>Hall</surname></persName>
		</editor>
		<editor>
			<persName><surname>Gamblin</surname></persName>
		</editor>
		<meeting><address><addrLine>SC; St. Louis, Missouri, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-11-14">2021. November 14-19, 2021. 2021</date>
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">OpenSeq2Seq: Extensible toolkit for distributed and mixed precision training of sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno>CoRR, abs/1805.10387</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Using FP8 with transformer engine</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
