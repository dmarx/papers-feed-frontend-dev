# The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations, and Anomalous Diffusion

## Abstract

## 

In this work we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance travelled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction between the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase space dynamics of the parameters and their instantaneous velocities from initialization to stationarity. Using the Fokker-Planck equation, we show that the key ingredient driving these dynamics is not the original training loss, but rather the combination of a modified loss, which implicitly regularizes the velocity, and probability currents, which cause oscillations in phase space. We identify qualitative and quantitative predictions of this theory in the dynamics of a ResNet-18 model trained on ImageNet. Through the lens of statistical physics, we uncover a mechanistic origin for the anomalous limiting dynamics of deep neural networks trained with SGD.

## Introduction

Deep neural networks have demonstrated remarkable generalization across a variety of datasets and tasks. Essential to their success has been a collection of good practices on how to train these models with stochastic gradient descent (SGD). Yet, despite their importance, these practices are mainly based on heuristic arguments and trial and error search. Without a general theory connecting the hyperparameters of optimization, the architecture of the network, and the geometry of the dataset, theory-driven design of deep learning systems is impossible. Existing theoretical works studying this interaction have leveraged the random structure of neural networks at initialization and in their infinite width limits in order to study their dynamics [Schoenholz et](#)  Rotskoff and Vanden-Eijnden (2018); [Chizat and Bach (2018)](#b7). Here we take a different approach and study the training dynamics of pre-trained networks that are ready to be used for inference. By leveraging the mathematical structures found at the end of training, we uncover an intricate interaction between the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix that corroborates previously identified empirical behavior such as anomalous limiting dynamics. Understanding the limiting dynamics of SGD is a critical stepping stone to building a complete theory for the learning dynamics of neural networks. Additionally, a series of recent works have demonstrated that the performance of pre-trained networks can be improved through averaging and ensembling their parameters [Garipov et al. (2018)](#b10); [Izmailov et al. (2018)](#b16); [Maddox et al. (2019)](#b34). Thus, the learning dynamics of neural networks, even at the end of training, are still quite mysterious and important to the final performance of the net-work. Combining empirical exploration and theoretical tools from statistical physics, we identify and uncover a mechanistic explanation for the limiting dynamics of neural networks trained with SGD. Our work is organized as follows:

1. We discuss related work which we build upon and delineate our contributions (section 3).

2. We demonstrate empirically that long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion where distance travelled grows as a power law in the number of steps with a nontrivial exponent (section 2).

3. To understand this empirical behavior, we derive a continuous-time model for SGD as an underdamped Langevin equation, accounting for the discretization error due to finite learning rates and gradient noise introduced by stochastic batches (section 4). [4](#b3). We show that for linear regression, these dynamics give rise to an Ornstein-Uhlenbeck process whose moments can be derived analytically as the sum of damped harmonic oscillators in the eigenbasis of the data (section 5). [5](#b4). We prove via the Fokker-Planck equation that the stationary distribution for this process is a Gibbs distribution on a modified (not the original) loss, which breaks detailed balance and gives rise to non-zero probability currents in phase space (section 6). [6](#b5). We demonstrate empirically that the limiting dynamics of a ResNet-18 model trained on ImageNet display these qualitative characteristics -no matter how anisotropic the original training loss, the limiting trajectory of the network will behave isotropically (section 7).

7. We derive theoretical expressions for the influence of the learning rate, batch size, and momentum coefficient on the limiting instantaneous speed of the network and the anomalous diffusion exponent, which quantitatively match empirics exactly (section 8).

## Diffusive Behavior in the Limiting Dynamics of SGD

Even after a neural network trained by SGD has reached its optimal performance, further gradient steps will cause it to continue to move through parameter space Jastrzębski 

$δ k = θ k -θ k-1 , ∆ k = θ k -θ * .(1)$As shown in Fig. [1](#fig_3), neither of these differences converge to zero across a variety of architectures, indicating that despite performance convergence, the networks continue to move through parameter space, both locally and globally. The squared norm of the local displacement ∥δ k ∥ 2 2 remains near a constant value, indicating the network is essentially moving at a constant instantaneous speed. This observation is quite similar  to the "equilibrium" phenomenon or "constant angular update" observed in Li et al.

(2020) and [Wan et al. (2020)](#b55) respectively. However, these works only studied the displacement for parameters immediately preceding a normalization layer. The constant instantaneous speed behavior we observe is for all parameters in the model and is even present in models without normalization layers.

While the squared norm of the local displacement is essentially constant, the squared norm of the global displacement ∥∆ k ∥ 2 2 is monotonically growing for all networks, implying even once trained, the network continues to diverge from where it has been. Indeed Fig. [1](#fig_3) indicates a power law relationship between global displacement and number of steps, given by ∥∆ k ∥ 2 2 ∝ k c . As we'll see in section 8, this relationship is indicative of anomalous diffusion where c corresponds to the anomalous diffusion exponent.

The term anomalous diffusion is used to distinguish it from standard Brownian motion, where the relationship between distance travelled and number of updates is linear. ∝ log(k), a form of ultra-slow diffusion. These empirical observations raise the natural questions, where is the network moving to and why? To answer these questions we will build a diffusion based theory of SGD, study these dynamics in the setting of linear regression, and use lessons learned in this fundamental setting to understand the limiting dynamics of neural networks.

Stan
## Related Work

There is a long line of literature studying both theoretically and empirically the learning 

$v k+1 = βv k -g(θ k ) -λθ k , θ k+1 = θ k + ηv k+1 ,(2)$where we initialize the network such that v 0 = 0 and θ 0 is the parameter initialization.

In order to understand the dynamics of the network through position and velocity space, which we will refer to as phase space, we express these discrete recursive equations as the discretization of some unknown ordinary differential equation (ODE). By incorporating a previous time step θ k-1 , we can rearrange the two update equations into the finite difference discretization,

$θ k+1 -θ k η Forward Euler -β θ k -θ k-1 η Backward Euler +λθ k = -g(θ k ).(3)$Forward and backward Euler discretizations are explicit and implicit discretizations respectively of the first order temporal derivative θ = d dt θ. Simply replacing the discretizations with the derivative θ would generate an inaccurate first-order model for the discrete process. Like all discretizations, the Euler discretizations introduce higherorder error terms proportional to the step size, which in this case are proportional to η 2 θ. These second-order error terms are commonly referred to as artificial diffusion, as they are not part of the original first-order ODE being discretized, but introduced by the discretization process. Incorporating the artificial diffusion terms into the first-order ODE, we get a second-order ODE, sometimes referred to as a modified equation as in Kovachki and Stuart (2019); Kunin et al. (2020), describing the dynamics of gradient descent

$η 2 (1 + β) θ + (1 -β) θ + λθ = -g(θ).(4)$While this second-order ODE models the gradient descent process, even at finite learning rates, it fails to account for the stochasticity introduced by choosing a random batch B of size S drawn uniformly from the set of N training points. This sampling yields the stochastic gradient g B (θ) = 1 S i∈B ∇ℓ(θ, x i ). To model this effect, we make the following assumption:

Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such that g B (θ)-g(θ) is a Gaussian random variable with mean 0 and covariance

$1 S Σ(θ).$This is a commonly applied analytic tool for handling noise introduced in stochastic algorithms, although certain works have questioned the correctness of this assumption, which we discuss further in appendix A. Incorporating this assumption of stochastic gradients into the previous finite difference equation and applying the stochastic counterparts to Euler discretizations, results in the second-order stochastic differential equation (SDE),

$η 2 (1 + β) θ + (1 -β) θ + λθ = -g(θ) + η S Σ(θ)ξ(t),(5)$where ξ(t) represents a fluctuating force. An equation of this form is commonly referred to as an underdamped Langevin equation and has a natural physical interpretation as the equation of motion for a particle moving in a potential field with a fluctuating force. In particular, the mass of the particle is η 2 (1 + β), the friction constant is (1β), the potential is the regularized training loss L(θ) + λ 2 ∥θ∥ 2 , and the fluctuating force is introduced by the gradient noise. While this form for the dynamics provides useful intuition, we must expand back into phase space in order to write the equation in the standard drift-diffusion form for an SDE,

$d     θ v     =     v -2 η(1+β) (g(θ) + λθ + (1 -β)v)     dt +     0 0 0 2 √ ηS(1+β) Σ(θ)     dW t ,(6)$where W t is a standard Wiener process. This is the continuous model we will study in this work:

Assumption 2 (SDE). We assume the underdamped Langevin equation ( [6](#formula_6)) accurately models the trajectory of the network driven by SGD through phase space such that

$θ(ηk) ≈ θ k and v(ηk) ≈ v k .$See appendix A for further discussion on the nuances of modeling SGD with an SDE.

5 Linear Regression with SGD is an Ornstein-Uhlenbeck

## Process

Equipped with a model for SGD, we seek to understand its dynamics in the fundamental setting of linear regression, one of the few cases where we have a complete model for the interaction of the dataset, architecture, and optimizer. Let X ∈ R N ×d be the input data, Y ∈ R N be the output labels, and θ ∈ R d be our vector of regression coefficients.

The least squares loss is the convex quadratic loss

$L(θ) = 1 2N ∥Y -Xθ∥ 2 with gradient g(θ) = Hθ -b, where H = X ⊺ X N and b = X ⊺ Y N .$Plugging this expression for the gradient into the underdamped Langevin equation ( [6](#formula_6)), and rearranging terms, results in the multivariate SDE,

$d     θ t v t     = -A         θ t v t     -     µ 0         dt + 2κ -1 D(θ)dW t ,(7)$where A and D(θ) are the drift and diffusion matrices respectively,

$A =     0 -I 2 η(1+β) (H + λI) 2(1-β) η(1+β) I     , D =     0 0 0 2(1-β) η(1+β) Σ(θ)     ,(8)$$κ = S(1 -β 2$) is an inverse temperature constant, and µ = (H + λI) -1 b is the ridge regression solution. In order to gain exact expressions for the dynamics of this SDE, we introduce the following assumption on the covariance of the gradient noise:

Assumption 3 (Covariance Structure). We assume the covariance of the gradient noise is spatially independent Σ(θ) = Σ and proportional to the Hessian of the least squares loss Σ = σ 2 H where σ ∈ R + is some unknown scalar.

This assumption, although strong, is actually quite natural when we assume our data is generated from a noisy, linear teacher model. If our training labels are generated as y i = x ⊺ i θ + σϵ i where, θ ∈ R d is the teacher model and ϵ i ∼ N (0, 1) is Gaussian noise, then it is not difficult to show that Σ(θ) ≈ σ 2 H. See appendix B for a complete derivation and discussion.

The result of assumption 3 is that the SDE equation 7 takes the form of a multivariate Ornstein-Uhlenbeck (OU) process. The solution to an OU process is a Gaussian process. By solving for the temporal dynamics of the first and second moments of the process, we can obtain an analytic expression for the trajectory at any time t. In particular, we can decompose the trajectory as the sum of a deterministic and stochastic component defined by the first and second moments respectively. The general solution for a multivariate OU process is derived in appendix D and explicit expressions for the solution of equation 7 are derived in appendix F.

Step Velocity

## Position

Step Velocity Position Eq. ( [9](#formula_12))

Figure [2](#): Oscillatory dynamics in linear regression. We train a linear network to perform regression on the CIFAR-10 dataset by using an MSE loss on the one-hot encoding of the labels. We compute the hessian of the loss, as well as its top eigenvectors.

The position and velocity trajectories are projected onto the first eigenvector of the hessian and visualized in black. The theoretically derived mean, equation [(9)](#b8), is shown in red. The top and bottom panels demonstrate the effect of varying momentum on the oscillation mode.

Deterministic component. Using the form of A in equation [(8)](#b7) we can decompose the expectation as a sum of harmonic oscillators in the eigenbasis {q 1 , . . . , q m } of the Hessian,

$E         θ t v t         =     µ 0     + m i=1     a i (t)     q i 0     + b i (t)     0 q i         .(9)$Here the coefficients a i (t) and b i (t) depend on the optimization hyperparameters η, β, λ, S and the respective eigenvalue of the Hessian ρ i as further explained in appendix F. We verify this expression nearly perfectly matches empirics on complex datasets under various hyperparameter settings as shown in Fig. [2](#).

Stochastic component. The cross-covariance of the process between two points in time

$t ≤ s, is Cov         θ t v t     ,     θ s v s         = κ -1 B -e -At Be -A ⊺ t e A ⊺ (t-s) ,(10)$where B solves the Lyapunov equation AB + BA ⊺ = 2D. In the limit as t → ∞, the process approaches a stationary solution,

$p ss = N         µ 0     , κ -1 B     ,(11)$with stationary cross-covariance Cov ss = κ -1 Be A ⊺ |t-s| .

## Understanding Stationarity via the Fokker-Planck Equation

The OU process is unique in that it is one of the few SDEs which we can solve exactly.

As shown in section 5, we were able to derive exact expressions for the dynamics of linear regression trained with SGD from initialization to stationarity by simply solving for the first and second moments. While the expression for the first moment provides an understanding of the intricate oscillatory relationship in the deterministic component of the process, the second moment, driving the stochastic component, is much more

opaque. An alternative route to solving the OU process that potentially provides more insight is the Fokker-Planck equation.

The Fokker-Planck (FP) equation is a PDE describing the time evolution for the probability distribution of a particle governed by Langevin dynamics. For an arbitrary potential Φ and diffusion matrix D, the Fokker-Planck equation (under an Itô integration prescription) is

$∂ t p = ∇ • ∇Φp + ∇ • κ -1 Dp -J ,(12)$where p represents the time-dependent probability distribution, and J is a vector field commonly referred to as the probability current. The FP equation is especially useful for explicitly solving for the stationary solution, assuming one exists, of the Langevin dynamics. The stationary solution p ss by definition obeys ∂ t p ss = 0 or equivalently ∇ • J ss = 0. From this second definition we see that there are two distinct settings of stationarity: detailed balance when J ss = 0 everywhere, or broken detailed balance when ∇ • J ss = 0 and J ss ̸ = 0.

For a general OU process, the potential is a convex quadratic function Φ(x) = x ⊺ Ax defined by the drift matrix A. When the diffusion matrix is isotropic (D ∝ I) and spatially independent (∇D = 0) the resulting stationary solution is a Gibbs distribution p ss (x) ∝ e -κΦ(x) determined by the original loss Φ(x) and is in detailed balance. Lesser known properties of the OU process arise when the diffusion matrix is anisotropic or spatially dependent [Gardiner et al. (1985)](#b9); [Risken (1996)](#b45). In this setting the solution is still a Gaussian process, but the stationary solution, if it exists, is no longer defined by the Gibbs distribution of the original loss Φ(x), but actually a modified loss Ψ(x).

Furthermore, the stationary solution may be in broken detailed balance leading to a non-zero probability current J ss (x). Depending on the relationship between the drift matrix A and the diffusion matrix D the resulting dynamics of the OU process can have very nontrivial behavior, as shown in Fig. [3](#fig_6).

In the setting of linear regression, anisotropy in the data distribution will lead to anisotropy in the gradient noise and thus an anisotropic diffusion matrix. This implies that for most datasets we should expect that the SGD trajectory is not driven by the original least squares loss, but by a modified loss and converges to a stationary solution with broken detailed balance, as predicted by [Chaudhari and Soatto (2018)](#b5). Using the explicit expressions for the drift A and diffusion D matrices in equation ( [8](#formula_10)) we can compute analytically the modified loss and stationary probability current,

$Ψ(θ, v) =         θ v     -     µ 0         ⊺ U 2         θ v     -     µ 0         , J ss (θ, v) = -QU         θ v     -     µ 0         p ss ,(13)$where Q is a skew-symmetric matrix and U is a positive definite matrix defined as,

$Q =     0 -Σ Σ 0     , U =     2 η(1+β) Σ -1 (H + λI) 0 0 Σ -1     .(14)$These new fundamental matrices, Q and U , relate to the original drift A and diffusion D matrices through the unique decomposition A = (D + Q)U , introduced by Ao (2004) and [Kwon et al. (2005)](#b26). Using this decomposition we can easily show that B = U -1 solves the Lyapunov equation and indeed the stationary solution p ss , de-scribed in equation ( [11](#formula_14)), is the Gibbs distribution defined by the modified loss Ψ(θ, v) in equation [(13)](#b12). Further, the stationary cross-covariance solved in section 5 reflects the oscillatory dynamics introduced by the stationary probability currents J ss (θ, v) in equation [(13)](#b12). Taken together, we gain the intuition that the limiting dynamics of SGD in linear regression are driven by a modified loss subject to oscillatory probability currents.

## Evidence of a Modified Loss and Oscillations in Deep Learning

Does the theory derived in the linear regression setting (sections 5, 6) help explain the empirical phenomena observed in the non-linear setting of deep neural networks (section 2)? In order for the theory built in the previous sections to apply to the limiting dynamics of neural networks, we must introduce the following simplifying assumption on the loss landscape at the end of training:

Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network can be approximated by the quadratic loss

$L(θ) = (θ -µ) ⊺ H 2 (θ -µ),$where H ⪰ 0 and µ is some unknown mean vector, corresponding to a local minimum.

This assumption is strong, but again quite natural given our study of the limiting dynamics of the network. See appendix C for a discussion on the motivations and limitations of this assumption.

Under assumption 4, then the expressions derived in the linear regression setting would apply to the limiting dynamics of deep neural networks and depend only on quantities that we can easily estimate empirically. Of course, these simplifications are strong, but as discussed in appendix A, B, and C also quite natural. Furthermore, we can empirically test their qualitative implications: (1) a modified isotropic loss driving the limiting dynamics through parameter space, (2) implicit regularization of the velocity trajectory, and (3) oscillatory phase space dynamics determined by the Hessian eigenstructure.

Modified loss. As discussed in section 6, due to the anisotropy of the diffusion matrix, the loss landscape driving the dynamics at the end of training is not the original training loss L(θ), but a modified loss Ψ(θ, v) in phase space. As shown in equation ( [13](#formula_16)), the modified loss decouples into a term Ψ θ that only depends on the parameters θ and a term Ψ v that only depends on the velocities v. Under assumption 3, the parameter dependent component is proportional to the convex quadratic,

$Ψ θ ∝ (θ -µ) ⊺ H -1 (H + λI) η(1 + β) (θ -µ) .(15)$This quadratic function has the same mean µ as the training loss, but a different curvature. Using this expression, notice that when λ ≈ 0, the modified loss is isotropic in the column space of H, regardless of what the nonzero eigenspectrum of H is. This striking prediction suggests that no matter how anisotropic the original training lossas reflected by poor conditioning of the Hessian eigenspectrum -the training trajectory of the network will behave isotropically, since it is driven not by the original anisotropic loss, but a modified isotropic loss.

We test this prediction by studying the limiting dynamics of a pre-trained ResNet-18 model with batch normalization that we continue to train on ImageNet according to the last setting of its hyperparameters [He et al. (2016)](#b14). Let θ * represent the initial pretrained parameters of the network, depicted with the white dot in figures 4 and 5. We estimate 1 the top thirty eigenvectors q 1 , . . . , q 30 of the Hessian matrix H * evaluated at θ * and project the limiting trajectory for the parameters onto the plane spanned by the top q 1 and bottom q 30 eigenvectors to maximize the illustrated anisotropy with our estimates. We sample the train and test loss in this subspace for a region around the projected trajectory. Additionally, using the hyperparameters of the optimization, the eigenvalues ρ 1 and ρ 30 , and the estimate for the mean µ = θ * -H -1 * g * (g * is the gradient evaluated at θ * ), we also sample from the modified loss equation [(15)](#b14) in the same region.

Figure [4](#fig_15) shows the projected parameter trajectory on the sampled train, test and modified losses. Contour lines of both the train and test loss exhibit anisotropic structure, with sharper curvature along eigenvector q 1 compared to eigenvector q 30 , as expected.

However, as predicted, the trajectory appears to cover both directions equally. This striking isotropy of the trajectory within a highly anisotropic slice of the loss landscape indicates qualitatively that the trajectory evolves in a modified isotropic loss landscape.

## Implicit velocity regularization.

A second qualitative prediction of the theory is that the velocity is regulated by the inverse Hessian of the training loss. Of course there are no explicit terms in either the train or test losses that depend on the velocity. Yet, the modified loss contains a component, Ψ v , that only depends on the velocities,

$Ψ v ∝ v ⊺ H -1 v. (16$$)$This additional term can be understood as a form of implicit regularization on the velocity trajectory. Indeed, when we project the velocity trajectory onto the plane spanned 1 To estimate the eigenvectors of H * we use subspace iteration, and limit ourselves to 30 eigenvectors to constrain computation time. See appendix I for details.          by the q 1 and q 30 eigenvectors, as shown in Fig. [5](#fig_16), we see that the trajectory closely resembles the curvature of the inverse Hessian H -1 . The modified loss is effectively penalizing SGD for moving in eigenvectors of the Hessian with small eigenvalues. A similar qualitative effect was recently proposed by Barrett and Dherin (2020) as a consequence of the discretization error due to finite learning rates.

$A D C s V J 9 x 0 7 A y 7 A E R j j N a 2 6 q a I L J G A 9 p X 6 P A E V V e N r 0 h t 4 6 1 M 7 D C W O o n w J q 6 v y c y H C k 1 i Q L d W W y s 5 m u F + V + t n 0 J 4 4 W V M J C l Q Q W Y f h S m 3 I L a K Q K w B k 5 Q A n 2 j A R D K 9 q 0 V G W G I C O r a a D s G Z P 3 k R O q c N R / P t W b 1 5 W c Z R R Y f o C J 0 g B 5 2 j J r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m r R W j n N l H f 2 R$$A D C s V J 9 x 0 7 A y 7 A E R j j N a 2 6 q a I L J G A 9 p X 6 P A E V V e N r 0 h t 4 6 1 M 7 D C W O o n w J q 6 v y c y H C k 1 i Q L d W W y s 5 m u F + V + t n 0 J 4 4 W V M J C l Q Q W Y f h S m 3 I L a K Q K w B k 5 Q A n 2 j A R D K 9 q 0 V G W G I C O r a a D s G Z P 3 k R O q c N R / P t W b 1 5 W c Z R R Y f o C J 0 g B 5 2 j J r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m r R W j n N l H f 2 R$$A D C s V J 9 x 0 7 A y 7 A E R j j N a 2 6 q a I L J G A 9 p X 6 P A E V V e N r 0 h t 4 6 1 M 7 D C W O o n w J q 6 v y c y H C k 1 i Q L d W W y s 5 m u F + V + t n 0 J 4 4 W V M J C l Q Q W Y f h S m 3 I L a K Q K w B k 5 Q A n 2 j A R D K 9 q 0 V G W G I C O r a a D s G Z P 3 k R O q c N R / P t W b 1 5 W c Z R R Y f o C J 0 g B 5 2 j J r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m r R W j n N l H f 2 R$$A D C s V J 9 x 0 7 A y 7 A E R j j N a 2 6 q a I L J G A 9 p X 6 P A E V V e N r 0 h t 4 6 1 M 7 D C W O o n w J q 6 v y c y H C k 1 i Q L d W W y s 5 m u F + V + t n 0 J 4 4 W V M J C l Q Q W Y f h S m 3 I L a K Q K w B k 5 Q A n 2 j A R D K 9 q 0 V G W G I C O r a a D s G Z P 3 k R O q c N R / P t W b 1 5 W c Z R R Y f o C J 0 g B 5 2 j J r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m r R W j n N l H f 2 R$$C q q R Q w z O / e v 2 I J Q G E y A R V q u v Y M b o p l c i Z g K z S S x T E l I 3 p E L o a Q x q A c t P p C Z l 1 r J 2 + N Y i k f i F a U / f 3 R E o D p S a B r z v z h d V 8 L T f / q 3 U T H F y 4 K Q / j B C F k s 4 8 G i b A w s v I 8 r D 6 X w F B M N F A m u d 7 V Y i M q K U O d W k W H 4 M y f v A i t 0 5 q j + f a s W r 8 s 4 i i T Q 3 J E T o h D z k m d X J M G a R J G H s k z e S V$$C q q R Q w z O / e v 2 I J Q G E y A R V q u v Y M b o p l c i Z g K z S S x T E l I 3 p E L o a Q x q A c t P p C Z l 1 r J 2 + N Y i k f i F a U / f 3 R E o D p S a B r z v z h d V 8 L T f / q 3 U T H F y 4 K Q / j B C F k s 4 8 G i b A w s v I 8 r D 6 X w F B M N F A m u d 7 V Y i M q K U O d W k W H 4 M y f v A i t 0 5 q j + f a s W r 8 s 4 i i T Q 3 J E T o h D z k m d X J M G a R J G H s k z e S V$$C q q R Q w z O / e v 2 I J Q G E y A R V q u v Y M b o p l c i Z g K z S S x T E l I 3 p E L o a Q x q A c t P p C Z l 1 r J 2 + N Y i k f i F a U / f 3 R E o D p S a B r z v z h d V 8 L T f / q 3 U T H F y 4 K Q / j B C F k s 4 8 G i b A w s v I 8 r D 6 X w F B M N F A m u d 7 V Y i M q K U O d W k W H 4 M y f v A i t 0 5 q j + f a s W r 8 s 4 i i T Q 3 J E T o h D z k m d X J M G a R J G H s k z e S V$$C q q R Q w z O / e v 2 I J Q G E y A R V q u v Y M b o p l c i Z g K z S S x T E l I 3 p E L o a Q x q A c t P p C Z l 1 r J 2 + N Y i k f i F a U / f 3 R E o D p S a B r z v z h d V 8 L T f / q 3 U T H F y 4 K Q / j B C F k s 4 8 G i b A w s v I 8 r D 6 X w F B M N F A m u d 7 V Y i M q K U O d W k W H 4 M y f v A i t 0 5 q j + f a s W r 8 s 4 i i T Q 3 J E T o h D z k m d X J M G a R J G H s k z e S V$Phase space oscillations. A final implication of the theory is that at stationarity the network is in broken detailed balance leading to non-zero probability currents flowing through phase space:

$J ss (θ, v) =     v -2 η(1+β) (H + λI) (θ -µ)     p ss .(17)$Notice, that while the joint phase space stationary distribution of the network is in broken detailed balance, its marginal distribution in either position or velocity space is actually in detailed balance. As such, the probability currents encourage oscillatory dynamics in the phase space planes characterized by the eigenvectors of the Hessian, at rates proportional to their eigenvalues. We consider the same projected trajectory of the ResNet-18 model visualized in figures 4 and 5, but plot the trajectory in phase space for the two eigenvectors q 1 and q 30 separately. Shown in Fig. [6](#), we see that both trajectories look like noisy clockwise rotations. Qualitatively, the trajectories for the different eigenvectors appear to be rotating at different rates.

The integral curves of the stationary probability current are one-dimensional paths confined to level sets of the modified loss. These paths might cross themselves, in which case they are limit cycles, or they could cover the entire surface of the level sets, in which case they are space-filling curves. This distinction depends on the relative fre-

Step Velocity

## Position

Step Velocity Position Figure [6](#): Phase space oscillations are determined by the eigendecomposition of the Hessian. We visualize the projected position and velocity trajectories in phase space. The top and bottom panels show the projections onto q 1 and q 30 respectively.

Oscillations at different rates are distinguishable for the different eigenvectors and were verified by comparing the dominant frequencies in the fast Fourier transform of the trajectories, as shown in appendix H.

quencies of the oscillations, as determined by the pairwise ratios of the eigenvalues of the Hessian. For real-world datasets, with a large spectrum of incommensurate frequencies, we expect to be in the latter setting, thus contradicting the suggestion that SGD in deep networks converges to limit cycles, as claimed in Chaudhari and Soatto (2018).

8 Understanding the Diffusive Behaviour of the Limiting Dynamics

Taken together the empirical results shown in section 7 indicate that many of the same qualitative behaviors of SGD identified theoretically for linear regression are evident in the limiting dynamics of neural networks. Can this theory quantitatively explain the results we identified in section 2?

Constant instantaneous speed. As noted in section 2, we observed that at the end of training, across various architectures, the squared norm of the local displacement ∥δ t ∥ 2 2 remains essentially constant. Assuming the limiting dynamics are described by the stationary solution in equation [(11)](#b10), the expectation of the local displacement is

$E ss ∥δ t ∥ 2 = η 2 S(1 -β 2 ) σ 2 tr (H) ,(18)$as derived in appendix G. We cannot test this prediction directly as we do not know σ 2 and computing tr(H) is computationally prohibitive. However, we can estimate σ 2 tr(H) by resuming training for a model, measuring the average ∥δ t ∥ 2 , and then inverting equation [(18)](#b17). Using this single estimate, we find that for a sweep of models with varying hyperparameters, equation ( [18](#formula_31)) accurately predicts their instantaneous speed.

Indeed, Fig. [7](#fig_17) shows an exact match between the empirics and theory, which strongly Exponent of anomalous diffusion. The expected value for the global displacement under the stationary solution can also be analytically expressed in terms of the optimization hyperparameters and the eigendecomposition of the Hessian as,

$E ss ∥∆ t ∥ 2 = η 2 S(1 -β 2 ) σ 2 tr (H) t + 2t t k=1 1 - k t m l=1 ρ l C l (k) ,(19)$where C l (k) is a trigonometric function describing the velocity of a harmonic oscillator with damping ratio ζ l = (1β)/ 2η(1 + β) (p l + λ), see appendix G for details.

As shown empirically in section 2, the squared norm ∥∆ t ∥ 2 monotonically increases as a power law in the number of steps, suggesting its expectation is proportional to t c for some unknown, constant c. The exponent c determines the regime of diffusion for the process. When c = 1, the process corresponds to standard Brownian diffusion.

For c > 1 or c < 1 the process corresponds to anomalous super-diffusion or subdiffusion respectively. Unfortunately, it is not immediately clear how to extract the explicit exponent c from equation [(19)](#b18). However, by exploring the functional form of C l (k) and its relationship to the hyperparameters of optimization through the damping ratio ζ l , we can determine overall trends in the diffusion exponent c.

Akin to how the exponent c determines the regime of diffusion, the damping ratio ζ l determines the regime for the harmonic oscillator describing the stationary velocityvelocity correlation in the l th eigenvector of the Hessian. When ζ l = 1, the oscillator is critically damped implying the velocity correlations converge to zero as quickly as possible. In the extreme setting of C l (k) = 0 for all l, k, then equation [(19)](#b18) simplifies to standard Brownian diffusion, E ss [∥∆ t ∥ 2 ] ∝ t. When ζ l > 1, the oscillator is overdamped implying the velocity correlations dampen slowly and remain positive even over long temporal lags. Such long lasting temporal correlations in velocity lead to faster global displacement. Indeed, in the extreme setting of C l (k) = 1 for all l, k, then equation (19) simplifies to a form of anomalous super-diffusion, E ss [∥∆ t ∥ 2 ] ∝ t 2 . When ζ l < 1, the oscillator is underdamped implying the velocity correlations will oscillate quickly between positive and negative values. Indeed, the only way equation (19) could describe anomalous sub-diffusion is if C l (k) took on negative values for certain l, k. 

## Discussion

Through combined empirics and theory based on statistical physics, we uncovered an intricate interplay between the optimization hyperparameters, structure in the gradient noise, and the Hessian matrix at the end of training.

Significance. The significance of our work lies in (1) the identification/verification of multiple empirical phenomena (constant instantaneous speed, anomalous diffusion in global displacement, isotropic parameter exploration despite anisotopic loss, velocity regularization, and slower global parameter exploration with faster learning rates) present in the limiting dynamics of deep neural networks, (2) the emphasis on studying the dynamics in velocity space in addition to parameter space, and (3) concrete quantitative as well as qualitative predictions of an SDE based theory that we empirically verified in deep networks trained on large scale datasets (indeed some of the above nontrivial phenomena were predictions of this theory). Of course, these contributions directly build upon a series of related works studying the immensely complex process of deep learning. To this end, we further clarify the originality of our contributions with respect to some relevant works. Overall, by identifying key phenomena, explaining them in a simpler setting, deriving predictions of new phenomena, and providing evidence for these predictions at scale, we are furthering the scientific study of deep learning. We hope our newly derived un-derstanding of the limiting dynamics of SGD, and its dependence on various important hyperparameters like batch size, learning rate, and momentum, can serve as a basis for future work that can turn these insights into algorithmic gains.

## A Modeling SGD with an SDE

As explained in section 4, in order to understand the dynamics of stochastic gradient descent we build a continuous Langevin equation in phase space modeling the effect of discrete updates and stochastic batches simultaneously.

## A.1 Modeling Discretization

To model the discretization effect we assume that the system of update equations ( [2](#formula_1)) is actually a discretization of some unknown ordinary differential equation. To uncover this ODE, we combine the two update equations in (2), by incorporating a previous time step θ k-1 , and rearrange into the form of a finite difference discretization, as shown in equation ( [3](#formula_2)). Like all discretizations, the Euler discretizations introduce error terms proportional to the step size, which in this case is the learning rate η. Taylor expanding θ k+1 and θ k-1 around θ k , its easy to show that both Euler discretizations introduce a second-order error term proportional to η 2 θ. 2 The difference between a forward Euler and backward Euler discretization is a second-order central discretization,

$θ k+1 -θ k η = θ + η 2 θ + O(η 2 ), θ k -θ k-1 η = θ - η 2 θ + O(η 2 ).$$θ k+1 -θ k η -θ k -θ k-1 η = η θ k+1 -2θ k +θ k-1 η 2 = η θ + O(η 2 ).$
## A.2 Modeling Stochasticity

In order to model the effect of stochastic batches, we first model a batch gradient with the following assumption:

Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such that g B (θ)-g(θ) is a Gaussian random variable with mean 0 and covariance

$1 S Σ(θ).$The two conditions needed for the CLT to hold are not exactly met in the setting of SGD. Independent and identically distributed. Generally we perform SGD by making a complete pass through the entire dataset before using a sample again which introduces a weak dependence between samples. While the covariance matrix without replacement more accurately models the dependence between samples within a batch, it fails to account for the dependence between batches. Finite variance. A different line of work has questioned the Gaussian assumption entirely because of the need for finite variance random variables. This work instead suggests using the generalized central limit theorem implying the noise would be a heavy-tailed α-stable random variable Simsekli et al. (2019). Thus, the previous assumption is implicitly assuming the i.i.d. and finite variance conditions apply for large enough datasets and small enough batches.

Under the CLT assumption, we must also replace the Euler discretizations with Euler-Maruyama discretizations. For a general stochastic process, dX t = µdt + σdW t , the Euler-Maruyama method extends the Euler method for ODEs to SDEs, resulting in the update equation X k+1 = X k + ∆tµ + √ ∆tσξ, where ξ ∼ N (0, 1). Notice, the key difference is that if the temporal step size is ∆t = η, then the noise is scaled by the square root √ η. In fact, the main argument against modeling SGD with an SDE, as nicely explained in [Yaida (2018)](#b56), is that most SDE approximations simultaneously assume that ∆t → 0 + , while maintaining that the square root of the learning rate √ η is finite. However, by modeling the discretization and stochastic effect simultaneously we can avoid this argument, bringing us to our second assumption:

Assumption 2 (SDE). We assume the underdamped Langevin equation ( [6](#formula_6)) accurately models the trajectory of the network driven by SGD through phase space such that

$θ(ηk) ≈ θ k and v(ηk) ≈ v k .$This approach of modeling discretization and stochasticity simultaneously is called stochastic modified equations, as further explained in [Li et al. (2017)](#b29).

## B Structure in the Covariance of the Gradient Noise

As we've mentioned before, SGD introduces highly structured noise into an optimization process, often assumed to be an essential ingredient for its ability to avoid local minima.

Assumption 3 (Covariance Structure). We assume the covariance of the gradient noise is spatially independent Σ(θ) = Σ and proportional to the Hessian of the least squares loss Σ = σ 2 H where σ ∈ R + is some unknown scalar.

In the setting of linear regression, this is a very natural assumption. If we assume the classic generative model for linear regression data y i = x ⊺ i θ + σϵ where, θ ∈ R d is the true model and ϵ ∼ N (0, 1), then provably Σ(θ) ≈ σ 2 H.

Proof. We can estimate the covariance as

$Σ(θ) ≈ 1 N N i=1 g i g ⊺ i -gg ⊺ . Near stationarity gg ⊺ ≪ 1 N N i=1 g i g ⊺ i ,$and thus,

$Σ(θ) ≈ 1 N N i=1 g i g ⊺ i .$Under the generative model y i = x ⊺ i θ + σϵ where ϵ ∼ N (0, 1) and σ ∈ R + , then the gradient g i is

$g i = (x ⊺ i (θ -θ) -σϵ)x i ,$and the matrix g i g ⊺ i is

$g i g ⊺ i = (x ⊺ i (θ -θ) -σϵ) 2 (x i x ⊺ i ).$Assuming θ ≈ θ at stationarity, then

$(x ⊺ i (θ -θ) -σϵ) 2 ≈ σ 2 . Thus, Σ(θ) ≈ σ 2 N N i=1 x i x ⊺ i = σ 2 N X ⊺ X = σ 2 H$Also notice that weight decay is independent of the data or batch and thus simply shifts the gradient distribution, but leaves the covariance of the gradient noise unchanged.

While the above analysis is in the linear regression setting, for deep neural networks it is reasonable to make the same assumption. See the appendix of Jastrzębski et al.

(2017) for a discussion on this assumption in the non-linear setting.

## Recent work by Ali et al. (2020) also studies the dynamics of SGD (without momentum)

in the setting of linear regression. This work, while studying the classic first-order stochastic differential equation, made a point to not introduce an assumption on the diffusion matrix. In particular, they make the point that even in the setting of linear regression, a constant covariance matrix will fail to capture the actual dynamics. To illustrate this point they consider the univariate responseless least squares problem,

$minimize θ∈R 1 2n n i=1 (x i θ) 2 .$As they explain, the SGD update for this problem would be

$θ k+1 = θ k - η S i∈B x i θ k = k i=1 (1 -η( 1 S i∈B x i ))θ 0 ,$from which they conclude for a small enough learning rate η, then with probability one θ k → 0. They contrast this with the Ornstein-Uhlenbeck process given by a constant covariance matrix where while the mean for θ k converges to zero its variance converges to a positive constant. At the heart of their argument is the fact that if an interpolating solution exists, then if SGD reaches this solution, then the noise introduced by stochastic batches would vanish, at which point the dynamics would stop. To capture this behavior we would require that our diffusion matrix is spatially dependent resulting in a process resembling a multivariate geometric Brownian motion rather than an OU process. In general, we implicitly assume that we have sufficient training data such that no interpolating solution exists.

## C A Quadratic Loss at the End of Training

Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network can be approximated by the quadratic loss

$L(θ) = (θ -µ) ⊺ H 2 (θ - µ)$, where H ⪰ 0 is the training loss Hessian and µ is some unknown mean vector, corresponding to a local minimum.

This assumption has been amply used in previous works such as [Mandt](#)  It is also a well known empirical fact that even at the end of training the Hessian can have negative eigenvalues [Papyan (2018)](#b41). This empirical observation is at odds with our assumption that the Hessian is positive semi-definite H ⪰ 0. Further analysis is needed to alleviate this inconsistency.

D Solving an Ornstein-Uhlenbeck Process with Anisotropic

## Noise

We will study the multivariate Ornstein-Uhlenbeck process described by the stochastic differential equation

$dX t = A(µ -X t )dt + √ 2κ -1 DdW t X 0 = x 0 ,(20)$where A ∈ S m ++ is a positive definite drift matrix, µ ∈ R m is a mean vector, κ ∈ R + is some positive constant, and D ∈ S m ++ is a positive definite diffusion matrix. This OU process is unique in that it is one of the few SDEs we can solve explicitly. We can derive an expression for X T as,

$X T = e -AT x 0 + I -e -AT µ + T 0 e A(t-T ) √ 2κ -1 DdW t .(21)$Proof. Consider the function f (t, x) = e At x where e A is a matrix exponential. Then by Itô's Lemma 3 we can evaluate the derivative of f (t, X t ) as

$df (t, X t ) = Ae At X t + e At A(µ -X t ) dt + e At √ 2κ -1 DdW t = Ae At µdt + e At √ 2κ -1 DdW t Integrating this expression from t = 0 to t = T gives f (T, X T ) -f (0, X 0 ) = T 0 Ae At µdt + T 0 e At √ 2κ -1 DdW t e AT X T -x 0 = e AT -I µ + T 0 e At √ 2κ -1 DdW t$which rearranged gives the expression for X T .

3 Itô's Lemma states that for any Itô drift-diffusion process dX t = µ t dt + σ t dW t and twice differen-

$tiable scalar function f (t, x), then df (t, X t ) = f t + µ t f x + σ 2 t 2 f xx dt + σ t f x dW t .$From this expression it is clear that X T is a Gaussian process. The mean of the process is

$E [X T ] = e -AT x 0 + I -e -AT µ,(22)$and the covariance and cross-covariance of the process are

$Var(X T ) = κ -1 T 0 e A(t-T ) 2De A ⊺ (t-T ) dt,(23) Cov$$(X T , X S ) = κ -1 min(T,S) 0 e A(t-T ) 2De A ⊺ (t-S) dt.(24)$These last two expressions are derived by Itô Isometry 4 .

## D.1 The Lyapunov Equation

We can explicitly solve the integral expressions for the covariance and cross-covariance exactly by solving for the unique matrix B ∈ S m ++ that solves the Lyapunov equation,

$AB + BA ⊺ = 2D.(25)$If B solves the Lyapunov equation, notice

$d dt e A(t-T ) Be A ⊺ (t-S) = e A(t-T ) ABe A ⊺ (t-S) + e A(t-T ) BA ⊺ e A ⊺ (t-S) = e A(t-T ) 2De A ⊺ (t-S)$Using this derivative, the integral expressions for the covariance and cross-covariance simplify as,

$Var(X T ) = κ -1 B -e -AT Be -A ⊺ T ,(26) Cov$$(X T , X S ) = κ -1 B -e -AT Be -A ⊺ T e A ⊺ (T -S) ,(27)$where we implicitly assume T ≤ S.

4 Itô Isometry states for any standard Itô process X t , then

$E t 0 X t dW t 2 = E t 0 X 2 t dt .$which rearranged and using A = A ⊺ gives the desired equation.

Let V ΛV ⊺ be the eigendecomposition of A and define the matrices D = V ⊺ DV and Q = V ⊺ QV . These matrices observe the following relationship,

$Q ij = λ i -λ j ρ i + λ j D ij .(30)$Proof. Replace A in the previous equality with its eigendecompsoition,

$V ΛV ⊺ D -DV ΛV ⊺ = QV ΛV ⊺ + V ΛV ⊺ Q.$Multiply this equation on the right by V and on the left by V ⊺ ,

$Λ D -DΛ = QΛ + Λ Q.$Looking at this equality element-wise and using the fact that Λ is diagonal gives the scalar equality for any i, j,

$(λ i -λ j ) D ij = (λ i + λ j ) Q ij ,$which rearranged gives the desired expression.

Thus, Q and U are given by,

$Q = V QV ⊺ , U = (D + Q) -1 A.(31)$This decomposition always holds uniquely when A, D ≻ 0, as 

## D.3 Stationary Solution

Using the Lyapunov equation and the drift decomposition, then X T ∼ p T , where

$p T = N e -AT x 0 + I -e -AT µ, κ -1 U -1 -e -AT U -1 e -A ⊺ T .(32)$In the limit as T → ∞, then e -AT → 0 and p T → p ss where

$p ss = N µ, κ -1 U -1 .(33)$Similarly, the cross-covariance converges to the stationary cross-covariance,

$Cov ss (X T , X S ) = κ -1 Be A ⊺ (T -S) .(34)$E A Variational Formulation of the OU Process with

## Anisotropic Noise

In this section we will describe an alternative, variational, route towards solving the dynamics of the OU process studied in appendix D.

Let Φ : R n → R be an arbitrary, non-negative potential and consider the stochastic differential equation describing the Langevin dynamics of a particle in this potential field,

$dX t = -∇Φ(X t )dt + 2κ -1 D(X t )dW t , X 0 = x 0 ,(35)$where D(X t ) is an arbitrary, spatially-dependent, diffusion matrix, κ is a temperature constant, and x 0 ∈ R m is the particle's initial position. The Fokker-Planck equation describes the time evolution for the probability distribution p of the particle's position such that p(x, t) = P(X t = x). The FP equation is the partial differential equation 5 ,

$∂ t p = ∇ • ∇Φ(X t )p + κ -1 ∇ • (D(X t )p) , p(x, 0) = δ(x 0 ),(36)$where ∇• denotes the divergence and δ(x 0 ) is a dirac delta distribution centered at the initialization x 0 . To assist in the exploration of the FP equation we define the vector field,

$J(x, t) = -∇Φ(X t )p -∇ • (D(X t )p) ,(37)$which is commonly referred to as the probability current. Notice, that this gives an alternative expression for the FP equation, ∂ t p = -∇ • J, demonstrating that J(x, t)

defines the flow of probability mass through space and time. This interpretation is espe-5 This PDE is also known as the Forward Kolmogorov equation.

cially useful for solving for the stationary solution p ss , which is the unique distribution that satisfies,

$∂ t p ss = -∇ • J ss = 0,(38)$where J ss is the probability current for p ss . The stationary condition can be obtained in two distinct ways:

1. Detailed balance. This is when J ss (x) = 0 for all x ∈ Ω. This is analogous to reversibility for discrete Markov chains, which implies that the probability mass flowing from a state i to any state j is the same as the probability mass flowing from state j to state i.

2. Broken detailed balance. This is when ∇ • J ss (x) = 0 but J ss (x) ̸ = 0 for all x ∈ Ω. This is analogous to irreversibility for discrete Markov chains, which only implies that the total probability mass flowing out of state i equals to the total probability mass flowing into state i.

The distinction between these two cases is critical for understanding the limiting dynamics of the process.

## E.1 The Variational Formulation of the Fokker-Planck Equation with Isotropic Diffusion

We will now consider the restricted setting of standard, isotropic diffusion (D = I). It is easy enough to check that in this setting the stationary solution is

$p ss (x) = e -κΦ(x) Z , Z = Ω e -κΦ(x) dx,(39)$where p ss is called a Gibbs distribution and Z is the partition function. Under this distribution, the stationary probability current is zero (J ss (x) = 0) and thus the process is in detailed balance. Interestingly, the Gibbs distribution p ss has another interpretation as the unique minimizer of the the Gibbs free energy functional,

$F (p) = E [Φ] -κ -1 H(p),(40)$where E [[Φ]](#) is the expectation of the potential Φ under the distribution p and H(p) =

-Ω p(x)log(p(x))dx is the Shannon entropy of p.

Proof. To prove that indeed p ss is the unique minimizer of the Gibbs free energy functional, consider the following equivalent expression

$F (p) = Ω p(x)Φ(x)dx + κ -1 Ω p(x)log(p(x))dx = κ -1 Ω p(x) (log(p(x)) -log(p ss (x))) dx -κ -1 Ω log(Z) = κ -1 D KL (p ∥ p ss ) -κ -1 log(Z)$From this expressions, it is clear that the Kullback-Leibler divergence is uniquely minimized when p = p ss .

In other words, with isotropic diffusion the stationary solution p ss can be thought of as the limiting distribution given by the Fokker-Planck equation or the unique minimizer of an energetic-entropic functional.

Seminal work by [Jordan et al. (1998)](#b21) deepened this connection between the Fokker-Planck equation and the Gibbs free energy functional. In particular, their work demonstrates that the solution p(x, t) to the Fokker-Planck equation is the Wasserstein gradient flow trajectory on the Gibbs free energy functional.

Steepest descent is always defined with respect to a distance metric. For example, the update equation, x k+1 = x k -η∇Φ(x k ), for classic gradient descent on a potential Φ(x), can be formulated as the solution to the minimization problem x k+1 = argmin x ηΦ(x)+ 1 2 d(x, x k ) 2 where d(x, x k ) = ∥xx k ∥ is the Euclidean distance metric. Gradient flow is the continuous-time limit of gradient descent where we take η → 0 + . Similarly, Wasserstein gradient flow is the continuous-time limit of steepest descent optimization defined by the Wasserstein metric. The Wasserstein metric is a distance metric between probability measures defined as,

$W 2 2 (µ 1 , µ 2 ) = inf p∈Π(µ 1 ,µ 2 ) R n ×R n |x -y| 2 p(dx, dy),(41)$where µ 1 and µ 2 are two probability measures on R n with finite second moments and Π(µ 1 , µ 2 ) defines the set of joint probability measures with marginals µ 1 and µ 2 . Thus, given an initial distribution and learning rate η, we can use the Wasserstein metric to derive a sequence of distributions minimizing some functional in the sense of steepest descent. In the continuous-time limit as η → 0 + this sequence defines a continuous trajectory of probability distributions minimizing the functional. [Jordan et al. (1997)](#b20) proved, through the following theorem, that this process applied to the Gibbs free energy functional converges to the solution to the Fokker-Planck equation with the same initialization:

Theorem 1 (JKO). Given an initial condition p 0 with finite second moment and an η > 0, define the iterative scheme p η with iterates defined by

$p k = argmin p η E [Φ] -κ -1 H(p) + W 2 2 (p, p k-1$).

As η → 0 + , then p η → p weakly in L 1 where p is the solution to the Fokker-Planck

In other words, the Fokker-Plank equation [(36)](#b35) with anisotropic diffusion can be interpreted as minimizing the expectation of a modified loss Ψ, while being implicitly regularized towards distributions that maximize entropy. The derivation requires we assume a stationary solution p ss exists and that the force j(x) implicitly defined by p ss is conservative. However, rather than implicitly define Ψ(x) and j(x) through assumption, if we can explicitly construct a modified loss Ψ(x) such that the resulting j(x) satisfies certain conditions, then the stationary solution exists and the variational formulation will apply as well. We formalize this statement with the following theorem, Theorem 3 (Explicit Construction). If there exists a potential Ψ(x) such that either j(x) = 0 or ∇ • j(x) = 0 and ∇Ψ(x) ⊥ j(x), then p ss is the Gibbs distribution ∝ e -κΨ(x) and the variational formulation given in Theorem 2 applies.

## E.3 Applying the Variational Formulation to the OU Process

Through explicit construction we now seek to find analytic expressions for the modified loss Ψ(x) and force j(x) hypothesised by [Chaudhari and Soatto (2018)](#b5) in the fundamental setting of an OU process with anisotropic diffusion, as described in section D.

We assume the diffusion matrix is anisotropic, but spatially independent, ∇ • D(x) = 0.

For the OU process the original potential generating the drift is

$Φ(x) = (x -µ) ⊺ A 2 (x -µ).(44)$Recall, that in order to extend the variational formulation we must construct some potential Ψ(x) such that ∇ • j(x) = 0 and ∇Ψ ⊥ j(x). It is possible to construct Ψ(x)

using the unique decomposition of the drift matrix A = (D + Q)U discussed in ap-pendix D. Define the modified potential,

$Ψ(x) = (x -µ) ⊺ U 2 (x -µ).(45)$Using this potential, the force j(x) is

$j(x) = -A(x -µ) + DU (x -µ) = -QU (x -µ). (46$$) Notice that j(x) is conservative, ∇ • j(x) = ∇ • -QU (x -µ) = 0 because Q is skew- symmetric. Additionally, j(x) is orthogonal, j(x) ⊺ ∇Ψ(x) = (x -µ) ⊺ U ⊺ QU (x -µ) =$0, again because Q is skew-symmetric. Thus, we have determined a modified potential Ψ(x) that results in a conservative orthogonal force j(x) satisfying the conditions for Theorem 3. Indeed the stationary Gibbs distribution given by Theorem 3 agrees with equation ( [33](#formula_63)) derived via the first and second moments in appendix D,

$e -κΨ(x) ∝ N µ, κ -1 U -1$In addition to the variational formulation, this interpretation further details explicitly the stationary probability current, J ss (x) = j(x)p ss , and whether or not the the stationary solution is in broken detailed balance.

F Explicit expressions for the OU process Generated by

## SGD

We will now consider the specific OU process generated by SGD with linear regression.

Here we repeat the setup as explained in section 5.

Let X ∈ R N ×d , Y ∈ R N be the input data, output labels respectively and θ ∈ R d be our vector of regression coefficients. The least squares loss is the convex quadratic loss

$L(θ) = 1 2N ∥Y -Xθ∥ 2 with gradient g(θ) = Hθ -b, where H = X ⊺ X N and b = X ⊺ Y N .$Plugging this expression for the gradient into the underdamped Langevin equation ( [6](#formula_6)), and rearranging terms, results in the multivariate Ornstein-Uhlenbeck (OU) process,

$d     θ t v t     = A         µ 0     -     θ t v t         dt + √ 2κ -1 DdW t ,(47)$where A and D are the drift and diffusion matrices respectively,

$A =     0 -I 2 η(1+β) (H + λI) 2(1-β) η(1+β) I     , D =     0 0 0 2(1-β) η(1+β) Σ(θ)     ,(48)$$κ = S(1 -β 2$) is a temperature constant, and µ = (H + λI) -1 b is the ridge regression solution.

## F.1 Solving for the Modified Loss and Conservative Force

In order to apply the expressions derived for a general OU process in appendix D and E, we must first decompose the drift as A = (D + Q)U . Under the simplification Σ(θ) = σ 2 H discussed in appendix B, then the matrices Q and U , as defined below, achieve this,

$Q =     0 -σ 2 H σ 2 H 0     , U =     2 η(1+β)σ 2 H -1 (H + λI) 0 0 1 σ 2 H -1     .(49)$Using these matrices we can now derive explicit expressions for the modified loss Ψ(θ, v) and conservative force j(θ, v). First notice that the least squares loss with L 2 regularization is proportional to the convex quadratic,

$Φ(θ) = (θ -µ) ⊺ (H + λI)(θ -µ).(50)$The modified loss Ψ is composed of two terms, one that only depends on the position,

$Ψ θ (θ) = (θ -µ) ⊺ H -1 (H + λI) η(1 + β)σ 2 (θ -µ) ,(51)$and another that only depends on the velocity,

$Ψ v (v) = v ⊺ H -1 σ 2 v.(52)$The conservative force j(θ, v) is

$j(θ, v) =     v -2 η(1+β) (H + λI) (θ -µ)     ,(53)$and thus the stationary probability current is J ss (θ, v) = j(θ, v)p ss .

## F.2 Decomposing the Trajectory into the Eigenbasis of the Hessian

As shown in appendix D, the temporal distribution for the OU process at some time

$T ≥ 0 is, p T         θ v         = N     e -AT     θ 0 v 0     + I -e -AT     µ 0     , κ -1 U -1 -e -AT U -1 e -A ⊺ T     .$Here we will now use the eigenbasis {q 1 , . . . , q m } of the Hessian with eigenvalues {ρ 1 , . . . , ρ m } to derive explicit expressions for the mean and covariance of the process through time.

Deterministic component. We can rearrange the expectation as

$E         θ v         =     µ 0     + e -AT     θ 0 -µ v 0     .$Notice that the second, time-dependent term is actually the solution to the system of

$ODEs     θ v     = -A     θ v     with initial condition θ 0 -µ v 0 ⊺$. This system of ODEs can be block diagonalized by factorizing A = OSO ⊺ where O is orthogonal and S is block diagonal defined as

$O =                 q 1 0 . . . q m 0 . . . 0 q 1 . . . 0 q m                 S =                 0 -1 2 η(1+β) (ρ 1 + λ) 2(1-β) η(1+β) . . . . . . . . . 0 -1 2 η(1+β) (ρ m + λ) 2(1-β) η(1+β)                $In otherwords in the plane spanned by q i 0 ⊺ and 0 q i ⊺ the system of ODEs decouples into the 2D system

$    a i b i     =     0 1 -2 η(1+β) (ρ i + λ) -2(1-β) η(1+β)         a i b i    $This system has a simple physical interpretation as a damped harmonic oscillator. If we let b i = ȧi , then we can unravel this system into the second order ODE

$äi + 2 1 -β η(1 + β) ȧi + 2 η(1 + β) (ρ i + λ)a i = 0$which is in standard form (i.e. ẍ + 2γ ẋ + ω 2 x = 0) for γ = 1-β η(1+β) and ω i = 2 η(1+β) (ρ i + λ). Let a i (0) = ⟨θ 0µ, q i ⟩ and b i (0) = ⟨v 0 , q i ⟩, then the solution in terms of γ and ω i is

$a i (t) =                      e -γt a i (0) cosh γ 2 -ω 2 i t + γa i (0)+b i (0) √ γ 2 -ω 2 i sinh γ 2 -ω 2 i t γ > ω i e -γt (a i (0) + (γa i (0) + b i (0))t) γ = ω i e -γt a i (0) cos ω 2 i -γ 2 t + γa i (0)+b i (0) √ ω 2 i -γ 2 sin ω 2 i -γ 2 t γ < ω i$Differentiating these equations gives us solutions for b i (t)

$b i (t) =                      e -γt b i (0) cosh γ 2 -ω 2 i t - ω 2 i a i (0)+γb i (0) √ γ 2 -ω 2 i sinh γ 2 -ω 2 i t γ > ω i e -γt (b i (0) -(ω 2 i a i (0) + γb i (0)) t) γ = ω i e -γt b i (0) cos ω 2 i -γ 2 t - ω 2 i a i (0)+γb i (0) √ ω 2 i -γ 2 sin ω 2 i -γ 2 t γ < ω i$Combining all these results, we can now analytically decompose the expectation as the sum,

$E         θ v         =     µ 0     + m i=1     a i (t)     q i 0     + b i (t)     0 q i         .$Intuitively, this equation describes a damped rotation (spiral) around the OLS solution in the planes defined by the the eigenvectors of the Hessian at a rate proportional to the respective eigenvalue.

## Stochastic component. Using the previous block diagonal decomposition A = OSO ⊺

we can simplify the variance as

$Var         θ v         = κ -1 U -1 -e -AT U -1 e -A ⊺ T = κ -1 U -1 -e -OSO ⊺ T U -1 e -OS ⊺ O ⊺ T = κ -1 O O ⊺ U -1 O -e -ST (O ⊺ U -1 O)e -ST ⊺ O ⊺ Interestingly, the matrix O ⊺ U -1 O is also block diagonal, O ⊺ U -1 O = O ⊺     η(1+β)σ 2 2 (H + λI) -1 H 0 0 σ 2 H     O =                 η(1+β)σ 2 2 ρ 1 ρ 1 +λ 0 0 σ 2 ρ 1 . . . . . . . . . η(1+β)σ 2 2 ρm ρm+λ 0 0 σ 2 ρ m                $Thus, similar to the mean, we can simply consider the variance in each of the planes spanned by q i 0 ⊺ and 0 q i ⊺ . If we define the block matrices,

$D i =     ησ 2 2S(1-β) ρ i ρ i +λ 0 0 σ 2 S(1-β 2 ) ρ i     S i =     0 1 -2 η(1+β) (ρ i + λ) -2(1-β) η(1+β)    $then the projected variance matrix in this plane simplifies as

$Var         q ⊺ i θ q ⊺ i v         = D i -e -S i T D i e -S i T ⊺$Using the solution to a damped harmonic osccilator discussed previously, we can express the matrix exponential e -S i T explicitly in terms of γ = 1-β η(1+β) and

$ω i = If we let α i = |γ 2 -ω 2 i |, then the matrix exponential is e -S i t =                                                  e -γt      cosh (α i t) + γ α i sinh (α i t) 1 α i sinh (α i t) - ω 2 i α i sinh (α i t) cosh (α i t) -γ α i sinh (α i t)      γ > ω i e -γt      1 + γt t -ω 2 i t 1 -γt      γ = ω i e -γt      cos (α i t) + γ α i sin (α i t) 1 α i sin (α i t) - ω 2 i α i sin (α i t) cos (α i t) -γ α i sin (α i t)      γ < ω i$
## G Analyzing Properties of the Stationary Solution

Assuming the stationary solution is given by equation [(11)](#b10) we can solve for the expected value of the norm of the local displacement and gain some intuition for the expected value of the norm of global displacement.

## G.1 Instantaneous Speed

$E ss ∥δ k ∥ 2 = E ss ∥θ k+1 -θ k ∥ 2 = η 2 E ss ∥v k+1 ∥ 2 = η 2 tr E ss v k+1 v ⊺ k+1 = η 2 tr (Var ss (v k+1 ) + E ss [v k+1 ] E ss [v k+1 ] ⊺ ) = η 2 tr κ -1 U -1 = η 2 S(1 -β 2 ) tr σ 2 H$Note that this follows directly from the definition of δ k in equation ( [1](#formula_0)) and the mean and variance of the stationary solution in equation [( 11)](#b10), as well as the follow-up derivation in appendix F.

## G.2 Anomalous Diffusion

Notice, that the global movement ∆ t = θ tθ 0 can be broken up into the sum of the local movements ∆ t = t i=1 δ i , where δ i = θ iθ i-1 . Applying this decomposition,

$E ss ∥∆ t ∥ 2 = E ss   t i=1 δ i 2   = t i=1 E ss ∥δ i ∥ 2 + t i̸ =j E ss [⟨δ i , δ j ⟩]$As we solved for previously,

$E ss ∥δ i ∥ 2 = η 2 E ss ∥v i ∥ 2 = η 2 tr (Var ss (v i )) = η 2 S(1 -β 2 ) tr σ 2 H .$By a similar simplification, we can express the second term in terms of the stationary cross-covariance,

$E ss [⟨δ i , δ j ⟩] = η 2 E ss [⟨v i , v j ⟩] = η 2 tr (Cov ss (v i , v j )) .$Thus, to simplify this expression we just need to consider the velocity-velocity covariance Cov ss (v i , v j ). At stationarity, the cross-covariance for the system in phase space,

$z i = θ i v i is Cov ss (z i , z j ) = κ -1 U -1 e -A ⊺ |i-j|$where κ = S(1β 2 ), and

$U =     2 η(1+β)σ 2 H -1 (H + λI) 0 0 1 σ 2 H -1     A =     0 -I 2 η(1+β) (H + λI) 2(1-β) η(1+β) I    $As discussed when solving for the mean of the OU trajectory, the drift matrix A can be block diagonalized as A = OSO ⊺ where O is orthogonal and S is block diagonal defined as

$O =                $q 1 0 . . . q m 0 . . .

$0 q 1 . . . 0 q m                 , S =                 0 -1 2 η(1+β) (ρ 1 + λ) 2(1-β) η(1+β)$. . . . . . . . .

$0 -1 2 η(1+β) (ρ m + λ) 2(1-β) η(1+β)                $.

Notice also that O diagonalizes U -1 such that, 

$Λ = O ⊺ U -1 O =                 η(1+β)σ 2 2 ρ 1 ρ 1 +λ$$e -S k |i-j| =                                                  e -γτ      cosh (α k τ ) + γ α k sinh (α k τ ) 1 α k sinh (α k τ ) - ω 2 k α k sinh (α k τ ) cosh (α k τ ) -γ α k sinh (α k τ )      γ > ω k e -γτ      1 + γτ τ -ω 2 k τ 1 -γτ      γ = ω k e -γτ      cos (α k τ ) + γ α k sin (α k τ ) 1 α k sin (α k τ ) - ω 2 k α k sin (α k τ ) cos (α k τ ) -γ α k sin (α k τ )      γ < ω k$Plugging in these expressions into previous expression and restricting to just the k th velocity component, we see

$Cov ss (v i,k , v j,k ) = κ -1 Λ k e -S ⊺ k |i-j| 1,1 =                    κ -1 σ 2 ρ k e -γτ cosh (α k τ ) -γ α k sinh (α k τ ) γ > ω k κ -1 σ 2 ρ k e -γτ (1 -γτ ) γ = ω k κ -1 σ 2 ρ k e -γτ cos (α k τ ) -γ α k sin (α k τ ) γ < ω k$Pulling it all together,

$E ss ∥∆ t ∥ 2 = η 2 σ 2 S(1 -β 2 )$tr (H) t + 2t where C l (k) is defined as This relationship between the expected potential and kinetic energy can be understood as a form of the equipartition theorem.

$C l (k) =                   $H Phase space oscillation rates in Deep Learning

In Fig. [6](#) we showed the position and velocity of the weights over training time, projected onto the first and 30th eigenvector of the Hessian. To supplement the qualitative observation that the oscillations in phase space seemed to occur at different rates, as is the case in linear regression, we measured a quantifiable difference in oscillation frequency for the position of the weights projected onto different eigenvectors by looking at the amplitude-weighted average of the frequencies identified by the fast Fourier

Transform. The velocity of the weights showed a smaller difference in the predominant oscillation frequency, but it was still noticeable.

H Phase space oscillation rates in Deep Learning

In Fig. [6](#) we showed the position and velocity of the weights over training time, projected onto the first and 30th eigenvector of the Hessian. To supplement the qualitative observation that the oscillations in phase space seemed to occur at different rates, as is the case in linear regression, we measured a quantifiable difference in oscillation frequency for the position of the weights projected onto different eigenvectors by looking at the amplitude-weighted average of the frequencies identified by the fast Fourier

Transform. The velocity of the weights showed a smaller difference in the predominant oscillation frequency, but it was still noticeable. Table 3: Figures 4,5,6 experiments training hyperparameters. I.6 Figure 7 We resumed training for an ImageNet pre-trained ResNet-18 from Torchvision Paszke et al. (2017) for 2 epochs, using the sweeps of hyperparameters shown in table 4. We indicate a sweep in a particular hyperparameter by [A, B], which denotes 20 evenly spaced numbers between A and B, inclusive. We kept track of the norms of the local and global displacement, ∥δ k ∥ 2 2 and ∥∆ k ∥ 2 2 ,

![al. (2016); Neal (1996); Lee et al. (2017); Jacot et al. (2018); Lee et al. (2019); Song et al. (2018);]()

![et al. (2017); Wan et al. (2020); Baity-Jesi et al. (2018); Chen et al. (2020). To demonstrate this behavior, we resume training of pre-trained convolutional networks while tracking the network trajectory through parameter space. Let θ * ∈ R m be the parameter vector for a pre-trained network and θ k ∈ R m be the parameter vector after k steps of resumed training. We track two metrics of the training trajectory, namely the local parameter displacement δ k between consecutive steps, and the global displacement ∆ k after k steps from the pre-trained initialization:]()

![Figure 1: Despite performance convergence, the network continues to move through parameter space. We plot the squared Euclidean norm for the local and global displacement (δ k and ∆ k ) of five classic convolutional neural network architectures. The networks are standard Pytorch models pre-trained on ImageNet Paszke et al. (2017). Their training is resumed for 10 additional epochs. We show the global displacement on a log-log plot where the slope of the least squares line c is the exponent of the power law ∥∆ k ∥ 2 2 ∝ k c . See appendix I for experimental details.]()

![Similar observation were made by Baity-Jesi et al. (2018) who noticed distinct phases of the training trajectory evident in the dynamics of the global displacement and Chen et al. (2020) who found that the exponent of diffusion changes through the course of training. A parallel observation is given by Hoffer et al. (2017) for the beginning of training, where they measure the global displacement from the initialization of an untrained network and observe a rate]()

![dynamics of deep neural networks trained with SGD. Our analysis and experiments build upon this literature.Continuous models for SGD. Many works consider how to improve the classic gradient flow model for SGD to more realistically reflect momentumQian (1999), discretization due to finite learning ratesKunin et al. (2020); Barrett and Dherin (2020), and stochasticity due to random batches Li et al. (2017); Smith et al. (2021). One line of work has studied the dynamics of networks in their infinite width limits through dynamical mean field theory Mignacco et al. (2020); Mannelli et al. (2020); Mignacco et al. (2021); Mannelli and Urbani (2021), while a different approach has used stochastic differential equations (SDEs) to model SGD directly, the approach we take in this work. However, recently, the validity of this approach has been questioned. The main argument, as nicely explained inYaida (2018), is that most SDE approximations simultaneously assume that ∆t → 0 + , while maintaining that the learning rate η = ∆t is finite. The worksSimsekli et al. (2019) andLi et al. (2021) have questioned the correctness of the using the central limit theorem (CLT) to model the gradient noise as Gaussian, arguing respectively that the heavy-tailed structure in the gradient noise and the weak dependence between batches leads the CLT to break down. In our work, we maintain the CLT assumption holds, which we discuss further in appendix A, but impor-tantly we avoid the pitfalls of many previous SDE approximations by simultaneously modeling the effect of finite learning rates and stochasticity.Limiting dynamics. A series of works have applied SDE models of SGD to study the limiting dynamics of neural networks. In the seminal work byMandt et al. (2016), the limiting dynamics were modeled with a multivariate Ornstein-Uhlenbeck process by combining a first-order SDE model for SGD with assumptions on the geometry of the loss and covariance matrix for the gradient noise. This analysis was extended by Jastrzębski et al. (2017) through additional assumptions on the covariance matrix to gain tractable insights and applied byAli et al. (2020) to the simpler setting of linear regression, which has a quadratic loss. A different approach was taken by Chaudhari and Soatto (2018), which did not formulate the dynamics as an OU process, nor assume directly a structure on the loss or gradient noise. Rather, this analysis studied the same first-order SDE via the Fokker-Planck equation to propose the existence of a modified loss and probability currents driving the limiting dynamics, but did not provide explicit expressions. Our analysis deepens and combines ideas from all these works, where our key insight is to lift the dynamics into phase space. By studying the dynamics of the parameters and their velocities, and by applying the analysis first in the setting of linear regression where assumptions are provably true, we are able to identify analytic expressions and explicit insights which lead to concrete predictions and testable hypothesis. Stationary dynamics. A different line of work avoids modeling the limiting dynamics of SGD with an SDE and instead chooses to leverage the property of stationarity. These works Yaida (2018); Zhang et al. (2019); Liu et al. (2021); Ziyin et al. (2021) assume that eventually the probability distribution governing the model parameters reaches sta-tionarity such that the discrete SGD process is simply sampling from this distribution. Yaida (2018) used this approach to derive fluctuation-dissipation relations that link measurable quantities of the parameters and hyperparameters of SGD. Liu et al. (2021) used this approach to derive properties for the stationary distribution of SGD with a quadratic loss. Similar to our analysis, this work identifies that the stationary distribution for the parameters reflects a modified loss function dependent on the relationship between the covariance matrix of the gradient noise and the Hessian matrix for the original loss. Empirical exploration. Another set of works analyzing the limiting dynamics of SGD has taken a purely empirical approach. Building on the intuition that flat minima generalize better than sharp minima, Keskar et al. (2016) demonstrated empirically that the hyperparameters of optimization influence the eigenvalue spectrum of the Hessian matrix at the end of training. Many subsequent works have studied the Hessian eigenspectrum during and at the end of training. Jastrzębski et al. (2018); Cohen et al. (2021) studied the dynamics of the top eigenvalues during training. Sagun et al. (2017); Papyan (2018); Ghorbani et al. (2019) demonstrated the spectrum has a bulk of values near zero plus a small number of larger outliers. Gur-Ari et al. (2018) demonstrated that the learning dynamics are constrained to the subspace spanned by the top eigenvectors, but found no special properties of the dynamics within this subspace. In our work we also determine that the top eigensubspace of the Hessian plays a crucial role in the limiting dynamics and by projecting the dynamics into this subspace in phase space, we see that the motion is not random, but consists of incoherent oscillations leading to anomalous diffusion. Hyperparameter schedules and algorithm development. Lastly, a set of works have used theoretical and empirical insights of the limiting dynamics to construct hyperparameter schedules and algorithms to improve performance. Most famously, the linear scaling rule, derived by Krizhevsky (2014) and Goyal et al. (2017), relates the influence of the batch size and the learning rate, facilitating stable training with increased parallelism. This relationship was extended by Smith et al. (2017) to account for the effect of momentum. Lucas et al. (2018) proposed a variant of SGD with multiple velocity buffers to increase stability in the presence of a nonuniform Hessian spectrum. Chaudhari et al. (2019) introduced a variant of SGD guided by local entropy to bias the dynamics into wider minima. Izmailov et al. (2018) demonstrated how a simple algorithm of stochastically averaging samples from the limiting dynamics of a network can improve generalization performance. While algorithm development is not the focus of our work, we believe that our careful and precise understanding of the deep learning limiting dynamics will similarly provide insight for future work in this direction. 4 Modeling SGD as an Underdamped Langevin Equation Following the route of previous works Mandt et al. (2016); Jastrzębski et al. (2017); Chaudhari and Soatto (2018) studying the limiting dynamics of neural networks, we first seek to model SGD as a continuous stochastic process. We consider a network parameterized by θ ∈ R m , a training dataset {x 1 , . . . , x N } of size N , and a training loss L(θ) = 1 N N i=1 ℓ(θ, x i ) with corresponding gradient g(θ) = ∂L ∂θ . The state of the network at the k th step of training is defined by the position vector θ k and velocity vector v k of the same dimension. The gradient descent update with learning rate η, momentum β, and weight decayλ is given by]()

![Figure 3: An anisotropic OU process is driven by a modified loss. We sample from an OU process with anisotropic diffusion and plot the trajectory (same black line on both plots). The left plot shows the original loss Φ(x) generating the drift. The right plot shows the modified loss Ψ(x). Notice the trajectory more closely resembles the curvature of Ψ(x) than Φ(x). The grey lines depict the stationary probability current]()









![Figure 4: The training trajectory behaves isotropically, regardless of the training loss. We resume training of a pre-trained ResNet-18 model on ImageNet and project its parameter trajectory (black line) onto the space spanned by the eigenvectors of itspre-trained Hessian q 1 , q 30 (with eigenvalue ratio ρ 1 /ρ 30 ≃ 6). We sample the training and test loss within the same 2D subspace and visualize them as a heatmap in the left and center panels respectively. We visualize the modified loss computed from the eigenvalues (ρ 1 , ρ 30 ) and optimization hyperparameters according to equation(15) in the right plot. Note the projected trajectory is isotropic, despite the anisotropy of the training and test loss.]()

![Figure 5: Implicit velocity regularization defined by the inverse Hessian. The shape of the projected velocity trajectory closely resembles the contours of the modified loss Ψ v , equation (16).]()

![Figure 7: Understanding how the hyperparameters of optimization influence the diffusion. We resume training of pre-trained ResNet-18 models on ImageNet using a range of learning rates, batch sizes, and momentum coefficients, tracking ∥δ t ∥ 2 and ∥∆ t ∥ 2 . Starting from the default hyperparameters, namely η = 1e -4, S = 256, and β = 0.9, we vary each one while keeping the others fixed. The top row shows the measured ∥δ t ∥ 2 in color, with the default hyperparameter setting highlighted in black.The dotted line depicts the predicted value from equation(18). The bottom row shows the estimated exponent c found by fitting a line to the ∥∆ t ∥ 2 trajectories on a log-log plot. The dotted line shows c = 1, corresponding to standard diffusion.]()

![Using the same sweep of models described previously, we can empirically confirm that the optimization hyperparameters each influence the diffusion exponent c. As shown in Fig.7, the learning rate, batch size, and momentum can each independently drive the exponent c into different regimes of anomalous diffusion. Notice how the influence of the learning rate and momentum on the diffusion exponent c closely resembles their respective influences on the damping ratio ζ l . Interestingly, a larger learning rate leads to underdamped oscillations, and the resultant temporal velocities' anti-correlations reduce the exponent of anomalous diffusion. Thus contrary to intuition, a larger learning rate actually leads to slower global transport in parameter space. The batch size on the other hand, has no influence on the damping ratio, but leads to an interesting, non-monotonic influence on the diffusion exponent. Overall, the hyperparameters of optimization and eigenspectrum of the Hessian all conspire to govern the degree of anomalous diffusion at the end of training.]()

![The empirical phenomena we present provide novel insight with respect to the works of Wan et al. (2020), Hoffer et al. (2017), and Chen et al. (2020). We observe that all parameters in the network (not just those with scale symmetry) move at a constant instantaneous speed at the end of training and diffuse anomalously at rates determined by the hyperparameters of optimization. In contrast to the work by Liu et al. (2021), we modeled the entire SGD process as an OU process which allows us to provide insight into the transient dynamics and identify oscillations in parameter and velocity space. We build on the theoretical framework used by Chaudhari and Soatto(2018) and provide explicit expressions for the limiting dynamics in the simplified linear regression setting and conclude that the oscillations present in the limiting dynamics are more likely to be space-filling curves (and not limit cycles) in deep learning due to many incommensurate oscillations.]()

![regulates the amount of backward Euler incorporated into the discretization. When β = 0, we remove all backward Euler discretization leaving just the forward Euler discretization. When β = 1, we have equal amounts of backward Euler as forward Euler resulting in a central second-order discretization 2 as noticed inQian (1999).]()

![et al. (2016), Jastrzębski et al. (2017), and Poggio et al. (2017). Particularly, Mandt et al. (2016) discuss how this assumption makes sense for smooth loss functions for which the stationary solution to the stochastic process reaches a deep local minimum from which it is difficult to escape. It is a well-studied fact, both empirically and theoretically, that the Hessian is lowrank near local minima as noted by Sagun et al. (2016), and Kunin et al. (2020). This degeneracy results in flat directions of equal loss. Kunin et al. (2020) discuss how differentiable symmetries, architectural features that keep the loss constant under certain weight transformations, give rise to these flat directions. Importantly, the Hessian and the covariance matrix share the same null space, and thus we can always restrict ourselves to the image space of the Hessian, where the drift and diffusion matrix will be full rank. Further discussion on the relationship between the Hessian and the covariance matrix can be found in Thomas et al. (2020).]()

![j exists and (D + Q) is invertible. See Kwon et al. (2005) for a discussion on the singularities of this decomposition.]()

![Applying these decompositions, properties of matrix exponentials, and the cyclic invariance of the trace, allows us to express the trace of the cross-covariance astr (Cov ss (z i , z j )) = κ -1 tr U -1 e -A ⊺ |i-j| = κ -1 tr U -1 Oe -S ⊺ |i-j| O ⊺ = κ -1 tr Λe -S ⊺ |i-j| = κ -1 n k=1 tr Λ k e -S ⊺k |i-j| where Λ k and S k are the blocks associated with each eigenvector of H. As solved for previously in the variance of the OU process, we can express the matrix exponential e -S k |i-j| explicitly in terms of γ = 1-β η(1+β) and ω k = |i -j| and α k = |γ 2ω 2 k |, then the matrix exponential is]()

![-γk cosh (α l k) -γ α l sinh (α l k) γ > ω l e -γk (1γk) γ = ω l e -γk cos (α l k) -γ α l sin (α l k) γ < ω lfor γ = 1-β η(1+β) , ω l = 2 η(1+β) (ρ l + λ), and α l = |γ 2ω 2 l |.G.3 Training Loss and Equipartition TheoremIn addition to solving for the expected values of the local and global displacements, we can consider the expected training loss and find an interesting relationship to the equipartition theorem from classical statistical mechanics.The regularized training loss isL λ (θ) = 1 2 (θµ) ⊺ H(θµ) + λ 2 ∥θ∥ 2, where H is the Hessian matrix and µ is the true mean. Taking the expectation with respect to the stationary distribution,E ss [L λ (θ)] = 1 2 tr ((H + λI)E ss [θθ ⊺ ])µ ⊺ HE ss [θ] + 1 2 µ ⊺ HµThe first and second moments of the stationary solution areE ss [θ] = µ E ss [θθ ⊺ ] = η S(1β) σ 2 2 (H + λI) -1 H + µµ ⊺Plugging these expressions in and canceling terms we getE ss [L λ (θ)] = η 4S(1β) tr σDefine the kinetic energy of the network as K(v) = 1 2 m∥v∥ 2 , where m = η 2 (1 + β) is the per-parameter "mass" of the network according to our previously derived Langevin dynamics. At stationarity,E ss [K(v)] = η(1 + β) 4 tr (E ss [vv ⊺ ]) = η 4S(1β) tr σ 2 Hwhere we used the fact that E ss [vv ⊺ ] = σ 2 H. In otherwords, at stationarity,E ss [L λ (θ)] = E ss [K(v)] + λ 2 ∥µ∥ 2 .]()

![Figure 8: Phase space oscillations at different rates are determined by the eigendecomposition of the Hessian Weight positions oscillate at a quantifiably different frequencies when projected onto different eigenvectors of the Hessian. Velocities fol-]()

η(1+β) (ρ i + λ).

η(1+β) (ρ k + λ). If we let τ =

