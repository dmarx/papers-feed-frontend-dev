- Decision to explore the limiting dynamics of SGD in deep neural networks
- Choice of using stochastic gradient descent as the optimization method
- Selection of a modified loss function for analysis
- Decision to model SGD as an underdamped Langevin equation
- Choice of linear regression as a fundamental setting for theoretical exploration
- Assumption of Gaussian noise in gradient estimates
- Decision to derive a continuous-time model for SGD
- Choice of Fokker-Planck equation to analyze phase space dynamics
- Decision to study the influence of hyperparameters on limiting dynamics
- Choice of ResNet-18 model for empirical validation
- Decision to analyze anomalous diffusion in parameter space
- Assumption of spatial independence in gradient noise covariance
- Decision to incorporate statistical physics concepts into the analysis
- Choice of empirical methods to validate theoretical predictions
- Decision to derive exact analytic expressions for dynamics
- Choice of using power law relationships to describe distance traveled in parameter space
- Decision to investigate the interaction between hyperparameters and gradient noise structure
- Choice to document qualitative and quantitative predictions of the theory
- Decision to explore the implications of anomalous diffusion for model performance
- Choice to leverage insights from previous empirical works on parameter averaging and ensembling