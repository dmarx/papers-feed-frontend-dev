<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations, and Anomalous Diffusion</title>
				<funder ref="#_nW8s9cM">
					<orgName type="full">Simons Foundation (Collaboration on the Global Brain</orgName>
				</funder>
				<funder ref="#_spTqpUR">
					<orgName type="full">Sloan Foundation</orgName>
				</funder>
				<funder ref="#_tj7qU7h #_hfxfefp">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford</orgName>
				</funder>
				<funder ref="#_9DHxTx3">
					<orgName type="full">CAREER</orgName>
				</funder>
				<funder ref="#_4MHGG5k">
					<orgName type="full">NTT Research</orgName>
				</funder>
				<funder ref="#_YB5QtRx">
					<orgName type="full">McDonnell Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Mexican National Council of Science and Technology (CONACYT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-12-28">28 Dec 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
							<email>kunin@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Javier</forename><surname>Sagastuy-Brena</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lauren</forename><surname>Gillespie</surname></persName>
							<email>gillespl@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eshed</forename><surname>Margalit</surname></persName>
							<email>eshedm@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hidenori</forename><surname>Tanaka</surname></persName>
							<email>hidenori.tanaka@ntt-research.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NTT Research</orgName>
								<address>
									<postCode>94085</postCode>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
							<email>sganguli@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<postCode>94025</postCode>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">L K</forename><surname>Yamins</surname></persName>
							<email>yamins@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations, and Anomalous Diffusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-28">28 Dec 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">768ADF805DC5C1100B87FCDBB11CD11B</idno>
					<idno type="arXiv">arXiv:2107.09133v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-23T01:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance travelled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction between the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase space dynamics of the parameters and their instantaneous velocities from initialization to stationarity. Using the Fokker-Planck equation, we show that the key ingredient driving these dynamics is not the original training loss, but rather the combination of a modified loss, which implicitly regularizes the velocity, and probability currents, which cause oscillations in phase space. We identify qualitative and quantitative predictions of this theory in the dynamics of a ResNet-18 model trained on ImageNet. Through the lens of statistical physics, we uncover a mechanistic origin for the anomalous limiting dynamics of deep neural networks trained with SGD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have demonstrated remarkable generalization across a variety of datasets and tasks. Essential to their success has been a collection of good practices on how to train these models with stochastic gradient descent (SGD). Yet, despite their importance, these practices are mainly based on heuristic arguments and trial and error search. Without a general theory connecting the hyperparameters of optimization, the architecture of the network, and the geometry of the dataset, theory-driven design of deep learning systems is impossible. Existing theoretical works studying this interaction have leveraged the random structure of neural networks at initialization and in their infinite width limits in order to study their dynamics <ref type="bibr">Schoenholz et</ref>  Rotskoff and Vanden-Eijnden (2018); <ref type="bibr" target="#b7">Chizat and Bach (2018)</ref>. Here we take a different approach and study the training dynamics of pre-trained networks that are ready to be used for inference. By leveraging the mathematical structures found at the end of training, we uncover an intricate interaction between the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix that corroborates previously identified empirical behavior such as anomalous limiting dynamics. Understanding the limiting dynamics of SGD is a critical stepping stone to building a complete theory for the learning dynamics of neural networks. Additionally, a series of recent works have demonstrated that the performance of pre-trained networks can be improved through averaging and ensembling their parameters <ref type="bibr" target="#b10">Garipov et al. (2018)</ref>; <ref type="bibr" target="#b16">Izmailov et al. (2018)</ref>; <ref type="bibr" target="#b34">Maddox et al. (2019)</ref>. Thus, the learning dynamics of neural networks, even at the end of training, are still quite mysterious and important to the final performance of the net-work. Combining empirical exploration and theoretical tools from statistical physics, we identify and uncover a mechanistic explanation for the limiting dynamics of neural networks trained with SGD. Our work is organized as follows:</p><p>1. We discuss related work which we build upon and delineate our contributions (section 3).</p><p>2. We demonstrate empirically that long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion where distance travelled grows as a power law in the number of steps with a nontrivial exponent (section 2).</p><p>3. To understand this empirical behavior, we derive a continuous-time model for SGD as an underdamped Langevin equation, accounting for the discretization error due to finite learning rates and gradient noise introduced by stochastic batches (section 4). <ref type="bibr" target="#b3">4</ref>. We show that for linear regression, these dynamics give rise to an Ornstein-Uhlenbeck process whose moments can be derived analytically as the sum of damped harmonic oscillators in the eigenbasis of the data (section 5). <ref type="bibr" target="#b4">5</ref>. We prove via the Fokker-Planck equation that the stationary distribution for this process is a Gibbs distribution on a modified (not the original) loss, which breaks detailed balance and gives rise to non-zero probability currents in phase space (section 6). <ref type="bibr" target="#b5">6</ref>. We demonstrate empirically that the limiting dynamics of a ResNet-18 model trained on ImageNet display these qualitative characteristics -no matter how anisotropic the original training loss, the limiting trajectory of the network will behave isotropically (section 7).</p><p>7. We derive theoretical expressions for the influence of the learning rate, batch size, and momentum coefficient on the limiting instantaneous speed of the network and the anomalous diffusion exponent, which quantitatively match empirics exactly (section 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Diffusive Behavior in the Limiting Dynamics of SGD</head><p>Even after a neural network trained by SGD has reached its optimal performance, further gradient steps will cause it to continue to move through parameter space Jastrzębski </p><formula xml:id="formula_0">δ k = θ k -θ k-1 , ∆ k = θ k -θ * .<label>(1)</label></formula><p>As shown in Fig. <ref type="figure" target="#fig_3">1</ref>, neither of these differences converge to zero across a variety of architectures, indicating that despite performance convergence, the networks continue to move through parameter space, both locally and globally. The squared norm of the local displacement ∥δ k ∥ 2 2 remains near a constant value, indicating the network is essentially moving at a constant instantaneous speed. This observation is quite similar  to the "equilibrium" phenomenon or "constant angular update" observed in Li et al.</p><p>(2020) and <ref type="bibr" target="#b55">Wan et al. (2020)</ref> respectively. However, these works only studied the displacement for parameters immediately preceding a normalization layer. The constant instantaneous speed behavior we observe is for all parameters in the model and is even present in models without normalization layers.</p><p>While the squared norm of the local displacement is essentially constant, the squared norm of the global displacement ∥∆ k ∥ 2 2 is monotonically growing for all networks, implying even once trained, the network continues to diverge from where it has been. Indeed Fig. <ref type="figure" target="#fig_3">1</ref> indicates a power law relationship between global displacement and number of steps, given by ∥∆ k ∥ 2 2 ∝ k c . As we'll see in section 8, this relationship is indicative of anomalous diffusion where c corresponds to the anomalous diffusion exponent.</p><p>The term anomalous diffusion is used to distinguish it from standard Brownian motion, where the relationship between distance travelled and number of updates is linear. ∝ log(k), a form of ultra-slow diffusion. These empirical observations raise the natural questions, where is the network moving to and why? To answer these questions we will build a diffusion based theory of SGD, study these dynamics in the setting of linear regression, and use lessons learned in this fundamental setting to understand the limiting dynamics of neural networks.</p><note type="other">Stan</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>There is a long line of literature studying both theoretically and empirically the learning </p><formula xml:id="formula_1">v k+1 = βv k -g(θ k ) -λθ k , θ k+1 = θ k + ηv k+1 ,<label>(2)</label></formula><p>where we initialize the network such that v 0 = 0 and θ 0 is the parameter initialization.</p><p>In order to understand the dynamics of the network through position and velocity space, which we will refer to as phase space, we express these discrete recursive equations as the discretization of some unknown ordinary differential equation (ODE). By incorporating a previous time step θ k-1 , we can rearrange the two update equations into the finite difference discretization,</p><formula xml:id="formula_2">θ k+1 -θ k η Forward Euler -β θ k -θ k-1 η Backward Euler +λθ k = -g(θ k ).<label>(3)</label></formula><p>Forward and backward Euler discretizations are explicit and implicit discretizations respectively of the first order temporal derivative θ = d dt θ. Simply replacing the discretizations with the derivative θ would generate an inaccurate first-order model for the discrete process. Like all discretizations, the Euler discretizations introduce higherorder error terms proportional to the step size, which in this case are proportional to η 2 θ. These second-order error terms are commonly referred to as artificial diffusion, as they are not part of the original first-order ODE being discretized, but introduced by the discretization process. Incorporating the artificial diffusion terms into the first-order ODE, we get a second-order ODE, sometimes referred to as a modified equation as in Kovachki and Stuart (2019); Kunin et al. (2020), describing the dynamics of gradient descent</p><formula xml:id="formula_3">η 2 (1 + β) θ + (1 -β) θ + λθ = -g(θ).<label>(4)</label></formula><p>While this second-order ODE models the gradient descent process, even at finite learning rates, it fails to account for the stochasticity introduced by choosing a random batch B of size S drawn uniformly from the set of N training points. This sampling yields the stochastic gradient g B (θ) = 1 S i∈B ∇ℓ(θ, x i ). To model this effect, we make the following assumption:</p><p>Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such that g B (θ)-g(θ) is a Gaussian random variable with mean 0 and covariance</p><formula xml:id="formula_4">1 S Σ(θ).</formula><p>This is a commonly applied analytic tool for handling noise introduced in stochastic algorithms, although certain works have questioned the correctness of this assumption, which we discuss further in appendix A. Incorporating this assumption of stochastic gradients into the previous finite difference equation and applying the stochastic counterparts to Euler discretizations, results in the second-order stochastic differential equation (SDE),</p><formula xml:id="formula_5">η 2 (1 + β) θ + (1 -β) θ + λθ = -g(θ) + η S Σ(θ)ξ(t),<label>(5)</label></formula><p>where ξ(t) represents a fluctuating force. An equation of this form is commonly referred to as an underdamped Langevin equation and has a natural physical interpretation as the equation of motion for a particle moving in a potential field with a fluctuating force. In particular, the mass of the particle is η 2 (1 + β), the friction constant is (1β), the potential is the regularized training loss L(θ) + λ 2 ∥θ∥ 2 , and the fluctuating force is introduced by the gradient noise. While this form for the dynamics provides useful intuition, we must expand back into phase space in order to write the equation in the standard drift-diffusion form for an SDE,</p><formula xml:id="formula_6">d     θ v     =     v -2 η(1+β) (g(θ) + λθ + (1 -β)v)     dt +     0 0 0 2 √ ηS(1+β) Σ(θ)     dW t ,<label>(6)</label></formula><p>where W t is a standard Wiener process. This is the continuous model we will study in this work:</p><p>Assumption 2 (SDE). We assume the underdamped Langevin equation ( <ref type="formula" target="#formula_6">6</ref>) accurately models the trajectory of the network driven by SGD through phase space such that</p><formula xml:id="formula_7">θ(ηk) ≈ θ k and v(ηk) ≈ v k .</formula><p>See appendix A for further discussion on the nuances of modeling SGD with an SDE.</p><p>5 Linear Regression with SGD is an Ornstein-Uhlenbeck</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Process</head><p>Equipped with a model for SGD, we seek to understand its dynamics in the fundamental setting of linear regression, one of the few cases where we have a complete model for the interaction of the dataset, architecture, and optimizer. Let X ∈ R N ×d be the input data, Y ∈ R N be the output labels, and θ ∈ R d be our vector of regression coefficients.</p><p>The least squares loss is the convex quadratic loss</p><formula xml:id="formula_8">L(θ) = 1 2N ∥Y -Xθ∥ 2 with gradient g(θ) = Hθ -b, where H = X ⊺ X N and b = X ⊺ Y N .</formula><p>Plugging this expression for the gradient into the underdamped Langevin equation ( <ref type="formula" target="#formula_6">6</ref>), and rearranging terms, results in the multivariate SDE,</p><formula xml:id="formula_9">d     θ t v t     = -A         θ t v t     -     µ 0         dt + 2κ -1 D(θ)dW t ,<label>(7)</label></formula><p>where A and D(θ) are the drift and diffusion matrices respectively,</p><formula xml:id="formula_10">A =     0 -I 2 η(1+β) (H + λI) 2(1-β) η(1+β) I     , D =     0 0 0 2(1-β) η(1+β) Σ(θ)     ,<label>(8)</label></formula><formula xml:id="formula_11">κ = S(1 -β 2</formula><p>) is an inverse temperature constant, and µ = (H + λI) -1 b is the ridge regression solution. In order to gain exact expressions for the dynamics of this SDE, we introduce the following assumption on the covariance of the gradient noise:</p><p>Assumption 3 (Covariance Structure). We assume the covariance of the gradient noise is spatially independent Σ(θ) = Σ and proportional to the Hessian of the least squares loss Σ = σ 2 H where σ ∈ R + is some unknown scalar.</p><p>This assumption, although strong, is actually quite natural when we assume our data is generated from a noisy, linear teacher model. If our training labels are generated as y i = x ⊺ i θ + σϵ i where, θ ∈ R d is the teacher model and ϵ i ∼ N (0, 1) is Gaussian noise, then it is not difficult to show that Σ(θ) ≈ σ 2 H. See appendix B for a complete derivation and discussion.</p><p>The result of assumption 3 is that the SDE equation 7 takes the form of a multivariate Ornstein-Uhlenbeck (OU) process. The solution to an OU process is a Gaussian process. By solving for the temporal dynamics of the first and second moments of the process, we can obtain an analytic expression for the trajectory at any time t. In particular, we can decompose the trajectory as the sum of a deterministic and stochastic component defined by the first and second moments respectively. The general solution for a multivariate OU process is derived in appendix D and explicit expressions for the solution of equation 7 are derived in appendix F.</p><p>Step Velocity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position</head><p>Step Velocity Position Eq. ( <ref type="formula" target="#formula_12">9</ref>)</p><p>Figure <ref type="figure">2</ref>: Oscillatory dynamics in linear regression. We train a linear network to perform regression on the CIFAR-10 dataset by using an MSE loss on the one-hot encoding of the labels. We compute the hessian of the loss, as well as its top eigenvectors.</p><p>The position and velocity trajectories are projected onto the first eigenvector of the hessian and visualized in black. The theoretically derived mean, equation <ref type="bibr" target="#b8">(9)</ref>, is shown in red. The top and bottom panels demonstrate the effect of varying momentum on the oscillation mode.</p><p>Deterministic component. Using the form of A in equation <ref type="bibr" target="#b7">(8)</ref> we can decompose the expectation as a sum of harmonic oscillators in the eigenbasis {q 1 , . . . , q m } of the Hessian,</p><formula xml:id="formula_12">E         θ t v t         =     µ 0     + m i=1     a i (t)     q i 0     + b i (t)     0 q i         .<label>(9)</label></formula><p>Here the coefficients a i (t) and b i (t) depend on the optimization hyperparameters η, β, λ, S and the respective eigenvalue of the Hessian ρ i as further explained in appendix F. We verify this expression nearly perfectly matches empirics on complex datasets under various hyperparameter settings as shown in Fig. <ref type="figure">2</ref>.</p><p>Stochastic component. The cross-covariance of the process between two points in time</p><formula xml:id="formula_13">t ≤ s, is Cov         θ t v t     ,     θ s v s         = κ -1 B -e -At Be -A ⊺ t e A ⊺ (t-s) ,<label>(10)</label></formula><p>where B solves the Lyapunov equation AB + BA ⊺ = 2D. In the limit as t → ∞, the process approaches a stationary solution,</p><formula xml:id="formula_14">p ss = N         µ 0     , κ -1 B     ,<label>(11)</label></formula><p>with stationary cross-covariance Cov ss = κ -1 Be A ⊺ |t-s| .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Understanding Stationarity via the Fokker-Planck Equation</head><p>The OU process is unique in that it is one of the few SDEs which we can solve exactly.</p><p>As shown in section 5, we were able to derive exact expressions for the dynamics of linear regression trained with SGD from initialization to stationarity by simply solving for the first and second moments. While the expression for the first moment provides an understanding of the intricate oscillatory relationship in the deterministic component of the process, the second moment, driving the stochastic component, is much more</p><p>opaque. An alternative route to solving the OU process that potentially provides more insight is the Fokker-Planck equation.</p><p>The Fokker-Planck (FP) equation is a PDE describing the time evolution for the probability distribution of a particle governed by Langevin dynamics. For an arbitrary potential Φ and diffusion matrix D, the Fokker-Planck equation (under an Itô integration prescription) is</p><formula xml:id="formula_15">∂ t p = ∇ • ∇Φp + ∇ • κ -1 Dp -J ,<label>(12)</label></formula><p>where p represents the time-dependent probability distribution, and J is a vector field commonly referred to as the probability current. The FP equation is especially useful for explicitly solving for the stationary solution, assuming one exists, of the Langevin dynamics. The stationary solution p ss by definition obeys ∂ t p ss = 0 or equivalently ∇ • J ss = 0. From this second definition we see that there are two distinct settings of stationarity: detailed balance when J ss = 0 everywhere, or broken detailed balance when ∇ • J ss = 0 and J ss ̸ = 0.</p><p>For a general OU process, the potential is a convex quadratic function Φ(x) = x ⊺ Ax defined by the drift matrix A. When the diffusion matrix is isotropic (D ∝ I) and spatially independent (∇D = 0) the resulting stationary solution is a Gibbs distribution p ss (x) ∝ e -κΦ(x) determined by the original loss Φ(x) and is in detailed balance. Lesser known properties of the OU process arise when the diffusion matrix is anisotropic or spatially dependent <ref type="bibr" target="#b9">Gardiner et al. (1985)</ref>; <ref type="bibr" target="#b45">Risken (1996)</ref>. In this setting the solution is still a Gaussian process, but the stationary solution, if it exists, is no longer defined by the Gibbs distribution of the original loss Φ(x), but actually a modified loss Ψ(x).</p><p>Furthermore, the stationary solution may be in broken detailed balance leading to a non-zero probability current J ss (x). Depending on the relationship between the drift matrix A and the diffusion matrix D the resulting dynamics of the OU process can have very nontrivial behavior, as shown in Fig. <ref type="figure" target="#fig_6">3</ref>.</p><p>In the setting of linear regression, anisotropy in the data distribution will lead to anisotropy in the gradient noise and thus an anisotropic diffusion matrix. This implies that for most datasets we should expect that the SGD trajectory is not driven by the original least squares loss, but by a modified loss and converges to a stationary solution with broken detailed balance, as predicted by <ref type="bibr" target="#b5">Chaudhari and Soatto (2018)</ref>. Using the explicit expressions for the drift A and diffusion D matrices in equation ( <ref type="formula" target="#formula_10">8</ref>) we can compute analytically the modified loss and stationary probability current,</p><formula xml:id="formula_16">Ψ(θ, v) =         θ v     -     µ 0         ⊺ U 2         θ v     -     µ 0         , J ss (θ, v) = -QU         θ v     -     µ 0         p ss ,<label>(13)</label></formula><p>where Q is a skew-symmetric matrix and U is a positive definite matrix defined as,</p><formula xml:id="formula_17">Q =     0 -Σ Σ 0     , U =     2 η(1+β) Σ -1 (H + λI) 0 0 Σ -1     .<label>(14)</label></formula><p>These new fundamental matrices, Q and U , relate to the original drift A and diffusion D matrices through the unique decomposition A = (D + Q)U , introduced by Ao (2004) and <ref type="bibr" target="#b26">Kwon et al. (2005)</ref>. Using this decomposition we can easily show that B = U -1 solves the Lyapunov equation and indeed the stationary solution p ss , de-scribed in equation ( <ref type="formula" target="#formula_14">11</ref>), is the Gibbs distribution defined by the modified loss Ψ(θ, v) in equation <ref type="bibr" target="#b12">(13)</ref>. Further, the stationary cross-covariance solved in section 5 reflects the oscillatory dynamics introduced by the stationary probability currents J ss (θ, v) in equation <ref type="bibr" target="#b12">(13)</ref>. Taken together, we gain the intuition that the limiting dynamics of SGD in linear regression are driven by a modified loss subject to oscillatory probability currents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evidence of a Modified Loss and Oscillations in Deep Learning</head><p>Does the theory derived in the linear regression setting (sections 5, 6) help explain the empirical phenomena observed in the non-linear setting of deep neural networks (section 2)? In order for the theory built in the previous sections to apply to the limiting dynamics of neural networks, we must introduce the following simplifying assumption on the loss landscape at the end of training:</p><p>Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network can be approximated by the quadratic loss</p><formula xml:id="formula_18">L(θ) = (θ -µ) ⊺ H 2 (θ -µ),</formula><p>where H ⪰ 0 and µ is some unknown mean vector, corresponding to a local minimum.</p><p>This assumption is strong, but again quite natural given our study of the limiting dynamics of the network. See appendix C for a discussion on the motivations and limitations of this assumption.</p><p>Under assumption 4, then the expressions derived in the linear regression setting would apply to the limiting dynamics of deep neural networks and depend only on quantities that we can easily estimate empirically. Of course, these simplifications are strong, but as discussed in appendix A, B, and C also quite natural. Furthermore, we can empirically test their qualitative implications: (1) a modified isotropic loss driving the limiting dynamics through parameter space, (2) implicit regularization of the velocity trajectory, and (3) oscillatory phase space dynamics determined by the Hessian eigenstructure.</p><p>Modified loss. As discussed in section 6, due to the anisotropy of the diffusion matrix, the loss landscape driving the dynamics at the end of training is not the original training loss L(θ), but a modified loss Ψ(θ, v) in phase space. As shown in equation ( <ref type="formula" target="#formula_16">13</ref>), the modified loss decouples into a term Ψ θ that only depends on the parameters θ and a term Ψ v that only depends on the velocities v. Under assumption 3, the parameter dependent component is proportional to the convex quadratic,</p><formula xml:id="formula_19">Ψ θ ∝ (θ -µ) ⊺ H -1 (H + λI) η(1 + β) (θ -µ) .<label>(15)</label></formula><p>This quadratic function has the same mean µ as the training loss, but a different curvature. Using this expression, notice that when λ ≈ 0, the modified loss is isotropic in the column space of H, regardless of what the nonzero eigenspectrum of H is. This striking prediction suggests that no matter how anisotropic the original training lossas reflected by poor conditioning of the Hessian eigenspectrum -the training trajectory of the network will behave isotropically, since it is driven not by the original anisotropic loss, but a modified isotropic loss.</p><p>We test this prediction by studying the limiting dynamics of a pre-trained ResNet-18 model with batch normalization that we continue to train on ImageNet according to the last setting of its hyperparameters <ref type="bibr" target="#b14">He et al. (2016)</ref>. Let θ * represent the initial pretrained parameters of the network, depicted with the white dot in figures 4 and 5. We estimate 1 the top thirty eigenvectors q 1 , . . . , q 30 of the Hessian matrix H * evaluated at θ * and project the limiting trajectory for the parameters onto the plane spanned by the top q 1 and bottom q 30 eigenvectors to maximize the illustrated anisotropy with our estimates. We sample the train and test loss in this subspace for a region around the projected trajectory. Additionally, using the hyperparameters of the optimization, the eigenvalues ρ 1 and ρ 30 , and the estimate for the mean µ = θ * -H -1 * g * (g * is the gradient evaluated at θ * ), we also sample from the modified loss equation <ref type="bibr" target="#b14">(15)</ref> in the same region.</p><p>Figure <ref type="figure" target="#fig_15">4</ref> shows the projected parameter trajectory on the sampled train, test and modified losses. Contour lines of both the train and test loss exhibit anisotropic structure, with sharper curvature along eigenvector q 1 compared to eigenvector q 30 , as expected.</p><p>However, as predicted, the trajectory appears to cover both directions equally. This striking isotropy of the trajectory within a highly anisotropic slice of the loss landscape indicates qualitatively that the trajectory evolves in a modified isotropic loss landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implicit velocity regularization.</head><p>A second qualitative prediction of the theory is that the velocity is regulated by the inverse Hessian of the training loss. Of course there are no explicit terms in either the train or test losses that depend on the velocity. Yet, the modified loss contains a component, Ψ v , that only depends on the velocities,</p><formula xml:id="formula_20">Ψ v ∝ v ⊺ H -1 v. (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>This additional term can be understood as a form of implicit regularization on the velocity trajectory. Indeed, when we project the velocity trajectory onto the plane spanned 1 To estimate the eigenvectors of H * we use subspace iteration, and limit ourselves to 30 eigenvectors to constrain computation time. See appendix I for details.          by the q 1 and q 30 eigenvectors, as shown in Fig. <ref type="figure" target="#fig_16">5</ref>, we see that the trajectory closely resembles the curvature of the inverse Hessian H -1 . The modified loss is effectively penalizing SGD for moving in eigenvectors of the Hessian with small eigenvalues. A similar qualitative effect was recently proposed by Barrett and Dherin (2020) as a consequence of the discretization error due to finite learning rates.</p><formula xml:id="formula_22">A D C s V J 9 x 0 7 A y 7 A E R j j N a 2 6 q a I L J G A 9 p X 6 P A E V V e N r 0 h t 4 6 1 M 7 D C W O o n w J q 6 v y c y H C k 1 i Q L d W W y s 5 m u F + V + t n 0 J 4 4 W V M J C l Q Q W Y f h S m 3 I L a K Q K w B k 5 Q A n 2 j A R D K 9 q 0 V G W G I C O r a a D s G Z P 3 k R O q c N R / P t W b 1 5 W c Z R R Y f o C J 0 g B 5 2 j J r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m r R W j n N l H f 2 R</formula><formula xml:id="formula_23">A D C s V J 9 x 0 7 A y 7 A E R j j N a 2 6 q a I L J G A 9 p X 6 P A E V V e N r 0 h t 4 6 1 M 7 D C W O o n w J q 6 v y c y H C k 1 i Q L d W W y s 5 m u F + V + t n 0 J 4 4 W V M J C l Q Q W Y f h S m 3 I L a K Q K w B k 5 Q A n 2 j A R D K 9 q 0 V G W G I C O r a a D s G Z P 3 k R O q c N R / P t W b 1 5 W c Z R R Y f o C J 0 g B 5 2 j J r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m r R W j n N l H f 2 R</formula><formula xml:id="formula_24">A D C s V J 9 x 0 7 A y 7 A E R j j N a 2 6 q a I L J G A 9 p X 6 P A E V V e N r 0 h t 4 6 1 M 7 D C W O o n w J q 6 v y c y H C k 1 i Q L d W W y s 5 m u F + V + t n 0 J 4 4 W V M J C l Q Q W Y f h S m 3 I L a K Q K w B k 5 Q A n 2 j A R D K 9 q 0 V G W G I C O r a a D s G Z P 3 k R O q c N R / P t W b 1 5 W c Z R R Y f o C J 0 g B 5 2 j J r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m r R W j n N l H f 2 R</formula><formula xml:id="formula_25">A D C s V J 9 x 0 7 A y 7 A E R j j N a 2 6 q a I L J G A 9 p X 6 P A E V V e N r 0 h t 4 6 1 M 7 D C W O o n w J q 6 v y c y H C k 1 i Q L d W W y s 5 m u F + V + t n 0 J 4 4 W V M J C l Q Q W Y f h S m 3 I L a K Q K w B k 5 Q A n 2 j A R D K 9 q 0 V G W G I C O r a a D s G Z P 3 k R O q c N R / P t W b 1 5 W c Z R R Y f o C J 0 g B 5 2 j J r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m r R W j n N l H f 2 R</formula><formula xml:id="formula_26">C q q R Q w z O / e v 2 I J Q G E y A R V q u v Y M b o p l c i Z g K z S S x T E l I 3 p E L o a Q x q A c t P p C Z l 1 r J 2 + N Y i k f i F a U / f 3 R E o D p S a B r z v z h d V 8 L T f / q 3 U T H F y 4 K Q / j B C F k s 4 8 G i b A w s v I 8 r D 6 X w F B M N F A m u d 7 V Y i M q K U O d W k W H 4 M y f v A i t 0 5 q j + f a s W r 8 s 4 i i T Q 3 J E T o h D z k m d X J M G a R J G H s k z e S V</formula><formula xml:id="formula_27">C q q R Q w z O / e v 2 I J Q G E y A R V q u v Y M b o p l c i Z g K z S S x T E l I 3 p E L o a Q x q A c t P p C Z l 1 r J 2 + N Y i k f i F a U / f 3 R E o D p S a B r z v z h d V 8 L T f / q 3 U T H F y 4 K Q / j B C F k s 4 8 G i b A w s v I 8 r D 6 X w F B M N F A m u d 7 V Y i M q K U O d W k W H 4 M y f v A i t 0 5 q j + f a s W r 8 s 4 i i T Q 3 J E T o h D z k m d X J M G a R J G H s k z e S V</formula><formula xml:id="formula_28">C q q R Q w z O / e v 2 I J Q G E y A R V q u v Y M b o p l c i Z g K z S S x T E l I 3 p E L o a Q x q A c t P p C Z l 1 r J 2 + N Y i k f i F a U / f 3 R E o D p S a B r z v z h d V 8 L T f / q 3 U T H F y 4 K Q / j B C F k s 4 8 G i b A w s v I 8 r D 6 X w F B M N F A m u d 7 V Y i M q K U O d W k W H 4 M y f v A i t 0 5 q j + f a s W r 8 s 4 i i T Q 3 J E T o h D z k m d X J M G a R J G H s k z e S V</formula><formula xml:id="formula_29">C q q R Q w z O / e v 2 I J Q G E y A R V q u v Y M b o p l c i Z g K z S S x T E l I 3 p E L o a Q x q A c t P p C Z l 1 r J 2 + N Y i k f i F a U / f 3 R E o D p S a B r z v z h d V 8 L T f / q 3 U T H F y 4 K Q / j B C F k s 4 8 G i b A w s v I 8 r D 6 X w F B M N F A m u d 7 V Y i M q K U O d W k W H 4 M y f v A i t 0 5 q j + f a s W r 8 s 4 i i T Q 3 J E T o h D z k m d X J M G a R J G H s k z e S V</formula><p>Phase space oscillations. A final implication of the theory is that at stationarity the network is in broken detailed balance leading to non-zero probability currents flowing through phase space:</p><formula xml:id="formula_30">J ss (θ, v) =     v -2 η(1+β) (H + λI) (θ -µ)     p ss .<label>(17)</label></formula><p>Notice, that while the joint phase space stationary distribution of the network is in broken detailed balance, its marginal distribution in either position or velocity space is actually in detailed balance. As such, the probability currents encourage oscillatory dynamics in the phase space planes characterized by the eigenvectors of the Hessian, at rates proportional to their eigenvalues. We consider the same projected trajectory of the ResNet-18 model visualized in figures 4 and 5, but plot the trajectory in phase space for the two eigenvectors q 1 and q 30 separately. Shown in Fig. <ref type="figure">6</ref>, we see that both trajectories look like noisy clockwise rotations. Qualitatively, the trajectories for the different eigenvectors appear to be rotating at different rates.</p><p>The integral curves of the stationary probability current are one-dimensional paths confined to level sets of the modified loss. These paths might cross themselves, in which case they are limit cycles, or they could cover the entire surface of the level sets, in which case they are space-filling curves. This distinction depends on the relative fre-</p><p>Step Velocity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position</head><p>Step Velocity Position Figure <ref type="figure">6</ref>: Phase space oscillations are determined by the eigendecomposition of the Hessian. We visualize the projected position and velocity trajectories in phase space. The top and bottom panels show the projections onto q 1 and q 30 respectively.</p><p>Oscillations at different rates are distinguishable for the different eigenvectors and were verified by comparing the dominant frequencies in the fast Fourier transform of the trajectories, as shown in appendix H.</p><p>quencies of the oscillations, as determined by the pairwise ratios of the eigenvalues of the Hessian. For real-world datasets, with a large spectrum of incommensurate frequencies, we expect to be in the latter setting, thus contradicting the suggestion that SGD in deep networks converges to limit cycles, as claimed in Chaudhari and Soatto (2018).</p><p>8 Understanding the Diffusive Behaviour of the Limiting Dynamics</p><p>Taken together the empirical results shown in section 7 indicate that many of the same qualitative behaviors of SGD identified theoretically for linear regression are evident in the limiting dynamics of neural networks. Can this theory quantitatively explain the results we identified in section 2?</p><p>Constant instantaneous speed. As noted in section 2, we observed that at the end of training, across various architectures, the squared norm of the local displacement ∥δ t ∥ 2 2 remains essentially constant. Assuming the limiting dynamics are described by the stationary solution in equation <ref type="bibr" target="#b10">(11)</ref>, the expectation of the local displacement is</p><formula xml:id="formula_31">E ss ∥δ t ∥ 2 = η 2 S(1 -β 2 ) σ 2 tr (H) ,<label>(18)</label></formula><p>as derived in appendix G. We cannot test this prediction directly as we do not know σ 2 and computing tr(H) is computationally prohibitive. However, we can estimate σ 2 tr(H) by resuming training for a model, measuring the average ∥δ t ∥ 2 , and then inverting equation <ref type="bibr" target="#b17">(18)</ref>. Using this single estimate, we find that for a sweep of models with varying hyperparameters, equation ( <ref type="formula" target="#formula_31">18</ref>) accurately predicts their instantaneous speed.</p><p>Indeed, Fig. <ref type="figure" target="#fig_17">7</ref> shows an exact match between the empirics and theory, which strongly Exponent of anomalous diffusion. The expected value for the global displacement under the stationary solution can also be analytically expressed in terms of the optimization hyperparameters and the eigendecomposition of the Hessian as,</p><formula xml:id="formula_32">E ss ∥∆ t ∥ 2 = η 2 S(1 -β 2 ) σ 2 tr (H) t + 2t t k=1 1 - k t m l=1 ρ l C l (k) ,<label>(19)</label></formula><p>where C l (k) is a trigonometric function describing the velocity of a harmonic oscillator with damping ratio ζ l = (1β)/ 2η(1 + β) (p l + λ), see appendix G for details.</p><p>As shown empirically in section 2, the squared norm ∥∆ t ∥ 2 monotonically increases as a power law in the number of steps, suggesting its expectation is proportional to t c for some unknown, constant c. The exponent c determines the regime of diffusion for the process. When c = 1, the process corresponds to standard Brownian diffusion.</p><p>For c &gt; 1 or c &lt; 1 the process corresponds to anomalous super-diffusion or subdiffusion respectively. Unfortunately, it is not immediately clear how to extract the explicit exponent c from equation <ref type="bibr" target="#b18">(19)</ref>. However, by exploring the functional form of C l (k) and its relationship to the hyperparameters of optimization through the damping ratio ζ l , we can determine overall trends in the diffusion exponent c.</p><p>Akin to how the exponent c determines the regime of diffusion, the damping ratio ζ l determines the regime for the harmonic oscillator describing the stationary velocityvelocity correlation in the l th eigenvector of the Hessian. When ζ l = 1, the oscillator is critically damped implying the velocity correlations converge to zero as quickly as possible. In the extreme setting of C l (k) = 0 for all l, k, then equation <ref type="bibr" target="#b18">(19)</ref> simplifies to standard Brownian diffusion, E ss [∥∆ t ∥ 2 ] ∝ t. When ζ l &gt; 1, the oscillator is overdamped implying the velocity correlations dampen slowly and remain positive even over long temporal lags. Such long lasting temporal correlations in velocity lead to faster global displacement. Indeed, in the extreme setting of C l (k) = 1 for all l, k, then equation (19) simplifies to a form of anomalous super-diffusion, E ss [∥∆ t ∥ 2 ] ∝ t 2 . When ζ l &lt; 1, the oscillator is underdamped implying the velocity correlations will oscillate quickly between positive and negative values. Indeed, the only way equation (19) could describe anomalous sub-diffusion is if C l (k) took on negative values for certain l, k. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion</head><p>Through combined empirics and theory based on statistical physics, we uncovered an intricate interplay between the optimization hyperparameters, structure in the gradient noise, and the Hessian matrix at the end of training.</p><p>Significance. The significance of our work lies in (1) the identification/verification of multiple empirical phenomena (constant instantaneous speed, anomalous diffusion in global displacement, isotropic parameter exploration despite anisotopic loss, velocity regularization, and slower global parameter exploration with faster learning rates) present in the limiting dynamics of deep neural networks, (2) the emphasis on studying the dynamics in velocity space in addition to parameter space, and (3) concrete quantitative as well as qualitative predictions of an SDE based theory that we empirically verified in deep networks trained on large scale datasets (indeed some of the above nontrivial phenomena were predictions of this theory). Of course, these contributions directly build upon a series of related works studying the immensely complex process of deep learning. To this end, we further clarify the originality of our contributions with respect to some relevant works. Overall, by identifying key phenomena, explaining them in a simpler setting, deriving predictions of new phenomena, and providing evidence for these predictions at scale, we are furthering the scientific study of deep learning. We hope our newly derived un-derstanding of the limiting dynamics of SGD, and its dependence on various important hyperparameters like batch size, learning rate, and momentum, can serve as a basis for future work that can turn these insights into algorithmic gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Modeling SGD with an SDE</head><p>As explained in section 4, in order to understand the dynamics of stochastic gradient descent we build a continuous Langevin equation in phase space modeling the effect of discrete updates and stochastic batches simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Modeling Discretization</head><p>To model the discretization effect we assume that the system of update equations ( <ref type="formula" target="#formula_1">2</ref>) is actually a discretization of some unknown ordinary differential equation. To uncover this ODE, we combine the two update equations in (2), by incorporating a previous time step θ k-1 , and rearrange into the form of a finite difference discretization, as shown in equation ( <ref type="formula" target="#formula_2">3</ref>). Like all discretizations, the Euler discretizations introduce error terms proportional to the step size, which in this case is the learning rate η. Taylor expanding θ k+1 and θ k-1 around θ k , its easy to show that both Euler discretizations introduce a second-order error term proportional to η 2 θ. 2 The difference between a forward Euler and backward Euler discretization is a second-order central discretization,</p><formula xml:id="formula_33">θ k+1 -θ k η = θ + η 2 θ + O(η 2 ), θ k -θ k-1 η = θ - η 2 θ + O(η 2 ).</formula><formula xml:id="formula_34">θ k+1 -θ k η -θ k -θ k-1 η = η θ k+1 -2θ k +θ k-1 η 2 = η θ + O(η 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Modeling Stochasticity</head><p>In order to model the effect of stochastic batches, we first model a batch gradient with the following assumption:</p><p>Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such that g B (θ)-g(θ) is a Gaussian random variable with mean 0 and covariance</p><formula xml:id="formula_35">1 S Σ(θ).</formula><p>The two conditions needed for the CLT to hold are not exactly met in the setting of SGD. Independent and identically distributed. Generally we perform SGD by making a complete pass through the entire dataset before using a sample again which introduces a weak dependence between samples. While the covariance matrix without replacement more accurately models the dependence between samples within a batch, it fails to account for the dependence between batches. Finite variance. A different line of work has questioned the Gaussian assumption entirely because of the need for finite variance random variables. This work instead suggests using the generalized central limit theorem implying the noise would be a heavy-tailed α-stable random variable Simsekli et al. (2019). Thus, the previous assumption is implicitly assuming the i.i.d. and finite variance conditions apply for large enough datasets and small enough batches.</p><p>Under the CLT assumption, we must also replace the Euler discretizations with Euler-Maruyama discretizations. For a general stochastic process, dX t = µdt + σdW t , the Euler-Maruyama method extends the Euler method for ODEs to SDEs, resulting in the update equation X k+1 = X k + ∆tµ + √ ∆tσξ, where ξ ∼ N (0, 1). Notice, the key difference is that if the temporal step size is ∆t = η, then the noise is scaled by the square root √ η. In fact, the main argument against modeling SGD with an SDE, as nicely explained in <ref type="bibr" target="#b56">Yaida (2018)</ref>, is that most SDE approximations simultaneously assume that ∆t → 0 + , while maintaining that the square root of the learning rate √ η is finite. However, by modeling the discretization and stochastic effect simultaneously we can avoid this argument, bringing us to our second assumption:</p><p>Assumption 2 (SDE). We assume the underdamped Langevin equation ( <ref type="formula" target="#formula_6">6</ref>) accurately models the trajectory of the network driven by SGD through phase space such that</p><formula xml:id="formula_36">θ(ηk) ≈ θ k and v(ηk) ≈ v k .</formula><p>This approach of modeling discretization and stochasticity simultaneously is called stochastic modified equations, as further explained in <ref type="bibr" target="#b29">Li et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Structure in the Covariance of the Gradient Noise</head><p>As we've mentioned before, SGD introduces highly structured noise into an optimization process, often assumed to be an essential ingredient for its ability to avoid local minima.</p><p>Assumption 3 (Covariance Structure). We assume the covariance of the gradient noise is spatially independent Σ(θ) = Σ and proportional to the Hessian of the least squares loss Σ = σ 2 H where σ ∈ R + is some unknown scalar.</p><p>In the setting of linear regression, this is a very natural assumption. If we assume the classic generative model for linear regression data y i = x ⊺ i θ + σϵ where, θ ∈ R d is the true model and ϵ ∼ N (0, 1), then provably Σ(θ) ≈ σ 2 H.</p><p>Proof. We can estimate the covariance as</p><formula xml:id="formula_37">Σ(θ) ≈ 1 N N i=1 g i g ⊺ i -gg ⊺ . Near stationarity gg ⊺ ≪ 1 N N i=1 g i g ⊺ i ,</formula><p>and thus,</p><formula xml:id="formula_38">Σ(θ) ≈ 1 N N i=1 g i g ⊺ i .</formula><p>Under the generative model y i = x ⊺ i θ + σϵ where ϵ ∼ N (0, 1) and σ ∈ R + , then the gradient g i is</p><formula xml:id="formula_39">g i = (x ⊺ i (θ -θ) -σϵ)x i ,</formula><p>and the matrix g i g ⊺ i is</p><formula xml:id="formula_40">g i g ⊺ i = (x ⊺ i (θ -θ) -σϵ) 2 (x i x ⊺ i ).</formula><p>Assuming θ ≈ θ at stationarity, then</p><formula xml:id="formula_41">(x ⊺ i (θ -θ) -σϵ) 2 ≈ σ 2 . Thus, Σ(θ) ≈ σ 2 N N i=1 x i x ⊺ i = σ 2 N X ⊺ X = σ 2 H</formula><p>Also notice that weight decay is independent of the data or batch and thus simply shifts the gradient distribution, but leaves the covariance of the gradient noise unchanged.</p><p>While the above analysis is in the linear regression setting, for deep neural networks it is reasonable to make the same assumption. See the appendix of Jastrzębski et al.</p><p>(2017) for a discussion on this assumption in the non-linear setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent work by Ali et al. (2020) also studies the dynamics of SGD (without momentum)</head><p>in the setting of linear regression. This work, while studying the classic first-order stochastic differential equation, made a point to not introduce an assumption on the diffusion matrix. In particular, they make the point that even in the setting of linear regression, a constant covariance matrix will fail to capture the actual dynamics. To illustrate this point they consider the univariate responseless least squares problem,</p><formula xml:id="formula_42">minimize θ∈R 1 2n n i=1 (x i θ) 2 .</formula><p>As they explain, the SGD update for this problem would be</p><formula xml:id="formula_43">θ k+1 = θ k - η S i∈B x i θ k = k i=1 (1 -η( 1 S i∈B x i ))θ 0 ,</formula><p>from which they conclude for a small enough learning rate η, then with probability one θ k → 0. They contrast this with the Ornstein-Uhlenbeck process given by a constant covariance matrix where while the mean for θ k converges to zero its variance converges to a positive constant. At the heart of their argument is the fact that if an interpolating solution exists, then if SGD reaches this solution, then the noise introduced by stochastic batches would vanish, at which point the dynamics would stop. To capture this behavior we would require that our diffusion matrix is spatially dependent resulting in a process resembling a multivariate geometric Brownian motion rather than an OU process. In general, we implicitly assume that we have sufficient training data such that no interpolating solution exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C A Quadratic Loss at the End of Training</head><p>Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network can be approximated by the quadratic loss</p><formula xml:id="formula_44">L(θ) = (θ -µ) ⊺ H 2 (θ - µ)</formula><p>, where H ⪰ 0 is the training loss Hessian and µ is some unknown mean vector, corresponding to a local minimum.</p><p>This assumption has been amply used in previous works such as <ref type="bibr">Mandt</ref>  It is also a well known empirical fact that even at the end of training the Hessian can have negative eigenvalues <ref type="bibr" target="#b41">Papyan (2018)</ref>. This empirical observation is at odds with our assumption that the Hessian is positive semi-definite H ⪰ 0. Further analysis is needed to alleviate this inconsistency.</p><p>D Solving an Ornstein-Uhlenbeck Process with Anisotropic</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise</head><p>We will study the multivariate Ornstein-Uhlenbeck process described by the stochastic differential equation</p><formula xml:id="formula_45">dX t = A(µ -X t )dt + √ 2κ -1 DdW t X 0 = x 0 ,<label>(20)</label></formula><p>where A ∈ S m ++ is a positive definite drift matrix, µ ∈ R m is a mean vector, κ ∈ R + is some positive constant, and D ∈ S m ++ is a positive definite diffusion matrix. This OU process is unique in that it is one of the few SDEs we can solve explicitly. We can derive an expression for X T as,</p><formula xml:id="formula_46">X T = e -AT x 0 + I -e -AT µ + T 0 e A(t-T ) √ 2κ -1 DdW t .<label>(21)</label></formula><p>Proof. Consider the function f (t, x) = e At x where e A is a matrix exponential. Then by Itô's Lemma 3 we can evaluate the derivative of f (t, X t ) as</p><formula xml:id="formula_47">df (t, X t ) = Ae At X t + e At A(µ -X t ) dt + e At √ 2κ -1 DdW t = Ae At µdt + e At √ 2κ -1 DdW t Integrating this expression from t = 0 to t = T gives f (T, X T ) -f (0, X 0 ) = T 0 Ae At µdt + T 0 e At √ 2κ -1 DdW t e AT X T -x 0 = e AT -I µ + T 0 e At √ 2κ -1 DdW t</formula><p>which rearranged gives the expression for X T .</p><p>3 Itô's Lemma states that for any Itô drift-diffusion process dX t = µ t dt + σ t dW t and twice differen-</p><formula xml:id="formula_48">tiable scalar function f (t, x), then df (t, X t ) = f t + µ t f x + σ 2 t 2 f xx dt + σ t f x dW t .</formula><p>From this expression it is clear that X T is a Gaussian process. The mean of the process is</p><formula xml:id="formula_49">E [X T ] = e -AT x 0 + I -e -AT µ,<label>(22)</label></formula><p>and the covariance and cross-covariance of the process are</p><formula xml:id="formula_50">Var(X T ) = κ -1 T 0 e A(t-T ) 2De A ⊺ (t-T ) dt,<label>(23) Cov</label></formula><formula xml:id="formula_51">(X T , X S ) = κ -1 min(T,S) 0 e A(t-T ) 2De A ⊺ (t-S) dt.<label>(24)</label></formula><p>These last two expressions are derived by Itô Isometry 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 The Lyapunov Equation</head><p>We can explicitly solve the integral expressions for the covariance and cross-covariance exactly by solving for the unique matrix B ∈ S m ++ that solves the Lyapunov equation,</p><formula xml:id="formula_52">AB + BA ⊺ = 2D.<label>(25)</label></formula><p>If B solves the Lyapunov equation, notice</p><formula xml:id="formula_53">d dt e A(t-T ) Be A ⊺ (t-S) = e A(t-T ) ABe A ⊺ (t-S) + e A(t-T ) BA ⊺ e A ⊺ (t-S) = e A(t-T ) 2De A ⊺ (t-S)</formula><p>Using this derivative, the integral expressions for the covariance and cross-covariance simplify as,</p><formula xml:id="formula_54">Var(X T ) = κ -1 B -e -AT Be -A ⊺ T ,<label>(26) Cov</label></formula><formula xml:id="formula_55">(X T , X S ) = κ -1 B -e -AT Be -A ⊺ T e A ⊺ (T -S) ,<label>(27)</label></formula><p>where we implicitly assume T ≤ S.</p><p>4 Itô Isometry states for any standard Itô process X t , then</p><formula xml:id="formula_56">E t 0 X t dW t 2 = E t 0 X 2 t dt .</formula><p>which rearranged and using A = A ⊺ gives the desired equation.</p><p>Let V ΛV ⊺ be the eigendecomposition of A and define the matrices D = V ⊺ DV and Q = V ⊺ QV . These matrices observe the following relationship,</p><formula xml:id="formula_57">Q ij = λ i -λ j ρ i + λ j D ij .<label>(30)</label></formula><p>Proof. Replace A in the previous equality with its eigendecompsoition,</p><formula xml:id="formula_58">V ΛV ⊺ D -DV ΛV ⊺ = QV ΛV ⊺ + V ΛV ⊺ Q.</formula><p>Multiply this equation on the right by V and on the left by V ⊺ ,</p><formula xml:id="formula_59">Λ D -DΛ = QΛ + Λ Q.</formula><p>Looking at this equality element-wise and using the fact that Λ is diagonal gives the scalar equality for any i, j,</p><formula xml:id="formula_60">(λ i -λ j ) D ij = (λ i + λ j ) Q ij ,</formula><p>which rearranged gives the desired expression.</p><p>Thus, Q and U are given by,</p><formula xml:id="formula_61">Q = V QV ⊺ , U = (D + Q) -1 A.<label>(31)</label></formula><p>This decomposition always holds uniquely when A, D ≻ 0, as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Stationary Solution</head><p>Using the Lyapunov equation and the drift decomposition, then X T ∼ p T , where</p><formula xml:id="formula_62">p T = N e -AT x 0 + I -e -AT µ, κ -1 U -1 -e -AT U -1 e -A ⊺ T .<label>(32)</label></formula><p>In the limit as T → ∞, then e -AT → 0 and p T → p ss where</p><formula xml:id="formula_63">p ss = N µ, κ -1 U -1 .<label>(33)</label></formula><p>Similarly, the cross-covariance converges to the stationary cross-covariance,</p><formula xml:id="formula_64">Cov ss (X T , X S ) = κ -1 Be A ⊺ (T -S) .<label>(34)</label></formula><p>E A Variational Formulation of the OU Process with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anisotropic Noise</head><p>In this section we will describe an alternative, variational, route towards solving the dynamics of the OU process studied in appendix D.</p><p>Let Φ : R n → R be an arbitrary, non-negative potential and consider the stochastic differential equation describing the Langevin dynamics of a particle in this potential field,</p><formula xml:id="formula_65">dX t = -∇Φ(X t )dt + 2κ -1 D(X t )dW t , X 0 = x 0 ,<label>(35)</label></formula><p>where D(X t ) is an arbitrary, spatially-dependent, diffusion matrix, κ is a temperature constant, and x 0 ∈ R m is the particle's initial position. The Fokker-Planck equation describes the time evolution for the probability distribution p of the particle's position such that p(x, t) = P(X t = x). The FP equation is the partial differential equation 5 ,</p><formula xml:id="formula_66">∂ t p = ∇ • ∇Φ(X t )p + κ -1 ∇ • (D(X t )p) , p(x, 0) = δ(x 0 ),<label>(36)</label></formula><p>where ∇• denotes the divergence and δ(x 0 ) is a dirac delta distribution centered at the initialization x 0 . To assist in the exploration of the FP equation we define the vector field,</p><formula xml:id="formula_67">J(x, t) = -∇Φ(X t )p -∇ • (D(X t )p) ,<label>(37)</label></formula><p>which is commonly referred to as the probability current. Notice, that this gives an alternative expression for the FP equation, ∂ t p = -∇ • J, demonstrating that J(x, t)</p><p>defines the flow of probability mass through space and time. This interpretation is espe-5 This PDE is also known as the Forward Kolmogorov equation.</p><p>cially useful for solving for the stationary solution p ss , which is the unique distribution that satisfies,</p><formula xml:id="formula_68">∂ t p ss = -∇ • J ss = 0,<label>(38)</label></formula><p>where J ss is the probability current for p ss . The stationary condition can be obtained in two distinct ways:</p><p>1. Detailed balance. This is when J ss (x) = 0 for all x ∈ Ω. This is analogous to reversibility for discrete Markov chains, which implies that the probability mass flowing from a state i to any state j is the same as the probability mass flowing from state j to state i.</p><p>2. Broken detailed balance. This is when ∇ • J ss (x) = 0 but J ss (x) ̸ = 0 for all x ∈ Ω. This is analogous to irreversibility for discrete Markov chains, which only implies that the total probability mass flowing out of state i equals to the total probability mass flowing into state i.</p><p>The distinction between these two cases is critical for understanding the limiting dynamics of the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 The Variational Formulation of the Fokker-Planck Equation with Isotropic Diffusion</head><p>We will now consider the restricted setting of standard, isotropic diffusion (D = I). It is easy enough to check that in this setting the stationary solution is</p><formula xml:id="formula_69">p ss (x) = e -κΦ(x) Z , Z = Ω e -κΦ(x) dx,<label>(39)</label></formula><p>where p ss is called a Gibbs distribution and Z is the partition function. Under this distribution, the stationary probability current is zero (J ss (x) = 0) and thus the process is in detailed balance. Interestingly, the Gibbs distribution p ss has another interpretation as the unique minimizer of the the Gibbs free energy functional,</p><formula xml:id="formula_70">F (p) = E [Φ] -κ -1 H(p),<label>(40)</label></formula><p>where E <ref type="bibr">[Φ]</ref> is the expectation of the potential Φ under the distribution p and H(p) =</p><p>-Ω p(x)log(p(x))dx is the Shannon entropy of p.</p><p>Proof. To prove that indeed p ss is the unique minimizer of the Gibbs free energy functional, consider the following equivalent expression</p><formula xml:id="formula_71">F (p) = Ω p(x)Φ(x)dx + κ -1 Ω p(x)log(p(x))dx = κ -1 Ω p(x) (log(p(x)) -log(p ss (x))) dx -κ -1 Ω log(Z) = κ -1 D KL (p ∥ p ss ) -κ -1 log(Z)</formula><p>From this expressions, it is clear that the Kullback-Leibler divergence is uniquely minimized when p = p ss .</p><p>In other words, with isotropic diffusion the stationary solution p ss can be thought of as the limiting distribution given by the Fokker-Planck equation or the unique minimizer of an energetic-entropic functional.</p><p>Seminal work by <ref type="bibr" target="#b21">Jordan et al. (1998)</ref> deepened this connection between the Fokker-Planck equation and the Gibbs free energy functional. In particular, their work demonstrates that the solution p(x, t) to the Fokker-Planck equation is the Wasserstein gradient flow trajectory on the Gibbs free energy functional.</p><p>Steepest descent is always defined with respect to a distance metric. For example, the update equation, x k+1 = x k -η∇Φ(x k ), for classic gradient descent on a potential Φ(x), can be formulated as the solution to the minimization problem x k+1 = argmin x ηΦ(x)+ 1 2 d(x, x k ) 2 where d(x, x k ) = ∥xx k ∥ is the Euclidean distance metric. Gradient flow is the continuous-time limit of gradient descent where we take η → 0 + . Similarly, Wasserstein gradient flow is the continuous-time limit of steepest descent optimization defined by the Wasserstein metric. The Wasserstein metric is a distance metric between probability measures defined as,</p><formula xml:id="formula_72">W 2 2 (µ 1 , µ 2 ) = inf p∈Π(µ 1 ,µ 2 ) R n ×R n |x -y| 2 p(dx, dy),<label>(41)</label></formula><p>where µ 1 and µ 2 are two probability measures on R n with finite second moments and Π(µ 1 , µ 2 ) defines the set of joint probability measures with marginals µ 1 and µ 2 . Thus, given an initial distribution and learning rate η, we can use the Wasserstein metric to derive a sequence of distributions minimizing some functional in the sense of steepest descent. In the continuous-time limit as η → 0 + this sequence defines a continuous trajectory of probability distributions minimizing the functional. <ref type="bibr" target="#b20">Jordan et al. (1997)</ref> proved, through the following theorem, that this process applied to the Gibbs free energy functional converges to the solution to the Fokker-Planck equation with the same initialization:</p><p>Theorem 1 (JKO). Given an initial condition p 0 with finite second moment and an η &gt; 0, define the iterative scheme p η with iterates defined by</p><formula xml:id="formula_73">p k = argmin p η E [Φ] -κ -1 H(p) + W 2 2 (p, p k-1</formula><p>).</p><p>As η → 0 + , then p η → p weakly in L 1 where p is the solution to the Fokker-Planck</p><p>In other words, the Fokker-Plank equation <ref type="bibr" target="#b35">(36)</ref> with anisotropic diffusion can be interpreted as minimizing the expectation of a modified loss Ψ, while being implicitly regularized towards distributions that maximize entropy. The derivation requires we assume a stationary solution p ss exists and that the force j(x) implicitly defined by p ss is conservative. However, rather than implicitly define Ψ(x) and j(x) through assumption, if we can explicitly construct a modified loss Ψ(x) such that the resulting j(x) satisfies certain conditions, then the stationary solution exists and the variational formulation will apply as well. We formalize this statement with the following theorem, Theorem 3 (Explicit Construction). If there exists a potential Ψ(x) such that either j(x) = 0 or ∇ • j(x) = 0 and ∇Ψ(x) ⊥ j(x), then p ss is the Gibbs distribution ∝ e -κΨ(x) and the variational formulation given in Theorem 2 applies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Applying the Variational Formulation to the OU Process</head><p>Through explicit construction we now seek to find analytic expressions for the modified loss Ψ(x) and force j(x) hypothesised by <ref type="bibr" target="#b5">Chaudhari and Soatto (2018)</ref> in the fundamental setting of an OU process with anisotropic diffusion, as described in section D.</p><p>We assume the diffusion matrix is anisotropic, but spatially independent, ∇ • D(x) = 0.</p><p>For the OU process the original potential generating the drift is</p><formula xml:id="formula_74">Φ(x) = (x -µ) ⊺ A 2 (x -µ).<label>(44)</label></formula><p>Recall, that in order to extend the variational formulation we must construct some potential Ψ(x) such that ∇ • j(x) = 0 and ∇Ψ ⊥ j(x). It is possible to construct Ψ(x)</p><p>using the unique decomposition of the drift matrix A = (D + Q)U discussed in ap-pendix D. Define the modified potential,</p><formula xml:id="formula_75">Ψ(x) = (x -µ) ⊺ U 2 (x -µ).<label>(45)</label></formula><p>Using this potential, the force j(x) is</p><formula xml:id="formula_76">j(x) = -A(x -µ) + DU (x -µ) = -QU (x -µ). (<label>46</label></formula><formula xml:id="formula_77">) Notice that j(x) is conservative, ∇ • j(x) = ∇ • -QU (x -µ) = 0 because Q is skew- symmetric. Additionally, j(x) is orthogonal, j(x) ⊺ ∇Ψ(x) = (x -µ) ⊺ U ⊺ QU (x -µ) =</formula><p>0, again because Q is skew-symmetric. Thus, we have determined a modified potential Ψ(x) that results in a conservative orthogonal force j(x) satisfying the conditions for Theorem 3. Indeed the stationary Gibbs distribution given by Theorem 3 agrees with equation ( <ref type="formula" target="#formula_63">33</ref>) derived via the first and second moments in appendix D,</p><formula xml:id="formula_78">e -κΨ(x) ∝ N µ, κ -1 U -1</formula><p>In addition to the variational formulation, this interpretation further details explicitly the stationary probability current, J ss (x) = j(x)p ss , and whether or not the the stationary solution is in broken detailed balance.</p><p>F Explicit expressions for the OU process Generated by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGD</head><p>We will now consider the specific OU process generated by SGD with linear regression.</p><p>Here we repeat the setup as explained in section 5.</p><p>Let X ∈ R N ×d , Y ∈ R N be the input data, output labels respectively and θ ∈ R d be our vector of regression coefficients. The least squares loss is the convex quadratic loss</p><formula xml:id="formula_79">L(θ) = 1 2N ∥Y -Xθ∥ 2 with gradient g(θ) = Hθ -b, where H = X ⊺ X N and b = X ⊺ Y N .</formula><p>Plugging this expression for the gradient into the underdamped Langevin equation ( <ref type="formula" target="#formula_6">6</ref>), and rearranging terms, results in the multivariate Ornstein-Uhlenbeck (OU) process,</p><formula xml:id="formula_80">d     θ t v t     = A         µ 0     -     θ t v t         dt + √ 2κ -1 DdW t ,<label>(47)</label></formula><p>where A and D are the drift and diffusion matrices respectively,</p><formula xml:id="formula_81">A =     0 -I 2 η(1+β) (H + λI) 2(1-β) η(1+β) I     , D =     0 0 0 2(1-β) η(1+β) Σ(θ)     ,<label>(48)</label></formula><formula xml:id="formula_82">κ = S(1 -β 2</formula><p>) is a temperature constant, and µ = (H + λI) -1 b is the ridge regression solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Solving for the Modified Loss and Conservative Force</head><p>In order to apply the expressions derived for a general OU process in appendix D and E, we must first decompose the drift as A = (D + Q)U . Under the simplification Σ(θ) = σ 2 H discussed in appendix B, then the matrices Q and U , as defined below, achieve this,</p><formula xml:id="formula_83">Q =     0 -σ 2 H σ 2 H 0     , U =     2 η(1+β)σ 2 H -1 (H + λI) 0 0 1 σ 2 H -1     .<label>(49)</label></formula><p>Using these matrices we can now derive explicit expressions for the modified loss Ψ(θ, v) and conservative force j(θ, v). First notice that the least squares loss with L 2 regularization is proportional to the convex quadratic,</p><formula xml:id="formula_84">Φ(θ) = (θ -µ) ⊺ (H + λI)(θ -µ).<label>(50)</label></formula><p>The modified loss Ψ is composed of two terms, one that only depends on the position,</p><formula xml:id="formula_85">Ψ θ (θ) = (θ -µ) ⊺ H -1 (H + λI) η(1 + β)σ 2 (θ -µ) ,<label>(51)</label></formula><p>and another that only depends on the velocity,</p><formula xml:id="formula_86">Ψ v (v) = v ⊺ H -1 σ 2 v.<label>(52)</label></formula><p>The conservative force j(θ, v) is</p><formula xml:id="formula_87">j(θ, v) =     v -2 η(1+β) (H + λI) (θ -µ)     ,<label>(53)</label></formula><p>and thus the stationary probability current is J ss (θ, v) = j(θ, v)p ss .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Decomposing the Trajectory into the Eigenbasis of the Hessian</head><p>As shown in appendix D, the temporal distribution for the OU process at some time</p><formula xml:id="formula_88">T ≥ 0 is, p T         θ v         = N     e -AT     θ 0 v 0     + I -e -AT     µ 0     , κ -1 U -1 -e -AT U -1 e -A ⊺ T     .</formula><p>Here we will now use the eigenbasis {q 1 , . . . , q m } of the Hessian with eigenvalues {ρ 1 , . . . , ρ m } to derive explicit expressions for the mean and covariance of the process through time.</p><p>Deterministic component. We can rearrange the expectation as</p><formula xml:id="formula_89">E         θ v         =     µ 0     + e -AT     θ 0 -µ v 0     .</formula><p>Notice that the second, time-dependent term is actually the solution to the system of</p><formula xml:id="formula_90">ODEs     θ v     = -A     θ v     with initial condition θ 0 -µ v 0 ⊺</formula><p>. This system of ODEs can be block diagonalized by factorizing A = OSO ⊺ where O is orthogonal and S is block diagonal defined as</p><formula xml:id="formula_91">O =                 q 1 0 . . . q m 0 . . . 0 q 1 . . . 0 q m                 S =                 0 -1 2 η(1+β) (ρ 1 + λ) 2(1-β) η(1+β) . . . . . . . . . 0 -1 2 η(1+β) (ρ m + λ) 2(1-β) η(1+β)                </formula><p>In otherwords in the plane spanned by q i 0 ⊺ and 0 q i ⊺ the system of ODEs decouples into the 2D system</p><formula xml:id="formula_92">    a i b i     =     0 1 -2 η(1+β) (ρ i + λ) -2(1-β) η(1+β)         a i b i    </formula><p>This system has a simple physical interpretation as a damped harmonic oscillator. If we let b i = ȧi , then we can unravel this system into the second order ODE</p><formula xml:id="formula_93">äi + 2 1 -β η(1 + β) ȧi + 2 η(1 + β) (ρ i + λ)a i = 0</formula><p>which is in standard form (i.e. ẍ + 2γ ẋ + ω 2 x = 0) for γ = 1-β η(1+β) and ω i = 2 η(1+β) (ρ i + λ). Let a i (0) = ⟨θ 0µ, q i ⟩ and b i (0) = ⟨v 0 , q i ⟩, then the solution in terms of γ and ω i is</p><formula xml:id="formula_94">a i (t) =                      e -γt a i (0) cosh γ 2 -ω 2 i t + γa i (0)+b i (0) √ γ 2 -ω 2 i sinh γ 2 -ω 2 i t γ &gt; ω i e -γt (a i (0) + (γa i (0) + b i (0))t) γ = ω i e -γt a i (0) cos ω 2 i -γ 2 t + γa i (0)+b i (0) √ ω 2 i -γ 2 sin ω 2 i -γ 2 t γ &lt; ω i</formula><p>Differentiating these equations gives us solutions for b i (t)</p><formula xml:id="formula_95">b i (t) =                      e -γt b i (0) cosh γ 2 -ω 2 i t - ω 2 i a i (0)+γb i (0) √ γ 2 -ω 2 i sinh γ 2 -ω 2 i t γ &gt; ω i e -γt (b i (0) -(ω 2 i a i (0) + γb i (0)) t) γ = ω i e -γt b i (0) cos ω 2 i -γ 2 t - ω 2 i a i (0)+γb i (0) √ ω 2 i -γ 2 sin ω 2 i -γ 2 t γ &lt; ω i</formula><p>Combining all these results, we can now analytically decompose the expectation as the sum,</p><formula xml:id="formula_96">E         θ v         =     µ 0     + m i=1     a i (t)     q i 0     + b i (t)     0 q i         .</formula><p>Intuitively, this equation describes a damped rotation (spiral) around the OLS solution in the planes defined by the the eigenvectors of the Hessian at a rate proportional to the respective eigenvalue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic component. Using the previous block diagonal decomposition A = OSO ⊺</head><p>we can simplify the variance as</p><formula xml:id="formula_97">Var         θ v         = κ -1 U -1 -e -AT U -1 e -A ⊺ T = κ -1 U -1 -e -OSO ⊺ T U -1 e -OS ⊺ O ⊺ T = κ -1 O O ⊺ U -1 O -e -ST (O ⊺ U -1 O)e -ST ⊺ O ⊺ Interestingly, the matrix O ⊺ U -1 O is also block diagonal, O ⊺ U -1 O = O ⊺     η(1+β)σ 2 2 (H + λI) -1 H 0 0 σ 2 H     O =                 η(1+β)σ 2 2 ρ 1 ρ 1 +λ 0 0 σ 2 ρ 1 . . . . . . . . . η(1+β)σ 2 2 ρm ρm+λ 0 0 σ 2 ρ m                </formula><p>Thus, similar to the mean, we can simply consider the variance in each of the planes spanned by q i 0 ⊺ and 0 q i ⊺ . If we define the block matrices,</p><formula xml:id="formula_98">D i =     ησ 2 2S(1-β) ρ i ρ i +λ 0 0 σ 2 S(1-β 2 ) ρ i     S i =     0 1 -2 η(1+β) (ρ i + λ) -2(1-β) η(1+β)    </formula><p>then the projected variance matrix in this plane simplifies as</p><formula xml:id="formula_99">Var         q ⊺ i θ q ⊺ i v         = D i -e -S i T D i e -S i T ⊺</formula><p>Using the solution to a damped harmonic osccilator discussed previously, we can express the matrix exponential e -S i T explicitly in terms of γ = 1-β η(1+β) and</p><formula xml:id="formula_100">ω i = If we let α i = |γ 2 -ω 2 i |, then the matrix exponential is e -S i t =                                                  e -γt      cosh (α i t) + γ α i sinh (α i t) 1 α i sinh (α i t) - ω 2 i α i sinh (α i t) cosh (α i t) -γ α i sinh (α i t)      γ &gt; ω i e -γt      1 + γt t -ω 2 i t 1 -γt      γ = ω i e -γt      cos (α i t) + γ α i sin (α i t) 1 α i sin (α i t) - ω 2 i α i sin (α i t) cos (α i t) -γ α i sin (α i t)      γ &lt; ω i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Analyzing Properties of the Stationary Solution</head><p>Assuming the stationary solution is given by equation <ref type="bibr" target="#b10">(11)</ref> we can solve for the expected value of the norm of the local displacement and gain some intuition for the expected value of the norm of global displacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Instantaneous Speed</head><formula xml:id="formula_101">E ss ∥δ k ∥ 2 = E ss ∥θ k+1 -θ k ∥ 2 = η 2 E ss ∥v k+1 ∥ 2 = η 2 tr E ss v k+1 v ⊺ k+1 = η 2 tr (Var ss (v k+1 ) + E ss [v k+1 ] E ss [v k+1 ] ⊺ ) = η 2 tr κ -1 U -1 = η 2 S(1 -β 2 ) tr σ 2 H</formula><p>Note that this follows directly from the definition of δ k in equation ( <ref type="formula" target="#formula_0">1</ref>) and the mean and variance of the stationary solution in equation <ref type="bibr" target="#b10">( 11)</ref>, as well as the follow-up derivation in appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Anomalous Diffusion</head><p>Notice, that the global movement ∆ t = θ tθ 0 can be broken up into the sum of the local movements ∆ t = t i=1 δ i , where δ i = θ iθ i-1 . Applying this decomposition,</p><formula xml:id="formula_102">E ss ∥∆ t ∥ 2 = E ss   t i=1 δ i 2   = t i=1 E ss ∥δ i ∥ 2 + t i̸ =j E ss [⟨δ i , δ j ⟩]</formula><p>As we solved for previously,</p><formula xml:id="formula_103">E ss ∥δ i ∥ 2 = η 2 E ss ∥v i ∥ 2 = η 2 tr (Var ss (v i )) = η 2 S(1 -β 2 ) tr σ 2 H .</formula><p>By a similar simplification, we can express the second term in terms of the stationary cross-covariance,</p><formula xml:id="formula_104">E ss [⟨δ i , δ j ⟩] = η 2 E ss [⟨v i , v j ⟩] = η 2 tr (Cov ss (v i , v j )) .</formula><p>Thus, to simplify this expression we just need to consider the velocity-velocity covariance Cov ss (v i , v j ). At stationarity, the cross-covariance for the system in phase space,</p><formula xml:id="formula_105">z i = θ i v i is Cov ss (z i , z j ) = κ -1 U -1 e -A ⊺ |i-j|</formula><p>where κ = S(1β 2 ), and</p><formula xml:id="formula_106">U =     2 η(1+β)σ 2 H -1 (H + λI) 0 0 1 σ 2 H -1     A =     0 -I 2 η(1+β) (H + λI) 2(1-β) η(1+β) I    </formula><p>As discussed when solving for the mean of the OU trajectory, the drift matrix A can be block diagonalized as A = OSO ⊺ where O is orthogonal and S is block diagonal defined as</p><formula xml:id="formula_107">O =                </formula><p>q 1 0 . . . q m 0 . . .</p><formula xml:id="formula_108">0 q 1 . . . 0 q m                 , S =                 0 -1 2 η(1+β) (ρ 1 + λ) 2(1-β) η(1+β)</formula><p>. . . . . . . . .</p><formula xml:id="formula_109">0 -1 2 η(1+β) (ρ m + λ) 2(1-β) η(1+β)                </formula><p>.</p><p>Notice also that O diagonalizes U -1 such that, </p><formula xml:id="formula_110">Λ = O ⊺ U -1 O =                 η(1+β)σ 2 2 ρ 1 ρ 1 +λ</formula><formula xml:id="formula_111">e -S k |i-j| =                                                  e -γτ      cosh (α k τ ) + γ α k sinh (α k τ ) 1 α k sinh (α k τ ) - ω 2 k α k sinh (α k τ ) cosh (α k τ ) -γ α k sinh (α k τ )      γ &gt; ω k e -γτ      1 + γτ τ -ω 2 k τ 1 -γτ      γ = ω k e -γτ      cos (α k τ ) + γ α k sin (α k τ ) 1 α k sin (α k τ ) - ω 2 k α k sin (α k τ ) cos (α k τ ) -γ α k sin (α k τ )      γ &lt; ω k</formula><p>Plugging in these expressions into previous expression and restricting to just the k th velocity component, we see</p><formula xml:id="formula_112">Cov ss (v i,k , v j,k ) = κ -1 Λ k e -S ⊺ k |i-j| 1,1 =                    κ -1 σ 2 ρ k e -γτ cosh (α k τ ) -γ α k sinh (α k τ ) γ &gt; ω k κ -1 σ 2 ρ k e -γτ (1 -γτ ) γ = ω k κ -1 σ 2 ρ k e -γτ cos (α k τ ) -γ α k sin (α k τ ) γ &lt; ω k</formula><p>Pulling it all together,</p><formula xml:id="formula_113">E ss ∥∆ t ∥ 2 = η 2 σ 2 S(1 -β 2 )</formula><p>tr (H) t + 2t where C l (k) is defined as This relationship between the expected potential and kinetic energy can be understood as a form of the equipartition theorem.</p><formula xml:id="formula_114">C l (k) =                   </formula><p>H Phase space oscillation rates in Deep Learning</p><p>In Fig. <ref type="figure">6</ref> we showed the position and velocity of the weights over training time, projected onto the first and 30th eigenvector of the Hessian. To supplement the qualitative observation that the oscillations in phase space seemed to occur at different rates, as is the case in linear regression, we measured a quantifiable difference in oscillation frequency for the position of the weights projected onto different eigenvectors by looking at the amplitude-weighted average of the frequencies identified by the fast Fourier</p><p>Transform. The velocity of the weights showed a smaller difference in the predominant oscillation frequency, but it was still noticeable.</p><p>H Phase space oscillation rates in Deep Learning</p><p>In Fig. <ref type="figure">6</ref> we showed the position and velocity of the weights over training time, projected onto the first and 30th eigenvector of the Hessian. To supplement the qualitative observation that the oscillations in phase space seemed to occur at different rates, as is the case in linear regression, we measured a quantifiable difference in oscillation frequency for the position of the weights projected onto different eigenvectors by looking at the amplitude-weighted average of the frequencies identified by the fast Fourier</p><p>Transform. The velocity of the weights showed a smaller difference in the predominant oscillation frequency, but it was still noticeable. Table 3: Figures 4,5,6 experiments training hyperparameters. I.6 Figure 7 We resumed training for an ImageNet pre-trained ResNet-18 from Torchvision Paszke et al. (2017) for 2 epochs, using the sweeps of hyperparameters shown in table 4. We indicate a sweep in a particular hyperparameter by [A, B], which denotes 20 evenly spaced numbers between A and B, inclusive. We kept track of the norms of the local and global displacement, ∥δ k ∥ 2 2 and ∥∆ k ∥ 2 2 ,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>al. (2016); Neal (1996); Lee et al. (2017); Jacot et al. (2018); Lee et al. (2019); Song et al. (2018);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>et al. (2017); Wan et al. (2020); Baity-Jesi et al. (2018); Chen et al. (2020). To demonstrate this behavior, we resume training of pre-trained convolutional networks while tracking the network trajectory through parameter space. Let θ * ∈ R m be the parameter vector for a pre-trained network and θ k ∈ R m be the parameter vector after k steps of resumed training. We track two metrics of the training trajectory, namely the local parameter displacement δ k between consecutive steps, and the global displacement ∆ k after k steps from the pre-trained initialization:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Despite performance convergence, the network continues to move through parameter space. We plot the squared Euclidean norm for the local and global displacement (δ k and ∆ k ) of five classic convolutional neural network architectures. The networks are standard Pytorch models pre-trained on ImageNet Paszke et al. (2017). Their training is resumed for 10 additional epochs. We show the global displacement on a log-log plot where the slope of the least squares line c is the exponent of the power law ∥∆ k ∥ 2 2 ∝ k c . See appendix I for experimental details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>- dard Brownian motion diffusion corresponds to c = 1 .</head><label>1</label><figDesc>Similar observation were made by Baity-Jesi et al. (2018) who noticed distinct phases of the training trajectory evident in the dynamics of the global displacement and Chen et al. (2020) who found that the exponent of diffusion changes through the course of training. A parallel observation is given by Hoffer et al. (2017) for the beginning of training, where they measure the global displacement from the initialization of an untrained network and observe a rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>dynamics of deep neural networks trained with SGD. Our analysis and experiments build upon this literature.Continuous models for SGD. Many works consider how to improve the classic gradient flow model for SGD to more realistically reflect momentum<ref type="bibr" target="#b44">Qian (1999)</ref>, discretization due to finite learning ratesKunin et  al. (2020); Barrett and Dherin (2020), and stochasticity due to random batches Li et al. (2017); Smith et al. (2021). One line of work has studied the dynamics of networks in their infinite width limits through dynamical mean field theory Mignacco et al. (2020); Mannelli et al. (2020); Mignacco et al. (2021); Mannelli and Urbani (2021), while a different approach has used stochastic differential equations (SDEs) to model SGD directly, the approach we take in this work. However, recently, the validity of this approach has been questioned. The main argument, as nicely explained in<ref type="bibr" target="#b56">Yaida (2018)</ref>, is that most SDE approximations simultaneously assume that ∆t → 0 + , while maintaining that the learning rate η = ∆t is finite. The works<ref type="bibr" target="#b50">Simsekli et al. (2019)</ref> and<ref type="bibr" target="#b31">Li et al. (2021)</ref> have questioned the correctness of the using the central limit theorem (CLT) to model the gradient noise as Gaussian, arguing respectively that the heavy-tailed structure in the gradient noise and the weak dependence between batches leads the CLT to break down. In our work, we maintain the CLT assumption holds, which we discuss further in appendix A, but impor-tantly we avoid the pitfalls of many previous SDE approximations by simultaneously modeling the effect of finite learning rates and stochasticity.Limiting dynamics. A series of works have applied SDE models of SGD to study the limiting dynamics of neural networks. In the seminal work by<ref type="bibr" target="#b35">Mandt et al. (2016)</ref>, the limiting dynamics were modeled with a multivariate Ornstein-Uhlenbeck process by combining a first-order SDE model for SGD with assumptions on the geometry of the loss and covariance matrix for the gradient noise. This analysis was extended by Jastrzębski et al. (2017) through additional assumptions on the covariance matrix to gain tractable insights and applied by<ref type="bibr" target="#b0">Ali et al. (2020)</ref> to the simpler setting of linear regression, which has a quadratic loss. A different approach was taken by Chaudhari and Soatto (2018), which did not formulate the dynamics as an OU process, nor assume directly a structure on the loss or gradient noise. Rather, this analysis studied the same first-order SDE via the Fokker-Planck equation to propose the existence of a modified loss and probability currents driving the limiting dynamics, but did not provide explicit expressions. Our analysis deepens and combines ideas from all these works, where our key insight is to lift the dynamics into phase space. By studying the dynamics of the parameters and their velocities, and by applying the analysis first in the setting of linear regression where assumptions are provably true, we are able to identify analytic expressions and explicit insights which lead to concrete predictions and testable hypothesis. Stationary dynamics. A different line of work avoids modeling the limiting dynamics of SGD with an SDE and instead chooses to leverage the property of stationarity. These works Yaida (2018); Zhang et al. (2019); Liu et al. (2021); Ziyin et al. (2021) assume that eventually the probability distribution governing the model parameters reaches sta-tionarity such that the discrete SGD process is simply sampling from this distribution. Yaida (2018) used this approach to derive fluctuation-dissipation relations that link measurable quantities of the parameters and hyperparameters of SGD. Liu et al. (2021) used this approach to derive properties for the stationary distribution of SGD with a quadratic loss. Similar to our analysis, this work identifies that the stationary distribution for the parameters reflects a modified loss function dependent on the relationship between the covariance matrix of the gradient noise and the Hessian matrix for the original loss. Empirical exploration. Another set of works analyzing the limiting dynamics of SGD has taken a purely empirical approach. Building on the intuition that flat minima generalize better than sharp minima, Keskar et al. (2016) demonstrated empirically that the hyperparameters of optimization influence the eigenvalue spectrum of the Hessian matrix at the end of training. Many subsequent works have studied the Hessian eigenspectrum during and at the end of training. Jastrzębski et al. (2018); Cohen et al. (2021) studied the dynamics of the top eigenvalues during training. Sagun et al. (2017); Papyan (2018); Ghorbani et al. (2019) demonstrated the spectrum has a bulk of values near zero plus a small number of larger outliers. Gur-Ari et al. (2018) demonstrated that the learning dynamics are constrained to the subspace spanned by the top eigenvectors, but found no special properties of the dynamics within this subspace. In our work we also determine that the top eigensubspace of the Hessian plays a crucial role in the limiting dynamics and by projecting the dynamics into this subspace in phase space, we see that the motion is not random, but consists of incoherent oscillations leading to anomalous diffusion. Hyperparameter schedules and algorithm development. Lastly, a set of works have used theoretical and empirical insights of the limiting dynamics to construct hyperparameter schedules and algorithms to improve performance. Most famously, the linear scaling rule, derived by Krizhevsky (2014) and Goyal et al. (2017), relates the influence of the batch size and the learning rate, facilitating stable training with increased parallelism. This relationship was extended by Smith et al. (2017) to account for the effect of momentum. Lucas et al. (2018) proposed a variant of SGD with multiple velocity buffers to increase stability in the presence of a nonuniform Hessian spectrum. Chaudhari et al. (2019) introduced a variant of SGD guided by local entropy to bias the dynamics into wider minima. Izmailov et al. (2018) demonstrated how a simple algorithm of stochastically averaging samples from the limiting dynamics of a network can improve generalization performance. While algorithm development is not the focus of our work, we believe that our careful and precise understanding of the deep learning limiting dynamics will similarly provide insight for future work in this direction. 4 Modeling SGD as an Underdamped Langevin Equation Following the route of previous works Mandt et al. (2016); Jastrzębski et al. (2017); Chaudhari and Soatto (2018) studying the limiting dynamics of neural networks, we first seek to model SGD as a continuous stochastic process. We consider a network parameterized by θ ∈ R m , a training dataset {x 1 , . . . , x N } of size N , and a training loss L(θ) = 1 N N i=1 ℓ(θ, x i ) with corresponding gradient g(θ) = ∂L ∂θ . The state of the network at the k th step of training is defined by the position vector θ k and velocity vector v k of the same dimension. The gradient descent update with learning rate η, momentum β, and weight decayλ is given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An anisotropic OU process is driven by a modified loss. We sample from an OU process with anisotropic diffusion and plot the trajectory (same black line on both plots). The left plot shows the original loss Φ(x) generating the drift. The right plot shows the modified loss Ψ(x). Notice the trajectory more closely resembles the curvature of Ψ(x) than Φ(x). The grey lines depict the stationary probability current</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>train &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n M m I z / X 8 6 x 5 + d g n x j P b 1 5 / T S n V 8 = " &gt; A A A C A 3 i c b Z D L S s N A F I Y n 9 V b r L e p O N 8 E i u C q J C L o s u n H h o o K 9 Q B P C Z D p p h 0 4 m Y e Z E L C H g x l d x 4 0 I R t 7 6 E O 9 / G S Z u F t v 4 w 8 P G f c 5 h z / i D h T I F t f x u V p e W V 1 b X q e m 1 j c 2 t 7 x 9 z d 6 6 g 4 l Y S 2 S c x j 2 Q u w o p w J 2 g Y G n P Y S S X E U c N o N x l d F v X t P p W K x u I N J Q r 0 I D w U L G c G g L d 8 8 c C M M I 4 J 5 d p P 7 m Q v 0 A T K Q m I k 8 9 8 2 6 3 b C n s h b B K a G O S r V 8 8 8 s d x C S N q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>8 / g C F e p i 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n M m I z / X 8 6 x 5 + d g n x j P b 1 5 / T S n V 8 = " &gt; A A A C A 3 i c b Z D L S s N A F I Y n 9 V b r L e p O N 8 E i u C q J C L o s u n H h o o K 9 Q B P C Z D p p h 0 4 m Y e Z E L C H g x l d x 4 0 I R t 7 6 E O 9 / G S Z u F t v 4 w 8 P G f c 5 h z / i D h T I F t f x u V p e W V 1 b X q e m 1 j c 2 t 7 x 9 z d 6 6 g 4 l Y S 2 S c x j 2 Q u w o p w J 2 g Y G n P Y S S X E U c N o N x l d F v X t P p W K x u I N J Q r 0 I D w U L G c G g L d 8 8 c C M M I 4 J 5 d p P 7 m Q v 0 A T K Q m I k 8 9 8 2 6 3 b C n s h b B K a G O S r V 8 8 8 s d x C S N q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>8 / g C F e p i 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n M m I z / X 8 6 x 5 + d g n x j P b 1 5 / T S n V 8 = " &gt; A A A C A 3 i c b Z D L S s N A F I Y n 9 V b r L e p O N 8 E i u C q J C L o s u n H h o o K 9 Q B P C Z D p p h 0 4 m Y e Z E L C H g x l d x 4 0 I R t 7 6 E O 9 / G S Z u F t v 4 w 8 P G f c 5 h z / i D h T I F t f x u V p e W V 1 b X q e m 1 j c 2 t 7 x 9 z d 6 6 g 4 l Y S 2 S c x j 2 Q u w o p w J 2 g Y G n P Y S S X E U c N o N x l d F v X t P p W K x u I N J Q r 0 I D w U L G c G g L d 8 8 c C M M I 4 J 5 d p P 7 m Q v 0 A T K Q m I k 8 9 8 2 6 3 b C n s h b B K a G O S r V 8 8 8 s d x C S N q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>8 / g C F e p i 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n M m I z / X 8 6 x 5 + d g n x j P b 1 5 / T S n V 8 = " &gt; A A A C A 3 i c b Z D L S s N A F I Y n 9 V b r L e p O N 8 E i u C q J C L o s u n H h o o K 9 Q B P C Z D p p h 0 4 m Y e Z E L C H g x l d x 4 0 I R t 7 6 E O 9 / G S Z u F t v 4 w 8 P G f c 5 h z / i D h T I F t f x u V p e W V 1 b X q e m 1 j c 2 t 7 x 9 z d 6 6 g 4 l Y S 2 S c x j 2 Q u w o p w J 2 g Y G n P Y S S X E U c N o N x l d F v X t P p W K x u I N J Q r 0 I D w U L G c G g L d 8 8 c C M M I 4 J 5 d p P 7 m Q v 0 A T K Q m I k 8 9 8 2 6 3 b C n s h b B K a G O S r V 8 8 8 s d x C S N q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>8 / g C F e p i 3 &lt; / l a t e x i t &gt; L test &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 e i g a V 1 g 7 L e R 2 f K 5 S O J 8 f X 0 u t b I = " &gt; A A A C A n i c b Z D L S s N A F I Y n 9 V b r L e p K 3 A S L 4 K o k I u i y 6 M a F i w r 2 A m 0 I k + l p O 3 R y Y e Z E L C G 4 8 V X c u F D E r U / h z r d x 0 m a h r T 8 M f P z n H O a c 3 4 8 F V 2 j b 3 0 Z p a X l l d a 2 8 X t n Y 3 N r e M X f 3 W i p K J I M m i 0 Q k O z 5 V I H g I T e Q o o B N L o I E v o O 2 P r / J 6 + x 6 k 4 l F 4 h 5 M Y 3 I A O Q z 7 g j K K 2 P P O g F 1 A c M S r S m 8 x L e w g P m C I o z D L P r N o 1 e y p r E Z w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>v x p P x Y r w b H 7 P W k l H M 7 J M / M j 5 / A M d c m E 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 e i g a V 1 g 7 L e R 2 f K 5 S O J 8 f X 0 u t b I = " &gt; A A A C A n i c b Z D L S s N A F I Y n 9 V b r L e p K 3 A S L 4 K o k I u i y 6 M a F i w r 2 A m 0 I k + l p O 3 R y Y e Z E L C G 4 8 V X c u F D E r U / h z r d x 0 m a h r T 8 M f P z n H O a c 3 4 8 F V 2 j b 3 0 Z p a X l l d a 2 8 X t n Y 3 N r e M X f 3 W i p K J I M m i 0 Q k O z 5 V I H g I T e Q o o B N L o I E v o O 2 P r / J 6 + x 6 k 4 l F 4 h 5 M Y 3 I A O Q z 7 g j K K 2 P P O g F 1 A c M S r S m 8 x L e w g P m C I o z D L P r N o 1 e y p r E Z w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><figDesc>v x p P x Y r w b H 7 P W k l H M 7 J M / M j 5 / A M d c m E 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 e i g a V 1 g 7 L e R 2 f K 5 S O J 8 f X 0 u t b I = " &gt; A A A C A n i c b Z D L S s N A F I Y n 9 V b r L e p K 3 A S L 4 K o k I u i y 6 M a F i w r 2 A m 0 I k + l p O 3 R y Y e Z E L C G 4 8 V X c u F D E r U / h z r d x 0 m a h r T 8 M f P z n H O a c 3 4 8 F V 2 j b 3 0 Z p a X l l d a 2 8 X t n Y 3 N r e M X f 3 W i p K J I M m i 0 Q k O z 5 V I H g I T e Q o o B N L o I E v o O 2 P r / J 6 + x 6 k 4 l F 4 h 5 M Y 3 I A O Q z 7 g j K K 2 P P O g F 1 A c M S r S m 8 x L e w g P m C I o z D L P r N o 1 e y p r E Z w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>v x p P x Y r w b H 7 P W k l H M 7 J M / M j 5 / A M d c m E 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 e i g a V 1 g 7 L e R 2 f K 5 S O J 8 f X 0 u t b I = " &gt; A A A C A n i c b Z D L S s N A F I Y n 9 V b r L e p K 3 A S L 4 K o k I u i y 6 M a F i w r 2 A m 0 I k + l p O 3 R y Y e Z E L C G 4 8 V X c u F D E r U / h z r d x 0 m a h r T 8 M f P z n H O a c 3 4 8 F V 2 j b 3 0 Z p a X l l d a 2 8 X t n Y 3 N r e M X f 3 W i p K J I M m i 0 Q k O z 5 V I H g I T e Q o o B N L o I E v o O 2 P r / J 6 + x 6 k 4 l F 4 h 5 M Y 3 I A O Q z 7 g j K K 2 P P O g F 1 A c M S r S m 8 x L e w g P m C I o z D L P r N o 1 e y p r E Z w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The training trajectory behaves isotropically, regardless of the training loss. We resume training of a pre-trained ResNet-18 model on ImageNet and project its parameter trajectory (black line) onto the space spanned by the eigenvectors of itspre-trained Hessian q 1 , q 30 (with eigenvalue ratio ρ 1 /ρ 30 ≃ 6). We sample the training and test loss within the same 2D subspace and visualize them as a heatmap in the left and center panels respectively. We visualize the modified loss computed from the eigenvalues (ρ 1 , ρ 30 ) and optimization hyperparameters according to equation<ref type="bibr" target="#b14">(15)</ref> in the right plot. Note the projected trajectory is isotropic, despite the anisotropy of the training and test loss.</figDesc><graphic coords="24,126.62,201.65,354.65,118.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Implicit velocity regularization defined by the inverse Hessian. The shape of the projected velocity trajectory closely resembles the contours of the modified loss Ψ v , equation (16).</figDesc><graphic coords="25,125.80,188.67,352.49,352.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Understanding how the hyperparameters of optimization influence the diffusion. We resume training of pre-trained ResNet-18 models on ImageNet using a range of learning rates, batch sizes, and momentum coefficients, tracking ∥δ t ∥ 2 and ∥∆ t ∥ 2 . Starting from the default hyperparameters, namely η = 1e -4, S = 256, and β = 0.9, we vary each one while keeping the others fixed. The top row shows the measured ∥δ t ∥ 2 in color, with the default hyperparameter setting highlighted in black.The dotted line depicts the predicted value from equation<ref type="bibr" target="#b17">(18)</ref>. The bottom row shows the estimated exponent c found by fitting a line to the ∥∆ t ∥ 2 trajectories on a log-log plot. The dotted line shows c = 1, corresponding to standard diffusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><figDesc>Using the same sweep of models described previously, we can empirically confirm that the optimization hyperparameters each influence the diffusion exponent c. As shown in Fig.7, the learning rate, batch size, and momentum can each independently drive the exponent c into different regimes of anomalous diffusion. Notice how the influence of the learning rate and momentum on the diffusion exponent c closely resembles their respective influences on the damping ratio ζ l . Interestingly, a larger learning rate leads to underdamped oscillations, and the resultant temporal velocities' anti-correlations reduce the exponent of anomalous diffusion. Thus contrary to intuition, a larger learning rate actually leads to slower global transport in parameter space. The batch size on the other hand, has no influence on the damping ratio, but leads to an interesting, non-monotonic influence on the diffusion exponent. Overall, the hyperparameters of optimization and eigenspectrum of the Hessian all conspire to govern the degree of anomalous diffusion at the end of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Originality.</head><figDesc>The empirical phenomena we present provide novel insight with respect to the works of Wan et al. (2020), Hoffer et al. (2017), and Chen et al. (2020). We observe that all parameters in the network (not just those with scale symmetry) move at a constant instantaneous speed at the end of training and diffuse anomalously at rates determined by the hyperparameters of optimization. In contrast to the work by Liu et al. (2021), we modeled the entire SGD process as an OU process which allows us to provide insight into the transient dynamics and identify oscillations in parameter and velocity space. We build on the theoretical framework used by Chaudhari and Soatto(2018) and provide explicit expressions for the limiting dynamics in the simplified linear regression setting and conclude that the oscillations present in the limiting dynamics are more likely to be space-filling curves (and not limit cycles) in deep learning due to many incommensurate oscillations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Notice how the momentum coefficient β ∈ [0, 1 ]</head><label>1</label><figDesc>regulates the amount of backward Euler incorporated into the discretization. When β = 0, we remove all backward Euler discretization leaving just the forward Euler discretization. When β = 1, we have equal amounts of backward Euler as forward Euler resulting in a central second-order discretization 2 as noticed in<ref type="bibr" target="#b44">Qian (1999)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><figDesc>et al. (2016), Jastrzębski et al. (2017), and Poggio et al. (2017). Particularly, Mandt et al. (2016) discuss how this assumption makes sense for smooth loss functions for which the stationary solution to the stochastic process reaches a deep local minimum from which it is difficult to escape. It is a well-studied fact, both empirically and theoretically, that the Hessian is lowrank near local minima as noted by Sagun et al. (2016), and Kunin et al. (2020). This degeneracy results in flat directions of equal loss. Kunin et al. (2020) discuss how differentiable symmetries, architectural features that keep the loss constant under certain weight transformations, give rise to these flat directions. Importantly, the Hessian and the covariance matrix share the same null space, and thus we can always restrict ourselves to the image space of the Hessian, where the drift and diffusion matrix will be full rank. Further discussion on the relationship between the Hessian and the covariance matrix can be found in Thomas et al. (2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>λ i -λ j λ i +λ</head><figDesc>j exists and (D + Q) is invertible. See Kwon et al. (2005) for a discussion on the singularities of this decomposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>.</head><figDesc>Applying these decompositions, properties of matrix exponentials, and the cyclic invariance of the trace, allows us to express the trace of the cross-covariance astr (Cov ss (z i , z j )) = κ -1 tr U -1 e -A ⊺ |i-j| = κ -1 tr U -1 Oe -S ⊺ |i-j| O ⊺ = κ -1 tr Λe -S ⊺ |i-j| = κ -1 n k=1 tr Λ k e -S ⊺k |i-j| where Λ k and S k are the blocks associated with each eigenvector of H. As solved for previously in the variance of the OU process, we can express the matrix exponential e -S k |i-j| explicitly in terms of γ = 1-β η(1+β) and ω k = |i -j| and α k = |γ 2ω 2 k |, then the matrix exponential is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>e 2 H + λ 2 ∥µ∥ 2 1 S( 1 -β 2 )</head><label>22112</label><figDesc>-γk cosh (α l k) -γ α l sinh (α l k) γ &gt; ω l e -γk (1γk) γ = ω l e -γk cos (α l k) -γ α l sin (α l k) γ &lt; ω lfor γ = 1-β η(1+β) , ω l = 2 η(1+β) (ρ l + λ), and α l = |γ 2ω 2 l |.G.3 Training Loss and Equipartition TheoremIn addition to solving for the expected values of the local and global displacements, we can consider the expected training loss and find an interesting relationship to the equipartition theorem from classical statistical mechanics.The regularized training loss isL λ (θ) = 1 2 (θµ) ⊺ H(θµ) + λ 2 ∥θ∥ 2, where H is the Hessian matrix and µ is the true mean. Taking the expectation with respect to the stationary distribution,E ss [L λ (θ)] = 1 2 tr ((H + λI)E ss [θθ ⊺ ])µ ⊺ HE ss [θ] + 1 2 µ ⊺ HµThe first and second moments of the stationary solution areE ss [θ] = µ E ss [θθ ⊺ ] = η S(1β) σ 2 2 (H + λI) -1 H + µµ ⊺Plugging these expressions in and canceling terms we getE ss [L λ (θ)] = η 4S(1β) tr σDefine the kinetic energy of the network as K(v) = 1 2 m∥v∥ 2 , where m = η 2 (1 + β) is the per-parameter "mass" of the network according to our previously derived Langevin dynamics. At stationarity,E ss [K(v)] = η(1 + β) 4 tr (E ss [vv ⊺ ]) = η 4S(1β) tr σ 2 Hwhere we used the fact that E ss [vv ⊺ ] = σ 2 H. In otherwords, at stationarity,E ss [L λ (θ)] = E ss [K(v)] + λ 2 ∥µ∥ 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 8 :Figure 8 :I. 5 Figures 4 , 5 , and 6 For Figure 4 ,ResNet</head><label>8854564</label><figDesc>Figure 8: Phase space oscillations at different rates are determined by the eigendecomposition of the Hessian Weight positions oscillate at a quantifiably different frequencies when projected onto different eigenvectors of the Hessian. Velocities fol-</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>η(1+β) (ρ i + λ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>η(1+β) (ρ k + λ). If we let τ =</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Jing An</rs>, <rs type="person">Pratik Chaudhari</rs>, <rs type="person">Ekdeep Singh</rs>, <rs type="person">Ben Sorscher</rs> and <rs type="person">Atsushi Yamamura</rs> for helpful discussions. This work was funded in part by the <rs type="institution">IBM-Watson AI Lab</rs>. D.K. thanks the <rs type="programName">Stanford Data Science Scholars program</rs> and <rs type="funder">NTT Research</rs> for support. J.S. thanks the <rs type="funder">Mexican National Council of Science and Technology (CONACYT)</rs> for support. S.G. thanks the <rs type="person">James S. McDonnell</rs> and <rs type="person">Simons Foundations</rs>, <rs type="funder">NTT Research</rs>, and an <rs type="funder">NSF</rs> <rs type="grantName">CAREER Award</rs> for support while at <rs type="funder">Stanford</rs>. D.L.K.Y thanks the <rs type="funder">McDonnell Foundation</rs> (<rs type="grantName">Understanding Human Cognition Award Grant</rs> No. <rs type="grantNumber">220020469</rs>), the <rs type="funder">Simons Foundation (Collaboration on the Global Brain</rs> Grant No. <rs type="grantNumber">543061</rs>), the <rs type="funder">Sloan Foundation</rs> (Fellowship <rs type="grantNumber">FG-2018-10963</rs>), the <rs type="funder">National Science Foundation</rs> (<rs type="grantNumber">RI 1703161</rs> and <rs type="funder">CAREER</rs> Award <rs type="grantNumber">1844724</rs>), and the <rs type="institution">DARPA Machine Common Sense program</rs> for support and the <rs type="institution">NVIDIA Corporation</rs> for hardware donations.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4MHGG5k">
					<orgName type="program" subtype="full">Stanford Data Science Scholars program</orgName>
				</org>
				<org type="funding" xml:id="_tj7qU7h">
					<orgName type="grant-name">CAREER Award</orgName>
				</org>
				<org type="funding" xml:id="_YB5QtRx">
					<idno type="grant-number">220020469</idno>
					<orgName type="grant-name">Understanding Human Cognition Award Grant</orgName>
				</org>
				<org type="funding" xml:id="_nW8s9cM">
					<idno type="grant-number">543061</idno>
				</org>
				<org type="funding" xml:id="_spTqpUR">
					<idno type="grant-number">FG-2018-10963</idno>
				</org>
				<org type="funding" xml:id="_hfxfefp">
					<idno type="grant-number">RI 1703161</idno>
				</org>
				<org type="funding" xml:id="_9DHxTx3">
					<idno type="grant-number">1844724</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Decomposing the Drift Matrix</head><p>While the Lyapunov equation simplifies the expressions for the covariance and crosscovariance, it does not explain how to actually solve for the unknown matrix B. Following a method proposed by <ref type="bibr" target="#b26">Kwon et al. (2005)</ref>, we will show how to solve for B explicitly in terms of the drift A and diffusion D.</p><p>The drift matrix A can be uniquely decomposed as,</p><p>where D is our symmetric diffusion matrix, Q is a skew-symmetric matrix (i.e. Q = -Q ⊺ ), and U is a positive definite matrix. Using this decomposition, then B = U -1 , solves the Lyapunov equation.</p><p>Proof. Plug B = U -1 into the left-hand side of equation ( <ref type="formula">25</ref>),</p><p>Here we used the symmetry of A, D, U and the skew-symmetry of Q.</p><p>All that is left is to do is solve for the unknown matrices Q and U . First notice the following identity,</p><p>Proof. Consider when D(X t ) is an anisotropic, spatially-dependent diffusion matrix. In this setting, the original Gibbs distribution given in equation <ref type="bibr" target="#b38">(39)</ref> does not necessarily satisfy the stationarity condition equation <ref type="bibr" target="#b37">(38)</ref>. In fact, it is not immediately clear what the stationary solution is or if the dynamics even have one. Thus, Chaudhari and Soatto (2018) make the following assumption:</p><p>Stationary Assumption. Assume there exists a unique distribution p ss that is the stationary solution to the Fokker-Planck equation irregardless of initial conditions.</p><p>Under this assumption we can implicitly define the potential Ψ(x) = -κ -1 log(p ss (x)).</p><p>Using this modified potential we can express the stationary solution as a Gibbs distribution,</p><p>Under this implicit definition we can define the stationary probability current as J ss (x) = j(x)p ss (x) where</p><p>The vector field j(x) reflects the discrepancy between the original potential Φ and the modified potential Ψ according to the diffusion D(x). Notice that in the isotropic case, when D(x) = I, then Φ = Ψ and j(x) = 0. Chaudhari and Soatto (2018) notice the following additional properties of j(x):</p><p>1. p ss is in detailed balance iff j(x) = 0 for all x ∈ Ω.</p><p>Chaudhari and Soatto (2018) introduce another property of j(x) through assumption, Conservative Assumption. Assume that the force j(x) is conservative (i.e. ∇ • j(x) = 0).</p><p>Using this assumption, Chaudhari and Soatto (2018) extends the variational formulation provided by the JKO theorem to the anisotropic setting, Theorem 2 (CS). Given an initial condition p 0 with finite second moment, then the energetic-entropic functional,</p><p>monotonically decreases throughout the trajectory given by the solution to the Fokker-Planck equation with the given initial condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Experimental Details</head><p>A version of our code, used to run all experiments and generate all the figures in this paper, can be found at <ref type="url" target="https://github.com/danielkunin/rethinking-SGD">https://github.com/danielkunin/rethinking-SGD</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 Computing the Hessian eigendecomposition</head><p>Computing the full Hessian of the loss with respect to the parameters is computationally intractable for large models. However, equipped with an autograd engine, we can compute Hessian-vector products. We use the subspace iteration on Hessian-vector products computed on a variety of datasets. For Cifar-10 we use the entire train dataset to compute the Hessian-vector products. For Imagenet, we use a subset 40,000 images sampled from the train dataset to keep the computation within reasonable limits.</p><p>For experiments on linear regression, the Hessian is independent of the model (it only depends on the data) and can be computed using any model checkpoint. For all other experiments, the Hessian eigenvectors were computed using the model at its initial pretrained state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Figure 1</head><p>We We kept track of the norms of the local and global displacement, ∥δ k ∥ 2 2 and ∥∆ k ∥ 2 2 , every 250 steps in the training process, to keep the length of the trajectories within reasonable limits. ∥δ k ∥ 2 2 is visualized directly, along with its 15 step moving average.</p><p>We then fit a power law of the form αk c to the ∥∆ k ∥ 2 2 trajectories for each model, using the last 2/3 of the saved trajectories. We visualize the ∥∆ k ∥ 2 2 trajectories along with their fits on a log-log plot. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.4 Figure 3</head><p>For this figure we constructed an arbitrary Ornstein-Uhlenbeck process with anisotropic noise which would help contrast the original and modified potentials. We sampled from a 2 dimensional OU process of the form</p><p>where we set b = [-0.1, 0.05] ⊺ and arbitrarily construct A such that it's eigenvectors are aligned with q 1 = [-1, 1] ⊺ and q 2 = [1, 1] ⊺ and it's eigenvalues are 4 and 1 as follows:</p><p>The background for the left panel was computed from the convex quadratic potential   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.7 Increasing rate of anomalous diffusion</head><p>Upon further experimentation with the fitting procedure for the rate of anomalous diffusion explained in Figure <ref type="figure">1</ref>, we observed an interesting phenomenon. The fitted exponent c for the power law relationship ∥∆ k ∥ 2 2 ∝ k c increases as a function of the length of the trajectory we fit to. As can be seen in Figure <ref type="figure">9</ref>, c increases at a diminishing rate with the length of the trajectory. This could be indicative of ∥∆ k ∥ 2 2 being governed by a sum of power laws where the leading term becomes dominant for longer trajectories.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The implicit regularization of stochastic gradient flow for least squares</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dobriban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Potential in stochastic differential equations: novel construction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of physics A: mathematical and general</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparing dynamics: Deep neural networks versus glassy systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baity-Jesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Spigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dherin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11162</idno>
		<title level="m">Implicit gradient regularization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124018</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Information Theory and Applications Workshop (ITA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10588</idno>
		<title level="m">Anomalous diffusion dynamics of learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the global convergence of gradient descent for over-parameterized models using optimal transport</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09545</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gradient descent on neural networks typically occurs at the edge of stability</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00065</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Handbook of stochastic methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Gardiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>springer</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8803" to="8812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An investigation into neural net optimization via hessian eigenvalue density</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2232" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gradient descent happens in a tiny subspace</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04754</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<title level="m">Averaging weights leads to wider optima and better generalization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07572</idno>
		<title level="m">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04623</idno>
		<title level="m">Three factors influencing minima in sgd</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05031</idno>
		<title level="m">On the relation between the sharpest directions of dnn loss and the sgd step length</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Free energy and the fokkerplanck equation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="265" to="271" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The variational formulation of the fokker-planck equation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on mathematical analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04285</idno>
		<title level="m">Analysis of momentum methods</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sagastuy-Brena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04728</idno>
		<title level="m">Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structure of stochastic dynamics near fixed points</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Thouless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="13029" to="13033" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00165</idno>
		<title level="m">Deep neural networks as gaussian processes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06720</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic modified equations and adaptive stochastic gradient algorithms</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2101" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14544" to="14555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12470</idno>
		<title level="m">On the validity of modeling sgd with stochastic differential equations (sdes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Noise and fluctuation of finite learning rate stochastic gradient descent</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ziyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ueda</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7045" to="7056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00325</idno>
		<title level="m">Aggregated momentum: Stability through passive damping</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02476</idno>
		<title level="m">A simple baseline for bayesian uncertainty in deep learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A variational analysis of stochastic gradient algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Just a momentum: Analytical study of momentum-based acceleration methods in paradigmatic high-dimensional nonconvex problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mannelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Urbani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11755</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Optimization and generalization of shallow neural networks with quadratic activation functions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mannelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15459</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Mignacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Urbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06098</idno>
		<title level="m">Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mignacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Urbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Priors for infinite networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Learning for Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="29" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07062</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<title level="m">Automatic differentiation in pytorch. Neural Information Processing Systems Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hidary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mhaskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00173</idno>
		<title level="m">Theory of deep learning iii: explaining the nonoverfitting puzzle</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fokker-planck equation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Risken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fokker-Planck Equation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="63" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Rotskoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7146" to="7155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07476</idno>
		<title level="m">Eigenvalues of the hessian in deep learning: Singularity and beyond</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Empirical analysis of the hessian of over-parametrized neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">U</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04454</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01232</idno>
		<title level="m">Deep information propagation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A tail-index analysis of stochastic gradient noise in deep neural networks</title>
		<author>
			<persName><forename type="first">U</forename><surname>Simsekli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gurbuzbalaban</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5827" to="5837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dherin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.12176</idno>
		<title level="m">On the origin of implicit regularization in stochastic gradient descent</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00489</idno>
		<title level="m">Don&apos;t decay the learning rate, increase the batch size</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A mean field view of the landscape of two-layers neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="7665" to="E7671" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the interplay between noise and curvature and its effect on optimization and generalization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3503" to="3513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08419</idno>
		<title level="m">Spherical motion dynamics of deep neural networks with batch normalization and weight decay</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Fluctuation-dissipation relations for stochastic gradient descent</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yaida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00004</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ziyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ueda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05375</idno>
		<title level="m">On minibatch noise: Discretetime sgd, overparametrization, and bayes</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
