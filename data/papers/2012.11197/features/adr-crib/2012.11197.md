The detailed technical explanations and justifications for the researchers' decisions regarding the various aspects of their work on neural joint entropy estimation are as follows:

### 1. Choice of Neural Network Architecture for Entropy Estimation
The researchers opted for deep neural networks (DNNs) due to their ability to model complex, non-linear relationships in data. DNNs can generalize well from limited samples, which is crucial for entropy estimation, especially when the sample size is small relative to the alphabet size. The architecture is designed to minimize the cross-entropy loss, which serves as an upper bound for entropy, allowing the model to learn a representation of the underlying probability distribution effectively.

### 2. Selection of Loss Function (Cross-Entropy) for Training
Cross-entropy is chosen as the loss function because it quantifies the difference between two probability distributions. By minimizing cross-entropy, the model effectively learns to approximate the true distribution of the data. This approach is less prone to negative bias and high variance, particularly in scenarios with large entropy values, making it a robust choice for entropy estimation.

### 3. Decision to Extend McAllester and Statos (2020) Methodology
The extension of McAllester and Statos' methodology is motivated by the need to address the limitations of existing entropy estimation techniques, particularly in high-dimensional spaces. By building on their work, the researchers aim to enhance the accuracy and consistency of entropy estimators, leveraging the strengths of neural networks to improve performance in challenging scenarios.

### 4. Assumptions About Underlying Probability Distributions
The researchers assume that the underlying probability distributions are sufficiently complex to warrant the use of neural networks for estimation. They acknowledge that while the model's performance is contingent on these assumptions, the flexibility of DNNs allows for the approximation of a wide range of distributions, making them suitable for various applications.

### 5. Method for Bias Correction in Entropy Estimation
To address the negative bias inherent in traditional plug-in estimators, the researchers implement a bias correction mechanism based on empirical distribution adjustments. This correction is crucial for improving the accuracy of entropy estimates, particularly in cases where the sample size is small compared to the alphabet size.

### 6. Approach for Estimating Conditional Entropy
The conditional entropy is estimated using a similar framework as the joint entropy, where the model is trained to predict the distribution of one variable given another. This approach leverages the chain rule of entropy, allowing for a systematic estimation of conditional relationships between variables.

### 7. Strategy for Mutual Information Estimation
Mutual information is estimated by calculating the difference between the estimated marginal and conditional entropies. This method is grounded in the relationship between entropy and mutual information, allowing the researchers to derive a consistent estimator that captures the dependencies between variables effectively.

### 8. Use of Autoregressive Models for Transfer Entropy Estimation
The researchers employ autoregressive models, such as recurrent neural networks (RNNs), for transfer entropy estimation due to their ability to capture temporal dependencies in time series data. This choice is particularly relevant for applications in fields like finance and neuroscience, where understanding the flow of information over time is critical.

### 9. Evaluation Metrics for Performance Comparison
The researchers utilize a range of evaluation metrics, including bias, variance, and consistency, to assess the performance of their estimators. These metrics provide a comprehensive view of the estimator's reliability and accuracy, allowing for meaningful comparisons with existing methods.

### 10. Dataset Selection for Empirical Studies
Datasets are selected based on their relevance to the specific information-theoretic measures being estimated. The researchers prioritize datasets that exhibit complex dependencies and large alphabet sizes, ensuring that their methods are rigorously tested in challenging scenarios.

### 11. Handling of Large Alphabet Sizes in Estimators
To manage large alphabet sizes, the researchers implement techniques such as dimensionality reduction and efficient sampling methods. These strategies help mitigate the curse of dimensionality, allowing for more accurate entropy estimation even when the number of possible outcomes is substantial.

### 12. Techniques for Reducing Variance in Estimations
Variance reduction techniques, such as bootstrapping and regularization, are employed to enhance the stability of the estimators. These methods help to ensure that the estimations are robust and less sensitive to fluctuations in the data, leading to more reliable results.

### 13. Implementation of Strong Consistency in Estimators
The researchers demonstrate strong consistency in their estimators by proving that as the sample size increases, the estimators converge to the true entropy values. This theoretical foundation is crucial for establishing the reliability of the proposed methods.

### 14. Framework for Independence Testing Using CMI
Conditional mutual information (CMI) is utilized as a framework for independence testing, allowing the researchers to assess whether two variables are conditionally independent given a third variable. This approach is grounded in information theory and provides a rigorous method for testing independence in complex datasets.

### 15. Integration of Domain Knowledge in Financial Dataset Analysis
The researchers incorporate domain knowledge into their analysis of financial datasets