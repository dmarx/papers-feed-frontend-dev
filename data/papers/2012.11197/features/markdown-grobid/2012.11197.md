# Neural Joint Entropy Estimation

## Abstract

## 

Estimating the entropy of a discrete random variable is a fundamental problem in information theory and related fields. This problem has many applications in various domains, including machine learning, statistics and data compression. Over the years, a variety of estimation schemes have been suggested. However, despite significant progress, most methods still struggle when the sample is small, compared to the variable's alphabet size. In this work, we introduce a practical solution to this problem, which extends the work of McAllester and Statos  (2020). The proposed scheme uses the generalization abilities of cross-entropy estimation in deep neural networks (DNNs) to introduce improved entropy estimation accuracy. Furthermore, we introduce a family of estimators for related informationtheoretic measures, such as conditional entropy and mutual information. We show that these estimators are strongly consistent and demonstrate their performance in a variety of use-cases. First, we consider large alphabet entropy estimation. Then, we extend the scope to mutual information estimation. Next, we apply the proposed scheme to conditional mutual information estimation, as we focus on independence testing tasks. Finally, we study a transfer entropy estimation problem. The proposed estimators demonstrate improved performance compared to existing methods in all tested setups.

## I. INTRODUCTION

E NTROPY is one of the basic building blocks of infor- mation theory [[1]](#b0). It quantifies the minimum average number of bits required to represent an event that follows a given probability distribution rule. Many important informationtheoretic measures such as mutual information (MI) and conditional MI (CMI) include marginal, conditional and joint entropies. These measures have many applications in machine learning, such as feature selection [[2]](#b1), [[3]](#b2), representation learning [[4]](#b3), [[5]](#b4) and analyses of the learning mechanism [[6]](#b5), [[7]](#b6).

One of the first and basic entropy estimation methods is the classic plug-in scheme. In this scheme, an empirical distribution replaces the true (unknown) probability rule, and the corresponding empirical entropy is the estimated entropy. Unfortunately, this estimation scheme suffers from a negative bias [[8]](#b7), [[9]](#b8), leading to limited outcomes. A variety of parametric and nonparametric methods have been proposed to improve the entropy estimation, such as in [[8]](#b7)- [[10]](#b9). Recently, a neural network-based method was proposed to estimate entropy by minimizing the cross-entropy (CE) loss [[11]](#b10) as an upper bound of the entropy. The CE measures the average number of bits required to represent an event that is generated from a probability distribution P by a different probability distribution Q. CE has its minimum when P = Q. Thus, minimizing CE Y. Shalev, A. Painsky and I. Ben-Gal are with the Department of Industrial Engineering, Tel-Aviv University, Ramat-Aviv 6997801, Israel. Correspondence to:<yuvalshalev@mail.tau.ac.il>.

implies searching for a Q that is as similar as possible in a log-loss [[12]](#b11), [[13]](#b12) sense to P . This approach has several advantages. First, it uses the generalization power of neural networks and their universality [[14]](#b13)- [[16]](#b15). Second, CE is less prone to negative bias and high variance in large entropy values [[11]](#b10). However, this approach has certain limitations. First, it requires prior assumptions on the true underlying distribution, as discussed in Section III. Second, the statistical properties of this CE estimator are currently unexplored. Therefore, the existence of a neural network-based estimator that can provide an accurate estimation of entropy, is not guaranteed.

These challenges in entropy estimation are also related to other information-theoretic measures. For example, one of the most common MI estimation schemes is the K-nearest neighbour (KNN) estimator [[17]](#b16). This estimator was shown to introduce a significant negative bias in setups with high dependencies between the variables, resulting in large MI values [[18]](#b17). Neural-network-based approaches have been recently proposed to overcome this problem using variational bound optimization [[18]](#b17)- [[20]](#b19). Although a significant improvement in the MI estimation has been achieved, the results are not yet satisfying and suffer from theoretical limitations that are primarily manifested in large MI values [[11]](#b10), [[19]](#b18). There is also a large body of work on fundamental estimation bounds for different information-theoretic measures (see [[9]](#b8), [[21]](#b20) and related work).

In this paper, we address the inherent estimation challenges discussed above. The proposed estimation scheme focuses on joint entropy estimation. This problem is similar to the standard entropy estimation problem as any univariate random vector may be represented, for example, as a binary multivariate vector. In particular, we combine the chain rule with the CE loss minimization procedure using neural networks to obtain a more accurate joint entropy estimation. We denote this estimation procedure as the Neural Joint Entropy Estimator (NJEE). We study the properties of NJEE and show that it is strongly consistent. In a similar manner, we obtain the conditional NJEE (C-NJEE), as an estimator for the joint conditional entropy between two or more multivariate variables.

Having these two estimators, it is straightforward to estimate the MI between two random variables. Adding a second conditioning variable results in the CMI estimator. Additionally, we apply the proposed scheme to transfer entropy estimation (TE). Given two time series, the TE is defined as the CMI between the "past" of the first series and the "future" of the second series given its "past". TE is used to explore the information flow and causality among time-dependent data in neuroscience [[22]](#b21), [[23]](#b22), finance [[24]](#b23), [[25]](#b24), process control [[26]](#b25), [[27]](#b26) and many other applications. We show that by using an autoregressive neural network model, such as a recurrent neural network, C-NJEE can be used for efficient TE estimation.

The advantages of the estimators proposed in this paper are demonstrated in various use-cases. First, we study the entropy estimation of a discrete random variable with a large alphabet size. Applying NJEE to this problem, we outperform existing methods when the sample size is much smaller than the alphabet size. Further, we focus on MI estimation between two multivariate variables. A commonly used toy problem is used for this task. The performance of the proposed MI estimator demonstrates improved results in terms of lower bias and variance, compared to existing methods. This result is specifically manifested in larger values of MI. Next, we demonstrate the performance of the suggested CMI estimator, as we focus on conditional independence tests. We study a real protein dataset where dependencies among the variables (protein elements) are known. Also, the proposed estimation scheme demonstrates better results than existing methods. Finally, the CMI estimator is applied to a TE estimation task. Specifically, we study a real financial dataset of stock index prices and show that the C-NJEE-based estimation provides additional insights on the information flow between the time series that are not discovered by the other methods. These insights are in line with domain knowledge and the world financial timeline.

To summarize, the contributions of this paper are threefold. First, we extend the work of [[11]](#b10) and introduce strongly consistent estimators for joint entropy and conditional joint entropy. The proposed estimators, NJEE and C-NJEE, are based on minimization of the CE loss while applying the entropy chain rule property. Second, we use these estimators to obtain estimators for related measures such as MI, CMI and TE. Third, we propose a practical implementation scheme of these estimators that demonstrates better performance than existing methods on various tasks and datasets.

The remainder of this paper is organized as follows. Related works on entropy, MI, CMI and TE estimation are discussed in Section II. In Section III, definitions and related mathematical overview are given to support the scheme and ideas proposed in this paper. The primary results are shown in Section IV. An empirical study of various tasks and comparisons with different benchmark methods are provided in Section V. We conclude this paper in Section VI.

## II. RELATED WORK

Estimating information-theoretic measures is a well-studied problem. We refer the reader to [[8]](#b7), [[9]](#b8), [[17]](#b16), [[19]](#b18), [[28]](#b27), [[29]](#b29) for a comprehensive review of these measures. The following literature review focuses on estimators that are relevant for this work.

## A. Entropy Estimation in Large Alphabet

As mentioned in Section I, the simplest method to estimate the entropy of a discrete random variable is the so plug-in estimator [[1]](#b0). The Miller-Madow estimator [[30]](#b28) adds a bias correction to the plug-in estimator. This correction depends on the ratio between the number of symbols from the alphabet that appear at least once in the sample and the sample size.

More recently, [[10]](#b9) proposed an estimator for the entropy of species in a community (in this biological context, the entropy is called the diversity index), where the number of species (alphabet size) is large and unknown. This estimator is based on the Horvitz-Thompson estimator for population size and the Good-Turing estimator for the probability of unseen events. In [[9]](#b8), an entropy estimator is obtained using a polynomial approximation for the terms in the entropy sum that involve small probabilities with respect to log k, where k is the alphabet size. For larger probabilities, an unbiased plug-in estimator is used that is similar to the Miller-Madow estimator. Thus, improved results are demonstrated on simulated data of discrete random variables with large alphabet sizes where many symbols have relatively low probability.

## B. MI and CMI Estimation

One of the most popular MI estimators in recent years is the KNN-based KSG estimator [[17]](#b16), which uses KNN-based density estimation over a shared space of the marginal and conditional entropy. Using the connection between the MI and entropy (see III-B), the entropies' bias terms are subtracted to provide a more accurate MI estimation. This metric suffers from the curse of dimensionality and underestimates the MI when the interaction between the variables is strong [[31]](#b30).

The recent advances in deep learning motivated various researchers to address the dimensionality problem by estimating the MI with neural networks. This is usually obtained by finding variational lower bound for the MI (typically, a differentiable function that its supremum is the MI). These functions are approximated by neural networks to maximize the lower bound [[18]](#b17)- [[20]](#b19). These methods yield improved results compared to the KNN-based estimator, but are quite limited when estimating large MI values, since their estimation complexity increases exponentially with the number of samples [[11]](#b10), [[19]](#b18). To overcome this problem, [[11]](#b10) proposed using the CE as an upper bound for the entropy and minimize it by training a neural network. Thus, an MI estimate is obtained by subtracting the estimated conditional entropy from the estimated marginal entropy. This approach underlines the proposed estimation scheme as discussed in further detail in subsection III-C. A similar approach for MI estimation using the softmax function (e.g., as the output layer in a neural network), was suggested in [[32]](#b31). However, this scheme is limited to the case where the input variable is multivariate, while the target variable is univariate. Classifier based conditional mutual information (CCMI) was proposed in [[33]](#b32). A two-sample classifier was used to distinguish between samples from the joint distribution and samples from the marginal distribution. Combining conditional generative models (e.g., Conditional Generative Adversarial Networks (CGAN) or Conditional Variational Autoencoders (CVAE)), an estimator for the CMI was developed. This approach introduced a significant improvement over other recently proposed methods.

## C. TE Estimation

The TE is defined as a form of CMI between time series. Specifically, T E(Y f uture ; X past ) =

CM I(Y f uture , X past |Y past ) (see a formal definition of TE in Subsection III-B). There are two primary approaches for TE estimation. The first approach considers every variable in every timestamp as a separate variable, and uses any MI or CMI estimator to estimate the TE [[34]](#b33)- [[36]](#b35). The second approach applies a sequential model that considers the time dependencies among different time lags to extract an estimator for the TE and its related measures [[37]](#b36), [[38]](#b37).

As a representative of the first approach, a recently proposed estimator [[36]](#b35) applies a neural network two-sample classifier to estimate the TE. Using the second approach, the Context Tree Weighting (CTW) algorithm [[39]](#b38) is utilized in [[37]](#b36) for directed information estimation (a closely related measure to TE [[40]](#b39)). Both works investigate a financial time series of index prices to evaluate their estimators. we use the same dataset to evaluate the proposed method.

## III. BACKGROUND A. Notations

The following notations are used throughout this paper. A univariate discrete random variable is denoted by an upper-case letter (e.g., X), that obtains values x from the alphabet A x = {1, . . . , a x }. A multivariate variable with dimensions d x is denoted by an underline, (e.g., X), where its values are denoted by underlined lower-case letter x. The m th component of X is denoted as X m , which obtains values x m from the alphabet

$A xm = {1 . . . a xm } which can be different for different values of m. The vector of the first k components of X is denoted by X k .$We denote H n (X) as the estimator of X's entropy given a sample S = {. . .} n i=1 , where it is implied from the text that S is a collection of n samples of X. This notation holds for other estimators as well. For example, I n (X; Y |Z) is an estimator of the CMI between X and Y given Z, from a collection of n samples from the joint distribution of X, Y and Z. To avoid an overload of notation, we denote x i as the i th sample in S, while X m is the m th component of the random vector X.

For the time notation, a multivariate variable in time t is represented by a bracket index, e.g., X (t) and a matrix that represents its past l time lags is represented by

$X (l) (t) = [X (t-l) . . . X (t) ].$
## B. Definitions

Let X be a discrete random variable that follows a probability distribution P (X). Shannon's entropy is defined as:

$H(X) = -E P (X) [log P (x)] .(1)$The entropy (1) can be represented by the chain-rule

$H(X) = H(X 1 , X 2 , . . . , X dx ) = dx m=1 H(X m |X m-1 , . . . , X 1 ),(2)$where H(X 1 |X 0 ) abbreviates H(X 1 ).

The CE between any two distribution functions P (X) and Q(X) is defined as:

$CE(Q(X)) = -E P (X) [log Q(X)] ,(3)$where the expectation is over the distribution of X, namely, P (X).

The following inequality holds for every pair of distributions P (X) and Q(X):

$CE(Q(X)) ≥ H(X),(4)$Where an equality is obtained for Q(X) = P (X).

A related measure to CE is the Kullback-Leibler divergence (D KL ) between P (X) and Q(X)

$D KL (P (X)||Q(X)) = E P (X) log P (X) Q(X) .(5)$The D KL is a nonnegative measure and equals zero iff

$P (X) = Q(X).$The MI, denoted as I(X; Y ), quantifies in bits the entropy reduction in X given the knowledge obtained from another random variable Y , i.e.,

$I(X; Y ) = H(X) -H(X|Y ).(6)$Another important measure that is represented by the difference of entropies is conditional mutual information (CMI)

$I(X; Y |Z) = H(Y |Z) -H(Y |X, Z).(7)$CMI is also used to evaluate the TE, which is defined as

$T E X→Y = I(X (k) (t) ; Y (t+1) |Y (l) (t) ).(8)$Assuming discrete time, the T E X→Y is the CMI between the past k time lags of X and Y at time t + 1 given the past l time lags of Y .

## C. CE-Based Entropy

Let P (X) be the distribution function of X. Let T θ (X) be a neural network that approximates it. In [[11]](#b10), the following upper bound for the entropy of X was proposed

$H Θ (X) = inf θ∈Θ CE(T θ (X)),(9)$and

$H Θ (X) = H(X) iff P (X) = T θ (X).$Given a sample S of size n, the sample mean is used to estimate the CE,

$CE n (T θ (X)) = - 1 n n i=1 log T θ (x i ).(10)$Then, an estimator of the entropy is:

$H n (X) = inf θ∈Θ CE n (T θ (X)).(11)$In [[11]](#b10), the authors suggest an entropy estimator based on the above. However, they require prior knowledge of P (X) for the training of their neural network.

## IV. MEASURING THE JOINT ENTROPY WITH NEURAL

## NETWORKS

In this section we discuss the primary concepts of this paper. First, the neural network classifier and its respective CE are formally defined. Then, the neural joint entropy estimator is introduced. Next, we define a strongly consistent estimator and show that the proposed joint entropy estimator satisfies this property. We also provide an algorithmic implementation of the proposed estimator and discuss practical aspects of its implementation. Next, estimator for the joint conditional entropy is provided with the corresponding algorithmic implementation. Using the estimators of the joint entropy and the conditional joint entropy, estimators for MI, CMI and TE are obtained.

## A. Neural Network Classifier and Classification CE

The following basic definitions are used throughout this section.

$Definition IV.1. (Neural network classifier). Let G θ (Y |X)$be a neural network model with a random variable input X and parameters θ in a compact domain Θ ∈ R k . The outputs of G θ (Y |X) are defined over the probability simplex:

${G θ (y|x) ∈ R ay : ay y=1 G θ (y|x) = 1, G θ (y|x) ≥ 0}.$Next, we define the CE of this classifier.

Definition IV.2. (Classifier CE). Let G θ (y|x) be a neural network classifier. The CE of this classifier is defined as

$CE(G θ (Y |X)) = -E P (X,Y ) log G θ (y|x),(12)$where Y ∈ A y = {1, . . . , a y }, a y ≥ 2.

We assume that -log(G θ (y|x)) ≤ η for all x ∈ X and for all θ ∈ R k , for any value of Y . Practically, this assumption is used in many model training procedures to avoid an unbounded loss [[13]](#b12). The empirical estimator of this CE is given by [[41]](#b40), namely

$CE n (G θ (Y |X)) = - 1 n n i=1 log(G θ (y i |x i )).(13)$
## B. Neural Joint Entropy Estimation

Using (2) and Definitions IV.1 and IV.2, we define the estimator of the joint entropy.

## Definition IV.3. (Neural Joint Entropy Estimator (NJEE)).

Let H n (X 1 ) be an estimated marginal entropy of the first components in X and let G θm (X m |X m-1 ) be a neural network classifier. Then, NJEE is defined as

$H n (X) = H n (X 1 ) + dx m=2 CE n (G θm (X m |X m-1 ). (14)$In words, the joint entropy estimator consists of a marginal estimator for the first component, followed by estimators for the conditional entropies H(X m |X m-1 ), for m = 2, . . . , d x .

Definition IV.4. (Strong consistency (following [[18]](#b17))). The estimator H n (X) is strongly consistent if for all , δ > 0 and a constant C > 0, there exists a positive integer N and a choice of a neural network such that:

$∀n ≥ N, |H(X) -H n (X)| ≤ C • + δ, a.e.$Theorem 1. NJEE is strongly consistent.

## C. Proof of Strong Consistency Property

In this section we follow the scheme shown in [[18]](#b17) to prove Theorem 1. This proof includes the following main steps:

1) Connecting the true CE of a classifier-based neural network and the conditional entropy H(Y |X) (Lemmas 1 and 2). 2) Showing the convergence of the empirical CE to the true CE (Lemma 4). 3) Showing that the empirical CE can approximate with high accuracy the conditional entropy (Lemma 5). 4) Applying the chain-rule property and the previous steps to show that the proposed estimator of the joint entropy is strongly consistent. We begin with the first step. Formally, since neural networks are universal approximation functions [[14]](#b13)- [[16]](#b15), the following holds:

Lemma 1. For any > 0, and any conditional distribution function P (Y |X), there exists a neural network G θ (Y |X) such that:

$D KL (P (Y |X)||G θ (Y |X)) ≤ 2 , a.e.(15)$That is, it is possible to find a neural network that can approximates P (Y |X) in any desired approximation level.

The next Lemma states that the CE can be used to estimate the conditional entropy.

Lemma 2. Let P (Y |X) be a conditional distribution and let H(Y |X) be the entropy associated with this distribution. Then, for any > 0, there exists a neural network G θ (Y |X) such that

$|CE (G θ (Y |X)) -H(Y |X)| ≤ 2 , a.e. (16$$)$The proof of this lemma follows the ideas shown in [[11]](#b10)).

$H(Y |X) = E P (X,Y ) log 1 P (y|x) = E P (X,Y ) log 1 G θ (y|x) G θ (y|x) P (y|x) = E P (X,Y ) log 1 G θ (y|x) -D KL (P (y|x)||G θ (y|x)) ≥ CE(G θ (Y |X)) - 2 ,(17)$where the last line follows Lemma 1. As shown in (4), we have that

$CE(G θ (Y |X)) -H(Y |X) ≥ 0,(18)$therefore

$|CE (G θ (Y |X)) -H(Y |X)| ≤ 2 .$The empirical estimator for this classifier CE is obtained from [(13)](#b12). The conditions for the convergence of this estimator are defined by the uniform law of large numbers. Lemma 3. The uniform law of large numbers [[42]](#b41). Let Θ be a compact set of parameters. Let f θ (x i ) be a continuous function at each θ ∈ Θ and x i ∈ X . Assume there exists an upper bound η(X) such that f (x) ≤ η(x) for all θ ∈ Θ and E[η(X)] < ∞. Then, E[f θ (X)] is continuous and

$sup θ∈Θ 1 n n i=1 f θ (x i ) -E[f θ (X)] p → 0. (19$$)$Using Lemma 3, the convergence of the classifier CE is obtained Lemma 4. For any > 0 and ∀θ ∈ Θ, there exists a positive integer n ≥ N such that:

$P (| CE n (G θ (Y |X)) -CE(G θ (Y |X))| ≤ 2 ) = 1. (20$$)$The proof of this Lemma is an immediate application of ( [13](#formula_18)) with

$f θ ((x i , y i )) = -log(G θ (y i |x i )).(21)$since -log(G θ (y i |x i )) ≤ η, then f θ ((x i , y i )) ≤ η and Lemma 3 holds.

Lemma 5. The estimator CE n (G θ (Y |X)) is strongly consistent. That is, for all > 0, there exists a positive integer n ≥ N and a choice of neural network such that:

$|H(Y |X) -CE n (G θ (Y |X))| ≤ , a.e. (22$$)$This lemma is obtained using the triangular inequality with Lemmas 2 and 4:

$|H(Y |X) -CE n (G θ (Y |X))| ≤ |CE (G θ (Y |X)) -H(Y |X)|+ | CE(G θ (Y |X)) -CE(G θ (Y |X))| ≤ . (23) Restating (2), H(X) = H(X 1 ) + dx m=2 H(X m |X m-1 ).(24)$Suppose there exists d x -1 neural networks that approximate each term in the sum with an accuracy. Then, the total error of the sum expression is • (d x -1). The marginal entropy H(X 1 ) is estimated with an estimator H n (X 1 ) that guarantees an error that is not larger than certain δ > 0. Several estimators can provide such a guarantee, e.g., [[8]](#b7), [[9]](#b8). In this case:

$|H(X) -H n (X)| = |H(X 1 ) -H n (X 1 )+ dx m=2 H(X m |X m-1 ) - dx m=2 CE n (G θm (X m |X m-1 )| ≤ |H(X 1 ) -H n (X 1 )|+ | dx m=2 H(X m |X m-1 ) - dx m=2 CE n (G θm (X m |X m-1 )| ≤ δ + C • ,(25)$where

$C = d x -1.$
## D. Algorithmic Implementation of NJEE

The implementation of the NJEE estimator is described in Algorithm 1. 

$Algorithm 1 NJEE 1: input: Sample S = {x i } n i=1 from P (X) 2: h m ← 0, for m = {1, . . . , d x } 3: h 1 ← H n (X 1 ) 4: Initialize {θ m } dx$$h m ← Minimize CE n (G θm (X m |X m-1 )) 7: end for 8: H n (X) ← h 1 + dx m=2 h m 9: return: H n (X)$Practically, Algorithm 1 can be implemented in parallel per each value of m. Another approach is to use a recurrent neural network (RNN) that replaces the d x -1 networks. In this case, the sequential input to the RNN is the components vector of X (e.g., see distribution estimation with RNN in [[43]](#b42)). Then, the estimated entropy would be the sum of all the CE losses in every time step. The empirical results of this implementation demonstrate similar performance to Algorithm 1.

We also note that by using the CE loss, it is possible to replace the neural network model with any other classifier to estimate the entropy. However, in this case, Lemma 1 may not apply, and strong consistency is not guaranteed.

## E. Conditional-Neural Joint Entropy Estimation

The conditional entropy of two multivariate random variables X and Y is

$H(X|Y ) = dx m=1 H(X m |Y , X m-1 ). (26$$)$To estimate [(26)](#b25), a slight change is made to NJEE, where all components in the proposed estimator are neural networks.

Definition IV.5. (Conditional Neural Joint Entropy Estimator (C-NJEE)). Let G θm (X m |Y , X m-1 ) be a neural network classifier with inputs Y and X m-1 . Then C-NJEE is defined as,

$H n (X|Y ) = dx m=1 CE n (G θm (X m |Y , X m-1 )). (27$$)$Corollary 1.1. C-NJEE is strongly consistent.

$|H(X|Y ) - dx m=1 CE n (G θm (X m |Y , X m-1 ))| ≤ d x • , a.e. (28$$)$The proof of Corollary 1.1 is straightforward. Notice that every conditional entropy in the sum expression of ( [26](#formula_39)) can be estimated by a classifier CE with estimation error. Since there are d x conditional entropies estimators, the total estimation error of H(X|Y

$) is d x • . The implementation of C-NJEE is described in Algorithm 2. Algorithm 2 C-NJEE 1: input: Sample S = {x i , y i } n i=1 from P (X, Y ) 2: h m ← 0, for m = {1, . . . , d x } 3: Initialize {θ m } dx m=1 4: for m in 1 to d x do 5: h m ← Minimize CE n (G θm (X m |Y , X m-1 )) 6: end for 7: H n (X|Y ) ← dx m=1 h m 8: return: H n (X|Y )$We now apply NJEE and C-NJEE to introduce an estimator for the MI.

$I n (X; Y ) = H n (X 1 ) + dx m=2 CE n (G θm (X m |X m-1 ) - dx m=1 CE n (G θm (X m |Y , X m-1 )),(29)$Similarly, given a variable Z, an estimator for the CMI ( [7](#formula_9)) can be obtained

$I n (X; Y |Z) = dx m=1 CE n (G(X m |Z, X m-1 )) - dx m=1 CE n (G(X m |Z, Y , X m-1 )).(30)$Again, since all models are trained independently, the worst case error of these estimators is the sum of the errors of NJEE and C-NJEE, thus these estimators are also strongly consistent.

## V. EXPERIMENTS

In this section we demonstrate the performance of the proposed estimators in various estimation tasks.

To apply these estimators, we train a set of neural networks. Unless stated otherwise, the following basic network structure is considered throughout these experiments: An input layer, two fully connected layers with 50 nodes, a ReLU activation function and an output softmax layer. The loss is optimized with the ADAM [[44]](#b43) optimizer with the following parameters (lr = 0.001, β 1 = 0.9, β 2 = 0.999).

## A. Entropy Estimation with Large Alphabet

We begin this experimental section with large alphabet entropy estimation using NJEE. Prior to applying NJEE, we change the univariate representation values of the alphabet to their binary representation. Any other small alphabet representation, such as ternary, is also valid. The evaluation is preformed on six simulated studies, most of which were used in previous works (e.g., [[9]](#b8)):

• Uniform distribution. • Zipf's law distribution with parameters α = 1, 2.

• Geometric distribution with p = 1/10 5 .

• Symmetric mixture of a Zipf's law distribution (α = 1)

and Geometric distribution (p = 2/10 5 ).

• Discrete Laplace (DL), where DL(X, σ) ∝[foot_0](#foot_0) 2σ e -X σ and σ = 10 -4 . The alphabet size of X is set to 10 5 (excluding the last experiment where the alphabet is not limited). Every simulated study (defined by a distribution type and a sample size) is repeated 100 times.

Figure [1](#fig_1) demonstrates the root mean squared (RMSE) of the entropy estimation as a function of the sample size for NJEE and other entropy estimators described in Section II-A 1 . As shown, NJEE demonstrates the lowest RMSE in most cases. Specifically, NJEE demonstrates the lowest error in all the experiments where n ≤ 1000.

## B. Multivariate MI Estimation

In the following set of experiments we apply the proposed scheme to a simple and commonly used multivariate MI estimation problems, as used in [[18]](#b17), [[19]](#b18), [[33]](#b32). The setup is defined as follows. Let X and Y be two random vectors in

$R d such that X Y T ∼ N (0, Σ XY ) Σ XY = I d ρI d ρI d I d .(31)$Notice that the correlation between the pairs (X i , Y j ) is ρ when i = j and zero otherwise. Further, Cov(X) = Cov(Y ) = I d , and the MI between X and Y is thus simply:

$I(X; Y ) = - d 2 • log(1 -ρ 2 ).$In this study, samples are generated from the model above, using different values of ρ (or equivalently, different values of MI). Since the proposed algorithm is designed for discrete variables, we quantize the samples using a simple binning scheme. Binning continuous data for MI estimation has been extensively studied over the years. The interested reader is referred to [[11]](#b10), [[25]](#b24), [[35]](#b34), [[37]](#b36), [[46]](#b45), [[47]](#b46) for a thorough discussion.

In Figure [2](#fig_2), the NJEE-based algorithms are compared to the KNN MI estimation method [[17]](#b16). With low absolute values of ρ, the two methods yield accurate results. As ρ increases (and thus the MI increases), the KNN estimator significantly deviates from the true value, as demonstrated in [[18]](#b17). NJEE yields better results for greater MI, similar to [[11]](#b10), yet without a prior assumption on the characteristics of the underlined distribution.

Let us now turn to an additional synthetic experiment, following [[19]](#b18). Again, we draw samples from the model described in [(31)](#b30). In this experiment, we begin with ρ = 0 and draw a total of 4000 batches with 64 samples in each batch. Then, we estimate the MI from the drawn samples. We increase ρ and repeat the previous step. We terminate at ρ = 1. I n (X; Y ) is compared to the recently proposed variational methods [2](#foot_1) . As demonstrated in Figure [3](#fig_3), the results achieved  by the proposed estimator exhibits lower bias and variance with respect to the variational benchmark methods. The upper rows of Table I demonstrate the best estimation results for each method obtained by hyperparameter grid search. The proposed NJEE scheme yields better results for most MI values ranging from 2 to 20. The reasons for the bias and variance errors in the variational bound methods are discussed in [[19]](#b18).

Let us now study estimator sensitivity to invertible transformation, in which we do not expect any change in the MI under such transformations. The cubic transformation y ⇒ z = (W y) 3 is chosen for this experiment, where W is an invertible d × d matrix with the entries w ij ∼ N (0, 1). The lower rows of Table I summarize the results. As shown, the proposed MI estimator yields identical results to the original problem, while the alternative methods yield lower estimates.

Due to stability issues in the benchmark methods, we could not obtain estimates for the cubic transformation when the underlying MI equals 20.0 nats. Table [I](#): Best results of every estimator following a hyperparameter grid search for the Gaussian setup (31) (upper rows) and its cubic transformation (lower rows). The true MI values are shown in the first row. The results of the benchmark methods for 2 to 10 nats are also reported in [[19]](#b18).

TRUE MUTUAL INFORMATION 2.0 4.0 6.0 8.0 10.0 20.0 GAUSSIAN SETUP NJEE 2.2 4.1 5.9 7.8 9.6 17.8 α 1.9 3.8 5.7 7.4 8.8 11.7 JS 1.2 3.0 4.8 6.5 8.1 15.5 NWJ 1.6 3.5 5.2 6.7 8 10.8 InfoNCE 1.9 3.6 4.9 5.7 6 6.2 CUBIC SETUP NJEE 2.2 4.1 5.9 7.8 9.6 17.8 α 1.7 3.6 5.4 6.9 8.2 -JS 1 2.8 4.5 6.1 7.6 -NWJ 1.5 3.2 4.7 5.9 6.9 -InfoNCE 1.7 3.2 4.1 4.6 4.8 -

## C. Conditional Independence Testing

We now investigate the proposed method in conditional independent testing (CIT). CIT is a basic task in statistics with applications to a variety of domains, such as Bayesian networks and causality analysis [[49]](#b48)- [[51]](#b50). In this experiment, we use a flow-cytometry dataset [[52]](#b51). This dataset describes the connections between eleven proteins in different experimental setups. Sachs et al., [[52]](#b51) introduced a consensus Bayesian network (see Figure [3](#fig_3) in their work) that is considered the ground truth of the connections mapping among the proteins. The flow-cytometry dataset was extensively studied in several works. In [[33]](#b32), the authors introduced a CIT method that incorporates a two-sampled classifier and generative models. In [[50]](#b49), a KNN bootstrap and binary classifier procedure was proposed to perform the CIT.

Before we describe the results of the experiment, we provide some preliminaries on Bayesian networks that are used for this experiment. In a Bayesian network, features are represented by nodes, and their dependencies are represented by edges [[53]](#b52). Node A is a parent of node B if there is a directed edge from A to B, and B is considered a child of A. Y is conditionally independent of X when Z is a subset of the features that holds all available information about Y . These features are the parents of Y , its children and the parents of its children (Markov Blanket [[54]](#b53)). Based on these notations, one can choose multiple combinations of dependent and conditionally independent triplet sets of variables. Following the procedures proposed in [[50]](#b49) and [[33]](#b32), 50 dependent and 50 conditionally independent triplets (X, Y, Z) are randomly chosen and their CMI is estimated using I n (X; Y |Z). For every triplet we have the ground truth (dependent/independent), and its corresponding estimate În (X; Y |Z). Since the estimates În (X; Y |Z) are continuous (nonnegative) numbers, we may set a decision threshold. Specifically, we say that a triplet is conditionally independent if its În (X; Y |Z) value is lower than a decision threshold (and vice versa). Thus, one could construct an ROC curve where every point in the curve represents a value of the threshold , the value of the false positive rate (the horizontal axis) and the true positive rate (the vertical axis). Figure [4](#fig_4) illustrates the ROC curve and the area under the curve (AUC) values of the independence test performed with I n (X; Y |Z) and with the benchmarks as reported in [[33]](#b32). As 

## D. Estimating TE on Financial Dataset

Finally, we apply C-NJEE to TE estimation. For this experiment, we study a financial dataset that contains the daily closing prices of the Dow-Jones Index (DJI -the stock index of 30 large companies in the U.S. stock exchange) and the Hang Seng Index (HSI -the stock index of 50 large companies in the Hong-Kong stock exchange) between 1990 and 2011. As the DJI index is considered more influential than the HSI on the world's financial markets, we expect the transfer entropy T E DJI→HSI to be significantly greater than T E HSI→DJI . Additionally, we expect to see changes in the TE that are coordinated with related economic events (e.g., significant financial crises).

To estimate the TE, we reproduce the preprocessing used in [[37]](#b36) and [[36]](#b35), and bin the data to three levels of daily price change. A negative change of more than -0.8% is denoted by -1, an absolute change that is below 0.8% is denoted by 0, and a change that is greater than 0.8% is denoted by +1. Then, the C-NJEE algorithm is applied with a recurrent neural network that has the following structure: an input layer, followed by an LSTM cell [[55]](#b54) with 50 nodes, a fully connected layer with 50 nodes with ReLU activation and an output softmax layer. Input data are divided into sequences of length five (i.e., five consecutive trading days). The optimization procedure includes an ADAM optimizer [[44]](#b43), with the following parameters: lr = 0.001, β 1 = 0.9, β 2 = 0.999.

The upper chart of Figure [5](#fig_0) illustrates the 30 day moving average of T E DJI→HSI and T E HSI→DJI , as measured by C-NJEE. As expected, the information flow from DJI to HSI exceeds that of the opposite direction. Compered to the real prices in the lower chart of Figure [5](#fig_0), a relatively sharp increase in T E DJI→HSI is observed in times of financial stress where prices decreasing sharply, such as in the Asian financial crisis [(1997)](#)[(1998)](#), the dot-com crisis [(2000)](#)[(2001)](#)[(2002)](#) and the 2008-2009 financial turmoil [[56]](#b55). This phenomenon is well known in the financial literature (e.g., [[25]](#b24)).

Comparing the results of the proposed method to those reported in [[37]](#b36) and [[36]](#b35), we observe that these methods also found that the information flow from DJI to HSI is much larger then in the opposite direction. However, they did not clearly determine a connection between information values and the world's financial timeline.

## VI. CONCLUSIONS

In this work, we introduce a neural joint entropy estimator (NJEE). The proposed estimator is based on minimizing the CE using neural networks. Expending earlier works, we show that NJEE is strongly consistent and provide a simple algorithmic implementation. We apply the proposed approach to entropy estimation of random variables, specifically those with a large alphabet, using a simple binary transformation. Further, we introduce the conditional neural joint entropy estimator (C-NJEE), which is an estimator for conditional joint entropy. We use NJEE and C-NJEE to estimate both mutual information (MI) and conditional mutual information (CMI).

We demonstrate the performance of the proposed schemes in synthetic and real-world experiments. NJEE achieves a lower RMSE on various simulated setups of random variables with large alphabets and relatively small sample size. Moreover, the proposed MI estimator exhibits lower bias and variance compared to newly-proposed variational lower bounds methods. This result is specifically evident in large MI values. The CMI estimator is further used to execute conditional independence tests. Again, the proposed estimator yields larger AUC value than other existing methods. Finally, we demonstrate the abilities of C-NJEE in estimating the TE. We investigate the dynamics of information flow among financial time series and show their correlation with significant economic events. Certain important characteristics of these dynamics are not captured by other estimation methods that were implemented on the same dataset.

We believe that future research will use the proposed entropy estimators to develop advanced compression schemes for various types of datasets. Additionally, the MI and CMI estimation capabilities can be used to improve the understanding of complex systems and deep learning frameworks.

![for m in 2 to d x do 6:]()

![Figure 1: The log of the RMSE of entropy estimations versus the log of the sample size for NJEE and benchmark methods in different simulated studies. The results are the average of 100 measurements per each sample size and distribution type.]()

![Figure 2: MI estimation of the study in (31) with various values of ρ. I n (X; Y ) is compared to the KNN (k = 3) method [17]. The dimensions of X and Y are 20.]()

![Figure 3: MI estimation with NJEE versus recently proposed variational methods from [19]. Samples from two multivariate random variables in d = 20 are generated according to (31) with an increasing ρ every 4000 batches. The estimated MI in every batch appears in light blue, the moving average of the MI over a rolling window of 200 batches is shown in dark blue and the true MI value is represented by the black line. The variational bounds shown in this figure are further discussed in the literature (see NWJ [48] , InfoNCE [5], Jensen-Shannon lower bound (JS), and the interpolated bound between NWJ and NCE with α = 0.01 and α = 0.99 [19]).]()

![Figure 4: The ROC curve and the AUC values of C-NJEE based estimation, CCIT [50] and CCMI [33] for conditional independence testing task on the flow-cytometry dataset. The dashed line denotes a random model.]()

![Figure 5: TE and daily closing prices of the Dow Jones Index (DJI) and the Hang Seng Index (HSI). The upper chart demonstrates the 30-day moving average of the TE estimated by the C-NJEE of DJI to HSI (DIJ → HSI) and in the opposite direction (HSI → DJI). The lower chart demonstrates the original closing prices of the two time series. Periods of financial stress with a significant decrease in the index prices are defined between a pair of dotted lines of the same color: the green lines represent the beginning and end of the Asian financial crisis, the red lines represent the beginning and end of the dot-com crisis, and the black lines represent the beginning and end of the 2008 global financial crisis.]()

The code of the polynomial method is provided by[[9]](#b8) in https://github.com/Albuso0/entropy. See the Entropy R package in[[45]](#b44) for the implementation of the other benchmark methods.

We thank the authors of[[19]](#b18) for providing us with the implementation code for the variational methods.

