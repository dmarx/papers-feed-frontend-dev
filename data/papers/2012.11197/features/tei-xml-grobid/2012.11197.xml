<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Joint Entropy Estimation</title>
				<funder>
					<orgName type="full">Koret foundation grant for Smart Cities and Digital Living</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-12-21">21 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuval</forename><surname>Shalev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amichai</forename><surname>Painsky</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Irad</forename><surname>Ben-Gal</surname></persName>
						</author>
						<title level="a" type="main">Neural Joint Entropy Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-21">21 Dec 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">F2DAF44EF4CDCFA73CBDB2560ADC62BC</idno>
					<idno type="arXiv">arXiv:2012.11197v1[cs.IT]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Joint Entropy</term>
					<term>Neural Network</term>
					<term>Cross-Entropy</term>
					<term>Mutual Information</term>
					<term>Transfer Entropy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the entropy of a discrete random variable is a fundamental problem in information theory and related fields. This problem has many applications in various domains, including machine learning, statistics and data compression. Over the years, a variety of estimation schemes have been suggested. However, despite significant progress, most methods still struggle when the sample is small, compared to the variable's alphabet size. In this work, we introduce a practical solution to this problem, which extends the work of McAllester and Statos  (2020). The proposed scheme uses the generalization abilities of cross-entropy estimation in deep neural networks (DNNs) to introduce improved entropy estimation accuracy. Furthermore, we introduce a family of estimators for related informationtheoretic measures, such as conditional entropy and mutual information. We show that these estimators are strongly consistent and demonstrate their performance in a variety of use-cases. First, we consider large alphabet entropy estimation. Then, we extend the scope to mutual information estimation. Next, we apply the proposed scheme to conditional mutual information estimation, as we focus on independence testing tasks. Finally, we study a transfer entropy estimation problem. The proposed estimators demonstrate improved performance compared to existing methods in all tested setups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E NTROPY is one of the basic building blocks of infor- mation theory <ref type="bibr" target="#b0">[1]</ref>. It quantifies the minimum average number of bits required to represent an event that follows a given probability distribution rule. Many important informationtheoretic measures such as mutual information (MI) and conditional MI (CMI) include marginal, conditional and joint entropies. These measures have many applications in machine learning, such as feature selection <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, representation learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and analyses of the learning mechanism <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>One of the first and basic entropy estimation methods is the classic plug-in scheme. In this scheme, an empirical distribution replaces the true (unknown) probability rule, and the corresponding empirical entropy is the estimated entropy. Unfortunately, this estimation scheme suffers from a negative bias <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, leading to limited outcomes. A variety of parametric and nonparametric methods have been proposed to improve the entropy estimation, such as in <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Recently, a neural network-based method was proposed to estimate entropy by minimizing the cross-entropy (CE) loss <ref type="bibr" target="#b10">[11]</ref> as an upper bound of the entropy. The CE measures the average number of bits required to represent an event that is generated from a probability distribution P by a different probability distribution Q. CE has its minimum when P = Q. Thus, minimizing CE Y. Shalev, A. Painsky and I. Ben-Gal are with the Department of Industrial Engineering, Tel-Aviv University, Ramat-Aviv 6997801, Israel. Correspondence to:&lt;yuvalshalev@mail.tau.ac.il&gt;.</p><p>implies searching for a Q that is as similar as possible in a log-loss <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> sense to P . This approach has several advantages. First, it uses the generalization power of neural networks and their universality <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Second, CE is less prone to negative bias and high variance in large entropy values <ref type="bibr" target="#b10">[11]</ref>. However, this approach has certain limitations. First, it requires prior assumptions on the true underlying distribution, as discussed in Section III. Second, the statistical properties of this CE estimator are currently unexplored. Therefore, the existence of a neural network-based estimator that can provide an accurate estimation of entropy, is not guaranteed.</p><p>These challenges in entropy estimation are also related to other information-theoretic measures. For example, one of the most common MI estimation schemes is the K-nearest neighbour (KNN) estimator <ref type="bibr" target="#b16">[17]</ref>. This estimator was shown to introduce a significant negative bias in setups with high dependencies between the variables, resulting in large MI values <ref type="bibr" target="#b17">[18]</ref>. Neural-network-based approaches have been recently proposed to overcome this problem using variational bound optimization <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Although a significant improvement in the MI estimation has been achieved, the results are not yet satisfying and suffer from theoretical limitations that are primarily manifested in large MI values <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>. There is also a large body of work on fundamental estimation bounds for different information-theoretic measures (see <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref> and related work).</p><p>In this paper, we address the inherent estimation challenges discussed above. The proposed estimation scheme focuses on joint entropy estimation. This problem is similar to the standard entropy estimation problem as any univariate random vector may be represented, for example, as a binary multivariate vector. In particular, we combine the chain rule with the CE loss minimization procedure using neural networks to obtain a more accurate joint entropy estimation. We denote this estimation procedure as the Neural Joint Entropy Estimator (NJEE). We study the properties of NJEE and show that it is strongly consistent. In a similar manner, we obtain the conditional NJEE (C-NJEE), as an estimator for the joint conditional entropy between two or more multivariate variables.</p><p>Having these two estimators, it is straightforward to estimate the MI between two random variables. Adding a second conditioning variable results in the CMI estimator. Additionally, we apply the proposed scheme to transfer entropy estimation (TE). Given two time series, the TE is defined as the CMI between the "past" of the first series and the "future" of the second series given its "past". TE is used to explore the information flow and causality among time-dependent data in neuroscience <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, finance <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, process control <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and many other applications. We show that by using an autoregressive neural network model, such as a recurrent neural network, C-NJEE can be used for efficient TE estimation.</p><p>The advantages of the estimators proposed in this paper are demonstrated in various use-cases. First, we study the entropy estimation of a discrete random variable with a large alphabet size. Applying NJEE to this problem, we outperform existing methods when the sample size is much smaller than the alphabet size. Further, we focus on MI estimation between two multivariate variables. A commonly used toy problem is used for this task. The performance of the proposed MI estimator demonstrates improved results in terms of lower bias and variance, compared to existing methods. This result is specifically manifested in larger values of MI. Next, we demonstrate the performance of the suggested CMI estimator, as we focus on conditional independence tests. We study a real protein dataset where dependencies among the variables (protein elements) are known. Also, the proposed estimation scheme demonstrates better results than existing methods. Finally, the CMI estimator is applied to a TE estimation task. Specifically, we study a real financial dataset of stock index prices and show that the C-NJEE-based estimation provides additional insights on the information flow between the time series that are not discovered by the other methods. These insights are in line with domain knowledge and the world financial timeline.</p><p>To summarize, the contributions of this paper are threefold. First, we extend the work of <ref type="bibr" target="#b10">[11]</ref> and introduce strongly consistent estimators for joint entropy and conditional joint entropy. The proposed estimators, NJEE and C-NJEE, are based on minimization of the CE loss while applying the entropy chain rule property. Second, we use these estimators to obtain estimators for related measures such as MI, CMI and TE. Third, we propose a practical implementation scheme of these estimators that demonstrates better performance than existing methods on various tasks and datasets.</p><p>The remainder of this paper is organized as follows. Related works on entropy, MI, CMI and TE estimation are discussed in Section II. In Section III, definitions and related mathematical overview are given to support the scheme and ideas proposed in this paper. The primary results are shown in Section IV. An empirical study of various tasks and comparisons with different benchmark methods are provided in Section V. We conclude this paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Estimating information-theoretic measures is a well-studied problem. We refer the reader to <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref> for a comprehensive review of these measures. The following literature review focuses on estimators that are relevant for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Entropy Estimation in Large Alphabet</head><p>As mentioned in Section I, the simplest method to estimate the entropy of a discrete random variable is the so plug-in estimator <ref type="bibr" target="#b0">[1]</ref>. The Miller-Madow estimator <ref type="bibr" target="#b28">[30]</ref> adds a bias correction to the plug-in estimator. This correction depends on the ratio between the number of symbols from the alphabet that appear at least once in the sample and the sample size.</p><p>More recently, <ref type="bibr" target="#b9">[10]</ref> proposed an estimator for the entropy of species in a community (in this biological context, the entropy is called the diversity index), where the number of species (alphabet size) is large and unknown. This estimator is based on the Horvitz-Thompson estimator for population size and the Good-Turing estimator for the probability of unseen events. In <ref type="bibr" target="#b8">[9]</ref>, an entropy estimator is obtained using a polynomial approximation for the terms in the entropy sum that involve small probabilities with respect to log k, where k is the alphabet size. For larger probabilities, an unbiased plug-in estimator is used that is similar to the Miller-Madow estimator. Thus, improved results are demonstrated on simulated data of discrete random variables with large alphabet sizes where many symbols have relatively low probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MI and CMI Estimation</head><p>One of the most popular MI estimators in recent years is the KNN-based KSG estimator <ref type="bibr" target="#b16">[17]</ref>, which uses KNN-based density estimation over a shared space of the marginal and conditional entropy. Using the connection between the MI and entropy (see III-B), the entropies' bias terms are subtracted to provide a more accurate MI estimation. This metric suffers from the curse of dimensionality and underestimates the MI when the interaction between the variables is strong <ref type="bibr" target="#b30">[31]</ref>.</p><p>The recent advances in deep learning motivated various researchers to address the dimensionality problem by estimating the MI with neural networks. This is usually obtained by finding variational lower bound for the MI (typically, a differentiable function that its supremum is the MI). These functions are approximated by neural networks to maximize the lower bound <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. These methods yield improved results compared to the KNN-based estimator, but are quite limited when estimating large MI values, since their estimation complexity increases exponentially with the number of samples <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>. To overcome this problem, <ref type="bibr" target="#b10">[11]</ref> proposed using the CE as an upper bound for the entropy and minimize it by training a neural network. Thus, an MI estimate is obtained by subtracting the estimated conditional entropy from the estimated marginal entropy. This approach underlines the proposed estimation scheme as discussed in further detail in subsection III-C. A similar approach for MI estimation using the softmax function (e.g., as the output layer in a neural network), was suggested in <ref type="bibr" target="#b31">[32]</ref>. However, this scheme is limited to the case where the input variable is multivariate, while the target variable is univariate. Classifier based conditional mutual information (CCMI) was proposed in <ref type="bibr" target="#b32">[33]</ref>. A two-sample classifier was used to distinguish between samples from the joint distribution and samples from the marginal distribution. Combining conditional generative models (e.g., Conditional Generative Adversarial Networks (CGAN) or Conditional Variational Autoencoders (CVAE)), an estimator for the CMI was developed. This approach introduced a significant improvement over other recently proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. TE Estimation</head><p>The TE is defined as a form of CMI between time series. Specifically, T E(Y f uture ; X past ) =</p><p>CM I(Y f uture , X past |Y past ) (see a formal definition of TE in Subsection III-B). There are two primary approaches for TE estimation. The first approach considers every variable in every timestamp as a separate variable, and uses any MI or CMI estimator to estimate the TE <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>. The second approach applies a sequential model that considers the time dependencies among different time lags to extract an estimator for the TE and its related measures <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p><p>As a representative of the first approach, a recently proposed estimator <ref type="bibr" target="#b35">[36]</ref> applies a neural network two-sample classifier to estimate the TE. Using the second approach, the Context Tree Weighting (CTW) algorithm <ref type="bibr" target="#b38">[39]</ref> is utilized in <ref type="bibr" target="#b36">[37]</ref> for directed information estimation (a closely related measure to TE <ref type="bibr" target="#b39">[40]</ref>). Both works investigate a financial time series of index prices to evaluate their estimators. we use the same dataset to evaluate the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BACKGROUND A. Notations</head><p>The following notations are used throughout this paper. A univariate discrete random variable is denoted by an upper-case letter (e.g., X), that obtains values x from the alphabet A x = {1, . . . , a x }. A multivariate variable with dimensions d x is denoted by an underline, (e.g., X), where its values are denoted by underlined lower-case letter x. The m th component of X is denoted as X m , which obtains values x m from the alphabet</p><formula xml:id="formula_0">A xm = {1 . . . a xm } which can be different for different values of m. The vector of the first k components of X is denoted by X k .</formula><p>We denote H n (X) as the estimator of X's entropy given a sample S = {. . .} n i=1 , where it is implied from the text that S is a collection of n samples of X. This notation holds for other estimators as well. For example, I n (X; Y |Z) is an estimator of the CMI between X and Y given Z, from a collection of n samples from the joint distribution of X, Y and Z. To avoid an overload of notation, we denote x i as the i th sample in S, while X m is the m th component of the random vector X.</p><p>For the time notation, a multivariate variable in time t is represented by a bracket index, e.g., X (t) and a matrix that represents its past l time lags is represented by</p><formula xml:id="formula_1">X (l) (t) = [X (t-l) . . . X (t) ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Definitions</head><p>Let X be a discrete random variable that follows a probability distribution P (X). Shannon's entropy is defined as:</p><formula xml:id="formula_2">H(X) = -E P (X) [log P (x)] .<label>(1)</label></formula><p>The entropy (1) can be represented by the chain-rule</p><formula xml:id="formula_3">H(X) = H(X 1 , X 2 , . . . , X dx ) = dx m=1 H(X m |X m-1 , . . . , X 1 ),<label>(2)</label></formula><p>where H(X 1 |X 0 ) abbreviates H(X 1 ).</p><p>The CE between any two distribution functions P (X) and Q(X) is defined as:</p><formula xml:id="formula_4">CE(Q(X)) = -E P (X) [log Q(X)] ,<label>(3)</label></formula><p>where the expectation is over the distribution of X, namely, P (X).</p><p>The following inequality holds for every pair of distributions P (X) and Q(X):</p><formula xml:id="formula_5">CE(Q(X)) ≥ H(X),<label>(4)</label></formula><p>Where an equality is obtained for Q(X) = P (X).</p><p>A related measure to CE is the Kullback-Leibler divergence (D KL ) between P (X) and Q(X)</p><formula xml:id="formula_6">D KL (P (X)||Q(X)) = E P (X) log P (X) Q(X) .<label>(5)</label></formula><p>The D KL is a nonnegative measure and equals zero iff</p><formula xml:id="formula_7">P (X) = Q(X).</formula><p>The MI, denoted as I(X; Y ), quantifies in bits the entropy reduction in X given the knowledge obtained from another random variable Y , i.e.,</p><formula xml:id="formula_8">I(X; Y ) = H(X) -H(X|Y ).<label>(6)</label></formula><p>Another important measure that is represented by the difference of entropies is conditional mutual information (CMI)</p><formula xml:id="formula_9">I(X; Y |Z) = H(Y |Z) -H(Y |X, Z).<label>(7)</label></formula><p>CMI is also used to evaluate the TE, which is defined as</p><formula xml:id="formula_10">T E X→Y = I(X (k) (t) ; Y (t+1) |Y (l) (t) ).<label>(8)</label></formula><p>Assuming discrete time, the T E X→Y is the CMI between the past k time lags of X and Y at time t + 1 given the past l time lags of Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CE-Based Entropy</head><p>Let P (X) be the distribution function of X. Let T θ (X) be a neural network that approximates it. In <ref type="bibr" target="#b10">[11]</ref>, the following upper bound for the entropy of X was proposed</p><formula xml:id="formula_11">H Θ (X) = inf θ∈Θ CE(T θ (X)),<label>(9)</label></formula><p>and</p><formula xml:id="formula_12">H Θ (X) = H(X) iff P (X) = T θ (X).</formula><p>Given a sample S of size n, the sample mean is used to estimate the CE,</p><formula xml:id="formula_13">CE n (T θ (X)) = - 1 n n i=1 log T θ (x i ).<label>(10)</label></formula><p>Then, an estimator of the entropy is:</p><formula xml:id="formula_14">H n (X) = inf θ∈Θ CE n (T θ (X)).<label>(11)</label></formula><p>In <ref type="bibr" target="#b10">[11]</ref>, the authors suggest an entropy estimator based on the above. However, they require prior knowledge of P (X) for the training of their neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MEASURING THE JOINT ENTROPY WITH NEURAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NETWORKS</head><p>In this section we discuss the primary concepts of this paper. First, the neural network classifier and its respective CE are formally defined. Then, the neural joint entropy estimator is introduced. Next, we define a strongly consistent estimator and show that the proposed joint entropy estimator satisfies this property. We also provide an algorithmic implementation of the proposed estimator and discuss practical aspects of its implementation. Next, estimator for the joint conditional entropy is provided with the corresponding algorithmic implementation. Using the estimators of the joint entropy and the conditional joint entropy, estimators for MI, CMI and TE are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neural Network Classifier and Classification CE</head><p>The following basic definitions are used throughout this section.</p><formula xml:id="formula_15">Definition IV.1. (Neural network classifier). Let G θ (Y |X)</formula><p>be a neural network model with a random variable input X and parameters θ in a compact domain Θ ∈ R k . The outputs of G θ (Y |X) are defined over the probability simplex:</p><formula xml:id="formula_16">{G θ (y|x) ∈ R ay : ay y=1 G θ (y|x) = 1, G θ (y|x) ≥ 0}.</formula><p>Next, we define the CE of this classifier.</p><p>Definition IV.2. (Classifier CE). Let G θ (y|x) be a neural network classifier. The CE of this classifier is defined as</p><formula xml:id="formula_17">CE(G θ (Y |X)) = -E P (X,Y ) log G θ (y|x),<label>(12)</label></formula><p>where Y ∈ A y = {1, . . . , a y }, a y ≥ 2.</p><p>We assume that -log(G θ (y|x)) ≤ η for all x ∈ X and for all θ ∈ R k , for any value of Y . Practically, this assumption is used in many model training procedures to avoid an unbounded loss <ref type="bibr" target="#b12">[13]</ref>. The empirical estimator of this CE is given by <ref type="bibr" target="#b40">[41]</ref>, namely</p><formula xml:id="formula_18">CE n (G θ (Y |X)) = - 1 n n i=1 log(G θ (y i |x i )).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Joint Entropy Estimation</head><p>Using (2) and Definitions IV.1 and IV.2, we define the estimator of the joint entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition IV.3. (Neural Joint Entropy Estimator (NJEE)).</head><p>Let H n (X 1 ) be an estimated marginal entropy of the first components in X and let G θm (X m |X m-1 ) be a neural network classifier. Then, NJEE is defined as</p><formula xml:id="formula_19">H n (X) = H n (X 1 ) + dx m=2 CE n (G θm (X m |X m-1 ). (14)</formula><p>In words, the joint entropy estimator consists of a marginal estimator for the first component, followed by estimators for the conditional entropies H(X m |X m-1 ), for m = 2, . . . , d x .</p><p>Definition IV.4. (Strong consistency (following <ref type="bibr" target="#b17">[18]</ref>)). The estimator H n (X) is strongly consistent if for all , δ &gt; 0 and a constant C &gt; 0, there exists a positive integer N and a choice of a neural network such that:</p><formula xml:id="formula_20">∀n ≥ N, |H(X) -H n (X)| ≤ C • + δ, a.e.</formula><p>Theorem 1. NJEE is strongly consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Strong Consistency Property</head><p>In this section we follow the scheme shown in <ref type="bibr" target="#b17">[18]</ref> to prove Theorem 1. This proof includes the following main steps:</p><p>1) Connecting the true CE of a classifier-based neural network and the conditional entropy H(Y |X) (Lemmas 1 and 2). 2) Showing the convergence of the empirical CE to the true CE (Lemma 4). 3) Showing that the empirical CE can approximate with high accuracy the conditional entropy (Lemma 5). 4) Applying the chain-rule property and the previous steps to show that the proposed estimator of the joint entropy is strongly consistent. We begin with the first step. Formally, since neural networks are universal approximation functions <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, the following holds:</p><p>Lemma 1. For any &gt; 0, and any conditional distribution function P (Y |X), there exists a neural network G θ (Y |X) such that:</p><formula xml:id="formula_21">D KL (P (Y |X)||G θ (Y |X)) ≤ 2 , a.e.<label>(15)</label></formula><p>That is, it is possible to find a neural network that can approximates P (Y |X) in any desired approximation level.</p><p>The next Lemma states that the CE can be used to estimate the conditional entropy.</p><p>Lemma 2. Let P (Y |X) be a conditional distribution and let H(Y |X) be the entropy associated with this distribution. Then, for any &gt; 0, there exists a neural network G θ (Y |X) such that</p><formula xml:id="formula_22">|CE (G θ (Y |X)) -H(Y |X)| ≤ 2 , a.e. (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>The proof of this lemma follows the ideas shown in <ref type="bibr" target="#b10">[11]</ref>).</p><formula xml:id="formula_24">H(Y |X) = E P (X,Y ) log 1 P (y|x) = E P (X,Y ) log 1 G θ (y|x) G θ (y|x) P (y|x) = E P (X,Y ) log 1 G θ (y|x) -D KL (P (y|x)||G θ (y|x)) ≥ CE(G θ (Y |X)) - 2 ,<label>(17)</label></formula><p>where the last line follows Lemma 1. As shown in (4), we have that</p><formula xml:id="formula_25">CE(G θ (Y |X)) -H(Y |X) ≥ 0,<label>(18)</label></formula><p>therefore</p><formula xml:id="formula_26">|CE (G θ (Y |X)) -H(Y |X)| ≤ 2 .</formula><p>The empirical estimator for this classifier CE is obtained from <ref type="bibr" target="#b12">(13)</ref>. The conditions for the convergence of this estimator are defined by the uniform law of large numbers. Lemma 3. The uniform law of large numbers <ref type="bibr" target="#b41">[42]</ref>. Let Θ be a compact set of parameters. Let f θ (x i ) be a continuous function at each θ ∈ Θ and x i ∈ X . Assume there exists an upper bound η(X) such that f (x) ≤ η(x) for all θ ∈ Θ and E[η(X)] &lt; ∞. Then, E[f θ (X)] is continuous and</p><formula xml:id="formula_27">sup θ∈Θ 1 n n i=1 f θ (x i ) -E[f θ (X)] p → 0. (<label>19</label></formula><formula xml:id="formula_28">)</formula><p>Using Lemma 3, the convergence of the classifier CE is obtained Lemma 4. For any &gt; 0 and ∀θ ∈ Θ, there exists a positive integer n ≥ N such that:</p><formula xml:id="formula_29">P (| CE n (G θ (Y |X)) -CE(G θ (Y |X))| ≤ 2 ) = 1. (<label>20</label></formula><formula xml:id="formula_30">)</formula><p>The proof of this Lemma is an immediate application of ( <ref type="formula" target="#formula_18">13</ref>) with</p><formula xml:id="formula_31">f θ ((x i , y i )) = -log(G θ (y i |x i )).<label>(21)</label></formula><p>since -log(G θ (y i |x i )) ≤ η, then f θ ((x i , y i )) ≤ η and Lemma 3 holds.</p><p>Lemma 5. The estimator CE n (G θ (Y |X)) is strongly consistent. That is, for all &gt; 0, there exists a positive integer n ≥ N and a choice of neural network such that:</p><formula xml:id="formula_32">|H(Y |X) -CE n (G θ (Y |X))| ≤ , a.e. (<label>22</label></formula><formula xml:id="formula_33">)</formula><p>This lemma is obtained using the triangular inequality with Lemmas 2 and 4:</p><formula xml:id="formula_34">|H(Y |X) -CE n (G θ (Y |X))| ≤ |CE (G θ (Y |X)) -H(Y |X)|+ | CE(G θ (Y |X)) -CE(G θ (Y |X))| ≤ . (23) Restating (2), H(X) = H(X 1 ) + dx m=2 H(X m |X m-1 ).<label>(24)</label></formula><p>Suppose there exists d x -1 neural networks that approximate each term in the sum with an accuracy. Then, the total error of the sum expression is • (d x -1). The marginal entropy H(X 1 ) is estimated with an estimator H n (X 1 ) that guarantees an error that is not larger than certain δ &gt; 0. Several estimators can provide such a guarantee, e.g., <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. In this case:</p><formula xml:id="formula_35">|H(X) -H n (X)| = |H(X 1 ) -H n (X 1 )+ dx m=2 H(X m |X m-1 ) - dx m=2 CE n (G θm (X m |X m-1 )| ≤ |H(X 1 ) -H n (X 1 )|+ | dx m=2 H(X m |X m-1 ) - dx m=2 CE n (G θm (X m |X m-1 )| ≤ δ + C • ,<label>(25)</label></formula><p>where</p><formula xml:id="formula_36">C = d x -1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Algorithmic Implementation of NJEE</head><p>The implementation of the NJEE estimator is described in Algorithm 1. </p><formula xml:id="formula_37">Algorithm 1 NJEE 1: input: Sample S = {x i } n i=1 from P (X) 2: h m ← 0, for m = {1, . . . , d x } 3: h 1 ← H n (X 1 ) 4: Initialize {θ m } dx</formula><formula xml:id="formula_38">h m ← Minimize CE n (G θm (X m |X m-1 )) 7: end for 8: H n (X) ← h 1 + dx m=2 h m 9: return: H n (X)</formula><p>Practically, Algorithm 1 can be implemented in parallel per each value of m. Another approach is to use a recurrent neural network (RNN) that replaces the d x -1 networks. In this case, the sequential input to the RNN is the components vector of X (e.g., see distribution estimation with RNN in <ref type="bibr" target="#b42">[43]</ref>). Then, the estimated entropy would be the sum of all the CE losses in every time step. The empirical results of this implementation demonstrate similar performance to Algorithm 1.</p><p>We also note that by using the CE loss, it is possible to replace the neural network model with any other classifier to estimate the entropy. However, in this case, Lemma 1 may not apply, and strong consistency is not guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Conditional-Neural Joint Entropy Estimation</head><p>The conditional entropy of two multivariate random variables X and Y is</p><formula xml:id="formula_39">H(X|Y ) = dx m=1 H(X m |Y , X m-1 ). (<label>26</label></formula><formula xml:id="formula_40">)</formula><p>To estimate <ref type="bibr" target="#b25">(26)</ref>, a slight change is made to NJEE, where all components in the proposed estimator are neural networks.</p><p>Definition IV.5. (Conditional Neural Joint Entropy Estimator (C-NJEE)). Let G θm (X m |Y , X m-1 ) be a neural network classifier with inputs Y and X m-1 . Then C-NJEE is defined as,</p><formula xml:id="formula_41">H n (X|Y ) = dx m=1 CE n (G θm (X m |Y , X m-1 )). (<label>27</label></formula><formula xml:id="formula_42">)</formula><p>Corollary 1.1. C-NJEE is strongly consistent.</p><formula xml:id="formula_43">|H(X|Y ) - dx m=1 CE n (G θm (X m |Y , X m-1 ))| ≤ d x • , a.e. (<label>28</label></formula><formula xml:id="formula_44">)</formula><p>The proof of Corollary 1.1 is straightforward. Notice that every conditional entropy in the sum expression of ( <ref type="formula" target="#formula_39">26</ref>) can be estimated by a classifier CE with estimation error. Since there are d x conditional entropies estimators, the total estimation error of H(X|Y</p><formula xml:id="formula_45">) is d x • . The implementation of C-NJEE is described in Algorithm 2. Algorithm 2 C-NJEE 1: input: Sample S = {x i , y i } n i=1 from P (X, Y ) 2: h m ← 0, for m = {1, . . . , d x } 3: Initialize {θ m } dx m=1 4: for m in 1 to d x do 5: h m ← Minimize CE n (G θm (X m |Y , X m-1 )) 6: end for 7: H n (X|Y ) ← dx m=1 h m 8: return: H n (X|Y )</formula><p>We now apply NJEE and C-NJEE to introduce an estimator for the MI.</p><formula xml:id="formula_46">I n (X; Y ) = H n (X 1 ) + dx m=2 CE n (G θm (X m |X m-1 ) - dx m=1 CE n (G θm (X m |Y , X m-1 )),<label>(29)</label></formula><p>Similarly, given a variable Z, an estimator for the CMI ( <ref type="formula" target="#formula_9">7</ref>) can be obtained</p><formula xml:id="formula_47">I n (X; Y |Z) = dx m=1 CE n (G(X m |Z, X m-1 )) - dx m=1 CE n (G(X m |Z, Y , X m-1 )).<label>(30)</label></formula><p>Again, since all models are trained independently, the worst case error of these estimators is the sum of the errors of NJEE and C-NJEE, thus these estimators are also strongly consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section we demonstrate the performance of the proposed estimators in various estimation tasks.</p><p>To apply these estimators, we train a set of neural networks. Unless stated otherwise, the following basic network structure is considered throughout these experiments: An input layer, two fully connected layers with 50 nodes, a ReLU activation function and an output softmax layer. The loss is optimized with the ADAM <ref type="bibr" target="#b43">[44]</ref> optimizer with the following parameters (lr = 0.001, β 1 = 0.9, β 2 = 0.999).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Entropy Estimation with Large Alphabet</head><p>We begin this experimental section with large alphabet entropy estimation using NJEE. Prior to applying NJEE, we change the univariate representation values of the alphabet to their binary representation. Any other small alphabet representation, such as ternary, is also valid. The evaluation is preformed on six simulated studies, most of which were used in previous works (e.g., <ref type="bibr" target="#b8">[9]</ref>):</p><p>• Uniform distribution. • Zipf's law distribution with parameters α = 1, 2.</p><p>• Geometric distribution with p = 1/10 5 .</p><p>• Symmetric mixture of a Zipf's law distribution (α = 1)</p><p>and Geometric distribution (p = 2/10 5 ).</p><p>• Discrete Laplace (DL), where DL(X, σ) ∝<ref type="foot" target="#foot_0">foot_0</ref> 2σ e -X σ and σ = 10 -4 . The alphabet size of X is set to 10 5 (excluding the last experiment where the alphabet is not limited). Every simulated study (defined by a distribution type and a sample size) is repeated 100 times.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> demonstrates the root mean squared (RMSE) of the entropy estimation as a function of the sample size for NJEE and other entropy estimators described in Section II-A 1 . As shown, NJEE demonstrates the lowest RMSE in most cases. Specifically, NJEE demonstrates the lowest error in all the experiments where n ≤ 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multivariate MI Estimation</head><p>In the following set of experiments we apply the proposed scheme to a simple and commonly used multivariate MI estimation problems, as used in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b32">[33]</ref>. The setup is defined as follows. Let X and Y be two random vectors in</p><formula xml:id="formula_48">R d such that X Y T ∼ N (0, Σ XY ) Σ XY = I d ρI d ρI d I d .<label>(31)</label></formula><p>Notice that the correlation between the pairs (X i , Y j ) is ρ when i = j and zero otherwise. Further, Cov(X) = Cov(Y ) = I d , and the MI between X and Y is thus simply:</p><formula xml:id="formula_49">I(X; Y ) = - d 2 • log(1 -ρ 2 ).</formula><p>In this study, samples are generated from the model above, using different values of ρ (or equivalently, different values of MI). Since the proposed algorithm is designed for discrete variables, we quantize the samples using a simple binning scheme. Binning continuous data for MI estimation has been extensively studied over the years. The interested reader is referred to <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> for a thorough discussion.</p><p>In Figure <ref type="figure" target="#fig_2">2</ref>, the NJEE-based algorithms are compared to the KNN MI estimation method <ref type="bibr" target="#b16">[17]</ref>. With low absolute values of ρ, the two methods yield accurate results. As ρ increases (and thus the MI increases), the KNN estimator significantly deviates from the true value, as demonstrated in <ref type="bibr" target="#b17">[18]</ref>. NJEE yields better results for greater MI, similar to <ref type="bibr" target="#b10">[11]</ref>, yet without a prior assumption on the characteristics of the underlined distribution.</p><p>Let us now turn to an additional synthetic experiment, following <ref type="bibr" target="#b18">[19]</ref>. Again, we draw samples from the model described in <ref type="bibr" target="#b30">(31)</ref>. In this experiment, we begin with ρ = 0 and draw a total of 4000 batches with 64 samples in each batch. Then, we estimate the MI from the drawn samples. We increase ρ and repeat the previous step. We terminate at ρ = 1. I n (X; Y ) is compared to the recently proposed variational methods <ref type="foot" target="#foot_1">2</ref> . As demonstrated in Figure <ref type="figure" target="#fig_3">3</ref>, the results achieved  by the proposed estimator exhibits lower bias and variance with respect to the variational benchmark methods. The upper rows of Table I demonstrate the best estimation results for each method obtained by hyperparameter grid search. The proposed NJEE scheme yields better results for most MI values ranging from 2 to 20. The reasons for the bias and variance errors in the variational bound methods are discussed in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Let us now study estimator sensitivity to invertible transformation, in which we do not expect any change in the MI under such transformations. The cubic transformation y ⇒ z = (W y) 3 is chosen for this experiment, where W is an invertible d × d matrix with the entries w ij ∼ N (0, 1). The lower rows of Table I summarize the results. As shown, the proposed MI estimator yields identical results to the original problem, while the alternative methods yield lower estimates.</p><p>Due to stability issues in the benchmark methods, we could not obtain estimates for the cubic transformation when the underlying MI equals 20.0 nats. Table <ref type="table">I</ref>: Best results of every estimator following a hyperparameter grid search for the Gaussian setup (31) (upper rows) and its cubic transformation (lower rows). The true MI values are shown in the first row. The results of the benchmark methods for 2 to 10 nats are also reported in <ref type="bibr" target="#b18">[19]</ref>.</p><p>TRUE MUTUAL INFORMATION 2.0 4.0 6.0 8.0 10.0 20.0 GAUSSIAN SETUP NJEE 2.2 4.1 5.9 7.8 9.6 17.8 α 1.9 3.8 5.7 7.4 8.8 11.7 JS 1.2 3.0 4.8 6.5 8.1 15.5 NWJ 1.6 3.5 5.2 6.7 8 10.8 InfoNCE 1.9 3.6 4.9 5.7 6 6.2 CUBIC SETUP NJEE 2.2 4.1 5.9 7.8 9.6 17.8 α 1.7 3.6 5.4 6.9 8.2 -JS 1 2.8 4.5 6.1 7.6 -NWJ 1.5 3.2 4.7 5.9 6.9 -InfoNCE 1.7 3.2 4.1 4.6 4.8 -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Conditional Independence Testing</head><p>We now investigate the proposed method in conditional independent testing (CIT). CIT is a basic task in statistics with applications to a variety of domains, such as Bayesian networks and causality analysis <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b50">[51]</ref>. In this experiment, we use a flow-cytometry dataset <ref type="bibr" target="#b51">[52]</ref>. This dataset describes the connections between eleven proteins in different experimental setups. Sachs et al., <ref type="bibr" target="#b51">[52]</ref> introduced a consensus Bayesian network (see Figure <ref type="figure" target="#fig_3">3</ref> in their work) that is considered the ground truth of the connections mapping among the proteins. The flow-cytometry dataset was extensively studied in several works. In <ref type="bibr" target="#b32">[33]</ref>, the authors introduced a CIT method that incorporates a two-sampled classifier and generative models. In <ref type="bibr" target="#b49">[50]</ref>, a KNN bootstrap and binary classifier procedure was proposed to perform the CIT.</p><p>Before we describe the results of the experiment, we provide some preliminaries on Bayesian networks that are used for this experiment. In a Bayesian network, features are represented by nodes, and their dependencies are represented by edges <ref type="bibr" target="#b52">[53]</ref>. Node A is a parent of node B if there is a directed edge from A to B, and B is considered a child of A. Y is conditionally independent of X when Z is a subset of the features that holds all available information about Y . These features are the parents of Y , its children and the parents of its children (Markov Blanket <ref type="bibr" target="#b53">[54]</ref>). Based on these notations, one can choose multiple combinations of dependent and conditionally independent triplet sets of variables. Following the procedures proposed in <ref type="bibr" target="#b49">[50]</ref> and <ref type="bibr" target="#b32">[33]</ref>, 50 dependent and 50 conditionally independent triplets (X, Y, Z) are randomly chosen and their CMI is estimated using I n (X; Y |Z). For every triplet we have the ground truth (dependent/independent), and its corresponding estimate În (X; Y |Z). Since the estimates În (X; Y |Z) are continuous (nonnegative) numbers, we may set a decision threshold. Specifically, we say that a triplet is conditionally independent if its În (X; Y |Z) value is lower than a decision threshold (and vice versa). Thus, one could construct an ROC curve where every point in the curve represents a value of the threshold , the value of the false positive rate (the horizontal axis) and the true positive rate (the vertical axis). Figure <ref type="figure" target="#fig_4">4</ref> illustrates the ROC curve and the area under the curve (AUC) values of the independence test performed with I n (X; Y |Z) and with the benchmarks as reported in <ref type="bibr" target="#b32">[33]</ref>. As </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Estimating TE on Financial Dataset</head><p>Finally, we apply C-NJEE to TE estimation. For this experiment, we study a financial dataset that contains the daily closing prices of the Dow-Jones Index (DJI -the stock index of 30 large companies in the U.S. stock exchange) and the Hang Seng Index (HSI -the stock index of 50 large companies in the Hong-Kong stock exchange) between 1990 and 2011. As the DJI index is considered more influential than the HSI on the world's financial markets, we expect the transfer entropy T E DJI→HSI to be significantly greater than T E HSI→DJI . Additionally, we expect to see changes in the TE that are coordinated with related economic events (e.g., significant financial crises).</p><p>To estimate the TE, we reproduce the preprocessing used in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b35">[36]</ref>, and bin the data to three levels of daily price change. A negative change of more than -0.8% is denoted by -1, an absolute change that is below 0.8% is denoted by 0, and a change that is greater than 0.8% is denoted by +1. Then, the C-NJEE algorithm is applied with a recurrent neural network that has the following structure: an input layer, followed by an LSTM cell <ref type="bibr" target="#b54">[55]</ref> with 50 nodes, a fully connected layer with 50 nodes with ReLU activation and an output softmax layer. Input data are divided into sequences of length five (i.e., five consecutive trading days). The optimization procedure includes an ADAM optimizer <ref type="bibr" target="#b43">[44]</ref>, with the following parameters: lr = 0.001, β 1 = 0.9, β 2 = 0.999.</p><p>The upper chart of Figure <ref type="figure" target="#fig_0">5</ref> illustrates the 30 day moving average of T E DJI→HSI and T E HSI→DJI , as measured by C-NJEE. As expected, the information flow from DJI to HSI exceeds that of the opposite direction. Compered to the real prices in the lower chart of Figure <ref type="figure" target="#fig_0">5</ref>, a relatively sharp increase in T E DJI→HSI is observed in times of financial stress where prices decreasing sharply, such as in the Asian financial crisis <ref type="bibr">(1997)</ref><ref type="bibr">(1998)</ref>, the dot-com crisis <ref type="bibr">(2000)</ref><ref type="bibr">(2001)</ref><ref type="bibr">(2002)</ref> and the 2008-2009 financial turmoil <ref type="bibr" target="#b55">[56]</ref>. This phenomenon is well known in the financial literature (e.g., <ref type="bibr" target="#b24">[25]</ref>).</p><p>Comparing the results of the proposed method to those reported in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b35">[36]</ref>, we observe that these methods also found that the information flow from DJI to HSI is much larger then in the opposite direction. However, they did not clearly determine a connection between information values and the world's financial timeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this work, we introduce a neural joint entropy estimator (NJEE). The proposed estimator is based on minimizing the CE using neural networks. Expending earlier works, we show that NJEE is strongly consistent and provide a simple algorithmic implementation. We apply the proposed approach to entropy estimation of random variables, specifically those with a large alphabet, using a simple binary transformation. Further, we introduce the conditional neural joint entropy estimator (C-NJEE), which is an estimator for conditional joint entropy. We use NJEE and C-NJEE to estimate both mutual information (MI) and conditional mutual information (CMI).</p><p>We demonstrate the performance of the proposed schemes in synthetic and real-world experiments. NJEE achieves a lower RMSE on various simulated setups of random variables with large alphabets and relatively small sample size. Moreover, the proposed MI estimator exhibits lower bias and variance compared to newly-proposed variational lower bounds methods. This result is specifically evident in large MI values. The CMI estimator is further used to execute conditional independence tests. Again, the proposed estimator yields larger AUC value than other existing methods. Finally, we demonstrate the abilities of C-NJEE in estimating the TE. We investigate the dynamics of information flow among financial time series and show their correlation with significant economic events. Certain important characteristics of these dynamics are not captured by other estimation methods that were implemented on the same dataset.</p><p>We believe that future research will use the proposed entropy estimators to develop advanced compression schemes for various types of datasets. Additionally, the MI and CMI estimation capabilities can be used to improve the understanding of complex systems and deep learning frameworks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>m=2 5 :</head><label>5</label><figDesc>for m in 2 to d x do 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The log of the RMSE of entropy estimations versus the log of the sample size for NJEE and benchmark methods in different simulated studies. The results are the average of 100 measurements per each sample size and distribution type.</figDesc><graphic coords="7,92.00,56.07,428.00,212.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MI estimation of the study in (31) with various values of ρ. I n (X; Y ) is compared to the KNN (k = 3) method [17]. The dimensions of X and Y are 20.</figDesc><graphic coords="7,58.83,320.08,231.33,181.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MI estimation with NJEE versus recently proposed variational methods from [19]. Samples from two multivariate random variables in d = 20 are generated according to (31) with an increasing ρ every 4000 batches. The estimated MI in every batch appears in light blue, the moving average of the MI over a rolling window of 200 batches is shown in dark blue and the true MI value is represented by the black line. The variational bounds shown in this figure are further discussed in the literature (see NWJ [48] , InfoNCE [5], Jensen-Shannon lower bound (JS), and the interpolated bound between NWJ and NCE with α = 0.01 and α = 0.99 [19]).</figDesc><graphic coords="8,92.25,56.07,427.50,212.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The ROC curve and the AUC values of C-NJEE based estimation, CCIT [50] and CCMI [33] for conditional independence testing task on the flow-cytometry dataset. The dashed line denotes a random model.</figDesc><graphic coords="8,344.28,367.90,186.45,136.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: TE and daily closing prices of the Dow Jones Index (DJI) and the Hang Seng Index (HSI). The upper chart demonstrates the 30-day moving average of the TE estimated by the C-NJEE of DJI to HSI (DIJ → HSI) and in the opposite direction (HSI → DJI). The lower chart demonstrates the original closing prices of the two time series. Periods of financial stress with a significant decrease in the index prices are defined between a pair of dotted lines of the same color: the green lines represent the beginning and end of the Asian financial crisis, the red lines represent the beginning and end of the dot-com crisis, and the black lines represent the beginning and end of the 2008 global financial crisis.</figDesc><graphic coords="9,324.32,56.07,226.38,291.48" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code of the polynomial method is provided by<ref type="bibr" target="#b8">[9]</ref> in https://github.com/Albuso0/entropy. See the Entropy R package in<ref type="bibr" target="#b44">[45]</ref> for the implementation of the other benchmark methods.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We thank the authors of<ref type="bibr" target="#b18">[19]</ref> for providing us with the implementation code for the variational methods.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors would like to thank <rs type="person">Digital Living</rs> 2030 grant and the <rs type="funder">Koret foundation grant for Smart Cities and Digital Living</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast binary feature selection with conditional mutual information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1531" to="1555" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimation of entropy and mutual information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1191" to="1253" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimax rates of entropy estimation on large alphabets via best polynomial approximation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3702" to="3720" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of shannon&apos;s index of diversity when there are unseen species in sample</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental and ecological statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Formal limitations on the measurement of mutual information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="875" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the universality of the logistic loss function</title>
		<author>
			<persName><forename type="first">A</forename><surname>Painsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="936" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bregman divergence bounds and universality properties of the logarithmic loss</title>
		<author>
			<persName><forename type="first">A</forename><surname>Painsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1658" to="1673" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A closer look at the approximation capabilities of neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F E</forename><surname>Chong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06505</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">Mine: mutual information neural estimation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06922</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding the limitations of variational mutual information estimators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06222</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minimax estimation of functionals of discrete distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2835" to="2885" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer entropy-a model-free measure of effective connectivity for the neurosciences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pipa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="67" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient transfer entropy analysis of non-stationary neural time series</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wollstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martinez-Zarzuela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Diaz-Pernas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analysing the information flow between financial time series</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marschinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B-Condensed Matter and Complex Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="281" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The impact of the financial crisis on transatlantic information flows: An intraday analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dimpfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of International Financial Markets, Institutions and Money</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Granger causality and transfer entropy are equivalent for gaussian variables</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">238701</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Direct causality detection via the transfer entropy approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on control systems technology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2052" to="2066" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Empirical estimation of information measures: A literature guide</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verdú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">720</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Note on the bias of information estimates</title>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information theory in psychology: Problems and methods</title>
		<imprint>
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An introduction to transfer entropy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bossomaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="65" to="95" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient estimation of mutual information for strongly dependent variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking softmax with cross-entropy: Neural network classifier as mutual information estimator</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10688</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ccmi: Classifier based conditional mutual information estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01824</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Escaping the curse of dimensionality in estimating multivariate transfer entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heitzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Petoukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page">258701</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mute: a matlab toolbox to compare established and novel estimators of the multivariate transfer entropy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Montalto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Faes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e109462</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Itene: Intrinsic transfer entropy neural estimator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simeone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cvetkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07277</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Universal estimation of directed information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Permuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6220" to="6242" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context based predictive information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben-Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">645</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The context-tree weighting method: Extensions</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Willems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="792" to="798" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The relationship between transfer entropy and directed information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aviyente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Statistical Signal Processing Workshop (SSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="73" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Large sample estimation and hypothesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfadden</surname></persName>
		</author>
		<editor>RF Engle and DL McFadden</editor>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="2112" to="2245" />
		</imprint>
	</monogr>
	<note>Handbook of Econometrics, IV</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural autoregressive distribution estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7184" to="7220" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Entropy inference and the james-stein estimator, with application to nonlinear gene association networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v10/hausser09a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="1469" to="1484" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Causality detection based on information-theoretic approaches in time series analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hlaváčková-Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using transfer entropy to measure information flows between financial markets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dimpfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Nonlinear Dynamics and Econometrics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="102" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A scoring function for learning bayesian networks based on mutual information and conditional independence tests</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M D</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2149" to="2187" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Model-powered conditional independence test</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakkottai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2951" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.3775</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter singlecell data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bayesian networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ben-Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of statistics in quality and reliability</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Algorithms for discovery of multiple markov boundaries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Statnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Lytkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lemeire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="499" to="566" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Profiteering from the dotcom bubble, subprime crisis and asian financial crisis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Japanese Economic Review</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="279" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
