<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-27">27 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoming</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
							<email>weigao@pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-27">27 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">9F390A9A3BDE8EC93B4EBA1ACCF1A371</idno>
					<idno type="arXiv">arXiv:2405.17472v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-image diffusion models can be fine-tuned in custom domains to adapt to specific user preferences, but such adaptability has also been utilized for illegal purposes, such as forging public figures' portraits, duplicating copyrighted artworks and generating explicit contents. Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data. In this paper, we present FreezeAsGuard, a new technique that addresses these limitations and enables irreversible mitigation of illegal adaptations of diffusion models. Our approach is that the model publisher selectively freezes tensors in pre-trained diffusion models that are critical to illegal model adaptations, to mitigate the fine-tuned model's representation power in illegal adaptations, but minimize the impact on other legal adaptations. Experiment results in multiple text-to-image application domains show that FreezeAsGuard provides 37% stronger power in mitigating illegal model adaptations compared to competitive baselines, while incurring less than 5% impact on legal model adaptations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Existing work vs. FreezeAsGuard in mitigating malicious adaptation of diffusion models</head><p>Text-to-image diffusion models <ref type="bibr" target="#b28">[44,</ref><ref type="bibr" target="#b27">43]</ref> are powerful tools to generate high-quality images aligned with user prompts. After pre-trained by model publishers to embed world knowledge from large image data <ref type="bibr" target="#b33">[49]</ref>, open-sourced diffusion models, such as Stable Diffusion (SD) [9, 10], can be conveniently adapted by users to generate their preferred images<ref type="foot" target="#foot_0">foot_0</ref> , through fine-tuning with custom data in specific domains. For example, diffusion models can be fine-tuned on cartoon datasets to synthesize avatars in video games <ref type="bibr" target="#b30">[46]</ref>, or on datasets of landscape photos to generate wallpapers <ref type="bibr">[11]</ref>.</p><p>An increasing risk of democratizing open-sourced diffusion models, however, is that the capability of model adaptation has been utilized for illegal purposes, such as forging public figures' portraits <ref type="bibr" target="#b6">[22,</ref><ref type="bibr" target="#b8">24]</ref>, duplicating copyrighted artworks <ref type="bibr" target="#b10">[26]</ref>, and generating explicit content <ref type="bibr" target="#b9">[25]</ref>. Most existing efforts aim to deter at-</p><p>a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% IMMA UCE IMMA UCE ing FreezeAsGuard-30% IMMA UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of elizabeth warren which shows her speaking at a rally a photo of elizabeth warren which shows her standing in front of a classroom a photo of elizabeth warren which shows her sitting in a chair Training sample Full fine-tuning Prompt a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% IMMA UCE IMMA UCE Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of emma watson which shows her wearing a hat IMMA UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE ing FreezeAsGuard-30% Training sample Full fine-tuning Prompt a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% IMMA UCE IMMA UCE ing FreezeAsGuard-30% IMMA UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of elizabeth warren which shows her speaking at a rally a photo of elizabeth warren which shows her standing in front of a classroom a photo of elizabeth warren which shows her sitting in a chair Training sample Full fine-tuning Prompt a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% IMMA UCE IMMA UCE Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of emma watson which shows her wearing a hat IMMA UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress Figure <ref type="figure">2</ref>: FreezeAsGuard ensures that portraits (left) and artworks (right) generated by diffusion models in illegal classes cannot be recognizable as target objects, even if the model has been fine-tuned with data samples in illegal classes. In contrast, unlearning schemes (UCE <ref type="bibr" target="#b7">[23]</ref> and IMMA <ref type="bibr" target="#b49">[65]</ref>) cannot prevent the unlearned knowledge of illegal classes from being relearned in fine-tuning.</p><p>tempts of illegal model adaptation with copyright detection <ref type="bibr" target="#b48">[64,</ref><ref type="bibr">16,</ref><ref type="bibr">17]</ref>, which embeds invisible but detectable watermarks into training data and further generated images, as shown in Figure <ref type="figure">1</ref>. However, such detection only applies to misuse of training data, and does not mitigate the user's capability of illegal model adaptation. Users can easily bypass such detection by collecting and using their own training data without being watermarked (e.g., users' self-taken photos of public figures).</p><p>Instead, an intuitive approach to mitigation is content filtering. However, filtering user prompts <ref type="bibr" target="#b3">[19]</ref> can be bypassed by fine-tuning the model to align innocent prompts with illegal image contents <ref type="bibr" target="#b39">[55]</ref>, and filtering the generated images [7] is often overpowered with high false-positive rates <ref type="bibr" target="#b1">[3]</ref>. Data poisoning techniques can avoid false positives by injecting invisible perturbations into training data <ref type="bibr" target="#b43">[59,</ref><ref type="bibr" target="#b46">62,</ref><ref type="bibr" target="#b35">51]</ref>, but cannot apply when public web data or users' private data is used for fine-tuning.</p><p>Recent unlearning methods allow model publishers to remove knowledge needed for illegal adaptation by modifying model weights <ref type="bibr" target="#b4">[20,</ref><ref type="bibr" target="#b7">23,</ref><ref type="bibr" target="#b41">57,</ref><ref type="bibr" target="#b49">65]</ref> , but cannot prevent relearning such knowledge via fine-tuning.</p><p>The key limitation of these techniques is that they focus on modifying the training data or model weights, but such modification can be reversed by users via fine-tuning with their own data. Such modification, further, cannot restrain the mitigation power only in illegal data classes (e.g., public figures' portraits) without affecting model adaptation in other legal data classes (e.g., the user's own portraits), due to the high ambiguity and possible overlap between these classes.</p><p>To prevent users from reversing the mitigation maneuvers being applied, in this paper we present FreezeAsGuard, a new technique that constrains the trainability of diffusion model's tensors in finetuning. As shown in Figure <ref type="figure">1</ref>, the model publisher selectively freezes tensors in pre-trained models that are critical to fine-tuning in illegal classes (e.g., public figures' portraits), to limit the model's representation power of being fine-tuned in illegal classes. In practice, since most illegal users are not professional and fine-tune diffusion models by simply following the instructions provided by model publishers, tensor freezing can be effectively enforced by model publishers through these instructions, to guide the users to adopt tensor freezing. Essentially, since freezing tensors lowers the trainable model parameters and reduces the computing costs of fine-tuning, users would be well motivated to adopt tensor freezing in fine-tuning practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Learning</head><p>freeze tensors update mask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Mask learning and fine-tuning as a bilevel optimization</head><p>The major challenge is how to properly evaluate the importance of tensors in model fine-tuning. Popular attribution-based importance metrics <ref type="bibr" target="#b22">[38,</ref><ref type="bibr" target="#b25">41]</ref> are used in model pruning with fixed weight values, but cannot reflect the impact of weight variations in fine-tuning. Such impact of weight variations, in fact, cannot be condensed into a single importance metric, due to the randomness and interdependencies of weight updates in finetuning iterations.</p><p>Instead, as shown in Figure <ref type="figure">3</ref>, we formulate the selection of frozen tensors in all the illegal classes as one trainable binary mask. Given a required ratio of frozen tensors specified by model publisher, we optimize such selection with training data in all the involved illegal classes, through bilevel optimization that combines the iterative process of mask learning and iterations of model fine-tuning. In this way, the mask being trained can timely learn the impact of weight variations on the training loss during fine-tuning.</p><p>With frozen tensors, the model's representation power should be retained when fine-tuned on other legal classes (e.g., user's own portraits). Hence, we incorporate training samples from legal classes into the bilevel optimization, to provide suppressing signals for selecting tensors being frozen. Hence, the learned mask of freezing tensors should skip tensors that are important to fine-tuning in legal classes.</p><p>We evaluated FreezeAsGuard in three different domains of illegal model adaptations: 1) forging public figures' portraits, 2) duplicating copyrighted artworks and 3) generating explicit contents. For each domain, we use open-sourced or self-collected datasets, and randomly select different data classes as illegal and legal classes. We use competitive model unlearning schemes as baselines, and multiple metrics to measure image quality. Our findings are as follows:</p><p>• FreezeAsGuard has strong mitigation power in illegal classes. Compared to the competitive baselines, it further reduces the quality of images generated by fine-tuned model by up to 37%, and ensures the generated images to be unrecognizable as subjects in illegal classes.</p><p>• FreezeAsGuard has the minimum impact on modal adaptation in legal classes. It ensures on-par quality of the generated images compared to regular full fine-tuning on legal data, with a difference of at most 5%.</p><p>• FreezeAsGuard has high compute efficiency. Compared to full fine-tuning, it can save up to 48% GPU memory and 21% wall-clock computing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Motivation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fine-Tuning Diffusion Models</head><p>Given text prompts y and images x as training data, fine-tuning a diffusion model approximates the conditional distribution p(x|y) by learning to reconstruct images that are progressively blurred with noise ϵ over step t = 1, ..., T . Training objective is to minimize the reconstruction loss:</p><formula xml:id="formula_0">L θ = E x,y,ϵ∼N (0,1),t ∥ϵ -ϵ θ (E(x t ), t, τ (y))∥ 2 2 ,<label>(1)</label></formula><p>where E(•) is the encoder of a pretrained VAE, τ (•) is a pretrained text encoder, and ϵ θ (•) is a denoising model with trainable parameters θ. Most diffusion models adopt UNet architecture <ref type="bibr" target="#b29">[45]</ref> as the denoising model.</p><p>In fine-tuning, the diffusion model learns new knowledge by adapting the generic knowledge in the pre-trained model <ref type="bibr">[13]</ref>. For example, new knowledge about "a green beetle" can be a combination of generic knowledge on "hornet" and "emerald". This behavior implies that fine-tuning in different classes may share the same knowledge base, and it is challenging to focus the mitigation power in illegal classes without affecting fine-tuning in other legal classes. This challenge motivates us to regulate FreezeAsGuard's mitigation power by incorporating training samples in legal classes, when selecting tensors being frozen for illegal classes.</p><p>Model component Being frozen CLIP (↑) TOPIQ (↑) FID (↓) No freezing 31.93 0.054 202.18 Attention projectors 31.60 0.051 208.40 Conv. layers 31.54 0.047 206.58 Time embeddings 31.46 0.045 212.79 50% random weights (seed 1) 32.25 0.054 206.53 50% random weights (seed 2) 32.62 0.051 216.12 Table 1: Quality of generated images with different model compoents being frozen, using CLIP [27], TOPIQ [14], and FID [28] image quality metrics and the captioned pokemon dataset [6] 2.2 Partial Model Fine-tuning freezing cross-attention freezing convolution no freezing freezing random 50% (seed 1) freezing random 50% (seed 2) freezing time embedding An intuitive solution to mitigating illegal model adaptation is to only allow fine-tuning some layers or components of the diffusion model. However, this solution is ineffective in practice, because shallow layers provide primary image features and deep layers enforce domainspecific semantics <ref type="bibr" target="#b45">[61]</ref>. They are, hence, both essential to the performance of the fine-tuned models in legal classes. Similarly, as shown in Table <ref type="table">1</ref> and Figure <ref type="figure" target="#fig_1">4</ref>, freezing critical model components such as attention projectors and time embeddings can cause large quality drop in generated images. Even when freezing the same amount of model weights (e.g., random 50%), the exact distribution of frozen weights could also affect the generated images' quality. Such heterogeneity motivates us to instead seek for globally optimal selections of freezing tensors across all model components, by jointly taking all model components into bilevel optimization.</p><p>Illegal Class 𝑪𝑪 illegal Legal Class 𝑪𝑪 legal 𝒎𝒎 𝜽𝜽 pre 𝜽𝜽 ft 𝜽𝜽(𝒎𝒎) = 𝒎𝒎⨀𝜽𝜽 pre + 1 -𝒎𝒎 ⨀𝜽𝜽 ft 𝒙𝒙 ill𝐞𝐞𝐞𝐞𝐞𝐞𝐞𝐞 𝒙𝒙 legal 𝐿𝐿 𝜽𝜽(𝒎𝒎) (𝒙𝒙 illegal ) 𝐿𝐿 𝜽𝜽(𝒎𝒎) (𝒙𝒙 legal ) Sampled batches Mask Learning Loop: learn a mask to divert convergence on illegal class Fine-tuning Loss 1 -1 1 Simulated User Loop: finetune model towards convergence Mask Learning by Model Publishers Pretrained Diffusion Model 𝐿𝐿 sparsity m 1 1 0 1 0 Mask for tensor freezing Fine-tuning by Users finetune Illegal Class Legal Class Low Quality High Quality freeze 1 Diffusion Model with Partial Freezing Our design of FreezeAsGuard builds on bilevel optimization, which embeds one optimization problem within another and both of them are multi-objective optimizations [15, <ref type="bibr" target="#b24">40,</ref><ref type="bibr" target="#b5">21]</ref>. This bilevel optimization can be formulated as</p><formula xml:id="formula_1">m * = arg min m -L θ * (m) (x illegal ), L θ * (m) (x legal )<label>(2)</label></formula><formula xml:id="formula_2">s.t. θ * (m) = arg min θ(m) L θ(m) (x illegal ), L θ(m) (x legal ) ,<label>(3)</label></formula><p>where m is the binary mask of selecting frozen tensors, m * is the optimized binary mask, θ(m) represents the model tensors frozen by m, and θ * (m) is the converged θ(m) after fine-tuning. x illegal and x legal denote training samples in all the illegal classes (C illegal ) and legal classes (C legal ), respectively. Such bilevel optimization is illustrated in Figure <ref type="figure" target="#fig_2">5</ref>. The lower-level problem in Eq. ( <ref type="formula" target="#formula_2">3</ref>) is a simulated user loop that the user fine-tunes the diffusion model by minimizing the loss over both illegal and legal classes. The upper-level problem in Eq. ( <ref type="formula" target="#formula_1">2</ref>) is a mask learning loop that learns m to mitigate the model's representation power when fine-tuned in illegal classes, without affecting fine-tuning in legal classes. We use the standard diffusion loss in Eq. (1) and adopt tensor-level freezing to ensure sufficient granularity<ref type="foot" target="#foot_1">foot_1</ref> , without incurring extra computing costs.</p><p>To apply the gradient solver, m and θ(m) should have differentiable dependencies with the loss function. We model θ(m) through the weighted summation of pre-trained model tensors θ pre and fine-tuned model tensors θ f t , such that</p><formula xml:id="formula_3">θ(m) = m ⊙ θ pre + (1 -m) ⊙ θ f t ,<label>(4)</label></formula><p>where ⊙ denotes element-wise multiplication. From the user's perspective, fine-tuning the partially frozen model θ(m) is equivalent to fine-tuning θ f t , controlled by Eq. (3). To improve compute efficiency, we initialize θ f t as the fully fine-tuned model tensors on both illegal and legal classes, and gradually enlarge the scope of tensor freezing. Since m is discrete and not differentiable, we adopt a continuous form m(w) = σ(w/T ) that applies sigmoid function σ(•) over a trainable tensor w. We also did code optimizations for vectorized gradient calculations as in Appendix A.</p><p>Note that, although we made m differentiable in bilevel optimizations, the optimized values in m * will be rounded to binary, to ensure complete freezing of selected tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mask Learning in the Upper-level Loop</head><p>To solve the upper-level optimization in Eq. ( <ref type="formula" target="#formula_1">2</ref>), we adopt linear scalarization <ref type="bibr" target="#b13">[29]</ref> to convert it into a single objective L upper via a weighted summation with weights (λ 1 , λ 2 ):</p><formula xml:id="formula_4">L upper = -λ 1 L θ * (m) (x illegal ) + λ 2 L θ * (m) (x legal ),<label>(5)</label></formula><p>to involve training samples in both illegal and legal classes when learning m. (λ 1 , λ 2 ) should ensure that gradient-based feedbacks from the two loss terms are not biased by inequality between the amounts of x illegal and x legal , and their values should be proportionally set based on these amounts.</p><p>Besides, x illegal and x legal could contain some knowledge in common, and masked learning from such data may hence affect model adaptation in legal classes. To address this problem, we add a sparsity constraint L sparsity to L upper to better control of the mask's mitigation power:</p><formula xml:id="formula_5">L sparsity = ∥1 ⊤ m/N -ρ∥ 2 2 , (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where N is the number of tensors and 1 ⊤ m/N measures the proportion of tensors being frozen. By minimizing L sparsity , the achieved ratio of tensor freezing should approach the given ρ. In this way, we can apply gradient descent to minimize L upper and iteratively refine m towards optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Fine-tuning in the Lower-level Loop</head><p>Effectiveness of mask learning at the upper level relies on timely feedback from the lower-level fine-tuning. Every time the mask has been updated by an iteration in the upper level, the lower-level loop should adopt the updated mask into fine-tuning, and return the fine-tuned model tensors and the correspondingly updated loss value as feedback to the upper level. Similar to Eq. ( <ref type="formula" target="#formula_4">5</ref>), the fine-tuning objective is the summation of diffusion losses for illegal and legal domains:</p><formula xml:id="formula_7">L lower = L θ * (m) (x illegal ) + L θ * (m) (x legal ).<label>(7)</label></formula><p>𝐿𝐿(𝑚𝑚 1 , 𝑡𝑡) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Towards Efficient Bilevel Optimization</head><p>Solving bilevel optimization is computationally expensive, due to the repeated switches between upper-level and lower-level loops <ref type="bibr" target="#b31">[47,</ref><ref type="bibr" target="#b49">65]</ref>. Rigorously, as shown in Figure <ref type="figure" target="#fig_3">6</ref> -Left, every time when the mask has been updated, the model should be fine-tuned with a sufficient number of iterations until convergence, before the next update of the mask. However, in practice, doing so is extremely expensive.</p><p>Instead, as shown in Figure <ref type="figure" target="#fig_3">6</ref> -Right, we observe that the fine-tuning loss typically drops fast in the first few iterations and then violently fluctuates (see Appendix B). Hence, every time in the lower-level loop of model fine-tuning, we do not wait for the loss to converge, but only fine-tune the model for the first few iterations before updating the mask to the upper-level loop of mask learning. After the model update, the fine-tuned model weights are inherited to the next loop of model fine-tuning, to ensure consistency and improve convergence. Hence, the optimization only needs one fine-tuning process, during which the mask can be updated with shorter intervals but higher learning quality. Details of deciding such a number of iterations are in Appendix B.</p><p>Further, to perform bilevel optimizations, three versions of diffusion model weights, i.e., θ(m), θ pre and θ f t , will be maintained for gradient computation. This could significantly increase the memory cost due to large sizes of diffusion models. To reduce such memory cost, we instead maintain only two versions of model weights, namely θ(m) and θ d = θ pre -θ f t . According to Appendix A, the involvement of both θ pre and θ f t can be removed by plugging θ d into the gradient descent calculation. More specifically, for a given model tensor i, the gradient descent to update the corresponding mask m i in the upper-level optimization is:</p><formula xml:id="formula_8">w i ← w i -η 1 ∂L upper ∂θ(m) i , θ (i) d 1 T σ w i T σ 1 - w i T ,<label>(8)</label></formula><p>where η 1 controls the step size of updates and m i is updated as σ • w i /T . Further, computing the update of θ(m) and θ d at the lower level should apply the chain rule:</p><formula xml:id="formula_9">θ (i) d ← θ (i) d + η 2 ∂L lower ∂θ(m) i (1 -m i ) (9) θ(m) i ← θ(m) i -η 2 ∂L lower ∂θ(m) i (1 -m i ) 2 . (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>In this way, as shown in Algorithm 1, FreezeAsGuard alternately runs upper and lower-level gradient descent steps, with the maximum compute efficiency and the minimum memory cost. We initialize the mask to all zeros and θ(m) starts as a fully fine-tuned model, to mitigate aggressive freezing. In practice, we set random negative values to w to ensure the continuous form of the mask is near zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Freezing Strategy in FreezeAsGuard</head><p>Require: Illegal and legal class data (C illegal , C legal ), step size η1 and η2, model weights θpre and θ f t 1:</p><formula xml:id="formula_11">θ d ← θpre -θ f t , m ← 0, θ(m) ← θ f t 2: for k = 1, ..., K do 3: for l = 1, ..., L do 4: (x illegal , x legal ) ← Sample(C illegal , C legal ) 5: ∂L lower ∂θ(m) ← Backprop(x illegal , x legal , L lower , θ(m)) 6: (θ d , θ(m)) ← Update ∂L lower ∂θ(m) , m, θ d , θ(m)</formula><p>// Refer to Eq. ( <ref type="formula">9</ref>) and (10) 7:</p><p>end for 8:</p><formula xml:id="formula_12">(x illegal , x legal ) ← Sample(C illegal , C legal ) 9: ∂Lupper ∂θ(m) ← Backprop(x illegal , x legal , Lupper, θ(m)) 10: m ← Update ∂Lupper ∂θ(m) , m, θ d , η2</formula><p>// Refer to Eq. ( <ref type="formula" target="#formula_8">8</ref>)</p><formula xml:id="formula_13">11: end for ⇒ Return Round(m)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In our experiments, we use three open-source diffusion models, SD v1.4 [8], v1.5 [9] and v2.1 [10], to evaluate three domains of illegal model adaptations: 1) forging public figures' portraits <ref type="bibr" target="#b6">[22,</ref><ref type="bibr" target="#b8">24]</ref>, 2) duplicating copyrighted artworks <ref type="bibr" target="#b10">[26]</ref> and 3) generating explicit content <ref type="bibr" target="#b9">[25]</ref>.</p><p>Datasets: For each domain, we use datasets as listed below, and random select different data classes as illegal and legal classes. We use 50% of samples in the selected classes for mask learning and model training, and the other samples for testing. More details about datasets are in Appendix C.</p><p>• Portraits of public figures: We use a self-collected dataset, namely Famous-Figures-25 (FF25), with 8,703 publicly available portraits of 25 public figures on the Web. Each image has a prompt "a photo of &lt;person_name&gt; showing &lt;content&gt;" as description.</p><p>• Copyrighted artworks: We use a self-collected dataset, namely Artwork, which contains 1,134 publicly available artwork images and text captions on the Web, from five famous digital artists with unique art styles.</p><p>• Explicit contents: We use the NSFW-caption dataset with 2,000 not-safe-for-work (NSFW) images and their captions [1] as the illegal class. We use the Modern-Logo-v4 [5] dataset, which contains 803 logo images labeled with informative text descriptions, as the legal class.</p><p>Baseline schemes: Our baselines include full fine-tuning (FT), random tensor freezing, and two competitive unlearning schemes, namely UCE [23 and IMMA <ref type="bibr" target="#b49">[65]</ref>. Existing data poisoning methods <ref type="bibr" target="#b43">[59,</ref><ref type="bibr" target="#b46">62,</ref><ref type="bibr" target="#b35">51]</ref> cannot be used because all data we use is publicly online and cannot be poisoned.</p><p>• Full FT: It fine-tunes all the tensors of the diffusion model's UNet and has the strongest representation power for adaptation in illegal domains.</p><p>• Random-ρ: It randomly freezes ρ% of model tensors, as a naive baseline of tensor freezing.</p><p>• UCE <ref type="bibr" target="#b7">[23]</ref>: It uses unlearning to guide the learned knowledge about illegal classes in the pre-trained model to be irrelevant or more generic.</p><p>• IMMA <ref type="bibr" target="#b49">[65]</ref>: It reinitializes the model weights so that it is hard for users to conduct effective fine-tuning on the reinitialized model, in both illegal and legal classes.</p><p>Measuring image quality: We used FID <ref type="bibr" target="#b12">[28]</ref> and CLIP <ref type="bibr" target="#b11">[27]</ref> scores to evaluate the quality of generated images. In addition, to better identify domain-specific details in generated images, we also adopted domain-specific image quality metrics, listed as below and described in detail in Appendix D. For each text prompt, the experiment results are averaged from 100 generated images with different random seeds.</p><p>• Domain-specific feature extractors: Existing work <ref type="bibr" target="#b38">[54]</ref> reported that FID and CLIP fail to measure the similarity between portraits of human subjects, and cannot reflect human perception in images. Hence, for human portraits and artworks, we apply specific feature extractors on real and generated images, and measure the quality of generated images as cosine distance between their feature vectors. For portraits, we use face feature extractors (FN-L, FN, VGG) in DeepFace <ref type="bibr" target="#b34">[50]</ref>. For artworks, we use a pretrained CSD model <ref type="bibr" target="#b36">[52]</ref>. Details are in Appendix D.1.</p><p>• NudeNet: We used NudeNet <ref type="bibr" target="#b0">[2]</ref> to decide the probability of whether the generated images contain explicit contents, as the image's safety score. Details are in Appendix D.2.</p><p>• Human Evaluation: To better capture human perception in generated images, we recruited 16 volunteers with diverse backgrounds to provide human evaluations on image quality. For each image, volunteers scored how the generated image is likely to depict the same subject as in the real image from 1 to 7, where 1 means "very unlikely" and 7 means "very likely". Details are in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mitigating Forgery of Public Figures' Portraits</head><p>We evaluate FreezeAsGuard in mitigating forgery of public figures' portraits, using FF25 dataset and SD v1.5 model. 10 classes are randomly selected from FF25 as illegal and legal classes, respectively. As shown in Table <ref type="table">2</ref>, FreezeAsGuard can mitigate illegal model adaptation by 40% compared to Full FT. When ρ varies from 10% to 50%, it also outperforms the unlearning schemes by 37%, because these schemes cannot prevent relearning knowledge in illegal classes with new training data. It also ensures better legal model adaptation. With ρ=30%, the impact on legal adaptation is &lt;5%.</p><p>When the freezing ratio (ρ) increases, the difference between FreezeAsGuard and random freezing diminishes, and their mitigation powers also reach a similar level. This means that only a portion of tensors are important for adaptation in specific illegal classes. With a high freezing ratio, random freezing is more likely to freeze these important tensors. Meanwhile, it could also freeze tensors that are important to legal classes, resulting in low performance in legal model adaptations. Hence, as shown in Figure <ref type="figure">7</ref>, when ρ=30%, the mitigation power is high enough that the generated images no longer resemble those in training data, and further increasing ρ could largely affect legal model adaptation.</p><p>Based on these results, we empirically consider ρ=30% as the optimal freezing ratio on SD v1.5 for the domain of public figures' portraits. Figure <ref type="figure" target="#fig_4">8</ref> shows example images of baseline methods and</p><p>Metric FN-L(↓) FN(↓) VGG(↓) FID(↓) Human (↓) Pre-trained model 0.96 0.92 0.93 164.8 -Full FT illegal 0.436 0.455 0.581 144.6 6.7 legal 0.436 0.455 0.581 144.6 6.7 UCE illegal 0.445 0.464 0.598 152.9 4.6 legal 0.442 0.465 0.583 151.4 5.4 IMMA illegal 0.467 0.493 0.624 148.8 5.1 legal 0.462 0.475 0.610 145.9 5.8 FG-10% illegal 0.441 0.451 0.603 148.0 4.9 legal 0.429 0.45 0.585 143.6 6.2 R-10% illegal 0.433 0.451 0.588 143.7 6.8 legal 0.431 0.457 0.582 144.0 6.8 FG-30% illegal 0.482 0.504 0.631 153.7 3.6 legal 0.449 0.478 0.590 146.7 6.0 R-30% illegal 0.429 0.456 0.590 145.0 5.9 legal 0.429 0.456 0.590 145.0 5.9 FG-50% illegal 0.530 0.638 0.647 155.5 2.1 legal 0.499 0.527 0.608 149.5 4.3 R-50% illegal 0.513 0.543 0.638 151.6 3.7 legal 0.512 0.522 0.632 153.2 3.7 Table 2: Mitigation power in 10 illegal classes and 10 legal classes from the FF25 dataset, where worse image quality indicates stronger mitigation power. FG-ρ% means using FreezeAsGuard to freeze ρ% tensors and R-ρ% means random freezing. Prompt Training sample Full fine-tuning FG-10% FG-30% FG-50% a photo of angela merkel which shows her smiling a photo of angela merkel which shows her looking at the camera a photo of angela merkel which shows her walking down a red carpet Figure 7: Examples of public figures' portraits generated by FreezeAsGuard under different freezing ratios (ρ) a photo of elizabeth warren which shows her speaking at a rally a photo of donald trump which shows him making a fist Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of emma watson which shows her wearing a hat IMMA UCE Training sample Full Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% Training sample Full fine-tuning Prompt a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% UCE IMMA UCE FreezeAsGuard-30% UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE a photo of elizabeth warren which shows her speaking at a rally a photo of donald trump which shows him making a fist Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of emma watson which shows her wearing a hat IMMA UCE Training sample Full Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress Prompt Training image Full fine-tuning IMMA UCE FG-30% a photo of emma watson which shows her wearing a hat a photo of halle berry which shows her posing in a gold dress </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mitigating Duplication of Copyright Artworks</head><p>We evaluate the capability of FreezeAsGuard in mitigating the duplication of copyrighted artworks, using the Artwork dataset and SD v2.1 model. One artist is randomly selected as the illegal class and the legal class, respectively.</p><p>The results with different freezing ratios are shown in Table <ref type="table" target="#tab_7">3</ref> and Figure <ref type="figure">9</ref>. Unlike results in Section 4.1 where data classes exhibit only subtle differences in facial features, different artists' artworks demonstrate markedly different styles. Hence, a higher freezing ratio is required for sufficient mitigation power. We empirically decide the optimal freezing ratio for the domain of artwork is 70%. When ρ=70%, FreezeAsGuard can provide 47% more mitigation power in illegal classes compared to full fine-tuning, and 30% more compared to unlearning schemes. Figure <ref type="figure" target="#fig_5">10</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mitigating Generation of Explicit Contents</head><p>To evaluate FreezeAsGuard's mitigation of explicit contents, we designate the NSFW-caption dataset as illegal class, and the Modern-Logo-v4 dataset as legal class. Results in Table <ref type="table">4</ref> and Figure <ref type="figure" target="#fig_6">11</ref> show that, with ρ=70%, FreezeAsGuard significantly reduces the model's capability of generating explicit contents by up to 38% compared to unlearning schemes, while maintaining the model's adaptability in legal class. More image examples are in Appendix F.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Scalability of Mitigation Power</head><p>To evaluate FreezeAsGuard's scalability over multiple illegal classes, we randomly pick 2, 5 and 10 public figures in the FF25 dataset, and 1, 2 and 3 artists in the Artworks dataset, as illegal classes. As shown in Table <ref type="table" target="#tab_10">5</ref> and 6, when the number of illegal classes increases, FreezeAsGuard can retain strong mitigation power in both cases, and continuously outperforms the unlearning schemes. Note that, with more illegal classes, the difference of mitigation power between FreezeAsGuard and random freezing is smaller, because more illegal classes correspond to more adaptation-critical tensors, and random freezing is more likely to cover them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Learned Selection of Frozen Tensors</head><p>In Figure <ref type="figure">12</ref> and 13, we visualized the learned binary masks of tensor freezing for different illegal classes on the FF-25 and Artwork datasets, respectively, with the SD v1.5 model. These results show that on both datasets, the tensors being frozen for different illegal classes largely vary, indicating that</p><p>Prompt Training sample Full fine-tuning FG-30% FG-50% FG-70% FG-85% there is a tiger and a dragon with a kite in the sky illustration of a man working on a machine with a dog nearby cartoon illustration of a man eating a hot dog in a crowded city Figure 9: Examples of artwork images generated by FreezeAsGuard with different freezing ratios Method Illegal Legal NudeNet(↑) FID(↓) CLIP(↑) Pre-trained model 0.47 --Full FT 1.29 158.1 32.79 UCE 1.20 158.5 30.07 IMMA 1.17 162.0 28.71 FG-30% 1.27 159.5 32.50 R-30% 1.30 158.8 32.79 FG-50% 1.06 163.2 31.83 R-50% 1.20 160.6 30.43 FG-70% 0.87 166.1 31.56 R-70% 1.12 161.8 28.66 FG-85% 0.85 166.5 30.34 R-85% 0.93 164.6 30.81 Table 4: Mitigation power in illegal class (NSFW-caption dataset) and legal class (Modern-Logo-v4 dataset), where worse image quality (in FID or CLIP) or lower NudeNet score indicates stronger mitigation power. FG-ρ% means using FreezeAsGuard to freeze ρ% tensors and R-ρ% means random freezing.</p><p>our mask learning method can properly capture the unique tensors that are critical to each class, hence ensuring scalability. Note that in practice, no matter how many illegal classes are involved, the total amount of frozen tensors will always be constrained by the freezing ratio (ρ). When more illegal classes are involved, our results show that FreezeAsGuard is capable of identifying the most critical set of tensors for mitigating the fine-tuned model's representation power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Mitigation Power with Different Models</head><p>As shown in Table <ref type="table" target="#tab_12">7</ref>, when applied to different SD models, FreezeAsGuard constantly outperforms baseline schemes. SD v1.4 and v1.5 are generally stronger than SD v2.1, and the gap between illegal and legal classes in FreezeAsGuard is slightly better for v1.4 and v1.5 models. We hypothesize that better pre-trained models have more modularized knowledge distribution over model parameters, and hence allow FreezeAsGuard to have less impact on legal classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Reduction of Computing Costs</head><p>One advantage of freezing tensors is that it reduces the computing costs of fine-tuning. As shown in Table <ref type="table">8</ref>, when fine-tuning the model on a A6000 GPU, by applying FreezeAsGuard's selection of tensor freezing, users can save 22%-48% GPU memory and 13%-21% wall-clock computing time, A serene and intimate moment, a nude woman lies on a plush bed, her slender body glistening in the soft light of the room. Her hair cascades down her back as she gazes up at the camera, her piercing brown eyes inviting and vulnerable.</p><p>A beautiful, topless young woman stands confidently in front of a backdrop, her long, honey-blonde hair cascading down her back. Her bright blue eyes sparkle with a warm smile, highlighting her smooth, sun-tanned skin and delicate features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion &amp; Broader Impact</head><p>In this paper, we present FreezeAsGuard, a new technique for mitigating illegal adaptation of diffusion models by freezing model tensors that are adaptation-critical only for illegal classes. FreezeAsGuard largely outperforms existing model unlearning schemes. Our rationale for tensor freezing is generic and can be applied to other large generative models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Vectorizing the Gradient Calculations in Bilevel Optimization</head><p>In practice, the solutions to bilevel optimization in Eq. ( <ref type="formula" target="#formula_1">2</ref>) and Eq. ( <ref type="formula" target="#formula_2">3</ref>) can usually be approximated through gradient-based optimizers. However, existing deep learning APIs (e.g., TensorFlow and PyTorch) maintain model tensors in either list or dictionary-like structures, and hence the gradient calculation for Eq. ( <ref type="formula" target="#formula_3">4</ref>) cannot be automatically vectorized with the mask vector m. To enhance the compute efficiency, we decompose the process of gradient calculation and assign the majority of compute workload to the highly optimized APIs.</p><p>Specifically, in mask learning in the upper-level loop specified in Eq. ( <ref type="formula" target="#formula_4">5</ref>), L upper 's gradient w.r.t a model tensor's w i can be decomposed via the chain rule as:</p><formula xml:id="formula_14">∂L upper ∂w i = ∂L upper ∂θ(m) i , ∂θ(m) i m i ∂m i ∂w i (11) = ∂L upper ∂θ(m) i , θ (i) pre -θ (i) f t 1 T σ w i T σ 1 - w i T ,<label>(12)</label></formula><p>where &lt; •, • &gt; denotes the inner product. The calculation of the gradient component, i.e., ∂L upper /∂θ(m) i , is then done by automatic differentiation APIs, because it is equivalent to standard backpropagation in diffusion model training. The other calculations are implemented by traversing over the list of model tensors.</p><p>Similarly, when fine-tuning the model tensors θ(m) in the lower-level loop specified in Eq. ( <ref type="formula" target="#formula_7">7</ref>), we also decompose its gradient calculation process. In particular, fine-tuning θ(m) is equivalent to fine-tuning θ f t , and the gradient descent is hence to update θ f t . More specifically, the gradient of a given tensor i is:</p><formula xml:id="formula_15">∂L lower ∂θ (i) f t = ∂L lower ∂θ(m) i ∂θ(m) i ∂θ (i) f t = ∂L lower ∂θ(m) i (1 -m i ),<label>(13)</label></formula><p>where we leave ∂L lower /∂θ In addition, computing gradients over large diffusion models is expensive when using automatic differentiation in existing deep learning APIs (e.g., PyTorch and TensorFlow). Instead, we apply code optimization in the backpropagation path of fine-tuning, to reuse the intermediate gradient results and hence reduce the peak memory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Deciding the Number of Fine-tuning Iterations in Bilevel Optimization</head><p>As shown in Figure <ref type="figure" target="#fig_9">14</ref>, we observe that in the lower-level loop of model fine-tuning, the fine-tuning loss typically drops fast in the first 5-10 iterations, but then starts to violently fluctuate. Such quick drop of loss at the initial stage of fine-tuning is particularly common in fine-tuning large generative models, because the difference between the fine-tuned and pre-trained weights can be so small that only a few weight updates can get close <ref type="bibr" target="#b44">[60]</ref>. The violent fluctuation afterwards, on the other hand, exhibits &gt;60% of loss value changes, which indicates that the loss plateau is very unsmooth although the model can quickly enter it.</p><p>Since the first few iterations contribute to most of the loss reduction during fine-tuning, we believe that the model weights have already been very close to those in the completely fine-tuned model. In that case, we do not wait for the fine-tuning loss to converge, but instead only fine-tune the model for the first 10 iterations before updating the mask to the upper-level loop of mask learning. In practice, the model publisher can still adopt large numbers of fine-tuning iterations as necessary, depending on the availability of computing resources and the specific requirements of mitigating illegal domain adaptations. Similar approximation schemes are also adopted in existing work <ref type="bibr" target="#b31">[47,</ref><ref type="bibr" target="#b49">65]</ref> to solve bilevel optimization problems, but most of them aggressively set the interval to be only one iteration, leading to arguably high approximation errors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Datasets</head><p>The Famous-Figures-25 (FF25) Dataset: Our FF25 dataset contains 8,703 portrait images of 25 public figures and the corresponding text descriptions. These 25 subjects include politicians, movie stars, writers, athletes and businessmen, with diverse genders, races, and career domains. As shown in Figure <ref type="figure" target="#fig_10">15</ref>, the dataset contains 400-1,300 images of each subject.</p><p>All the images were crawled from publicly available sources on the Web, using the AutoCrawler tool <ref type="bibr" target="#b2">[4]</ref>. We only consider images that 1) has a resolution higher than 512×512 and 2) contains &gt;3 faces detected by OpenCV face recognition API [12] as valid. Each raw image is then center-cropped to a resolution of 512×512. For each image, we use a pre-trained BLIP2 image captioning model <ref type="bibr" target="#b23">[39]</ref> to generate the corresponding text description, and prompt BLIP2 with the input of "a photo of &lt;person_name&gt; which shows" to avoid hallucination. For example, "a photo of Cristiano Ronaldo which shows", when being provided to the BLIP2 model as input, could result in text description of "a photo of Cristiano Ronaldo which shows him smiling in a hotel hallway". We empirically find that adopting this input structure to the BLIP2 model produces much fewer irrelevant captions. More sample images and their corresponding text descriptions are shown in Figure <ref type="figure" target="#fig_11">16</ref>.</p><p>The Artwork Dataset: We selected five renowned digital artists, each of which has a unique art style, and manually downloaded 100-300 representative images from their Instagram accounts. The total amount of images in the dataset is hence 1,134. We then used a pre-trained BLIP2 image captioning model <ref type="bibr" target="#b23">[39]</ref> to generate text prompts for each image. In Figure <ref type="figure" target="#fig_13">17</ref>, we show a sample image and its text prompt for each artist.</p><p>The NSFW-Caption Dataset: This dataset contains 2,000 NSFW images collected from MetArt, and each image has a very detailed caption, as shown in Figure <ref type="figure" target="#fig_14">18</ref>.  In general, we measure the quality of images generated by the fine-tuned diffusion model by comparing their similarity with the original training images used to fine-tune the diffusion model. Most commonly used image similarity metrics, such as FID <ref type="bibr" target="#b12">[28]</ref>, LPIPS <ref type="bibr" target="#b15">[31]</ref> and CLIP score <ref type="bibr" target="#b11">[27]</ref>, compute the similarity between the distributions of the extracted features from the generated and original images <ref type="bibr" target="#b27">[43,</ref><ref type="bibr" target="#b11">27]</ref>. The feature vectors are obtained using image feature extractors like the Inception model <ref type="bibr" target="#b12">[28]</ref>. They often perform reasonably well in measuring similarity between images of common objects, such as those included in the ImageNet data samples <ref type="bibr" target="#b32">[48]</ref>.</p><p>However, existing studies find that these metrics cannot reliably measure the similarity between very similar subjects, such as human faces of different human subjects or artworks in different art styles <ref type="bibr" target="#b14">[30,</ref><ref type="bibr" target="#b38">54]</ref>. In practice, we observe that the measured image quality by these metrics could even contradict human perception. For example, as shown in Figure <ref type="figure" target="#fig_16">20</ref>, while images generated with FreezeAsGuard are significantly lower in quality and differ more from the training images from a human perspective, the LPIPS scores of images generated by the fully fine-tuned model (without applying FreezeAsGuard) are similar to ours, even though they look quite different visually.</p><p>Therefore, to address the limitations of these generic image quality metrics, as described in the paper, we use domain-specific feature extractors to obtain features from the training and generated images, then compute the cosine distance between the feature vectors as the final measure of the generated images' quality. For human faces, we select three top feature extractors, namely FaceNet-512 (FN-L), FaceNet (FN), and VGG-Face (VGG), as provided in the DeepFace package <ref type="bibr" target="#b34">[50]</ref>. For art styles in artworks, we use a pretrained CSD model from <ref type="bibr" target="#b36">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 NudeNet score</head><p>We use a NSFW detector, namely NudeNet <ref type="bibr" target="#b0">[2]</ref>, to decide if the generated images contain any explicit content. For an input image, NudeNet can output a list of detected human body parts (such as ANUS_EXPOSED and FACE_FEMALE), along with the corresponding probabilities of these body parts' appearances in the image. We sum all these probabilities together as the NudeNet score of the image, with a lower score indicating a lower probability of containing explicit content.</p><p>The full list of the detectable human body parts is as follows: FEMALE_GENITALIA_COVERED,FACE_FEMALE, BUTTOCKS_EXPOSED,FEMALE_BREAST_EXPOSED, arafed image of a man walking past a giant brain sculpture dystopian themes, vivid imagery Beeple Artist art style example image prompt a close up of a cartoon city with a lot of buildings fantasy, surrealism, and pop culture David Sossella a group of people standing around a group of monsters hyper-realistic style with traditional colored-pencil aesthetics Kyle Lambert arafed poster of a woman smoking a cigarette with a border line sign pop culture references with old comic book aesthetics Butcher Billy a close up of a painting of a woman with red hair hyper-realistic portraits, imaginative character designs Mandy Jurgens </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Details of Human Evaluations</head><p>Our human evaluation involves 16 participants of college students. These participants ranged in age from 19 to 28, with 14 identifying as male and 2 as female. We conduct our human evaluation by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Prompt</head><p>A serene and intimate moment on a plush bed, a young woman with a warm smile lies on a bed of white sheets, her long auburn hair cascading down her back. She's wrapped in a delicate pink skirt, but it's clear she's not wearing much underneath. Her piercing blue eyes sparkle as she glances up at the camera, a hint of playfulness in her expression. The soft glow of the bedside lamp casts a warm ambiance, making this quiet moment feel both private and inviting. Figure <ref type="figure" target="#fig_18">21</ref> shows an example of such a set of images in the questionnaire. The questionnaire contains a total number of 220 sets of images for participants to rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details of Evaluation Setup</head><p>For each illegal class and legal class in FF25 and the artwork dataset, we generally select 100 images in each class for mask learning, but if the number of images in the class is smaller than 150, we select half of the images for mask learning. For explicit content generation, we use 500 images from legal and illegal class, separately, for mask learning, and the remaining data samples in the dataset are used for illegal model fine-tuning. Note that, to mitigate model adaptation in specific illegal classes, we will need to use data samples in the same class for mask learning. However, in our evaluations, the set of data samples used for mask learning and the set of data samples used for illegal model fine-tuning never have any overlap. For example, to mitigate the fine-tuned model's capability of generating portrait images of Barack Obama, we will use a set of portrait images of Barack Obama to learn the mask for tensor freezing. Then, another set of Barack Obama's portrait images are used to emulate illegal users' fine-tuning the diffusion model, and FreezeAsGuard's performance of mitigating illegal model adaptation is then evaluated by the quality of images generated by the fine-tuned model regarding this subject.</p><p>For mask learning, we set the gradient step size to 10, the simulated user learning rate to 1e-5, and iterate sufficient steps with the batch size of 16. The temperature for the mask's continuous form is set to 0.2, which we empirically find to ensure sufficient sharpness without impairing trainability. When fine-tuning the diffusion model as an illegal user, we adopt a learning rate of 1e-5 and the batch size of 4 with Adam <ref type="bibr" target="#b20">[36]</ref> optimizer. For FF25 and artwork datasets, we fine-tune 2,000 iterations on illegal user's data samples. And for explicit content, since the pre-trained diffusion model has little knowledge about the explicit contents, we fine-tune 5,000 iterations to ensure the quality of generated images. Following the standard sampling setting of diffusion models, the loss is only calculated from a random denoising step during fine-tuning for every iteration, to ensure training efficiency. For image generation, we adopt the PNDMScheduler <ref type="bibr" target="#b17">[33]</ref> and proceed with 50 denoising steps to ensure sufficient image quality.</p><p>a logo of electronic online shop, shopping bag shown.</p><p>On it in the middle is the mouse arrow pointing to the upper left corner, indianred background, darkorchid, midnightblue foreground, minimalism, modern a logo of asian cafe restaurant bar with a bowl of soup with noodles, noodles are taken by the chinese chopsticks, all of it in the square, darkslategray background, sandybrown, bisque foreground, minimalism, modern a logo of coffee shop, White round background with black rim, cup, pretzel, horizontal stripe and cookery lettering, tan background, snow, darkslategray foreground, minimalism, modern  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Generation of Explicit Contents</head><p>As shown in Figure <ref type="figure" target="#fig_21">24</ref>, the generated images with FreezeAsGuard can effectively avoid explicit contents from being shown in different ways. In rows 4 and 5, the human subjects in images generated with FreezeAsGuard are all clothed. In Rows 1, 2 and 3, the image is zoomed in to prevent explicit content from being shown. In Row 6, the image quality is degraded so that no recognizable human appears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Ethical Issues of Using the Public Portrait Images and Artwork Images</head><p>In this section, we affirm that the use of our self-collected public portrait images and artwork image dataset does not raise ethical issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Image Source</head><p>For the FF-25 dataset, we use the Google images search API to crawl the images from the Web. Since the crawled images are from a large collection of websites, we cannot list all the websites here or associate each image with the corresponding website. However, we can confirm that the majority of websites from which images are crawled allow non-restricted non-commercial use, i.e., the CC NC or CC BY-NC license. Some examples of these websites are listed as follows:</p><p>• Wikipedia.org  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Image Usage</head><p>Our collection and use of these images are strictly limited to non-commercial research use, and these images will only be released to a small group of professional audience (i.e., CVPR reviewers) instead of the wide public. Hence, our use complies with the fair use policy of copyrighted images, which allows researchers to use copyrighted images for non-commercial research purpose without the permission from copyright owners. More information about such policy can be found at most university's libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Use Policy in the Research Community</head><p>We noticed that such fair use policy mentioned before has been widely applied in the research community to allow usage of copyrighted images of public figures' portraits and artworks for research purposes. For example, many datasets of celebrities' portraits such as CelebA <ref type="bibr" target="#b26">[42]</ref>, PubFig <ref type="bibr" target="#b21">[37]</ref> and MillionCelebs <ref type="bibr" target="#b47">[63]</ref>) and artwork such as Wikiart <ref type="bibr" target="#b37">[53]</ref> and LION <ref type="bibr" target="#b33">[49]</ref> are publicly available online. These datasets have been also used in a large quantity of research papers published at AI, ML and CV conferences. For examples: <ref type="bibr" target="#b19">[35,</ref><ref type="bibr">18]</ref> used the CelebA dataset, <ref type="bibr" target="#b16">[32,</ref><ref type="bibr" target="#b18">34]</ref> used the PubFig dataset and <ref type="bibr" target="#b42">[58]</ref> use the WikiArt dataset.  x A young woman with a warm smile lies on a bed of white sheets, her long auburan hair cascading down her back. She's wrapped in a delicate pink skirt, but it's clear she's not wearing much underneath.</p><p>A bright smile lights up the face of a young woman with long, golden locks and piercing blue eyes. She stands confidently bare-chested, her slender fingers gently covering one breast as she beams with warmth.</p><p>A carefree moment captured -a smiling woman with a playful grin, tongue out and eyes closed, leans against a wooden table, her bare chest on full display, one hand cradling her breast, the other ruffling her hair.</p><p>x</p><p>A young woman sits comfortably in a woven chair, her long brown hair down her back. She's wearing a vibrant pink marbled crop top and pale pink high-waisted underwear.</p><p>x A bright and radiant smile lights up the face of this lovely woman, her warm smile and sparkling eyes drawing you in. With long, wavy brown hair cascading down her shoulders.</p><p>x A young woman lies on a white bed, her long dark hair splayed out around her. Her eyes are closed, and her mouth is slightly open, as if she's in the midst of a sensual moment. Her hand is placed on her hip. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Generated images with different model components being frozen, with prompt "a pikachu with a pink dress and a pink bow"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of FreezeAsGuard design</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: FreezeAsGuard vs. Naive optimization iterations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Examples of generated public figures' portraits by FreezeAsGuard with ρ=30% and other baseline methods</figDesc><graphic coords="9,183.33,387.84,301.31,59.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Examples of generated artworks by FreezeAsGuard with ρ=70% and other baseline methods</figDesc><graphic coords="12,182.50,387.09,56.88,57.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Examples of generated images with explicit contents by FreezeAsGuard with ρ=70% and other baseline methods</figDesc><graphic coords="12,411.15,536.51,50.85,50.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>[Figure 13 :</head><label>13</label><figDesc>Figure 13: The frozen tensors for illegal classes on the Artwork dataset, with ρ=70%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>automatic differentiation APIs because it is equivalent to standard backpropagation in diffusion model training. Note that this backpropagation shares the same model weights as ∂L upper /∂θ(m) i in Eq. (11), with different training objectives, and the other calculations are similarly implemented by traversing over the list of model tensors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Fine-tuning loss after the 5th and 10th mask updates during bilevel optimization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Statistics of the Famous-Figures-25 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Examples of portrait images in the Famous-Figures-25 dataset</figDesc><graphic coords="20,167.74,151.15,75.18,75.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>Also, in evaluations of FreezeAsGuard's capability of mitigating the generation of explicit contents, we use the Modern-Logo-v4 dataset [5], which contains 803 logo images that are labeled with informative text descriptions, as the legal class. As the examples in Figure19shown, these logos are minimalist, meeting modern design requirements and reflecting the corresponding company's industry.D Details of Image Quality MetricsD.1 Domain-specific feature extractor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Examples of collected painting from 5 artists</figDesc><graphic coords="21,306.52,401.98,74.33,75.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: One sample in the NSFW-Caption dataset</figDesc><graphic coords="22,170.36,95.84,111.45,111.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 19 :F. 2</head><label>192</label><figDesc>Figure 19: Examples in the Modern-Logo-v4 dataset</figDesc><graphic coords="23,312.84,345.90,126.75,121.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Evaluating the similarity in art style using the LPIPS score [31], where a higher score means more difference from the original training image.</figDesc><graphic coords="24,159.84,239.77,93.25,92.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>•</head><figDesc>whitehouse.gov • ifeng.com • theconversation.com • house.gov • cartercenter.org</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Example of the questionnaire for human evaluation</figDesc><graphic coords="25,124.56,68.40,362.87,223.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Examples of generated images after applying FreezeAsGuard-30% to Stable Diffusion v1.5 on illegal classes, where each prompt adopts the same seed for generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Examples of generated images after applying FreezeAsGuard-70% to Stable Diffusion v2.1 on illegal classes, where each prompt adopts the same seed for generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Examples of generated images after applying FreezeAsGuard-70% to Stable Diffusion v1.4 on illegal classes, where each prompt adopts the same seed for generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Mitigation power in one illegal class and one legal class from the Artwork dataset, where worse image quality indicates stronger mitigation power. FG-ρ% means using FreezeAsGuard to freeze ρ% tensors and R-ρ% means random freezing.</figDesc><table><row><cell cols="2">Metric</cell><cell cols="4">CSD(↓) FID(↓) CLIP(↑) Human(↓)</cell></row><row><cell cols="2">Pre-trained model</cell><cell>0.841</cell><cell>323.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Full</cell><cell>illegal</cell><cell>0.347</cell><cell>187.6</cell><cell>32.31</cell><cell>5.9</cell></row><row><cell></cell><cell>legal</cell><cell>0.365</cell><cell>194.0</cell><cell>32.19</cell><cell>5.4</cell></row><row><cell>UCE</cell><cell>illegal</cell><cell>0.426</cell><cell>190.9</cell><cell>32.28</cell><cell>3.3</cell></row><row><cell></cell><cell>legal</cell><cell>0.381</cell><cell>195.1</cell><cell>32.17</cell><cell>3.1</cell></row><row><cell>IMMA</cell><cell>illegal</cell><cell>0.396</cell><cell>190.8</cell><cell>32.61</cell><cell>4.6</cell></row><row><cell></cell><cell>legal</cell><cell>0.377</cell><cell>195</cell><cell>32.98</cell><cell>5.1</cell></row><row><cell cols="2">FG-30% illegal</cell><cell>0.373</cell><cell>190.6</cell><cell>32.37</cell><cell>5.7</cell></row><row><cell></cell><cell>legal</cell><cell>0.382</cell><cell>194.1</cell><cell>32.10</cell><cell>5.2</cell></row><row><cell>R-30%</cell><cell>illegal</cell><cell>0.351</cell><cell>186.7</cell><cell>32.45</cell><cell>5.6</cell></row><row><cell></cell><cell>legal</cell><cell>0.363</cell><cell>194.1</cell><cell>32.56</cell><cell>5.1</cell></row><row><cell>FG-50%</cell><cell>illegal</cell><cell>0.453</cell><cell>194.5</cell><cell>32.04</cell><cell>3.5</cell></row><row><cell></cell><cell>legal</cell><cell>0.40</cell><cell>195.3</cell><cell>32.49</cell><cell>3.9</cell></row><row><cell>R-50%</cell><cell>illegal</cell><cell>0.383</cell><cell>189.7</cell><cell>32.21</cell><cell>5.3</cell></row><row><cell></cell><cell>legal</cell><cell>0.405</cell><cell>196.0</cell><cell>32.43</cell><cell>3.7</cell></row><row><cell cols="2">FG-70% illegal</cell><cell>0.511</cell><cell>195.7</cell><cell>31.96</cell><cell>1.7</cell></row><row><cell></cell><cell>legal</cell><cell>0.41</cell><cell>195.3</cell><cell>32.58</cell><cell>3.8</cell></row><row><cell>R-70%</cell><cell>illegal</cell><cell>0.441</cell><cell>189.2</cell><cell>32.12</cell><cell>4.9</cell></row><row><cell></cell><cell>legal</cell><cell>0.454</cell><cell>196.4</cell><cell>32.15</cell><cell>4.2</cell></row><row><cell cols="2">FG-85% illegal</cell><cell>0.574</cell><cell>201.2</cell><cell>31.74</cell><cell>1.6</cell></row><row><cell></cell><cell>legal</cell><cell>0.526</cell><cell>214.8</cell><cell>31.91</cell><cell>2.1</cell></row><row><cell>R-85%</cell><cell>illegal</cell><cell>0.565</cell><cell>197.6</cell><cell>32.08</cell><cell>2.8</cell></row><row><cell></cell><cell>legal</cell><cell>0.586</cell><cell>210.4</cell><cell>32.09</cell><cell>2.7</cell></row></table><note><p>further shows example images generated by FreezeAsGuard with ρ=70%, and more examples can be found in Appendix F.2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Mitigation power in the FF25 dataset, measured by the FN-L score, with different numbers of illegal classes.</figDesc><table><row><cell>Method</cell><cell cols="2">2 classes</cell><cell cols="2">5 classes</cell><cell cols="2">10 classes</cell></row><row><cell></cell><cell cols="6">illegal legal illegal legal illegal legal</cell></row><row><cell>Full FT</cell><cell cols="6">0.397 0.397 0.424 0.424 0.436 0.436</cell></row><row><cell>UCE</cell><cell cols="6">0.435 0.444 0.443 0.437 0.445 0.442</cell></row><row><cell>IMMA</cell><cell cols="6">0.412 0.428 0.461 0.463 0.467 0.462</cell></row><row><cell cols="7">FG-30% 0.467 0.426 0.474 0.458 0.482 0.449</cell></row><row><cell>Method</cell><cell cols="2">1 class</cell><cell cols="2">2 classes</cell><cell cols="2">3 classes</cell></row><row><cell></cell><cell>illegal</cell><cell>legal</cell><cell>illegal</cell><cell>legal</cell><cell>illegal</cell><cell>legal</cell></row><row><cell>Full FT</cell><cell>0.348</cell><cell>0.356</cell><cell>0.415</cell><cell>0.411</cell><cell>0.434</cell><cell>0.458</cell></row><row><cell>UCE</cell><cell>0.426</cell><cell>0.381</cell><cell>0.538</cell><cell>0.521</cell><cell>0.552</cell><cell>0.574</cell></row><row><cell>IMMA</cell><cell>0.396</cell><cell>0.377</cell><cell>0.483</cell><cell>0.463</cell><cell>0.536</cell><cell>0.496</cell></row><row><cell>FG-70%</cell><cell>0.511</cell><cell>0.410</cell><cell>0.609</cell><cell>0.473</cell><cell>0.648</cell><cell>0.525</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Mitigation power in the Artwork dataset, measured by the CSD score, with different numbers of illegal classes</figDesc><table><row><cell>Method</cell><cell>SD 1.4</cell><cell>SD 1.5</cell><cell>SD 2.1</cell></row><row><cell></cell><cell cols="3">illegal legal illegal legal illegal legal</cell></row><row><cell>Full</cell><cell cols="3">0.435 0.435 0.436 0.436 0.439 0.439</cell></row><row><cell>UCE</cell><cell cols="3">0.447 0.442 0.445 0.442 0.445 0.441</cell></row><row><cell>IMMA</cell><cell cols="3">0.451 0.448 0.467 0.462 0.463 0.454</cell></row><row><cell cols="4">FG-30% 0.489 0.453 0.482 0.449 0.474 0.450</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Mitigation power in the FF25 dataset, measured by the FN-L score, with different diffusion models</figDesc><table><row><cell>Fine-tuning Cost</cell><cell>ρ=0%</cell><cell>ρ=1%</cell><cell>ρ=5%</cell><cell>ρ=10%</cell></row><row><cell>GPU Memory (GB)</cell><cell>18.28</cell><cell>18.26</cell><cell>16.97</cell><cell>16.96</cell></row><row><cell>Per-batch computing time (s)</cell><cell>1.17</cell><cell>1.14</cell><cell>1.09</cell><cell>1.06</cell></row><row><cell>Fine-tuning Cost</cell><cell cols="4">ρ=20% ρ=30% ρ=40% ρ=80%</cell></row><row><cell>GPU Memory (GB)</cell><cell>15.43</cell><cell>14.15</cell><cell>13.61</cell><cell>9.49</cell></row><row><cell>Per-batch computing time (s)</cell><cell>1.05</cell><cell>1.02</cell><cell>1.00</cell><cell>0.91</cell></row><row><cell cols="5">Table 8: Computing cost with FreezeAsGuard-ρ on SD v1.5 model, using an NVidia A6000 GPU</cell></row><row><cell cols="5">[10] stable diffusion v2.1. https://huggingface.co/runwayml/stable-diffusion-v1-5,</cell></row><row><cell>2023. 1, 6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>[11] Diffusion wallpaper. https://serp.ai/tools/diffusion-wallpaper/, 2024. 1</p><p>[12] Opencv face recognition. https://opencv.org/opencv-face-recognition/, 2024. 19 [13] H. Chefer, O. Lang, M. Geva, V. Polosukhin, A. Shocher, M. Irani, I. Mosseri, and L. Wolf. The hidden language of diffusion models. arXiv preprint arXiv:2306.00966, 2023. 3 [14] C. Chen, J. Mo, J. Hou, H. Wu, L. Liao, W. Sun, Q. Yan, and W. Lin. Topiq: A top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 2024. 3 [15] Y. Chen, B. Chen, X. He, C. Gao, Y. Li, J.-G. Lou, and Y. Wang. λopt: Learn to regularize recommender models in finer levels. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 978-986, 2019. 4 [16] Y. Cui, J. Ren, Y. Lin, H. Xu, P. He, Y. Xing, W. Fan, H. Liu, and J. Tang. Ft-shield: A watermark against unauthorized fine-tuning in text-to-image diffusion models. arXiv preprint arXiv:2310.02401, 2023. 2 [17] Y. Cui, J. Ren, H. Xu, P. He, H. Liu, L. Sun, and J. Tang. Diffusionshield: A watermark for copyright protection against generative diffusion models. arXiv preprint arXiv:2306.04642, 2023. 2</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Many APIs, such as HuggingFace Diffusers<ref type="bibr" target="#b40">[56]</ref>, can be used for fine-tuning open-sourced diffusion models with the minimum user efforts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Most existing diffusion models have parameter sizes between 1B and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>3.5B, which correspond to at least 686 tensors over the UNet-based denoiser.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/notAI-tech/NudeNet.Ac-cessed" />
		<title level="m">Nudenet: lightweight nudity detection</title>
		<imprint>
			<date type="published" when="1920">2024-10-30. 7, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://vickiboykis.com/2022/11/18/some-notes-on-the-stable-diffusion-safety-filter/,2022.2" />
		<title level="m">the-stable-diffusion-safety-filter</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Autocrawler</surname></persName>
		</author>
		<ptr target="https://github.com/YoongiKim/AutoCrawler" />
		<imprint>
			<date type="published" when="2019">2023. 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beyond the safeguards: Exploring the security risks of chatgpt</title>
		<author>
			<persName><forename type="first">E</forename><surname>Derner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batistič</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08005</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12508</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are deepfakes concerning? analyzing conversations of deepfakes on reddit and exploring societal implications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghasiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bonagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sasahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unified concept editing in diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gandikota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Orgad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Materzyńska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Politics and porn: how news media characterizes problems presented by deepfakes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burkell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical Studies in Media Communication</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ai-generated child sex images spawn new nightmare for the web</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Wall Street Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">This artist is dominating ai-generated art. and he&apos;s not happy about it</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technology Review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Clipscore: A reference-free evaluation metric for image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08718</idno>
		<imprint>
			<date type="published" when="1920">2021. 3, 7, 20</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="1920">2017. 3, 7, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple objective decision making-methods and applications: a state-of-the-art survey</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S M</forename><surname>Masud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">164</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking fid: Towards a better evaluation metric for image generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pipal: a large-scale image quality assessment dataset for perceptual image restoration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jinjin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haoming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haoyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiaoxing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Label-only model inversion attacks via boundary repulsion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kahla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Elucidating the design space of diffusion-based generative models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Testing using privileged information by adapting features with statistical dependence</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning debiased classifier with biased committee</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2009">2009. 2009</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02340</idno>
		<title level="m">Snip: Single-shot network pruning based on connection sensitivity</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Group fisher pruning for practical network compression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="7021" to="7032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01952</idno>
		<title level="m">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">October 5-9, 2015. 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>proceedings, part III 18</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Xgan: Unsupervised image-to-image translation for many-to-many mappings. Domain Adaptation for Visual Understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A benchmark of facial recognition pipelines and co-usability performances of modules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Serengil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozpinar</surname></persName>
		</author>
		<idno type="DOI">10.17671/gazibtd.1399077</idno>
		<ptr target="https://dergipark.org.tr/en/pub/gazibtd/issue/84331/1399077.7" />
	</analytic>
	<monogr>
		<title level="j">Journal of Information Technologies</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glaze: Protecting artists from style mimicry by {Text-to-Image} models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd USENIX Security Symposium (USENIX Security 23)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Somepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.01292</idno>
		<title level="m">Measuring style similarity in diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved artgan for conditional synthesis of natural image and artwork</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2866698</idno>
		<ptr target="https://doi.org/10.1109/TIP.2018.2866698" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rassin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.15002</idno>
		<title level="m">How many van goghs does it take to van gogh? finding the imitation threshold</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Do prompt-based models really understand the meaning of their prompts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01247</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.05779</idno>
		<title level="m">Erasediff: Erasing data influence in diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning dynamic style kernels for artistic style transfer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Duaw: Data-free universal adversarial watermark against stable diffusion customization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09889</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.03099</idno>
		<title level="m">Language models are super mario: Absorbing abilities from homologous models as a free lunch</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 13</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Editguard: Versatile image watermarking for tamper localization and copyright protection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.08883</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Global-local gcn: Large-scale label noise cleansing for face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.10137</idno>
		<title level="m">A recipe for watermarking diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Imma: Immunizing text-to-image models against malicious adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.18815</idno>
		<imprint>
			<date type="published" when="2023">2023. 2, 5, 7, 19</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
