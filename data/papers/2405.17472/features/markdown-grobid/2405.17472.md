# FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing

## Abstract

## 

Text-to-image diffusion models can be fine-tuned in custom domains to adapt to specific user preferences, but such adaptability has also been utilized for illegal purposes, such as forging public figures' portraits, duplicating copyrighted artworks and generating explicit contents. Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data. In this paper, we present FreezeAsGuard, a new technique that addresses these limitations and enables irreversible mitigation of illegal adaptations of diffusion models. Our approach is that the model publisher selectively freezes tensors in pre-trained diffusion models that are critical to illegal model adaptations, to mitigate the fine-tuned model's representation power in illegal adaptations, but minimize the impact on other legal adaptations. Experiment results in multiple text-to-image application domains show that FreezeAsGuard provides 37% stronger power in mitigating illegal model adaptations compared to competitive baselines, while incurring less than 5% impact on legal model adaptations.

## Figure 1: Existing work vs. FreezeAsGuard in mitigating malicious adaptation of diffusion models

Text-to-image diffusion models [[44,](#b28)[43]](#b27) are powerful tools to generate high-quality images aligned with user prompts. After pre-trained by model publishers to embed world knowledge from large image data [[49]](#b33), open-sourced diffusion models, such as Stable Diffusion (SD) [9, 10], can be conveniently adapted by users to generate their preferred images[foot_0](#foot_0) , through fine-tuning with custom data in specific domains. For example, diffusion models can be fine-tuned on cartoon datasets to synthesize avatars in video games [[46]](#b30), or on datasets of landscape photos to generate wallpapers [[11]](#).

An increasing risk of democratizing open-sourced diffusion models, however, is that the capability of model adaptation has been utilized for illegal purposes, such as forging public figures' portraits [[22,](#b6)[24]](#b8), duplicating copyrighted artworks [[26]](#b10), and generating explicit content [[25]](#b9). Most existing efforts aim to deter at-

a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% IMMA UCE IMMA UCE ing FreezeAsGuard-30% IMMA UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of elizabeth warren which shows her speaking at a rally a photo of elizabeth warren which shows her standing in front of a classroom a photo of elizabeth warren which shows her sitting in a chair Training sample Full fine-tuning Prompt a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% IMMA UCE IMMA UCE Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of emma watson which shows her wearing a hat IMMA UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE ing FreezeAsGuard-30% Training sample Full fine-tuning Prompt a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% IMMA UCE IMMA UCE ing FreezeAsGuard-30% IMMA UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of elizabeth warren which shows her speaking at a rally a photo of elizabeth warren which shows her standing in front of a classroom a photo of elizabeth warren which shows her sitting in a chair Training sample Full fine-tuning Prompt a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% IMMA UCE IMMA UCE Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of emma watson which shows her wearing a hat IMMA UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress Figure [2](#): FreezeAsGuard ensures that portraits (left) and artworks (right) generated by diffusion models in illegal classes cannot be recognizable as target objects, even if the model has been fine-tuned with data samples in illegal classes. In contrast, unlearning schemes (UCE [[23]](#b7) and IMMA [[65]](#b49)) cannot prevent the unlearned knowledge of illegal classes from being relearned in fine-tuning.

tempts of illegal model adaptation with copyright detection [[64,](#b48)[16,](#)[17]](#), which embeds invisible but detectable watermarks into training data and further generated images, as shown in Figure [1](#). However, such detection only applies to misuse of training data, and does not mitigate the user's capability of illegal model adaptation. Users can easily bypass such detection by collecting and using their own training data without being watermarked (e.g., users' self-taken photos of public figures).

Instead, an intuitive approach to mitigation is content filtering. However, filtering user prompts [[19]](#b3) can be bypassed by fine-tuning the model to align innocent prompts with illegal image contents [[55]](#b39), and filtering the generated images [7] is often overpowered with high false-positive rates [[3]](#b1). Data poisoning techniques can avoid false positives by injecting invisible perturbations into training data [[59,](#b43)[62,](#b46)[51]](#b35), but cannot apply when public web data or users' private data is used for fine-tuning.

Recent unlearning methods allow model publishers to remove knowledge needed for illegal adaptation by modifying model weights [[20,](#b4)[23,](#b7)[57,](#b41)[65]](#b49) , but cannot prevent relearning such knowledge via fine-tuning.

The key limitation of these techniques is that they focus on modifying the training data or model weights, but such modification can be reversed by users via fine-tuning with their own data. Such modification, further, cannot restrain the mitigation power only in illegal data classes (e.g., public figures' portraits) without affecting model adaptation in other legal data classes (e.g., the user's own portraits), due to the high ambiguity and possible overlap between these classes.

To prevent users from reversing the mitigation maneuvers being applied, in this paper we present FreezeAsGuard, a new technique that constrains the trainability of diffusion model's tensors in finetuning. As shown in Figure [1](#), the model publisher selectively freezes tensors in pre-trained models that are critical to fine-tuning in illegal classes (e.g., public figures' portraits), to limit the model's representation power of being fine-tuned in illegal classes. In practice, since most illegal users are not professional and fine-tune diffusion models by simply following the instructions provided by model publishers, tensor freezing can be effectively enforced by model publishers through these instructions, to guide the users to adopt tensor freezing. Essentially, since freezing tensors lowers the trainable model parameters and reduces the computing costs of fine-tuning, users would be well motivated to adopt tensor freezing in fine-tuning practices.

## Diffusion Model

## Binary mask

## Fine-tuning

## Mask Learning

freeze tensors update mask

## Figure 3: Mask learning and fine-tuning as a bilevel optimization

The major challenge is how to properly evaluate the importance of tensors in model fine-tuning. Popular attribution-based importance metrics [[38,](#b22)[41]](#b25) are used in model pruning with fixed weight values, but cannot reflect the impact of weight variations in fine-tuning. Such impact of weight variations, in fact, cannot be condensed into a single importance metric, due to the randomness and interdependencies of weight updates in finetuning iterations.

Instead, as shown in Figure [3](#), we formulate the selection of frozen tensors in all the illegal classes as one trainable binary mask. Given a required ratio of frozen tensors specified by model publisher, we optimize such selection with training data in all the involved illegal classes, through bilevel optimization that combines the iterative process of mask learning and iterations of model fine-tuning. In this way, the mask being trained can timely learn the impact of weight variations on the training loss during fine-tuning.

With frozen tensors, the model's representation power should be retained when fine-tuned on other legal classes (e.g., user's own portraits). Hence, we incorporate training samples from legal classes into the bilevel optimization, to provide suppressing signals for selecting tensors being frozen. Hence, the learned mask of freezing tensors should skip tensors that are important to fine-tuning in legal classes.

We evaluated FreezeAsGuard in three different domains of illegal model adaptations: 1) forging public figures' portraits, 2) duplicating copyrighted artworks and 3) generating explicit contents. For each domain, we use open-sourced or self-collected datasets, and randomly select different data classes as illegal and legal classes. We use competitive model unlearning schemes as baselines, and multiple metrics to measure image quality. Our findings are as follows:

‚Ä¢ FreezeAsGuard has strong mitigation power in illegal classes. Compared to the competitive baselines, it further reduces the quality of images generated by fine-tuned model by up to 37%, and ensures the generated images to be unrecognizable as subjects in illegal classes.

‚Ä¢ FreezeAsGuard has the minimum impact on modal adaptation in legal classes. It ensures on-par quality of the generated images compared to regular full fine-tuning on legal data, with a difference of at most 5%.

‚Ä¢ FreezeAsGuard has high compute efficiency. Compared to full fine-tuning, it can save up to 48% GPU memory and 21% wall-clock computing time.

## Background & Motivation

## Fine-Tuning Diffusion Models

Given text prompts y and images x as training data, fine-tuning a diffusion model approximates the conditional distribution p(x|y) by learning to reconstruct images that are progressively blurred with noise œµ over step t = 1, ..., T . Training objective is to minimize the reconstruction loss:

$L Œ∏ = E x,y,œµ‚àºN (0,1),t ‚à•œµ -œµ Œ∏ (E(x t ), t, œÑ (y))‚à• 2 2 ,(1)$where E(‚Ä¢) is the encoder of a pretrained VAE, œÑ (‚Ä¢) is a pretrained text encoder, and œµ Œ∏ (‚Ä¢) is a denoising model with trainable parameters Œ∏. Most diffusion models adopt UNet architecture [[45]](#b29) as the denoising model.

In fine-tuning, the diffusion model learns new knowledge by adapting the generic knowledge in the pre-trained model [[13]](#). For example, new knowledge about "a green beetle" can be a combination of generic knowledge on "hornet" and "emerald". This behavior implies that fine-tuning in different classes may share the same knowledge base, and it is challenging to focus the mitigation power in illegal classes without affecting fine-tuning in other legal classes. This challenge motivates us to regulate FreezeAsGuard's mitigation power by incorporating training samples in legal classes, when selecting tensors being frozen for illegal classes.

Model component Being frozen CLIP (‚Üë) TOPIQ (‚Üë) FID (‚Üì) No freezing 31.93 0.054 202.18 Attention projectors 31.60 0.051 208.40 Conv. layers 31.54 0.047 206.58 Time embeddings 31.46 0.045 212.79 50% random weights (seed 1) 32.25 0.054 206.53 50% random weights (seed 2) 32.62 0.051 216.12 Table 1: Quality of generated images with different model compoents being frozen, using CLIP [27], TOPIQ [14], and FID [28] image quality metrics and the captioned pokemon dataset [6] 2.2 Partial Model Fine-tuning freezing cross-attention freezing convolution no freezing freezing random 50% (seed 1) freezing random 50% (seed 2) freezing time embedding An intuitive solution to mitigating illegal model adaptation is to only allow fine-tuning some layers or components of the diffusion model. However, this solution is ineffective in practice, because shallow layers provide primary image features and deep layers enforce domainspecific semantics [[61]](#b45). They are, hence, both essential to the performance of the fine-tuned models in legal classes. Similarly, as shown in Table [1](#) and Figure [4](#fig_1), freezing critical model components such as attention projectors and time embeddings can cause large quality drop in generated images. Even when freezing the same amount of model weights (e.g., random 50%), the exact distribution of frozen weights could also affect the generated images' quality. Such heterogeneity motivates us to instead seek for globally optimal selections of freezing tensors across all model components, by jointly taking all model components into bilevel optimization.

Illegal Class ùë™ùë™ illegal Legal Class ùë™ùë™ legal ùíéùíé ùúΩùúΩ pre ùúΩùúΩ ft ùúΩùúΩ(ùíéùíé) = ùíéùíé‚®ÄùúΩùúΩ pre + 1 -ùíéùíé ‚®ÄùúΩùúΩ ft ùíôùíô illùêûùêûùêûùêûùêûùêûùêûùêû ùíôùíô legal ùêøùêø ùúΩùúΩ(ùíéùíé) (ùíôùíô illegal ) ùêøùêø ùúΩùúΩ(ùíéùíé) (ùíôùíô legal ) Sampled batches Mask Learning Loop: learn a mask to divert convergence on illegal class Fine-tuning Loss 1 -1 1 Simulated User Loop: finetune model towards convergence Mask Learning by Model Publishers Pretrained Diffusion Model ùêøùêø sparsity m 1 1 0 1 0 Mask for tensor freezing Fine-tuning by Users finetune Illegal Class Legal Class Low Quality High Quality freeze 1 Diffusion Model with Partial Freezing Our design of FreezeAsGuard builds on bilevel optimization, which embeds one optimization problem within another and both of them are multi-objective optimizations [15, [40,](#b24)[21]](#b5). This bilevel optimization can be formulated as

$m * = arg min m -L Œ∏ * (m) (x illegal ), L Œ∏ * (m) (x legal )(2)$$s.t. Œ∏ * (m) = arg min Œ∏(m) L Œ∏(m) (x illegal ), L Œ∏(m) (x legal ) ,(3)$where m is the binary mask of selecting frozen tensors, m * is the optimized binary mask, Œ∏(m) represents the model tensors frozen by m, and Œ∏ * (m) is the converged Œ∏(m) after fine-tuning. x illegal and x legal denote training samples in all the illegal classes (C illegal ) and legal classes (C legal ), respectively. Such bilevel optimization is illustrated in Figure [5](#fig_2). The lower-level problem in Eq. ( [3](#formula_2)) is a simulated user loop that the user fine-tunes the diffusion model by minimizing the loss over both illegal and legal classes. The upper-level problem in Eq. ( [2](#formula_1)) is a mask learning loop that learns m to mitigate the model's representation power when fine-tuned in illegal classes, without affecting fine-tuning in legal classes. We use the standard diffusion loss in Eq. (1) and adopt tensor-level freezing to ensure sufficient granularity[foot_1](#foot_1) , without incurring extra computing costs.

To apply the gradient solver, m and Œ∏(m) should have differentiable dependencies with the loss function. We model Œ∏(m) through the weighted summation of pre-trained model tensors Œ∏ pre and fine-tuned model tensors Œ∏ f t , such that

$Œ∏(m) = m ‚äô Œ∏ pre + (1 -m) ‚äô Œ∏ f t ,(4)$where ‚äô denotes element-wise multiplication. From the user's perspective, fine-tuning the partially frozen model Œ∏(m) is equivalent to fine-tuning Œ∏ f t , controlled by Eq. (3). To improve compute efficiency, we initialize Œ∏ f t as the fully fine-tuned model tensors on both illegal and legal classes, and gradually enlarge the scope of tensor freezing. Since m is discrete and not differentiable, we adopt a continuous form m(w) = œÉ(w/T ) that applies sigmoid function œÉ(‚Ä¢) over a trainable tensor w. We also did code optimizations for vectorized gradient calculations as in Appendix A.

Note that, although we made m differentiable in bilevel optimizations, the optimized values in m * will be rounded to binary, to ensure complete freezing of selected tensors.

## Mask Learning in the Upper-level Loop

To solve the upper-level optimization in Eq. ( [2](#formula_1)), we adopt linear scalarization [[29]](#b13) to convert it into a single objective L upper via a weighted summation with weights (Œª 1 , Œª 2 ):

$L upper = -Œª 1 L Œ∏ * (m) (x illegal ) + Œª 2 L Œ∏ * (m) (x legal ),(5)$to involve training samples in both illegal and legal classes when learning m. (Œª 1 , Œª 2 ) should ensure that gradient-based feedbacks from the two loss terms are not biased by inequality between the amounts of x illegal and x legal , and their values should be proportionally set based on these amounts.

Besides, x illegal and x legal could contain some knowledge in common, and masked learning from such data may hence affect model adaptation in legal classes. To address this problem, we add a sparsity constraint L sparsity to L upper to better control of the mask's mitigation power:

$L sparsity = ‚à•1 ‚ä§ m/N -œÅ‚à• 2 2 , (6$$)$where N is the number of tensors and 1 ‚ä§ m/N measures the proportion of tensors being frozen. By minimizing L sparsity , the achieved ratio of tensor freezing should approach the given œÅ. In this way, we can apply gradient descent to minimize L upper and iteratively refine m towards optimum.

## Model Fine-tuning in the Lower-level Loop

Effectiveness of mask learning at the upper level relies on timely feedback from the lower-level fine-tuning. Every time the mask has been updated by an iteration in the upper level, the lower-level loop should adopt the updated mask into fine-tuning, and return the fine-tuned model tensors and the correspondingly updated loss value as feedback to the upper level. Similar to Eq. ( [5](#formula_4)), the fine-tuning objective is the summation of diffusion losses for illegal and legal domains:

$L lower = L Œ∏ * (m) (x illegal ) + L Œ∏ * (m) (x legal ).(7)$ùêøùêø(ùëöùëö 1 , ùë°ùë°) 

## Towards Efficient Bilevel Optimization

Solving bilevel optimization is computationally expensive, due to the repeated switches between upper-level and lower-level loops [[47,](#b31)[65]](#b49). Rigorously, as shown in Figure [6](#fig_3) -Left, every time when the mask has been updated, the model should be fine-tuned with a sufficient number of iterations until convergence, before the next update of the mask. However, in practice, doing so is extremely expensive.

Instead, as shown in Figure [6](#fig_3) -Right, we observe that the fine-tuning loss typically drops fast in the first few iterations and then violently fluctuates (see Appendix B). Hence, every time in the lower-level loop of model fine-tuning, we do not wait for the loss to converge, but only fine-tune the model for the first few iterations before updating the mask to the upper-level loop of mask learning. After the model update, the fine-tuned model weights are inherited to the next loop of model fine-tuning, to ensure consistency and improve convergence. Hence, the optimization only needs one fine-tuning process, during which the mask can be updated with shorter intervals but higher learning quality. Details of deciding such a number of iterations are in Appendix B.

Further, to perform bilevel optimizations, three versions of diffusion model weights, i.e., Œ∏(m), Œ∏ pre and Œ∏ f t , will be maintained for gradient computation. This could significantly increase the memory cost due to large sizes of diffusion models. To reduce such memory cost, we instead maintain only two versions of model weights, namely Œ∏(m) and Œ∏ d = Œ∏ pre -Œ∏ f t . According to Appendix A, the involvement of both Œ∏ pre and Œ∏ f t can be removed by plugging Œ∏ d into the gradient descent calculation. More specifically, for a given model tensor i, the gradient descent to update the corresponding mask m i in the upper-level optimization is:

$w i ‚Üê w i -Œ∑ 1 ‚àÇL upper ‚àÇŒ∏(m) i , Œ∏ (i) d 1 T œÉ w i T œÉ 1 - w i T ,(8)$where Œ∑ 1 controls the step size of updates and m i is updated as œÉ ‚Ä¢ w i /T . Further, computing the update of Œ∏(m) and Œ∏ d at the lower level should apply the chain rule:

$Œ∏ (i) d ‚Üê Œ∏ (i) d + Œ∑ 2 ‚àÇL lower ‚àÇŒ∏(m) i (1 -m i ) (9) Œ∏(m) i ‚Üê Œ∏(m) i -Œ∑ 2 ‚àÇL lower ‚àÇŒ∏(m) i (1 -m i ) 2 . (10$$)$In this way, as shown in Algorithm 1, FreezeAsGuard alternately runs upper and lower-level gradient descent steps, with the maximum compute efficiency and the minimum memory cost. We initialize the mask to all zeros and Œ∏(m) starts as a fully fine-tuned model, to mitigate aggressive freezing. In practice, we set random negative values to w to ensure the continuous form of the mask is near zero.

## Algorithm 1 Freezing Strategy in FreezeAsGuard

Require: Illegal and legal class data (C illegal , C legal ), step size Œ∑1 and Œ∑2, model weights Œ∏pre and Œ∏ f t 1:

$Œ∏ d ‚Üê Œ∏pre -Œ∏ f t , m ‚Üê 0, Œ∏(m) ‚Üê Œ∏ f t 2: for k = 1, ..., K do 3: for l = 1, ..., L do 4: (x illegal , x legal ) ‚Üê Sample(C illegal , C legal ) 5: ‚àÇL lower ‚àÇŒ∏(m) ‚Üê Backprop(x illegal , x legal , L lower , Œ∏(m)) 6: (Œ∏ d , Œ∏(m)) ‚Üê Update ‚àÇL lower ‚àÇŒ∏(m) , m, Œ∏ d , Œ∏(m)$// Refer to Eq. ( [9](#)) and (10) 7:

end for 8:

$(x illegal , x legal ) ‚Üê Sample(C illegal , C legal ) 9: ‚àÇLupper ‚àÇŒ∏(m) ‚Üê Backprop(x illegal , x legal , Lupper, Œ∏(m)) 10: m ‚Üê Update ‚àÇLupper ‚àÇŒ∏(m) , m, Œ∏ d , Œ∑2$// Refer to Eq. ( [8](#formula_8))

$11: end for ‚áí Return Round(m)$
## Experiments

In our experiments, we use three open-source diffusion models, SD v1.4 [8], v1.5 [9] and v2.1 [10], to evaluate three domains of illegal model adaptations: 1) forging public figures' portraits [[22,](#b6)[24]](#b8), 2) duplicating copyrighted artworks [[26]](#b10) and 3) generating explicit content [[25]](#b9).

Datasets: For each domain, we use datasets as listed below, and random select different data classes as illegal and legal classes. We use 50% of samples in the selected classes for mask learning and model training, and the other samples for testing. More details about datasets are in Appendix C.

‚Ä¢ Portraits of public figures: We use a self-collected dataset, namely Famous-Figures-25 (FF25), with 8,703 publicly available portraits of 25 public figures on the Web. Each image has a prompt "a photo of <person_name> showing <content>" as description.

‚Ä¢ Copyrighted artworks: We use a self-collected dataset, namely Artwork, which contains 1,134 publicly available artwork images and text captions on the Web, from five famous digital artists with unique art styles.

‚Ä¢ Explicit contents: We use the NSFW-caption dataset with 2,000 not-safe-for-work (NSFW) images and their captions [1] as the illegal class. We use the Modern-Logo-v4 [5] dataset, which contains 803 logo images labeled with informative text descriptions, as the legal class.

Baseline schemes: Our baselines include full fine-tuning (FT), random tensor freezing, and two competitive unlearning schemes, namely UCE [23 and IMMA [[65]](#b49). Existing data poisoning methods [[59,](#b43)[62,](#b46)[51]](#b35) cannot be used because all data we use is publicly online and cannot be poisoned.

‚Ä¢ Full FT: It fine-tunes all the tensors of the diffusion model's UNet and has the strongest representation power for adaptation in illegal domains.

‚Ä¢ Random-œÅ: It randomly freezes œÅ% of model tensors, as a naive baseline of tensor freezing.

‚Ä¢ UCE [[23]](#b7): It uses unlearning to guide the learned knowledge about illegal classes in the pre-trained model to be irrelevant or more generic.

‚Ä¢ IMMA [[65]](#b49): It reinitializes the model weights so that it is hard for users to conduct effective fine-tuning on the reinitialized model, in both illegal and legal classes.

Measuring image quality: We used FID [[28]](#b12) and CLIP [[27]](#b11) scores to evaluate the quality of generated images. In addition, to better identify domain-specific details in generated images, we also adopted domain-specific image quality metrics, listed as below and described in detail in Appendix D. For each text prompt, the experiment results are averaged from 100 generated images with different random seeds.

‚Ä¢ Domain-specific feature extractors: Existing work [[54]](#b38) reported that FID and CLIP fail to measure the similarity between portraits of human subjects, and cannot reflect human perception in images. Hence, for human portraits and artworks, we apply specific feature extractors on real and generated images, and measure the quality of generated images as cosine distance between their feature vectors. For portraits, we use face feature extractors (FN-L, FN, VGG) in DeepFace [[50]](#b34). For artworks, we use a pretrained CSD model [[52]](#b36). Details are in Appendix D.1.

‚Ä¢ NudeNet: We used NudeNet [[2]](#b0) to decide the probability of whether the generated images contain explicit contents, as the image's safety score. Details are in Appendix D.2.

‚Ä¢ Human Evaluation: To better capture human perception in generated images, we recruited 16 volunteers with diverse backgrounds to provide human evaluations on image quality. For each image, volunteers scored how the generated image is likely to depict the same subject as in the real image from 1 to 7, where 1 means "very unlikely" and 7 means "very likely". Details are in Appendix D.3.

## Mitigating Forgery of Public Figures' Portraits

We evaluate FreezeAsGuard in mitigating forgery of public figures' portraits, using FF25 dataset and SD v1.5 model. 10 classes are randomly selected from FF25 as illegal and legal classes, respectively. As shown in Table [2](#), FreezeAsGuard can mitigate illegal model adaptation by 40% compared to Full FT. When œÅ varies from 10% to 50%, it also outperforms the unlearning schemes by 37%, because these schemes cannot prevent relearning knowledge in illegal classes with new training data. It also ensures better legal model adaptation. With œÅ=30%, the impact on legal adaptation is <5%.

When the freezing ratio (œÅ) increases, the difference between FreezeAsGuard and random freezing diminishes, and their mitigation powers also reach a similar level. This means that only a portion of tensors are important for adaptation in specific illegal classes. With a high freezing ratio, random freezing is more likely to freeze these important tensors. Meanwhile, it could also freeze tensors that are important to legal classes, resulting in low performance in legal model adaptations. Hence, as shown in Figure [7](#), when œÅ=30%, the mitigation power is high enough that the generated images no longer resemble those in training data, and further increasing œÅ could largely affect legal model adaptation.

Based on these results, we empirically consider œÅ=30% as the optimal freezing ratio on SD v1.5 for the domain of public figures' portraits. Figure [8](#fig_4) shows example images of baseline methods and

Metric FN-L(‚Üì) FN(‚Üì) VGG(‚Üì) FID(‚Üì) Human (‚Üì) Pre-trained model 0.96 0.92 0.93 164.8 -Full FT illegal 0.436 0.455 0.581 144.6 6.7 legal 0.436 0.455 0.581 144.6 6.7 UCE illegal 0.445 0.464 0.598 152.9 4.6 legal 0.442 0.465 0.583 151.4 5.4 IMMA illegal 0.467 0.493 0.624 148.8 5.1 legal 0.462 0.475 0.610 145.9 5.8 FG-10% illegal 0.441 0.451 0.603 148.0 4.9 legal 0.429 0.45 0.585 143.6 6.2 R-10% illegal 0.433 0.451 0.588 143.7 6.8 legal 0.431 0.457 0.582 144.0 6.8 FG-30% illegal 0.482 0.504 0.631 153.7 3.6 legal 0.449 0.478 0.590 146.7 6.0 R-30% illegal 0.429 0.456 0.590 145.0 5.9 legal 0.429 0.456 0.590 145.0 5.9 FG-50% illegal 0.530 0.638 0.647 155.5 2.1 legal 0.499 0.527 0.608 149.5 4.3 R-50% illegal 0.513 0.543 0.638 151.6 3.7 legal 0.512 0.522 0.632 153.2 3.7 Table 2: Mitigation power in 10 illegal classes and 10 legal classes from the FF25 dataset, where worse image quality indicates stronger mitigation power. FG-œÅ% means using FreezeAsGuard to freeze œÅ% tensors and R-œÅ% means random freezing. Prompt Training sample Full fine-tuning FG-10% FG-30% FG-50% a photo of angela merkel which shows her smiling a photo of angela merkel which shows her looking at the camera a photo of angela merkel which shows her walking down a red carpet Figure 7: Examples of public figures' portraits generated by FreezeAsGuard under different freezing ratios (œÅ) a photo of elizabeth warren which shows her speaking at a rally a photo of donald trump which shows him making a fist Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of emma watson which shows her wearing a hat IMMA UCE Training sample Full Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% Training sample Full fine-tuning Prompt a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% UCE IMMA UCE FreezeAsGuard-30% UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE a photo of elizabeth warren which shows her speaking at a rally a photo of donald trump which shows him making a fist Training sample Full fine-tuning FreezeAsGuard-30% Prompt a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of emma watson which shows her wearing a hat IMMA UCE Training sample Full Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress a photo of donald trump which shows him standing in front of flags a photo of donald trump which shows him speaking into a microphone a photo of donald trump which shows him making a fist FreezeAsGuard-30% UCE Training sample Full fine-tuning Prompt a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress a photo of halle berry which shows her posing in a gold dress FreezeAsGuard-30% IMMA UCE a photo of emma watson which shows her with her hair in a bob a photo of emma watson which shows her wearing a blue sweater a photo of halle berry which shows her doing a workout a photo of halle berry which shows her wearing a gold dress Prompt Training image Full fine-tuning IMMA UCE FG-30% a photo of emma watson which shows her wearing a hat a photo of halle berry which shows her posing in a gold dress 

## Mitigating Duplication of Copyright Artworks

We evaluate the capability of FreezeAsGuard in mitigating the duplication of copyrighted artworks, using the Artwork dataset and SD v2.1 model. One artist is randomly selected as the illegal class and the legal class, respectively.

The results with different freezing ratios are shown in Table [3](#tab_7) and Figure [9](#). Unlike results in Section 4.1 where data classes exhibit only subtle differences in facial features, different artists' artworks demonstrate markedly different styles. Hence, a higher freezing ratio is required for sufficient mitigation power. We empirically decide the optimal freezing ratio for the domain of artwork is 70%. When œÅ=70%, FreezeAsGuard can provide 47% more mitigation power in illegal classes compared to full fine-tuning, and 30% more compared to unlearning schemes. Figure [10](#fig_5)

## Mitigating Generation of Explicit Contents

To evaluate FreezeAsGuard's mitigation of explicit contents, we designate the NSFW-caption dataset as illegal class, and the Modern-Logo-v4 dataset as legal class. Results in Table [4](#) and Figure [11](#fig_6) show that, with œÅ=70%, FreezeAsGuard significantly reduces the model's capability of generating explicit contents by up to 38% compared to unlearning schemes, while maintaining the model's adaptability in legal class. More image examples are in Appendix F.3.

## Scalability of Mitigation Power

To evaluate FreezeAsGuard's scalability over multiple illegal classes, we randomly pick 2, 5 and 10 public figures in the FF25 dataset, and 1, 2 and 3 artists in the Artworks dataset, as illegal classes. As shown in Table [5](#tab_10) and 6, when the number of illegal classes increases, FreezeAsGuard can retain strong mitigation power in both cases, and continuously outperforms the unlearning schemes. Note that, with more illegal classes, the difference of mitigation power between FreezeAsGuard and random freezing is smaller, because more illegal classes correspond to more adaptation-critical tensors, and random freezing is more likely to cover them.

## The Learned Selection of Frozen Tensors

In Figure [12](#) and 13, we visualized the learned binary masks of tensor freezing for different illegal classes on the FF-25 and Artwork datasets, respectively, with the SD v1.5 model. These results show that on both datasets, the tensors being frozen for different illegal classes largely vary, indicating that

Prompt Training sample Full fine-tuning FG-30% FG-50% FG-70% FG-85% there is a tiger and a dragon with a kite in the sky illustration of a man working on a machine with a dog nearby cartoon illustration of a man eating a hot dog in a crowded city Figure 9: Examples of artwork images generated by FreezeAsGuard with different freezing ratios Method Illegal Legal NudeNet(‚Üë) FID(‚Üì) CLIP(‚Üë) Pre-trained model 0.47 --Full FT 1.29 158.1 32.79 UCE 1.20 158.5 30.07 IMMA 1.17 162.0 28.71 FG-30% 1.27 159.5 32.50 R-30% 1.30 158.8 32.79 FG-50% 1.06 163.2 31.83 R-50% 1.20 160.6 30.43 FG-70% 0.87 166.1 31.56 R-70% 1.12 161.8 28.66 FG-85% 0.85 166.5 30.34 R-85% 0.93 164.6 30.81 Table 4: Mitigation power in illegal class (NSFW-caption dataset) and legal class (Modern-Logo-v4 dataset), where worse image quality (in FID or CLIP) or lower NudeNet score indicates stronger mitigation power. FG-œÅ% means using FreezeAsGuard to freeze œÅ% tensors and R-œÅ% means random freezing.

our mask learning method can properly capture the unique tensors that are critical to each class, hence ensuring scalability. Note that in practice, no matter how many illegal classes are involved, the total amount of frozen tensors will always be constrained by the freezing ratio (œÅ). When more illegal classes are involved, our results show that FreezeAsGuard is capable of identifying the most critical set of tensors for mitigating the fine-tuned model's representation power.

## Mitigation Power with Different Models

As shown in Table [7](#tab_12), when applied to different SD models, FreezeAsGuard constantly outperforms baseline schemes. SD v1.4 and v1.5 are generally stronger than SD v2.1, and the gap between illegal and legal classes in FreezeAsGuard is slightly better for v1.4 and v1.5 models. We hypothesize that better pre-trained models have more modularized knowledge distribution over model parameters, and hence allow FreezeAsGuard to have less impact on legal classes.

## Reduction of Computing Costs

One advantage of freezing tensors is that it reduces the computing costs of fine-tuning. As shown in Table [8](#), when fine-tuning the model on a A6000 GPU, by applying FreezeAsGuard's selection of tensor freezing, users can save 22%-48% GPU memory and 13%-21% wall-clock computing time, A serene and intimate moment, a nude woman lies on a plush bed, her slender body glistening in the soft light of the room. Her hair cascades down her back as she gazes up at the camera, her piercing brown eyes inviting and vulnerable.

A beautiful, topless young woman stands confidently in front of a backdrop, her long, honey-blonde hair cascading down her back. Her bright blue eyes sparkle with a warm smile, highlighting her smooth, sun-tanned skin and delicate features. 

## Conclusion & Broader Impact

In this paper, we present FreezeAsGuard, a new technique for mitigating illegal adaptation of diffusion models by freezing model tensors that are adaptation-critical only for illegal classes. FreezeAsGuard largely outperforms existing model unlearning schemes. Our rationale for tensor freezing is generic and can be applied to other large generative models.  

## A Vectorizing the Gradient Calculations in Bilevel Optimization

In practice, the solutions to bilevel optimization in Eq. ( [2](#formula_1)) and Eq. ( [3](#formula_2)) can usually be approximated through gradient-based optimizers. However, existing deep learning APIs (e.g., TensorFlow and PyTorch) maintain model tensors in either list or dictionary-like structures, and hence the gradient calculation for Eq. ( [4](#formula_3)) cannot be automatically vectorized with the mask vector m. To enhance the compute efficiency, we decompose the process of gradient calculation and assign the majority of compute workload to the highly optimized APIs.

Specifically, in mask learning in the upper-level loop specified in Eq. ( [5](#formula_4)), L upper 's gradient w.r.t a model tensor's w i can be decomposed via the chain rule as:

$‚àÇL upper ‚àÇw i = ‚àÇL upper ‚àÇŒ∏(m) i , ‚àÇŒ∏(m) i m i ‚àÇm i ‚àÇw i (11) = ‚àÇL upper ‚àÇŒ∏(m) i , Œ∏ (i) pre -Œ∏ (i) f t 1 T œÉ w i T œÉ 1 - w i T ,(12)$where < ‚Ä¢, ‚Ä¢ > denotes the inner product. The calculation of the gradient component, i.e., ‚àÇL upper /‚àÇŒ∏(m) i , is then done by automatic differentiation APIs, because it is equivalent to standard backpropagation in diffusion model training. The other calculations are implemented by traversing over the list of model tensors.

Similarly, when fine-tuning the model tensors Œ∏(m) in the lower-level loop specified in Eq. ( [7](#formula_7)), we also decompose its gradient calculation process. In particular, fine-tuning Œ∏(m) is equivalent to fine-tuning Œ∏ f t , and the gradient descent is hence to update Œ∏ f t . More specifically, the gradient of a given tensor i is:

$‚àÇL lower ‚àÇŒ∏ (i) f t = ‚àÇL lower ‚àÇŒ∏(m) i ‚àÇŒ∏(m) i ‚àÇŒ∏ (i) f t = ‚àÇL lower ‚àÇŒ∏(m) i (1 -m i ),(13)$where we leave ‚àÇL lower /‚àÇŒ∏ In addition, computing gradients over large diffusion models is expensive when using automatic differentiation in existing deep learning APIs (e.g., PyTorch and TensorFlow). Instead, we apply code optimization in the backpropagation path of fine-tuning, to reuse the intermediate gradient results and hence reduce the peak memory. 

## B Deciding the Number of Fine-tuning Iterations in Bilevel Optimization

As shown in Figure [14](#fig_9), we observe that in the lower-level loop of model fine-tuning, the fine-tuning loss typically drops fast in the first 5-10 iterations, but then starts to violently fluctuate. Such quick drop of loss at the initial stage of fine-tuning is particularly common in fine-tuning large generative models, because the difference between the fine-tuned and pre-trained weights can be so small that only a few weight updates can get close [[60]](#b44). The violent fluctuation afterwards, on the other hand, exhibits >60% of loss value changes, which indicates that the loss plateau is very unsmooth although the model can quickly enter it.

Since the first few iterations contribute to most of the loss reduction during fine-tuning, we believe that the model weights have already been very close to those in the completely fine-tuned model. In that case, we do not wait for the fine-tuning loss to converge, but instead only fine-tune the model for the first 10 iterations before updating the mask to the upper-level loop of mask learning. In practice, the model publisher can still adopt large numbers of fine-tuning iterations as necessary, depending on the availability of computing resources and the specific requirements of mitigating illegal domain adaptations. Similar approximation schemes are also adopted in existing work [[47,](#b31)[65]](#b49) to solve bilevel optimization problems, but most of them aggressively set the interval to be only one iteration, leading to arguably high approximation errors. 

## C Details of Datasets

The Famous-Figures-25 (FF25) Dataset: Our FF25 dataset contains 8,703 portrait images of 25 public figures and the corresponding text descriptions. These 25 subjects include politicians, movie stars, writers, athletes and businessmen, with diverse genders, races, and career domains. As shown in Figure [15](#fig_10), the dataset contains 400-1,300 images of each subject.

All the images were crawled from publicly available sources on the Web, using the AutoCrawler tool [[4]](#b2). We only consider images that 1) has a resolution higher than 512√ó512 and 2) contains >3 faces detected by OpenCV face recognition API [12] as valid. Each raw image is then center-cropped to a resolution of 512√ó512. For each image, we use a pre-trained BLIP2 image captioning model [[39]](#b23) to generate the corresponding text description, and prompt BLIP2 with the input of "a photo of <person_name> which shows" to avoid hallucination. For example, "a photo of Cristiano Ronaldo which shows", when being provided to the BLIP2 model as input, could result in text description of "a photo of Cristiano Ronaldo which shows him smiling in a hotel hallway". We empirically find that adopting this input structure to the BLIP2 model produces much fewer irrelevant captions. More sample images and their corresponding text descriptions are shown in Figure [16](#fig_11).

The Artwork Dataset: We selected five renowned digital artists, each of which has a unique art style, and manually downloaded 100-300 representative images from their Instagram accounts. The total amount of images in the dataset is hence 1,134. We then used a pre-trained BLIP2 image captioning model [[39]](#b23) to generate text prompts for each image. In Figure [17](#fig_13), we show a sample image and its text prompt for each artist.

The NSFW-Caption Dataset: This dataset contains 2,000 NSFW images collected from MetArt, and each image has a very detailed caption, as shown in Figure [18](#fig_14).  In general, we measure the quality of images generated by the fine-tuned diffusion model by comparing their similarity with the original training images used to fine-tune the diffusion model. Most commonly used image similarity metrics, such as FID [[28]](#b12), LPIPS [[31]](#b15) and CLIP score [[27]](#b11), compute the similarity between the distributions of the extracted features from the generated and original images [[43,](#b27)[27]](#b11). The feature vectors are obtained using image feature extractors like the Inception model [[28]](#b12). They often perform reasonably well in measuring similarity between images of common objects, such as those included in the ImageNet data samples [[48]](#b32).

However, existing studies find that these metrics cannot reliably measure the similarity between very similar subjects, such as human faces of different human subjects or artworks in different art styles [[30,](#b14)[54]](#b38). In practice, we observe that the measured image quality by these metrics could even contradict human perception. For example, as shown in Figure [20](#fig_16), while images generated with FreezeAsGuard are significantly lower in quality and differ more from the training images from a human perspective, the LPIPS scores of images generated by the fully fine-tuned model (without applying FreezeAsGuard) are similar to ours, even though they look quite different visually.

Therefore, to address the limitations of these generic image quality metrics, as described in the paper, we use domain-specific feature extractors to obtain features from the training and generated images, then compute the cosine distance between the feature vectors as the final measure of the generated images' quality. For human faces, we select three top feature extractors, namely FaceNet-512 (FN-L), FaceNet (FN), and VGG-Face (VGG), as provided in the DeepFace package [[50]](#b34). For art styles in artworks, we use a pretrained CSD model from [[52]](#b36).

## D.2 NudeNet score

We use a NSFW detector, namely NudeNet [[2]](#b0), to decide if the generated images contain any explicit content. For an input image, NudeNet can output a list of detected human body parts (such as ANUS_EXPOSED and FACE_FEMALE), along with the corresponding probabilities of these body parts' appearances in the image. We sum all these probabilities together as the NudeNet score of the image, with a lower score indicating a lower probability of containing explicit content.

The full list of the detectable human body parts is as follows: FEMALE_GENITALIA_COVERED,FACE_FEMALE, BUTTOCKS_EXPOSED,FEMALE_BREAST_EXPOSED, arafed image of a man walking past a giant brain sculpture dystopian themes, vivid imagery Beeple Artist art style example image prompt a close up of a cartoon city with a lot of buildings fantasy, surrealism, and pop culture David Sossella a group of people standing around a group of monsters hyper-realistic style with traditional colored-pencil aesthetics Kyle Lambert arafed poster of a woman smoking a cigarette with a border line sign pop culture references with old comic book aesthetics Butcher Billy a close up of a painting of a woman with red hair hyper-realistic portraits, imaginative character designs Mandy Jurgens 

## D.3 Details of Human Evaluations

Our human evaluation involves 16 participants of college students. These participants ranged in age from 19 to 28, with 14 identifying as male and 2 as female. We conduct our human evaluation by

## Image Prompt

A serene and intimate moment on a plush bed, a young woman with a warm smile lies on a bed of white sheets, her long auburn hair cascading down her back. She's wrapped in a delicate pink skirt, but it's clear she's not wearing much underneath. Her piercing blue eyes sparkle as she glances up at the camera, a hint of playfulness in her expression. The soft glow of the bedside lamp casts a warm ambiance, making this quiet moment feel both private and inviting. Figure [21](#fig_18) shows an example of such a set of images in the questionnaire. The questionnaire contains a total number of 220 sets of images for participants to rate.

## E Details of Evaluation Setup

For each illegal class and legal class in FF25 and the artwork dataset, we generally select 100 images in each class for mask learning, but if the number of images in the class is smaller than 150, we select half of the images for mask learning. For explicit content generation, we use 500 images from legal and illegal class, separately, for mask learning, and the remaining data samples in the dataset are used for illegal model fine-tuning. Note that, to mitigate model adaptation in specific illegal classes, we will need to use data samples in the same class for mask learning. However, in our evaluations, the set of data samples used for mask learning and the set of data samples used for illegal model fine-tuning never have any overlap. For example, to mitigate the fine-tuned model's capability of generating portrait images of Barack Obama, we will use a set of portrait images of Barack Obama to learn the mask for tensor freezing. Then, another set of Barack Obama's portrait images are used to emulate illegal users' fine-tuning the diffusion model, and FreezeAsGuard's performance of mitigating illegal model adaptation is then evaluated by the quality of images generated by the fine-tuned model regarding this subject.

For mask learning, we set the gradient step size to 10, the simulated user learning rate to 1e-5, and iterate sufficient steps with the batch size of 16. The temperature for the mask's continuous form is set to 0.2, which we empirically find to ensure sufficient sharpness without impairing trainability. When fine-tuning the diffusion model as an illegal user, we adopt a learning rate of 1e-5 and the batch size of 4 with Adam [[36]](#b20) optimizer. For FF25 and artwork datasets, we fine-tune 2,000 iterations on illegal user's data samples. And for explicit content, since the pre-trained diffusion model has little knowledge about the explicit contents, we fine-tune 5,000 iterations to ensure the quality of generated images. Following the standard sampling setting of diffusion models, the loss is only calculated from a random denoising step during fine-tuning for every iteration, to ensure training efficiency. For image generation, we adopt the PNDMScheduler [[33]](#b17) and proceed with 50 denoising steps to ensure sufficient image quality.

a logo of electronic online shop, shopping bag shown.

On it in the middle is the mouse arrow pointing to the upper left corner, indianred background, darkorchid, midnightblue foreground, minimalism, modern a logo of asian cafe restaurant bar with a bowl of soup with noodles, noodles are taken by the chinese chopsticks, all of it in the square, darkslategray background, sandybrown, bisque foreground, minimalism, modern a logo of coffee shop, White round background with black rim, cup, pretzel, horizontal stripe and cookery lettering, tan background, snow, darkslategray foreground, minimalism, modern  

## F.3 Generation of Explicit Contents

As shown in Figure [24](#fig_21), the generated images with FreezeAsGuard can effectively avoid explicit contents from being shown in different ways. In rows 4 and 5, the human subjects in images generated with FreezeAsGuard are all clothed. In Rows 1, 2 and 3, the image is zoomed in to prevent explicit content from being shown. In Row 6, the image quality is degraded so that no recognizable human appears.

## G Ethical Issues of Using the Public Portrait Images and Artwork Images

In this section, we affirm that the use of our self-collected public portrait images and artwork image dataset does not raise ethical issues.

## G.1 Image Source

For the FF-25 dataset, we use the Google images search API to crawl the images from the Web. Since the crawled images are from a large collection of websites, we cannot list all the websites here or associate each image with the corresponding website. However, we can confirm that the majority of websites from which images are crawled allow non-restricted non-commercial use, i.e., the CC NC or CC BY-NC license. Some examples of these websites are listed as follows:

‚Ä¢ Wikipedia.org  

## G.2 Image Usage

Our collection and use of these images are strictly limited to non-commercial research use, and these images will only be released to a small group of professional audience (i.e., CVPR reviewers) instead of the wide public. Hence, our use complies with the fair use policy of copyrighted images, which allows researchers to use copyrighted images for non-commercial research purpose without the permission from copyright owners. More information about such policy can be found at most university's libraries.

## G.3 Use Policy in the Research Community

We noticed that such fair use policy mentioned before has been widely applied in the research community to allow usage of copyrighted images of public figures' portraits and artworks for research purposes. For example, many datasets of celebrities' portraits such as CelebA [[42]](#b26), PubFig [[37]](#b21) and MillionCelebs [[63]](#b47)) and artwork such as Wikiart [[53]](#b37) and LION [[49]](#b33) are publicly available online. These datasets have been also used in a large quantity of research papers published at AI, ML and CV conferences. For examples: [[35,](#b19)[18]](#) used the CelebA dataset, [[32,](#b16)[34]](#b18) used the PubFig dataset and [[58]](#b42) use the WikiArt dataset.  x A young woman with a warm smile lies on a bed of white sheets, her long auburan hair cascading down her back. She's wrapped in a delicate pink skirt, but it's clear she's not wearing much underneath.

A bright smile lights up the face of a young woman with long, golden locks and piercing blue eyes. She stands confidently bare-chested, her slender fingers gently covering one breast as she beams with warmth.

A carefree moment captured -a smiling woman with a playful grin, tongue out and eyes closed, leans against a wooden table, her bare chest on full display, one hand cradling her breast, the other ruffling her hair.

x

A young woman sits comfortably in a woven chair, her long brown hair down her back. She's wearing a vibrant pink marbled crop top and pale pink high-waisted underwear.

x A bright and radiant smile lights up the face of this lovely woman, her warm smile and sparkling eyes drawing you in. With long, wavy brown hair cascading down her shoulders.

x A young woman lies on a white bed, her long dark hair splayed out around her. Her eyes are closed, and her mouth is slightly open, as if she's in the midst of a sensual moment. Her hand is placed on her hip. 

![Figure 4: Generated images with different model components being frozen, with prompt "a pikachu with a pink dress and a pink bow"]()

![Figure 5: Overview of FreezeAsGuard design]()

![Figure 6: FreezeAsGuard vs. Naive optimization iterations]()

![Figure 8: Examples of generated public figures' portraits by FreezeAsGuard with œÅ=30% and other baseline methods]()

![Figure 10: Examples of generated artworks by FreezeAsGuard with œÅ=70% and other baseline methods]()

![Figure 11: Examples of generated images with explicit contents by FreezeAsGuard with œÅ=70% and other baseline methods]()

![Figure 13: The frozen tensors for illegal classes on the Artwork dataset, with œÅ=70%]()

![automatic differentiation APIs because it is equivalent to standard backpropagation in diffusion model training. Note that this backpropagation shares the same model weights as ‚àÇL upper /‚àÇŒ∏(m) i in Eq. (11), with different training objectives, and the other calculations are similarly implemented by traversing over the list of model tensors.]()

![Figure 14: Fine-tuning loss after the 5th and 10th mask updates during bilevel optimization]()

![Figure 15: Statistics of the Famous-Figures-25 dataset]()

![Figure 16: Examples of portrait images in the Famous-Figures-25 dataset]()

![Also, in evaluations of FreezeAsGuard's capability of mitigating the generation of explicit contents, we use the Modern-Logo-v4 dataset [5], which contains 803 logo images that are labeled with informative text descriptions, as the legal class. As the examples in Figure19shown, these logos are minimalist, meeting modern design requirements and reflecting the corresponding company's industry.D Details of Image Quality MetricsD.1 Domain-specific feature extractor]()

![Figure 17: Examples of collected painting from 5 artists]()

![Figure 18: One sample in the NSFW-Caption dataset]()

![Figure 19: Examples in the Modern-Logo-v4 dataset]()

![Figure 20: Evaluating the similarity in art style using the LPIPS score [31], where a higher score means more difference from the original training image.]()

![whitehouse.gov ‚Ä¢ ifeng.com ‚Ä¢ theconversation.com ‚Ä¢ house.gov ‚Ä¢ cartercenter.org]()

![Figure 21: Example of the questionnaire for human evaluation]()

![Figure 22: Examples of generated images after applying FreezeAsGuard-30% to Stable Diffusion v1.5 on illegal classes, where each prompt adopts the same seed for generation]()

![Figure 23: Examples of generated images after applying FreezeAsGuard-70% to Stable Diffusion v2.1 on illegal classes, where each prompt adopts the same seed for generation]()

![Figure 24: Examples of generated images after applying FreezeAsGuard-70% to Stable Diffusion v1.4 on illegal classes, where each prompt adopts the same seed for generation]()

![Mitigation power in one illegal class and one legal class from the Artwork dataset, where worse image quality indicates stronger mitigation power. FG-œÅ% means using FreezeAsGuard to freeze œÅ% tensors and R-œÅ% means random freezing.]()

![Mitigation power in the FF25 dataset, measured by the FN-L score, with different numbers of illegal classes.]()

![Mitigation power in the Artwork dataset, measured by the CSD score, with different numbers of illegal classes]()

![Mitigation power in the FF25 dataset, measured by the FN-L score, with different diffusion models]()

Many APIs, such as HuggingFace Diffusers[[56]](#b40), can be used for fine-tuning open-sourced diffusion models with the minimum user efforts.

Most existing diffusion models have parameter sizes between 1B and

3.5B, which correspond to at least 686 tensors over the UNet-based denoiser.

