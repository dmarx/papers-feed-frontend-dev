- Decision to focus on pixel-space diffusion models (PDMs) rather than latent diffusion models (LDMs)
- Choice of adversarial attack methods to evaluate against PDMs
- Assumption that existing adversarial attacks on LDMs would not transfer to PDMs
- Decision to use PDMs as a purification method for adversarial perturbations
- Selection of network architectures (e.g., U-Net, Transformer) for experiments
- Choice of training datasets for evaluating PDMs and LDMs
- Decision to analyze different input resolutions in experiments
- Assumption that adversarial robustness of PDMs can be generalized across various models
- Decision to propose PDM-Pure as a universal purifier
- Choice of performance metrics for evaluating the effectiveness of PDM-Pure
- Decision to make code publicly available for reproducibility
- Assumption that existing protection methods can be easily bypassed by PDM-based purification
- Decision to conduct extensive experiments to validate findings
- Choice to highlight the implications of findings for intellectual property protection
- Decision to structure the paper around the gap in existing literature regarding PDMs and adversarial attacks