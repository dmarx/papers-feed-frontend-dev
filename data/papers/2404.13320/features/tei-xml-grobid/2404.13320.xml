<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than We Think</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haotian</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
							<email>yongchen@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than We Think</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EE3420716C1F5EA0DA15D76F89A9AF51</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion models have demonstrated an impressive capability to edit or imitate images, which has raised concerns regarding the safeguarding of intellectual property. To address these concerns, the adoption of adversarial attacks, which introduce adversarial perturbations into protected images, has proven successful. Consequently, diffusion models, like many other deep network models, are believed to be susceptible to adversarial attacks. However, in this work, we draw attention to an important oversight in existing research, as all previous studies have focused solely on attacking latent diffusion models (LDMs), neglecting adversarial examples for diffusion models in the pixel space (PDMs). Through extensive experiments, we demonstrate that nearly all existing adversarial attack methods designed for LDMs fail when applied to PDMs. We attribute the vulnerability of LDMs to their encoders, indicating that diffusion models exhibit strong robustness against adversarial attacks. Building upon this insight, we propose utilizing PDMs as an off-the-shelf purifier to effectively eliminate adversarial patterns generated by LDMs, thereby maintaining the integrity of images. Notably, we highlight that most existing protection methods can be easily bypassed using PDM-based purification. We hope our findings prompt a reevaluation of adversarial samples for diffusion models as potential protection methods. Codes are available in <ref type="url" target="https://github.com/xavihart/PDM-Pure">https://github.com/xavihart/PDM-Pure</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative diffusion models (DMs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b32">32]</ref> have achieved great success in generating images with high fidelity. However, this remarkable generative capability of diffusion models is accompanied by safety concerns <ref type="bibr" target="#b44">[44]</ref>, especially on the unauthorized editing or imitation of personal images such as portraits or individual artworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">36]</ref>. Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">22]</ref> show that adversarial samples (adv-samples) for diffusion models can be applied as a protection against malicious editing. Small perturbations generated by conventional methods in adversarial machine learning <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b10">11]</ref> can effectively fool popular diffusion models such as Stable Diffusion <ref type="bibr" target="#b32">[32]</ref> to produce chaotic results when an imitation attempt is made. However, a significantly overlooked aspect is that all the existing works focus on latent diffusion models (LDMs) and the pixel-space diffusion models (PDMs) are not studied. For LDMs, perturbations are not directly introduced to the input of the diffusion models. Instead, they are applied externally and propagated through an encoder. It has been shown that the encoder-decoder of LDMs is vulnerable to adversarial perturbations <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b42">42]</ref>, which means that the adv-samples for LDMs have a very different mechanism compared with the adv-samples for PDMs. Moreover, some existing works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">33]</ref> show that combining encoder-specific loss can enhance the adversary, <ref type="bibr" target="#b42">[42]</ref> further demonstrating that the encoder is the bottleneck for attacking LDMs. Building upon this observation, in this paper, we draw attention to rethink existing adversarial attack methods for diffusion models:  We address this question by systematically investigating adv-samples for PDMs. We conduct experiments on various LDMs or PDMs with different network architectures (e.g. U-Net <ref type="bibr" target="#b13">[14]</ref> or Transformer <ref type="bibr" target="#b28">[28]</ref>), different training datasets, and different input resolutions (e.g. 64, 256, 512). Through extensive experiments, we demonstrate that all the existing methods we tested <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b19">20]</ref>, targeting to attack LDMs, fail to generate effective adv-samples for PDMs. This implies that PDMs are more adversarial robust than we think.</p><p>Building on this insight that PDMs are strongly robust against adversarial perturbations, we further propose PDM-Pure, a universal purifier that can effectively remove the protective perturbations of different scales (e.g. Mist-v2 <ref type="bibr" target="#b50">[50]</ref> and Glaze <ref type="bibr" target="#b37">[37]</ref>) based on PDMs trained on large datasets. Through extensive experiments, we demonstrate that PDM-Pure achieves way better performance than all baseline methods.</p><p>To summarize, the pixel is a barrier to adversarial attack; the diffusion process in the pixel space makes PDMs much more robust than LDMs. This property of PDMs also makes real protection against the misusage of diffusion models difficult since all the existing protections can be easily purified using a strong PDM. Our contributions are listed below.</p><p>1. We observe that most existing works on adversarial examples for protection focus on LDMs. Adversarial attacks against PDMs are largely overlooked in this field. 2. We fill in the gap in the literature by conducting extensive experiments on various LDMs and PDMs. We discover that all the existing methods fail to attack the PDMs, indicating that PDMs are much more adversarially robust than LDMs. 3. Based on this novel insight, we propose a simple yet effective framework termed PDM-Pure that applies strong PDMs as a universal purifier to remove attack-agnostic adversarial perturbations, easily bypassing almost all existing protective methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Safety Issues in Diffusion Models The impressive generative capability of the diffusion models has raised numerous safety issues <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b1">2]</ref>. As a result, there has been a growing interest in preventing DMs from being abused. Some of the existing works focus on the protection of intellectual property of diffusion models by applying watermarks <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b5">6]</ref> and some of them are on concept removal to prevent the DMs from generating NSFW images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b9">10]</ref>. In the era of generative models, caution should be taken to guarantee safe and responsible applications of these models.</p><p>Adversarial Examples for DMs Adversarial samples <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">37]</ref> are clean samples perturbed by an imperceptible small noise that can fool the deep neural networks into making wrong decisions. Under the white-box settings, gradient-based methods are widely used to generate adv-samples. Among them, the projected gradient descent (PGD) algorithm <ref type="bibr" target="#b23">[23]</ref> is one of the most effective methods.</p><p>Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">33]</ref> show that it is also easy to find adv-samples for diffusion models (AdvDM): with a proper loss to attack the denoising process, the perturbed image can fool the diffusion models to generate chaotic images when operating diffusion-based mimicry. Furthermore, many improved</p><p>𝑥 !"# SDEdit(𝑥 !"# ) SDEdit(𝑥) DiT-256 GD 𝑥 !"# SDEdit(𝑥 !"# ) SDEdit(𝑥) SD-V-1.5 IF-Stage II PDM, Not Attacked LDM, Attacked LDM, Attacked PDM, Not Attacked (b) Using End-to-End Attack for PDMs (Strongest but impractical to apply) (a) LDM vs PDM Under Adversarial Attack 𝜃 𝜃 𝜃 𝜃 … ℒ( ) , ∇ $ ||Edit 𝑥, 𝜃 -𝑦|| % % 𝑥 Edit 𝑥, 𝜃 𝑦 𝑥 !"# SDEdit(𝑥 !"# ) End-to-End attack Fails on PDMs algorithms <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">42]</ref> have been proposed to generate better AdvDM samples. However, to our best knowledge, all the AdvDM methods listed above are used on LDMs, and those for the PDMs are rarely explored.</p><p>Adversarial Perturbation as Protection Adversarial perturbation against DMs turns out to be an effective method to safeguard images against unauthorized editing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">22]</ref>. It has found applications (e.g., Glaze <ref type="bibr" target="#b37">[37]</ref> and Mist <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b18">19]</ref>) for individual artists to protect their creations. SDS-attack <ref type="bibr" target="#b42">[42]</ref> further investigates the mechanism behind the attack and proposes some tools to make the protection more effective. However, they are limited to protecting LDMs only. In addition, some works <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b34">34]</ref> find that these protective perturbations can be purified. For instance, GrIDPure <ref type="bibr" target="#b49">[49]</ref> find that DiffPure <ref type="bibr" target="#b26">[26]</ref> can be used to purify the adversarial patterns, but they did not realize that the reason behind this is the robustness of PDMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Generative Diffusion Models The generative diffusion model <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">40]</ref> is one type of generative model, and it has demonstrated remarkable generative capability in numerous fields such as image <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b2">3]</ref>, 3D <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b20">21]</ref>, video <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">39]</ref>, story <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b31">31]</ref> and music <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b16">17]</ref> generation. Diffusion models, like other generative models, are parametrized models p θ (x 0 ) that can estimate an unknown distribution q(x 0 ). For image generation tasks, q(x 0 ) is the distribution of real images.</p><p>There are two processes involved in a diffusion model, a forward diffusion process and a reverse denoising process. The forward diffusion process progressively injects noise into the clean image, and the t-th step diffusion is formulated as</p><formula xml:id="formula_0">q(x t | x t-1 ) = N (x t ; √ 1 -β t x t-1 , β t I). Accumulating the noise, we have q t (x t | x 0 ) = N (x t ; √ ᾱt x t-1 , (1 -ᾱt )I).</formula><p>Here β t growing from 0 to 1 are pre-defined values, α t = 1 -β t , and ᾱt = Π t s=1 α s . Finally, x T will become approximately an isotropic Gaussian random variable when ᾱt → 0.</p><p>Reversely, p θ (x t-1 |x t ) can generate samples from Gaussian xT ∼ N (0, I), where p θ be reparameterized by learning a noise estimator ϵ θ , the training loss is</p><formula xml:id="formula_1">E t,x0,ϵ [λ(t)∥ϵ θ (x t , t) -ϵ∥ 2 ]</formula><p>weighted by λ(t), where ϵ is the noise used to diffuse x 0 following q t (x t |x 0 ). Finally, by iteratively applying p θ (x t-1 |x t ), we can sample realistic images following p θ (x 0 ).</p><p>Since the above diffusion process operates directly in the pixel space, we call such diffusion models Pixel-Space Diffusion Models (PDMs). Another popular choice is to move the diffusion process into the latent space to make it more scalable, resulting in the Latent Diffusion Models (LDMs) <ref type="bibr" target="#b32">[32]</ref>. More specifically, LDMs first use an encoder E ϕ parameterized by ϕ to encode x 0 into a latent variable z 0 = E ϕ (x 0 ). The denoising diffusion process is the same as PDMs. At the end of the denoising process, ẑ0 can be projected back to the pixel space using decoder D ψ parameterized by ψ as x0 = D ψ (ẑ 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Examples for Diffusion Models</head><p>Recent works <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b19">20]</ref> find that adding small perturbations to clean images will make the diffusion models perform badly in noise prediction, and further generate chaotic results in tasks like image editing and customized generation. The adversarial perturbations for LDMs can be generated by optimizing the Monte-Carlo-based adversarial loss:</p><formula xml:id="formula_2">L adv (x) = E t,ϵ E zt∼qt(E ϕ (x)) ∥ϵ θ (z t , t) -ϵ∥ 2 2 .<label>(1)</label></formula><p>Other encoder-based losses <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b42">42]</ref> further enhance the attack to make it more effective. With the carefully designed adversarial loss, we can run Projected Gradient Descent (PGD) <ref type="bibr" target="#b23">[23]</ref> with ℓ ∞ budget δ to generate adversarial perturbations:</p><formula xml:id="formula_3">x k+1 = P B∞(x 0 ,δ) x k + η sign∇ x k L adv (x k )<label>(2)</label></formula><p>In the above equation, P B∞(x 0 ,δ) (•) is the projection operator on the ℓ ∞ ball, where x 0 is the clean image to be perturbed. We use superscript x k to represent the iterations of the PGD and subscript x t for the diffusion steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rethink Adversarial Examples for Diffusion Models</head><p>Adversarial examples of LDMs are widely adopted as a protection mechanism to prevent unauthorized images from being edited or imitated <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b18">19]</ref>. However, a significant issue overlooked is that all the adversarial examples in existing work are generated using LDMs, primarily due to the wide impact of the Stable Diffusion; no attempts have been made to attack PDMs.</p><p>This lack of investigation may mislead us to conclude that diffusion models, like most deep neural networks, are vulnerable to adversarial perturbations, and that the algorithms used in LDMs can be transferred to PDMs by simply applying the same adversarial loss in the pixel space formulated as:  </p><formula xml:id="formula_4">𝒒 𝒕 * (</formula><formula xml:id="formula_5">L adv (x) = E t,ϵ E xt∼qt(x) ∥ϵ θ (x t , t) -ϵ∥ 2 2</formula><p>(3) However, we show through experiments that PDMs are robust against this form of attack (Figure <ref type="figure" target="#fig_2">2</ref>), which means all the existing attacks against diffusion models are, in fact, special cases of attacks against the LDMs only. Prior to this study, there may have been a prevailing belief that diffusion models could be easily deceived. However, our research reveals an important distinction: it is the LDMs that exhibit vulnerability, while the PDMs demonstrate significantly higher adversarial robustness. We conduct extensive experiments on popular LDMs and PDMs structures including DiT, Guided Diffusion, Stable Diffusion, and DeepFloyd, and demonstrate in Table <ref type="table" target="#tab_1">1</ref> that only the LDMs can be attacked and PDMs are not that susceptible to adversarial perturbations. More details and analysis can be found in the experiment section.</p><p>The vulnerability of the LDMs is caused by the vulnerability of the latent space <ref type="bibr" target="#b42">[42]</ref>, meaning that although we may set budgets for perturbations in the pixel space, the perturbations in the latent space can be large. In <ref type="bibr" target="#b42">[42]</ref>, the authors show statistics of perturbations in the latent space over the perturbations in the pixel space and this value |δz| |δx| can be as large as 10. In contrast, the PDMs directly work in the pixel space, and thus the injected noise combined with the random Gaussian noise will not easily fool the denoiser as it is trained to be robust to Gaussian noise of different levels.</p><p>Almost all the copyright protection perturbations <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b50">50]</ref> are based on the insight that it is easy to craft adversarial examples to fool the diffusion models. We need to rethink the adversarial samples of diffusion models since there are a lot of PDMs that cannot be attacked easily. Next, we show that PDMs can be utilized to purify all adversarial patterns generated by existing methods in Section 5. This new landscape poses new challenges to ensure the security and robustness of diffusion-based copyright protection techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PDM-Pure: PDM as a Strong Universal Purifier</head><p>Given the robustness of PDMs, a natural idea emerges: we can utilize PDMs as a universal purification network. This approach could potentially eliminate any adversarial patterns without knowing the nature of the attacks. We term this framework PDM-Pure, which is a general framework to deal with all the perturbations nowadays. To fully harness the capabilities of PDM-Pure, we need to fulfill two basic requirements: <ref type="bibr" target="#b0">(1)</ref> The perturbation shows out-of-distribution pattern as reflected in existing works on adversarial purification/attacks using diffusion models <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b43">43]</ref> (2) The PDM being used is strong enough to represent p(x 0 ), which can be largely determined by the dataset they are trained on.</p><p>It is effortless to design a PDM-Pure. The key idea behind this method is to run SDEdit in the pixel space. Given any strong pixel-space diffusion model, we add a small noise to the protected images and run the denoising process (Figure <ref type="figure" target="#fig_4">3</ref>), and then the adversarial pattern should be removed. The key idea of PDM-Pure is simple. In practice, we need to adjust the pipeline to fit the resolution of the PDMs being used.  <ref type="bibr" target="#b24">[24]</ref> using the Stage II model:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AdvDM AdvDM(-) SDS(-) SDS(+) SDST</head><formula xml:id="formula_6">x t = IF-II(x t+1 , x 64×64 , P)<label>(4)</label></formula><p>where t = T edit -1, ..., 1, 0, x Tedit = x 256×256 . A larger T edit may be used for larger noise. x 0 is the purified image we get in the 256 × 256 resolution space, where the adversarial patterns should be already purified. We can then use IF Stage III to further up-sample it into 1024 × 1024 with x 1024×1024 = IF-III(x 0 , p). Finally, we can sample into H × W as we want through downsampling. This whole process is demonstrated in Figure <ref type="figure" target="#fig_4">3</ref>. After purification, the image is no longer adversarial to the targeted diffusion models and can be effectively used in downstream tasks.</p><p>In the main paper, we conduct experiments on purifying protected images sized 512 × 512. For images with a larger resolution, purifying in the resolution of 256 × 256 may lose information. In Appendix F we show PDM-Pure can also applied to purify patches of high-resolution inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we conduct experiments on various attacking methods and various models to support the following two conclusions:</p><p>• (C1): PDMs are much more adversarial robust than LDMs, and PDMs can not be effectively attacked using all the existing attacks for LDMs. • (C2): PDMs can be applied to effectively purify all of the existing protective perturbations. Our PDM-Pure based on DeepFloyd-IF shows state-of-the-art purification power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Models, Datasets, and Metrics</head><p>The models we used can be categorized into LDMs and PDMs. For LDMs, we use Stable Diffusion V-1.4, V-1.5 (SD-V-1.4, SD-V-1.5) <ref type="bibr" target="#b32">[32]</ref>, and Diffusion Transformer (DiT-XL/2) <ref type="bibr" target="#b28">[28]</ref>, and for PDMs we use Guided Diffusion (GD) <ref type="bibr" target="#b7">[8]</ref> trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>, and DeepFloyd Stage I and Stage II <ref type="bibr" target="#b38">[38]</ref>.</p><p>For models trained on the ImageNet (DiT, GD), we run adversarial attacks and purification on a 1k subset of the ImageNet validation dataset. For models trained on LAION, we run tests on the dataset proposed in <ref type="bibr" target="#b42">[42]</ref>, which includes 400 cartoon, artwork, landscape, and portrait images. The metrics for testing the quality of generated images are included in the Appendix.</p><p>For protection methods, we consider almost all the representative approaches, including AdvDM <ref type="bibr" target="#b19">[20]</ref>, SDS <ref type="bibr" target="#b42">[42]</ref>, Mist <ref type="bibr" target="#b18">[19]</ref>, Mist-v2 <ref type="bibr" target="#b50">[50]</ref>, Photoguard <ref type="bibr" target="#b33">[33]</ref> and Glaze <ref type="bibr" target="#b37">[37]</ref>. We also test the methods in the design space proposed in <ref type="bibr" target="#b42">[42]</ref>, including SDS(-), AdvDM(-), and SDST. In contrast to other existing methods, they are based on gradient descent and have shown great performance in deceiving the LDMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">(C1) PDMs are Much More Robust Than We Think</head><p>In Table <ref type="table" target="#tab_1">1</ref>, we attack different LDMs and PDMs with one of the most popular adversarial loss <ref type="bibr" target="#b50">[50]</ref> in Equation 1 and Equation 3, which can be interpreted as fooling the denoiser using a Monte-Carlobased loss. Given the attacked samples, we test the SDEdit results on the attacked samples, which can be generally used to test whether the samples are adversarial for the diffusion model or not. We use FID-score <ref type="bibr" target="#b12">[13]</ref>, SSIM <ref type="bibr" target="#b41">[41]</ref>, LPIPS <ref type="bibr" target="#b47">[47]</ref>, and IA-Score <ref type="bibr" target="#b17">[18]</ref> to measure the quality of the attack.</p><p>If the quality of generated images decreases a lot compared with editing the clean images, then the attack is successful. We can see that LDMs can be easily attacked, while PDMs are quite robust; the quality of the edited images is still good. We also show some visualizations in Figure <ref type="figure" target="#fig_2">2</ref>, which illustrates that the perturbation will affect the LDMs but not the PDMs.</p><p>To further investigate how robust PDM is, we test other advanced attacking methods, including the End-to-End Diffusion Attacks (E2E-Photoguard) proposed in <ref type="bibr" target="#b33">[33]</ref> and the Improved Targeted Attack (ITA) proposed in <ref type="bibr" target="#b50">[50]</ref>. Though the End-to-End attack is usually impractical to run, it shows the strongest performance to attack LDMs. We find that both attacks are not successful in PDM settings. We show attacked samples and edited samples in Figure <ref type="figure" target="#fig_2">2</ref> as well as the Appendix. In conclusion, existing adversarial attack methods for diffusion models can only work for the LDMs, and PDMs are more robust than we think.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">(C2) PDM-Pure: A Universal Purifier that is Simple yet Effective</head><p>PDM-Pure is simple: basically, we just run SDEdit to purify the protected image in the pixel space. Given our assumption that PDMs are quite robust, we can use PDMs trained on large-scale datasets as a universal black-box purifier. We follow the model pipeline introduced in Section 5 and purify images protected by various methods in Table <ref type="table" target="#tab_2">2</ref>.</p><p>PDM-Pure is effective: from Table <ref type="table" target="#tab_2">2</ref> we can see that the purification will remove adversarial patterns for all the protection methods we tested, largely decreasing the FID score for the SDEdit task. Also, we test the protected images and purified images in more tasks including Image Inpainting <ref type="bibr" target="#b40">[40]</ref>, Textual-Inversion <ref type="bibr" target="#b8">[9]</ref>, and LoRA customization <ref type="bibr" target="#b15">[16]</ref> in Figure <ref type="figure" target="#fig_7">4</ref>. Both qualitative and quantitative results show that the purified images are no more adversarial and can be effectively edited or imitated in different tasks without any obstruction.</p><p>Also, PDM-Pure shows SOTA results compared with previous purification methods, including some simple purifiers based on compression and filtering like Adv-Clean, crop-and-resize, JPEG Compression, and SDEdit-based methods like GrIDPure <ref type="bibr" target="#b49">[49]</ref>, which uses patchified SDEdit with a GD <ref type="bibr" target="#b7">[8]</ref>. We also add LDM-Pure as a baseline to show that LDMs can not be used to purify the protected images. For GrIDPure, we use Guided-Diffusion trained on ImageNet to run patchified purification. All the experiments are conducted on the datasets collected in <ref type="bibr" target="#b42">[42]</ref> under the resolution of 512 × 512. Results for higher resolutions are presented in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Directions</head><p>In this paper, we present novel insights that while many studies demonstrate the ease of finding adversarial samples for Latent Diffusion Models (LDMs), Pixel Diffusion Models (PDMs) exhibit far greater adversarial robustness than previously assumed. We are the first to investigate the adversarial samples for PDMs, revealing a surprising discovery that existing attacks fail to fool PDMs. Leveraging this insight, we propose utilizing strong PDMs as universal purifiers, resulting in PDM-Pure, a simple yet effective framework that can generate protective perturbations in a black-box manner.</p><p>Pixel is a barrier for us to do real protection against adversarial attacks. Since PDMs are quite robust, they cannot be easily attacked. PDMs can even be used to purify the protective perturbations, challenging the current assumption for the safe protection of generative diffusion models. We advocate rethinking the problem of adversarial samples for generative diffusion models and unauthorized image protection based on it. More rigorous study can be conducted to better understand the mechanism behind the robustness of PDMs. Furthermore, we can utilize it as a new structure for many other tasks   Contents 1 Introduction 1 2 Related Works 2 3 Preliminaries 3 4 Rethink Adversarial Examples for Diffusion Models 4 5 PDM-Pure: PDM as a Strong Universal Purifier 5 6 Experiments 6 6.1 Models, Datasets, and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 6.2 (C1) PDMs are Much More Robust Than We Think . . . . . . . . . . . . . . . . . 7 6.3 (C2) PDM-Pure: A Universal Purifier that is Simple yet Effective . . . . . . . . . . 7 7 Conclusions and Future Directions 7 A Details about Different Diffusion Models in this Paper 13 B Details about Different Protection Methods in this Paper 13 C Details about The Evaluation Metrics 14 D Details about Different Purification Methods 14 E More Experimental Results 15 E.1 More Visualizations of Attacking PDMs . . . . . . . . . . . . . . . . . . . . . . . 15 E.2 More Visualizaitons of PDM-Pure and Baseline Methods . . . . . . . . . . . . . . 15 E.3 More Visualizaitons of PDM-Pure for Downstreaming Tasks . . . . . . . . . . . . 15 F PDM-Pure For Higher Resolution 15 G Ablations of t * in PDM-Pure 19</p><p>Mist Mist <ref type="bibr" target="#b18">[19]</ref> finds that L T (x) can better enhance the attacks if the target image y is chosen to be periodical patterns, the final loss combined L T (x) and L S (x):</p><formula xml:id="formula_7">L = λL T (x) + L S (x)<label>(7)</label></formula><p>SDS(+) Proposed in <ref type="bibr" target="#b42">[42]</ref>, it is proven to be a more effective attack compared with the original AdvDM, where the gradient ∇ x L(x) is expensive to compute. By using the score distillation-based loss, it shows good performance and remains effective at the same time:</p><formula xml:id="formula_8">∇ x L SDS (x) = E t,ϵ E zt λ(t)(ϵ θ (z t , t) -ϵ) ∂z t ∂x t<label>(8)</label></formula><p>SDS(-) Similar to SDS(+), it swaps gradient ascent in the original PGD with gradient descent, which turns out to be even more effective.</p><formula xml:id="formula_9">∇ x L SDS(-) (x) = -E t,ϵ E zt λ(t)(ϵ θ (z t , t) -ϵ) ∂z t ∂x t<label>(9)</label></formula><p>Mist-v2 It was proposed in <ref type="bibr" target="#b50">[50]</ref> using the Improved Targeted Attack (ITA), which turns out to be very effective, especially when the limit budget is small. It is also more effective to attack LoRA:</p><formula xml:id="formula_10">L S (x) = E t,ϵ E zt∼qt(E ϕ (x)) ∥ϵ θ (z t , t) -z 0 ∥ 2 2<label>(10)</label></formula><p>where z 0 = E(y) is the latent of a target image, which is the same as the typical image used in Mist.</p><p>Glaze It is the most popular protection claimed to safeguard artists from unauthorized imitation <ref type="bibr" target="#b37">[37]</ref> and is widely used by the community. while it is not open-sourced, it also attacks the encoder like the Photoguard. Here we only test it in the purification stage, where we show that the protection can also be bypassed.</p><p>End-to-End Attack It is also first proposed in <ref type="bibr" target="#b33">[33]</ref>, which attacks the editing pipeline in a end-toend manner. Although it is strong, it is not practical to use and does not show dominant privilege compared with other protection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details about The Evaluation Metrics</head><p>Here we introduce the quantitative measurement we used in our experiments:</p><p>• We measure the SDEdit results after the adversarial attacks using Fréchet Inception Distance (FID) <ref type="bibr" target="#b12">[13]</ref> over the relevant datasets (for model trained on ImageNet such as GD <ref type="bibr" target="#b7">[8]</ref> and DiT <ref type="bibr" target="#b28">[28]</ref> we use a sub-dataset of ImageNet as the relevant dataset, for those trained on LAION, we use the collected dataset to calculate the FID). We also use Image-Alignment Score (IA-score) <ref type="bibr" target="#b17">[18]</ref>, which can be used to calculate the cosine-similarity between the CLIP embedding of the edited image and the original image. Also, we use some basic evaluations, where we calculate the Structural Similarity (SSIM) <ref type="bibr" target="#b41">[41]</ref> and Perceptual Similarity (LPIPS) <ref type="bibr" target="#b47">[47]</ref> compared with the original images.</p><p>• To measure the purification results, we test the Fréchet Inception Distance (FID) <ref type="bibr" target="#b12">[13]</ref> over the collected dataset compared with the dataset generated by running SDEdit over the purified images in the strength of 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details about Different Purification Methods</head><p>Adv-Clean: <ref type="url" target="https://github.com/lllyasviel/AdverseCleaner">https://github.com/lllyasviel/AdverseCleaner</ref>, a training-free filter-based method that can remove adversarial noise for a diffusion model, it works well to remove highfrequency noise. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>PhotoGuard</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pixel is a Barrier for Attacking DMs: (a) Pixel-based diffusion models are harder to attack using white-box attacks like project-gradient-descent than diffusion models in the latent space. (b) Strong PDM can be used as a universal purifier to effectively remove the protective perturbation generated by existing protection methods. (c) Pixel is a barrier and the pixel-space diffusion model is quite robust, and we cannot achieve real safety and protection if pixel-space diffusion is not attacked.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PDMs Cannot be Attacked as LDMs: (a) LDMs can be easily fooled but PDMs cannot be. (b) Even End-to-End attack does not work on PDMs. (Best viewed with zoom-in)</figDesc><graphic coords="3,258.31,211.67,181.67,90.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>)</head></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PDM-Pure is Easy to Design: (a) PDM-Pure applies SDEdit<ref type="bibr" target="#b24">[24]</ref> in the pixel space: it first runs forward diffusion with a small step t * and then runs denoising process. (b) We adapt the framework to DeepFloyd-IF<ref type="bibr" target="#b38">[38]</ref>, one of the strongest PDMs. PDM-Pure can effectively remove strong protective perturbations (e.g. δ = 16/255). The images we tested are sized 512 × 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>man and a dog.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>man and a dog.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PDM-Pure makes the Protected Images no more Protected: Here we show qualitative results of PDM-Pure on three scenarios where unauthorized editing may occur: (a) Inpainting, (b)Text-Inversion<ref type="bibr" target="#b8">[9]</ref> and (c) LoRA customization<ref type="bibr" target="#b15">[16]</ref>. While the protected images incur bad generation quality, the purified ones can fully bypass the protection.</figDesc><graphic coords="8,343.99,578.51,76.81,76.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: PDM-Pure with Different t *</figDesc><graphic coords="21,303.27,445.69,77.23,77.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantiative</figDesc><table><row><cell>Models</cell><cell cols="2">FID-score↑</cell><cell></cell><cell>SSIM ↓</cell><cell></cell><cell>LPIPS ↑</cell><cell></cell><cell>IA-Score ↓</cell><cell>Type</cell></row><row><cell>δ = 4/255</cell><cell cols="2">Clean Adv</cell><cell>∆</cell><cell>Clean Adv</cell><cell>∆</cell><cell>Clean Adv</cell><cell>∆</cell><cell>Clean Adv</cell><cell>∆</cell></row><row><cell>DiT-256</cell><cell>131</cell><cell>167</cell><cell>+36</cell><cell cols="2">0.37 0.35 -0.02</cell><cell cols="2">0.44 0.54 +0.10</cell><cell cols="2">0.74 0.70 -0.04 LDM</cell></row><row><cell>SD-V-1.4</cell><cell>44</cell><cell>114</cell><cell>+70</cell><cell cols="2">0.68 0.55 -0.13</cell><cell cols="2">0.22 0.46 +0.24</cell><cell cols="2">0.92 0.84 -0.08 LDM</cell></row><row><cell>SD-V-1.5</cell><cell>45</cell><cell>113</cell><cell>+68</cell><cell cols="2">0.73 0.59 -0.14</cell><cell cols="2">0.20 0.38 +0.138</cell><cell cols="2">0.94 0.89 -0.05 LDM</cell></row><row><cell>GD-ImageNet</cell><cell>109</cell><cell>109</cell><cell>+0</cell><cell cols="2">0.66 0.66 -0.00</cell><cell cols="2">0.21 0.21 +0.00</cell><cell cols="2">0.90 0.90 -0.00 PDM</cell></row><row><cell>IF-I</cell><cell>186</cell><cell>187</cell><cell>+1</cell><cell cols="2">0.59 0.58 -0.01</cell><cell cols="2">0.14 0.14 +0.00</cell><cell cols="2">0.86 0.86 -0.00 PDM</cell></row><row><cell>IF-II</cell><cell>85</cell><cell>87</cell><cell>+2</cell><cell cols="2">0.84 0.84 -0.00</cell><cell cols="2">0.15 0.15 +0.00</cell><cell cols="2">0.91 0.91 -0.00 PDM</cell></row><row><cell>δ = 8/255</cell><cell cols="2">Clean Adv</cell><cell>∆</cell><cell>Clean Adv</cell><cell>∆</cell><cell>Clean Adv</cell><cell>∆</cell><cell>Clean Adv</cell><cell>∆</cell></row><row><cell>DiT-256</cell><cell>131</cell><cell>186</cell><cell>+55</cell><cell cols="2">0.37 0.31 -0.06</cell><cell cols="2">0.44 0.63 +0.19</cell><cell cols="2">0.74 0.66 -0.08 LDM</cell></row><row><cell>SD-V-1.4</cell><cell>44</cell><cell cols="2">178 +134</cell><cell cols="2">0.68 0.44 -0.24</cell><cell cols="2">0.22 0.60 +0.38</cell><cell cols="2">0.92 0.78 -0.14 LDM</cell></row><row><cell>SD-V-1.5</cell><cell>45</cell><cell cols="2">179 +134</cell><cell cols="2">0.73 0.49 -0.24</cell><cell cols="2">0.20 0.51 +0.31</cell><cell cols="2">0.94 0.84 -0.10 LDM</cell></row><row><cell>GD-ImageNet</cell><cell>109</cell><cell>110</cell><cell>+1</cell><cell cols="2">0.66 0.64 -0.02</cell><cell cols="2">0.21 0.22 +0.01</cell><cell cols="2">0.90 0.90 -0.00 PDM</cell></row><row><cell>IF-I</cell><cell>186</cell><cell>188</cell><cell>+2</cell><cell cols="2">0.59 0.59 -0.00</cell><cell cols="2">0.14 0.14 +0.00</cell><cell cols="2">0.86 0.86 +0.00 PDM</cell></row><row><cell>IF-II</cell><cell>85</cell><cell>82</cell><cell>-3</cell><cell cols="2">0.84 0.83 -0.01</cell><cell cols="2">0.15 0.16 +0.01</cell><cell cols="2">0.91 0.92 +0.01 PDM</cell></row><row><cell>δ = 16/255</cell><cell cols="2">clean adv</cell><cell>∆</cell><cell>clean adv</cell><cell>∆</cell><cell>clean adv</cell><cell>∆</cell><cell>clean adv</cell><cell>∆</cell></row><row><cell>DiT-256</cell><cell>131</cell><cell>220</cell><cell>+89</cell><cell cols="2">0.37 0.26 -0.11</cell><cell cols="2">0.44 0.70 +0.26</cell><cell cols="2">0.74 0.63 -0.11 LDM</cell></row><row><cell>SD-V-1.4</cell><cell>44</cell><cell cols="2">225 +181</cell><cell cols="2">0.68 0.34 -0.34</cell><cell cols="2">0.22 0.68 +0.46</cell><cell cols="2">0.92 0.72 -0.20 LDM</cell></row><row><cell>SD-V-1.5</cell><cell>45</cell><cell cols="2">226 +181</cell><cell cols="2">0.73 0.37 -0.36</cell><cell cols="2">0.20 0.62 +0.42</cell><cell cols="2">0.94 0.78 -0.16 LDM</cell></row><row><cell>GD-ImageNet</cell><cell>109</cell><cell>110</cell><cell>+1</cell><cell cols="2">0.66 0.57 -0.09</cell><cell cols="2">0.21 0.26 +0.05</cell><cell cols="2">0.90 0.89 -0.01 PDM</cell></row><row><cell>IF-I</cell><cell>186</cell><cell>188</cell><cell>+2</cell><cell cols="2">0.59 0.58 -0.01</cell><cell cols="2">0.14 0.15 +0.01</cell><cell cols="2">0.86 0.87 +0.01 PDM</cell></row><row><cell>IF-II</cell><cell>85</cell><cell>86</cell><cell>+1</cell><cell cols="2">0.84 0.76 -0.08</cell><cell cols="2">0.15 0.21 +0.06</cell><cell cols="2">0.91 0.95 +0.04 PDM</cell></row></table><note><p>Measurement of PGD-based Adv-Attacks for LDMs and PDMs: gradientbased diffusion attacks can attack LDMs effectively, making the difference ∆ across all evaluation metrics between edited clean image and edited adversarial image large, which means the quality of edited images drops dramatically (in red). However, the PDMs are not affected much by the crafted adversarial perturbations, showing small ∆ before and after the attacks.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantiative Measurement of Different Purification Methods in Different Scale (FIDscore): We compute the FID-score of editing purified images over the clean dataset. PDM-Pure is the strongest to remove all the tested protection, under strong protection with δ = 16. GrIDPure<ref type="bibr" target="#b49">[49]</ref> can also do reasonable protection, but the performance is limited because the PDM they used is not strong enough.Here, we explain in detail how to adapt DeepFloyd-IF<ref type="bibr" target="#b38">[38]</ref>, the strongest open-source PDM as far as we know, for PDM-Pure. DeepFloyd-IF is a cascaded text-to-image diffusion model trained on 1.2B text-image pairs from LAION dataset<ref type="bibr" target="#b35">[35]</ref>. It contains three stages named IF-Stage I, II, and III. Here we only use Stage II and III since Stage I works in a resolution of 64 which is too low. Given a perturbed image x W ×H sized W × H, we first resize it into x 64×64 and x 256×256 . Then we use a general prompt P to do SDEdit</figDesc><table><row><cell>Photoguard Mist Mist-v2</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Details about Different Diffusion Models in this Paper</head><p>Here we introduce the diffusion models used in this work, which cover different types of diffusion (LDM, PDM), different training datasets, different resolutions, and different model structures (U-Net, Transformer):</p><p>Guided Diffusion (PDM) We use the implementation and checkpoint from <ref type="url" target="https://github.com/openai/guided-diffusion">https://github. com/openai/guided-diffusion</ref>, the Guided Diffusion models we used are trained on Ima-geNet <ref type="bibr" target="#b6">[7]</ref> in resolution 256 × 256, the editing results are tested on sub-dataset of ImageNet validation set sized 500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IF-Stage I (PDM)</head><p>This is the first stage of the cascaded DeepFloyd IF model <ref type="bibr" target="#b38">[38]</ref> from <ref type="url" target="https://github.com/deep-floyd/IF">https: //github.com/deep-floyd/IF</ref>. It is trained on LAION 1.2B with text annotation. It has a resolution of 64 × 64. the editing results are tested on the image dataset introduced in <ref type="bibr" target="#b42">[42]</ref>, including 400 anime, portrait, landscape, and artwork images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IF-Stage II (PDM)</head><p>This is the second stage of the cascaded DeepFloyd IF model <ref type="bibr" target="#b38">[38]</ref> from <ref type="url" target="https://github.com/deep-floyd/IF">https://github.com/deep-floyd/IF</ref>. It is a conditional diffusion model in the pixel space with 256 × 256, which is conditioned on 64 × 64 low-resolution images. During the attack, we freeze the image condition and only attack the target image to be edited.</p><p>Stable Diffusion V-1.4 (LDM) It is one of the most popular LDMs from <ref type="url" target="https://huggingface.co/CompVis/stable-diffusion-v1-4">https://huggingface. co/CompVis/stable-diffusion-v1-4</ref>, also trained on text-image pairs, which has been widely studied in this field. It supports resolutions of 256 × 256 and 512 × 512, both can be easily attacked. The encoder first encodes the image sized H × W into the latent space sized 4 × H/4 × W/4, and then uses U-Net combined with cross-attention to run the denoising process.</p><p>Stable Diffusion V-1.5 (LDM) It has the same structure as Stable Diffusion V-1.4, which is also stronger since it is trained with more steps, from <ref type="url" target="https://huggingface.co/runwayml/stable-diffusion-v1-5">https://huggingface.co/runwayml/ stable-diffusion-v1-5</ref>.</p><p>DiT-XL (LDM) It is another popular latent diffusion model, that uses the backbone of the Transformer instead of the U-Net. We use the implementation from the original repository <ref type="url" target="https://github.com/facebookresearch/DiT/">https://github.com/facebookresearch/DiT/</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details about Different Protection Methods in this Paper</head><p>We introduce different protection methods tested in this paper, of which all the original versions are designed for LDMs. All the adversarial attacks work under the white box settings of PGD-attack, varying from each other with different adversarial losses:</p><p>AdvDM AdvDM is one of the first adversarial attacks proposed in <ref type="bibr" target="#b19">[20]</ref>, it used a Monte-Carlobased adversarial loss which can effectively attack the latent diffusion models, we also call this loss semantic loss:</p><p>PhotoGuard PhotoGuard is proposed in <ref type="bibr" target="#b33">[33]</ref>, it takes the encoder, making the encoded image close to a target image y, we also call it textural loss:</p><p>Crop &amp; Resize: we first crop the image by 20% and then resize the image to the original size, it turns out to be one of the most effective defense methods <ref type="bibr" target="#b18">[19]</ref>.</p><p>JPEG compression: <ref type="bibr" target="#b34">[34]</ref> reveals that JPEG compression can be a good purification method, and we adopt the 65% as the quality of compression in <ref type="bibr" target="#b34">[34]</ref>.</p><p>LDM-Pure: We also try to use LDMs to run SDEdit as a naive purifier, sadly it cannot work, because the adversarial protection transfers well between different LDMs.</p><p>GrIDPure: It is proposed in <ref type="bibr" target="#b49">[49]</ref> as a purifier, GrIDPure first divides an image into patches sized 128 × 128, and then purifies the 9 patches sized 256 × 256. Also, it combined the four corners sized 128 × 128 to purify it so we have 10 patches to purify in total. After running SDEdit with a small noise (set to 0.1T ), we reassemble the patches into the original size, pixel values are assigned using the average values of the patches they belong to. More details can be seen in <ref type="bibr" target="#b49">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Experimental Results</head><p>In this section, we present more experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 More Visualizations of Attacking PDMs</head><p>We show more results of attacking LDMs and PDMs in Figure <ref type="figure">5</ref>, where we attack them with different budget δ = 4, 8, 16. We can see all the LDMs can be easily attacked, while PDMs cannot be attacked, even the largest perturbations will not fool the editing process. Actually, the editing process is trying to purify the strange perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 More Visualizaitons of PDM-Pure and Baseline Methods</head><p>We show more qualitative results of the proposed PDM-Pure based on IF. First, we show purified samples of PDM-Pure in Figure . 7, from which we can see that PDM-Pure can remove large protective perturbations and largely preserve details.</p><p>Compared with GrIDPure <ref type="bibr" target="#b49">[49]</ref>, we find that PDM-Pure shows better results when the noise is large and colorful, as is illustrated in Figure <ref type="figure">8</ref>. Also, though GrIDPure merges patches, it still shows boundary lines between patches.</p><p>Compared with other baseline purification methods such as Adv-Clean, Crop-and-Resize, and JPEG compression, PDM-Pure shows much better results (Figure <ref type="figure">6</ref>) for different kinds of protective noise, showing that it is capable to serve as a universal purifier. We choose AdvDM, Mist, and SDS as the representative of three kinds of protection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 More Visualizaitons of PDM-Pure for Downstreaming Tasks</head><p>After applying PDM-Pure to the protected images, they are no longer adversarial to LDMs and can be easily edited or imitated. Here we will demonstrate more results on editing the purified images on downstream tasks.</p><p>In Figure <ref type="figure">9</ref>, we show more results to prove that the purified images can be edited easily, and the quality of editing results is high. It means that PDM-Pure can bypass the protection very well for inpainting tasks.</p><p>In Figure <ref type="figure">10</ref> we show more results on purifying Mist <ref type="bibr" target="#b18">[19]</ref> and Glaze <ref type="bibr" target="#b37">[37]</ref> perturbations, and then running LoRA customized generation. From the figure, we can see that PDM-Pure can make the protected images easy to imitate again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F PDM-Pure For Higher Resolution</head><p>In this paper, we mainly apply PDM-Pure for images sized 512 × 512, which is also the most widely used resolution for latent diffusion models. When the resolution is 512 × 512, running SDEdit using     G Ablations of t * in PDM-Pure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IF-Stage I (PDM) 64x64</head><p>The PDM-Pure on DeepFloyd-IF we used in this paper uses the default settings of SDEdit with t * = 0.1T . And we respace the diffusion model into 100 steps, so we only need to run 10 denoising steps. It can be run on one A6000 GPU, occupying 22G VRAM in 30 seconds.</p><p>Here we show some ablation about the choice of t * . In fact, in many SDEdit papers, t * can be roughly defined by trying, different t * that can be used to purify different levels of noise. We try t * = 0.01, 0.1, 0.2, in Figure <ref type="figure">12</ref> we can see that when t * = 0.01 the noise is not fully purified, and when t * = 0.2, the details in the painting are blurred. It should be noted that the sweet point for different images and different noises can be slightly different, so it will be more useful to do some trials before purification.</p><p>1038 x 1000 (w x h) 509 x 503 (w x h) 679 x 770 (w x h)</p><p>Figure <ref type="figure">11</ref>: PDM-Pure Working On Images with Higher Resolution: we show the results of applying PDM-Pure for images with higher resolutions, the images are protected using Glaze <ref type="bibr" target="#b37">[37]</ref>. We can see from the figure that the adversarial patterns (in red box) can be effectively purified (in green box). Zoom in on the computer for a better view.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Imperceptible protection against style imitation from diffusion models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19254</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Us district court for the northern district of california</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01324</idno>
		<title level="m">Text-to-image diffusion models with an ensemble of expert denoisers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ieee symposium on security and privacy (sp)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring adversarial attacks against latent diffusion model from the perspective of adversarial transferability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.07087</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Diffusionshield: A watermark for copyright protection against generative diffusion models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04642</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth one word: Personalizing text-to-image generation using textual inversion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01618</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gandikota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Orgad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Materzyńska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.14761</idno>
		<title level="m">Unified concept editing in diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Continual learning for forgetting in deep generative models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video diffusion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8633" to="8646" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Noise2music: Text-conditioned music generation with diffusion models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03917</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-concept customization of text-toimage diffusion</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1931" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mist: Towards improved adversarial examples for diffusion models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12683</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20763" to="20786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C.-H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10440</idno>
		<title level="m">Magic3d: High-resolution text-to-3d content creation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Toward robust imperceptible perturbation against unauthorized text-to-image diffusion-based synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sdedit: Guided image synthesis and editing with stochastic differential equations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16091</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07460</idno>
		<title level="m">Diffusion models for adversarial purification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Synthesizing coherent story with auto-regressive latent diffusion models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10950</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Protecting the intellectual property of diffusion models by the watermark diffusion process</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.03436</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dreamfusion: Text-to-3d using 2d diffusion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Make-a-story: Visual memory conditioned consistent story generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2493" to="2502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Raising the cost of malicious ai-powered image editing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khaddaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06588</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Jpeg compressed images can bypass protections against ai editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sandoval-Segura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02234</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25278" to="25294" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ai art generators hit with copyright suit over artists&apos; images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Setty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Glaze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04222</idno>
		<title level="m">Protecting artists from style mimicry by text-to-image models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Shonenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Konstantinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bakshandaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Klokova</surname></persName>
		</author>
		<ptr target="https://github.com/deep-floyd/IF" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">U</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gafni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Toward effective protection against diffusion-based mimicry through score distillation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Diffusion-based adversarial sample generation for improved stealthiness and controllability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07909</idno>
		<title level="m">Text-to-image diffusion model in generative ai: A survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Forget-me-not: Learning to forget in text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17591</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.08257</idno>
		<title level="m">On the robustness of latent diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.10137</idno>
		<title level="m">A recipe for watermarking diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Can protective perturbation safeguard personal data from being exploited by stable diffusion?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Z Z D Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.00084</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Understanding and improving adversarial attacks on latent diffusion model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04687</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
