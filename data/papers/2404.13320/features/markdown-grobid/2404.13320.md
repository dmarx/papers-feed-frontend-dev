# Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than We Think

## Abstract

## 

Diffusion models have demonstrated an impressive capability to edit or imitate images, which has raised concerns regarding the safeguarding of intellectual property. To address these concerns, the adoption of adversarial attacks, which introduce adversarial perturbations into protected images, has proven successful. Consequently, diffusion models, like many other deep network models, are believed to be susceptible to adversarial attacks. However, in this work, we draw attention to an important oversight in existing research, as all previous studies have focused solely on attacking latent diffusion models (LDMs), neglecting adversarial examples for diffusion models in the pixel space (PDMs). Through extensive experiments, we demonstrate that nearly all existing adversarial attack methods designed for LDMs fail when applied to PDMs. We attribute the vulnerability of LDMs to their encoders, indicating that diffusion models exhibit strong robustness against adversarial attacks. Building upon this insight, we propose utilizing PDMs as an off-the-shelf purifier to effectively eliminate adversarial patterns generated by LDMs, thereby maintaining the integrity of images. Notably, we highlight that most existing protection methods can be easily bypassed using PDM-based purification. We hope our findings prompt a reevaluation of adversarial samples for diffusion models as potential protection methods. Codes are available in [https://github.com/xavihart/PDM-Pure](https://github.com/xavihart/PDM-Pure).

## Introduction

Generative diffusion models (DMs) [[14,](#b13)[40,](#b40)[32]](#b32) have achieved great success in generating images with high fidelity. However, this remarkable generative capability of diffusion models is accompanied by safety concerns [[44]](#b44), especially on the unauthorized editing or imitation of personal images such as portraits or individual artworks [[2,](#b1)[36]](#b36). Recent works [[20,](#b19)[37,](#b37)[33,](#b33)[42,](#b42)[50,](#b50)[5,](#b4)[1,](#b0)[22]](#b22) show that adversarial samples (adv-samples) for diffusion models can be applied as a protection against malicious editing. Small perturbations generated by conventional methods in adversarial machine learning [[23,](#b23)[11]](#b10) can effectively fool popular diffusion models such as Stable Diffusion [[32]](#b32) to produce chaotic results when an imitation attempt is made. However, a significantly overlooked aspect is that all the existing works focus on latent diffusion models (LDMs) and the pixel-space diffusion models (PDMs) are not studied. For LDMs, perturbations are not directly introduced to the input of the diffusion models. Instead, they are applied externally and propagated through an encoder. It has been shown that the encoder-decoder of LDMs is vulnerable to adversarial perturbations [[46,](#b46)[42]](#b42), which means that the adv-samples for LDMs have a very different mechanism compared with the adv-samples for PDMs. Moreover, some existing works [[19,](#b18)[33]](#b33) show that combining encoder-specific loss can enhance the adversary, [[42]](#b42) further demonstrating that the encoder is the bottleneck for attacking LDMs. Building upon this observation, in this paper, we draw attention to rethink existing adversarial attack methods for diffusion models:  We address this question by systematically investigating adv-samples for PDMs. We conduct experiments on various LDMs or PDMs with different network architectures (e.g. U-Net [[14]](#b13) or Transformer [[28]](#b28)), different training datasets, and different input resolutions (e.g. 64, 256, 512). Through extensive experiments, we demonstrate that all the existing methods we tested [[19,](#b18)[50,](#b50)[37,](#b37)[42,](#b42)[5,](#b4)[33,](#b33)[20]](#b19), targeting to attack LDMs, fail to generate effective adv-samples for PDMs. This implies that PDMs are more adversarial robust than we think.

Building on this insight that PDMs are strongly robust against adversarial perturbations, we further propose PDM-Pure, a universal purifier that can effectively remove the protective perturbations of different scales (e.g. Mist-v2 [[50]](#b50) and Glaze [[37]](#b37)) based on PDMs trained on large datasets. Through extensive experiments, we demonstrate that PDM-Pure achieves way better performance than all baseline methods.

To summarize, the pixel is a barrier to adversarial attack; the diffusion process in the pixel space makes PDMs much more robust than LDMs. This property of PDMs also makes real protection against the misusage of diffusion models difficult since all the existing protections can be easily purified using a strong PDM. Our contributions are listed below.

1. We observe that most existing works on adversarial examples for protection focus on LDMs. Adversarial attacks against PDMs are largely overlooked in this field. 2. We fill in the gap in the literature by conducting extensive experiments on various LDMs and PDMs. We discover that all the existing methods fail to attack the PDMs, indicating that PDMs are much more adversarially robust than LDMs. 3. Based on this novel insight, we propose a simple yet effective framework termed PDM-Pure that applies strong PDMs as a universal purifier to remove attack-agnostic adversarial perturbations, easily bypassing almost all existing protective methods.

## Related Works

Safety Issues in Diffusion Models The impressive generative capability of the diffusion models has raised numerous safety issues [[44,](#b44)[36,](#b36)[2]](#b1). As a result, there has been a growing interest in preventing DMs from being abused. Some of the existing works focus on the protection of intellectual property of diffusion models by applying watermarks [[48,](#b48)[29,](#b29)[6]](#b5) and some of them are on concept removal to prevent the DMs from generating NSFW images [[12,](#b11)[45,](#b45)[10]](#b9). In the era of generative models, caution should be taken to guarantee safe and responsible applications of these models.

Adversarial Examples for DMs Adversarial samples [[11,](#b10)[4,](#b3)[37]](#b37) are clean samples perturbed by an imperceptible small noise that can fool the deep neural networks into making wrong decisions. Under the white-box settings, gradient-based methods are widely used to generate adv-samples. Among them, the projected gradient descent (PGD) algorithm [[23]](#b23) is one of the most effective methods.

Recent works [[20,](#b19)[33]](#b33) show that it is also easy to find adv-samples for diffusion models (AdvDM): with a proper loss to attack the denoising process, the perturbed image can fool the diffusion models to generate chaotic images when operating diffusion-based mimicry. Furthermore, many improved

ùë• !"# SDEdit(ùë• !"# ) SDEdit(ùë•) DiT-256 GD ùë• !"# SDEdit(ùë• !"# ) SDEdit(ùë•) SD-V-1.5 IF-Stage II PDM, Not Attacked LDM, Attacked LDM, Attacked PDM, Not Attacked (b) Using End-to-End Attack for PDMs (Strongest but impractical to apply) (a) LDM vs PDM Under Adversarial Attack ùúÉ ùúÉ ùúÉ ùúÉ ‚Ä¶ ‚Ñí( ) , ‚àá $ ||Edit ùë•, ùúÉ -ùë¶|| % % ùë• Edit ùë•, ùúÉ ùë¶ ùë• !"# SDEdit(ùë• !"# ) End-to-End attack Fails on PDMs algorithms [[50,](#b50)[5,](#b4)[42]](#b42) have been proposed to generate better AdvDM samples. However, to our best knowledge, all the AdvDM methods listed above are used on LDMs, and those for the PDMs are rarely explored.

Adversarial Perturbation as Protection Adversarial perturbation against DMs turns out to be an effective method to safeguard images against unauthorized editing [[20,](#b19)[37,](#b37)[33,](#b33)[42,](#b42)[50,](#b50)[5,](#b4)[1,](#b0)[22]](#b22). It has found applications (e.g., Glaze [[37]](#b37) and Mist [[50,](#b50)[19]](#b18)) for individual artists to protect their creations. SDS-attack [[42]](#b42) further investigates the mechanism behind the attack and proposes some tools to make the protection more effective. However, they are limited to protecting LDMs only. In addition, some works [[49,](#b49)[34]](#b34) find that these protective perturbations can be purified. For instance, GrIDPure [[49]](#b49) find that DiffPure [[26]](#b26) can be used to purify the adversarial patterns, but they did not realize that the reason behind this is the robustness of PDMs.

## Preliminaries

Generative Diffusion Models The generative diffusion model [[14,](#b13)[40]](#b40) is one type of generative model, and it has demonstrated remarkable generative capability in numerous fields such as image [[32,](#b32)[3]](#b2), 3D [[30,](#b30)[21]](#b20), video [[15,](#b14)[39]](#b39), story [[27,](#b27)[31]](#b31) and music [[25,](#b25)[17]](#b16) generation. Diffusion models, like other generative models, are parametrized models p Œ∏ (x 0 ) that can estimate an unknown distribution q(x 0 ). For image generation tasks, q(x 0 ) is the distribution of real images.

There are two processes involved in a diffusion model, a forward diffusion process and a reverse denoising process. The forward diffusion process progressively injects noise into the clean image, and the t-th step diffusion is formulated as

$q(x t | x t-1 ) = N (x t ; ‚àö 1 -Œ≤ t x t-1 , Œ≤ t I). Accumulating the noise, we have q t (x t | x 0 ) = N (x t ; ‚àö ·æ±t x t-1 , (1 -·æ±t )I).$Here Œ≤ t growing from 0 to 1 are pre-defined values, Œ± t = 1 -Œ≤ t , and ·æ±t = Œ† t s=1 Œ± s . Finally, x T will become approximately an isotropic Gaussian random variable when ·æ±t ‚Üí 0.

Reversely, p Œ∏ (x t-1 |x t ) can generate samples from Gaussian xT ‚àº N (0, I), where p Œ∏ be reparameterized by learning a noise estimator œµ Œ∏ , the training loss is

$E t,x0,œµ [Œª(t)‚à•œµ Œ∏ (x t , t) -œµ‚à• 2 ]$weighted by Œª(t), where œµ is the noise used to diffuse x 0 following q t (x t |x 0 ). Finally, by iteratively applying p Œ∏ (x t-1 |x t ), we can sample realistic images following p Œ∏ (x 0 ).

Since the above diffusion process operates directly in the pixel space, we call such diffusion models Pixel-Space Diffusion Models (PDMs). Another popular choice is to move the diffusion process into the latent space to make it more scalable, resulting in the Latent Diffusion Models (LDMs) [[32]](#b32). More specifically, LDMs first use an encoder E œï parameterized by œï to encode x 0 into a latent variable z 0 = E œï (x 0 ). The denoising diffusion process is the same as PDMs. At the end of the denoising process, ·∫ë0 can be projected back to the pixel space using decoder D œà parameterized by œà as x0 = D œà (·∫ë 0 ).

## Adversarial Examples for Diffusion Models

Recent works [[33,](#b33)[20]](#b19) find that adding small perturbations to clean images will make the diffusion models perform badly in noise prediction, and further generate chaotic results in tasks like image editing and customized generation. The adversarial perturbations for LDMs can be generated by optimizing the Monte-Carlo-based adversarial loss:

$L adv (x) = E t,œµ E zt‚àºqt(E œï (x)) ‚à•œµ Œ∏ (z t , t) -œµ‚à• 2 2 .(1)$Other encoder-based losses [[37,](#b37)[19,](#b18)[50,](#b50)[42]](#b42) further enhance the attack to make it more effective. With the carefully designed adversarial loss, we can run Projected Gradient Descent (PGD) [[23]](#b23) with ‚Ñì ‚àû budget Œ¥ to generate adversarial perturbations:

$x k+1 = P B‚àû(x 0 ,Œ¥) x k + Œ∑ sign‚àá x k L adv (x k )(2)$In the above equation, P B‚àû(x 0 ,Œ¥) (‚Ä¢) is the projection operator on the ‚Ñì ‚àû ball, where x 0 is the clean image to be perturbed. We use superscript x k to represent the iterations of the PGD and subscript x t for the diffusion steps.

## Rethink Adversarial Examples for Diffusion Models

Adversarial examples of LDMs are widely adopted as a protection mechanism to prevent unauthorized images from being edited or imitated [[37,](#b37)[19]](#b18). However, a significant issue overlooked is that all the adversarial examples in existing work are generated using LDMs, primarily due to the wide impact of the Stable Diffusion; no attempts have been made to attack PDMs.

This lack of investigation may mislead us to conclude that diffusion models, like most deep neural networks, are vulnerable to adversarial perturbations, and that the algorithms used in LDMs can be transferred to PDMs by simply applying the same adversarial loss in the pixel space formulated as:  

$ùíí ùíï * ($$L adv (x) = E t,œµ E xt‚àºqt(x) ‚à•œµ Œ∏ (x t , t) -œµ‚à• 2 2$(3) However, we show through experiments that PDMs are robust against this form of attack (Figure [2](#fig_2)), which means all the existing attacks against diffusion models are, in fact, special cases of attacks against the LDMs only. Prior to this study, there may have been a prevailing belief that diffusion models could be easily deceived. However, our research reveals an important distinction: it is the LDMs that exhibit vulnerability, while the PDMs demonstrate significantly higher adversarial robustness. We conduct extensive experiments on popular LDMs and PDMs structures including DiT, Guided Diffusion, Stable Diffusion, and DeepFloyd, and demonstrate in Table [1](#tab_1) that only the LDMs can be attacked and PDMs are not that susceptible to adversarial perturbations. More details and analysis can be found in the experiment section.

The vulnerability of the LDMs is caused by the vulnerability of the latent space [[42]](#b42), meaning that although we may set budgets for perturbations in the pixel space, the perturbations in the latent space can be large. In [[42]](#b42), the authors show statistics of perturbations in the latent space over the perturbations in the pixel space and this value |Œ¥z| |Œ¥x| can be as large as 10. In contrast, the PDMs directly work in the pixel space, and thus the injected noise combined with the random Gaussian noise will not easily fool the denoiser as it is trained to be robust to Gaussian noise of different levels.

Almost all the copyright protection perturbations [[37,](#b37)[19,](#b18)[50]](#b50) are based on the insight that it is easy to craft adversarial examples to fool the diffusion models. We need to rethink the adversarial samples of diffusion models since there are a lot of PDMs that cannot be attacked easily. Next, we show that PDMs can be utilized to purify all adversarial patterns generated by existing methods in Section 5. This new landscape poses new challenges to ensure the security and robustness of diffusion-based copyright protection techniques.

## PDM-Pure: PDM as a Strong Universal Purifier

Given the robustness of PDMs, a natural idea emerges: we can utilize PDMs as a universal purification network. This approach could potentially eliminate any adversarial patterns without knowing the nature of the attacks. We term this framework PDM-Pure, which is a general framework to deal with all the perturbations nowadays. To fully harness the capabilities of PDM-Pure, we need to fulfill two basic requirements: [(1)](#b0) The perturbation shows out-of-distribution pattern as reflected in existing works on adversarial purification/attacks using diffusion models [[26,](#b26)[43]](#b43) (2) The PDM being used is strong enough to represent p(x 0 ), which can be largely determined by the dataset they are trained on.

It is effortless to design a PDM-Pure. The key idea behind this method is to run SDEdit in the pixel space. Given any strong pixel-space diffusion model, we add a small noise to the protected images and run the denoising process (Figure [3](#fig_4)), and then the adversarial pattern should be removed. The key idea of PDM-Pure is simple. In practice, we need to adjust the pipeline to fit the resolution of the PDMs being used.  [[24]](#b24) using the Stage II model:

## Methods

## AdvDM AdvDM(-) SDS(-) SDS(+) SDST

$x t = IF-II(x t+1 , x 64√ó64 , P)(4)$where t = T edit -1, ..., 1, 0, x Tedit = x 256√ó256 . A larger T edit may be used for larger noise. x 0 is the purified image we get in the 256 √ó 256 resolution space, where the adversarial patterns should be already purified. We can then use IF Stage III to further up-sample it into 1024 √ó 1024 with x 1024√ó1024 = IF-III(x 0 , p). Finally, we can sample into H √ó W as we want through downsampling. This whole process is demonstrated in Figure [3](#fig_4). After purification, the image is no longer adversarial to the targeted diffusion models and can be effectively used in downstream tasks.

In the main paper, we conduct experiments on purifying protected images sized 512 √ó 512. For images with a larger resolution, purifying in the resolution of 256 √ó 256 may lose information. In Appendix F we show PDM-Pure can also applied to purify patches of high-resolution inputs.

## Experiments

In this section, we conduct experiments on various attacking methods and various models to support the following two conclusions:

‚Ä¢ (C1): PDMs are much more adversarial robust than LDMs, and PDMs can not be effectively attacked using all the existing attacks for LDMs. ‚Ä¢ (C2): PDMs can be applied to effectively purify all of the existing protective perturbations. Our PDM-Pure based on DeepFloyd-IF shows state-of-the-art purification power.

## Models, Datasets, and Metrics

The models we used can be categorized into LDMs and PDMs. For LDMs, we use Stable Diffusion V-1.4, V-1.5 (SD-V-1.4, SD-V-1.5) [[32]](#b32), and Diffusion Transformer (DiT-XL/2) [[28]](#b28), and for PDMs we use Guided Diffusion (GD) [[8]](#b7) trained on ImageNet [[7]](#b6), and DeepFloyd Stage I and Stage II [[38]](#b38).

For models trained on the ImageNet (DiT, GD), we run adversarial attacks and purification on a 1k subset of the ImageNet validation dataset. For models trained on LAION, we run tests on the dataset proposed in [[42]](#b42), which includes 400 cartoon, artwork, landscape, and portrait images. The metrics for testing the quality of generated images are included in the Appendix.

For protection methods, we consider almost all the representative approaches, including AdvDM [[20]](#b19), SDS [[42]](#b42), Mist [[19]](#b18), Mist-v2 [[50]](#b50), Photoguard [[33]](#b33) and Glaze [[37]](#b37). We also test the methods in the design space proposed in [[42]](#b42), including SDS(-), AdvDM(-), and SDST. In contrast to other existing methods, they are based on gradient descent and have shown great performance in deceiving the LDMs.

## (C1) PDMs are Much More Robust Than We Think

In Table [1](#tab_1), we attack different LDMs and PDMs with one of the most popular adversarial loss [[50]](#b50) in Equation 1 and Equation 3, which can be interpreted as fooling the denoiser using a Monte-Carlobased loss. Given the attacked samples, we test the SDEdit results on the attacked samples, which can be generally used to test whether the samples are adversarial for the diffusion model or not. We use FID-score [[13]](#b12), SSIM [[41]](#b41), LPIPS [[47]](#b47), and IA-Score [[18]](#b17) to measure the quality of the attack.

If the quality of generated images decreases a lot compared with editing the clean images, then the attack is successful. We can see that LDMs can be easily attacked, while PDMs are quite robust; the quality of the edited images is still good. We also show some visualizations in Figure [2](#fig_2), which illustrates that the perturbation will affect the LDMs but not the PDMs.

To further investigate how robust PDM is, we test other advanced attacking methods, including the End-to-End Diffusion Attacks (E2E-Photoguard) proposed in [[33]](#b33) and the Improved Targeted Attack (ITA) proposed in [[50]](#b50). Though the End-to-End attack is usually impractical to run, it shows the strongest performance to attack LDMs. We find that both attacks are not successful in PDM settings. We show attacked samples and edited samples in Figure [2](#fig_2) as well as the Appendix. In conclusion, existing adversarial attack methods for diffusion models can only work for the LDMs, and PDMs are more robust than we think.

## (C2) PDM-Pure: A Universal Purifier that is Simple yet Effective

PDM-Pure is simple: basically, we just run SDEdit to purify the protected image in the pixel space. Given our assumption that PDMs are quite robust, we can use PDMs trained on large-scale datasets as a universal black-box purifier. We follow the model pipeline introduced in Section 5 and purify images protected by various methods in Table [2](#tab_2).

PDM-Pure is effective: from Table [2](#tab_2) we can see that the purification will remove adversarial patterns for all the protection methods we tested, largely decreasing the FID score for the SDEdit task. Also, we test the protected images and purified images in more tasks including Image Inpainting [[40]](#b40), Textual-Inversion [[9]](#b8), and LoRA customization [[16]](#b15) in Figure [4](#fig_7). Both qualitative and quantitative results show that the purified images are no more adversarial and can be effectively edited or imitated in different tasks without any obstruction.

Also, PDM-Pure shows SOTA results compared with previous purification methods, including some simple purifiers based on compression and filtering like Adv-Clean, crop-and-resize, JPEG Compression, and SDEdit-based methods like GrIDPure [[49]](#b49), which uses patchified SDEdit with a GD [[8]](#b7). We also add LDM-Pure as a baseline to show that LDMs can not be used to purify the protected images. For GrIDPure, we use Guided-Diffusion trained on ImageNet to run patchified purification. All the experiments are conducted on the datasets collected in [[42]](#b42) under the resolution of 512 √ó 512. Results for higher resolutions are presented in Appendix F.

## Conclusions and Future Directions

In this paper, we present novel insights that while many studies demonstrate the ease of finding adversarial samples for Latent Diffusion Models (LDMs), Pixel Diffusion Models (PDMs) exhibit far greater adversarial robustness than previously assumed. We are the first to investigate the adversarial samples for PDMs, revealing a surprising discovery that existing attacks fail to fool PDMs. Leveraging this insight, we propose utilizing strong PDMs as universal purifiers, resulting in PDM-Pure, a simple yet effective framework that can generate protective perturbations in a black-box manner.

Pixel is a barrier for us to do real protection against adversarial attacks. Since PDMs are quite robust, they cannot be easily attacked. PDMs can even be used to purify the protective perturbations, challenging the current assumption for the safe protection of generative diffusion models. We advocate rethinking the problem of adversarial samples for generative diffusion models and unauthorized image protection based on it. More rigorous study can be conducted to better understand the mechanism behind the robustness of PDMs. Furthermore, we can utilize it as a new structure for many other tasks   Contents 1 Introduction 1 2 Related Works 2 3 Preliminaries 3 4 Rethink Adversarial Examples for Diffusion Models 4 5 PDM-Pure: PDM as a Strong Universal Purifier 5 6 Experiments 6 6.1 Models, Datasets, and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 6.2 (C1) PDMs are Much More Robust Than We Think . . . . . . . . . . . . . . . . . 7 6.3 (C2) PDM-Pure: A Universal Purifier that is Simple yet Effective . . . . . . . . . . 7 7 Conclusions and Future Directions 7 A Details about Different Diffusion Models in this Paper 13 B Details about Different Protection Methods in this Paper 13 C Details about The Evaluation Metrics 14 D Details about Different Purification Methods 14 E More Experimental Results 15 E.1 More Visualizations of Attacking PDMs . . . . . . . . . . . . . . . . . . . . . . . 15 E.2 More Visualizaitons of PDM-Pure and Baseline Methods . . . . . . . . . . . . . . 15 E.3 More Visualizaitons of PDM-Pure for Downstreaming Tasks . . . . . . . . . . . . 15 F PDM-Pure For Higher Resolution 15 G Ablations of t * in PDM-Pure 19

Mist Mist [[19]](#b18) finds that L T (x) can better enhance the attacks if the target image y is chosen to be periodical patterns, the final loss combined L T (x) and L S (x):

$L = ŒªL T (x) + L S (x)(7)$SDS(+) Proposed in [[42]](#b42), it is proven to be a more effective attack compared with the original AdvDM, where the gradient ‚àá x L(x) is expensive to compute. By using the score distillation-based loss, it shows good performance and remains effective at the same time:

$‚àá x L SDS (x) = E t,œµ E zt Œª(t)(œµ Œ∏ (z t , t) -œµ) ‚àÇz t ‚àÇx t(8)$SDS(-) Similar to SDS(+), it swaps gradient ascent in the original PGD with gradient descent, which turns out to be even more effective.

$‚àá x L SDS(-) (x) = -E t,œµ E zt Œª(t)(œµ Œ∏ (z t , t) -œµ) ‚àÇz t ‚àÇx t(9)$Mist-v2 It was proposed in [[50]](#b50) using the Improved Targeted Attack (ITA), which turns out to be very effective, especially when the limit budget is small. It is also more effective to attack LoRA:

$L S (x) = E t,œµ E zt‚àºqt(E œï (x)) ‚à•œµ Œ∏ (z t , t) -z 0 ‚à• 2 2(10)$where z 0 = E(y) is the latent of a target image, which is the same as the typical image used in Mist.

Glaze It is the most popular protection claimed to safeguard artists from unauthorized imitation [[37]](#b37) and is widely used by the community. while it is not open-sourced, it also attacks the encoder like the Photoguard. Here we only test it in the purification stage, where we show that the protection can also be bypassed.

End-to-End Attack It is also first proposed in [[33]](#b33), which attacks the editing pipeline in a end-toend manner. Although it is strong, it is not practical to use and does not show dominant privilege compared with other protection methods.

## C Details about The Evaluation Metrics

Here we introduce the quantitative measurement we used in our experiments:

‚Ä¢ We measure the SDEdit results after the adversarial attacks using Fr√©chet Inception Distance (FID) [[13]](#b12) over the relevant datasets (for model trained on ImageNet such as GD [[8]](#b7) and DiT [[28]](#b28) we use a sub-dataset of ImageNet as the relevant dataset, for those trained on LAION, we use the collected dataset to calculate the FID). We also use Image-Alignment Score (IA-score) [[18]](#b17), which can be used to calculate the cosine-similarity between the CLIP embedding of the edited image and the original image. Also, we use some basic evaluations, where we calculate the Structural Similarity (SSIM) [[41]](#b41) and Perceptual Similarity (LPIPS) [[47]](#b47) compared with the original images.

‚Ä¢ To measure the purification results, we test the Fr√©chet Inception Distance (FID) [[13]](#b12) over the collected dataset compared with the dataset generated by running SDEdit over the purified images in the strength of 0.3.

## D Details about Different Purification Methods

Adv-Clean: [https://github.com/lllyasviel/AdverseCleaner](https://github.com/lllyasviel/AdverseCleaner), a training-free filter-based method that can remove adversarial noise for a diffusion model, it works well to remove highfrequency noise. 

![PhotoGuard]()

![Figure 1: Pixel is a Barrier for Attacking DMs: (a) Pixel-based diffusion models are harder to attack using white-box attacks like project-gradient-descent than diffusion models in the latent space. (b) Strong PDM can be used as a universal purifier to effectively remove the protective perturbation generated by existing protection methods. (c) Pixel is a barrier and the pixel-space diffusion model is quite robust, and we cannot achieve real safety and protection if pixel-space diffusion is not attacked.]()

![Figure 2: PDMs Cannot be Attacked as LDMs: (a) LDMs can be easily fooled but PDMs cannot be. (b) Even End-to-End attack does not work on PDMs. (Best viewed with zoom-in)]()

![]()

![Figure 3: PDM-Pure is Easy to Design: (a) PDM-Pure applies SDEdit[24] in the pixel space: it first runs forward diffusion with a small step t * and then runs denoising process. (b) We adapt the framework to DeepFloyd-IF[38], one of the strongest PDMs. PDM-Pure can effectively remove strong protective perturbations (e.g. Œ¥ = 16/255). The images we tested are sized 512 √ó 512.]()

![man and a dog.]()

![man and a dog.]()

![Figure 4: PDM-Pure makes the Protected Images no more Protected: Here we show qualitative results of PDM-Pure on three scenarios where unauthorized editing may occur: (a) Inpainting, (b)Text-Inversion[9] and (c) LoRA customization[16]. While the protected images incur bad generation quality, the purified ones can fully bypass the protection.]()

![Figure 12: PDM-Pure with Different t *]()

![Quantiative]()

![Quantiative Measurement of Different Purification Methods in Different Scale (FIDscore): We compute the FID-score of editing purified images over the clean dataset. PDM-Pure is the strongest to remove all the tested protection, under strong protection with Œ¥ = 16. GrIDPure[49] can also do reasonable protection, but the performance is limited because the PDM they used is not strong enough.Here, we explain in detail how to adapt DeepFloyd-IF[38], the strongest open-source PDM as far as we know, for PDM-Pure. DeepFloyd-IF is a cascaded text-to-image diffusion model trained on 1.2B text-image pairs from LAION dataset[35]. It contains three stages named IF-Stage I, II, and III. Here we only use Stage II and III since Stage I works in a resolution of 64 which is too low. Given a perturbed image x W √óH sized W √ó H, we first resize it into x 64√ó64 and x 256√ó256 . Then we use a general prompt P to do SDEdit]()

