- Decision to use a Transformer-based architecture for multimodal input processing
- Choice of asymmetric attention mechanism to handle large input sizes
- Implementation of a latent bottleneck to reduce computational complexity
- Use of cross-attention to project high-dimensional inputs to a lower-dimensional latent space
- Design decision to alternate between cross-attention and self-attention layers
- Strategy for incorporating position and modality-specific features into the input representation
- Decision to avoid hard architectural biases for flexibility across modalities
- Choice of hyperparameters for latent array size in relation to input data
- Approach to training on diverse datasets (ImageNet, AudioSet, ModelNet-40)
- Decision to not use causal attention mechanisms in the architecture
- Rationale for treating the model as a recurrent neural network (RNN) unrolled in depth
- Decision to share weights between instances of the Transformer tower
- Approach to handling multimodal inputs without requiring separate feature extractors
- Strategy for evaluating model performance across different modalities
- Decision to leverage high-fidelity Fourier features for input representation
- Choice to avoid preprocessing steps that impose grid-like structures on inputs