<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Space is a latent sequence: Structured sequence learning as a unified theory of representation in the hippocampus</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-12-03">3 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rajkumar</forename><surname>Vasudeva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">Swaroop</forename><surname>Guntupalli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guangyao</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Miguel</forename><surname>Lázaro-Gredilla</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dileep</forename><surname>George</surname></persName>
							<email>dileepgeorge@deepmind.com</email>
						</author>
						<title level="a" type="main">Space is a latent sequence: Structured sequence learning as a unified theory of representation in the hippocampus</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-03">3 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">4660FAAB7D3483CADF2FC5BF502BB627</idno>
					<idno type="arXiv">arXiv:2212.01508v1[q-bio.NC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fascinating and puzzling phenomena, such as landmark vector cells, splitter cells, and event-specific representations to name a few, are regularly discovered in the hippocampus. Without a unifying principle that can explain these divergent observations, each experiment seemingly discovers a new anomaly or coding type. Here, we provide a unifying principle that the mental representation of space is an emergent property of latent higher-order sequence learning. Treating space as a sequence resolves myriad phenomena, and suggests that the place-field mapping methodology where sequential neuron responses are interpreted in spatial and Euclidean terms might itself be a source of anomalies. Our model, called Clone-structured Causal Graph (CSCG), uses a specific higher-order graph scaffolding to learn latent representations by mapping sensory inputs to unique contexts. Learning to compress sequential and episodic experiences using CSCGs result in the emergence of cognitive maps -mental representations of spatial and conceptual relationships in an environment that are suited for planning, introspection, consolidation, and abstraction. We demonstrate that over a dozen different hippocampal phenomena, ranging from those reported in classic experiments to the most recent ones, are succinctly and mechanistically explained by our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The hippocampus is known for its role in episodic memory, map-like spatial representations, relational inference, and fast learning -a seemingly disparate set of requirements. Simultaneously, hippocampal cells are categorized into a wide variety of types based on their firing patterns ranging from place cells, splitter cells, time cells, lap cells, event specific representations and exhibit a variety of remapping phenomena in response to environmental changes <ref type="bibr" target="#b23">(Kubie et al., 2020)</ref>. These phenomena often get characterized using Euclidean spatial concepts such as object vector cells <ref type="bibr" target="#b5">(Bicanski and Burgess, 2020)</ref>, landmark vector cells <ref type="bibr" target="#b12">(Deshmukh and Knierim, 2013)</ref>, and distance coding <ref type="bibr" target="#b12">(Deshmukh and Knierim, 2013;</ref><ref type="bibr" target="#b42">Sarel et al., 2017)</ref>, without a coherent underlying explanation, and remain unresolved with other phenomena like splitter cells <ref type="bibr">(Ainge et al., 2007a,b;</ref><ref type="bibr" target="#b13">Dudchenko and Wood, 2014)</ref> and event-specific representations <ref type="bibr" target="#b48">(Sun et al., 2020)</ref>. Could these divergent requirements and myriad phenomena be explained using a simple set of principles that are computationally grounded, implemented, and easy to understand? Here we show that treating space as a sequence can resolve many of the divergent phenomena ascribed to spatial mapping, and help clarify the connections between spatial, temporal, abstract, and relational representations in the hippocampal complex.</p><p>Treating space as a sequence is a necessity for humans and other animals because they do not have a global positioning system that enables direct sensing of location coordinates. Consequently, they need to acquire and abstract the concepts of locations and space from purely sensory-motor experience. However, sensations from the world are aliased and do not convey locations directlyidentical sensations can occur at multiple locations or in different sequential contexts. To develop internal space-like maps from these aliased sensations (as illustrated in the sketch in Fig. <ref type="figure" target="#fig_0">1A</ref>), the learning agent has to appropriately split or merge sensations based on sequential contexts <ref type="bibr" target="#b33">(Niv, 2019;</ref><ref type="bibr" target="#b38">Plitt and Giocomo, 2021)</ref>. Our model, clone-structured causal graph (CSCG), tackles this problem by learning different latent states (called clones) to represent the same observation in different sequential contexts <ref type="bibr" target="#b10">(Dedieu et al., 2019;</ref><ref type="bibr" target="#b17">Eichenbaum, 2004;</ref><ref type="bibr" target="#b20">George et al., 2021)</ref>, merging or splitting them as necessary. In CSCGs, allocentric "spatial" representations naturally arise from higher-order sequence learning on egocentric sensory inputs, without making any Euclidean assumptions, and without having locations as an input. An organism or an agent can utilize a CSCG for navigation, foraging, context-recognition, and shortcut finding without having to explicitly compute place fields, or having to decode locations. Importantly, our model suggests that place field maps need to be interpreted carefully because they overlay sequential responses on to Euclidean maps. Directly characterizing the place field maps in terms of spatial and Euclidean concepts could be a source of anomalies because the underlying phenomena are inherently sequential and dynamic <ref type="bibr" target="#b52">(Warren, 2019)</ref>. In contrast, CSCG explicates how the learning of sequential contexts gives rise to spatial representations that an agent can use to drive behavior without explicitly representing location coordinates. CSCGs predict the conditions under which place fields are expected to change in response to visible or invisible environmental changes, and when they do not, resolving a variety of phenomena with a simple principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We consider experimental setups where an agent moves around in an environment and receives local sensations which are aliased in the sense that they do not correspond uniquely to locations in the environment. The environment need not be Euclidean, the agent makes no Euclidean assumptions and does not have access to a map of the environment. If the sensations from the environment are vectors (for example, visual patterns) in a continuous space, they are discretized using a vector quantizer. From a sequence of discretized observations and actions, both of which could be egocentric, an agent has to discover the latent topology of its environment to vicariously evaluate different options for navigation. This is a difficult problem due to the aliasing of the observations and a lack of Euclidean assumptions.</p><p>This can be formulated as the problem of learning a latent graph from aliased observations at its nodes. An agent performs a sequence of actions 𝑎 1 , . . . , 𝑎 𝑁 (with discrete 𝑎 𝑛 ∈ {1, . . . , 𝑁 actions }) in an environment 𝐺, and as a result of each action, it receives an observation, obtaining the stream 𝑥 1 , . . . , 𝑥 𝑁 (with discrete 𝑥 𝑛 ∈ {1, . . . , 𝑁 obs }). The goal is to recover the topology of the environment 𝐺 from sequences of actions and observations. An environment is defined by a directed multigraph 𝐺 ≡ {𝑉, 𝐸} with nodes 𝑉 ≡ {𝑣 1 , . . . 𝑣 𝑁 nodes } and edges 𝐸 ≡ {𝑒 1 , . . . 𝑒 𝑁 edges }. Every node is labeled with a discrete observation. At each time step 𝑛, the agent will exist at a node and observe 𝑥 𝑛 to be its label. Multiple nodes can have the same label, so the observation does not identify the node. We use 𝐶 (𝑥) to refer to nodes with label 𝑥, also called the clones of 𝑥. When an agent at node 𝑣 𝑖 executes an action 𝑎, it will transition to 𝑣 𝑗 with probability 𝑃(𝑣 𝑗 |𝑣 𝑖 , 𝑎). Whenever this probability is larger than 0, an associated directed edge from 𝑣 𝑖 to 𝑣 𝑗 is introduced in the graph, labeled with the corresponding action and probability. Note that this means that the graph can contain multiple edges with the same starting and ending node, but labeled with different actions. (This is what makes 𝐺 a multigraph and not a simple graph). For consistency, all edges originating from the same node and labeled with the same action must have their probabilities sum up to 1.</p><p>For a given sequence of actions, 𝐺 encodes a distribution over sequences, establishing a connection between temporal sequences and arbitrary (not necessarily Euclidean) topologies. To do this effectively, requires a graph learning mechanism that can merge or split contexts appropriately <ref type="bibr" target="#b33">(Niv, 2019)</ref>. CSCG achieves this by creating a latent space of clones that has the flexibility to split or merge contexts, and having a smooth parameterization of the graph learning problem. Having the latent space allows the model to represent long-term temporal dependencies <ref type="bibr" target="#b8">(Cormack and Horspool, 1987)</ref>, and gives it the flexibility that is not available to models that purely concatenate temporal context in the observation space. The above definitions result in a precise, action-conditional probabilistic model for sequences. Using 𝑧 𝑛 to represent the (unobserved) node at step 𝑛, and adding a simple per-node policy 𝑃(𝑎 𝑛 |𝑧 𝑛 ) to also model the actions, results in the CSCG model. The joint probability of a sequence of observations and actions is</p><formula xml:id="formula_0">𝑃 (𝑥 1 , 𝑎 1 , 𝑥 2 , . . . , 𝑥 𝑁-1 , 𝑎 𝑁-1 , 𝑥 𝑁 ) = ∑︁ 𝑧 1 ∈𝐶 (𝑥 1 )</formula><p>. . .</p><formula xml:id="formula_1">∑︁ 𝑧 𝑁 ∈𝐶 (𝑥 𝑁 ) 𝑃 (𝑧 1 ) 𝑁-1 𝑛=1 𝑃 (𝑧 𝑛+1 |𝑧 𝑛 , 𝑎 𝑛 ) 𝑃 (𝑎 𝑛 |𝑧 𝑛 ) ,<label>(1)</label></formula><p>depicted as a graphical model in Fig. <ref type="figure" target="#fig_0">1E</ref>. Observe that in the action-conditional setting, this corresponds to a hidden Markov model in which the emission matrix is determined by the cloning structure and fixed, which improves its learnability. The model supports causal semantics <ref type="bibr" target="#b36">(Pearl, 2009)</ref> and learning from interventions <ref type="bibr">(Eaton and Murphy, 2007a;</ref><ref type="bibr" target="#b37">Peters et al., 2017)</ref>. A learned transition matrix is a directed multi-graph, and reusing this transition matrix and the cloning structure to remap to a new environment can be considered as learning using soft interventions <ref type="bibr">(Eaton and Murphy, 2007b)</ref>.</p><p>Inference and learning in CSCG can be achieved using biologically plausible mechanisms. The clones in CSCG can be represented by an assembly of neurons. Message-passing inference in CSCG is computationally cheap and biologically plausible using simple integrate-and-fire neurons <ref type="bibr" target="#b39">(Rao, 2004)</ref>. Learning is achieved using Expectation Maximization (EM) which maximizes the likelihood of the model using a local update mechanism analogous to spike timing dependent plasticity <ref type="bibr" target="#b31">(Nessler et al., 2009</ref><ref type="bibr" target="#b32">(Nessler et al., , 2013))</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>Phenomena Publications Geometry changes Place field remaps as determined by geometry O'Keefe and Burgess (1996) Visual cue rotation Place field rotates with cue card Muller and Kubie (1987) Barrier addition Place field disruption near barrier Muller and Kubie (1987) Landmark vector cells Place field remaps w.r.t a landmark Deshmukh and Knierim (2013) Linear track Place field remaps w.r.t start and end of the track Sheehan et al. (2021) Directional place fields Place field remapping is sensitive to movement direction O'Keefe and Burgess (1996) Laps on a track Event specific rate remapping and lap cells Sun et al. (2020) Four connected rooms Place fields are unaffected by closed doors Duvelle et al. (2021) Two identical rooms Place fields are repeated in two rooms Fuhs et al. (2005) Hairpin maze Direction specific repetition of place fields Derdikman et al. (2009) Room size expansion Place fields expand or stretch based on location w.r.t boundaries Tanni et al. (2022) Table 1 | List of experiments, their observed phenomena, and related publications.</p><p>We tested the CSCG model in a variety of experimental settings. The first set of experiments investigated the ability of a CSCG to learn latent topologies from perceptually aliased observation sequences, ability to represent multiple maps in the same model, and the ability to transitively stitch global maps from disjoint overlapping experiences. Furthermore, we investigated the ability of the model to use previously acquired structural knowledge to guide behavior in novel environments. All these properties are important for the performance of an animal. The second set of experiments investigated CSCG's ability to reproduce and explain a broad set of well known experimental phenomena from the hippocampus (see Table <ref type="table">1</ref>). These phenomena can be broadly divided into spatial, geometry-related, and landmark-related remapping <ref type="bibr" target="#b12">(Deshmukh and Knierim, 2013;</ref><ref type="bibr" target="#b28">Muller and Kubie, 1987;</ref><ref type="bibr" target="#b34">O'Keefe and Burgess, 1996;</ref><ref type="bibr" target="#b45">Sheehan et al., 2021)</ref>, phenomena with both spatial and temporal components <ref type="bibr" target="#b34">(O'Keefe and Burgess, 1996;</ref><ref type="bibr" target="#b48">Sun et al., 2020)</ref>, and place field repetition, distortion, and changes with respect to environmental connectivity <ref type="bibr" target="#b11">(Derdikman et al., 2009;</ref><ref type="bibr" target="#b14">Duvelle et al., 2021;</ref><ref type="bibr" target="#b19">Fuhs et al., 2005)</ref>. In addition, we performed a set of experiments that serve as testable predictions for CSCG's ability to explain the mechanisms underlying hippocampal phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSCG can construct maps from aliased egocentric observations in diverse environments.</head><p>CSCGs are successful in learning latent topologies in a variety of environments, including 2D and 3D layouts (Fig. <ref type="figure" target="#fig_1">2A</ref> and <ref type="figure">B</ref>) and mazes, from purely sequential aliased random walk observations. In the uniform room example in Fig. <ref type="figure" target="#fig_1">2A</ref>, the agent received egocentric visual observations quantized though a vector quantizer, and took egocentric actions, with four possible heading directions in each location. The visible input to the agent depended on its location as well as head direction. Learning in CSCG discovered the latent headings and locations and represented them using separate clones (Fig. <ref type="figure" target="#fig_1">2A</ref>)(iii). Each node in the graph (Fig. <ref type="figure" target="#fig_1">2A</ref>(iii)) corresponds to a clone, and its color represents the local observation it is attached to. Note that the learning of the transition graph discovered 4 clones per spatial location, this corresponds to the 4 possible headings that an agent can be in (see "Methods" for more details). CSCGs are also able to correctly learn the topology of 3D surfaces (bucky ball, cube) from sequential aliased egocentric observations (Fig. <ref type="figure" target="#fig_1">2B</ref>), correctly inferring the latent local and global loop closures.</p><p>While each clone in the transition graph in Fig. <ref type="figure" target="#fig_1">2A</ref>(iii) is 'bottom-up' responding to the local sensation indicated by the color, that sensation needs to occur in the latent sequential context specified by the transition graph. By representing sequential contexts in the latent space, these clones come to represent variables like locations and heading that are not directly sensed. An experimenter can obtain the place field of a clone by creating a map representing the arena that the agent is moving in, and marking and accumulating the instantaneous activities of the clone at the present ground-truth location of the agent on that map. Examples of such place fields are shown in Fig. <ref type="figure" target="#fig_1">2A</ref>(iv). The clones in Fig. <ref type="figure" target="#fig_1">2A</ref>(iii) are also head direction sensitive, which corresponds well with the observation in <ref type="bibr" target="#b0">Acharya et al. (2016)</ref> that place fields show head direction sensitivity when they are mapped conditioned on head direction. The head direction sensitivity will be strongest in those locations where the animal has very different visual inputs based on the head direction, compared to the locations in the middle where the animal receives the same visual input in all head directions, consistent with contemporary observations about view sensitivity of place fields <ref type="bibr" target="#b0">(Acharya et al., 2016;</ref><ref type="bibr" target="#b22">Jercog et al., 2019;</ref><ref type="bibr" target="#b25">Moore et al., 2021;</ref><ref type="bibr" target="#b30">Muller et al., 1994)</ref>. While the place field can give rise to the interpretation that the clone is responding to that particular location, this is purely an interpretive convenience for the experimenter. The agent itself has no mechanism by which it can derive a place field from the activity of its neurons. As we show in the next section, the agent does not need to compute place fields to locate itself, nor need to decode locations from the clones to make navigation decisions.</p><p>CSCGs make complex latent transitive inferences during learning, and represent the learned information to enable novel transitive inferences <ref type="bibr" target="#b18">(Eichenbaum et al., 1999)</ref>. When different overlapping sections of an environment are exposed to the agent in disjoint episodes, CSCGs learn the underlying 𝐷. When environments are really disjoint, CSCGs learn to separate the maps, and simultaneously represent multiple maps in memory without being explicitly instructed about map boundaries during training (Fig. <ref type="figure" target="#fig_1">2D</ref>). The appropriate map can then be recalled as hidden state inference <ref type="bibr">(Sanders et al., 2020b)</ref>, and used to guide behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replay-based planning and schema-based transfer enable shortcut inference in dynamic settings</head><p>A behaving agent can keep track of its state as the most likely clone given past observations, without having to invoke any concepts about space or place fields. If the agent wants to navigate to a remembered goal of a visual sensation, the action sequences that achieve this can be inferred directly by clamping the corresponding clones and propagating messages. If the environment is learned correctly, this inference process is exact, and will recover the sequence of actions that will take the agent from the current state to the desired goal. Message-passing based planning in CSCGs is akin to replays in the hippocampus <ref type="bibr">(Ólafsdóttir et al., 2018)</ref>. A striking advantage of CSCG in comparison to models that purely predict, is that learned maps can be quickly reconfigured to reflect changes in the environment. When a previously passable route is blocked, the corresponding structural modification can be made in the latent graph, and message-passing based inference will utilize this updated information about the environment to navigate around obstacles.</p><p>CSCGs can also transfer prior knowledge to new environments and infer novel shortcut paths through unobserved locations by treating the learned transition graph as a schema <ref type="bibr" target="#b3">(Baraduc et al., 2019;</ref><ref type="bibr" target="#b4">Barry et al., 2006)</ref> and learning just the emission matrix. To demonstrate this ability, we first trained a CSCG using aliased observations from a random walk in a room (room 1, Fig. <ref type="figure">3A</ref>). Next, we placed the agent in an unfamiliar room with the same structure (room 2, Fig. <ref type="figure">3B</ref>). As the agent walks in the new room, we keep the transition matrix of the CSCG fixed and update the emission matrix with the EM algorithm. Just by walking along the periphery of room 2, the CSCG is able to infer the shortest path between visited locations through previously unvisited locations. Further, if we block the path with obstacles and a planned action fails, the CSCG is able to initiate replanning at the blocked location and reroute to the target (Fig. <ref type="figure">3C</ref>). Thus, even with partial knowledge of a novel room, an agent can vicariously evaluate the sequence of actions to be taken to reach a destination by reusing the CSCG's transition graph from a similar, previously experienced room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remapping due to changes in overall geometry, visual cues, or landmarks can be explained using sequence learning</head><p>Changing the interpretation of place fields from explicitly representing spatial locations to representing the sequential context in which a sensation occurs explains a wide variety of place cell remapping phenomena. In the transition graph in Fig. <ref type="figure" target="#fig_1">2A</ref>(iii), each state should be interpreted, not as responding to a specific location in the room, but as responding to the specific sequence of observations leading up to that location. As we demonstrate, sequential interpretation of spatial representations can explain a variety remapping driven by changes in geometry, visual cues, transparent or opaque barriers, where similar sequential contexts will be observed. In these experiments, place fields that developed while the rat trained in an arena remapped in a geometry-dependent manner when the arena was  <ref type="bibr">(1987)</ref>. A CSCG was first trained using a random walk in a circular room with a cue at the 12 o' clock position. Place fields were then computed in the training room, the room with the cue rotated by 90°, and the room with a barrier introduced. For each clone, we observe that the place fields also rotate by 90°when the cue is rotated. For most clones, the place fields completely disappear when the barrier is introduced. An exception is clone 179, where the place field in the original layout is sufficiently far away from the barrier. C. Landmark vector cells, <ref type="bibr" target="#b12">Deshmukh and Knierim (2013)</ref>. A CSCG was trained in a rectangular room with a landmark (depicted by a hexagon) on one side of the room. Place fields were then computed in the training room as well as a modified room in which the landmark was moved to a different location. For the modified layout, we observe that the place fields now have two components -one at the same location as the place field in the original layout, and another at the same vector displacement from the new location of the landmark. D. A compressed representation of spatial distance in the rodent hippocampus, <ref type="bibr" target="#b45">Sheehan et al. (2021)</ref>. A CSCG was trained on a linear track of length 18 steps using outbound (left to right) and inbound (right to left) walks. Place fields were computed using trials with different starting positions for the outbound trajectories and different end positions for the inbound trajectories. The top and bottom rows correspond to place fields in the reference frame of the track and the start box, respectively. We observe that most place cells coded for distance from the starting box. A few clones, e.g., clones 8 and 20, are anchored to the end box. Further, clones have gradually widening fields with distance from the starting or ending locations. elongated or widened. We demonstrate this by first training a CSCG on a small square (SS) room (size 9 × 9) and uniform interior and observing the place field changes of clones in test rooms that varied in size along the two dimensions (see Fig <ref type="figure" target="#fig_3">4A</ref>). The activations of clones in a CSCG represents the posterior distribution over latent states given the past sequence of observations. As described earlier, the specific sequential context in which clones activate can be interpreted as coding for location. Since the interiors of a uniform room have undifferentiated local sequential context, the responses of clones in the center will be anchored with respect to the boundaries because of the relative uniqueness of the observations there. When navigating an elongated room using the CSCG learned from the smaller room, the internal states will reliably signal end-of-room states when the agent is near the boundaries of the new room. This effectively creates two loci for sequential contexts. The same clone that fired in the sequential context corresponding to a specific location in the original room will now fire at two different locations due to the splitting of the sequential contexts in the elongated room, as reflected in the remapped responses of clones 161 and 748 in Fig. <ref type="figure" target="#fig_3">4A</ref>. In contrast, the response of clone 314 does not remap and remains the same in all four rooms. This is because this neuron's sensory input already includes part of the boundary, and also because the sequence it represents has shorter undifferentiated segments from the boundary, making it strongly anchored. Although these results were originally characterized as boundary-vector coding, our results show that the major findings of O'Keefe and Burgess (1996) can be explained using sequence representation without using geometric concepts.</p><p>As we describe later, the sequence perspective also naturally explains the temporal dependence of the remapped place fields. Of course, with further training in the new environment, the remapping will diminish because new place fields representing the new environment will develop with more experience in that environment.</p><p>The classic Muller and Kubie experiments <ref type="bibr" target="#b28">(Muller and Kubie, 1987)</ref> showing a variety of remapping phenomena can also be explained using CSCG, which we illustrate in Fig. <ref type="figure" target="#fig_3">4B</ref>. In these experiments, rats were trained in a circular arena with a cue card placed on the wall. Researchers found a variety of remapping phenomena with respect to rotation of the cue card and introduction of opaque or transparent barriers. To investigate these phenomena, we first trained a CSCG in a circular arena with a cue card at the 12 o' clock position. In this environment, the differentiated sequential contexts will develop with reference to the cue card. When we computed place fields with this CSCG in an arena where the cue was rotated, the place fields also rotated accordingly because they are always referenced to the context and not the absolute location. Placing a barrier in the arena has two effects that destroy the place field for some clones. One effect is that the barrier prevents the agent from taking some trajectories that are important for revealing the relevant sequential contexts for some clones. The second is that the presence of the barrier can change the visual sensation in its vicinity. Both these effects combine to explain why place fields are disrupted when a barrier is placed through its center, and not affected when the barrier is far away.</p><p>CSCGs also explain why place cells can be seen as encoding a vector relationship to local landmarks <ref type="bibr" target="#b12">(Deshmukh and Knierim, 2013)</ref>. Just like cue cards, or boundaries, landmark objects placed in an environment act as disambiguating contexts with respect to which sensations at other locations are encoded. Thus, when a landmark is moved, some of sequential contexts also move in reference to that landmark. We illustrate this landmark vector remapping phenomenon in Fig. <ref type="figure" target="#fig_3">4C</ref>. We first trained a CSCG in a rectangular layout with a landmark on one side of the room. We computed place fields in this layout as well as a modified version in which the landmark was moved to a different location. In the modified layout, the place fields now have two components -one at the same location as in the original layout, and the second at the same relative displacement from the new location of the landmark.</p><p>In more recent experiments <ref type="bibr" target="#b45">(Sheehan et al., 2021)</ref>, rats were trained on outbound and inbound traversals on a linear track that could be changed in length. Responses to the appropriate sequential contexts in a CSCG naturally explain the remapping of place fields observed as the track length varies. To demonstrate this, we first trained a CSCG on a linear track of length 18 steps using both outbound (left to right) and inbound (right to left) walks. We then computed place fields separately on outbound and inbound trajectories, for various track lengths (Fig. <ref type="figure" target="#fig_3">4D</ref>). We observed that most clones coded for distance from the starting position. The place fields gradually widened with distance from the starting position reflecting the growing uncertainty in the distance from the starting point. There were also clones anchored to the end point of the trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence representation can explain puzzling phenomena that mix spatial and temporal effects</head><p>Sequential contexts naturally explain the direction sensitivity of place field remapping reported in O' <ref type="bibr">Keefe and Burgess (1996)</ref>. When the room is elongated, some place fields that were unimodal in the original room remapped to produce two peaks, corresponding to two subcomponents in the elongated room. It was observed that these peaks were direction sensitive: the left subcomponent was active during rightward travel and vice versa. We tested CSCG for the same effects using the same settings as in Fig. <ref type="figure" target="#fig_3">4A</ref>, by plotting the fields conditioned on the direction of travel. In the HR room, rightward and leftward trajectories of the agent strongly activated the left and right peaks of the place field, respectively, as shown in Fig. <ref type="figure" target="#fig_5">5A</ref>. This is because only one of the sequential contexts that activate a clone occurs in a directional walk, which is a natural consequence of representing locations using sequential contexts. In contrast, a purely geometric model like the boundary vector model <ref type="bibr" target="#b4">(Barry et al., 2006)</ref> does not offer an explanation for the direction sensitivity of place field remapping.</p><p>CSCG can also explain recently discovered phenomena like event-specific rate remapping (ESR) cells <ref type="bibr" target="#b48">(Sun et al., 2020)</ref>, which signal a combination of location and lap number for different laps around a maze, without postulating special coding mechanisms. Fig. <ref type="figure" target="#fig_5">5B</ref> shows a similar setting to an  <ref type="bibr">(1996)</ref>. We first trained a CSCG in a square room. In an elongated rectangular room, place fields of clones of the CSCG elongate with two strong peaks (second column from the left). The left peak is located such that its distance from the left wall is the same as in the square room. The same holds true for the right peak relative to the right wall. Further, when we use directional random walks to compute place fields, the two independent subcomponents corresponding to these peaks are revealed. A particular subcomponent is stronger when the walk starts from the wall to which the location of the subcomponent is tied: left and right components in the → and ← walks, respectively. (continued on next page) experiment in <ref type="bibr" target="#b48">Sun et al. (2020)</ref> where a rat runs multiple laps in a looping rectangular track before receiving a reward. We trained a CSCG on trials comprising three laps of a rectangular track with a reward state at the end of the third lap. A CSCG exposed to the sequence of observations from such Lap-neurons and event-specific representations. A CSCG was trained on observations from laps in a rectangular maze similar to <ref type="bibr" target="#b48">Sun et al. (2020)</ref>. The training sequence consisted of three laps followed by a reward state (green * ) at the end. We also considered test trials with four laps where the reward state was at the end of the fourth lap. We show the place fields computed using the training and test trials, respectively. Rows correspond to clones. The place fields on the training trials show that there are different clones that are maximally active for different laps.</p><p>But most clones are also partially active at their corresponding location in other laps, similar to the neurophysiological observations in <ref type="bibr" target="#b48">Sun et al. (2020)</ref>. For the test trials, we observe that the lap three clones are significantly active in both the third and the fourth laps. This additional activation shifted precisely by one lap reflects the fact that the third lap is no longer rewarded and that an extra lap is needed to receive a reward.</p><p>trials learned to distinguish the laps and to predict the reward at the end of the third lap, without the help of any explicit lap-boundary markers in the training sequence. This is reflected in the place fields of the clones for the training trials (left panel in Fig. <ref type="figure" target="#fig_5">5B</ref>) -each clone is maximally active for an observation when it occurs in its specific lap. However, each clone also shows weak activations when its corresponding observation is encountered in other laps, a signature of ESR. This occurs naturally in the CSCG due to smoothing and the inference dynamics. CSCGs can also explain the remapping of ESR cells. We computed place fields on test trials comprising four laps, instead of three, in which the the reward was at the end of the fourth lap. The lap three clones were strongly activated in both the third and the fourth laps, reflecting the change in when the reward state is reached (right panel in Fig. <ref type="figure" target="#fig_5">5B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSCG can predict what kinds of changes in the environment lead to remapping</head><p>CSCGs show that environmental connectivity changes need not lead to place field remapping even when the agents' behavior shows adaptation to the change, a phenomenon that researchers found puzzling. In <ref type="bibr" target="#b14">Duvelle et al. (2021)</ref>, rats ran in a 4-room maze where the doors connecting the rooms could be selectively locked to change the connectivity of the arena. The agent's behavior reflected that it recognized the connectivity changes of the environment, but the place fields did not remap in response to these connectivity changes. The authors found this lack of remapping puzzling and argued that place cells do not encode a topological map. However, CSCGs show that place cells can encode global location in their activations, global topology in the cell-to-cell connectivity, and still not show remapping in response to the manipulations in <ref type="bibr" target="#b14">Duvelle et al. (2021)</ref>.</p><p>To demonstrate this, we trained a CSCG using a random walk in an environment comprising four square rooms that are connected by two-way doors, similar to the experimental setting in <ref type="bibr" target="#b14">Duvelle et al. (2021)</ref>. Each room had visual cues that distinguished it from the other rooms. CSCG learned the global topology of the arena in the transition matrix, and the activation of clones corresponded to locations, as in previous experiments (Fig. <ref type="figure" target="#fig_6">6A</ref> top row). We then tested for two environmental modifications used in Duvelle et al. ( <ref type="formula">2021</ref>) -(i) one door was locked both ways effectively creating a blockade, and (ii) all doors were locked in one way allowing only an anti-clockwise direction of traversal in the environment. The corresponding modifications were made in the CSCG transition matrix by modifying the connections appropriately, and planning routes in this modified CSCG corresponded to the reported successful navigation. We then computed place fields using the appropriately modified CSCGs paired with the arena connectivity changes, and compared these to the fields from the original CSCG in the original arena. In Fig. <ref type="figure" target="#fig_6">6A</ref>, we show that the place fields were the same across all three settings, consistent with the observations in <ref type="bibr" target="#b14">Duvelle et al. (2021)</ref>.</p><p>The reason for lack of remapping can be understood by realizing that the connectivity change blocked paths without any change in the visual cues. The blocked path affected only a few of the potential sequences that were responsible for that place field, a change that is too small to be reflected in the aggregated sequential responses. However, the connectivity change can still lead to large changes in behavior, for example, in navigation between the two rooms. In CSCGs, those changes will be reflected in the replay messages used for planning, and in the computed shortest paths.</p><p>Sequence learning explains place field repetition, size and shape variations.</p><p>Place fields distort along the boundaries, and increase in size systematically towards the center of an empty arena <ref type="bibr" target="#b50">(Tanni et al., 2022)</ref>. In very elongated rooms, place fields have multiple lobes. In some settings, place fields are known to repeat in identical rooms <ref type="bibr" target="#b19">(Fuhs et al., 2005;</ref><ref type="bibr" target="#b46">Skaggs and McNaughton, 1998)</ref>. While all these phenomena appear to be spatial, CSCGs provide cogent explanations for these in terms of sequence learning: all of them result from state aliasing due to the difficulty in creating different latent states for temporal contexts that are identical for long number of steps.</p><p>To demonstrate place field repetition in visually identical environments, we trained a CSCG in a layout comprising two visually identical rooms in the same orientation and connected by a corridor, as shown in Fig. <ref type="figure" target="#fig_6">6B</ref>, similar to the setting in <ref type="bibr" target="#b19">Fuhs et al. (2005)</ref>. Place fields computed in this layout show repetition, i.e., clones are active at the same location in both rooms. We also considered a layout in which the two rooms were abutted by rotating them such that their orientations differed by 180°. In <ref type="bibr" target="#b19">Fuhs et al. (2005)</ref>, it was reported that place field repetition disappears in the differentorientation setting. This was attributed to the rats potentially being able to maintain their inertial angular orientation. With CSCGs, in the absence of an external "compass", we observe that place field repetition persists in the modified layout, even after the introduction of an asymmetric connection between the two rooms. However, when the CSCG was retrained on the different-orientation setting with an asymmetric connection between the rooms, it was able to partially split contexts in the two rooms. This resulted in unique place fields for most clones, as show in Fig. <ref type="figure" target="#fig_6">6C</ref>. If the sensory input to CSCG is augmented with an external head direction input, then the different orientation setting results in unique place fields in CSCGs, similar to what is observed in <ref type="bibr" target="#b19">Fuhs et al. (2005)</ref>.</p><p>In Fig. <ref type="figure" target="#fig_6">6D</ref>, we reproduce the direction-dependent place field repetition reported in <ref type="bibr" target="#b11">Derdikman et al. (2009)</ref>. We trained a CSCG on a hairpin maze, with distinct end markers, using left to right (𝐿 → 𝑅) and right to left (𝑅 → 𝐿) walks. Place fields computed using this CSCG using only 𝐿 → 𝑅 or 𝑅 → 𝐿 walks reveal direction dependent place field repetition, as shown in Fig. <ref type="figure" target="#fig_6">6D</ref>. For example, clone 17 is activated at the same location in all segments of the maze, but only in the 𝐿 → 𝑅 traversal. The two ends of the maze have different observations, which provides the CSCG enough context to disambiguate the two directions of travel. However, for each direction of traversal, the observations are the same in all segments of the maze resulting in the repetition of place fields.</p><p>To study the effect of room size on place fields <ref type="bibr" target="#b50">(Tanni et al., 2022)</ref>, we trained three different CSCGs on square rooms, with uniform interiors, of side length 7, 9 and 11, respectively. As an agent moves away from the boundaries to the center of an empty room, different sequential trajectories start to look the same, making it difficult for the learning algorithm to split the contexts into different clones. This results in the same clone representing more contexts than it would in the periphery of the room where contexts can be easily distinguished. In place field mapping, this will appear as an enlargement of the place fields in the center of the room ("center" column in Fig. <ref type="figure" target="#fig_6">6E</ref>). Similarly, the observations along the edge of a room might not all develop into distinct clones, resulting in multiple observations along the edge being aliased into the same clone. This aliasing, due to the elongation of the same evidence, will appear as an elongation of the place field ("edge" column in Fig. <ref type="figure" target="#fig_6">6E</ref>). We also considered a second modification, where we introduced an asymmetery in the connection between the abutting rooms. In all these three layouts, we observed place field repetition across the two rooms. Note that these results are incongruent with those in <ref type="bibr" target="#b19">Fuhs et al. (2005)</ref>, where they observed place field repetition only in the same orientation layout. C. However, when we retrain a CSCG in the opposite orientation layout with asymmetric connectivity, we observe that the place field repetition disappears. (continued on next page)</p><p>Place field size expansion <ref type="bibr" target="#b50">(Tanni et al., 2022)</ref> in an empty arena happens because of the same reason as place field repetition in two identical iso-oriented rooms. Both can be explained by the inability of the model to split very long-term temporal contexts into distinct latent clones with the Figure <ref type="figure" target="#fig_6">6</ref> (continued) | CSCG can reproduce various observations about place cells such as place field repetition and size, shape variations. D. Direction dependent place field repetition in a hairpin maze, <ref type="bibr" target="#b11">Derdikman et al. (2009)</ref>. A CSCG was trained using 𝐿 → 𝑅 and 𝑅 → 𝐿 walks on a hairpin maze. Place fields were then computed using only 𝐿 → 𝑅 (top row) or 𝑅 → 𝐿 (bottom row) walks. We observe direction dependent repetition of place fields. E. Place field size and shape as a function of room size, <ref type="bibr" target="#b50">Tanni et al. (2022)</ref>. We trained three different CSCGs on square rooms, with uniform interiors, of side length 7, 9 and 11, respectively. The capability of a CSCG to learn the map of a room with uniform interior degrades as the room gets larger. This is reflected in the place fields. Place fields that are anchored to the corner of the room retain their size and shape across sizes. However, place fields at the edge and center of the room elongate and become larger as the room size increases.</p><p>given amount of training. (Of course longer training will partially overcome this problem, which is observed in animals as well.) In that sense, larger place fields are the same as place field repetition, just happening in adjacent locations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novel experiments and predictions</head><p>CSCGs can also make experimentally testable predictions for yet to be observed phenomena. One such prediction is the following. What controls how place fields change is not the rate of visual change, but the uniqueness of the visual context. To demonstrate this, we trained two CSCGs on square rooms with checkerboard and random patterns on the floor, respectively. We observed that the place fields in the checkerboard room were more expanded, as shown in Fig. <ref type="figure" target="#fig_7">7A</ref>. This is because the same context repeats throughout the interior of the room, making it difficult for the learning to split the contexts into different clones.</p><p>CSCGs provide a mechanistic explanation for the question of when and why do place fields globally or partially remap? The answer: place cell responses are driven by their sequential contexts, and changes that significantly affect the sequential context of a neuron is what determines when and how its field will remap. Any change that makes the same sequential context occur in different parts of the room, will result in that field partially appearing in the new place. The organization and specificity of local context driving the responses of a cell will have a significant impact on its remapping. A cell that is tuned to sequences in the middle of a uniform room, will have its place fields anchored by the boundaries that are relatively more unique, causing the field to remap when when the boundaries are moved. However, if the cell had some other local cues, for example markings on the floor, that would provide it a unique sequential context, then the cell's field will not remap when the boundary is moved. In Fig. <ref type="figure" target="#fig_7">7B</ref>, clone 15 and clone 128 are two cells from the CSCG trained in the training layout. When the the room is elongated, the place field of clone 128 remaps as shown. This is because the local sequential context for clone 128 was anchored by the cyan marking on the left and the boundary on the right. These partial contexts occur in two different place in the elongated room. In contrast, the local sequential context for clone 15 is anchored by the blue marking on its left and the cyan marking on its right, and those did not change when the room was elongated. This means, locally, clone 15 will see the same sequential contexts after room elongation, resulting in a lack of remapping in its place field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The discovery of place cells is a striking success of hippocampus research, and place field mapping has served as a valuable tool in revealing the representational properties of neurons in the HPC. However, anomalies have been accumulating over the simple view that place cells represent just locations <ref type="bibr" target="#b0">(Acharya et al., 2016;</ref><ref type="bibr" target="#b7">Buzsáki and Llinás, 2017;</ref><ref type="bibr" target="#b48">Sun et al., 2020)</ref>. Place fields distort around boundaries, and split along trajectories. They are direction sensitive <ref type="bibr" target="#b0">(Acharya et al., 2016;</ref><ref type="bibr" target="#b25">Moore et al., 2021)</ref>, and can even represent the lap count in running loops <ref type="bibr" target="#b48">(Sun et al., 2020)</ref>. In some cases, insertion of a boundary in between a place field clearly disrupts the place field, suggesting that fields are related to the connectivity of the underlying environment <ref type="bibr" target="#b28">(Muller and Kubie, 1987</ref>). Yet, place fields can remain unchanged when the connectivity of the environment changes without any visible cues <ref type="bibr" target="#b14">(Duvelle et al., 2021)</ref>. If the environmental change is not reflected in place fields, how are the rats able to change their behavior in the new environment? In summary, many of these questions about the role of place cells -what they represent, how those representations are learned, how they are used, and how they change with respect to environmental manipulations remain unanswered in the location-centric description of hippocampal neurons.</p><p>In this paper, we pursued the strong hypothesis that the hippocampus performs a singular algorithm that learns a sequential, relational, content-agnostic structure of its environment <ref type="bibr" target="#b7">(Buzsáki and Llinás, 2017;</ref><ref type="bibr" target="#b9">Dabaghian et al., 2014)</ref>, and demonstrated evidence for its validity. Our learning model, the CSCG, inverts the observation stream to learn a latent generative model that is producing the stream of sensations. With CSCGs, we demonstrated how a vast variety of experimental observations about hippocampal place cells can be explained by the singular key insight that spatial representation is an emergent property of latent higher-order sequence learning. We demonstrated this by first showing that pure temporal learning is sufficient to acquire cognitive maps that have locations, space, heading etc. We also showed how such latent graphs can transfer knowledge across environments. And finally, we showed that multiple phenomena that we observe are natural byproducts of sequence learning and inference, without having to directly model the phenomena itself.</p><p>While CSCG draws up on many past and contemporary models of hippocampus <ref type="bibr" target="#b51">(Uria et al., 2022)</ref>, it is significantly different in many aspects. In contrast to temporal context models (TCM) <ref type="bibr" target="#b21">(Howard and Kahana, 2002)</ref> that accumulate sequential context in the observation space, the sequential representation in CSCG is in the latent space, giving it the ability to model more complex and long duration temporal dependencies. The ability of CSCG to represent locations as sequences crucially depends up on having a latent representation. Although successor representations <ref type="bibr" target="#b47">(Stachenfeld et al., 2017)</ref> can model temporal relations, they are not directly applicable in the aliased settings we consider here, and do not learn spatial representations from egocentric sensory inputs. Contemporary work on Tolman-Eichenbaum machines (TEM) <ref type="bibr" target="#b53">(Whittington et al., 2018)</ref> have many similarities to CSCG in inspiration. However, unlike CSCG, TEMs do not learn latent graphs in aliased settings like ours. Instead, TEM focuses on learning general transitivity rules applicable to a single graph from multiple noisy realizations of that graph. Moreover, TEMs do not deal with multiple graphs at the same time <ref type="bibr">(Sanders et al., 2020a)</ref>, or do latent transitive stitching. In the context of learning spatial representations, TEMs have so far been demonstrated only in allocentric settings. More importantly, TEM is formulated purely as a predictive model and its internal representation does not learn a modifiable graph that corresponds to the environment. Therefore, TEM doesn't have the same ability as CSCGs to deal with dynamic environments quickly by changing its graph connectivity, or to form hierarchies through community detection <ref type="bibr" target="#b43">(Schapiro et al., 2016)</ref> on the latent graph.</p><p>Unlike other computational models of place fields, CSCGs do not use grid fields to learn place fields and still explain varied remapping phenomena. Recent experimental evidence suggests that grid cells are not necessary for learning <ref type="bibr" target="#b6">(Brandon et al., 2014;</ref><ref type="bibr" target="#b49">Tan et al., 2017)</ref> and continued functioning of place cells <ref type="bibr" target="#b6">(Brandon et al., 2014)</ref>. If grid cells outputs are available, CSCG can utilize those as additional sensations. This would speed up learning in the middle portions of empty arenas where unique sensations are not available, and it will also help stabilize the place fields away from the boundaries or other landmark cues <ref type="bibr" target="#b24">(Mallory et al., 2018;</ref><ref type="bibr" target="#b26">Muessig et al., 2015)</ref>, consistent with the idea of grid cells providing an optional scaffolding for place cells <ref type="bibr" target="#b27">(Mulders et al., 2021)</ref>.</p><p>The most important message from our work is that many of the diverse fascinating hippocampal phenomena might be artifacts of Euclidean place field mapping. Hippocampal cells are usually interpreted by plotting their responses on to a 2D map corresponding to the environment, collapsing the sequential responses in to a static place field. Characterizing place field maps in terms of Euclidean concepts is akin to characterizing the effects rather than the underlying causes, and might be the source of new phenomena. Often these new phenomena are explained away invoking familiar, but ultimately unsatisfactory, answers like distributed coding, or mixed selectivity. These answers are unsatisfactory because instead of answering the questions they just shift the questions elsewhere. Our experiments show that phenomena that look extremely different -for example place-field expansion in a uniform room and event-specific responses in a lap running -can have the same underlying explanation which can be understood through the sequence learning model. We hope this opens up a new avenue of exploration that takes us away from the familiar questions centered on encoding and decoding locations.</p><p>Much remains to be explored on this new path we have struck out on. We have only briefly touched up on replay based planning, and schemas, and both can be expanded in future research. Our work can also be expanded in the direction of active learning and inference. Reward mechanisms can be layered on top of CSCG. CSCGs have the ability for temporal abstractions via community detection <ref type="bibr" target="#b43">(Schapiro et al., 2016)</ref> on the underlying graphs, an idea worthy of more exploration. Current models are learned using random walks. Potentially efficient exploration techniques can be developed as active learning on CSCGs. Most importantly, we hope our work gives a concrete tool that would help hippocampal researchers think beyond the place field paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expectation-Maximization learning of CSCGs</head><p>Cloned Hidden Markov Models (HMMs), first introduced in <ref type="bibr" target="#b10">Dedieu et al. (2019)</ref>, are a sparse restriction of overcomplete HMMs <ref type="bibr" target="#b44">(Sharan et al., 2017)</ref> that can overcome many of the training shortcomings of dynamic Markov coding <ref type="bibr" target="#b8">(Cormack and Horspool, 1987)</ref>. Similar to HMMs, cloned HMMs assume the observed data 𝑥 1 , . . . , 𝑥 𝑁 are generated from a hidden process 𝑧 that obeys the Markovian property 𝑃(𝑥 1 , . . . , 𝑥 𝑁 , 𝑧 1 , . . . , 𝑧 𝑁 ) = 𝑃(𝑧 1 )</p><formula xml:id="formula_2">𝑁-1 𝑛=1 𝑃(𝑧 𝑛+1 |𝑧 𝑛 ) 𝑁 𝑛=1 𝑃(𝑥 𝑛 |𝑧 𝑛 )</formula><p>Here 𝑃(𝑧 1 ) is the initial hidden state distribution, 𝑃(𝑧 𝑛+1 |𝑧 𝑛 ) is the transition probability from 𝑧 𝑛 to 𝑧 𝑛+1 , and 𝑃(𝑥 𝑛 |𝑧 𝑛 ) is the probability of emitting 𝑥 𝑛 from the hidden state 𝑧 𝑛 .</p><p>In contrast to HMMs, cloned HMMs assume that each hidden state maps deterministically to a single observation. Further, cloned HMMs allow multiple hidden states to emit the same observation. All the hidden states that emit the same observation are called the clones of that observation.</p><p>CSCGs build on top of cloned HMMs by augmenting the model with the actions of an agent. In this section we first review the expection-maximization learning of cloned HMMs, before describing the learning of CSCGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expectation-Maximization learning of Cloned HMMs</head><p>The standard algorithm to train HMMs is the expectation-maximization (EM) algorithm <ref type="bibr" target="#b54">(Wu et al., 1983)</ref>, which in this context is known as the Baum-Welch algorithm. Learning a cloned HMM using the Baum-Welch algorithm requires a few simple modifications: the sparsity of the emission matrix can be exploited to only use small blocks of the transition matrix both in the Expectation (E) and Maximization (M) steps.</p><p>Learning a cloned HMM requires optimizing the vector of prior probabilities 𝜋: 𝜋 𝑢 = 𝑃(𝑧 1 = 𝑢) and the transition matrix T: T 𝑢𝑣 = 𝑃(𝑧 𝑛+1 = 𝑣|𝑧 𝑛 = 𝑢). To this end, we assume the hidden states are indexed such that all the clones of the first emission appear first, all the clones of the second emission appear next, etc. Let 𝑁 obs be the total number of emitted symbols. The transition matrix T can then be broken down into smaller submatrices T(𝑖, 𝑗), 𝑖, 𝑗 ∈ {1, . . . , 𝑁 obs }. The submatrix T(𝑖, 𝑗) contains the transition probabilities 𝑃(𝑧 𝑛+1 |𝑧 𝑛 ) for 𝑧 𝑛 ∈ 𝐶 (𝑖) and 𝑧 𝑛+1 ∈ 𝐶 ( 𝑗), where 𝐶 (𝑖) and 𝐶 ( 𝑗) correspond to the hidden states (clones) of emissions 𝑖 and 𝑗 respectively.</p><p>The standard Baum-Welch equations can then be expressed in a simpler form in the case of cloned HMM. The E-step recursively computes the forward and backward probabilities and then updates the posterior probabilities. The M-step updates the transition matrix via row normalization. where • and denote the element-wise product and division, respectively (with broadcasting where needed). All vectors are 𝑀 × 1 column vectors, where 𝑀 is the number of clones per emission. We use a constant number of clones per emission for simplicity here, but the number of clones can be selected independently per emission. Cloned HMMs exploit the sparsity pattern in the emission matrix when performing training updates and inference, and achieve significant computational savings when compared with HMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSCGs: Action-augmented cloned HMMs</head><p>CSCGs are an extension of cloned HMMs in which an action happens at every timestep (conditional on the current hidden state) and the hidden state of the next timestep depends not only on the current hidden state, but also on the current action. The joint probability density function on the observations and the actions is given by 𝑃(𝑥 1 , . . . , 𝑥 𝑁 , 𝑎 1 , . . . , 𝑎 𝑁-1 ) = ∑︁ 𝑧 1 ∈𝐶 (𝑥 1 )</p><p>. . .</p><formula xml:id="formula_3">∑︁ 𝑧 𝑛 ∈𝐶 (𝑥 𝑛 ) 𝑃(𝑧 1 ) 𝑁-1 𝑛=1 𝑃(𝑧 𝑛+1 |𝑧 𝑛 , 𝑎 𝑛 )𝑃(𝑎 𝑛 |𝑧 𝑛 )</formula><p>and the standard cloned HMM can be recovered by integrating out the actions.</p><p>We group the actions with the next hidden state to remove loops, and create a chain that is amenable to exact inference. In other words, we rewrite the joint probability density function as 𝑃(𝑥 1 , . . . , 𝑥 𝑁 , 𝑎 1 , . . . , 𝑎 𝑁-1 ) = ∑︁ 𝑧 1 ∈𝐶 (𝑥 1 )</p><p>. . .</p><formula xml:id="formula_4">∑︁ 𝑧 𝑛 ∈𝐶 (𝑥 𝑛 ) 𝑃(𝑧 1 ) 𝑁-1 𝑛=1 𝑃(𝑧 𝑛+1 , 𝑎 𝑛 |𝑧 𝑛 )</formula><p>Learning a CSCG requires optimizing the vector of prior probabilities 𝜋: 𝜋 𝑢 = 𝑃(𝑧 1 = 𝑢) and the action-augmented transition matrix T: T 𝑢𝑣𝑤 = 𝑃(𝑧 𝑛+1 = 𝑣, 𝑎 𝑛 = 𝑤|𝑧 𝑛 = 𝑢). Similar to cloned HMMs, we can break the action-augmented transition matrix T into smaller submatrices T(𝑖, 𝑘, 𝑗), 𝑖, 𝑗 ∈ {1, . . . , 𝑁 obs }, 𝑘 ∈ {1, . . . , 𝑁 actions }. The submatrix T(𝑖, 𝑘, 𝑗) contains the transition probabilities 𝑃(𝑧 𝑛+1 , 𝑎 𝑛 = 𝑘|𝑧 𝑛 ) for 𝑧 𝑛 ∈ 𝐶 (𝑖), 𝑧 𝑛+1 ∈ 𝐶 ( 𝑗), where 𝐶 (𝑖) and 𝐶 ( 𝑗) correspond to the hidden states (clones) of emissions 𝑖 and 𝑗 respectively. All the previous considerations about cloned HMMs apply to CSCGs and the EM equations for learning are also very similar: In <ref type="bibr" target="#b20">George et al. (2021)</ref>, it was observed that the convergence of EM for learning the parameters of a CSCG can be improved by using a smoothing parameter called the pseudocount. The pseudocount is a small constant that is added to the accumulated counts statistics matrix ( 𝑁 𝑛=1 𝜉 𝑖𝑘 𝑗 (𝑛)), which ensures that any transition under any action has a non-zero probability. This ensures that the model does not have zero probability for any sequence of observations at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Egocentric actions and observations</head><p>To demonstrate how CSCGs can reproduce various experimental findings regarding the hippocampus, we consider experimental setups where an agent performs egocentric actions and makes egocentric observations. We assume that the agent is exploring a layout on an axis-aligned grid. At each time step, the agent can perform one of three actions: (i) go forward by one step along its current heading, (ii) turn left by 90°, and (iii) turn right by 90°. Note that the agent's heading at any given time can only be one of four headings, which we denote by the four cardinal directions.</p><p>The observation of the agent at any given time depends on its position in the layout and its current heading. We assume that the agent has a field of view of length 𝑓 𝑙 and width 𝑓 𝑤 . This field of view is such that the agent can see up to 𝑓 𝑙 -1 steps in front and 1 step behind it, and is symmetric along the width axis. Fig. <ref type="figure" target="#fig_10">8A</ref> shows the four heading dependent observations at the location marked by the black dot in an example 7 × 7 square layout. Inaccessible or invisible regions in the field of view are marked by gray slashes. Fig. <ref type="figure" target="#fig_10">8B</ref> shows the set of all possible observations of size ( 𝑓 𝑙 = 4, 𝑓 𝑤 = 3) for this example layout. Each possible observation is also assigned a label/index. Fig. <ref type="figure" target="#fig_11">9A</ref> (right panel) shows the observation index at each position, for all four possible headings, in the example layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSCGs in allocentric vs. egocentric settings</head><p>For some of the results used to highlight the properties of a CSCG, we used allocentric actions and observations. In this setting, at each time step, the agent can perform one of four actions: go (i) left, (ii) right, (iii) up, or (iv) down. The observation at each time step is just the color/index of the current location in the layout.</p><p>Here, we compare CSCGs trained using observations and actions in allocentric vs. egocentric settings. We use a square layout of size 7 × 7, as shown in Fig. <ref type="figure" target="#fig_10">8A</ref>. For the egocentric setting we use a In each setting, we collected a stream of 50, 000 action-observations pairs. We allocated 50 clones per observation, set the pseudocount to 5 × 10 -4 , and ran EM for a maximum of 1000 iterations to train a CSCG. We then used Viterbi decoding to identify the relevant states/clones that are in use. Fig. <ref type="figure" target="#fig_11">9C</ref> shows the learned transition graphs of the CSCGs learned in the two settings. In both cases, each node in the graph corresponds to a clone and its color corresponds to the observation the clone emits. The edges correspond to non-zero probability transitions between clones. In the allocentric case, each spatial location is represented by one clone in the graph. On the other hand, in the egocentric case, each location is represented by four clones corresponding to four possible headings. Importantly, a CSCG is able to learn a graph that correctly represents the topology of the environment in either setting, without any spatial information as inputs during learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place fields of clones in CSCGs</head><p>Given a sequence of observations and actions, we define the activation of a clone 𝑖 at time 𝑛 as the following marginal posterior probability, 𝑟 𝑖 (𝑛) = 𝑃(𝑧 𝑛 = 𝑖|𝑥 1 , . . . , 𝑥 𝑛 , 𝑎 1 , . . . , 𝑎 𝑛-1 )</p><p>Since the CSCG model (with the action 𝑎 𝑛-1 and hidden state 𝑧 𝑛 collapsed in a single variable) forms a chain, inference on it using belief propagation is exact. The marginal posterior distribution can be To compute place fields, we first obtain activations of the clones from 𝑁 trials random walks of the agent, each of length 𝑁 seg , in an environment. We can then use the agent's ground truth spatial information to compute the average activation of a clone at each spatial location in the environment, thus obtaining its place field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 |</head><label>1</label><figDesc>Figure1| Clone-structured cognitive graph. A. Learning cognitive maps from sequential sensory observations is challenging because observations do not identify locations uniquely. B. The cognitive map learning problem can be understood as learning a latent graph from observations emitted at every node, where two different nodes can emit the same observation. The challenge is to learn context-specific representations that will disambiguate sensory observations in the latent space. The observation 𝐷 occurs in three different contexts in sequences 𝐴 → 𝐷 → 𝐸 (purple), 𝐵 → 𝐷 → 𝐸 (green), and 𝐶 → 𝐷 → 𝐹 (orange) from the environment, a distinction that is not represented in a first-order Markov model. Two of these contexts (purple, and green) correspond to the same latent state, and the third (orange) to a different latent state. Cloning 𝐷 into multiple latent states allows for flexible merging and splitting of contexts as appropriate. C. The cloning structure of dynamic Markov coding can be incorporated in an HMM with a structured emission matrix, the cloned HMM. D. Neural implementation of a cloned HMM. Neurons in each column are clones of each other that receive bottom-up input from the same observation. Arrow represent axons, and the lateral connections correspond to the cloned HMM transition matrix. Different sequences are in different colors. E. Probabilistic model for CSCG which extends cloned HMMs by including actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 |</head><label>2</label><figDesc>Figure 2 | CSCGs learn diverse latent topologies, transitively stitch them, and transfer structure to new environments. A. CSCGs learn allocentric maps from aliased egocentric local observations from a room with uniform interiors (i) even with long runs of the same observation. (ii) Each sensation, shown by the color, is attached to a set of latent states (clones) through the emission matrix. Through learning of the transition matrix, these clones learn to represent different temporal contexts of that sensation. (iii) Learned transition graph among clones. Each clone's color represents the observation it is attached to. (iv) Activations of the clones as the agent navigates the room can be used to compute their place fields, which reveal the spatial locations they represent. B. CSCGs are able to correctly learn the topology of 3D surfaces. Shown are the learned transition graphs with node colors indicating the observation that node is connected to. (continued on next page)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>CDFigure 3 |Figure 4 |</head><label>34</label><figDesc>Figure3| Shortcut finding in a novel room. The learned transition graph of a CSCG trained on one room (A) can be considered a reusable schema. Given partial observations in a second, novel room with an identical layout (B), the CSCG can re-use the previously learned latent structure to rapidly navigate around obstacles and find the shortest path to a target (C). (D) The graphs in the top row are a visualization of message propagation during planning and re-planning. Messages propagate outward from the starting clone. The first plan is unaware of the obstacles, and the agent discovers an obstacle only when the action sequence is executed (bottom row) and a planned action fails (at a node in black). This initiates re-planning from the new location, and the new plan routes around the obstacle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 (</head><label>4</label><figDesc>Figure4(continued) | CSCG can reproduce several place field remapping phenomena. B. Effects of changes in the environment on the spatial firing of hippocampal place cells,<ref type="bibr" target="#b28">Muller and Kubie (1987)</ref>. A CSCG was first trained using a random walk in a circular room with a cue at the 12 o' clock position. Place fields were then computed in the training room, the room with the cue rotated by 90°, and the room with a barrier introduced. For each clone, we observe that the place fields also rotate by 90°when the cue is rotated. For most clones, the place fields completely disappear when the barrier is introduced. An exception is clone 179, where the place field in the original layout is sufficiently far away from the barrier. C. Landmark vector cells,<ref type="bibr" target="#b12">Deshmukh and Knierim (2013)</ref>. A CSCG was trained in a rectangular room with a landmark (depicted by a hexagon) on one side of the room. Place fields were then computed in the training room as well as a modified room in which the landmark was moved to a different location. For the modified layout, we observe that the place fields now have two components -one at the same location as the place field in the original layout, and another at the same vector displacement from the new location of the landmark. D. A compressed representation of spatial distance in the rodent hippocampus,<ref type="bibr" target="#b45">Sheehan et al. (2021)</ref>. A CSCG was trained on a linear track of length 18 steps using outbound (left to right) and inbound (right to left) walks. Place fields were computed using trials with different starting positions for the outbound trajectories and different end positions for the inbound trajectories. The top and bottom rows correspond to place fields in the reference frame of the track and the start box, respectively. We observe that most place cells coded for distance from the starting box. A few clones, e.g., clones 8 and 20, are anchored to the end box. Further, clones have gradually widening fields with distance from the starting or ending locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure5| A. We reproduce the directional place fields reported by O'Keefe and Burgess (1996). We first trained a CSCG in a square room. In an elongated rectangular room, place fields of clones of the CSCG elongate with two strong peaks (second column from the left). The left peak is located such that its distance from the left wall is the same as in the square room. The same holds true for the right peak relative to the right wall. Further, when we use directional random walks to compute place fields, the two independent subcomponents corresponding to these peaks are revealed. A particular subcomponent is stronger when the walk starts from the wall to which the location of the subcomponent is tied: left and right components in the → and ← walks, respectively. (continued on next page)</figDesc><graphic coords="12,102.64,337.13,146.66,198.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 (</head><label>5</label><figDesc>Figure 5 (continued) | B.Lap-neurons and event-specific representations. A CSCG was trained on observations from laps in a rectangular maze similar to<ref type="bibr" target="#b48">Sun et al. (2020)</ref>. The training sequence consisted of three laps followed by a reward state (green * ) at the end. We also considered test trials with four laps where the reward state was at the end of the fourth lap. We show the place fields computed using the training and test trials, respectively. Rows correspond to clones. The place fields on the training trials show that there are different clones that are maximally active for different laps. But most clones are also partially active at their corresponding location in other laps, similar to the neurophysiological observations in<ref type="bibr" target="#b48">Sun et al. (2020)</ref>. For the test trials, we observe that the lap three clones are significantly active in both the third and the fourth laps. This additional activation shifted precisely by one lap reflects the fact that the third lap is no longer rewarded and that an extra lap is needed to receive a reward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 |</head><label>6</label><figDesc>Figure6| CSCG can reproduce various observations about place cells such as place field repetition and size, shape variations. A. Hippocampal place cells encode global location but not connectivity in a complex space,<ref type="bibr" target="#b14">Duvelle et al. (2021)</ref>. A CSCG was first trained in a layout comprising four square rooms that are connected by two-way doors. Place fields were computed in the training layout and two modified layouts -(i) one door locked both ways, and (ii) all doors locked one way. We observe that place fields remain the same across all three settings. B. Place field repetition in visually identical environments. A CSCG was first trained on a layout comprising two identical rooms in the same orientation connected by a corridor. The behavior of the CSCG was then studied in a layout where the two rooms were abutted by rotating them such that their orientations differed by 180°. We also considered a second modification, where we introduced an asymmetery in the connection between the abutting rooms. In all these three layouts, we observed place field repetition across the two rooms. Note that these results are incongruent with those in<ref type="bibr" target="#b19">Fuhs et al. (2005)</ref>, where they observed place field repetition only in the same orientation layout. C. However, when we retrain a CSCG in the opposite orientation layout with asymmetric connectivity, we observe that the place field repetition disappears. (continued on next page)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 |</head><label>7</label><figDesc>Figure 7 | CSCG based predictions about place fields. A. What controls place field change is not the rate of visual field change, but the uniqueness of the visual context. We observe that a trained on the checkerboard room has more expanded place fields compared one trained on a room with a random pattern. B. Influence of local landmarks during a room elongation experiment. We first trained a CSCG on a rectangular room with local landmarks on one side of the room. We computed place fields in the training room and an elongated room in which the landmarks are in the same position as the training room. Place fields anchored to the landmark (clone 15) stay the same in both layouts, but place fields sufficiently far from the landmarks expand in the elongated layout (clone 128).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>= 𝜋(𝑥 1 ) 𝛼(𝑛 + 1) = 𝛼(𝑛) T(𝑥 𝑛 , 𝑥 𝑛+1 ) 𝛽(𝑁) = 1(𝑥 𝑁 ) 𝛽(𝑛) = T(𝑥 𝑛 , 𝑥 𝑛+1 ) 𝛽(𝑛 + 1) 𝜉 𝑖 𝑗 (𝑛) = 𝛼(𝑛) • T(𝑖, 𝑗) • 𝛽(𝑛 + 1) 𝛼(𝑛) T(𝑖, 𝑗) 𝛽(𝑛 + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>= 𝜋(𝑥 1 ) 𝛼(𝑛 + 1) = 𝛼(𝑛) T(𝑥 𝑛 , 𝑎 𝑛 , 𝑥 𝑛+1 ) 𝛽(𝑁) = 1(𝑥 𝑁 ) 𝛽(𝑛) = T(𝑥 𝑛 , 𝑎 𝑛 , 𝑥 𝑛+1 ) 𝛽(𝑛 + 1) 𝜉 𝑖𝑘 𝑗 (𝑛) = 𝛼(𝑛) • T(𝑖, 𝑎 𝑛 , 𝑗) • 𝛽(𝑛 + 1) 𝛼(𝑛) T(𝑖, 𝑎 𝑛 , 𝑗) 𝛽(𝑛 + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 |</head><label>8</label><figDesc>Figure 8 | Egocentric observations for an example 7 × 7 square layout. A. The four heading dependent observations at the location denoted by the black dot, labeled by the corresponding observation indices. The observation window is of size 𝑓 𝑙 = 4, 𝑓 𝑤 = 3. B. The set of all unique observations for this example layout and their corresponding indices. The slashed areas correspond to regions outside the layout that are not visible to the agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 |</head><label>9</label><figDesc>Figure9| Allocentric vs. egocentric settings. A. Observation maps for an example 7 × 7 square layout in the allocentric (left) and egocentric (right) settings. In the egocentric case, the maps specify the observation index at any given position and heading, for a field of view of size 𝑓 𝑙 = 4, 𝑓 𝑤 = 3. B. The sequence of observations and actions for an example trajectory in the two cases. For the egocentric case, the observations include the actual 4 × 3 observation patches (top row) and their corresponding indices (bottom row). C. The learned transition graphs for CSCGs trained on the example layout using allocentric (left) and egocentric (right) settings. In both cases, the learned graphs correctly correspond to a 7 × 7 grid.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Matt Botvinick</rs>, <rs type="person">Kimberly Stachenfeld</rs>, <rs type="person">Dharshan Kumaran</rs>, <rs type="person">Charles Blundell</rs>, <rs type="person">Murray Shanahan</rs> and <rs type="person">Demis Hassabis</rs> for critically reading this manuscript and for insightful discussions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causal influence of visual cues on hippocampal directional selectivity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Aghajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hippocampal CA1 place cells encode intended destination on a maze with multiple choice points</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ainge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tamosiunaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Woergoetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dudchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="9769" to="9779" />
		</imprint>
	</monogr>
	<note>Sept. 2007a</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring the role of contextdependent hippocampal activity in spatial alternation behavior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ainge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A</forename><surname>Van Der Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hippocampus</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="988" to="1002" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Schema cells in the macaque hippocampus</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baraduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Duhamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="issue">6427</biblScope>
			<biblScope unit="page" from="635" to="639" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The boundary vector cell model of place cell firing and spatial memory</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>O'keefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jeffery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews in the Neurosciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neuronal vector coding in spatial cognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bicanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="453" to="470" />
			<date type="published" when="2020-09">Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New and distinct hippocampal place codes are generated in a new environment during septal inactivation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Brandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Leutgeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutgeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="789" to="796" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Space and time in the brain</title>
		<author>
			<persName><forename type="first">G</forename><surname>Buzsáki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Llinás</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">6362</biblScope>
			<biblScope unit="page" from="482" to="485" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data compression using dynamic Markov modelling</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N S</forename><surname>Horspool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="541" to="550" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Dabaghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
	<note>Reconceiving the hippocampal map as a topological template. Elife, 3:e03476</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning higher-order sequential structure with cloned hmms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dedieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gothoskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swingle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lehrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>George</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00507</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fragmentation of grid cell maps in a multicompartment environment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Derdikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Whitlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fyhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hafting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-B</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Moser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1325" to="1332" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Influence of local objects on hippocampal representations: Landmark vectors and memory</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Knierim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hippocampus</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="253" to="267" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Splitter cells: Hippocampal place cells whose firing is modulated by where the animal is going or where it has been</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dudchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Space,Time and Memory in the Hippocampal Formation</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Derdikman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Knierim</surname></persName>
		</editor>
		<meeting><address><addrLine>Vienna; Vienna</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="253" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hippocampal place cells encode global location but not connectivity in a complex space</title>
		<author>
			<persName><forename type="first">É</forename><surname>Duvelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Grieves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jedidi-Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holeniewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Donnarumma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lefort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Jeffery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1221" to="1233" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exact bayesian structure learning from uncertain interventions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exact bayesian structure learning from uncertain interventions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v2/eaton07a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</editor>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-03">Mar 2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hippocampus: cognitive processes and neural representations that underlie declarative memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Eichenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="120" />
			<date type="published" when="2004-09">Sept. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The hippocampus, memory, and place cells: is it spatial memory or a memory space?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Eichenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dudchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Influence of path integration versus environmental orientation on place cell remapping between visually identical environments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Fuhs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Vanrhoads</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Casale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2603" to="2616" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clonestructured graph representations enable flexible learning and vicarious evaluation of cognitive maps</title>
		<author>
			<persName><forename type="first">D</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Rikhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gothoskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Guntupalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dedieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2392</biblScope>
			<date type="published" when="2021-04">Apr. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A distributed representation of temporal context</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="299" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heading direction with respect to a reference point modulates place-cell activity</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Jercog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ahmadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deb-Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Kandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2333</biblScope>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Is hippocampal remapping the physiological basis for context?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Kubie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R J</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Fenton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hippocampus</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="851" to="864" />
			<date type="published" when="2020-08">Aug. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grid scale drives the scale and long-term stability of place maps</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Mallory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hardcastle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Giocomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="282" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linking hippocampal multiplexed tuning, hebbian plasticity and navigation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Popeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">599</biblScope>
			<biblScope unit="issue">7885</biblScope>
			<biblScope unit="page" from="442" to="448" />
			<date type="published" when="2021-11">Nov. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A developmental switch in place cell accuracy coincides with grid cell maturation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Muessig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Wills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cacucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1167" to="1173" />
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A structured scaffold underlies activity in the hippocampus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Taillefumier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Fiete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Kubie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1951" to="1968" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">U</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the directional firing properties of hippocampal place cells</title>
		<author>
			<persName><forename type="first">E</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Taube</surname></persName>
		</author>
		<author>
			<persName><surname>Kubie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7235" to="7251" />
			<date type="published" when="1994-12">Dec. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stdp enables spiking neurons to detect hidden causes of their inputs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning task-state representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1544" to="1553" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometric determinants of the place fields of hippocampal neurons</title>
		<author>
			<persName><forename type="first">J</forename><surname>O'keefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6581</biblScope>
			<biblScope unit="page" from="425" to="428" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The role of hippocampal replay in memory and planning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Ólafsdóttir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="R50" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Experience-dependent contextual codes in the hippocampus</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Plitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Giocomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="705" to="714" />
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayesian computation in recurrent neural circuits</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient inference in structured spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fiete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1148" />
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<title level="m">Hippocampal remapping as hidden state inference. Elife</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">51140</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vectorial representation of spatial goals in the hippocampus of bats</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ulanovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6321</biblScope>
			<biblScope unit="page" from="176" to="180" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical learning of temporal community structure in the hippocampus</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Turk-Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hippocampus</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="8" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning overcomplete hmms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="940" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A compressed representation of spatial distance in the rodent hippocampus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Charczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Fordyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hasselmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatial firing properties of hippocampal ca1 populations in an environment containing two visually identical regions</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Skaggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Mcnaughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="8455" to="8466" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The hippocampus as a predictive map</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Stachenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1643</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hippocampal neurons represent events as transferable units of experience</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tonegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="651" to="663" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The development of spatial and memory circuits in the rat</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Wills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cacucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip. Rev. Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">State transitions in the statistically stable place cell population correspond to rate of perceptual change</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">De</forename><surname>Cothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3505" to="3514" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A model of egocentric to allocentric understanding in mammalian brains</title>
		<author>
			<persName><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2031" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Non-Euclidean navigation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Biol</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
	<note>Pt Suppl 1</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generalisation of structural knowledge in the hippocampal-entorhinal system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Behrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8484" to="8495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the convergence properties of the em algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
