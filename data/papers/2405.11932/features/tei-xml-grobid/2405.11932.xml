<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonequilbrium physics of generative diffusion models</title>
				<funder ref="#_Bn7aaYm">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Xn89XdH">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_kRZqSz4">
					<orgName type="full">Guangdong Provincial Key Laboratory of Magnetoelectric Physics and Devices</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-03">December 3, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhendong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Physics</orgName>
								<orgName type="laboratory">PMI Lab</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Haiping</forename><surname>Huang</surname></persName>
							<email>huanghp7@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Physics</orgName>
								<orgName type="laboratory">PMI Lab</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Magnetoelectric Physics and Devices</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nonequilbrium physics of generative diffusion models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-03">December 3, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">92F34AE15E02EB2869CD63881B729632</idno>
					<idno type="arXiv">arXiv:2405.11932v3[cond-mat.stat-mech]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interests from engineering, statistics and physics, but a complete picture about inherent mechanisms is still lacking. In this paper, we provide a transparent physics analysis of diffusion models, formulating the fluctuation theorem, entropy production, equilibrium measure, and Franz-Parisi potential to understand the dynamic process and intrinsic phase transitions. Our analysis is rooted in a path integral representation of both forward and backward dynamics, and in treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder akin to that in spin glass theory. Our study thus links stochastic thermodynamics, statistical inference and geometry based analysis together to yield a coherent picture about how the generative diffusion models work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Neural network based machine learning has triggered a lot of research interests in a variety of fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. One of current active directions is the generative diffusion models (GDMs) <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, which are rooted in nonequilibrium physics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Forward and backward stochastic differential equations (SDEs, or Langevin equations in physics) are used; the forward part is to diffuse a data sample (e.g., a real image) into a Gaussian white noise distribution, after that, taking a sample from this Gaussian white noise distribution starts the backward process, driven by the gradient of log-state-likelihood, and finally this reverse Langevin dynamics collapses onto a real sample subject to the true data distribution, thereby completing the unsupervised data generation. This process is in essence a physical process, whose cornerstone is nonequilibrium dynamics, a central topic of statistical physics <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>.</p><p>Recent interests from physics community focused on symmetry breaking in the diffusion process <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, Bayesoptimal denoising interpretation of the GDMs <ref type="bibr" target="#b14">[15]</ref>, reformulation as equilibrium statistical mechanics <ref type="bibr" target="#b15">[16]</ref>, and path integral representation of the stochastic trajectories <ref type="bibr" target="#b16">[17]</ref>. We remark that the symmetry breaking concept in unsupervised learning (GDM is one type of unsupervised learning) has been introduced and analyzed in earlier works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. In this work, we define a high-dimensional Gaussian mixture data model that allows for analytic studies. Although recent works also studied this Gaussian mixture data <ref type="bibr" target="#b13">[14]</ref>, our main contributions in this work are distinct from recent works in the following three aspects. Our formulas apply to the high-dimensional dynamics, but we show one-dimensional examples for demonstration of the concept.</p><p>First, the GDM is analyzed through the concept of entropy production, especially for the reverse generative process. The derived formulas apply to the high-dimensional case. For a simple demonstration, the environmental and system entropy changes are explicitly computed, obeying the fluctuation theorem. The ensemble entropy production rate is also calculated for an example of one-dimensional diffusion, displaying distinct qualitative behaviors in both forward and backward processes. We also demonstrate the same dynamic state probability for both processes, and the probability currents have the same magnitude but opposite directions for the forward and reverse processes.</p><p>Second, we derive a generalized form of statistical inference for our current data model, where a temperature-like parameter is set by a variance matrix. Moreover, we prove that for this generalized form, the free energy is exactly the potential energy derived previously in Ref. <ref type="bibr" target="#b12">[13]</ref>. By an intuitive exploration of the qualitative shape of the potential (or equivalently the free energy of the statistical inference problem), we get the speciation transition point derived previously by an alternative method of Landau expansion or overlap dynamics <ref type="bibr" target="#b13">[14]</ref>. Moreover, we analyze the case of previously-unexplored one-dimensional diffusion example of arbitrary data mean and variance, and reveal a phase diagram of symmetry breaking, symmetry un-breaking and unstable symmetry breaking phases. Hence, we unify relevant results in our generalized form of statistical inference.</p><p>Third, we provide a geometry oriented way to look at a recently discovered collapse transition in the reverse diffusion process. In previous works, an empirical distribution for the reverse dynamic state is assumed, and the collapse transition is thus related to the number of data points used to represent the empirical distribution and the dimensionality of the dynamic state as well. In contrast, we start from the statistical inference standpoint without using the empirical distribution, and define a geometry measure, i.e., Franz-Parisi potential (see the first application in neural networks <ref type="bibr" target="#b19">[20]</ref>) to capture the potential emergence of hidden structures in the conditional probability of the statistical inference. We provide an effective description in a one-dimensional example. The complicated yet tractable computation of the high dimensional case is left for future works. Our work focuses on a pure understanding of an analytically tractable GDM, without taking into account the algorithmic design, which typically requires training a complex neural network for non-Gaussian real dataset. The paper is organized as follows. We first introduce the forward diffusion process, together with the fluctuation theorem derived from the forward process, and concepts of stochastic entropy and ensemble average. Then we derive the reverse generative dynamics applied to generate the real data samples in machine learning, and introduce in details the concept of potential energy and free energy to analyze the denoising process formulated as a statistical inference. We also derive the fluctuation theorem and entropy production rate for the reverse process, together with Franz-Parisi potential applied to analyze how fragmented the configuration space for the inference is as the time approaches the starting point of the forward dynamics. We finally summarize our studies and make future perspectives in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. NONEQUILIBRIUM PHYSICS OF FORWARD DIFFUSION PROCESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Forward diffusion dynamics</head><p>A classic example of stochastic dynamics is the well-known Brownian motion, whose dynamics is called the Langevin dynamics. Consistence between Brownian motion and thermodynamics has been established in 1905 <ref type="bibr" target="#b20">[21]</ref>. Current AI studies make nonequilibrium physics of Langevin dynamics regain intense research interests <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. In the forward process, we use Ornstein-Ulhenbeck (OU) process <ref type="bibr" target="#b6">[7]</ref> to turn a real data point into a white noise, which is detailed as a high dimensional SDE:</p><formula xml:id="formula_0">Ẋ = -X + √ 2ξ,<label>(1)</label></formula><p>where X ∈ R d and ξ ∈ R d are time-dependent high dimensional state and noise quantities, respectively, and ξ is a Gaussian white noise with correlation ⟨ξ i (t)ξ j (t ′ )⟩ = δ ij δ(t -t ′ ). Given the initial condition X(0) ≡ X 0 , the above SDE has a solution:</p><formula xml:id="formula_1">X t = e -t X 0 + √ 2e -t t 0 dse s ξ(s),<label>(2)</label></formula><p>from which, X(t) ≡ X t is clearly a Gaussian random variable, which can be reformulated as the following form using independent standard Gaussian random variable:</p><formula xml:id="formula_2">X t = e -t X 0 + 1 -e -2t Z t ,<label>(3)</label></formula><p>where Z t is the standard Gaussian random variable, and the variance of X t is given by 1 -e -2t . In the following, we make no difference between X(t) and X t .</p><p>For simplicity, we choose the distribution of the data as a Gaussian mixture of two classes (e.g., two kinds of images):</p><formula xml:id="formula_3">p(X 0 ) = 1 2 N (µ, I d ) + 1 2 N (-µ, I d ) , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where µ is the d-dimensional constant vector, and I d is the d-dimensional identity covariance matrix. In most parts of this paper, we consider this simple Gaussian mixture with unit variance. The more general case of non-unit variance is also discussed when necessary. It is also straightforward to generalize our analysis to multiple classes. Then at time t, the probability distribution p(X t , t) can be calculated by a convolution <ref type="bibr" target="#b21">[22]</ref> p(X t , t</p><formula xml:id="formula_5">) = dX 0 p(X 0 )p(X t |X 0 ) = dX 0 p(X 0 )N (X t ; X 0 e -t , Σ t I d ) = 1 2 N (X t ; µ t , I d ) + 1 2 N (X t ; -µ t , I d ),<label>(5) 0 3 3 0</label></formula><p>FIG. <ref type="figure">1:</ref> A schematic illustration of the generative diffusion process of two-dimensional Gaussian mixture data. The forward process from time t = 0 to t = 3 is shown together with the reverse process from t = 3 back to t = 0. The gradient of log-statelikelihood can be analytically estimated as the state probability is given by p(Xt, t)</p><formula xml:id="formula_6">= 1 2 N µe -t , I d + 1 2 N -µe -t , I d .</formula><p>where µ t ≡ µe -t , and Σ t ≡ 1 -e -2t . Representative trajectories of the forward process are shown in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fluctuation theorem for the forward diffusion</head><p>Because of stochasticity, the trajectories are not differentiable any more in general. A specific time-discretization scheme for the stochastic differential equation must be carefully chosen, for which the stochastic calculus is established (see details below). Then we would derive the path probability of {X t } given an initial point X 0 . To do this, we have to specify the discretization scheme for the above SDEs. We first define random variable W(t) for the Wiener process as follows:</p><formula xml:id="formula_7">W(t) = t t0 dt ′ ξ(t ′ ) = t t0 dW.<label>(6)</label></formula><p>The stochastic integral can be expressed as</p><formula xml:id="formula_8">t t0 dt ′ ξ(t ′ )f (X t , t) = t t0 dWf (X t , t</formula><p>), called the Riemann-Stieltjes integral as well. In studies of SDEs, we have two commonly used conventions to represent this stochastic integral. The first one is the Ito convention, or the initial point scheme. More precisely, the Riemann-Stieltjes integral is calculated as</p><formula xml:id="formula_9">t t0 dWf (X t , t) = lim dt→0 N k=0 [W(t k + dt) -W(t k )] f (X(t k ), t k ),<label>(7)</label></formula><p>where t k = t 0 + kdt, and N = t-t0 dt . In general, we interpolate the time between t k and t k+1 as τ = (1 -λ)t k + λt k+1 . Therefore, the Ito convention corresponds to λ = 0. This stochastic integral is clearly dependent on the discretization scheme <ref type="bibr" target="#b9">[10]</ref>.</p><p>The second one is the Stratonovich convention, i.e., mid-point scheme with λ = 1/2. Then, we have the following expression:</p><formula xml:id="formula_10">t t0 dWf (X t , t) = lim dt→0 N k=0 [W(t k + dt) -W(t k )] f ([X(t k + dt) + X(t k )]/2, t k + dt/2) .<label>(8)</label></formula><p>Next we rewrite Eq. (1) as follows,</p><formula xml:id="formula_11">dX t = f (X t , t)dt + √ 2dW, (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where dt is a small step size as used before. According to the general discretization, we have</p><formula xml:id="formula_13">X t+dt -X t dt = f ((1 -λ)X t + λX t+dt , t + λdt) + η t ,<label>(10)</label></formula><p>where η i,t ∼ N (0, 2/dt). In the following we can take t + dt as t ′ , or t ′ -t = dt in the Markovian process defined by Eq. <ref type="bibr" target="#b9">(10)</ref>; dt is the unit of the above discretization. We next use the following probability transformation identity:</p><formula xml:id="formula_14">P (X t ′ , t ′ |X t , t) = P (η t ) ∂η t ∂X t ′ ,<label>(11)</label></formula><p>where the determinant ∂ηt ∂X t ′ is a Jacobian measuring the change of volume for the transformed probability density. This Jacobian can be easily computed using Eq. <ref type="bibr" target="#b9">(10)</ref>.</p><formula xml:id="formula_15">∂η t ∂X t ′ = ∂ X t+dt -Xt dt -f ((1 -λ)X t + λX t+dt , t + λdt) ∂X t+dt ∝ e -λ∇•f dt ,<label>(12)</label></formula><p>where I d is an d-dimensional identity matrix, we have used the Taylor expansion (dt → 0) and the matrix identity det e K = e Tr K . Finally, based on the known form of the white noise distribution, we have the following infinitesimal propagator:</p><formula xml:id="formula_16">P (X t+dt , t + dt|X t , t) ∝ e -λ∇•f dt e -| Ẋt -f (X t ,t)| 2 4 dt . (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>Using the Markovian chain property, we get the conditional trajectory probability:</p><formula xml:id="formula_18">P (X([T ])|X 0 ) = t ′ P (X t ′ , t ′ |X t , t) = t ′ e -λ∇•f dt e -| Ẋt -f (X t ,t)| 2 4 dt ∝ e T 0 [-λ∇•f - | Ẋt -f (X t ,t)| 2 4 ] λ ⊙ dt ,<label>(14)</label></formula><p>where X([T ]) ≡ {X T , . . . , X 1 } specifying a trajectory starting from X 0 , T denotes the length of the individual trajectory, and λ ⊙ denotes the corresponding λ-convention for the stochastic integral. An alternative way to get the same result is to use the following property of Dirac delta function:</p><formula xml:id="formula_19">δ (X(t) -X ξ (t)) = δ (O[X(t)]) det δO δX ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_20">X ξ (t) is a solution of O[X] = Ẋ -f -ξ = 0. Therefore, P (X[T ]|X 0 ) = ⟨ t δ(X(t) -X ξ (t))⟩ {ξt} . Now, taking f = -X [see Eq. (<label>1</label></formula><p>)], we can derive the trajectory probability for the forward OU process</p><formula xml:id="formula_21">P (X([T ])|X 0 ) = t ′ P (X t ′ , t ′ |X t , t) ∝ exp   - T 0 1 4 Ẋ + X 2 -λd λ ⊙ dt   , (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>where d is the dimensionality of the dynamics, and the term in the exponent is called the action in physics for the path probability, and the integrand inside the time integral of the action is called the Lagrangian L <ref type="bibr" target="#b22">[23]</ref>, and the optimal path of maximal trajectory probability is determined by the Euler-Lagrange equation d dt ∂L ∂ Ẋ -∂L ∂X = 0. Next, we consider a reverse dynamics, i.e., X(s) = X(t), where s = T -t, and T is the time length of the trajectory.</p><p>It is clear that X(0) = X(T ), and X(T ) = X(0). We have then Ẋ(s) = -Ẋ(t). In analogy to the forward trajectory, the path probability of the backward trajectory given the initial point is given by</p><formula xml:id="formula_23">P X([T ]) | X0 = N exp -A( X([T ])) , (<label>17</label></formula><formula xml:id="formula_24">)</formula><p>where N is a normalization factor, Xt ≡ X(t) (similar for X t ), and the action reads,</p><formula xml:id="formula_25">A( X([T ])) = T 0 ds λ ⊙ ( Ẋ + X) 2 /4 -λd = T 0 dt 1-λ ⊙ (-Ẋ + X) 2 /4 -λd .<label>(18)</label></formula><p>Note that the time reversal changes the λ-convention to (1 -λ)-convention <ref type="bibr" target="#b22">[23]</ref>. The ratio between the conditional path probabilities is thus given by ln</p><formula xml:id="formula_26">  P [X([T ]) | X 0 ] P X([T ]) | X0   = A( X([T ])) -A(X([T ])) = - T 0 dt 1-λ ⊙ Ẋ • X 2 - T 0 dt λ ⊙ Ẋ • X 2 = T 0 dt 1 2 ⊙[-Ẋ • X],<label>(19)</label></formula><p>where the action for the direct dynamics can be read off from Eq. ( <ref type="formula" target="#formula_21">16</ref>), the calculation in the last step leads to the result independent of λ (or equivalently λ = 1 2 ). This result can be interpreted as heat dissipated into the environment, since in an overdamped system, the product of total mechanical force and displacement equals to dissipation <ref type="bibr" target="#b8">[9]</ref>. Then we identify the following entropy change of environment:</p><formula xml:id="formula_27">∆S E = T 0 dt 1 2 ⊙[-Ẋ • X],<label>(20)</label></formula><p>where we have assumed the temperature for the forward OU process equals to one. In addition, the starting and final states in the forward diffusion process can be treated as equilibrium states, subject to an analytic form of distribution (in fact they are Gaussian mixture). Then if we define a stochastic or trajectory-dependent entropy of the system as S(t) = -ln p(X t , t), we can derive the entropy change of the system as follows,</p><formula xml:id="formula_28">∆S = ln p (X(0), 0) p(X(T ), T ) = ln   exp -(X(0)-µ) 2 2 + exp -(X(0)+µ) 2 2 exp -(X(T )-µe -T ) 2 2 + exp -(X(T )+µe -T ) 2 2   . (<label>21</label></formula><formula xml:id="formula_29">)</formula><p>We obtain then the total entropy change: which implies that the ratio of the path probabilities is exactly e ∆Stot . Note that in Eq. ( <ref type="formula" target="#formula_30">22</ref>) X([T ]) includes the initial state X(0). Equation ( <ref type="formula" target="#formula_30">22</ref>) is the well-known detailed fluctuation theorem, while the integral fluctuation theorem can be readily derived as</p><formula xml:id="formula_30">∆S tot = ∆S E + ∆S = ln   P [X([T ])] P X([T ])   ,<label>(22</label></formula><formula xml:id="formula_31">e -∆Stot = P [X([T ])] e -∆Stot dX([T ]) = P X([T ]) d X([T ]) = 1. (<label>23</label></formula><formula xml:id="formula_32">)</formula><p>From the integral fluctuation theorem, one can derive the stochastic second thermodynamics law ⟨∆S tot ⟩ ≥ 0 based on the convexity of the exponential function.</p><p>Finally we verify the integral fluctuation theorem in the forward diffusion models (see Fig. <ref type="figure" target="#fig_0">2</ref>) in an example of one-dimensional OU process. As the number of trajectories gets large, the trajectory average ⟨∆S tot ⟩ converges to one, as predicted by the theory. Each contribution of the entropy change is shown in Fig. <ref type="figure" target="#fig_1">3</ref>. Some trajectories bear a negative entropy change, i.e., entropy decreases during the evolution, but on average, the stochastic second law of thermodynamics is still valid. Experimental details to get the entropy contribution are given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Rate of stochastic entropy production</head><p>We first recall the Fokker-Planck equation (FPE) corresponding to the OU forward process under the Ito convention as follows,</p><formula xml:id="formula_33">∂p(X t , t) ∂t = -∇ • [f (X t , t)p(X t , t)] + d i=1 ∂ 2 p(X t , t) ∂X i ∂X i = -∇ • J,<label>(24)</label></formula><p>where the probability current reads J = f (X t , t)p(X t , t) -∇p(X t , t), and we have written the force term in Eq. ( <ref type="formula" target="#formula_0">1</ref>) as a general function f (X t , t) which we will specify in the reverse generative dynamics as well. The FPE is an equation of probability conservation <ref type="bibr" target="#b6">[7]</ref>. We then define the stochastic entropy as before <ref type="bibr" target="#b23">[24]</ref> </p><formula xml:id="formula_35">S(t) = -ln p(X t , t),<label>(25)</label></formula><p>where we set k B = 1 in our paper. The rate of the stochastic entropy can be derived directly:</p><formula xml:id="formula_36">Ṡ(t) = - ∂ t p(X t , t) p(X t , t) - ∇p(X t , t) p(X t , t) Ẋ = - ∂ t p(X t , t) p(X t , t) - f (X t , t)p(X t , t) -J p(X t , t) Ẋ = - ∂ t p(X t , t) p(X t , t) -f (X t , t) • Ẋ + J • Ẋ p(X t , t) . (<label>26</label></formula><formula xml:id="formula_37">)</formula><p>To derive the above equation, we have used the expression of the probability current. The second term in the last equality of Eq. ( <ref type="formula" target="#formula_36">26</ref>) is actually the rate of heat dissipation to the environment, i.e., q(t) = f (X t , t) Ẋ = ṠE (t) (notice that we have set the unit temperature). Then, we can define the total entropy production rate Ṡtot (t) = ṠE (t) + Ṡ(t), and as a result, Ṡtot (t) reads,</p><formula xml:id="formula_38">Ṡtot (t) = - ∂ t p(X t , t) p(X t , t) + J • Ẋ pX t , t) .<label>(27)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ensemble entropy production rate</head><p>We first define the ensemble average of the stochastic entropy as follows <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_39">S(t) ≡ ⟨S(t)⟩ = -p(X t , t) ln p(X t , t)dX t .<label>(28)</label></formula><p>The rate of entropy change of the system can be readily expanded by inserting the FPE as follows,</p><formula xml:id="formula_40">dS(t) dt = d dt -p (X t , t) ln p (X t , t) dX t = - d i=1 ∂ ln p(X t , t) ∂X i J i dX t ,<label>(29)</label></formula><p>where J i is the i-th component of J, and we have used the fact that dX t ∂p(Xt,t) ∂t = 0, or at the boundary, the current vanishes. According to the definition of the probability current J [see its expression below Eq. ( <ref type="formula" target="#formula_33">24</ref>)], we get its component</p><formula xml:id="formula_41">J i = f i p -p ∂ ln p</formula><p>∂Xi , and replace ∂ ln p ∂Xi by f i -J i /p, where f i is the i-th component of the high dimensional force f (X t , t). Therefore,</p><formula xml:id="formula_42">dS(t) dt = d i=1 -f i J i + J 2 i p(X t , t) dX t , = π -ϕ,<label>(30)</label></formula><p>where π ≡ i J 2 i p(Xt,t) dX t is non-negative, and is actually the entropy production rate, the rate at which the total entropy of the system and environment changes; and ϕ ≡ i f i (X t , t)J i dX t denotes the entropy flux into or out of the system (from or to) the environment. The entropy flux can be positive, suggesting reduction of the system entropy (a characteristic of emergence of order). Provided that the dynamics reaches equilibrium, both π and ϕ will vanish, but even if S(t) is stationary, π = ϕ ̸ = 0, which is a key characteristic of nonequilibrium steady states. The above derivations are consistent with those derived in Refs. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, and are applied to GDMs we study in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NONEQUILIBRIUM PHYSICS OF REVERSE GENERATIVE DYNAMICS A. Backward generative SDE</head><p>In this section, we first derive the reverse generative dynamics equation based on the forward diffusion process [Eq. ( <ref type="formula" target="#formula_0">1</ref>)]. Then we would give a thorough physics analysis of this backward generative SDE.</p><p>We first define the following backward conditional distribution:</p><formula xml:id="formula_43">P (X t , t|X t+dt , t + dt) = P (X t+dt , t + dt|X t , t) p(X t+dt , t + dt) p(X t , t),<label>(31)</label></formula><p>where the Bayes' rule is used. According to Eq. ( <ref type="formula" target="#formula_0">1</ref>), we have</p><formula xml:id="formula_44">X t+dt -X t = f (X t , t)dt + √ 2dtη t</formula><p>, where η t is an i.i.d. Gaussian random variable of zero mean and unit variance. It is easy to derive that</p><formula xml:id="formula_45">P (X t+dt , t + dt|X t , t) ∝ exp - (X t+dt -X t -f (X t , t)dt) 2 4dt . (<label>32</label></formula><formula xml:id="formula_46">)</formula><p>In addition, the probability ratio</p><formula xml:id="formula_47">p(X t , t) p(X t+dt , t + dt) = exp [-(ln p(X t+dt , t + dt) -ln p(X t , t))] = exp -dX • ∇ ln p(X t , t) -dt ∂ ln p(X t , t) ∂t ,<label>(33)</label></formula><p>where we have done the Taylor expansion of ln p(X + dX, t + dt) in both spatial and temporal dimensions.</p><p>Finally, we collect all intermediate results into the Bayes' formula and get</p><formula xml:id="formula_48">P (X t , t|X t+dt , t + dt) ∝ exp - (X t+dt -X t -f (X t , t)dt) 2 4dt -dX • ∇ ln p(X t , t) -dt ∂ ln p(X t , t) ∂t , ∝ exp - ∥X t -X t+dt + (f (X t , t) -2∇ ln p(X t , t)) dt∥ 2 4dt ,<label>(34)</label></formula><p>where the order of O(dt) is neglected. Equation (34) suggests that the following SDE for the reverse dynamics reads</p><formula xml:id="formula_49">Ẋ = f (X t , t) -2∇ ln p(X t , t) + √ 2ξ. (<label>35</label></formula><formula xml:id="formula_50">)</formula><p>This reverse diffusion equation was first proposed in Ref. <ref type="bibr" target="#b25">[26]</ref>, and can be thought as a non-linear Langevin dynamics, a central subject we shall study in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning the score function</head><p>The gradient of log-likelihood is called the score function in machine learning. It is usually hard to estimate in real data learning, but can be approximated by a neural network whose parameters are trained to minimize the following mean-squared cost function:</p><formula xml:id="formula_51">L(θ) = E Xt∼p(Xt,t) ∥s θ (X t , t) -∇ ln p(X t , t)∥ 2 , (<label>36</label></formula><formula xml:id="formula_52">)</formula><p>where s θ represents a function implemented by a neural network parameterized by θ. It is proved in a previous work <ref type="bibr" target="#b26">[27]</ref> that the score function can be replaced by ∇ ln P (X t |X 0 ) (in the sense that the expectation over p(X t , t) is considered), which is more convenient to compute as</p><formula xml:id="formula_53">∇ ln P (X t |X 0 ) = -Σ -1 (X t -X 0 e -t ) = - 1 1 -e -2t I d (X t -X 0 e -t ) = - 1 √ 1 -e -2t Z t ,<label>(37)</label></formula><p>where Σ = Σ t I d , and Z t is a d-dimensional standard Gaussian random variable. Equation (37) can be used to train a neural network in practice. After the score function is learned, the reverse SDE can be used to generate data samples starting from a Gaussian white noise, in a very similar spirit to variational auto-encoder and generative adversarial networks <ref type="bibr" target="#b0">[1]</ref>.</p><p>In our current model, the score function can be computed in an analytic form, which proceeds as follows. We already know that</p><formula xml:id="formula_54">p(X t , t) = 1 2 N (µ t , I d ) + 1 2 N (-µ t , I d ) ,<label>(38)</label></formula><p>where µ t = µe -t . We get then the score function <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_55">∇ ln p(X t , t) = w +,t (X t )µ +,t + w -,t (X t )µ -,t -X t = w +,t (X t )µ t -(1 -w +,t (X t ))µ t -X t = (2w +,t (X t ) -1)µ t -X t = tanh µ ⊤ t X t µ t -X t ,<label>(39)</label></formula><p>where the weight for the positive mean µ +,t is given by w +,t (X t ) =</p><formula xml:id="formula_56">1 1+exp ∥X t -µ t ∥ 2 2 - ∥X t +µ t ∥ 2 2 = 1 1+exp(-2µ ⊤ t Xt)</formula><p>. Note that the score function becomes more complicated but still analytic in the case of non-unit variance of the ground truth Gaussian mixture distribution. We show this complicated expression in Appendix E. Driven by the score function, typical trajectories starting from a standard normal random vectors evolve to target samples of Gaussian mixture data distribution, which is shown as an example in Fig. <ref type="figure">1</ref>. What remains mysterious is the nature of the reverse generative process. In the following, we will address this question in our current Gaussian mixture model by using physics concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Potential and free energy</head><p>Next, we follow recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref> to introduce the potential and free energy of the reverse SDEs. Note that we formulate these concepts in our current model in the general form and prove their equivalence through a statistical inference mapping of the denoising process, and finally demonstrate how to identify phase transitions during the reverse Langevin dynamics based on our derivation.</p><p>We first derive a potential function for the reverse dynamics [Eq. ( <ref type="formula" target="#formula_49">35</ref>)]. Let t = T -s, then decreasing t is equivalent to increasing s. The reverse SDE can thus be written as</p><formula xml:id="formula_57">d Xs = 2∇ ln p( Xs , T -s) -f ( Xs , T -s) ds + √ 2dW = -∇U ( Xs , T -s) + √ 2dW,<label>(40)</label></formula><p>where U ( Xs , T -s) is defined as the potential of the reverse dynamics. We have thus the following equality:</p><formula xml:id="formula_58">-∇U ( Xs , T -s) = 2∇ ln p( Xs , T -s) -f ( Xs , T -s).<label>(41)</label></formula><p>Carrying out an integral of both sides in Eq. ( <ref type="formula" target="#formula_87">64</ref>), we get the expression of the potential as</p><formula xml:id="formula_59">U ( Xs , T -s) = -2 ln p( Xs , T -s) + Xs 0 f (Z, T -s)dZ,<label>(42)</label></formula><p>where we have neglected all irrelevant constants contributed by an arbitrary lower limit (but not Xs ) of the integral.</p><p>In our Gaussian mixture data case, the score function can be exactly computed. Therefore, the potential can be estimated with the following analytic form:</p><formula xml:id="formula_60">U = 1 2 X2 s -2 ln cosh Xs • µe -(T -s) .<label>(43)</label></formula><p>We reshape the time as T -s = t, and then Xs = X t , leading to the following form in decreasing t:</p><formula xml:id="formula_61">U = 1 2 X 2 t -2 ln cosh X t • µe -t .<label>(44)</label></formula><p>This analytic expression of the potential can be plotted as a function of time and X t . For an example of onedimensional dynamics, figure <ref type="figure">4</ref> shows that at some moment t = t S , the symmetry of the potential is broken, producing two local minima, which bears similarity with the spontaneous symmetry breaking in ferromagnetic Ising model when the temperature is lowered down. This spontaneous symmetry breaking is common in other types of unsupervised learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The critical time t S is thus determined by the condition ∂ 2 U ∂X 2 | X=0 (t S ) = 0, a qualitative change of convexity at X = 0. This condition leads to the transition condition µ 2 e -2t S = 1 2 for our current model setting. The specific moment t = t S in the reverse dynamics trajectory corresponds to the speciation transition recently identified in Ref. <ref type="bibr" target="#b13">[14]</ref> where an overlap dynamics was analyzed.</p><p>Next, we turn to the free energy concept. We assume p(X 0 ) as the data distribution, and then according to the forward process, the distribution of X t is expressed as a probability convolution:</p><formula xml:id="formula_62">p(X t , t) = N (X t ; X 0 e -t , Σ t I d )p(X 0 )dX 0 . (<label>45</label></formula><formula xml:id="formula_63">)</formula><p>The score function can then be expressed as</p><formula xml:id="formula_64">∇ ln p(X t , t) = Σ -1 t dX 0 p(X 0 )X 0 e -t N (X t ; X 0 e -t , Σ t I d ) dX 0 p(X 0 )N (X t ; X 0 e -t , Σ t I d ) -X t = Σ -1 t X 0 e -t Xt -X t .<label>(46)</label></formula><p>This form of score function suggests that a statistical inference can be defined below <ref type="bibr" target="#b14">[15]</ref>: where Σ = Σ t I d . We can thus write down an equivalent Hamiltonian H(X 0 |X t , t) = 1 2 (X t -X 0 e -t ) ⊤ Σ -1 (X t -X 0 e -t ) -ln p(X 0 ), and the partition function reads Z(X t , t) = e -H(X0|Xt,t) dX 0 .</p><formula xml:id="formula_65">P (X 0 |X t , t) = P (X t , t|X 0 )p(X 0 ) p(X t , t) = N (X t ; X 0 e -t , Σ t I d )p(X 0 ) p(X t , t) ∝ exp - 1 2 (X t -X 0 e -t ) ⊤ Σ -1 (X t -X 0 e -t ) + ln p(X 0 ) ,<label>(47)</label></formula><p>(48)</p><p>Note that this is a generalized form of that introduced in Ref. <ref type="bibr" target="#b15">[16]</ref>. The dynamic state X t during the reverse dynamics plays the role of quenched disorder in traditional spin glass theory <ref type="bibr" target="#b1">[2]</ref>. The free energy can then be written as F (X t , t) = -1 β ln Z(X t , t), where an equivalent inverse temperature β = Σ -1 (different from that introduced in Ref. <ref type="bibr" target="#b15">[16]</ref>). It is straightforward to compute the following free-energy gradient:</p><formula xml:id="formula_66">∇F (X t , t) = ∇ - 1 β ln Z(X t , t) = - 1 β ∇Z(X t , t) Z(X t , t) = -X 0 e -t Xt + X t ,<label>(49)</label></formula><p>which is exactly the gradient of log-state-likelihood (up to a pre-factor), i.e., ∇F = -1 β ∇ ln p, from which we can finally derive the explicit form of the free energy:</p><formula xml:id="formula_67">F = - 1 β ln cosh(X t • µ t ) + 1 2β X 2 t .<label>(50)</label></formula><p>It is then convenient to write the driving force in the reverse dynamics [see Eq. ( <ref type="formula" target="#formula_57">40</ref>)], i.e., 2∇ ln p -f = -∇ F , and then it is straightforward to derive the following expression for the generalized free energy F :</p><formula xml:id="formula_68">F = 2βF + f d Xs ,<label>(51)</label></formula><p>which is exactly the potential energy derived before [Eq. ( <ref type="formula" target="#formula_61">44</ref>)]. This suggests that, the reverse generative dynamics can be considered as minimizing the generalized free energy in the dynamic-state space, and the generated samples follows the minimum free energy principle. We thus conclude that the speciation transition corresponds to a change of curvature of the free energy landscape. Although our results are derived in the high-dimensional Gaussian mixture context and formulated as a statistical inference, the conclusion is consistent with that derived in a simpler setting through a regularized free energy <ref type="bibr" target="#b15">[16]</ref>.</p><p>We finally discuss the symmetry breaking transition in arbitrary values of data mean µ and variance σ 2 . For simplicity, we focus on one-dimensional example. According to the theoretical analysis in this section, the speciation time t S is determined by t S = 1 2 ln µ 2 + σ 4 -2σ 2 + µ 4 + 1 where the potential starts the spontaneous symmetry breaking, which is obtained by searching for the time when the second derivative of the potential or free energy vanishes (see detailed calculations in Appendix B). The theoretical formula based on a perturbative analysis in Ref. <ref type="bibr" target="#b13">[14]</ref> does not apply to our current case where d = 1 and speciation may take place at a small time. The following finding is also not revealed in a recent work <ref type="bibr" target="#b12">[13]</ref> where the arbitrary variance was not considered. Hence, in our current setting, we reveal a new type of phase diagram for this simple setting (Fig. <ref type="figure">5</ref>). More precisely, µ 2 + σ 4 -2σ 2 + µ 4 + 1 = 1 determining a phase boundary between symmetry breaking and symmetry un-breaking. In the symmetry un-breaking phase, we do not have a positive value of t S . Moreover, it is surprising that when the variance is higher than a critical value, we have another phase transition to an unstable symmetry breaking, whose threshold is determined by t U SB = 1 2 ln σ 2 -1 (see details in Appendix B). If the unstable symmetry breaking is triggered, i.e., t U SB ≥ 0, the symmetry breaking profile of double-minima potential is destabilized to an inverted U shape (Fig. <ref type="figure" target="#fig_3">6</ref>). The slope of the potential at X t = ∞ changes discontinuously to a positive value, since at the threshold, the slope is still negative. An immediate result is that the generative sampling fails for the reverse diffusion process. Although we clarify this phase diagram in the simple one-dimensional diffusion process, it is still very interesting to see whether the picture qualitatively holds in high dimensional generative process, which is more relevant in practical applications. We leave this idea in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fluctuation theorem in the reverse dynamics</head><p>We have derived the following reverse SDE [see Eq. ( <ref type="formula" target="#formula_49">35</ref>)]:</p><formula xml:id="formula_69">dX t = -2 tanh µ ⊤ t X t µ t dt + X t dt + √ 2dW. (<label>52</label></formula><formula xml:id="formula_70">)</formula><p>With increasing time s, we make a definition X(s) = X(t) as before, where s = T -t, and we have the following equivalent SDE:</p><formula xml:id="formula_71">d Xs = 2 tanh µ ⊤ T -s Xs µ T -s -Xs ds + √ 2dW. (<label>53</label></formula><formula xml:id="formula_72">)</formula><p>We then use the λ-convention for the following discretization:</p><formula xml:id="formula_73">Xs+ds -Xs ds = 2 tanh µ ⊤ T -s-λds {λ Xs+ds + (1 -λ) Xs } µ T -s-λds -(λ Xs+ds + (1 -λ) Xs ) + √ 2η s ,<label>(54)</label></formula><p>where η s ∼ N (0, 1 ds I d ). The conditional probability of Xs+ds can be expressed using the noise distribution via the following probability density transform:</p><formula xml:id="formula_74">P Xs+ds | Xs = P √ 2η ∂ √ 2η ∂ Xs+ds ∝ exp   - Ẋs -2 tanh µ ⊤ T -s Xs µ T -s + Xs 2 ds 4 -λ∇ • 2 tanh µ ⊤ T -s Xs µ T -s -Xs ,<label>(55)</label></formula><p>where</p><formula xml:id="formula_75">∂ √ 2η ∂ Xs+ds</formula><p>represents the Jacobian. Finally, we use the Markovian chain property and get</p><formula xml:id="formula_76">P X([T ])| X0 = s P Xs+ds | Xs ∝ e -T 0 ds λ ⊙ ( Ẋ-2 tanh ( µ ⊤ T -s X) µ T -s + X) 2 4 +λ∇•[2 tanh(µ ⊤ T -s X)µT -s -X] .<label>(56)</label></formula><p>Given an initial condition, the path probability of a trajectory X([T ]) with time from 0 to T can be represented as</p><formula xml:id="formula_77">P ( X([T ])| X0 ) = N exp -A( X([T ])) ,<label>(57)</label></formula><p>where N denotes a normalization, and</p><formula xml:id="formula_78">A( X([T ])) = T 0 ds λ ⊙    Ẋ -2 tanh µ ⊤ T -s X µ T -s + X 2 4 + λ∇ • [2 tanh µ ⊤ T -s X µ T -s -X]    . (<label>58</label></formula><formula xml:id="formula_79">)</formula><p>Taking a backward process on the same forward trajectory, we have X(t) = X(T -s) = X(s), and thus we have</p><formula xml:id="formula_80">Ẋ(t) = Ẋ(T -s) = -Ẋ(s).</formula><p>The path probability for this reverse version of the forward trajectory can be derived similarly based on the new definition of the state variable. The resulting formula is given below.</p><formula xml:id="formula_81">P (X([T ])|X 0 ) = N exp[-A(X([T ]))],<label>(59)</label></formula><p>where</p><formula xml:id="formula_82">A(X([T ])) = T 0 ds 1-λ ⊙    -Ẋ -2 tanh µ ⊤ T -s X µ T -s + X 2 4 + λ∇ • [2 tanh µ ⊤ T -s X µ T -s -X]    .<label>(60)</label></formula><p>The entropy change in the environment is thus calculated as follows,</p><formula xml:id="formula_83">∆S E = ln P ( X([T ])| X0 ) P (X([T ])|X 0 ) = T 0 ds 1 2 ⊙ Ẋ • (2 tanh µ ⊤ T -s X µ T -s -X) . (<label>61</label></formula><formula xml:id="formula_84">)</formula><p>The change of the system entropy is calculated as the ratio between the probabilities of the initial and final states. More precisely,</p><formula xml:id="formula_85">∆S = ln p( X0 ) p( XT ) = ln    exp -( X0-µe -T ) 2 2 + exp -( X0+µe -T ) 2 2 exp -( XT -µ) 2 2 + exp -( XT +µ) 2 2    .<label>(62)</label></formula><p>Combining the above two entropy contributions, we get the total entropy change:</p><formula xml:id="formula_86">∆S tot = ∆S E + ∆S = ln   P X([T ]) P [X([T ])]   ,<label>(63)</label></formula><p>where X([T ]) and X([T ]) have included the starting point. Therefore, the reverse dynamics also obeys the following integral fluctuation theorem: which suggests that the second law of thermodynamics ⟨∆S tot ⟩ ≥ 0. This integral fluctuation theorem is verified by our experiments in Fig. <ref type="figure" target="#fig_4">7</ref> of one-dimensional example. Detailed contribution of entropy is given in Fig. <ref type="figure" target="#fig_5">8</ref> as well. Compared to the forward process, the system and total entropy changes are biased toward positive values. Experimental details are given in Appendix A.</p><formula xml:id="formula_87">e -∆Stot = P X([T ]) e -∆Stot d X([T ]) = P [X([T ])] dX([T ]) = 1,<label>(64)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Entropy production rate</head><p>In this section, we analyze the time-dependent entropy change in both forward and reverse dynamics, focusing on the reverse generative dynamics.</p><p>For the ensemble entropy production rate, we consider the following initial distribution:</p><formula xml:id="formula_88">p(X 0 ) = 1 2 N µ, σ 2 I d + 1 2 N -µ, σ 2 I d ,<label>(65)</label></formula><p>where all components of µ are equal to a positive value µ. This implies that the probability of X t at time t is given by a convolution of the initial distribution and the Gaussian transition kernel:</p><formula xml:id="formula_89">p(X t , t) = 1 2 N µ t , (I d -e -2t I d ) + σ 2 I d e -2t + 1 2 N -µ t I d , (I d -e -2t I d ) + σ 2 I d e -2t ,<label>(66)</label></formula><p>where µ t = µe -t . Next, we write down the expression of the probability current for both forward and reverse dynamics:</p><formula xml:id="formula_90">J = f (X t , t)p(X t , t) -∇p(X t , t),<label>(67)</label></formula><p>where f (X t , t) = -X t for the forward dynamics, and 2 tanh µ ⊤ T -s Xs µ T -s -Xs for the reverse dynamics (σ 2 = 1). Note that the forward and backward SDEs share the same state probability distribution p(X t , t) (see Eq. (66), and a proof given in Appendix C). Note also that the probability current of the forward OU process has the same magnitude but opposite direction with the reverse generative SDE (see a proof in Appendix D). Inserting the above definitions, one can estimate the entropy production rate π and the entropy flux ϕ according to Eq. ( <ref type="formula" target="#formula_42">30</ref>). Although our formula applies to the high-dimensional case (the dimensionality can be set to an arbitrary number), we consider a one-dimensional example to analyze the entropy production rate and entropy flux for simplicity. Due to the anti-symmetry properties of probability currents and the identical state probability, we have the same entropy production rate:</p><formula xml:id="formula_91">π = π * = X t p(X t , t) + ∂p(Xt,t) ∂Xt 2 p(X t , t) dX t .<label>(68)</label></formula><p>We use the superscript * to indicate the reverse process. However, the entropy fluxes are different depending on the specific forms of the drift force. According to Sec. II D, we have the following results:</p><formula xml:id="formula_92">ϕ = X t X t p(X t , t) + ∂p(X t , t) ∂X t dX t , ϕ * = -f rev (X t , t) X t p(X t , t) + ∂p(X t , t) ∂X t dX t .<label>(69)</label></formula><p>Note that for σ 2 = 1, the drift force for the reverse dynamics is given by f rev (X t , t) = 2 tanh (µ t X t ) µ t -X t . The results for the case of σ 2 ̸ = 1 are derived in Appendix E. As derived in Sec. III C, the speciation time t S = 1 2 ln µ 2 + σ 4 -2σ 2 + µ 4 + 1 for a one-dimensional generative examples of arbitrary values of mean and variance. This corresponds exactly to the first appearance of the inflection point on the potential or free energy curve (see Sec. III C). As shown in Fig. <ref type="figure">9</ref> for the forward OU process, the entropy production rate drops monotonically toward zero, while the system entropy rate decreases first and then increases toward zero. With increasing time in a reverse process (from noise sample to data sample), the entropy flux is first negative, and after some time step earlier than t S , the flux becomes positive, indicating that the order is generated (Fig. <ref type="figure">10</ref>). The change rate of system entropy is first positive, and particularly before t S , the system entropy rate starts to decrease. In particular, the rate drops sharply as the starting point (t = 0) is approached, and at t = 0 one real sample is generated. Meanwhile, the entropy flux increases sharply as well, indicating a generative process in sampling the target data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Glass transition</head><p>It was recently argued that at a time less than t S (in the reverse dynamics), a collapse transition would take place, i.e., the trajectory condenses onto one single sample of data distribution <ref type="bibr" target="#b13">[14]</ref>, which further clarified that the essence is the glass transition. This recent analysis relies on the empirical distribution of time-dependent state (e.g., X t here), where n data points are selected as an initial condition for the diffusion dynamics. Here, we re-interpret this picture using the Franz-Parisi potential, a powerful statistical physics tool to characterize the geometric structure of glassy energy landscape <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>, as in our current case, the score can be analytically computed, and as shown before, the dynamical state at some specific moment during the reverse process plays a role of quenched disorder for the statistical inference of the denoising process.</p><p>We start from the statistical inference defined by Eq. ( <ref type="formula" target="#formula_65">47</ref>). We select an equilibrium reference configuration, namely X ′ 0 , and consider a restricted Boltzmann measure</p><formula xml:id="formula_93">p(X 0 |X t , X ′ 0 ) = 1 Z(X t , X ′ 0 , q) exp [-H(X 0 |X t , t)] δ [q -d(X 0 , X ′ 0 )] ,<label>(70)</label></formula><p>where d(X 0 , X ′ 0 ) denotes the Euclidean distance of two high dimensional vectors. This restricted Boltzmann measure can be transformed to a soft constraint with a coupling filed ϵ. Therefore, we shall focus on the following constrained free energy:</p><formula xml:id="formula_94">F (ϵ) = - dX ′ 0 p(X ′ 0 |X t ) ln dX 0 e -H(X0|Xt,t)+ϵd(X0,X ′ 0 ) Xt ,<label>(71)</label></formula><p>where X t ∼ p(X t , t) derived before, and acts as quenched disorder as in usual spin glass theory <ref type="bibr" target="#b28">[29]</ref>. We thus define the Franz-Parisi potential V (q) = ⟨-ln Z(X t , X ′ 0 , q)⟩, where ⟨•⟩ means the average over X ′ 0 and X t . The Franz-Parisi potential V (q) is obtained by a Legendre transform of F (ϵ) which is given below.</p><formula xml:id="formula_95">V (q) = min ϵ F (ϵ) + ϵq,<label>(72)</label></formula><p>where q is determined by ∂F ∂ϵ = -q. The Franz-Parisi potential develops the second minimum if a dynamical glass transition occurs, implying that the ergodicity breaks. Furthermore, if the second minimum reaches the same height with the first minimum, a static glass transition occurs with vanishing complexity <ref type="bibr" target="#b27">[28]</ref>. We next show an example of one-dimensional Franz-Parisi potential which can be computed directly. The Hamiltonian reads,</p><formula xml:id="formula_96">H(X 0 |X t , t) = (X t -X 0 e -t ) 2 2(1 -e -2t ) -ln 1 2 √ 2πσ 2 exp - (X 0 -µ) 2 2σ 2 + 1 2 √ 2πσ 2 exp - (X 0 + µ) 2 2σ 2 . (<label>73</label></formula><formula xml:id="formula_97">)</formula><p>The constrained partition function can be written as follows,</p><formula xml:id="formula_98">Z(X t , X 0 ′ , q, t) = exp(-H(X 0 |X t , t))δ(q -(X 0 -X 0 ′ ) 2 )dX 0 .<label>(74)</label></formula><p>Therefore, the Franz-Parisi potential reads,</p><formula xml:id="formula_99">V (q, t) = - p X 0 ′ | X t , t ln Z(X t , X 0 ′ , q, t)dX 0 ′ Xt .<label>(75)</label></formula><p>To proceed, we have to use the following property of Dirac delta function:</p><formula xml:id="formula_100">δ(q -(X 0 -X 0 ′ ) 2 ) = 1 2 √ q δ(X 0 -X 0 ′ - √ q) + δ(X 0 -X 0 ′ + √ q) .<label>(76)</label></formula><p>Then, we can simplify the constrained partition function as follows.</p><p>Z</p><formula xml:id="formula_101">(X t , X 0 ′ , q, t) = exp(-H(X 0 |X t , t))δ(q -(X 0 -X 0 ′ ) 2 )dX 0 = 1 2 √ q exp -H(X 0 ′ + √ q|X t , t) + 1 2 √ q exp -H(X 0 ′ - √ q|X t , t) .<label>(77)</label></formula><p>We already know the joint distribution p(X ′ 0 , X t , t) as follows,</p><formula xml:id="formula_102">p X 0 ′ , X t , t = p(X t , t|X ′ 0 )p(X ′ 0 ) = exp -(Xt-X0 ′ e -t ) 2 2(1-e -2t ) 2π(1 -e -2t ) 1 2 √ 2πσ 2 exp - (X 0 ′ -µ) 2 2σ 2 + 1 2 √ 2πσ 2 exp - (X 0 ′ + µ) 2 2σ 2 = 1 4π σ 2 (1 -e -2t ) exp - (X t -X 0 ′ e -t ) 2 2(1 -e -2t ) exp - (X 0 ′ -µ) 2 2σ 2 + exp - (X 0 ′ + µ) 2 2σ 2 .<label>(78)</label></formula><p>It is therefore straightforward to use Monte-Carlo method to estimate the Franz-Parisi potential. We first generate T pairs of (X ′ 0 , X t ) according to the above joint distribution. Then the potential is estimated in a simple way.</p><formula xml:id="formula_103">V (q, t) = - 1 T T ℓ=1 ln Z(X t,ℓ , X ′ 0,ℓ , q, t).<label>(79)</label></formula><p>The Franz-Parisi potential is plotted as a function of Euclidean distance q for the one-dimensional example in Fig. <ref type="figure">11</ref>. When the time decreases from the starting point of the reverse diffusion, the potential develops an inflection point where the second derivative vanishes at some moment. As the time approaches the starting time (t = 0), more inflection points appear, due to fragmented (inferred) data space given the current X t value. This demonstrates that the reverse trajectory will collapse onto a single data sample. The complicated yet tractable computation of the high dimensional case is left for future works, where order parameters characterizing the transition would emerge.</p><formula xml:id="formula_104">(b) (c) (d) (e) (f) (g) (h) (i)<label>(a)</label></formula><p>FIG. <ref type="figure">11</ref>: The Franz-Parisi potential and its first and second derivatives for the case of µ = 1 and σ 2 = 0.1. tS = 0.42622. Three time steps are considered, i.e., t = 0.42, t = 0.015, and t = 0.01.</p><p>It is then verified that the gradient of the log-likelihood is given by</p><formula xml:id="formula_105">∇ ln p(X t , t) = e t -e t X t + µ tanh e t µXt -1+e 2t +σ 2 -1 + e 2t + σ 2 . (B2)</formula><p>A step-by-step calculation is presented in Appendix E. Therefore, the first derivative of the potential is given by</p><formula xml:id="formula_106">∂U (X t , t) ∂X t = -X t + 2X t 1 + e -2t (σ 2 -1) - 2µe t tanh µe t Xt e 2t +σ 2 -1 e 2t + σ 2 -1 . (B3)</formula><p>The second derivative is then given as follows:</p><formula xml:id="formula_107">∂ 2 U (X t , t) ∂X 2 t = -1 + 2 1 + e -2t (σ 2 -1) -2 µe t e 2t + σ 2 -1 2 1 -tanh 2 µe t X t e 2t + σ 2 -1 . (B4)</formula><p>Finally, we check that there are four solutions to ∂ 2 U (Xt,t)</p><formula xml:id="formula_108">∂X 2 t | Xt=0 = 0.</formula><p>Only one of them gives a real value of time, i.e., t S = 1 2 ln µ 2 + σ 4 -2σ 2 + µ 4 + 1 . It is surprising that the symmetry breaking can become unstable if the sign of the first derivative of the potential at X t → ∞ changes from negative to positive. This is determined by the following limit:</p><formula xml:id="formula_109">lim Xt→∞ ∂U (X t , t) ∂X t = - 2e t µ C(σ, t) + lim Xt→∞ 2X t e -2t C(σ, t) -X t ,<label>(B5)</label></formula><p>where we have used Eq. (B3), and C(σ, t) = e 2t + σ 2 -1. It is clear that when C(σ, t) &lt; 2e 2t , the potential shape will change from the one of double minima to an inverted U shape. At the critical line C(σ, t) = 2e 2t , the potential shape is still of the double-minima type, because of its negative slope at X t = ∞ (the exact value is given by -µe -t ). Therefore, we determine the critical time as t U SB = 1 2 ln σ 2 -1 , which is independent of µ. Moreover, the transition is of the first order.</p><p>Appendix C: Proof of the same state probability for both forward and reverse dynamics</p><p>The forward OU process is given by</p><formula xml:id="formula_110">dX t = f (X t , t)dt + √ 2dW, (<label>C1</label></formula><formula xml:id="formula_111">)</formula><p>where dW is the Wiener process. The solution of the corresponding FPE is specified by p(X t , t). The reverse generative diffusion dynamics is described by (C4)</p><p>Noticing that Xs = X(T -t) where t is the forward time, we can conclude that the last equation in Eq. (C4) is exactly the same with the forward FPE. Therefore, both processes bear the same state probability distribution, as intuitively expected.</p><p>p(X t , t) is obtained from Eq. (66) by setting d = 1. The entropy production rates are equal in both forward and reverse generative processes. They can be estimated below:</p><formula xml:id="formula_112">π = π * = X t p</formula><p>(X t , t) + ∂p(Xt,t) ∂Xt 2 p(X t , t) dX t = X 2 t p(X t , t)dX t + 2X t ∂p(X t , t) ∂X t dX t + 1 p(X t , t) ∂p(X t , t) ∂X t 2 dX t = X 2 t p -2 -∂ 2 ln p(X t , t) ∂X t 2 p , (E3) where ∂ 2 ln p(Xt,t) ∂Xt 2 p = ∂ 2 ln p(Xt,t) ∂Xt 2 p(X t , t)dX t . The entropy flux for the reverse generative process can be estimated as follows, ϕ * = X t p(X t , t) + ∂p(X t , t) ∂X t f * (X t , t)dX t = ⟨f * (X t , t)X t ⟩ p + f * (X t , t) ∂p(X t , t) ∂X t dX t = ⟨f * (X t , t)X t ⟩ p -∂f * (X t , t) ∂X t p = 2 ∂ ln p(X t , t) ∂X t X t p + X t 2 p -2 ∂ 2 ln p(X t , t) ∂X t 2 p -1 = -3 + X t 2 p -2 ∂ 2 ln p(X t , t) ∂X t 2 p . (E4)</p><p>where * indicates the reverse process, and f * (X t , t) = 2 ∂ ln p(Xt,t) ∂Xt + X t for the reverse process. The system entropy rate for the forward OU process can now be written as follows:</p><formula xml:id="formula_113">Ṡ = π -ϕ = -1 - ∂ 2 ln p(X t , t) ∂X t 2 p . (<label>E5</label></formula><formula xml:id="formula_114">)</formula><p>The same rate for the reverse process is given as follows:</p><formula xml:id="formula_115">Ṡ * = π * -ϕ * = 1 + ∂ 2 ln p(X t , t) ∂X t 2 p . (<label>E6</label></formula><formula xml:id="formula_116">)</formula><p>We conclude that Ṡ and Ṡ * are anti-symmetric quantities.</p><p>To explicitly compute the above physical quantities, we need to compute the following expectations. The first one is the second moment of X t , which reads X 2 t p = µ 2 e -2t + 1 -e -2t + σ 2 e -2t . (E7)</p><p>The second one ∂ 2 ln p(Xt,t) ∂Xt 2 p is derived below using the score function in Eq. (E1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(E9)</head><p>In the case of σ 2 = 1, the above formulas reduce to the following simple results: ϕ = µ 2 e -2t , π = π * = e -2t µ 2 tanh 2 (e -t µX t ) p , ϕ * = -µ 2 e -2t + 2e -2t µ 2 tanh 2 (e -t µX t ) p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(E10)</head><p>It is also interesting to show that ⟨tanh(µ t X t )µ t X t ⟩ = µ 2 t (µ t = µe -t in one dimension), which holds only for σ 2 = 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 2 :</head><label>2</label><figDesc>FIG.2: Ensemble average of e -∆S tot as a function of the number of trajectories used to do the average, simulated by solving the forward OU process. µ = 1, and other parameters are detailed in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 3 :</head><label>3</label><figDesc>FIG. 3: Normalized histogram of entropy changes estimated from 10 000 trajectories for the forward OU process. µ = 1, and other parameters are detailed in Appendix A. (a) Statistics of system entropy change. (b) Statistics of environment entropy change. (c) Statistics of the total entropy change. (d) Statistics of e -∆S tot .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 FIG. 4 :FIG. 5 :</head><label>145</label><figDesc>FIG.4: Evolution of potential energy (µ = 1) in one dimension. The speciation transition time is given by tS ≈ 0.35.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>tFIG. 6 :</head><label>6</label><figDesc>FIG. 6: Typical profiles of potential in the reverse generative process corresponding to different phases in Fig. 5. (a) Stable symmetry breaking. (b) Symmetry un-breaking. (c) Unstable symmetry breaking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIG. 7 :</head><label>7</label><figDesc>FIG. 7: Ensemble average of e -∆S tot as a function of the number of trajectories used to estimate the average, simulated by solving the backward SDE. µ = 1, and other parameters are detailed in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 8 :</head><label>8</label><figDesc>FIG. 8: Normalized histogram of entropy changes estimated from 10 000 Trajectories for the reverse generative process. µ = 1, and other parameters are detailed in Appendix A. (a) Statistics of system entropy change. (b) Statistics of environment entropy change. (c) Statistics of total entropy change. (d) Statistics of e -∆S tot .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 9 :FIG. 10 :</head><label>910</label><figDesc>FIG. 9: Entropy production rate and flux (µ = 1, σ 2 = 0.4). The results are plotted for the forward OU process from time 0 to 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>d∂ 2 P∂ 2</head><label>22</label><figDesc>Xs = 2∇ ln p( Xs , T -s) -f ( Xs , T -s) ds + √ 2dW, (C2)where s is in an increasing order. The corresponding FPE is given below:∂P ( Xs , s) ∂s = -∇ • 2∇ ln p( Xs , T -s) -f ( Xs , T -s) P ( Xs , s) ( Xs , s) ∂ Xi ∂ Xi . (C3)We then replace P ( Xs , s) by p( Xs , T -s), and keep others unchanged, obtaining the following result:∂p( Xs , T -s) ∂s = -∇ • 2∇ ln p( Xs , T -s) -f ( Xs , T -s) p( Xs , T -s) p( Xs , T -s) ∂ Xi ∂ Xi = ∇ • [f ( Xs , T -s)p( Xs , T -s)] -d i=1 ∂ 2 p( Xs , T -s) ∂ Xi ∂ Xi ⇒ ∂p( Xs , T -s) ∂(T -s) = -∇ • [f ( Xs , T-s)p( Xs , T -s)] + d i=1 ∂ 2 p( Xs , T -s) ∂ Xi ∂ Xi .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>∂ 2 = e -2t µ 2 ( 1 2 - 1 1 2 ( 1 - 2 +e -2t µ 2 tanh 2 (e -t µXt 1 -( 1 - 2 + 1 1 2 e -2t µ 2 ( 1 - 2 + 2 e -2t µ 2 tanh 2 (e -t µXt 1 -( 1 - 2 + 2 1</head><label>22121212211212212221122</label><figDesc>ln p(X t , t-2t + e -2t σ 2 ∂ tanh e -t µXt 1-e -2t +e -2t σ 2 (e -t µ) -X t ∂X t p = 1 1 -e -2t + e -2t σ 2 1 -tanh 2 e -t µX t 1 -e -2t + e -2t σ 2 e -2t µ 2 1 -e -2t + e -2t σ 2 -1 p -e -2t + e -2t σ 2 ) 2 -e -2t µ 2 tanh 2 ( e -t µXt 1-e -2t +e -2t σ 2 ) p (1 -e -2t + e -2t σ 2 )-e -2t + e -2t σ 2 .(E8)Inserting these two expression into the entropy flux and production rate, we can get the final general results forσ 2 ̸ = 1. ϕ = µ 2 e -2t -e -2t + σ 2 e -2t , π = π * = µ 2 e -2t -e -2t + σ 2 e -2t -1 -e -2t µ e -2t + e -2t σ 2 ) e -2t +e -2t σ 2 ) p e -2t + e -2t σ 2 ) -e -2t + e -2t σ 2 , ϕ * = µ 2 e -2t -e -2t + σ 2 e -2t -2e -2t + e -2t σ 2 )e -2t +e -2t σ 2 ) p e -2t + e -2t σ 2 )-e -2t + e -2t σ 2 .</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research was supported by the <rs type="funder">National Natural Science Foundation of China</rs> for Grant Number <rs type="grantNumber">12122515</rs> (H.H.), and <rs type="funder">Guangdong Provincial Key Laboratory of Magnetoelectric Physics and Devices</rs> (No. <rs type="grantNumber">2022B1212010008</rs>), and <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (Grant No. <rs type="grantNumber">2023B1515040023</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Bn7aaYm">
					<idno type="grant-number">12122515</idno>
				</org>
				<org type="funding" xml:id="_kRZqSz4">
					<idno type="grant-number">2022B1212010008</idno>
				</org>
				<org type="funding" xml:id="_Xn89XdH">
					<idno type="grant-number">2023B1515040023</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we provide a thorough study of generative diffusion models widely used in unsupervised machine learning, especially in Sora <ref type="bibr" target="#b29">[30]</ref>. We use nonequlibrium physics concepts to dissect the mechanisms of the diffusion model, and derive the entropy production, the second law of stochastic thermodynamics and path probability, and we also treat the reverse generative process as a statistical inference problem where the state variable at the reverse time step serves as quenched disorder like in a standard spin glass problem <ref type="bibr" target="#b1">[2]</ref>, and apply the concept of equilibrium physics such as potential energy, free energy, and Franz-Parisi potential to study different kinds of phase transitions in the reverse process. Although some results are revealed by recent works from different angles <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, our results provide a complete physics picture from nonequilibrium (such as fluctuation theorem, entropy production) to equilibrium (especially spontaneous symmetry breaking and geometric method to study glass transition), making generative diffusion models more transparent. Our main contributions have been elaborated in the introduction. We hope these methods or concepts elaborated in our paper will inspire statistical physicists to study the currently active research frontier of diffusion models, for which a pure understanding may inspire new inductive biases for designing better machine learning models and verification of physical laws in machine learning as well. In this section, we give an example of calculating the entropy change in one-dimensional SDE. The one-dimensional SDE reads</p><p>with a given initial distribution:</p><p>We consider the Ito convention λ = 0. Thus, we have the following discretized equation:</p><p>where η ∼ N (0, 1) (i.i.d. in time), T = 10, and dt = 0.01. We sample from the initial distribution and run Eq. (A3) for a duration of 1000 steps to obtain an ensemble of stochastic trajectories X ∈ R 1 000×M , where M is the number of total trajectories. We define X τ (n) is the n-th element of the column vector for τ -th trajectory. We calculate the entropy change of the system as</p><p>The environment entropy change is calculated as follows,</p><p>The total entropy change can be calculated using</p><p>We sampled 10 000 trajectories of the discrete Langevin dynamics. From these trajectories, we compute the statistics of all three entropy quantities and verify the integral fluctuation theorem. When verifying the convergence of e -∆Stot , we change the size of the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Backward process</head><p>We also use a one-dimensional example to calculate the entropy quantities. First, the one-dimensional backward SDE reads</p><p>where the initial distribution is specified by</p><p>We consider the Ito convention λ = 0. Then, the discretized SDE reads</p><p>where η ∼ N (0, 1) independently for every time step, T = 10, and ds = 0.01. We start from X0 , and run the reverse dynamics for a total of 1 000 steps and get a trajectory vector Xτ . We define Xτ (n) as the n-th step of the trajectory vector.</p><p>The entropy change of the system can be estimated as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(A11)</head><p>We calculate the total entropy production as follows,</p><p>These three entropy quantities are estimated from an ensemble of 10 000 stochastic trajectories. When verifying the convergence of e -∆Stot , we change the size of the ensemble. All codes used in this paper are accessible through our GitHub link <ref type="bibr">[31]</ref>.</p><p>Appendix B: Speciation time for one dimensional example of arbitrary data mean and variance</p><p>According to the analysis in Sec. III C, we have the following potential function:</p><p>where p(X t , t) can be obtained from a similar convolution to that in Eq. ( <ref type="formula">5</ref>) with the result p(X t , t) =</p><p>Appendix D: Probability currents for both forward and reverse dynamics</p><p>In this section, we prove that the probability currents have the same magnitude but opposite directions for forward and reverse processes. First, one can write down the forward probability current as follows,</p><p>The force term in the reverse SDE is given by f rev = 2∇ ln p( Xs , T -s) -f ( Xs , T -s), whose corresponding probability current reads</p><p>where we have replaced P ( Xs , s) by p( Xs , T -s) as in Sec. C.</p><p>Appendix E: Entropy production rate for σ 2 ̸ = 1</p><p>In the case of σ 2 ̸ = 1, the score function can also be computed in an analytic form. We only derive the formulas for the one-dimensional example. The details are given below. We first compute the score function.</p><p>2(1-e -2t +e -2t σ 2 ) (-e -t µ+Xt) </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Statistical Mechanics of Neural Networks</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Fokker-Planck Equation</title>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Risken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Van Kampen</surname></persName>
		</author>
		<title level="m">Stochastic Processes in Physics and Chemistry</title>
		<meeting><address><addrLine>North-Holland Personal Library, North-Holland, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic thermodynamics, fluctuation theorems and molecular machines</title>
		<author>
			<persName><forename type="first">Udo</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reports on Progress in Physics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">126001</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stochastic thermodynamics: an introduction</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Peliti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Pigolotti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Introduction to dynamical mean-field theory of randomly connected neural networks with bidirectionally correlated couplings</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SciPost Phys. Lect. Notes</title>
		<imprint>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative diffusion in very large dimensions</title>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Mézard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="volume">2023</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">93402</biblScope>
			<date type="published" when="2023-10">oct 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spontaneous symmetry breaking in generative diffusion models</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Raya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ambrogioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="66377" to="66389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Bonnaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>De Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Mezard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18491</idno>
		<title level="m">Dynamical regimes of diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Davide</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatin</forename><surname>Dandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenka</forename><surname>Zdeborová</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.14085</idno>
		<title level="m">Sampling with flows, diffusion and autoregressive neural networks: A spin-glass perspective</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The statistical thermodynamics of generative diffusion models: Phase transitions, symmetry breaking and critical instability</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ambrogioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17467</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Understanding diffusion models by feynman&apos;s path integral</title>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Hirono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akinori</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukushima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.11262</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimal model of permutation symmetry in unsupervised learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Tianqi Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Michael Wong</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and Theoretical</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">414001</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical physics of unsupervised learning with prior knowledge in neural networks</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">248302</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Origin of the computational hardness for learning with binary synapses</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiyuki</forename><surname>Kabashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">52813</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The theory of the brownian movement</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Einstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann der Physik</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">549</biblScope>
			<date type="published" when="1905">1905</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Kulin</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Klivans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01178</idno>
		<title level="m">Learning mixtures of gaussians using the ddpm objective</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rules of calculus in the path integral representation of white noise langevin equations: the onsager-machlup approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Leticia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Cugliandolo</surname></persName>
		</author>
		<author>
			<persName><surname>Lecomte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and Theoretical</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page">345001</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Entropy production along a stochastic trajectory and an integral fluctuation theorem</title>
		<author>
			<persName><forename type="first">Udo</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">40602</biblScope>
			<date type="published" when="2005-07">Jul 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entropy production in nonequilibrium systems described by a fokker-planck equation</title>
		<author>
			<persName><forename type="first">Tânia</forename><surname>Tomé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brazilian Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1285" to="1289" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName><forename type="first">Brian Do</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective potential in glassy systems: theory and simulations</title>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="339" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mézard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Virasoro</surname></persName>
		</author>
		<title level="m">Spin Glass Theory and Beyond</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
