# Nonequilbrium physics of generative diffusion models

## Abstract

## 

Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interests from engineering, statistics and physics, but a complete picture about inherent mechanisms is still lacking. In this paper, we provide a transparent physics analysis of diffusion models, formulating the fluctuation theorem, entropy production, equilibrium measure, and Franz-Parisi potential to understand the dynamic process and intrinsic phase transitions. Our analysis is rooted in a path integral representation of both forward and backward dynamics, and in treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder akin to that in spin glass theory. Our study thus links stochastic thermodynamics, statistical inference and geometry based analysis together to yield a coherent picture about how the generative diffusion models work.

## I. INTRODUCTION

Neural network based machine learning has triggered a lot of research interests in a variety of fields [[1,](#b0)[2]](#b1). One of current active directions is the generative diffusion models (GDMs) [[3]](#b2)[[4]](#b3)[[5]](#b4)[[6]](#b5), which are rooted in nonequilibrium physics [[7,](#b6)[8]](#b7). Forward and backward stochastic differential equations (SDEs, or Langevin equations in physics) are used; the forward part is to diffuse a data sample (e.g., a real image) into a Gaussian white noise distribution, after that, taking a sample from this Gaussian white noise distribution starts the backward process, driven by the gradient of log-state-likelihood, and finally this reverse Langevin dynamics collapses onto a real sample subject to the true data distribution, thereby completing the unsupervised data generation. This process is in essence a physical process, whose cornerstone is nonequilibrium dynamics, a central topic of statistical physics [[7]](#b6)[[8]](#b7)[[9]](#b8)[[10]](#b9)[[11]](#b10).

Recent interests from physics community focused on symmetry breaking in the diffusion process [[12]](#b11)[[13]](#b12)[[14]](#b13), Bayesoptimal denoising interpretation of the GDMs [[15]](#b14), reformulation as equilibrium statistical mechanics [[16]](#b15), and path integral representation of the stochastic trajectories [[17]](#b16). We remark that the symmetry breaking concept in unsupervised learning (GDM is one type of unsupervised learning) has been introduced and analyzed in earlier works [[18,](#b17)[19]](#b18). In this work, we define a high-dimensional Gaussian mixture data model that allows for analytic studies. Although recent works also studied this Gaussian mixture data [[14]](#b13), our main contributions in this work are distinct from recent works in the following three aspects. Our formulas apply to the high-dimensional dynamics, but we show one-dimensional examples for demonstration of the concept.

First, the GDM is analyzed through the concept of entropy production, especially for the reverse generative process. The derived formulas apply to the high-dimensional case. For a simple demonstration, the environmental and system entropy changes are explicitly computed, obeying the fluctuation theorem. The ensemble entropy production rate is also calculated for an example of one-dimensional diffusion, displaying distinct qualitative behaviors in both forward and backward processes. We also demonstrate the same dynamic state probability for both processes, and the probability currents have the same magnitude but opposite directions for the forward and reverse processes.

Second, we derive a generalized form of statistical inference for our current data model, where a temperature-like parameter is set by a variance matrix. Moreover, we prove that for this generalized form, the free energy is exactly the potential energy derived previously in Ref. [[13]](#b12). By an intuitive exploration of the qualitative shape of the potential (or equivalently the free energy of the statistical inference problem), we get the speciation transition point derived previously by an alternative method of Landau expansion or overlap dynamics [[14]](#b13). Moreover, we analyze the case of previously-unexplored one-dimensional diffusion example of arbitrary data mean and variance, and reveal a phase diagram of symmetry breaking, symmetry un-breaking and unstable symmetry breaking phases. Hence, we unify relevant results in our generalized form of statistical inference.

Third, we provide a geometry oriented way to look at a recently discovered collapse transition in the reverse diffusion process. In previous works, an empirical distribution for the reverse dynamic state is assumed, and the collapse transition is thus related to the number of data points used to represent the empirical distribution and the dimensionality of the dynamic state as well. In contrast, we start from the statistical inference standpoint without using the empirical distribution, and define a geometry measure, i.e., Franz-Parisi potential (see the first application in neural networks [[20]](#b19)) to capture the potential emergence of hidden structures in the conditional probability of the statistical inference. We provide an effective description in a one-dimensional example. The complicated yet tractable computation of the high dimensional case is left for future works. Our work focuses on a pure understanding of an analytically tractable GDM, without taking into account the algorithmic design, which typically requires training a complex neural network for non-Gaussian real dataset. The paper is organized as follows. We first introduce the forward diffusion process, together with the fluctuation theorem derived from the forward process, and concepts of stochastic entropy and ensemble average. Then we derive the reverse generative dynamics applied to generate the real data samples in machine learning, and introduce in details the concept of potential energy and free energy to analyze the denoising process formulated as a statistical inference. We also derive the fluctuation theorem and entropy production rate for the reverse process, together with Franz-Parisi potential applied to analyze how fragmented the configuration space for the inference is as the time approaches the starting point of the forward dynamics. We finally summarize our studies and make future perspectives in the last section.

## II. NONEQUILIBRIUM PHYSICS OF FORWARD DIFFUSION PROCESS

## A. Forward diffusion dynamics

A classic example of stochastic dynamics is the well-known Brownian motion, whose dynamics is called the Langevin dynamics. Consistence between Brownian motion and thermodynamics has been established in 1905 [[21]](#b20). Current AI studies make nonequilibrium physics of Langevin dynamics regain intense research interests [[11,](#b10)[17]](#b16). In the forward process, we use Ornstein-Ulhenbeck (OU) process [[7]](#b6) to turn a real data point into a white noise, which is detailed as a high dimensional SDE:

$Ẋ = -X + √ 2ξ,(1)$where X ∈ R d and ξ ∈ R d are time-dependent high dimensional state and noise quantities, respectively, and ξ is a Gaussian white noise with correlation ⟨ξ i (t)ξ j (t ′ )⟩ = δ ij δ(t -t ′ ). Given the initial condition X(0) ≡ X 0 , the above SDE has a solution:

$X t = e -t X 0 + √ 2e -t t 0 dse s ξ(s),(2)$from which, X(t) ≡ X t is clearly a Gaussian random variable, which can be reformulated as the following form using independent standard Gaussian random variable:

$X t = e -t X 0 + 1 -e -2t Z t ,(3)$where Z t is the standard Gaussian random variable, and the variance of X t is given by 1 -e -2t . In the following, we make no difference between X(t) and X t .

For simplicity, we choose the distribution of the data as a Gaussian mixture of two classes (e.g., two kinds of images):

$p(X 0 ) = 1 2 N (µ, I d ) + 1 2 N (-µ, I d ) , (4$$)$where µ is the d-dimensional constant vector, and I d is the d-dimensional identity covariance matrix. In most parts of this paper, we consider this simple Gaussian mixture with unit variance. The more general case of non-unit variance is also discussed when necessary. It is also straightforward to generalize our analysis to multiple classes. Then at time t, the probability distribution p(X t , t) can be calculated by a convolution [[22]](#b21) p(X t , t

$) = dX 0 p(X 0 )p(X t |X 0 ) = dX 0 p(X 0 )N (X t ; X 0 e -t , Σ t I d ) = 1 2 N (X t ; µ t , I d ) + 1 2 N (X t ; -µ t , I d ),(5) 0 3 3 0$FIG. [1:](#) A schematic illustration of the generative diffusion process of two-dimensional Gaussian mixture data. The forward process from time t = 0 to t = 3 is shown together with the reverse process from t = 3 back to t = 0. The gradient of log-statelikelihood can be analytically estimated as the state probability is given by p(Xt, t)

$= 1 2 N µe -t , I d + 1 2 N -µe -t , I d .$where µ t ≡ µe -t , and Σ t ≡ 1 -e -2t . Representative trajectories of the forward process are shown in Fig. [1](#).

## B. Fluctuation theorem for the forward diffusion

Because of stochasticity, the trajectories are not differentiable any more in general. A specific time-discretization scheme for the stochastic differential equation must be carefully chosen, for which the stochastic calculus is established (see details below). Then we would derive the path probability of {X t } given an initial point X 0 . To do this, we have to specify the discretization scheme for the above SDEs. We first define random variable W(t) for the Wiener process as follows:

$W(t) = t t0 dt ′ ξ(t ′ ) = t t0 dW.(6)$The stochastic integral can be expressed as

$t t0 dt ′ ξ(t ′ )f (X t , t) = t t0 dWf (X t , t$), called the Riemann-Stieltjes integral as well. In studies of SDEs, we have two commonly used conventions to represent this stochastic integral. The first one is the Ito convention, or the initial point scheme. More precisely, the Riemann-Stieltjes integral is calculated as

$t t0 dWf (X t , t) = lim dt→0 N k=0 [W(t k + dt) -W(t k )] f (X(t k ), t k ),(7)$where t k = t 0 + kdt, and N = t-t0 dt . In general, we interpolate the time between t k and t k+1 as τ = (1 -λ)t k + λt k+1 . Therefore, the Ito convention corresponds to λ = 0. This stochastic integral is clearly dependent on the discretization scheme [[10]](#b9).

The second one is the Stratonovich convention, i.e., mid-point scheme with λ = 1/2. Then, we have the following expression:

$t t0 dWf (X t , t) = lim dt→0 N k=0 [W(t k + dt) -W(t k )] f ([X(t k + dt) + X(t k )]/2, t k + dt/2) .(8)$Next we rewrite Eq. (1) as follows,

$dX t = f (X t , t)dt + √ 2dW, (9$$)$where dt is a small step size as used before. According to the general discretization, we have

$X t+dt -X t dt = f ((1 -λ)X t + λX t+dt , t + λdt) + η t ,(10)$where η i,t ∼ N (0, 2/dt). In the following we can take t + dt as t ′ , or t ′ -t = dt in the Markovian process defined by Eq. [(10)](#b9); dt is the unit of the above discretization. We next use the following probability transformation identity:

$P (X t ′ , t ′ |X t , t) = P (η t ) ∂η t ∂X t ′ ,(11)$where the determinant ∂ηt ∂X t ′ is a Jacobian measuring the change of volume for the transformed probability density. This Jacobian can be easily computed using Eq. [(10)](#b9).

$∂η t ∂X t ′ = ∂ X t+dt -Xt dt -f ((1 -λ)X t + λX t+dt , t + λdt) ∂X t+dt ∝ e -λ∇•f dt ,(12)$where I d is an d-dimensional identity matrix, we have used the Taylor expansion (dt → 0) and the matrix identity det e K = e Tr K . Finally, based on the known form of the white noise distribution, we have the following infinitesimal propagator:

$P (X t+dt , t + dt|X t , t) ∝ e -λ∇•f dt e -| Ẋt -f (X t ,t)| 2 4 dt . (13$$)$Using the Markovian chain property, we get the conditional trajectory probability:

$P (X([T ])|X 0 ) = t ′ P (X t ′ , t ′ |X t , t) = t ′ e -λ∇•f dt e -| Ẋt -f (X t ,t)| 2 4 dt ∝ e T 0 [-λ∇•f - | Ẋt -f (X t ,t)| 2 4 ] λ ⊙ dt ,(14)$where X([T ]) ≡ {X T , . . . , X 1 } specifying a trajectory starting from X 0 , T denotes the length of the individual trajectory, and λ ⊙ denotes the corresponding λ-convention for the stochastic integral. An alternative way to get the same result is to use the following property of Dirac delta function:

$δ (X(t) -X ξ (t)) = δ (O[X(t)]) det δO δX ,(15)$where

$X ξ (t) is a solution of O[X] = Ẋ -f -ξ = 0. Therefore, P (X[T ]|X 0 ) = ⟨ t δ(X(t) -X ξ (t))⟩ {ξt} . Now, taking f = -X [see Eq. (1$)], we can derive the trajectory probability for the forward OU process

$P (X([T ])|X 0 ) = t ′ P (X t ′ , t ′ |X t , t) ∝ exp   - T 0 1 4 Ẋ + X 2 -λd λ ⊙ dt   , (16$$)$where d is the dimensionality of the dynamics, and the term in the exponent is called the action in physics for the path probability, and the integrand inside the time integral of the action is called the Lagrangian L [[23]](#b22), and the optimal path of maximal trajectory probability is determined by the Euler-Lagrange equation d dt ∂L ∂ Ẋ -∂L ∂X = 0. Next, we consider a reverse dynamics, i.e., X(s) = X(t), where s = T -t, and T is the time length of the trajectory.

It is clear that X(0) = X(T ), and X(T ) = X(0). We have then Ẋ(s) = -Ẋ(t). In analogy to the forward trajectory, the path probability of the backward trajectory given the initial point is given by

$P X([T ]) | X0 = N exp -A( X([T ])) , (17$$)$where N is a normalization factor, Xt ≡ X(t) (similar for X t ), and the action reads,

$A( X([T ])) = T 0 ds λ ⊙ ( Ẋ + X) 2 /4 -λd = T 0 dt 1-λ ⊙ (-Ẋ + X) 2 /4 -λd .(18)$Note that the time reversal changes the λ-convention to (1 -λ)-convention [[23]](#b22). The ratio between the conditional path probabilities is thus given by ln

$  P [X([T ]) | X 0 ] P X([T ]) | X0   = A( X([T ])) -A(X([T ])) = - T 0 dt 1-λ ⊙ Ẋ • X 2 - T 0 dt λ ⊙ Ẋ • X 2 = T 0 dt 1 2 ⊙[-Ẋ • X],(19)$where the action for the direct dynamics can be read off from Eq. ( [16](#formula_21)), the calculation in the last step leads to the result independent of λ (or equivalently λ = 1 2 ). This result can be interpreted as heat dissipated into the environment, since in an overdamped system, the product of total mechanical force and displacement equals to dissipation [[9]](#b8). Then we identify the following entropy change of environment:

$∆S E = T 0 dt 1 2 ⊙[-Ẋ • X],(20)$where we have assumed the temperature for the forward OU process equals to one. In addition, the starting and final states in the forward diffusion process can be treated as equilibrium states, subject to an analytic form of distribution (in fact they are Gaussian mixture). Then if we define a stochastic or trajectory-dependent entropy of the system as S(t) = -ln p(X t , t), we can derive the entropy change of the system as follows,

$∆S = ln p (X(0), 0) p(X(T ), T ) = ln   exp -(X(0)-µ) 2 2 + exp -(X(0)+µ) 2 2 exp -(X(T )-µe -T ) 2 2 + exp -(X(T )+µe -T ) 2 2   . (21$$)$We obtain then the total entropy change: which implies that the ratio of the path probabilities is exactly e ∆Stot . Note that in Eq. ( [22](#formula_30)) X([T ]) includes the initial state X(0). Equation ( [22](#formula_30)) is the well-known detailed fluctuation theorem, while the integral fluctuation theorem can be readily derived as

$∆S tot = ∆S E + ∆S = ln   P [X([T ])] P X([T ])   ,(22$$e -∆Stot = P [X([T ])] e -∆Stot dX([T ]) = P X([T ]) d X([T ]) = 1. (23$$)$From the integral fluctuation theorem, one can derive the stochastic second thermodynamics law ⟨∆S tot ⟩ ≥ 0 based on the convexity of the exponential function.

Finally we verify the integral fluctuation theorem in the forward diffusion models (see Fig. [2](#fig_0)) in an example of one-dimensional OU process. As the number of trajectories gets large, the trajectory average ⟨∆S tot ⟩ converges to one, as predicted by the theory. Each contribution of the entropy change is shown in Fig. [3](#fig_1). Some trajectories bear a negative entropy change, i.e., entropy decreases during the evolution, but on average, the stochastic second law of thermodynamics is still valid. Experimental details to get the entropy contribution are given in Appendix A.

## C. Rate of stochastic entropy production

We first recall the Fokker-Planck equation (FPE) corresponding to the OU forward process under the Ito convention as follows,

$∂p(X t , t) ∂t = -∇ • [f (X t , t)p(X t , t)] + d i=1 ∂ 2 p(X t , t) ∂X i ∂X i = -∇ • J,(24)$where the probability current reads J = f (X t , t)p(X t , t) -∇p(X t , t), and we have written the force term in Eq. ( [1](#formula_0)) as a general function f (X t , t) which we will specify in the reverse generative dynamics as well. The FPE is an equation of probability conservation [[7]](#b6). We then define the stochastic entropy as before [[24]](#b23)

$S(t) = -ln p(X t , t),(25)$where we set k B = 1 in our paper. The rate of the stochastic entropy can be derived directly:

$Ṡ(t) = - ∂ t p(X t , t) p(X t , t) - ∇p(X t , t) p(X t , t) Ẋ = - ∂ t p(X t , t) p(X t , t) - f (X t , t)p(X t , t) -J p(X t , t) Ẋ = - ∂ t p(X t , t) p(X t , t) -f (X t , t) • Ẋ + J • Ẋ p(X t , t) . (26$$)$To derive the above equation, we have used the expression of the probability current. The second term in the last equality of Eq. ( [26](#formula_36)) is actually the rate of heat dissipation to the environment, i.e., q(t) = f (X t , t) Ẋ = ṠE (t) (notice that we have set the unit temperature). Then, we can define the total entropy production rate Ṡtot (t) = ṠE (t) + Ṡ(t), and as a result, Ṡtot (t) reads,

$Ṡtot (t) = - ∂ t p(X t , t) p(X t , t) + J • Ẋ pX t , t) .(27)$
## D. Ensemble entropy production rate

We first define the ensemble average of the stochastic entropy as follows [[25]](#b24):

$S(t) ≡ ⟨S(t)⟩ = -p(X t , t) ln p(X t , t)dX t .(28)$The rate of entropy change of the system can be readily expanded by inserting the FPE as follows,

$dS(t) dt = d dt -p (X t , t) ln p (X t , t) dX t = - d i=1 ∂ ln p(X t , t) ∂X i J i dX t ,(29)$where J i is the i-th component of J, and we have used the fact that dX t ∂p(Xt,t) ∂t = 0, or at the boundary, the current vanishes. According to the definition of the probability current J [see its expression below Eq. ( [24](#formula_33))], we get its component

$J i = f i p -p ∂ ln p$∂Xi , and replace ∂ ln p ∂Xi by f i -J i /p, where f i is the i-th component of the high dimensional force f (X t , t). Therefore,

$dS(t) dt = d i=1 -f i J i + J 2 i p(X t , t) dX t , = π -ϕ,(30)$where π ≡ i J 2 i p(Xt,t) dX t is non-negative, and is actually the entropy production rate, the rate at which the total entropy of the system and environment changes; and ϕ ≡ i f i (X t , t)J i dX t denotes the entropy flux into or out of the system (from or to) the environment. The entropy flux can be positive, suggesting reduction of the system entropy (a characteristic of emergence of order). Provided that the dynamics reaches equilibrium, both π and ϕ will vanish, but even if S(t) is stationary, π = ϕ ̸ = 0, which is a key characteristic of nonequilibrium steady states. The above derivations are consistent with those derived in Refs. [[24,](#b23)[25]](#b24), and are applied to GDMs we study in this paper.

## III. NONEQUILIBRIUM PHYSICS OF REVERSE GENERATIVE DYNAMICS A. Backward generative SDE

In this section, we first derive the reverse generative dynamics equation based on the forward diffusion process [Eq. ( [1](#formula_0))]. Then we would give a thorough physics analysis of this backward generative SDE.

We first define the following backward conditional distribution:

$P (X t , t|X t+dt , t + dt) = P (X t+dt , t + dt|X t , t) p(X t+dt , t + dt) p(X t , t),(31)$where the Bayes' rule is used. According to Eq. ( [1](#formula_0)), we have

$X t+dt -X t = f (X t , t)dt + √ 2dtη t$, where η t is an i.i.d. Gaussian random variable of zero mean and unit variance. It is easy to derive that

$P (X t+dt , t + dt|X t , t) ∝ exp - (X t+dt -X t -f (X t , t)dt) 2 4dt . (32$$)$In addition, the probability ratio

$p(X t , t) p(X t+dt , t + dt) = exp [-(ln p(X t+dt , t + dt) -ln p(X t , t))] = exp -dX • ∇ ln p(X t , t) -dt ∂ ln p(X t , t) ∂t ,(33)$where we have done the Taylor expansion of ln p(X + dX, t + dt) in both spatial and temporal dimensions.

Finally, we collect all intermediate results into the Bayes' formula and get

$P (X t , t|X t+dt , t + dt) ∝ exp - (X t+dt -X t -f (X t , t)dt) 2 4dt -dX • ∇ ln p(X t , t) -dt ∂ ln p(X t , t) ∂t , ∝ exp - ∥X t -X t+dt + (f (X t , t) -2∇ ln p(X t , t)) dt∥ 2 4dt ,(34)$where the order of O(dt) is neglected. Equation (34) suggests that the following SDE for the reverse dynamics reads

$Ẋ = f (X t , t) -2∇ ln p(X t , t) + √ 2ξ. (35$$)$This reverse diffusion equation was first proposed in Ref. [[26]](#b25), and can be thought as a non-linear Langevin dynamics, a central subject we shall study in the following sections.

## B. Learning the score function

The gradient of log-likelihood is called the score function in machine learning. It is usually hard to estimate in real data learning, but can be approximated by a neural network whose parameters are trained to minimize the following mean-squared cost function:

$L(θ) = E Xt∼p(Xt,t) ∥s θ (X t , t) -∇ ln p(X t , t)∥ 2 , (36$$)$where s θ represents a function implemented by a neural network parameterized by θ. It is proved in a previous work [[27]](#b26) that the score function can be replaced by ∇ ln P (X t |X 0 ) (in the sense that the expectation over p(X t , t) is considered), which is more convenient to compute as

$∇ ln P (X t |X 0 ) = -Σ -1 (X t -X 0 e -t ) = - 1 1 -e -2t I d (X t -X 0 e -t ) = - 1 √ 1 -e -2t Z t ,(37)$where Σ = Σ t I d , and Z t is a d-dimensional standard Gaussian random variable. Equation (37) can be used to train a neural network in practice. After the score function is learned, the reverse SDE can be used to generate data samples starting from a Gaussian white noise, in a very similar spirit to variational auto-encoder and generative adversarial networks [[1]](#b0).

In our current model, the score function can be computed in an analytic form, which proceeds as follows. We already know that

$p(X t , t) = 1 2 N (µ t , I d ) + 1 2 N (-µ t , I d ) ,(38)$where µ t = µe -t . We get then the score function [[22]](#b21):

$∇ ln p(X t , t) = w +,t (X t )µ +,t + w -,t (X t )µ -,t -X t = w +,t (X t )µ t -(1 -w +,t (X t ))µ t -X t = (2w +,t (X t ) -1)µ t -X t = tanh µ ⊤ t X t µ t -X t ,(39)$where the weight for the positive mean µ +,t is given by w +,t (X t ) =

$1 1+exp ∥X t -µ t ∥ 2 2 - ∥X t +µ t ∥ 2 2 = 1 1+exp(-2µ ⊤ t Xt)$. Note that the score function becomes more complicated but still analytic in the case of non-unit variance of the ground truth Gaussian mixture distribution. We show this complicated expression in Appendix E. Driven by the score function, typical trajectories starting from a standard normal random vectors evolve to target samples of Gaussian mixture data distribution, which is shown as an example in Fig. [1](#). What remains mysterious is the nature of the reverse generative process. In the following, we will address this question in our current Gaussian mixture model by using physics concepts.

## C. Potential and free energy

Next, we follow recent works [[13,](#b12)[16]](#b15) to introduce the potential and free energy of the reverse SDEs. Note that we formulate these concepts in our current model in the general form and prove their equivalence through a statistical inference mapping of the denoising process, and finally demonstrate how to identify phase transitions during the reverse Langevin dynamics based on our derivation.

We first derive a potential function for the reverse dynamics [Eq. ( [35](#formula_49))]. Let t = T -s, then decreasing t is equivalent to increasing s. The reverse SDE can thus be written as

$d Xs = 2∇ ln p( Xs , T -s) -f ( Xs , T -s) ds + √ 2dW = -∇U ( Xs , T -s) + √ 2dW,(40)$where U ( Xs , T -s) is defined as the potential of the reverse dynamics. We have thus the following equality:

$-∇U ( Xs , T -s) = 2∇ ln p( Xs , T -s) -f ( Xs , T -s).(41)$Carrying out an integral of both sides in Eq. ( [64](#formula_87)), we get the expression of the potential as

$U ( Xs , T -s) = -2 ln p( Xs , T -s) + Xs 0 f (Z, T -s)dZ,(42)$where we have neglected all irrelevant constants contributed by an arbitrary lower limit (but not Xs ) of the integral.

In our Gaussian mixture data case, the score function can be exactly computed. Therefore, the potential can be estimated with the following analytic form:

$U = 1 2 X2 s -2 ln cosh Xs • µe -(T -s) .(43)$We reshape the time as T -s = t, and then Xs = X t , leading to the following form in decreasing t:

$U = 1 2 X 2 t -2 ln cosh X t • µe -t .(44)$This analytic expression of the potential can be plotted as a function of time and X t . For an example of onedimensional dynamics, figure [4](#) shows that at some moment t = t S , the symmetry of the potential is broken, producing two local minima, which bears similarity with the spontaneous symmetry breaking in ferromagnetic Ising model when the temperature is lowered down. This spontaneous symmetry breaking is common in other types of unsupervised learning [[18,](#b17)[19]](#b18). The critical time t S is thus determined by the condition ∂ 2 U ∂X 2 | X=0 (t S ) = 0, a qualitative change of convexity at X = 0. This condition leads to the transition condition µ 2 e -2t S = 1 2 for our current model setting. The specific moment t = t S in the reverse dynamics trajectory corresponds to the speciation transition recently identified in Ref. [[14]](#b13) where an overlap dynamics was analyzed.

Next, we turn to the free energy concept. We assume p(X 0 ) as the data distribution, and then according to the forward process, the distribution of X t is expressed as a probability convolution:

$p(X t , t) = N (X t ; X 0 e -t , Σ t I d )p(X 0 )dX 0 . (45$$)$The score function can then be expressed as

$∇ ln p(X t , t) = Σ -1 t dX 0 p(X 0 )X 0 e -t N (X t ; X 0 e -t , Σ t I d ) dX 0 p(X 0 )N (X t ; X 0 e -t , Σ t I d ) -X t = Σ -1 t X 0 e -t Xt -X t .(46)$This form of score function suggests that a statistical inference can be defined below [[15]](#b14): where Σ = Σ t I d . We can thus write down an equivalent Hamiltonian H(X 0 |X t , t) = 1 2 (X t -X 0 e -t ) ⊤ Σ -1 (X t -X 0 e -t ) -ln p(X 0 ), and the partition function reads Z(X t , t) = e -H(X0|Xt,t) dX 0 .

$P (X 0 |X t , t) = P (X t , t|X 0 )p(X 0 ) p(X t , t) = N (X t ; X 0 e -t , Σ t I d )p(X 0 ) p(X t , t) ∝ exp - 1 2 (X t -X 0 e -t ) ⊤ Σ -1 (X t -X 0 e -t ) + ln p(X 0 ) ,(47)$(48)

Note that this is a generalized form of that introduced in Ref. [[16]](#b15). The dynamic state X t during the reverse dynamics plays the role of quenched disorder in traditional spin glass theory [[2]](#b1). The free energy can then be written as F (X t , t) = -1 β ln Z(X t , t), where an equivalent inverse temperature β = Σ -1 (different from that introduced in Ref. [[16]](#b15)). It is straightforward to compute the following free-energy gradient:

$∇F (X t , t) = ∇ - 1 β ln Z(X t , t) = - 1 β ∇Z(X t , t) Z(X t , t) = -X 0 e -t Xt + X t ,(49)$which is exactly the gradient of log-state-likelihood (up to a pre-factor), i.e., ∇F = -1 β ∇ ln p, from which we can finally derive the explicit form of the free energy:

$F = - 1 β ln cosh(X t • µ t ) + 1 2β X 2 t .(50)$It is then convenient to write the driving force in the reverse dynamics [see Eq. ( [40](#formula_57))], i.e., 2∇ ln p -f = -∇ F , and then it is straightforward to derive the following expression for the generalized free energy F :

$F = 2βF + f d Xs ,(51)$which is exactly the potential energy derived before [Eq. ( [44](#formula_61))]. This suggests that, the reverse generative dynamics can be considered as minimizing the generalized free energy in the dynamic-state space, and the generated samples follows the minimum free energy principle. We thus conclude that the speciation transition corresponds to a change of curvature of the free energy landscape. Although our results are derived in the high-dimensional Gaussian mixture context and formulated as a statistical inference, the conclusion is consistent with that derived in a simpler setting through a regularized free energy [[16]](#b15).

We finally discuss the symmetry breaking transition in arbitrary values of data mean µ and variance σ 2 . For simplicity, we focus on one-dimensional example. According to the theoretical analysis in this section, the speciation time t S is determined by t S = 1 2 ln µ 2 + σ 4 -2σ 2 + µ 4 + 1 where the potential starts the spontaneous symmetry breaking, which is obtained by searching for the time when the second derivative of the potential or free energy vanishes (see detailed calculations in Appendix B). The theoretical formula based on a perturbative analysis in Ref. [[14]](#b13) does not apply to our current case where d = 1 and speciation may take place at a small time. The following finding is also not revealed in a recent work [[13]](#b12) where the arbitrary variance was not considered. Hence, in our current setting, we reveal a new type of phase diagram for this simple setting (Fig. [5](#)). More precisely, µ 2 + σ 4 -2σ 2 + µ 4 + 1 = 1 determining a phase boundary between symmetry breaking and symmetry un-breaking. In the symmetry un-breaking phase, we do not have a positive value of t S . Moreover, it is surprising that when the variance is higher than a critical value, we have another phase transition to an unstable symmetry breaking, whose threshold is determined by t U SB = 1 2 ln σ 2 -1 (see details in Appendix B). If the unstable symmetry breaking is triggered, i.e., t U SB ≥ 0, the symmetry breaking profile of double-minima potential is destabilized to an inverted U shape (Fig. [6](#fig_3)). The slope of the potential at X t = ∞ changes discontinuously to a positive value, since at the threshold, the slope is still negative. An immediate result is that the generative sampling fails for the reverse diffusion process. Although we clarify this phase diagram in the simple one-dimensional diffusion process, it is still very interesting to see whether the picture qualitatively holds in high dimensional generative process, which is more relevant in practical applications. We leave this idea in future works.

## D. Fluctuation theorem in the reverse dynamics

We have derived the following reverse SDE [see Eq. ( [35](#formula_49))]:

$dX t = -2 tanh µ ⊤ t X t µ t dt + X t dt + √ 2dW. (52$$)$With increasing time s, we make a definition X(s) = X(t) as before, where s = T -t, and we have the following equivalent SDE:

$d Xs = 2 tanh µ ⊤ T -s Xs µ T -s -Xs ds + √ 2dW. (53$$)$We then use the λ-convention for the following discretization:

$Xs+ds -Xs ds = 2 tanh µ ⊤ T -s-λds {λ Xs+ds + (1 -λ) Xs } µ T -s-λds -(λ Xs+ds + (1 -λ) Xs ) + √ 2η s ,(54)$where η s ∼ N (0, 1 ds I d ). The conditional probability of Xs+ds can be expressed using the noise distribution via the following probability density transform:

$P Xs+ds | Xs = P √ 2η ∂ √ 2η ∂ Xs+ds ∝ exp   - Ẋs -2 tanh µ ⊤ T -s Xs µ T -s + Xs 2 ds 4 -λ∇ • 2 tanh µ ⊤ T -s Xs µ T -s -Xs ,(55)$where

$∂ √ 2η ∂ Xs+ds$represents the Jacobian. Finally, we use the Markovian chain property and get

$P X([T ])| X0 = s P Xs+ds | Xs ∝ e -T 0 ds λ ⊙ ( Ẋ-2 tanh ( µ ⊤ T -s X) µ T -s + X) 2 4 +λ∇•[2 tanh(µ ⊤ T -s X)µT -s -X] .(56)$Given an initial condition, the path probability of a trajectory X([T ]) with time from 0 to T can be represented as

$P ( X([T ])| X0 ) = N exp -A( X([T ])) ,(57)$where N denotes a normalization, and

$A( X([T ])) = T 0 ds λ ⊙    Ẋ -2 tanh µ ⊤ T -s X µ T -s + X 2 4 + λ∇ • [2 tanh µ ⊤ T -s X µ T -s -X]    . (58$$)$Taking a backward process on the same forward trajectory, we have X(t) = X(T -s) = X(s), and thus we have

$Ẋ(t) = Ẋ(T -s) = -Ẋ(s).$The path probability for this reverse version of the forward trajectory can be derived similarly based on the new definition of the state variable. The resulting formula is given below.

$P (X([T ])|X 0 ) = N exp[-A(X([T ]))],(59)$where

$A(X([T ])) = T 0 ds 1-λ ⊙    -Ẋ -2 tanh µ ⊤ T -s X µ T -s + X 2 4 + λ∇ • [2 tanh µ ⊤ T -s X µ T -s -X]    .(60)$The entropy change in the environment is thus calculated as follows,

$∆S E = ln P ( X([T ])| X0 ) P (X([T ])|X 0 ) = T 0 ds 1 2 ⊙ Ẋ • (2 tanh µ ⊤ T -s X µ T -s -X) . (61$$)$The change of the system entropy is calculated as the ratio between the probabilities of the initial and final states. More precisely,

$∆S = ln p( X0 ) p( XT ) = ln    exp -( X0-µe -T ) 2 2 + exp -( X0+µe -T ) 2 2 exp -( XT -µ) 2 2 + exp -( XT +µ) 2 2    .(62)$Combining the above two entropy contributions, we get the total entropy change:

$∆S tot = ∆S E + ∆S = ln   P X([T ]) P [X([T ])]   ,(63)$where X([T ]) and X([T ]) have included the starting point. Therefore, the reverse dynamics also obeys the following integral fluctuation theorem: which suggests that the second law of thermodynamics ⟨∆S tot ⟩ ≥ 0. This integral fluctuation theorem is verified by our experiments in Fig. [7](#fig_4) of one-dimensional example. Detailed contribution of entropy is given in Fig. [8](#fig_5) as well. Compared to the forward process, the system and total entropy changes are biased toward positive values. Experimental details are given in Appendix A.

$e -∆Stot = P X([T ]) e -∆Stot d X([T ]) = P [X([T ])] dX([T ]) = 1,(64)$
## E. Entropy production rate

In this section, we analyze the time-dependent entropy change in both forward and reverse dynamics, focusing on the reverse generative dynamics.

For the ensemble entropy production rate, we consider the following initial distribution:

$p(X 0 ) = 1 2 N µ, σ 2 I d + 1 2 N -µ, σ 2 I d ,(65)$where all components of µ are equal to a positive value µ. This implies that the probability of X t at time t is given by a convolution of the initial distribution and the Gaussian transition kernel:

$p(X t , t) = 1 2 N µ t , (I d -e -2t I d ) + σ 2 I d e -2t + 1 2 N -µ t I d , (I d -e -2t I d ) + σ 2 I d e -2t ,(66)$where µ t = µe -t . Next, we write down the expression of the probability current for both forward and reverse dynamics:

$J = f (X t , t)p(X t , t) -∇p(X t , t),(67)$where f (X t , t) = -X t for the forward dynamics, and 2 tanh µ ⊤ T -s Xs µ T -s -Xs for the reverse dynamics (σ 2 = 1). Note that the forward and backward SDEs share the same state probability distribution p(X t , t) (see Eq. (66), and a proof given in Appendix C). Note also that the probability current of the forward OU process has the same magnitude but opposite direction with the reverse generative SDE (see a proof in Appendix D). Inserting the above definitions, one can estimate the entropy production rate π and the entropy flux ϕ according to Eq. ( [30](#formula_42)). Although our formula applies to the high-dimensional case (the dimensionality can be set to an arbitrary number), we consider a one-dimensional example to analyze the entropy production rate and entropy flux for simplicity. Due to the anti-symmetry properties of probability currents and the identical state probability, we have the same entropy production rate:

$π = π * = X t p(X t , t) + ∂p(Xt,t) ∂Xt 2 p(X t , t) dX t .(68)$We use the superscript * to indicate the reverse process. However, the entropy fluxes are different depending on the specific forms of the drift force. According to Sec. II D, we have the following results:

$ϕ = X t X t p(X t , t) + ∂p(X t , t) ∂X t dX t , ϕ * = -f rev (X t , t) X t p(X t , t) + ∂p(X t , t) ∂X t dX t .(69)$Note that for σ 2 = 1, the drift force for the reverse dynamics is given by f rev (X t , t) = 2 tanh (µ t X t ) µ t -X t . The results for the case of σ 2 ̸ = 1 are derived in Appendix E. As derived in Sec. III C, the speciation time t S = 1 2 ln µ 2 + σ 4 -2σ 2 + µ 4 + 1 for a one-dimensional generative examples of arbitrary values of mean and variance. This corresponds exactly to the first appearance of the inflection point on the potential or free energy curve (see Sec. III C). As shown in Fig. [9](#) for the forward OU process, the entropy production rate drops monotonically toward zero, while the system entropy rate decreases first and then increases toward zero. With increasing time in a reverse process (from noise sample to data sample), the entropy flux is first negative, and after some time step earlier than t S , the flux becomes positive, indicating that the order is generated (Fig. [10](#)). The change rate of system entropy is first positive, and particularly before t S , the system entropy rate starts to decrease. In particular, the rate drops sharply as the starting point (t = 0) is approached, and at t = 0 one real sample is generated. Meanwhile, the entropy flux increases sharply as well, indicating a generative process in sampling the target data space.

## F. Glass transition

It was recently argued that at a time less than t S (in the reverse dynamics), a collapse transition would take place, i.e., the trajectory condenses onto one single sample of data distribution [[14]](#b13), which further clarified that the essence is the glass transition. This recent analysis relies on the empirical distribution of time-dependent state (e.g., X t here), where n data points are selected as an initial condition for the diffusion dynamics. Here, we re-interpret this picture using the Franz-Parisi potential, a powerful statistical physics tool to characterize the geometric structure of glassy energy landscape [[20,](#b19)[28]](#b27), as in our current case, the score can be analytically computed, and as shown before, the dynamical state at some specific moment during the reverse process plays a role of quenched disorder for the statistical inference of the denoising process.

We start from the statistical inference defined by Eq. ( [47](#formula_65)). We select an equilibrium reference configuration, namely X ′ 0 , and consider a restricted Boltzmann measure

$p(X 0 |X t , X ′ 0 ) = 1 Z(X t , X ′ 0 , q) exp [-H(X 0 |X t , t)] δ [q -d(X 0 , X ′ 0 )] ,(70)$where d(X 0 , X ′ 0 ) denotes the Euclidean distance of two high dimensional vectors. This restricted Boltzmann measure can be transformed to a soft constraint with a coupling filed ϵ. Therefore, we shall focus on the following constrained free energy:

$F (ϵ) = - dX ′ 0 p(X ′ 0 |X t ) ln dX 0 e -H(X0|Xt,t)+ϵd(X0,X ′ 0 ) Xt ,(71)$where X t ∼ p(X t , t) derived before, and acts as quenched disorder as in usual spin glass theory [[29]](#b28). We thus define the Franz-Parisi potential V (q) = ⟨-ln Z(X t , X ′ 0 , q)⟩, where ⟨•⟩ means the average over X ′ 0 and X t . The Franz-Parisi potential V (q) is obtained by a Legendre transform of F (ϵ) which is given below.

$V (q) = min ϵ F (ϵ) + ϵq,(72)$where q is determined by ∂F ∂ϵ = -q. The Franz-Parisi potential develops the second minimum if a dynamical glass transition occurs, implying that the ergodicity breaks. Furthermore, if the second minimum reaches the same height with the first minimum, a static glass transition occurs with vanishing complexity [[28]](#b27). We next show an example of one-dimensional Franz-Parisi potential which can be computed directly. The Hamiltonian reads,

$H(X 0 |X t , t) = (X t -X 0 e -t ) 2 2(1 -e -2t ) -ln 1 2 √ 2πσ 2 exp - (X 0 -µ) 2 2σ 2 + 1 2 √ 2πσ 2 exp - (X 0 + µ) 2 2σ 2 . (73$$)$The constrained partition function can be written as follows,

$Z(X t , X 0 ′ , q, t) = exp(-H(X 0 |X t , t))δ(q -(X 0 -X 0 ′ ) 2 )dX 0 .(74)$Therefore, the Franz-Parisi potential reads,

$V (q, t) = - p X 0 ′ | X t , t ln Z(X t , X 0 ′ , q, t)dX 0 ′ Xt .(75)$To proceed, we have to use the following property of Dirac delta function:

$δ(q -(X 0 -X 0 ′ ) 2 ) = 1 2 √ q δ(X 0 -X 0 ′ - √ q) + δ(X 0 -X 0 ′ + √ q) .(76)$Then, we can simplify the constrained partition function as follows.

Z

$(X t , X 0 ′ , q, t) = exp(-H(X 0 |X t , t))δ(q -(X 0 -X 0 ′ ) 2 )dX 0 = 1 2 √ q exp -H(X 0 ′ + √ q|X t , t) + 1 2 √ q exp -H(X 0 ′ - √ q|X t , t) .(77)$We already know the joint distribution p(X ′ 0 , X t , t) as follows,

$p X 0 ′ , X t , t = p(X t , t|X ′ 0 )p(X ′ 0 ) = exp -(Xt-X0 ′ e -t ) 2 2(1-e -2t ) 2π(1 -e -2t ) 1 2 √ 2πσ 2 exp - (X 0 ′ -µ) 2 2σ 2 + 1 2 √ 2πσ 2 exp - (X 0 ′ + µ) 2 2σ 2 = 1 4π σ 2 (1 -e -2t ) exp - (X t -X 0 ′ e -t ) 2 2(1 -e -2t ) exp - (X 0 ′ -µ) 2 2σ 2 + exp - (X 0 ′ + µ) 2 2σ 2 .(78)$It is therefore straightforward to use Monte-Carlo method to estimate the Franz-Parisi potential. We first generate T pairs of (X ′ 0 , X t ) according to the above joint distribution. Then the potential is estimated in a simple way.

$V (q, t) = - 1 T T ℓ=1 ln Z(X t,ℓ , X ′ 0,ℓ , q, t).(79)$The Franz-Parisi potential is plotted as a function of Euclidean distance q for the one-dimensional example in Fig. [11](#). When the time decreases from the starting point of the reverse diffusion, the potential develops an inflection point where the second derivative vanishes at some moment. As the time approaches the starting time (t = 0), more inflection points appear, due to fragmented (inferred) data space given the current X t value. This demonstrates that the reverse trajectory will collapse onto a single data sample. The complicated yet tractable computation of the high dimensional case is left for future works, where order parameters characterizing the transition would emerge.

$(b) (c) (d) (e) (f) (g) (h) (i)(a)$FIG. [11](#): The Franz-Parisi potential and its first and second derivatives for the case of µ = 1 and σ 2 = 0.1. tS = 0.42622. Three time steps are considered, i.e., t = 0.42, t = 0.015, and t = 0.01.

It is then verified that the gradient of the log-likelihood is given by

$∇ ln p(X t , t) = e t -e t X t + µ tanh e t µXt -1+e 2t +σ 2 -1 + e 2t + σ 2 . (B2)$A step-by-step calculation is presented in Appendix E. Therefore, the first derivative of the potential is given by

$∂U (X t , t) ∂X t = -X t + 2X t 1 + e -2t (σ 2 -1) - 2µe t tanh µe t Xt e 2t +σ 2 -1 e 2t + σ 2 -1 . (B3)$The second derivative is then given as follows:

$∂ 2 U (X t , t) ∂X 2 t = -1 + 2 1 + e -2t (σ 2 -1) -2 µe t e 2t + σ 2 -1 2 1 -tanh 2 µe t X t e 2t + σ 2 -1 . (B4)$Finally, we check that there are four solutions to ∂ 2 U (Xt,t)

$∂X 2 t | Xt=0 = 0.$Only one of them gives a real value of time, i.e., t S = 1 2 ln µ 2 + σ 4 -2σ 2 + µ 4 + 1 . It is surprising that the symmetry breaking can become unstable if the sign of the first derivative of the potential at X t → ∞ changes from negative to positive. This is determined by the following limit:

$lim Xt→∞ ∂U (X t , t) ∂X t = - 2e t µ C(σ, t) + lim Xt→∞ 2X t e -2t C(σ, t) -X t ,(B5)$where we have used Eq. (B3), and C(σ, t) = e 2t + σ 2 -1. It is clear that when C(σ, t) < 2e 2t , the potential shape will change from the one of double minima to an inverted U shape. At the critical line C(σ, t) = 2e 2t , the potential shape is still of the double-minima type, because of its negative slope at X t = ∞ (the exact value is given by -µe -t ). Therefore, we determine the critical time as t U SB = 1 2 ln σ 2 -1 , which is independent of µ. Moreover, the transition is of the first order.

Appendix C: Proof of the same state probability for both forward and reverse dynamics

The forward OU process is given by

$dX t = f (X t , t)dt + √ 2dW, (C1$$)$where dW is the Wiener process. The solution of the corresponding FPE is specified by p(X t , t). The reverse generative diffusion dynamics is described by (C4)

Noticing that Xs = X(T -t) where t is the forward time, we can conclude that the last equation in Eq. (C4) is exactly the same with the forward FPE. Therefore, both processes bear the same state probability distribution, as intuitively expected.

p(X t , t) is obtained from Eq. (66) by setting d = 1. The entropy production rates are equal in both forward and reverse generative processes. They can be estimated below:

$π = π * = X t p$(X t , t) + ∂p(Xt,t) ∂Xt 2 p(X t , t) dX t = X 2 t p(X t , t)dX t + 2X t ∂p(X t , t) ∂X t dX t + 1 p(X t , t) ∂p(X t , t) ∂X t 2 dX t = X 2 t p -2 -∂ 2 ln p(X t , t) ∂X t 2 p , (E3) where ∂ 2 ln p(Xt,t) ∂Xt 2 p = ∂ 2 ln p(Xt,t) ∂Xt 2 p(X t , t)dX t . The entropy flux for the reverse generative process can be estimated as follows, ϕ * = X t p(X t , t) + ∂p(X t , t) ∂X t f * (X t , t)dX t = ⟨f * (X t , t)X t ⟩ p + f * (X t , t) ∂p(X t , t) ∂X t dX t = ⟨f * (X t , t)X t ⟩ p -∂f * (X t , t) ∂X t p = 2 ∂ ln p(X t , t) ∂X t X t p + X t 2 p -2 ∂ 2 ln p(X t , t) ∂X t 2 p -1 = -3 + X t 2 p -2 ∂ 2 ln p(X t , t) ∂X t 2 p . (E4)

where * indicates the reverse process, and f * (X t , t) = 2 ∂ ln p(Xt,t) ∂Xt + X t for the reverse process. The system entropy rate for the forward OU process can now be written as follows:

$Ṡ = π -ϕ = -1 - ∂ 2 ln p(X t , t) ∂X t 2 p . (E5$$)$The same rate for the reverse process is given as follows:

$Ṡ * = π * -ϕ * = 1 + ∂ 2 ln p(X t , t) ∂X t 2 p . (E6$$)$We conclude that Ṡ and Ṡ * are anti-symmetric quantities.

To explicitly compute the above physical quantities, we need to compute the following expectations. The first one is the second moment of X t , which reads X 2 t p = µ 2 e -2t + 1 -e -2t + σ 2 e -2t . (E7)

The second one ∂ 2 ln p(Xt,t) ∂Xt 2 p is derived below using the score function in Eq. (E1). 

## (E9)

In the case of σ 2 = 1, the above formulas reduce to the following simple results: ϕ = µ 2 e -2t , π = π * = e -2t µ 2 tanh 2 (e -t µX t ) p , ϕ * = -µ 2 e -2t + 2e -2t µ 2 tanh 2 (e -t µX t ) p .

## (E10)

It is also interesting to show that ⟨tanh(µ t X t )µ t X t ⟩ = µ 2 t (µ t = µe -t in one dimension), which holds only for σ 2 = 1.

![FIG.2: Ensemble average of e -∆S tot as a function of the number of trajectories used to do the average, simulated by solving the forward OU process. µ = 1, and other parameters are detailed in Appendix A.]()

![FIG. 3: Normalized histogram of entropy changes estimated from 10 000 trajectories for the forward OU process. µ = 1, and other parameters are detailed in Appendix A. (a) Statistics of system entropy change. (b) Statistics of environment entropy change. (c) Statistics of the total entropy change. (d) Statistics of e -∆S tot .]()

![FIG.4: Evolution of potential energy (µ = 1) in one dimension. The speciation transition time is given by tS ≈ 0.35.]()

![FIG. 6: Typical profiles of potential in the reverse generative process corresponding to different phases in Fig. 5. (a) Stable symmetry breaking. (b) Symmetry un-breaking. (c) Unstable symmetry breaking.]()

![FIG. 7: Ensemble average of e -∆S tot as a function of the number of trajectories used to estimate the average, simulated by solving the backward SDE. µ = 1, and other parameters are detailed in Appendix A.]()

![FIG. 8: Normalized histogram of entropy changes estimated from 10 000 Trajectories for the reverse generative process. µ = 1, and other parameters are detailed in Appendix A. (a) Statistics of system entropy change. (b) Statistics of environment entropy change. (c) Statistics of total entropy change. (d) Statistics of e -∆S tot .]()

![FIG. 9: Entropy production rate and flux (µ = 1, σ 2 = 0.4). The results are plotted for the forward OU process from time 0 to 3.]()

![Xs = 2∇ ln p( Xs , T -s) -f ( Xs , T -s) ds + √ 2dW, (C2)where s is in an increasing order. The corresponding FPE is given below:∂P ( Xs , s) ∂s = -∇ • 2∇ ln p( Xs , T -s) -f ( Xs , T -s) P ( Xs , s) ( Xs , s) ∂ Xi ∂ Xi . (C3)We then replace P ( Xs , s) by p( Xs , T -s), and keep others unchanged, obtaining the following result:∂p( Xs , T -s) ∂s = -∇ • 2∇ ln p( Xs , T -s) -f ( Xs , T -s) p( Xs , T -s) p( Xs , T -s) ∂ Xi ∂ Xi = ∇ • [f ( Xs , T -s)p( Xs , T -s)] -d i=1 ∂ 2 p( Xs , T -s) ∂ Xi ∂ Xi ⇒ ∂p( Xs , T -s) ∂(T -s) = -∇ • [f ( Xs , T-s)p( Xs , T -s)] + d i=1 ∂ 2 p( Xs , T -s) ∂ Xi ∂ Xi .]()

![ln p(X t , t-2t + e -2t σ 2 ∂ tanh e -t µXt 1-e -2t +e -2t σ 2 (e -t µ) -X t ∂X t p = 1 1 -e -2t + e -2t σ 2 1 -tanh 2 e -t µX t 1 -e -2t + e -2t σ 2 e -2t µ 2 1 -e -2t + e -2t σ 2 -1 p -e -2t + e -2t σ 2 ) 2 -e -2t µ 2 tanh 2 ( e -t µXt 1-e -2t +e -2t σ 2 ) p (1 -e -2t + e -2t σ 2 )-e -2t + e -2t σ 2 .(E8)Inserting these two expression into the entropy flux and production rate, we can get the final general results forσ 2 ̸ = 1. ϕ = µ 2 e -2t -e -2t + σ 2 e -2t , π = π * = µ 2 e -2t -e -2t + σ 2 e -2t -1 -e -2t µ e -2t + e -2t σ 2 ) e -2t +e -2t σ 2 ) p e -2t + e -2t σ 2 ) -e -2t + e -2t σ 2 , ϕ * = µ 2 e -2t -e -2t + σ 2 e -2t -2e -2t + e -2t σ 2 )e -2t +e -2t σ 2 ) p e -2t + e -2t σ 2 )-e -2t + e -2t σ 2 .]()

