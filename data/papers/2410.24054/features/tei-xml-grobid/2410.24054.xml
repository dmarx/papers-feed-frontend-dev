<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EigenVI: score-based variational inference with orthogonal function expansions</title>
				<funder ref="#_Pzw3mHN #_myCN3cj">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_S6yTCsa">
					<orgName type="full">Simons Foundation</orgName>
				</funder>
				<funder ref="#_JDPhGrA #_BZmJfUy">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Diana</forename><surname>Cai</surname></persName>
							<email>dcai@flatironinstitute.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Flatiron Institute</orgName>
								<orgName type="department" key="dep2">Flatiron Institute</orgName>
								<orgName type="department" key="dep3">Flatiron Institute</orgName>
								<orgName type="department" key="dep4">Flatiron Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chirag</forename><surname>Modi</surname></persName>
							<email>cmodi@flatironinstitute.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Flatiron Institute</orgName>
								<orgName type="department" key="dep2">Flatiron Institute</orgName>
								<orgName type="department" key="dep3">Flatiron Institute</orgName>
								<orgName type="department" key="dep4">Flatiron Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><forename type="middle">C</forename><surname>Margossian</surname></persName>
							<email>cmargossian@flatironinstitute.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Flatiron Institute</orgName>
								<orgName type="department" key="dep2">Flatiron Institute</orgName>
								<orgName type="department" key="dep3">Flatiron Institute</orgName>
								<orgName type="department" key="dep4">Flatiron Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Gower</surname></persName>
							<email>rgower@flatironinstitute.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Flatiron Institute</orgName>
								<orgName type="department" key="dep2">Flatiron Institute</orgName>
								<orgName type="department" key="dep3">Flatiron Institute</orgName>
								<orgName type="department" key="dep4">Flatiron Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
							<email>david.blei@columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Flatiron Institute</orgName>
								<orgName type="department" key="dep2">Flatiron Institute</orgName>
								<orgName type="department" key="dep3">Flatiron Institute</orgName>
								<orgName type="department" key="dep4">Flatiron Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
							<email>lsaul@flatironinstitute.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Flatiron Institute</orgName>
								<orgName type="department" key="dep2">Flatiron Institute</orgName>
								<orgName type="department" key="dep3">Flatiron Institute</orgName>
								<orgName type="department" key="dep4">Flatiron Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EigenVI: score-based variational inference with orthogonal function expansions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7E3C7EFEA55C58B4F84BB1C2916BCD22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop EigenVI, an eigenvalue-based approach for black-box variational inference (BBVI). EigenVI constructs its variational approximations from orthogonal function expansions. For distributions over R D , the lowest order term in these expansions provides a Gaussian variational approximation, while higher-order terms provide a systematic way to model non-Gaussianity. These approximations are flexible enough to model complex distributions (multimodal, asymmetric), but they are simple enough that one can calculate their low-order moments and draw samples from them. EigenVI can also model other types of random variables (e.g., nonnegative, bounded) by constructing variational approximations from different families of orthogonal functions. Within these families, EigenVI computes the variational approximation that best matches the score function of the target distribution by minimizing a stochastic estimate of the Fisher divergence. Notably, this optimization reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps the iterative gradient-based optimizations that are required for many other BBVI algorithms. (Gradient-based methods can be sensitive to learning rates, termination criteria, and other tunable hyperparameters.) We use EigenVI to approximate a variety of target distributions, including a benchmark suite of Bayesian models from posteriordb. On these distributions, we find that EigenVI is more accurate than existing methods for Gaussian BBVI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic modeling is a cornerstone of modern data analysis, uncertainty quantification, and decision making. A key challenge of probabilistic inference is computing a target distribution of interest; for instance, in Bayesian modeling, the goal is to compute a posterior distribution, which is often intractable. Variational inference (VI) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">45]</ref> is a popular method for scalable probabilistic inference that has worked across a range of applications. The idea behind VI is to approximate the target distribution by the closest member of some tractable family.</p><p>One major focus of research is to develop black-box algorithms for variational inference <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>. Algorithms for black-box variational inference (BBVI) can be used to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). approximate any target distribution that is differentiable and computable up to some multiplicative (normalizing) constant; as such, they are extremely flexible. These algorithms have been widely implemented in popular probabilistic programming languages, and they are part of the modern toolbox for practitioners in computational statistics and data analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Traditionally, the variational approximations in BBVI are optimized by minimizing the Kullback-Leibler (KL) divergence between the variational family and the target (equivalently, maximizing the ELBO). This strategy is powerful and scalable, but it relies on stochastic gradient descent (SGD), which can be difficult to tune <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b50">51]</ref>. These difficulties can be acute even for Gaussian variational approximations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40]</ref>, particularly if these approximations employ full covariance matrices.</p><p>More recently, researchers have proposed algorithms for Gaussian BBVI that do not require the use of SGD <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>. Instead of minimizing the KL divergence, these methods aim to match the scores, or the gradients of the log densities, between the variational distribution and the target density. These methods exploit the special form of Gaussian distributions to derive closed-form proximal point updates for score-matching. These updates are as inexpensive as SGD, but not as brittle. They show that score-based BBVI can be applied in an elegant way to Gaussian variational families.</p><p>In this paper, we show that score-based BBVI also yields simple, closed-form updates for a much broader family of variational approximations. Specifically, we propose a new class of variational families constructed from orthogonal function expansions and inspired by solutions to the Schrödinger equation in quantum mechanics. These families are expressive enough to parameterize a wide range of target distributions; at the same time, the distributions in these families are sufficiently tractable that one can calculate low-order moments and draw samples from them. In this paper, we mostly use orthogonal function expansions to construct distributions supported on R D ; in this case, the lowest-order term in the expansion is sufficient to model Gaussian behavior, while higher-order terms account for increasing amounts of non-Gaussianity. More generally, we also show how different basis sets of orthogonal functions can be used to construct variational families over other spaces.</p><p>To optimize over a variational family from this class, we minimize an estimate of the Fisher divergence, which measures the scores of the variational distribution against those of the target distribution. We show that this optimization reduces to a minimum eigenvalue problem, thus avoiding the need for gradient-based methods. For this reason, we call our approach EigenVI.</p><p>We study EigenVI with a variational family constructed from weighted Hermite polynomials. We first demonstrate the expressiveness of this family on a variety of multimodal, asymmetric, and heavytailed distributions. We then use EigenVI to approximate a diverse collection of non-Gaussian target distributions from posteriordb <ref type="bibr" target="#b34">[35]</ref>, a benchmark suite of Bayesian hierarchical models. On these problems, EigenVI provides more accurate posterior approximations than leading implementations of Gaussian BBVI based on KL minimization and score-matching.</p><p>The organization of this paper is as follows. In Section 2 we introduce the variational families that arise from orthgonal function expansions, and we show how score-matching in these families reduces to an eigenvalue problem. In Section 3 we review the literature related to EigenVI. In Section 4, we evaluate EigenVI on a variety of synthetic and real-data targets. Finally, in Section 5, we discuss limitations and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Score-based variational inference with orthogonal function expansions</head><p>In this section we use orthogonal function expansions to develop new variational families for approximate probabilistic inference. In Section 2.1, we review the basic properties of these expansions. In Section 2.2, we introduce a score-based divergence for VI with these families; notably, for this divergence, the optimization for VI reduces to an eigenvalue problem. Finally in Section 2.3, we consider how to use these variational approximations for unstandardized distributions; in these settings we must carefully manage the trade-off between expressiveness and computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Orthogonal function expansions</head><p>Let Z ⊆ R D denote the support of the target distribution p. Suppose that there exists a complete set of orthonormal basis functions {ϕ k (z)} ∞ k=1 on this set. By complete, we mean that any sufficiently wellbehaved function f : Z → R can be approximated, to arbitrary accuracy, by a particular weighted </p><formula xml:id="formula_0">(•) z ∈ [-1, 1] Legendre polynomials {1, z, 3z 2 -1, 5z 3 -3z, . . .} z = e iθ ∈ S 1 Fourier basis {1, cos θ, sin θ, cos 2θ, sin 2θ, . . .} z ∈ [0, ∞) weighted Laguerre polynomials e -z 2 {1, 1-z, z 2 -4z +2, . . .} z ∈ R</formula><p>weighted Hermite polynomials e -z 2 4 {1, z, (z 2 -1), (z 3 -3z), . . .} sum of these basis functions, and by orthonormal, we mean that the basis functions satisfy</p><formula xml:id="formula_1">ϕ k (z)ϕ k ′ (z) dz = 1 if k = k ′ , 0 otherwise,<label>(1)</label></formula><p>where the integral is over Z. Define the K th -order variational family Q K to be the set containing all distributions of the form</p><formula xml:id="formula_2">q(z) = K k=1 α k ϕ k (z) 2 where K k=1 α 2 k = 1,<label>(2)</label></formula><p>and where α k ∈ R for k = 1, . . . , K are the parameters of the family Q K . In words, Q K contains all distributions that can be obtained by taking weighted sums of the first K basis functions and then squaring the result.</p><p>Eq. 2 involves a squaring operation, a sum-of-squares constraint, and a weighted sum. The squaring operation ensures that the density functions in Q K are nonnegative (i.e., with q(z) ≥ 0 for all z ∈ Z), while the sum-of-squares constraint ensures that they are normalized:</p><formula xml:id="formula_3">q(z) dz = K k=1 α k ϕ k (z) 2 dz = K k,k ′ =1 α k α k ′ ϕ k (z)ϕ k ′ (z) dz = K k=1 α 2 k = 1.<label>(3)</label></formula><p>The weighted sum in Eq. 2 bears a superficial similarity to a mixture model, but note that neither the basis functions ϕ k (z) nor the weights α k in Eq. 2 are constrained to be nonnegative. Distributions of this form arise naturally in physics from the quantum-mechanical wave functions that satisfy Schrödinger's equation <ref type="bibr" target="#b15">[16]</ref>. In that setting, though, it is typical to consider complex-valued weights and basis functions, whereas here we only consider real-valued ones.</p><p>The simplest examples of orthogonal function expansions arise in one dimension. For example, functions on the interval [-1, 1] can be represented as weighted sums of Legendre polynomials, while functions on the unit circle can be represented by Fourier series of sines and cosines; see Table <ref type="table" target="#tab_0">1</ref>. Distributions on unbounded intervals can also be represented in this way. On the real line, for example, we may consider approximations of the form in Eq. 2 where</p><formula xml:id="formula_4">ϕ k+1 (z) = √ 2πk! -1 2 e -1 2 z 2 1 2 H k (z),<label>(4)</label></formula><p>and H k (z) are the probabilist's Hermite polynomials given by</p><formula xml:id="formula_5">H k (z) = (-1) k e z 2 2 d k dz k e -z 2 2 . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Note how the lowest-order basis function ϕ 1 (z) in this family gives rise (upon squaring) to a Gaussian distribution with zero mean and unit variance.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> shows how various multimodal distributions with one-dimensional support can be approximated by computing weighted sums of basis functions and squaring their result. We emphasize that the more basis functions in the sum, the better the approximation.</p><p>Orthogonal function expansions in one dimension are also important because their Cartesian products can be used to generate orthogonal function expansions in higher dimensions. For example, we can  2 and Table 1. approximate distributions over (say) R 3 by</p><formula xml:id="formula_7">q(z 1 , z 2 , z 3 ) =   K1 i=1 K2 j=1 K3 k=1 β ijk ϕ i (z 1 )ϕ j (z 2 )ϕ k (z 3 )   2 where ijk β 2 ijk = 1,<label>(6)</label></formula><p>where β ijk ∈ R now parametrize the family. Note that there are a total K 1 K 2 K 3 parameters in the above expansion, so that this method of Cartesian products does not scale well to high dimensions if multiple basis functions are used per dimension. Note that the same strategy can also be used for random variables of mixed type: for example, from Table <ref type="table" target="#tab_0">1</ref>, we can create a variational family of distributions over R×[-1, 1]×[0, ∞) from the Cartesian product of orthogonal function expansions involving Hermite, Legendre, and Laguerre polynomials.</p><p>As shown in Figure <ref type="figure" target="#fig_1">1</ref>, the approximating distributions from K th -order expansions can model the presence of multiple modes as well as many types of asymmetry, and this expressiveness also extends to higher dimensions. Nevertheless, it remains tractable to sample from these distributions and even to calculate (analytically) their low-order moments, as we show in Appendices A and B.</p><p>For concreteness, consider the distribution over R 3 in Eq. 6. The marginal distribution q(z 1 ) is</p><formula xml:id="formula_8">q(z 1 ) = q(z 1 , z 2 , z 3 ) dz 2 dz 3 = ii ′   jk β ijk β i ′ jk   ϕ i (z 1 )ϕ i ′ (z 1 ),<label>(7)</label></formula><p>and from this expression, moments such as</p><p>E[z 1 ] and Var[z 1 ] can be calculated by evaluating integrals involving the elementary functions in Table 1. (In practice, these integrals are further simplified by recursion relations that relate basis functions of different orders; we demonstrate how to compute the first two moments for the normalized Hermite family in Eqs. B.19 and B.22.)</p><p>To generate samples {z (t) }, each dimension is sampled as follows: we draw z (t)</p><p>1 ∼ q(z 1 ) by computing the cumulative distribution function (CDF) of this marginal distribution and then numerically inverting this CDF. Finally, extending these ideas, we can calculate higher-order moments and obtain joint samples via the nested draws</p><formula xml:id="formula_9">z (t) 1 ∼ q(z 1 ), z (t) 2 ∼ q(z 2 | z 1 ), z (t) 3 ∼ q(z 3 | z 1 , z 2 ).<label>(8)</label></formula><p>The overall complexity of these procedures scales no worse than quadratically in the number of basis functions in the expansion. These extensions are discussed further in Appendices A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EigenVI</head><p>In variational inference, we posit a parameterized family of approximating distributions and then compute the particular approximation in this family that is closest to a target distribution of interest. Eq. 2 constructs a variational family Q K from the orthogonal functions {ϕ k (z)} K k=1 whose variational parameters are the weights {α k } K k=1 . We now derive EigenVI, a method to find q ∈ Q K that is close to the target distribution p(z).</p><p>We first define the measure of closeness that we will minimize. EigenVI measures the quality of an approximate density by the Fisher divergence <ref type="bibr" target="#b17">[18]</ref>,</p><formula xml:id="formula_10">D(q, p) = ∥∇ log q(z) -∇ log p(z)∥ 2 q(z)dz,<label>(9)</label></formula><p>where ∇ log q(z) and ∇ log p(z) are the score functions of the variational approximation and target, respectively. Suppose that q and p have the same support; then the Fisher divergence vanishes if and only if the scores of q and p are everywhere equal.</p><p>Though p is, by assumption, intractable to compute, in many applications it is possible to efficiently compute the score ∇ log p at any point z ∈ Z. For example, in Bayesian models the score of the target posterior is equal to the gradient of the log joint. This observation is the main motivation for score-based methods in probabilistic modeling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48</ref>].</p><p>Here we seek the q ∈ Q K that minimizes D(q, p). But now a challenge arises: it is generally difficult to evaluate the integral for D(q, p) in Eq. 9, let alone to minimize it as a function of q. While it is possible to sample from the distribution q, it is not straightforward to simultaneously sample from q and optimize over the variational parameters {α k } K k=1 in terms of which it is defined. Instead, we construct an unbiased estimator of D(q, p) by importance sampling, which also decouples the sampling distribution from the optimization. Let {z 1 , z 2 , . . . z B } denote a batch of B samples drawn from some proposal distribution π on Z. From these samples we can form the unbiased estimator</p><formula xml:id="formula_11">D π (q, p) = B b=1 q(z b ) π(z b ) ∇ log q(z b ) -∇ log p(z b ) 2 . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>This estimator should be accurate for appropriately broad proposal distributions and for sufficiently large batch sizes. We can therefore attempt to minimize Eq. 10 in place of Eq. 9.</p><p>Now we show that the minimization of Eq. 10 over q ∈ Q K simplifies to a minimum eigenvalue problem for the weights {α k } K k=1 . To obtain the eigenvalue problem, we substitute the orthogonal function expansion in Eq. 2 into Eq. 10 for the unbiased estimator of D(q, p). As an intermediate step, we differentiate Eq. 2 to obtain the scores</p><formula xml:id="formula_13">∇ log q(z b ) = 2 k α k ∇ϕ k (z b ) k α k ϕ k (z b ) . (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>Further substitution of the scores provides the key result behind our approach: the unbiased estimator in Eq. 10 is a simple quadratic form in the weights α := [α 1 , . . . , α K ] ⊤ of the orthogonal function expansion,</p><formula xml:id="formula_15">D π (q, p) = α ⊤ M α,<label>(12)</label></formula><p>where the coefficients of the quadratic form are given by</p><formula xml:id="formula_16">M jk = B b=1 1 π(z b ) 2∇ϕ j (z b ) -ϕ j (z b )∇ log p(z b ) • 2∇ϕ k (z b ) -ϕ k (z b )∇ log p(z b ) . (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>Note that the elements of the K ×K symmetric matrix M capture all of the dependence on the batch of samples {z b } B b=1 , the scores of p and q at these samples, and the choice of the family of orthogonal functions. Next we minimize the quadratic form in Eq. 12 subject to the sum-of-squares constraint</p><formula xml:id="formula_18">k α 2 k = 1 in Eq. 2.</formula><p>In this way we obtain the eigenvalue problem <ref type="bibr" target="#b7">[8]</ref> min</p><formula xml:id="formula_19">q∈Q K D π (q, p) = min ∥α∥=1 α ⊤ M α =: λ min (M ),<label>(14)</label></formula><p>where λ min (M ) is the minimal eigenvalue of M , and the optimal weights are given (up to an arbitrary sign) by its corresponding eigenvector; see Appendix C for a proof. EigenVI solves Eq. 14.</p><p>We note that the eigenvalue problem in EigenVI arises from the curious alignment of three particular choices-namely, (i) the choice of variational family (based on orthogonal function expansions), (ii) the choice of divergence (based on score-matching), and (iii) the choice of estimator for the divergence (based on importance sampling). The simplicity of this eigenvalue problem stands in contrast to the many heuristics of gradient-based optimizations-involving learning rates, terminating criteria, and perhaps other algorithmic hyperparameters-that are typically required for ELBO-based BBVI <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. But EigenVI is also not entirely free of heuristics; to compute the estimator in Eq. 10 we must also specify the proposal distribution π and the number of samples B; see Appendix D for a discussion.</p><p>The size of the eigenvalue problem in EigenVI is equal to the number of basis functions K in the orthogonal function expansion of Eq. 2. The eigenvalue problem also generalizes to orthogonal function expansions that are formed from Cartesian products of one-dimensional families, but in this case, if multiple basis functions are used per dimension, then the overall basis size grows exponentially in the dimensionality. Thus, for example, the eigenvalue problem would be of size K 1 K 2 K 3 for the approximation in Eq. 6, as can be seen by "flattening" the tensor of weights β in Eq. 6 into the vector of weights α = vec(β) in Eq. 2. Finally, we note that EigenVI only needs to compute the minimal eigenvector of M in Eq. 14, and therefore it can benefit from specialized routines that are much less expensive than a full diagonalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">EigenVI in R D : the Hermite family and standardization</head><p>We now discuss the specific case of EigenVI for Z = R D with the Hermite-based variational family in Eq. 4. For this case, we propose a transformation of the domain that serves to precondition or standardize the target distribution before applying EigenVI. While this standardization is not required to use EigenVI, it helps to reduce the number of basis functions needed to approximate the target, leading to a more computationally efficient procedure. It also suggests natural default choices for the proposal distribution π in Eq. 10.</p><p>Recall that the eigenvalue problem grows linearly in size with the number of basis functions. Before applying EigenVI, our goal is therefore to transform the domain in a way that reduces the number of basis functions needed for a good approximation. To meet this goal for distributions over R D , we observe that the lowest-order basis function of the Hermite family in Eq. 4 yields (upon squaring) a standard multivariate Gaussian, with zero mean and unit covariance. Intuitively, we might expect the approximation of EigenVI to require fewer basis functions if the statistics of the target distribution nearly match those of this lowest-order basis function. The goal of standardization is to achieve this match, to whatever extent possible, by a suitable transformation of the underlying domain. Having done so, EigenVI in R D can then be viewed as a systematic framework to model non-Gaussian effects via a small number of higher-order terms in its orthogonal function expansion.</p><p>Concretely, we consider a linear transformation of the domain:</p><formula xml:id="formula_20">z = Σ -1 2 (z-µ),<label>(15)</label></formula><p>where µ and Σ are estimates of the mean and covariance obtained from some other algorithm (e.g., a Laplace approximation, Gaussian variational inference, Monte Carlo, or domain-specific knowledge).</p><p>We then apply the EigenVI to fit a Kth-order variational approximation q(z) to the target distribution p(z) that is induced by this transformation; afterwards, we reverse the change-of-variables to obtain the final approximation to p(z), i.e.,</p><formula xml:id="formula_21">q(z) = q(z)|Σ| -1/2 . (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>Figure <ref type="figure">2</ref> shows why it is more difficult to approximate distributions that are badly centered or poorly scaled. The left panel shows the effect of translating a standard Gaussian away from the origin and shrinking its variance; note how a comparable approximation to the uncentered Gaussian now requires a 16th-order expansion. On the other hand, after standardization, the target can be perfectly fitted by the base distribution in the orthogonal family of reweighted Hermite polynomials. The right panel shows the similar effect of translating the mixture distribution in Figure <ref type="figure" target="#fig_1">1</ref> (right panel); comparing these panels, we see that twice as many basis functions (K = 14 versus K = 7) are required to provide a comparable fit of the uncentered mixture.</p><p>Finally, we note another benefit of standardizing the target before fitting EigenVI; when the target has nearly zero mean and unit covariance, it becomes simpler to identify natural choices for the proposal distribution π. Intuitively, in this case, we want a proposal distribution that has the same mean but heavier tails than a standard Gaussian. In our experiments, we use two types of centered proposal distributions-uniform and isotropic Gaussian-whose variances are greater than one. Figure <ref type="figure">2</ref>: Higher-order expansions may be required to approximate target distributions (black) that are not standardized. Left: approximation of a non-standardized Gaussian. Right: approximation of the mixture distribution in Figure <ref type="figure" target="#fig_1">1</ref> after translating its largest modes away from the origin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Several recent works have considered BBVI methods based on score-matching. These methods take a particularly simple form for Gaussian variational families <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>. The Fisher divergence <ref type="bibr" target="#b17">[18]</ref> has been previously studied as a divergence for variational inference <ref type="bibr" target="#b46">[47]</ref>. Yu and Zhang <ref type="bibr" target="#b47">[48]</ref> propose minimizing a Fisher divergence for semi-implicit (non-Gaussian) variational families; the divergence is minimized using gradient-based optimization. In another line of work, Zhang et al. <ref type="bibr" target="#b49">[50]</ref> consider variational families of energy-based models and derive a closed-form solution to minimize the Fisher divergence in this setting.</p><p>More generally, there have many studies of VI with non-Gaussian variational families. One common extension is to consider families of mixture models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>; these are typically optimized via ELBO maximization. BBVI algorithms have also been derived for more expressive variational families of energy-based models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> and normalizing flows <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>. However the performance of these models, especially the normalizing flows, is often sensitive to the hyperparameters of the flow architecture and optimizer, as well as the parameters of the base distribution <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>. Other aspects of these variational approximations are also less straightforward; for example, one cannot compute their low-order moments, and one cannot easily evaluate or draw samples from the densities of energy-based models.</p><p>The variational approximation in EigenVI is based on the idea of squaring a weighted sum of basis functions. Probability distributions of this form arise most famously in quantum mechanics <ref type="bibr" target="#b15">[16]</ref>. This idea has also been used to model distributions in machine learning, though not quite in the way proposed here. Novikov et al. <ref type="bibr" target="#b37">[38]</ref> propose a tensor train-based model for density estimation, but they do not consider orthogonal basis sets. Similarly, Loconte et al. <ref type="bibr" target="#b32">[33]</ref> obtain distributions by squaring a mixture model with negative weights, and they study this model in conjunction with probabilistic circuits. By contrast in this work, we consider this idea in the context of variational inference, and we focus specifically on the use of orthogonal function expansions, which have many simplifying properties; additionally, the specific objective we optimize leads to a minimum eigenvalue problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate EigenVI on 9 synthetic targets and 8 real data targets. In these experiments we use the orthogonal family induced by normalized Hermite polynomials (see Table <ref type="table" target="#tab_0">1</ref>), whose lowest-order expansion is Gaussian. Thus, this variational family can model non-Gaussian behavior with the higher-order functions in its basis. We first study 2D synthetic targets and use them to demonstrate the expressiveness of these higher-order expansions. Next, we experiment with target distributions where we systematically vary the tail heaviness and amount of skew. Finally, we apply EigenVI to a set of hierarchical Bayesian models from real-world applications and benchmark its performance against other Gaussian BBVI algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">2D synthetic targets</head><p>We first demonstrate how higher-order expansions of the variational family yield more accurate approximations on a range of 2D non-Gaussian target distributions (Figure <ref type="figure" target="#fig_3">3</ref>); see Appendix E. ), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3-5), where</p><formula xml:id="formula_23">K = K 1 K 2 .</formula><p>for details. We report an estimate of KL(p; q) above each variational approximation. The Gaussian variational approximation is fit using batch and match VI <ref type="bibr" target="#b5">[6]</ref>, which minimizes a score-based divergence. For EigenVI, the target distributions were not standardized before fitting EigenVI (we compare the costs of the methods in Figure E.1), and the total number of basis functions is</p><formula xml:id="formula_24">K = K 1 K 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Non-Gaussianity: varying skew and tails in the sinh-arcsinh distribution</head><p>We now consider the sinh-arcsinh normal distribution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, which is induced by transforming a multivariate Gaussian using parameters that control the amount of skew and the weight of the tails. We construct several targets (D = 2, 5) of increasing amounts of non-Gaussianity in the skew or the tails of the distribution, and we refer to these targets as slight skew and tails, more skew and tails, and slight skew and heavier tails; see Appendix E.3 for details. In Figure <ref type="figure" target="#fig_7">4a</ref>, we visualize the 2D targets and the EigenVI fits along with their forward KLs. Before applying EigenVI, we standardize the target using a mean and covariance estimated from batch and match VI <ref type="bibr" target="#b5">[6]</ref>. In Figure <ref type="figure" target="#fig_7">4b</ref>, we measure the EigenVI forward KL under varying numbers of samples B and across increasing numbers of basis functions, given by K = D d=1 K d . We also present the forward KL resulting from batch and match VI (BaM) and automatic differentiation VI (ADVI), which both use Gaussian variational families and are run using the same budget in terms of number gradient evaluations. Next we consider similar targets with D = 5, which are visualized in in Figure E.2, along with the resulting EigenVI variational approximations. In Figure <ref type="figure" target="#fig_7">4c</ref>, we observe greater differences in the number of importance samples needed to lead to good approximations, especially as the number of basis functions increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hierarchical modeling benchmarks from posteriordb</head><p>We now evaluate EigenVI on a set of hierarchical Bayesian models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>, which are summarized in Table <ref type="table">E</ref>.1. The goal is posterior inference: given data observations x 1:N , the posterior of z is</p><formula xml:id="formula_25">p(z | x 1:N ) ∝ p(z)p(x 1:N | z) =: ρ(z),<label>(17)</label></formula><p>where p(z) is the prior and p(x 1:N | z) denotes the likelihood.</p><p>We compare EigenVI to 1) automatic differentation VI (ADVI) <ref type="bibr" target="#b26">[27]</ref>, which maximizes the ELBO over a full-covariance Gaussian family (ADVI), 2) Gaussian score matching (GSM) <ref type="bibr" target="#b36">[37]</ref>, a score-based BBVI approach with a full-covariance Gaussian family, and 3) batch and match VI (BaM) <ref type="bibr" target="#b5">[6]</ref>, which   In these models, we do not have access to the target distribution, p(z | x 1:N ), only the unnormalized target ρ. Thus, we cannot evaluate an estimate of the forward KL. Instead, to evaluate the fidelity of the fitted variational distributions, we compute the empirical Fisher divergence using reference samples from the posterior obtained via Hamiltonian Monte Carlo (HMC):</p><formula xml:id="formula_26">1 S S s=1 ∥∇ log ρ(z s ) -∇ log q(z s )∥ 2 , z s ∼ p(z | x 1:N ).<label>(18)</label></formula><p>Note that this measure is not the objective that EigenVI minimizes; it is analogous to the forward KL divergence, as the expectation is taken with respect to p. We report the results in Figure <ref type="figure" target="#fig_9">5</ref>, computing the Fisher divergence for EigenVI with increasing numbers of basis functions. We typically found that with more basis functions, the scores becomes closer to that of the target.</p><p>Finally, we provide a qualitative comparison with real-NVP normalizing flows (NFs) <ref type="bibr" target="#b11">[12]</ref>, a flexible variational family that is fit by minimizing the reverse KL. We found that after tuning the batch-size and learning rate, NFs generally had a suitable fit. We visualize the posterior marginals for a subset of dimensions from 8schools in the top three rows, comparing EigenVI, the NF, and BaM. Here, we observe that the Gaussian struggles to fit the tails of this target distribution. On the other hand, EigenVI provides a competitive fit to the normalizing flow. In Appendix E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion of limitations and future work</head><p>In this work, we introduced EigenVI, a new approach for score-based variational inference based on orthgonal function expansions. The score-based objective for EigenVI is minimized by solving an eigenvalue problem, and thus this framework provides an alternative to gradient-based methods for BBVI. Importantly, many computations in EigenVI can be parallelized with respect to the batch of  samples, unlike in iterative methods. We applied EigenVI to many synthetic and real-world targets, and these experiments show that EigenVI provides a principled way of improving upon Gaussian variational families.</p><p>Many future directions remain. First, the approach described in this paper relies on importance sampling, and thus it may benefit from more sophisticated methods for adaptive importance sampling. Second, it may be useful to construct variational families from different orthogonal function expansions. Our empirical study focused on the family built from normalized Hermite polynomials. But this family may require a very high-order expansion to model highly non-Gaussian targets, and such an expansion will be very expensive in high dimensions. Though this family was sufficient for many of the targets we simulated, others will be crucial for modeling highly non-Gaussian targets.</p><p>Another direction is to develop variational families whose orthogonal function expansions scale more favorably with the dimension, perhaps by incorporating low rank structure in the target's covariance. Finally, it would be interesting to explore iterative versions of EigenVI in which each iteration solves a minimum eigenvalue problem on some subsample of data points. With such an approach, EigenVI could potentially be applied to very large-scale problems in Bayesian inference.</p><p>Next we consider how to implement this procedure efficiently in practice, and in particular, how to calculate the definite integral for the CDF in Eq. A.8. As shorthand, we define the doubly-indexed set of real-valued functions</p><formula xml:id="formula_27">Φ kℓ (ξ) = ξ -∞ ϕ k (z)ϕ ℓ (z) dz. (A.11)</formula><p>It follows from orthogonality that Φ kl (+∞) = δ kl and from the Cauchy-Schwartz inequality that |Φ kℓ (ξ)| ≤ 1 for all ξ ∈ R. Our interest in these functions stems from the observation that</p><formula xml:id="formula_28">C(ξ) = K k,ℓ=1</formula><p>S kℓ Φ kl (ξ) = trace[SΦ(ξ)], (A.12) so that if we have already computed the functions Φ kℓ (ξ), then we can use Eq. A.12 to compute the CDF whose inverse we need in Eq. A.10. In practice, we can use numerical quadrature to pre-compute Φ kℓ (ξ) for many values along the real line and then solve Eq. A.10 quickly by interpolation; that is, given u, we find ξ satisfying trace[SΦ(ξ)] = u. The result is an unbiased sample drawn from the density ρ(ξ) in Eq. A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequential sampling</head><p>Finally we show that each draw in Eqs. A.3-A.5 reduces to the problem described above. As in Section 2.1, we work out the steps specifically for an example in D = 3, where we must draw the samples z 1 ∼ q(z 1 ), z 2 ∼ q(z 2 |z 1 ) and z 3 ∼ q(z 3 |z 1 , z 2 ). This example illustrates all the ideas needed for the general case but with a minimum of indices.</p><p>Consider the joint distribution given by</p><formula xml:id="formula_29">q(z 1 , z 2 , z 3 ) =   K1 i=1 K2 j=1 K3 k=1 β ijk ϕ i (z 1 )ϕ j (z 2 )ϕ k (z 3 )   2 where ijk β 2 ijk = 1. (A.13)</formula><p>From this joint distribution, we can compute marginal distributions by integrating out subsets of variables, and each integration over R gives rise to a contraction of indices, as in Eq. 7, due to the property of orthogonality. In particular, expanding the square in Eq. A.13, we can write this joint distribution as</p><formula xml:id="formula_30">q(z 1 , z 2 , z 3 ) = K3 k,k ′ =1   K1 i,i ′ =1 K2 j,j ′ =1 β ijk β i ′ j ′ k ′ ϕ i (z 1 )ϕ i ′ (z 1 )ϕ j (z 2 )ϕ j ′ (z 2 )   ϕ k (z 3 )ϕ k ′ (z 3 ),</formula><p>(A.14) and we can then contract the index k ′ when integrating over z 3 , since ϕ k (z 3 )ϕ k ′ (z 3 )dz 3 = δ kk ′ .</p><p>In this way we find that the marginal distributions are</p><formula xml:id="formula_31">q(z 1 , z 2 ) = K2 j,j ′ =1   K1 i,i ′ =1 K3 k=1 β ijk β i ′ j ′ k ϕ i (z 1 )ϕ i ′ (z 1 )   ϕ j (z 2 )ϕ j ′ (z 2 ), (A.15) q(z 1 ) = K1 i,i ′ =1   K2 j=1 K3 k=1 β ijk β i ′ jk   ϕ i (z 1 )ϕ i ′ (z 1 ). (A.16)</formula><p>Now note from the brackets in Eq. A.16 that this marginal distribution is already in the quadratic form of Eq. A.6 with coefficients</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>(1)</p><formula xml:id="formula_32">ii ′ = K2 j=1 K3 k=1 β ijk β i ′ jk . (A.17)</formula><p>From this first quadratic form, we can therefore use inverse transform sampling to obtain a draw z 1 ∼ q(z 1 ).</p><p>which simply encapsulate the bracketed term in Eq. B.6. Note that there are K 2 1 of these coefficients, each of which can be computed in O(K 2 K 3 . . . K D ). With this shorthand, we can write</p><formula xml:id="formula_33">E q [z 1 ] = K1 i,j=1 A ij µ ij , (B.8) E q [z 2 1 ] = K1 i,j=1 A ij ν ij , (B.9)</formula><p>where µ ij and ν ij are the integrals defined in Eqs. B.2-B.3. Thus the problem has been reduced to a weighted sum of one-dimensional integrals.</p><p>A similar calculation gives the result we need for correlations. Again, without loss of generality, we focus on calculating E q [z 1 z 2 ]. Analogous to Eq. B.7, we define the tensor of coefficients</p><formula xml:id="formula_34">B ijkℓ = K3 k3=1 • • • K D k D =1 α ikk3...k D α jℓk3...k D , (B.10)</formula><p>which arises from marginalizing over the variables (z 3 , z 4 , . . . , z D ). There are K 2 1 K 2 2 of these coefficients, each of which can be computed in O(K 3 K 4 . . . K D ). With this shorthand, we can write</p><formula xml:id="formula_35">E q [z 1 z 2 ] = K1 i,j=1 K2 k,ℓ=1 B ijkℓ µ ij µ kℓ . (B.11)</formula><p>where µ ij is again the integral defined in Eq. B.2). Thus the problem has been reduced to a weighted sum of (the product of) one-dimensional integrals.</p><p>Finally, we show how to evaluate the integrals in Eqs. B.2-B.3 for the specific case of orthogonal function expansions with weighted Hermite polynomials; similar computations apply in the case of Legendre polynomials. Recall in this case that</p><formula xml:id="formula_36">ϕ k+1 (z) = √ 2πk! -1 2 e -1 2 z 2 1 2</formula><p>H k (z), (B.12)</p><p>where H k (z) are the probabilist's Hermite polynomials given by</p><formula xml:id="formula_37">H k (z) = (-1) k e z 2 2 d k dz k e -z 2 2 . (B.13)</formula><p>To evaluate the integrals for this particular family, we can exploit the following recursions that are satisfied by Hermite polynomials:</p><formula xml:id="formula_38">H k+1 (z) = zH k (z) -H ′ k (z), (B.14) H ′ k (z) = kH k-1 (z). (B.15)</formula><p>Eliminating the derivatives H ′ k (z) in Eqs. B.14-B.15, we see that zH k (z) = H k+1 (z) + kH k-1 (z). We can then substitute Eq. B.12 to obtain a recursion for the orthogonal basis functions themselves:</p><formula xml:id="formula_39">zϕ k (z) = √ kϕ k+1 (z) + √ k-1ϕ k-1 (z). (B.16)</formula><p>With the above recursion, we can now read off these integrals from the property of orthogonality. For example, starting from Eq. B.2, we find that</p><formula xml:id="formula_40">µ ij = ∞ -∞ ϕ i (z)ϕ j (z) z dz, (B.17) = ∞ -∞ ϕ i (z) jϕ j+1 (z) + j -1ϕ j-1 (z) dz, (B.18) = δ i,j+1 j + δ i,j-1 √ i, (B.19)</formula><p>where δ ij is the Kronecker delta function. Next we consider the integral in Eq. B.3, which involves a power of z 2 in the integrand. In this case we can make repeated use of the recursion:</p><formula xml:id="formula_41">ν ij = ∞ -∞ ϕ i (z)ϕ j (z) z 2 dz, (B.20) = ∞ -∞ √ iϕ i+1 (z) + √ i-1ϕ i-1 (z) jϕ j+1 (z) + j -1ϕ j-1 (z) dz, (B.21) = δ ij ij + (i-1)(j -1) + δ i-1,j+1 j(j +1) + δ j-1,i+1 i(i+1). (B.22)</formula><p>Note that the matrices in Eq. B.19 and Eq. B.22 can be computed for whatever size is required by the orthogonal basis function expansion in Eq. B.1. Once these matrices are computed, it is a simple matter of substitution<ref type="foot" target="#foot_0">foot_0</ref> to compute the moments</p><formula xml:id="formula_42">E q [z 1 ], E q [z 2 1 ]</formula><p>, and E q [z 1 z 2 ] from Eqs. B.8-B.9 and Eq. B.11. Finally, we can compute other low-order moments (such as E q [z 5 ] or E q [z 3 z 7 ]) by an appropriate permutation of indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Eigenvalue problem</head><p>In this appendix we show in detail how the optimization for EigenVI reduces to a minimum eigenvalue problem. In particular we prove the following.</p><formula xml:id="formula_43">Lemma C.1. Let {ϕ k (z)} ∞</formula><p>k=1 be an orthogonal function expansion, and let q ∈ Q K be the variational approximation parameterized by</p><formula xml:id="formula_44">q(z) = K k=1 α k ϕ k (z) 2 , (C.1)</formula><p>where the weights satisfy K k=1 α 2 k = 1, thus ensuring that the distribution is normalized. Suppose furthermore that q is chosen to minimize the empirical estimate of the Fisher divergence given, as in eq. ( <ref type="formula" target="#formula_11">10</ref>), by</p><formula xml:id="formula_45">D π (q, p) = B b=1 q(z b ) π(z b ) ∇ log q(z b ) -∇ log p(z b ) 2 .</formula><p>Then the optimal variational approximation q in this family can be computed by solving the minimum eigenvalue problem</p><formula xml:id="formula_46">min q∈Q K D π (q, p) = min ∥α∥=1 α ⊤ M α =: λ min (M ), (C.2)</formula><p>where M is given in Eq. 13 and α = [α 1 , . . . , α K ] ∈ R K . The optimal weights α are given (up to an arbitrary sign) by the corresponding eigenvector of this minimal eigenvalue.</p><p>Proof. The scores of q in this variational family are given by</p><formula xml:id="formula_47">∇ log q(z b ) = 2 k α k ∇ϕ k (z b ) k α k ϕ k (z b ) .</formula><p>Substituting the above into the empirical divergence, we find that</p><formula xml:id="formula_48">D π (q, p) = B b=1 q(z b ) π(z b ) ∇ log q(z b ) -∇ log p(z b ) 2 = B b=1 k α k ϕ k (z b ) 2 π(z b ) 2 k α k ∇ϕ k (z b ) k α k ϕ k (z b ) -∇ log p(z b ) 2 = B b=1 1 π(z b ) 2 k α k ∇ϕ k (z b ) - k α k ϕ k (z b ) ∇ log p(z b ) 2 = B b=1 1 π(z b ) k α k 2∇ϕ k (z b ) -ϕ k (z b )∇ log p(z b ) 2 = α ⊤ M α,</formula><p>where M is given in ( <ref type="formula" target="#formula_16">13</ref>) and α = [α 1 , . . . , α K ] ∈ R K . Thus the optimal weights α are found by minimizing the quadratic form α ⊤ M α subject to the constraint α ⊤ α = 1. Equivalently, a solution can be found by minimizing the Rayleigh quotient</p><formula xml:id="formula_49">argmin v v ⊤ M v v ⊤ v (C.3)</formula><p>and setting α = v/∥v∥. It then follows from the Rayleigh-Ritz theorem <ref type="bibr" target="#b7">[8]</ref> for symmetric matrices that α is the eigenvector corresponding to the minimal eigenvalue of M , and this proves the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Practical considerations of EigenVI D.1 EigenVI vs gradient-based BBVI</head><p>Recall that EigenVI has two hyperparameters: the number of basis functions K and the number of importance samples B. We note there is an important difference between these two hyperparameters and the learning rate in ADVI and other gradient-based methods. Here, as we use more basis functions and more samples, the resulting fit is a better approximation. So, we can increase the number of basis functions and importance samples until a budget is reached, or until the resulting variational approximation is a sufficient fit. On the other hand, tuning the learning rate in gradient-based optimization is much more sensitive because it cannot be too large or too small. If it is too large, ADVI may diverge. If the learning rate is too small, it may take too long to converge in which case it may exceed computational budgets.</p><p>Another fundamental difference in setting the number of basis functions as compared to the learning rate or batch size of gradient based optimization is that once we have evaluated the score of the target distribution for the samples, these same samples can be reused for solving the eigenvalue problem with any choice of the number of basis functions, as these tasks are independent. By contrast, in iterative BBVI, the optimization problem needs to be re-solved for every choice of hyperparameters, and the samples from different runs cannot be mixed together.</p><p>Furthermore, solving the eigenvalue problem is fast, and scores can be computed in parallel. In our implementation, we use off-the-shelf eigenvalue solvers, such as ARPACK <ref type="bibr" target="#b29">[30]</ref> or Julia's eigenvalue decomposition function, eigen. In many problems with complicated targets, the main cost comes from gradient evaluation and not the eigenvalue solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Choosing the number of samples B</head><p>Intuitively, if the target p is in the variational family Q (i.e., it can be represented using an order-K expansion), then we should choose the number of samples B to roughly equal the number of basis function K. If p is very different from Q, we need more samples, and in our experiments, we use a multiple of the number of basis functions (say of order 10). As discussed before, once we have evaluated a set of scores, these can be reused to fit a larger number of basis functions.</p><p>K 2 = K values used, i.e., 3, 6, and 10 (resulting in a total number of basis functions of 3 2 , 6 2 , and 10 2 ). In the bottom row of the plot, we also show wall clock timings (computed without parallelization) to show how the cost grows with the increase in the number of basis functions and importance samples. The horizontal dotted line denotes the result from batch and match VI, which fits a Gaussian via score matching; here a batch size of 16 was used and a learning rate of λ t = BD t+1 . The black star denotes the number of score evaluations used by the Gaussian VI method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Sinh-arcsinh targets</head><p>The sinh-arcsinh normal distribution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> has parameters s ∈ R D , τ ∈ R D + , Σ ∈ S ++ ; it is induced by transforming a Gaussian Z 0 ∼ N (0, Σ) to Z = S s,τ (Z 0 ), where We constructed 3 targets in 2 dimensions and 3 targets in 5 dimensions, each with varying amounts of non-Gaussianity. The details of each target are below. In all experiments, EigenVI was applied with standardization, where a Gaussian was fit using batch and match VI with a batch size of 16 and a learning rate λ t = BD t+1 . For all experiments, we used a proposal distribution π that was uniform on [-5, 5] 2 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D sinh-arcsinh normal experiment</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Target probability distributions (black dashed curves) on the interval [-1, 1] (left), the unit circle (middle), and the real line (right), and their approximations by orthogonal function expansions from different families and of different orders; see Eq.2 and Table 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Mixture target (translation of Figure1c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3-5), where K = K 1 K 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Example 2D targets (left) varying the skew s or tail weight τ components and their EigenVI fits (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sinh-arcsinh normal distribution synthetic target. Panel (a) shows the three targets we consider in 2D, and their resulting EigenVI fit. Panel (b) shows measures KL(p; q) for D = 2, and panel (c) shows KL(p; q) for D = 5; the x-axis shows the number of basis functions, K = d K d . minimizes a regularized score-based divergence over a full-covariance Gaussian family. In these examples, we standardize the target using either GSM or BaM before applying EigenVI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4 ,</head><label>4</label><figDesc>we show the full corner plot in Figure E.3 and marginals of the garch11 model in Figure E.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>Batch and match VI with a full covariance Gaussian family 8-schools, D = 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results on posteriordb models. Top three rows: marginal distributions of the even dimensions from 8-schools. Reference samples from HMC are outlined in gray, and the VI samples are in green. Bottom two rows: evaluation of methods with the (forward) Fisher divergence. The x-axis shows the number of basis functions, K = d K d . Shaded regions represent standard errors computed with respect to 5 random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>S 2 D d=1 ( 1 S</head><label>2d=11</label><figDesc>s,τ (z) := [S s1,τ1 (z 1 ), . . . , S s D ,τD (z D )] ⊤ , S s d ,τ d (z d ) := sinh 1 τ d sinh -1 (z d ) + s d τ d . (E.1)Here s d controls the amount of skew in the dth dimension, and τ d controls the tail weight in that dimension. When s d = 0 and τ d = 1 in all dimensions d, the distribution is Gaussian.The sinh-arcsinh normal distribution has the following density:p(z; s, τ, Σ) = [(2π) D |Σ|] -1 + z 2 d ) -1 2 τ d C s d ,τ d (z d ) exp -1 2 S s,τ (z) ⊤ Σ -1 S s,τ ,(E.2)where we define the functionsC s d ,τ d (z d ) := (1 + S 2 s d ,τ d (z))s d ,τ d (z d ) := sinh(τ d sinh -1 (z d ) -s d ), S s,τ (z) = [S s1,τ1 (z 1 ), . . . , S s D ,τ D (z D )] ⊤ . (E.4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>For D = 2 ( 5 )</head><label>25</label><figDesc>Figure 4b), we consider the slight skew and tails target with parameters s = [0.2, 0.2], τ = [1.1, 1.1], the more skew and tails target with s = [0.2, 0.5], τ = [1.1, 1.1], and the slight skew and heavier tails with s = [0.2, 0.2], τ = [1.4, 1.1]. Note that s = [0, 0], τ = [1, 1] recovers the multivariate Gaussian. These three target are visualized in Figure 4a. 5D sinh-archsinh normal experiment We constructed three targets P 1 (slight skew and tails), P 2 (more skew and tails), and P 3 (slight skew and heavier tails) each with The skew and tail weight parameters used were: s 1 = [0., 0., 0.2, 0.2, 0.2]; τ 1 = [1., 1., 1., 1., 1.1], s 2 = [0.0, 0.0, 0.6, 0.4, -0.5]; τ 2 = [1., 1., 1., 1., 1.1], and s 3 = [0.2, 0.2, 0.2, 0.2, 0.2]; τ 3 = [1.1, 1.1, 1., 1.4, 1.6]. See Figure E.2 for a visualization of the marginals of each target distribution. In the second row, we show examples of resulting EigenVI fit (visualized using samples from q) from B = 20,000 and K = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure E. 3 : 23 Figure E. 4 :</head><label>3234</label><figDesc>Figure E.3: Comparison of EigenVI, normalizing flow, and Gaussian score-based BBVI methods on 8schools.23</figDesc><graphic coords="23,201.76,498.91,208.49,212.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of orthogonal function expansions in one dimension. The basis functions in the table are not normalized, but they can be rescaled so that their squares integrate to one.</figDesc><table><row><cell>support</cell><cell>orthogonal family</cell><cell>basis functions ϕ k</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>With further bookkeeping, one can also exploit the sparsity of µij and νij to derive more efficient calculations of these moments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We thank <rs type="person">Bob Carpenter</rs> and <rs type="person">Yuling Yao</rs> for helpful discussions and anonymous reviewers for their time and feedback on the paper. The <rs type="institution">Flatiron Institute</rs> is a division of the <rs type="funder">Simons Foundation</rs>. This work was supported in part by <rs type="funder">NSF</rs> <rs type="grantNumber">IIS-2127869</rs>, <rs type="funder">NSF</rs> <rs type="grantNumber">DMS-2311108</rs>, <rs type="grantNumber">NSF/DoD PHY-2229929</rs>, <rs type="grantNumber">ONR N00014-17-1-2131</rs>, <rs type="grantNumber">ONR N00014-15-1-2209</rs>, the <rs type="funder">Simons Foundation</rs>, and <rs type="person">Open Philanthropy</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Pzw3mHN">
					<idno type="grant-number">IIS-2127869</idno>
				</org>
				<org type="funding" xml:id="_myCN3cj">
					<idno type="grant-number">DMS-2311108</idno>
				</org>
				<org type="funding" xml:id="_JDPhGrA">
					<idno type="grant-number">NSF/DoD PHY-2229929</idno>
				</org>
				<org type="funding" xml:id="_BZmJfUy">
					<idno type="grant-number">ONR N00014-17-1-2131</idno>
				</org>
				<org type="funding" xml:id="_S6yTCsa">
					<idno type="grant-number">ONR N00014-15-1-2209</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Sampling from orthogonal function expansions</head><p>In this appendix we show how to sample from a density on R D constructed from a Cartesian product of orthogonal function expansions. Specifically, we assume that the density is of the form</p><p>where {ϕ k (•)} ∞ k=1 define a family of orthonormal functions on R and where the density is normalized by requiring that To draw samples from this density, we describe a sequential procedure based on inverse transform sampling. In particular, we obtain a sample z ∈ R D by the sequence of draws z 1 ∼ q(z 1 ), (A.3) z 2 ∼ q(z 2 |z 1 ), (A.4) . . . z D ∼ q(z D |z 1 , z 2 , . . . , z D-1 ).</p><p>(A.5)</p><p>This basic strategy can also be used to sample from distributions whose domains are Cartesian products of different one-dimensional spaces.</p><p>In what follows, we first introduce a "core primitive" density, and we show how to sample efficiently from its distribution. We then show how the sampling procedure in Eqs. A.3-A.5 reduces to sampling from this core primitive; a key component of this procedure is the property of orthogonality, which helps facilitate the efficient computation of marginal distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core primitive</head><p>First we describe the core primitive that we will use for each of the draws in Eqs. A.3-A.5. To begin, we observe the following: if S is any positive semidefinite matrix with trace(S) = 1, then</p><p>defines a normalized density over R. In particular, since S ⪰ 0, it follows that ρ(ξ) ≥ 0 for all ξ ∈ R, and since trace(S) = 1, it follows that</p><p>The core primitive that we need is an efficient procedure to sample from a normalized density of this form. We will see later that all of the densities in Eqs. A.3-A.5 can be expressed in this form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse transform sampling</head><p>Since the density in Eq. A.6 is one-dimensional, we can obtain the draw we need by inverse transform sampling. In particular, let C(ξ) denote the cumulative distribution function (CDF) associated with Eq. A.6, which is given by</p><p>and let C -1 (ξ) denote the inverse CDF. Then at least in principle, we can draw a sample from ρ by the two-step procedure u ∼ Uniform[0, 1], (A.9)</p><p>Next we consider how to sample from the conditional q(z 2 |z 1 ) = q(z 1 , z 2 )/q(z 1 ). Again, from the brackets in Eq. A.15, we see that this conditional distribution is also in the quadratic form of Eq. A.6 with coefficients</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>(2)</p><p>From this second quadratic form, we can therefore use inverse transform sampling to obtain a draw z 2 ∼ q(z 2 |z 1 ). Finally, we consider how to sample from q(z 3 |z 1 , z 2 ) = q(z 1 , z 2 , z 3 )/q(z 1 , z 2 ). From Eq. A.14, we see that this conditional distribution is also in the quadratic form of Eq. A.6 with coefficients</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>(3)</p><p>From this third quadratic form, we can therefore use inverse transform sampling to obtain a draw z 3 ∼ q(z 3 |z 1 , z 2 ). Finally, from the sums in Eq. A. <ref type="bibr" target="#b18">19</ref>, we see that the overall cost of this procedure is</p><p>), or quadratic in the total number of basis functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Calculation of moments</head><p>In this appendix we show how to calculate the low-order moments of a density constructed from the Cartesian product of orthogonal function expansions. In particular, we assume that the density is over R D and of the form</p><p>where {ϕ k (•)} ∞ k=1 are orthogonal functions on R and where the coefficients are properly normalized so that the density integrates to one. For such a density, we show that the calculation of first and second-order moments boils down to evaluating one-dimensional integrals of the form</p><p>We also show how to evaluate these integrals specifically for the orthogonal family of weighted Hermite polynomials.</p><p>First we consider how to calculate moments such as E q [z p d ], where p ∈ {1, 2}, and without loss of generality we focus on calculating E q [z p 1 ]. We start from the joint distribution in Eq. B.1 and proceed by marginalizing over the variables (z 2 , z 3 , . . . , z D ). Exploiting orthogonality, we find that</p><p>We can rewrite this expression more compactly as a quadratic form over integrals of the form in Eqs. B.2-B.3. To this end, we define the coefficients We compare the number of score evaluations wallclock vs FKL divergence for the target distributions in Figure <ref type="figure">3</ref>: the Gaussian mixture (column 1), the funnel (column 2), and the cross (column 3) distributions. The K used for EigenVI is reported in each figure legend, where</p><p>The black star denotes the number of gradient evaluations for the Gaussian method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional experiments and details E.1 Computational resources</head><p>The experiments were run on a Linux workstation with a 32-core Intel(R) Xeon(R) w5-3435X processor and with 503 GB of memory. Experiments were run on CPU. In the sinh-arcsinh and posteriordb experiments, computations to construct the matrix M were parallelized over 28 threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 2D synthetic targets</head><p>We considered the following synthetic 2D targets:</p><p>• 3-component Gaussian mixture:</p><p>where we define Σ = 2 0.1 0.1 2 .</p><p>• Funnel distribution with σ 2 = 1.2:</p><p>• Cross distribution:</p><p>where Σ 1 = 0.15 0.9 0 0 1 and Σ 2 = 1 0 0 0.15 0.9 . These experiments were conducted without standardization with a Gaussian VI estimate. The EigenVI proposal distribution π used was a uniform([-9, 9]) distribution.</p><p>In  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Posteriordb experiments</head><p>We consider 8 real data targets from posteriordb, a suite of benchmark Bayesian models for real data problems. In Table <ref type="table">E</ref>.1, we summarize the models considered in the study. These target distributions are non-Gaussian, typically with some skew or different tails. To access the log target probability and their gradients, we used the BridgeStan library <ref type="bibr" target="#b41">[42]</ref>, which by default transforms the target to be supported on R D .</p><p>For all experiments, we fixed the number of importance samples to be B = 40,000; to construct the EigenVI matrix M , the computations were parallelized over the samples. These experiments were repeated over 5 random seeds, and we report the mean and standard errors in Figure <ref type="figure">5</ref>; for lower dimensions, there was little variation between runs.</p><p>The target distributions were standardized using a Gaussian fit from score matching before applying EigenVI. In most cases, the proposal distribution π was chosen to be uniform over [-6, 6] D . For the models 8-schools, which has a longer tail, we used a multivariate Gaussian proposal with zero mean and a scaled diagonal covariance σI, with σ = 3 2 .</p><p>For the Gaussian score matching (GSM) method <ref type="bibr" target="#b36">[37]</ref>, we chose a batch size of 16 for all experiments. We generally found the results were not too sensitive in comparison to other batch sizes of 4,8, and 32. For the batch and match (BaM) method <ref type="bibr" target="#b5">[6]</ref>, we chose a batch size of 16. The learning rate was fixed at λ t = BD t+1 , which was a recommended schedule for non-Gaussian targets. For all ELBO optimization methods (full covariance Gaussian family and normalizing flow family), we used Adam to optimize the ELBO. We performed a grid search over the learning rate 0.01, 0.02, 0.05, 0.1 and batch size B = 4, 8, 16, 32. For the normalizing flow model, we used a real NVP <ref type="bibr" target="#b11">[12]</ref> with 8 layers and 32 neurons. We found empirically that the computational of the scores was unreliable <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b48">49]</ref>; hence we do not show their Fisher divergence in Figure <ref type="figure">5</ref>.</p><p>In Figure E.3 and Figure E.4, we show the corner plots that compare an EigenVI fit, a normalizing flow fit, and a Gaussian fit (BaM). In each plot, we plot the samples from the variational distribution against samples from Hamiltonian Monte Carlo. We observe that the two more expressive families EigenVI and the normalizing flow are able to model the tails of the distribution better than the Gaussian fit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Broader impacts</head><p>EigenVI adds to the literature on BBVI, which has been an important line of work for developing automated, approximate Bayesian inference methods. In terms of positive societal impacts, Bayesian models are used throughout the sciences and engineering, and advances in fast and automated inference will aid in advances in these fields. In terms of negative societal impacts, advances in BBVI could be used to train generative models with malicious or unintended uses.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PyMC: a modern, and comprehensive probabilistic programming framework in Python</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abril-Pla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Andreani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Fonnesbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kochurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Luhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1516">1516. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advances in black-box VI: Normalizing flows, importance weighting, and optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sylvester normalizing flows for variational inference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyro: Deep universal probabilistic programming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jankowiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karaletsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szerlip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Horsfall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="973" to="978" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch and match: black-box variational inference with a score-based divergence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pillaud-Vivien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Margossian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stan: A probabilistic programming language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riddell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Methoden der Mathematischen Physik</title>
		<author>
			<persName><forename type="first">R</forename><surname>Courant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Julius Springer</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1924">1924</date>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kernel exponential family estimation via doubly dual embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust, accurate stochastic optimization for variational inference</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dhaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Catalina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Challenges and opportunities in high dimensional variational inference</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dhaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Catalina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welandawe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Turing: a language for flexible probabilistic inference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<title level="m">Nonparametric variational inference. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Black box variational inference with a deterministic objective: Faster, more accurate, and even more black box</title>
		<author>
			<persName><forename type="first">R</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Schroeter</surname></persName>
		</author>
		<title level="m">Introduction to Quantum Mechanics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05559</idno>
		<title level="m">Boosting variational inference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sinh-arcsinh distributions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pewsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="780" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The sinh-arcsinh normal distribution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pewsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Significance</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="7" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep directed generative models with energy-based probability estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03439</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Normalizing flows: An introduction and review of current methods</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3964" to="3979" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krämer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Noé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smooth normalizing flows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic differentiation variational inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Energy-inspired models: Learning with sampler-induced distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting Structured Data</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ARPACK Users&apos; Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lehoucq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.caam.rice.edu/software/ARPACK/" />
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stein variational gradient descent: a general purpose Bayesian inference algorithm</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting black box variational inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dresdner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Subtractive mixture models via squaring: Representation and learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Loconte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sladek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vergari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiplicative normalizing flows for variational Bayesian neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">posteriordb: a set of posteriors for Bayesian inference and probabilistic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<ptr target="https://github.com/stan-dev/posteriordb" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational boosting: Iteratively refining posterior approximations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Foti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2420" to="2429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Variational inference with Gaussian score matching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Margossian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tensor-train density estimation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Oseledets</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Normalizing flows for probabilistic modeling and inference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">57</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Black box variational inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="814" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BridgeStan: Efficient in-memory access to Stan programs through Python</title>
		<author>
			<persName><forename type="first">E</forename><surname>Roualdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Axen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<ptr target="https://github.com/roualdes/bridgestan" />
	</analytic>
	<monogr>
		<title level="j">Julia, and R</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic programming in Python using PyMC3</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salvatier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Wiecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fonnesbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational Bayes for non-conjugate inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dual control variate for faster black-box variational inference</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Geffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bondell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05284</idno>
		<title level="m">Variational approximations using Fisher divergence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi-implicit variational inference via score matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural Posterior Estimation with Differentiable Simulators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeghal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lanusse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boucaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Remy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Variational Hamiltonian Monte Carlo via score matching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahbaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">485</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pathfinder: Parallel quasi-newton variational inference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">306</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards a unified theory for texture modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Filters, random fields and maximum entropy</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="107" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Slice sampling reparameterization gradients</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zoltowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="23532" to="23544" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
