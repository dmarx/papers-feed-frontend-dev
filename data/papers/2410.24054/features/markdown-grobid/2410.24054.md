# EigenVI: score-based variational inference with orthogonal function expansions

## Abstract

## 

We develop EigenVI, an eigenvalue-based approach for black-box variational inference (BBVI). EigenVI constructs its variational approximations from orthogonal function expansions. For distributions over R D , the lowest order term in these expansions provides a Gaussian variational approximation, while higher-order terms provide a systematic way to model non-Gaussianity. These approximations are flexible enough to model complex distributions (multimodal, asymmetric), but they are simple enough that one can calculate their low-order moments and draw samples from them. EigenVI can also model other types of random variables (e.g., nonnegative, bounded) by constructing variational approximations from different families of orthogonal functions. Within these families, EigenVI computes the variational approximation that best matches the score function of the target distribution by minimizing a stochastic estimate of the Fisher divergence. Notably, this optimization reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps the iterative gradient-based optimizations that are required for many other BBVI algorithms. (Gradient-based methods can be sensitive to learning rates, termination criteria, and other tunable hyperparameters.) We use EigenVI to approximate a variety of target distributions, including a benchmark suite of Bayesian models from posteriordb. On these distributions, we find that EigenVI is more accurate than existing methods for Gaussian BBVI.

## Introduction

Probabilistic modeling is a cornerstone of modern data analysis, uncertainty quantification, and decision making. A key challenge of probabilistic inference is computing a target distribution of interest; for instance, in Bayesian modeling, the goal is to compute a posterior distribution, which is often intractable. Variational inference (VI) [[5,](#b4)[21,](#b20)[45]](#b44) is a popular method for scalable probabilistic inference that has worked across a range of applications. The idea behind VI is to approximate the target distribution by the closest member of some tractable family.

One major focus of research is to develop black-box algorithms for variational inference [[6,](#b5)[15,](#b14)[23,](#b22)[27,](#b26)[32,](#b31)[37,](#b36)[40,](#b39)[44,](#b43)[46]](#b45). Algorithms for black-box variational inference (BBVI) can be used to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). approximate any target distribution that is differentiable and computable up to some multiplicative (normalizing) constant; as such, they are extremely flexible. These algorithms have been widely implemented in popular probabilistic programming languages, and they are part of the modern toolbox for practitioners in computational statistics and data analysis [[1,](#b0)[4,](#b3)[7,](#b6)[13,](#b12)[43]](#b42).

Traditionally, the variational approximations in BBVI are optimized by minimizing the Kullback-Leibler (KL) divergence between the variational family and the target (equivalently, maximizing the ELBO). This strategy is powerful and scalable, but it relies on stochastic gradient descent (SGD), which can be difficult to tune [[10,](#b9)[11,](#b10)[51]](#b50). These difficulties can be acute even for Gaussian variational approximations [[27,](#b26)[40]](#b39), particularly if these approximations employ full covariance matrices.

More recently, researchers have proposed algorithms for Gaussian BBVI that do not require the use of SGD [[6,](#b5)[37]](#b36). Instead of minimizing the KL divergence, these methods aim to match the scores, or the gradients of the log densities, between the variational distribution and the target density. These methods exploit the special form of Gaussian distributions to derive closed-form proximal point updates for score-matching. These updates are as inexpensive as SGD, but not as brittle. They show that score-based BBVI can be applied in an elegant way to Gaussian variational families.

In this paper, we show that score-based BBVI also yields simple, closed-form updates for a much broader family of variational approximations. Specifically, we propose a new class of variational families constructed from orthogonal function expansions and inspired by solutions to the Schrödinger equation in quantum mechanics. These families are expressive enough to parameterize a wide range of target distributions; at the same time, the distributions in these families are sufficiently tractable that one can calculate low-order moments and draw samples from them. In this paper, we mostly use orthogonal function expansions to construct distributions supported on R D ; in this case, the lowest-order term in the expansion is sufficient to model Gaussian behavior, while higher-order terms account for increasing amounts of non-Gaussianity. More generally, we also show how different basis sets of orthogonal functions can be used to construct variational families over other spaces.

To optimize over a variational family from this class, we minimize an estimate of the Fisher divergence, which measures the scores of the variational distribution against those of the target distribution. We show that this optimization reduces to a minimum eigenvalue problem, thus avoiding the need for gradient-based methods. For this reason, we call our approach EigenVI.

We study EigenVI with a variational family constructed from weighted Hermite polynomials. We first demonstrate the expressiveness of this family on a variety of multimodal, asymmetric, and heavytailed distributions. We then use EigenVI to approximate a diverse collection of non-Gaussian target distributions from posteriordb [[35]](#b34), a benchmark suite of Bayesian hierarchical models. On these problems, EigenVI provides more accurate posterior approximations than leading implementations of Gaussian BBVI based on KL minimization and score-matching.

The organization of this paper is as follows. In Section 2 we introduce the variational families that arise from orthgonal function expansions, and we show how score-matching in these families reduces to an eigenvalue problem. In Section 3 we review the literature related to EigenVI. In Section 4, we evaluate EigenVI on a variety of synthetic and real-data targets. Finally, in Section 5, we discuss limitations and future work.

## Score-based variational inference with orthogonal function expansions

In this section we use orthogonal function expansions to develop new variational families for approximate probabilistic inference. In Section 2.1, we review the basic properties of these expansions. In Section 2.2, we introduce a score-based divergence for VI with these families; notably, for this divergence, the optimization for VI reduces to an eigenvalue problem. Finally in Section 2.3, we consider how to use these variational approximations for unstandardized distributions; in these settings we must carefully manage the trade-off between expressiveness and computational cost.

## Orthogonal function expansions

Let Z ⊆ R D denote the support of the target distribution p. Suppose that there exists a complete set of orthonormal basis functions {ϕ k (z)} ∞ k=1 on this set. By complete, we mean that any sufficiently wellbehaved function f : Z → R can be approximated, to arbitrary accuracy, by a particular weighted 

$(•) z ∈ [-1, 1] Legendre polynomials {1, z, 3z 2 -1, 5z 3 -3z, . . .} z = e iθ ∈ S 1 Fourier basis {1, cos θ, sin θ, cos 2θ, sin 2θ, . . .} z ∈ [0, ∞) weighted Laguerre polynomials e -z 2 {1, 1-z, z 2 -4z +2, . . .} z ∈ R$weighted Hermite polynomials e -z 2 4 {1, z, (z 2 -1), (z 3 -3z), . . .} sum of these basis functions, and by orthonormal, we mean that the basis functions satisfy

$ϕ k (z)ϕ k ′ (z) dz = 1 if k = k ′ , 0 otherwise,(1)$where the integral is over Z. Define the K th -order variational family Q K to be the set containing all distributions of the form

$q(z) = K k=1 α k ϕ k (z) 2 where K k=1 α 2 k = 1,(2)$and where α k ∈ R for k = 1, . . . , K are the parameters of the family Q K . In words, Q K contains all distributions that can be obtained by taking weighted sums of the first K basis functions and then squaring the result.

Eq. 2 involves a squaring operation, a sum-of-squares constraint, and a weighted sum. The squaring operation ensures that the density functions in Q K are nonnegative (i.e., with q(z) ≥ 0 for all z ∈ Z), while the sum-of-squares constraint ensures that they are normalized:

$q(z) dz = K k=1 α k ϕ k (z) 2 dz = K k,k ′ =1 α k α k ′ ϕ k (z)ϕ k ′ (z) dz = K k=1 α 2 k = 1.(3)$The weighted sum in Eq. 2 bears a superficial similarity to a mixture model, but note that neither the basis functions ϕ k (z) nor the weights α k in Eq. 2 are constrained to be nonnegative. Distributions of this form arise naturally in physics from the quantum-mechanical wave functions that satisfy Schrödinger's equation [[16]](#b15). In that setting, though, it is typical to consider complex-valued weights and basis functions, whereas here we only consider real-valued ones.

The simplest examples of orthogonal function expansions arise in one dimension. For example, functions on the interval [-1, 1] can be represented as weighted sums of Legendre polynomials, while functions on the unit circle can be represented by Fourier series of sines and cosines; see Table [1](#tab_0). Distributions on unbounded intervals can also be represented in this way. On the real line, for example, we may consider approximations of the form in Eq. 2 where

$ϕ k+1 (z) = √ 2πk! -1 2 e -1 2 z 2 1 2 H k (z),(4)$and H k (z) are the probabilist's Hermite polynomials given by

$H k (z) = (-1) k e z 2 2 d k dz k e -z 2 2 . (5$$)$Note how the lowest-order basis function ϕ 1 (z) in this family gives rise (upon squaring) to a Gaussian distribution with zero mean and unit variance.

Figure [1](#fig_1) shows how various multimodal distributions with one-dimensional support can be approximated by computing weighted sums of basis functions and squaring their result. We emphasize that the more basis functions in the sum, the better the approximation.

Orthogonal function expansions in one dimension are also important because their Cartesian products can be used to generate orthogonal function expansions in higher dimensions. For example, we can  2 and Table 1. approximate distributions over (say) R 3 by

$q(z 1 , z 2 , z 3 ) =   K1 i=1 K2 j=1 K3 k=1 β ijk ϕ i (z 1 )ϕ j (z 2 )ϕ k (z 3 )   2 where ijk β 2 ijk = 1,(6)$where β ijk ∈ R now parametrize the family. Note that there are a total K 1 K 2 K 3 parameters in the above expansion, so that this method of Cartesian products does not scale well to high dimensions if multiple basis functions are used per dimension. Note that the same strategy can also be used for random variables of mixed type: for example, from Table [1](#tab_0), we can create a variational family of distributions over R×[-1, 1]×[0, ∞) from the Cartesian product of orthogonal function expansions involving Hermite, Legendre, and Laguerre polynomials.

As shown in Figure [1](#fig_1), the approximating distributions from K th -order expansions can model the presence of multiple modes as well as many types of asymmetry, and this expressiveness also extends to higher dimensions. Nevertheless, it remains tractable to sample from these distributions and even to calculate (analytically) their low-order moments, as we show in Appendices A and B.

For concreteness, consider the distribution over R 3 in Eq. 6. The marginal distribution q(z 1 ) is

$q(z 1 ) = q(z 1 , z 2 , z 3 ) dz 2 dz 3 = ii ′   jk β ijk β i ′ jk   ϕ i (z 1 )ϕ i ′ (z 1 ),(7)$and from this expression, moments such as

E[z 1 ] and Var[z 1 ] can be calculated by evaluating integrals involving the elementary functions in Table 1. (In practice, these integrals are further simplified by recursion relations that relate basis functions of different orders; we demonstrate how to compute the first two moments for the normalized Hermite family in Eqs. B.19 and B.22.)

To generate samples {z (t) }, each dimension is sampled as follows: we draw z (t)

1 ∼ q(z 1 ) by computing the cumulative distribution function (CDF) of this marginal distribution and then numerically inverting this CDF. Finally, extending these ideas, we can calculate higher-order moments and obtain joint samples via the nested draws

$z (t) 1 ∼ q(z 1 ), z (t) 2 ∼ q(z 2 | z 1 ), z (t) 3 ∼ q(z 3 | z 1 , z 2 ).(8)$The overall complexity of these procedures scales no worse than quadratically in the number of basis functions in the expansion. These extensions are discussed further in Appendices A and B.

## EigenVI

In variational inference, we posit a parameterized family of approximating distributions and then compute the particular approximation in this family that is closest to a target distribution of interest. Eq. 2 constructs a variational family Q K from the orthogonal functions {ϕ k (z)} K k=1 whose variational parameters are the weights {α k } K k=1 . We now derive EigenVI, a method to find q ∈ Q K that is close to the target distribution p(z).

We first define the measure of closeness that we will minimize. EigenVI measures the quality of an approximate density by the Fisher divergence [[18]](#b17),

$D(q, p) = ∥∇ log q(z) -∇ log p(z)∥ 2 q(z)dz,(9)$where ∇ log q(z) and ∇ log p(z) are the score functions of the variational approximation and target, respectively. Suppose that q and p have the same support; then the Fisher divergence vanishes if and only if the scores of q and p are everywhere equal.

Though p is, by assumption, intractable to compute, in many applications it is possible to efficiently compute the score ∇ log p at any point z ∈ Z. For example, in Bayesian models the score of the target posterior is equal to the gradient of the log joint. This observation is the main motivation for score-based methods in probabilistic modeling [[6,](#b5)[31,](#b30)[37,](#b36)[48](#b47)].

Here we seek the q ∈ Q K that minimizes D(q, p). But now a challenge arises: it is generally difficult to evaluate the integral for D(q, p) in Eq. 9, let alone to minimize it as a function of q. While it is possible to sample from the distribution q, it is not straightforward to simultaneously sample from q and optimize over the variational parameters {α k } K k=1 in terms of which it is defined. Instead, we construct an unbiased estimator of D(q, p) by importance sampling, which also decouples the sampling distribution from the optimization. Let {z 1 , z 2 , . . . z B } denote a batch of B samples drawn from some proposal distribution π on Z. From these samples we can form the unbiased estimator

$D π (q, p) = B b=1 q(z b ) π(z b ) ∇ log q(z b ) -∇ log p(z b ) 2 . (10$$)$This estimator should be accurate for appropriately broad proposal distributions and for sufficiently large batch sizes. We can therefore attempt to minimize Eq. 10 in place of Eq. 9.

Now we show that the minimization of Eq. 10 over q ∈ Q K simplifies to a minimum eigenvalue problem for the weights {α k } K k=1 . To obtain the eigenvalue problem, we substitute the orthogonal function expansion in Eq. 2 into Eq. 10 for the unbiased estimator of D(q, p). As an intermediate step, we differentiate Eq. 2 to obtain the scores

$∇ log q(z b ) = 2 k α k ∇ϕ k (z b ) k α k ϕ k (z b ) . (11$$)$Further substitution of the scores provides the key result behind our approach: the unbiased estimator in Eq. 10 is a simple quadratic form in the weights α := [α 1 , . . . , α K ] ⊤ of the orthogonal function expansion,

$D π (q, p) = α ⊤ M α,(12)$where the coefficients of the quadratic form are given by

$M jk = B b=1 1 π(z b ) 2∇ϕ j (z b ) -ϕ j (z b )∇ log p(z b ) • 2∇ϕ k (z b ) -ϕ k (z b )∇ log p(z b ) . (13$$)$Note that the elements of the K ×K symmetric matrix M capture all of the dependence on the batch of samples {z b } B b=1 , the scores of p and q at these samples, and the choice of the family of orthogonal functions. Next we minimize the quadratic form in Eq. 12 subject to the sum-of-squares constraint

$k α 2 k = 1 in Eq. 2.$In this way we obtain the eigenvalue problem [[8]](#b7) min

$q∈Q K D π (q, p) = min ∥α∥=1 α ⊤ M α =: λ min (M ),(14)$where λ min (M ) is the minimal eigenvalue of M , and the optimal weights are given (up to an arbitrary sign) by its corresponding eigenvector; see Appendix C for a proof. EigenVI solves Eq. 14.

We note that the eigenvalue problem in EigenVI arises from the curious alignment of three particular choices-namely, (i) the choice of variational family (based on orthogonal function expansions), (ii) the choice of divergence (based on score-matching), and (iii) the choice of estimator for the divergence (based on importance sampling). The simplicity of this eigenvalue problem stands in contrast to the many heuristics of gradient-based optimizations-involving learning rates, terminating criteria, and perhaps other algorithmic hyperparameters-that are typically required for ELBO-based BBVI [[10,](#b9)[11]](#b10). But EigenVI is also not entirely free of heuristics; to compute the estimator in Eq. 10 we must also specify the proposal distribution π and the number of samples B; see Appendix D for a discussion.

The size of the eigenvalue problem in EigenVI is equal to the number of basis functions K in the orthogonal function expansion of Eq. 2. The eigenvalue problem also generalizes to orthogonal function expansions that are formed from Cartesian products of one-dimensional families, but in this case, if multiple basis functions are used per dimension, then the overall basis size grows exponentially in the dimensionality. Thus, for example, the eigenvalue problem would be of size K 1 K 2 K 3 for the approximation in Eq. 6, as can be seen by "flattening" the tensor of weights β in Eq. 6 into the vector of weights α = vec(β) in Eq. 2. Finally, we note that EigenVI only needs to compute the minimal eigenvector of M in Eq. 14, and therefore it can benefit from specialized routines that are much less expensive than a full diagonalization.

## EigenVI in R D : the Hermite family and standardization

We now discuss the specific case of EigenVI for Z = R D with the Hermite-based variational family in Eq. 4. For this case, we propose a transformation of the domain that serves to precondition or standardize the target distribution before applying EigenVI. While this standardization is not required to use EigenVI, it helps to reduce the number of basis functions needed to approximate the target, leading to a more computationally efficient procedure. It also suggests natural default choices for the proposal distribution π in Eq. 10.

Recall that the eigenvalue problem grows linearly in size with the number of basis functions. Before applying EigenVI, our goal is therefore to transform the domain in a way that reduces the number of basis functions needed for a good approximation. To meet this goal for distributions over R D , we observe that the lowest-order basis function of the Hermite family in Eq. 4 yields (upon squaring) a standard multivariate Gaussian, with zero mean and unit covariance. Intuitively, we might expect the approximation of EigenVI to require fewer basis functions if the statistics of the target distribution nearly match those of this lowest-order basis function. The goal of standardization is to achieve this match, to whatever extent possible, by a suitable transformation of the underlying domain. Having done so, EigenVI in R D can then be viewed as a systematic framework to model non-Gaussian effects via a small number of higher-order terms in its orthogonal function expansion.

Concretely, we consider a linear transformation of the domain:

$z = Σ -1 2 (z-µ),(15)$where µ and Σ are estimates of the mean and covariance obtained from some other algorithm (e.g., a Laplace approximation, Gaussian variational inference, Monte Carlo, or domain-specific knowledge).

We then apply the EigenVI to fit a Kth-order variational approximation q(z) to the target distribution p(z) that is induced by this transformation; afterwards, we reverse the change-of-variables to obtain the final approximation to p(z), i.e.,

$q(z) = q(z)|Σ| -1/2 . (16$$)$Figure [2](#) shows why it is more difficult to approximate distributions that are badly centered or poorly scaled. The left panel shows the effect of translating a standard Gaussian away from the origin and shrinking its variance; note how a comparable approximation to the uncentered Gaussian now requires a 16th-order expansion. On the other hand, after standardization, the target can be perfectly fitted by the base distribution in the orthogonal family of reweighted Hermite polynomials. The right panel shows the similar effect of translating the mixture distribution in Figure [1](#fig_1) (right panel); comparing these panels, we see that twice as many basis functions (K = 14 versus K = 7) are required to provide a comparable fit of the uncentered mixture.

Finally, we note another benefit of standardizing the target before fitting EigenVI; when the target has nearly zero mean and unit covariance, it becomes simpler to identify natural choices for the proposal distribution π. Intuitively, in this case, we want a proposal distribution that has the same mean but heavier tails than a standard Gaussian. In our experiments, we use two types of centered proposal distributions-uniform and isotropic Gaussian-whose variances are greater than one. Figure [2](#): Higher-order expansions may be required to approximate target distributions (black) that are not standardized. Left: approximation of a non-standardized Gaussian. Right: approximation of the mixture distribution in Figure [1](#fig_1) after translating its largest modes away from the origin.

## Related work

Several recent works have considered BBVI methods based on score-matching. These methods take a particularly simple form for Gaussian variational families [[6,](#b5)[37]](#b36). The Fisher divergence [[18]](#b17) has been previously studied as a divergence for variational inference [[47]](#b46). Yu and Zhang [[48]](#b47) propose minimizing a Fisher divergence for semi-implicit (non-Gaussian) variational families; the divergence is minimized using gradient-based optimization. In another line of work, Zhang et al. [[50]](#b49) consider variational families of energy-based models and derive a closed-form solution to minimize the Fisher divergence in this setting.

More generally, there have many studies of VI with non-Gaussian variational families. One common extension is to consider families of mixture models [[14,](#b13)[17,](#b16)[36]](#b35); these are typically optimized via ELBO maximization. BBVI algorithms have also been derived for more expressive variational families of energy-based models [[9,](#b8)[22,](#b21)[28,](#b27)[29,](#b28)[52,](#b51)[53]](#b52) and normalizing flows [[3,](#b2)[24,](#b23)[25,](#b24)[34,](#b33)[39,](#b38)[41]](#b40). However the performance of these models, especially the normalizing flows, is often sensitive to the hyperparameters of the flow architecture and optimizer, as well as the parameters of the base distribution [[2,](#b1)[11]](#b10). Other aspects of these variational approximations are also less straightforward; for example, one cannot compute their low-order moments, and one cannot easily evaluate or draw samples from the densities of energy-based models.

The variational approximation in EigenVI is based on the idea of squaring a weighted sum of basis functions. Probability distributions of this form arise most famously in quantum mechanics [[16]](#b15). This idea has also been used to model distributions in machine learning, though not quite in the way proposed here. Novikov et al. [[38]](#b37) propose a tensor train-based model for density estimation, but they do not consider orthogonal basis sets. Similarly, Loconte et al. [[33]](#b32) obtain distributions by squaring a mixture model with negative weights, and they study this model in conjunction with probabilistic circuits. By contrast in this work, we consider this idea in the context of variational inference, and we focus specifically on the use of orthogonal function expansions, which have many simplifying properties; additionally, the specific objective we optimize leads to a minimum eigenvalue problem.

## Experiments

We evaluate EigenVI on 9 synthetic targets and 8 real data targets. In these experiments we use the orthogonal family induced by normalized Hermite polynomials (see Table [1](#tab_0)), whose lowest-order expansion is Gaussian. Thus, this variational family can model non-Gaussian behavior with the higher-order functions in its basis. We first study 2D synthetic targets and use them to demonstrate the expressiveness of these higher-order expansions. Next, we experiment with target distributions where we systematically vary the tail heaviness and amount of skew. Finally, we apply EigenVI to a set of hierarchical Bayesian models from real-world applications and benchmark its performance against other Gaussian BBVI algorithms.

## 2D synthetic targets

We first demonstrate how higher-order expansions of the variational family yield more accurate approximations on a range of 2D non-Gaussian target distributions (Figure [3](#fig_3)); see Appendix E. ), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3-5), where

$K = K 1 K 2 .$for details. We report an estimate of KL(p; q) above each variational approximation. The Gaussian variational approximation is fit using batch and match VI [[6]](#b5), which minimizes a score-based divergence. For EigenVI, the target distributions were not standardized before fitting EigenVI (we compare the costs of the methods in Figure E.1), and the total number of basis functions is

$K = K 1 K 2 .$
## Non-Gaussianity: varying skew and tails in the sinh-arcsinh distribution

We now consider the sinh-arcsinh normal distribution [[19,](#b18)[20]](#b19), which is induced by transforming a multivariate Gaussian using parameters that control the amount of skew and the weight of the tails. We construct several targets (D = 2, 5) of increasing amounts of non-Gaussianity in the skew or the tails of the distribution, and we refer to these targets as slight skew and tails, more skew and tails, and slight skew and heavier tails; see Appendix E.3 for details. In Figure [4a](#fig_7), we visualize the 2D targets and the EigenVI fits along with their forward KLs. Before applying EigenVI, we standardize the target using a mean and covariance estimated from batch and match VI [[6]](#b5). In Figure [4b](#fig_7), we measure the EigenVI forward KL under varying numbers of samples B and across increasing numbers of basis functions, given by K = D d=1 K d . We also present the forward KL resulting from batch and match VI (BaM) and automatic differentiation VI (ADVI), which both use Gaussian variational families and are run using the same budget in terms of number gradient evaluations. Next we consider similar targets with D = 5, which are visualized in in Figure E.2, along with the resulting EigenVI variational approximations. In Figure [4c](#fig_7), we observe greater differences in the number of importance samples needed to lead to good approximations, especially as the number of basis functions increase.

## Hierarchical modeling benchmarks from posteriordb

We now evaluate EigenVI on a set of hierarchical Bayesian models [[7,](#b6)[35,](#b34)[42]](#b41), which are summarized in Table [E](#).1. The goal is posterior inference: given data observations x 1:N , the posterior of z is

$p(z | x 1:N ) ∝ p(z)p(x 1:N | z) =: ρ(z),(17)$where p(z) is the prior and p(x 1:N | z) denotes the likelihood.

We compare EigenVI to 1) automatic differentation VI (ADVI) [[27]](#b26), which maximizes the ELBO over a full-covariance Gaussian family (ADVI), 2) Gaussian score matching (GSM) [[37]](#b36), a score-based BBVI approach with a full-covariance Gaussian family, and 3) batch and match VI (BaM) [[6]](#b5), which   In these models, we do not have access to the target distribution, p(z | x 1:N ), only the unnormalized target ρ. Thus, we cannot evaluate an estimate of the forward KL. Instead, to evaluate the fidelity of the fitted variational distributions, we compute the empirical Fisher divergence using reference samples from the posterior obtained via Hamiltonian Monte Carlo (HMC):

$1 S S s=1 ∥∇ log ρ(z s ) -∇ log q(z s )∥ 2 , z s ∼ p(z | x 1:N ).(18)$Note that this measure is not the objective that EigenVI minimizes; it is analogous to the forward KL divergence, as the expectation is taken with respect to p. We report the results in Figure [5](#fig_9), computing the Fisher divergence for EigenVI with increasing numbers of basis functions. We typically found that with more basis functions, the scores becomes closer to that of the target.

Finally, we provide a qualitative comparison with real-NVP normalizing flows (NFs) [[12]](#b11), a flexible variational family that is fit by minimizing the reverse KL. We found that after tuning the batch-size and learning rate, NFs generally had a suitable fit. We visualize the posterior marginals for a subset of dimensions from 8schools in the top three rows, comparing EigenVI, the NF, and BaM. Here, we observe that the Gaussian struggles to fit the tails of this target distribution. On the other hand, EigenVI provides a competitive fit to the normalizing flow. In Appendix E. 

## Discussion of limitations and future work

In this work, we introduced EigenVI, a new approach for score-based variational inference based on orthgonal function expansions. The score-based objective for EigenVI is minimized by solving an eigenvalue problem, and thus this framework provides an alternative to gradient-based methods for BBVI. Importantly, many computations in EigenVI can be parallelized with respect to the batch of  samples, unlike in iterative methods. We applied EigenVI to many synthetic and real-world targets, and these experiments show that EigenVI provides a principled way of improving upon Gaussian variational families.

Many future directions remain. First, the approach described in this paper relies on importance sampling, and thus it may benefit from more sophisticated methods for adaptive importance sampling. Second, it may be useful to construct variational families from different orthogonal function expansions. Our empirical study focused on the family built from normalized Hermite polynomials. But this family may require a very high-order expansion to model highly non-Gaussian targets, and such an expansion will be very expensive in high dimensions. Though this family was sufficient for many of the targets we simulated, others will be crucial for modeling highly non-Gaussian targets.

Another direction is to develop variational families whose orthogonal function expansions scale more favorably with the dimension, perhaps by incorporating low rank structure in the target's covariance. Finally, it would be interesting to explore iterative versions of EigenVI in which each iteration solves a minimum eigenvalue problem on some subsample of data points. With such an approach, EigenVI could potentially be applied to very large-scale problems in Bayesian inference.

Next we consider how to implement this procedure efficiently in practice, and in particular, how to calculate the definite integral for the CDF in Eq. A.8. As shorthand, we define the doubly-indexed set of real-valued functions

$Φ kℓ (ξ) = ξ -∞ ϕ k (z)ϕ ℓ (z) dz. (A.11)$It follows from orthogonality that Φ kl (+∞) = δ kl and from the Cauchy-Schwartz inequality that |Φ kℓ (ξ)| ≤ 1 for all ξ ∈ R. Our interest in these functions stems from the observation that

$C(ξ) = K k,ℓ=1$S kℓ Φ kl (ξ) = trace[SΦ(ξ)], (A.12) so that if we have already computed the functions Φ kℓ (ξ), then we can use Eq. A.12 to compute the CDF whose inverse we need in Eq. A.10. In practice, we can use numerical quadrature to pre-compute Φ kℓ (ξ) for many values along the real line and then solve Eq. A.10 quickly by interpolation; that is, given u, we find ξ satisfying trace[SΦ(ξ)] = u. The result is an unbiased sample drawn from the density ρ(ξ) in Eq. A.6.

## Sequential sampling

Finally we show that each draw in Eqs. A.3-A.5 reduces to the problem described above. As in Section 2.1, we work out the steps specifically for an example in D = 3, where we must draw the samples z 1 ∼ q(z 1 ), z 2 ∼ q(z 2 |z 1 ) and z 3 ∼ q(z 3 |z 1 , z 2 ). This example illustrates all the ideas needed for the general case but with a minimum of indices.

Consider the joint distribution given by

$q(z 1 , z 2 , z 3 ) =   K1 i=1 K2 j=1 K3 k=1 β ijk ϕ i (z 1 )ϕ j (z 2 )ϕ k (z 3 )   2 where ijk β 2 ijk = 1. (A.13)$From this joint distribution, we can compute marginal distributions by integrating out subsets of variables, and each integration over R gives rise to a contraction of indices, as in Eq. 7, due to the property of orthogonality. In particular, expanding the square in Eq. A.13, we can write this joint distribution as

$q(z 1 , z 2 , z 3 ) = K3 k,k ′ =1   K1 i,i ′ =1 K2 j,j ′ =1 β ijk β i ′ j ′ k ′ ϕ i (z 1 )ϕ i ′ (z 1 )ϕ j (z 2 )ϕ j ′ (z 2 )   ϕ k (z 3 )ϕ k ′ (z 3 ),$(A.14) and we can then contract the index k ′ when integrating over z 3 , since ϕ k (z 3 )ϕ k ′ (z 3 )dz 3 = δ kk ′ .

In this way we find that the marginal distributions are

$q(z 1 , z 2 ) = K2 j,j ′ =1   K1 i,i ′ =1 K3 k=1 β ijk β i ′ j ′ k ϕ i (z 1 )ϕ i ′ (z 1 )   ϕ j (z 2 )ϕ j ′ (z 2 ), (A.15) q(z 1 ) = K1 i,i ′ =1   K2 j=1 K3 k=1 β ijk β i ′ jk   ϕ i (z 1 )ϕ i ′ (z 1 ). (A.16)$Now note from the brackets in Eq. A.16 that this marginal distribution is already in the quadratic form of Eq. A.6 with coefficients

## S

(1)

$ii ′ = K2 j=1 K3 k=1 β ijk β i ′ jk . (A.17)$From this first quadratic form, we can therefore use inverse transform sampling to obtain a draw z 1 ∼ q(z 1 ).

which simply encapsulate the bracketed term in Eq. B.6. Note that there are K 2 1 of these coefficients, each of which can be computed in O(K 2 K 3 . . . K D ). With this shorthand, we can write

$E q [z 1 ] = K1 i,j=1 A ij µ ij , (B.8) E q [z 2 1 ] = K1 i,j=1 A ij ν ij , (B.9)$where µ ij and ν ij are the integrals defined in Eqs. B.2-B.3. Thus the problem has been reduced to a weighted sum of one-dimensional integrals.

A similar calculation gives the result we need for correlations. Again, without loss of generality, we focus on calculating E q [z 1 z 2 ]. Analogous to Eq. B.7, we define the tensor of coefficients

$B ijkℓ = K3 k3=1 • • • K D k D =1 α ikk3...k D α jℓk3...k D , (B.10)$which arises from marginalizing over the variables (z 3 , z 4 , . . . , z D ). There are K 2 1 K 2 2 of these coefficients, each of which can be computed in O(K 3 K 4 . . . K D ). With this shorthand, we can write

$E q [z 1 z 2 ] = K1 i,j=1 K2 k,ℓ=1 B ijkℓ µ ij µ kℓ . (B.11)$where µ ij is again the integral defined in Eq. B.2). Thus the problem has been reduced to a weighted sum of (the product of) one-dimensional integrals.

Finally, we show how to evaluate the integrals in Eqs. B.2-B.3 for the specific case of orthogonal function expansions with weighted Hermite polynomials; similar computations apply in the case of Legendre polynomials. Recall in this case that

$ϕ k+1 (z) = √ 2πk! -1 2 e -1 2 z 2 1 2$H k (z), (B.12)

where H k (z) are the probabilist's Hermite polynomials given by

$H k (z) = (-1) k e z 2 2 d k dz k e -z 2 2 . (B.13)$To evaluate the integrals for this particular family, we can exploit the following recursions that are satisfied by Hermite polynomials:

$H k+1 (z) = zH k (z) -H ′ k (z), (B.14) H ′ k (z) = kH k-1 (z). (B.15)$Eliminating the derivatives H ′ k (z) in Eqs. B.14-B.15, we see that zH k (z) = H k+1 (z) + kH k-1 (z). We can then substitute Eq. B.12 to obtain a recursion for the orthogonal basis functions themselves:

$zϕ k (z) = √ kϕ k+1 (z) + √ k-1ϕ k-1 (z). (B.16)$With the above recursion, we can now read off these integrals from the property of orthogonality. For example, starting from Eq. B.2, we find that

$µ ij = ∞ -∞ ϕ i (z)ϕ j (z) z dz, (B.17) = ∞ -∞ ϕ i (z) jϕ j+1 (z) + j -1ϕ j-1 (z) dz, (B.18) = δ i,j+1 j + δ i,j-1 √ i, (B.19)$where δ ij is the Kronecker delta function. Next we consider the integral in Eq. B.3, which involves a power of z 2 in the integrand. In this case we can make repeated use of the recursion:

$ν ij = ∞ -∞ ϕ i (z)ϕ j (z) z 2 dz, (B.20) = ∞ -∞ √ iϕ i+1 (z) + √ i-1ϕ i-1 (z) jϕ j+1 (z) + j -1ϕ j-1 (z) dz, (B.21) = δ ij ij + (i-1)(j -1) + δ i-1,j+1 j(j +1) + δ j-1,i+1 i(i+1). (B.22)$Note that the matrices in Eq. B.19 and Eq. B.22 can be computed for whatever size is required by the orthogonal basis function expansion in Eq. B.1. Once these matrices are computed, it is a simple matter of substitution[foot_0](#foot_0) to compute the moments

$E q [z 1 ], E q [z 2 1 ]$, and E q [z 1 z 2 ] from Eqs. B.8-B.9 and Eq. B.11. Finally, we can compute other low-order moments (such as E q [z 5 ] or E q [z 3 z 7 ]) by an appropriate permutation of indices.

## C Eigenvalue problem

In this appendix we show in detail how the optimization for EigenVI reduces to a minimum eigenvalue problem. In particular we prove the following.

$Lemma C.1. Let {ϕ k (z)} ∞$k=1 be an orthogonal function expansion, and let q ∈ Q K be the variational approximation parameterized by

$q(z) = K k=1 α k ϕ k (z) 2 , (C.1)$where the weights satisfy K k=1 α 2 k = 1, thus ensuring that the distribution is normalized. Suppose furthermore that q is chosen to minimize the empirical estimate of the Fisher divergence given, as in eq. ( [10](#formula_11)), by

$D π (q, p) = B b=1 q(z b ) π(z b ) ∇ log q(z b ) -∇ log p(z b ) 2 .$Then the optimal variational approximation q in this family can be computed by solving the minimum eigenvalue problem

$min q∈Q K D π (q, p) = min ∥α∥=1 α ⊤ M α =: λ min (M ), (C.2)$where M is given in Eq. 13 and α = [α 1 , . . . , α K ] ∈ R K . The optimal weights α are given (up to an arbitrary sign) by the corresponding eigenvector of this minimal eigenvalue.

Proof. The scores of q in this variational family are given by

$∇ log q(z b ) = 2 k α k ∇ϕ k (z b ) k α k ϕ k (z b ) .$Substituting the above into the empirical divergence, we find that

$D π (q, p) = B b=1 q(z b ) π(z b ) ∇ log q(z b ) -∇ log p(z b ) 2 = B b=1 k α k ϕ k (z b ) 2 π(z b ) 2 k α k ∇ϕ k (z b ) k α k ϕ k (z b ) -∇ log p(z b ) 2 = B b=1 1 π(z b ) 2 k α k ∇ϕ k (z b ) - k α k ϕ k (z b ) ∇ log p(z b ) 2 = B b=1 1 π(z b ) k α k 2∇ϕ k (z b ) -ϕ k (z b )∇ log p(z b ) 2 = α ⊤ M α,$where M is given in ( [13](#formula_16)) and α = [α 1 , . . . , α K ] ∈ R K . Thus the optimal weights α are found by minimizing the quadratic form α ⊤ M α subject to the constraint α ⊤ α = 1. Equivalently, a solution can be found by minimizing the Rayleigh quotient

$argmin v v ⊤ M v v ⊤ v (C.3)$and setting α = v/∥v∥. It then follows from the Rayleigh-Ritz theorem [[8]](#b7) for symmetric matrices that α is the eigenvector corresponding to the minimal eigenvalue of M , and this proves the lemma.

## D Practical considerations of EigenVI D.1 EigenVI vs gradient-based BBVI

Recall that EigenVI has two hyperparameters: the number of basis functions K and the number of importance samples B. We note there is an important difference between these two hyperparameters and the learning rate in ADVI and other gradient-based methods. Here, as we use more basis functions and more samples, the resulting fit is a better approximation. So, we can increase the number of basis functions and importance samples until a budget is reached, or until the resulting variational approximation is a sufficient fit. On the other hand, tuning the learning rate in gradient-based optimization is much more sensitive because it cannot be too large or too small. If it is too large, ADVI may diverge. If the learning rate is too small, it may take too long to converge in which case it may exceed computational budgets.

Another fundamental difference in setting the number of basis functions as compared to the learning rate or batch size of gradient based optimization is that once we have evaluated the score of the target distribution for the samples, these same samples can be reused for solving the eigenvalue problem with any choice of the number of basis functions, as these tasks are independent. By contrast, in iterative BBVI, the optimization problem needs to be re-solved for every choice of hyperparameters, and the samples from different runs cannot be mixed together.

Furthermore, solving the eigenvalue problem is fast, and scores can be computed in parallel. In our implementation, we use off-the-shelf eigenvalue solvers, such as ARPACK [[30]](#b29) or Julia's eigenvalue decomposition function, eigen. In many problems with complicated targets, the main cost comes from gradient evaluation and not the eigenvalue solver.

## D.2 Choosing the number of samples B

Intuitively, if the target p is in the variational family Q (i.e., it can be represented using an order-K expansion), then we should choose the number of samples B to roughly equal the number of basis function K. If p is very different from Q, we need more samples, and in our experiments, we use a multiple of the number of basis functions (say of order 10). As discussed before, once we have evaluated a set of scores, these can be reused to fit a larger number of basis functions.

K 2 = K values used, i.e., 3, 6, and 10 (resulting in a total number of basis functions of 3 2 , 6 2 , and 10 2 ). In the bottom row of the plot, we also show wall clock timings (computed without parallelization) to show how the cost grows with the increase in the number of basis functions and importance samples. The horizontal dotted line denotes the result from batch and match VI, which fits a Gaussian via score matching; here a batch size of 16 was used and a learning rate of λ t = BD t+1 . The black star denotes the number of score evaluations used by the Gaussian VI method.

## E.3 Sinh-arcsinh targets

The sinh-arcsinh normal distribution [[19,](#b18)[20]](#b19) has parameters s ∈ R D , τ ∈ R D + , Σ ∈ S ++ ; it is induced by transforming a Gaussian Z 0 ∼ N (0, Σ) to Z = S s,τ (Z 0 ), where We constructed 3 targets in 2 dimensions and 3 targets in 5 dimensions, each with varying amounts of non-Gaussianity. The details of each target are below. In all experiments, EigenVI was applied with standardization, where a Gaussian was fit using batch and match VI with a batch size of 16 and a learning rate λ t = BD t+1 . For all experiments, we used a proposal distribution π that was uniform on [-5, 5] 2 .  

## 2D sinh-arcsinh normal experiment

![Figure 1: Target probability distributions (black dashed curves) on the interval [-1, 1] (left), the unit circle (middle), and the real line (right), and their approximations by orthogonal function expansions from different families and of different orders; see Eq.2 and Table 1.]()

![Mixture target (translation of Figure1c)]()

![Figure3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3-5), where K = K 1 K 2 .]()

![Example 2D targets (left) varying the skew s or tail weight τ components and their EigenVI fits (right).]()

![Figure 4: Sinh-arcsinh normal distribution synthetic target. Panel (a) shows the three targets we consider in 2D, and their resulting EigenVI fit. Panel (b) shows measures KL(p; q) for D = 2, and panel (c) shows KL(p; q) for D = 5; the x-axis shows the number of basis functions, K = d K d . minimizes a regularized score-based divergence over a full-covariance Gaussian family. In these examples, we standardize the target using either GSM or BaM before applying EigenVI.]()

![we show the full corner plot in Figure E.3 and marginals of the garch11 model in Figure E.4.]()

![Batch and match VI with a full covariance Gaussian family 8-schools, D = 10]()

![Figure 5: Results on posteriordb models. Top three rows: marginal distributions of the even dimensions from 8-schools. Reference samples from HMC are outlined in gray, and the VI samples are in green. Bottom two rows: evaluation of methods with the (forward) Fisher divergence. The x-axis shows the number of basis functions, K = d K d . Shaded regions represent standard errors computed with respect to 5 random seeds.]()

![s,τ (z) := [S s1,τ1 (z 1 ), . . . , S s D ,τD (z D )] ⊤ , S s d ,τ d (z d ) := sinh 1 τ d sinh -1 (z d ) + s d τ d . (E.1)Here s d controls the amount of skew in the dth dimension, and τ d controls the tail weight in that dimension. When s d = 0 and τ d = 1 in all dimensions d, the distribution is Gaussian.The sinh-arcsinh normal distribution has the following density:p(z; s, τ, Σ) = [(2π) D |Σ|] -1 + z 2 d ) -1 2 τ d C s d ,τ d (z d ) exp -1 2 S s,τ (z) ⊤ Σ -1 S s,τ ,(E.2)where we define the functionsC s d ,τ d (z d ) := (1 + S 2 s d ,τ d (z))s d ,τ d (z d ) := sinh(τ d sinh -1 (z d ) -s d ), S s,τ (z) = [S s1,τ1 (z 1 ), . . . , S s D ,τ D (z D )] ⊤ . (E.4)]()

![Figure 4b), we consider the slight skew and tails target with parameters s = [0.2, 0.2], τ = [1.1, 1.1], the more skew and tails target with s = [0.2, 0.5], τ = [1.1, 1.1], and the slight skew and heavier tails with s = [0.2, 0.2], τ = [1.4, 1.1]. Note that s = [0, 0], τ = [1, 1] recovers the multivariate Gaussian. These three target are visualized in Figure 4a. 5D sinh-archsinh normal experiment We constructed three targets P 1 (slight skew and tails), P 2 (more skew and tails), and P 3 (slight skew and heavier tails) each with The skew and tail weight parameters used were: s 1 = [0., 0., 0.2, 0.2, 0.2]; τ 1 = [1., 1., 1., 1., 1.1], s 2 = [0.0, 0.0, 0.6, 0.4, -0.5]; τ 2 = [1., 1., 1., 1., 1.1], and s 3 = [0.2, 0.2, 0.2, 0.2, 0.2]; τ 3 = [1.1, 1.1, 1., 1.4, 1.6]. See Figure E.2 for a visualization of the marginals of each target distribution. In the second row, we show examples of resulting EigenVI fit (visualized using samples from q) from B = 20,000 and K = 10.]()

![Figure E.3: Comparison of EigenVI, normalizing flow, and Gaussian score-based BBVI methods on 8schools.23]()

![Examples of orthogonal function expansions in one dimension. The basis functions in the table are not normalized, but they can be rescaled so that their squares integrate to one.]()

With further bookkeeping, one can also exploit the sparsity of µij and νij to derive more efficient calculations of these moments.

