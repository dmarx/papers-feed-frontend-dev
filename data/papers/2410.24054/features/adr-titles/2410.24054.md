- Choice of using orthogonal function expansions for variational approximations
- Decision to minimize Fisher divergence instead of KL divergence
- Selection of basis functions (e.g., Hermite, Legendre, Laguerre)
- Approach to handle non-Gaussian distributions
- Method for calculating low-order moments of approximations
- Strategy for sampling from variational distributions
- Decision to avoid gradient-based optimization methods
- Choice of optimization technique (minimum eigenvalue problem)
- Handling of multimodal and asymmetric distributions
- Trade-off management between expressiveness and computational cost
- Use of importance sampling for estimating Fisher divergence
- Framework for evaluating performance on benchmark distributions
- Consideration of computational complexity in high dimensions
- Approach to constructing variational families for mixed-type random variables
- Decision to focus on Bayesian hierarchical models for evaluation
- Limitations and future work considerations in the design of EigenVI