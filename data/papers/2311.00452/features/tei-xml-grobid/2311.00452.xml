<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6444F39DCA5BE51E3A43DBC93E87D7D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-23T01:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study delves into the intricate dynamics of trained deep neural networks and their relationships with network parameters. Trained networks predominantly continue training in a single direction, known as the drift mode. This drift mode can be explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. We unveil a correlation between Hessian eigenvectors and network weights. This relationship, hinging on the magnitude of eigenvalues, allows us to discern parameter directions within the network. Notably, the significance of these directions relies on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extends to the decomposition of weight matrices through singular value decomposition. This approach proves practical in identifying critical directions within the Hessian, considering both their magnitude and curvature. Furthermore, our examination showcases the applicability of principal component analysis in approximating the Hessian, with update parameters emerging as a superior choice over weights for this purpose. Remarkably, our findings unveil a similarity between the largest Hessian eigenvalues of individual layers and the entire network. Notably, higher eigenvalues are concentrated more in deeper layers. Leveraging these insights, we venture into addressing catastrophic forgetting, a challenge of neural networks when learning new tasks while retaining knowledge from previous ones. By applying our discoveries, we formulate an effective strategy to mitigate catastrophic forgetting, offering a possible solution that can be applied to networks of varying scales, including larger architectures. This study is a step to uncover intricate behavior of deep neural networks and also provides practical solutions for enhancing their capabilities and addressing critical challenges. I would like to express my heartfelt gratitude to several individuals who have played instrumental roles in the completion of this thesis. First and foremost, I extend my sincere appreciation to Marcel Kühn, Matthias Thamm, and Max Staats. Their invaluable insights, thoughtful discussions, and unwavering support have been pivotal throughout this research journey. Without their guidance, this thesis would not have been possible. I am also deeply thankful to Professor Dr. Bernd Rosenow for his mentorship, and continuous encouragement. His expertise has been a guiding light in navigating the complexities of this study. Furthermore, I extend my appreciation to Mr. Thamm and Mr. Staats for their dedicated supervision, which has contributed significantly to the refinement of this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gottfried Wilhelm Leibniz foresaw the potential of solving complex mathematical problems through binary classifications of True or False, thus laying the groundwork for the first mechanical computer <ref type="bibr" target="#b33">[34]</ref>. As time passed, it became increasingly clear that computers easily surpassed humans in tasks that can be broken down into straightforward algorithms, consisting of a simple list of instructions. The key issue is that any problem we can formulate into an algorithm can be solved, given enough computational power and processing time. However, a significant challenge arose with this advancement: How to overcome obstacles that do not easily translate into algorithmic structures? Take, for example, the field of image classification. The task of differentiating between inanimate objects and living animals is a relatively straightforward one. This can be achieved by developing guidelines to check for the existence of specific attributes like eyes, noses, and fur, which are unique to animals and not present in stones. However, the task becomes more intricate when faced with the challenge of differentiating between cats and dogs. While dogs and cats share many similarities, it is difficult to pinpoint the exact distinguishing features that universally set all dogs apart from all cats. In response to this enigma, a bold solution emerged from an unexpected source: The process of translating these complex challenges into algorithmic frameworks, akin to learning, underwent a transformational shift, becoming an algorithm on its own that can be resolved computationally <ref type="bibr" target="#b11">[12]</ref>. By digitizing data and the desired solution for these enigmas, we applied the fundamental principles of derivatives -attributable to Leibniz's legacy <ref type="bibr" target="#b33">[34]</ref> -to autonomously simplify these complexities. For example, when analyzing pictures, we supply the computer with images of both dogs and cats and allow it to determine a way to differentiate between the two. This process encompasses various names, initially as Cybernetics, later evolving into Artificial Intelligence, and currently maturing as Deep Learning. The principles underpinning Deep Learning were not exclusively forged within the realm of scientific inquiry, but rather stem from the very fabric of nature itself. Our cognitive architecture, constituted by neurons intricately interconnected, forms a neural network that embodies the essence of learning. Analyzing the structure and functioning of our brain enabled us to decipher the algorithmic mechanisms of learning, which could then be harnessed and adapted to meet the computational requirements of our machines <ref type="bibr" target="#b30">[31]</ref>. As a consequence, the learning algorithms we employ bear the moniker of Deep Neural Networks (DNNs), a nomenclature that resonates with the intricate neural structure and functionality of our own minds. Yet, the profound irony remains: Just as we sometimes find it challenging to elucidate how our own brain navigates certain problems, providing the same explanations to our computational counterparts, as in the case of distinguishing between cats and dogs, proves to be equally formidable. In recent years, DNNs have emerged as a transformative force across diverse domains, achieving remarkable feats in various applications such as natural language translation like DeepL and Google Translator, conversational agents like ChatGPT and Bing Chat, and strategic game-playing in chess and go <ref type="bibr" target="#b34">[35]</ref>. These networks' extraordinary capabilities underscore their potential to reshape our technological landscape fundamentally. Yet, beneath their impressive performance lies a veil of mystery, as the decisions made by DNNs often remain opaque and elusive. Since we no longer can comprehend the instructions the computer finds, when searching for an algorithm <ref type="bibr" target="#b8">[9]</ref>. The intrinsic power of DNNs originates from their ability to model intricate relationships within complex data, converting raw inputs into meaningful predictions. These models, characterized by webs of interconnected parameters, raise fundamental questions about the interplay between network architecture, training dynamics, and task proficiency. Addressing these questions not only holds theoretical significance but also carries practical implications for enhancing the reliability, interpretability, and generalization of DNNs in real-world applications <ref type="bibr" target="#b0">[1]</ref>. Driven by the quest to unveil the inner workings of DNNs, researchers have pursued various approaches, each aimed at shedding light on distinct aspects of these models. Two of many paradigms warrant our attention: Random Matrix Theory (RMT) and Principal Component Analysis (PCA) of training dynamics. These complementary strategies offer unique insights into the behavior of DNNs and the factors driving their performance. RMT, rooted in the mathematical foundations of linear algebra, delves into the structure of DNNs' weight matrices. These matrices, emerging from the network's layered architecture and initialized with randomness, undergo change during training, giving rise to an interplay between deterministic and random components. RMT together with singular value decomposition can be used to discern meaningful patterns within these matrices, shedding light on the evolution of network parameters and identifying key components that drive the network's performance <ref type="bibr" target="#b26">[27]</ref>. In contrast, PCA of training dynamics delves into the trajectory of network updates as dictated by the Stochastic Gradient Descent (SGD), the workhorse optimization algorithm driving DNN training. By analyzing the development of parameter updates, this approach offers a window into the network's adaptation process and unveils directions of maximal change, which are intrinsically tied to critical information for task-solving <ref type="bibr" target="#b2">[3]</ref>. While both RMT and PCA have independently contributed to advancing our understanding of DNNs, a comprehensive comparative exploration that compares these approaches remains absent. Bridging the gap between the network parameters and training dynamics holds the potential to catalyze a more comprehensive behavior of DNN and yield insights with far-reaching ramifications. Beyond theoretical implications, these insights could pave the way for practical applications, such as combatting catastrophic forgetting <ref type="bibr" target="#b2">[3]</ref> and navigating the challenges posed by noisy data <ref type="bibr" target="#b36">[37]</ref>. In light of these considerations, this study embarks on an journey to untangle DNN behavior by synergistically employing RMT and PCA. Through a comparative analysis, we strive to elucidate the interplay between network architecture, training dynamics, and task performance. Our exploration not only enriches the theoretical discourse surrounding DNNs but also holds the promise of empowering practical solutions for the challenges that lie ahead. The subsequent chapters of this thesis are structured as follows: Chapter 2 introduces the theoretical and mathematical foundations of DNNs necessary for comprehending the subsequent sections. Chapter 3 explores PCA and its current state of research in the context of DNNs. Chapter 4 delves into RMT and its application to DNNs. Chapter 5 provides insights into the practical aspects of network realization, including network setups and datasets used. Chapter 6 presents our comparative analysis of PCA and RMT results in DNNs, along with an exploration of PCA properties in relation to the Hessian matrix. Chapter 7 shifts our focus to the Hessian matrix, its properties, its relationships with network weights, and its connection to RMT. Finally, we discuss our findings, offer suggestions for further research, and conclude this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Introduction to Deep Neural Networks</head><p>This chapter delves into modern Deep Neural Networks (DNNs) for image recognition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8]</ref>, which are designed to transform inputs, denoted as x (e.g., images), into outputs, represented by y (e.g., image labels), in order to classify images. Formally, the relationship is expressed as y = f (x, w), where w signifies the network's weights. Deep Neural Networks are structured in layers, which can be interpreted as transformations themselves. Let y (l) = f (l) (x (l) , w (l) ) be the output of layer l. The weights of a layer are a part of all weights. The input of a layer is commonly the output of the previous layer, i.e. x (l) = y (l-1) <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation Functions</head><p>Activation functions modify the output of each layer to introduce nonlinearity. A widely used activation function is the Rectified Linear Unit (ReLU) <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_0">y i (x) = max(x i , 0) . (2.1)</formula><p>Another important activation function is the softmax function:</p><formula xml:id="formula_1">y i (x) = e x i j e x j .</formula><p>(2.</p><p>2)</p><p>The softmax activation is commonly employed in the output layer for classification tasks, providing probabilities for each class. The class with the highest probability is the predicted label, often referred to as the top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The network's performance assessment and enhancement are facilitated through a loss function, typically employing the categorical cross-entropy loss:</p><formula xml:id="formula_2">L(x, z) = - i z i log(y i (x)) , (<label>2.3)</label></formula><p>where z i = 1 only for the correct output label of input x and 0 otherwise. This loss function is conceptually aligned with information entropy, rewarding accurate approximations of the training data's probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Gradient Descent</head><p>Let us consider datasets of inputs x i and their corresponding correct labels z i , where the upper index corresponds to a single sample of the dataset. When training the network, the loss function is computed for multiple training samples in a batch B k ≡ B k(t) :</p><formula xml:id="formula_3">l (k) (x, z) = 1 S i∈B k L(x (i) , z (i)</formula><p>) . (2.4) Batches are typically of size S ∈ [32, 512] [19]. The network updates its weights using Stochastic Gradient Descent (SGD) w(t + 1) = w(t) + v(t + 1), where t is a time step and:</p><formula xml:id="formula_4">v(t + 1) = -η∇ w l (k(t)) (w, x, z) + βv(t) ,<label>(2.5)</label></formula><p>where η &gt; 0 is the learning rate and β ∈ [0, 1) is the momentum. The term ∇ w l (k(t)) (w, x, z) corresponds to the gradient of the loss for a batch B k (t) with respect to the network's weights. The concept of stochasticity arises from the random sampling of batches and the variability introduced by this process.</p><p>A challenge that arises due to updating all weights from the last layer backwards is the vanishing gradient problem, wherein the gradients diminish in magnitude as the optimization process progresses, particularly affecting early layers, due to the chain rule of derivatives. This phenomenon hinders the learning of these layers and can be attributed to the multiplication of gradients during the chain rule process <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization</head><p>Regularization techniques play a critical role in preventing overfitting. One prevalent form is the L 2 regularization, adding λ||w|| 2 2 to the loss l (k) in Eq. <ref type="bibr">(2.5)</ref>. This technique discourages the network from focusing excessively on unimportant features, improving the model's generalization capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers</head><p>Various layer types are employed in contemporary DNN architectures <ref type="bibr" target="#b0">[1]</ref>, including convolutional layers, pooling layers, and dense layers. These layer types are mathematically characterized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Layers</head><p>Dense layers implement linear matrix multiplications on flattened inputs:</p><formula xml:id="formula_5">y = W x + b , (2.6)</formula><p>where y is the output, W is the weight matrix, x is the input, and b is the bias vector.</p><p>Flattening is applied to matrix inputs, converting them into vectors. These layers form a versatile foundation by linking each input to each output individually <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Layers</head><p>3-D convolutional layers involve convolutions of input X with a weight tensor W :</p><formula xml:id="formula_6">Y ijk = m∈I 1 n∈I 2 l∈I 3 W mnlk X i+m,j+n,l + B ijk , (<label>2.7)</label></formula><p>where</p><formula xml:id="formula_7">I 1 = [-d 1 , d 1 ], I 2 = [-d 2 , d 2 ], I 3 = [1, d 3 ]</formula><p>. d 1 and d 2 are the kernel sizes in the x and y directions, respectively, and d 3 corresponds to the input depth. These layers are particularly efficient for image recognition tasks, as they exploit local correlations among nearby pixels <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling Layers</head><p>Max Pooling layers, often employed after convolutional layers, increase translation invariance. They select the maximum value from neighboring inputs within a translational distance d:</p><formula xml:id="formula_8">Y ij = max m,n∈K X i+m,j+n , K = [-d, d] . (2.8)</formula><p>Max Pooling enhances robustness by retaining key features while reducing sensitivity to small input variations <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Normalization Layer</head><p>Batch normalization normalizes the input according to the rule:</p><formula xml:id="formula_9">y = γ x -⟨x⟩ k σ 2 k (x) + ϵ + β , (<label>2.9)</label></formula><p>where γ = 1, β = 0 are learnable parameters, ϵ is a small constant, ⟨x⟩ k is the mean of the input batch, and σ 2 k is its variance. The layer enhances training speed and testing accuracy by aligning input statistics with a standard distribution <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Layers</head><p>In very deep networks, it has been found useful to include shortcuts, called residuals, in the network to further increase the accuracy of the test data. If we denote one of the previously defined layers or multiple layers as a transformation, i.e., y (l) = f (l) (x (l) ), then the shortcut is as simple as: (l) .</p><formula xml:id="formula_10">y (l) = f (l) (x (l) ) + x</formula><p>(2.10)</p><p>For the residual to be well-defined, it is required that the dimension of x (l) and y (l)  match, or that we can propagate the input to match the dimension of the output, e.g. when adding to a higher order convolutional layer. While residual networks proved to generalize better than those without shortcuts and without increasing the number of parameters, computing the gradient of batches becomes more costly by increasing the number of floating point operations (FLOP) the network needs to perform greatly <ref type="bibr" target="#b7">[8]</ref>.</p><p>3 Dynamics in Deep Neural Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Loss Landscape and the Hessian Matrix</head><p>To comprehend network dynamics, it is useful to consider the Hessian matrix:</p><formula xml:id="formula_11">H ij = ∂ 2 L(w) ∂w i ∂w j (3.1)</formula><p>Several studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref> propose a theoretical similarity between eigenvectors of the Hessian matrix and the covariance matrix of mean gradients. This similarity is observed in successful networks with a high test accuracy. Following <ref type="bibr" target="#b12">[13]</ref>, it is posited that:</p><formula xml:id="formula_12">L(w) ≈ L min + (w -µ) T H 2 (w -µ) ,<label>(3.2)</label></formula><p>where L min represents the minimum of the loss function, and µ denotes the coordinates of the loss function's minimum in weight space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Principal Component Analysis</head><p>Consider a weight matrix flattened to a vector, denoted as w(t), for a specific layer or the entire network, measured over a sequence of T ≥ n discrete time steps t such that [0, T ] ⊂ N 0 → R n . The covariance matrix can be defined as:</p><formula xml:id="formula_13">Σ ij = ⟨w i w j -⟨w i ⟩⟨w j ⟩⟩ , (<label>3.3)</label></formula><p>where ⟨X(t)⟩ = 1 T T t=0 X(t). The covariance matrix quantifies the relationships between different components of the weight vector over time. The eigenvectors of the covariance matrix are termed principal components, represented as p i . Corresponding to these eigenvectors are their associated eigenvalues, denoted as σ 2 i :</p><formula xml:id="formula_14">Σp i = σ 2 i p i . (3.4)</formula><p>The eigenvectors furnish an orthonormal basis for the R n space, given the symmetry of Σ. Consequently, it becomes possible to express the weight matrix in this basis:</p><formula xml:id="formula_15">w(t) = n i=1 θ i (t)p i , θ i (t) := w(t) • p i . (3.5)</formula><p>It is noteworthy that the variance of θ(t) corresponds to the eigenvalue of the respective principal component. Using a decomposition into principal components allows to examine the dynamics of weights in a late training phase known as the exploration phase <ref type="bibr" target="#b2">[3]</ref>. During this phase, the generalization error does not improve significantly, and the training loss function changes gradually. The central notion is that the network is proximate to a minimum of L(w), and further training does not cause it to deviate substantially from this minimum. When the weights vary in the direction of the principal components, the loss function can be represented as:</p><formula xml:id="formula_16">L(δθ) i := L(w + δθp i ) . (3.6)</formula><p>This behavior is observed in <ref type="bibr" target="#b2">[3]</ref> to follow a potential well:</p><formula xml:id="formula_17">L(δθ) i ∝ δθ 2<label>(3.7)</label></formula><p>as δθ → 0. The eigendirections of the loss landscape may not align precisely with the principal components. Nonetheless, this loss function behavior holds true in all directions spanned by eigenvectors of the Hessian matrix with sufficiently large eigenvalues. Due to the observation of a quadratic potential well and by that the principal components and Hessian eigenbasis are supposedly close in being diagonal as shown in <ref type="bibr" target="#b12">[13]</ref>, the flatness F i of the minima of the principal components and the Hessian eigenvalues can be related to F -2 i ∝ h i . The eigenvalues of the principal components can then be linked to the eigenvalues of the Hessian matrix:</p><formula xml:id="formula_18">σ 2 i ∝ h α i , (<label>3.8)</label></formula><p>where σ 2 i are the eigenvalues of the principal components in descending order, and h i are the Hessian eigenvalues in descending order. Experimental findings of <ref type="bibr" target="#b2">[3]</ref> suggest that α ≈ 2, shown only for the flatness. In particular, <ref type="bibr" target="#b17">[18]</ref> shows that the empirical result of α is affected by measuring for only a short period of time and provides a theoretical relationship:</p><formula xml:id="formula_19">σ 2 i ∝      h i h i &lt; h cross const h i &gt; h cross ,<label>(3.9)</label></formula><p>where h cross := 3S</p><p>1-β ηN train , S is the batch size, β signifies the momentum, and N train is the number of training examples per epoch. The dependency of α on the corresponding eigenvalue highlights its non-constant nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Catastrophic Forgetting</head><p>Catastrophic forgetting emerges when a network learns multiple tasks independently, resulting in performance degradation on previously learned tasks. This issue stems from optimizing the loss function for one task, inadvertently neglecting others, and allowing their associated loss values to escalate. To mitigate this phenomenon, <ref type="bibr" target="#b2">[3]</ref> suggests weakly constraining the largest N lim principal components through a regularization term integrated into the loss function:</p><formula xml:id="formula_20">L cf (w) = λ cf N lim i=1 1 F 2 i ((w -μ1 ) • p i ) 2 , (<label>3.10)</label></formula><p>where λ cf denotes a positive real regularization constant, μ1 represents the weights of the network or layer at the end of training for the previous task that are close to a minimum µ 1 , and F i signifies the flatness in direction of the principal components, while p i corresponds to their associated principal component, computed after training for the previous task with the dataset of the previous task. It is noteworthy that due to Eq.(3.8), measuring N lim + 1 time steps is deemed sufficient to approximate the N lim the largest Hessian eigenvectors using principal components, rendering computation more feasible, if we assume that the principal components approximate the Hessian eigenvectors sufficiently. In <ref type="bibr" target="#b2">[3]</ref> this relation was applied to connect the regularization to the Hessian landscape model. We can use this relation backwards to convert the problem to one of the Hessian:</p><formula xml:id="formula_21">L cf (w) = λ cf N lim i=1 h i ((w -μ1 ) • h i ) 2 , (<label>3.11)</label></formula><p>where again λ cf denotes a positive real regularization constant, μ1 represents the weights of the network or layer at the end of training for the previous task that are close to a minimum µ 1 , and h i signifies the Hessian eigenvalues in descending order, while h i corresponds to their associated eigenvector, computed after training for the previous task with the dataset of the previous task. When all eigenvalues are considered, this equation exhibits similarity to Eq.(3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Random Matrix Theory and Singular Value Decomposition in Deep Neural Networks</head><p>The weight matrix W ∈ R N ×M of a given layer can be decomposed using singular value decomposition (SVD):</p><formula xml:id="formula_22">W = U ΣV T , Σ = diag(ν 1 , ..., ν N ) , (<label>4.1)</label></formula><p>where U and V are orthogonal matrices and ν 1 , ..., ν N are the singular values, arranged in decreasing order. These singular values are accompanied by their corresponding right singular vectors, which form the rows of V , and the left singular vectors, which are extracted from U . When elements of the weight matrix W ij adhere to a normal distribution W ij ∼ N (0, σ 2 mp ), an observation can be made for the limit N, M → ∞ with Q := N/M ∈ R ≥1 , which gives rise to a Marchenko-Pastur distribution (MP):</p><formula xml:id="formula_23">ρ(ν) =      Q 2πσ 2 mp λ (λ + -λ)(λ -λ -) if λ ∈ [λ -, λ + ] 0 otherwise , (<label>4.2)</label></formula><p>where</p><formula xml:id="formula_24">λ ± = σ 2 mp 1 ± 1 √ Q 2 [27]</formula><p>. For deep neural networks, it has been demonstrated that the behavior of singular values mirrors that of random matrices <ref type="bibr" target="#b26">[27]</ref>. Specifically, they adhere to the Marchenko-Pastur distribution (Eq.(4.2)). This behavior is attributed to their initialization as randomly distributed values, and due to the fact that the majority of weights undergo limited change during training. Among the scrutinized trained networks, a few singular values reside outside the bulk and encapsulate nearly all network information. This is highlighted by the fact that removing bulk singular values by setting them to zero does not detrimentally affect network accuracy <ref type="bibr" target="#b36">[37]</ref>. Singular values hold a close relationship with the eigenvalues of the PCA applied to the same matrix. The eigenvalues obtained from PCA are essentially the square of the singular values. In light of this connection, the distributions of the unfolded spacings between singular values exhibit a behavior akin to the Wigner surmise, which the RMT theory predicts. This surmise is captured by the equation:</p><formula xml:id="formula_25">p(s) = πs 2 e -πs 2 4 , (<label>4.3)</label></formula><p>where s = ξ n -ξ n+1 is the unfolded spacing, with ξ n are the singular values, such that the spacings are locally normalized <ref type="bibr" target="#b25">[26]</ref>. In a recent study <ref type="bibr" target="#b37">[38]</ref>, compelling evidence was presented demonstrating that the singular values of weight matrices in DNNs conform to the expectations set forth by the Wigner surmise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Deep Neural Network Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The primary dataset for comparison is CIFAR-10 <ref type="bibr" target="#b15">[16]</ref>, which consists of 10 classes including airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Each image has 32 × 32 pixels and three color values for red, green, and blue (RGB) from 0 to 255 (uint8), so each image is a tensor of shape <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3)</ref>. The dataset comprises 60000 images, with 6000 images per class. A validation set of 10000 images is used to estimate the generalization error. While CIFAR-10 serves as a benchmark for smaller networks, more modern, larger networks tend to achieve test accuracies beyond 99.5% <ref type="bibr" target="#b13">[14]</ref>, making comparisons challenging due to the high accuracy ceiling. For more complex networks, ImageNet <ref type="bibr" target="#b31">[32]</ref> is a significant benchmark. However, due to the large network size and long training times, it is not employed in this study. Instead, a simpler dataset, MNIST <ref type="bibr" target="#b22">[23]</ref>, is additionally used. It consists of 70000 grayscale images of handwritten digits with a 28 × 28 pixel resolution. This dataset is considered to be much simpler than CIFAR-10, since even small MLP networks can perform extremely well on this dataset. Nevertheless, it is historically an important dataset, is computationally extremely easy due to the small image size, and can make analysis more feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Networks Initialization</head><p>The networks' weights are initialized using random seeds. In contemporary networks, each layer is independently initialized using either a uniform or normal distribution. The Glorot initialization <ref type="bibr" target="#b3">[4]</ref>, also known as Xavier initialization, is used for uniform and normal distribution initialization. For the uniform distribution:</p><formula xml:id="formula_26">W (l) ∼ U - 6 n l + n l+1 , 6 n l + n l+1 , (<label>5</label></formula><p>.1) Name Dataset # Layers with Weights # Parameters Test Accuracy MLP 50 MNIST 3 42k 98.2% MLP 256 CIFAR-10 4 828k 44.2% LeNet CIFAR-10 7 137k 65.3% miniAlexNet CIFAR-10 8 1023k 74.9% ResNet 20 CIFAR-10 22 272k 68.1% Table 5.1: Summary of network types used in the study.</p><p>where W (l) ∈ R n l ×n l+1 represents the weights of the layer. For the normal distribution:</p><formula xml:id="formula_27">W (l) ∼ N 0, 2 n l + n l+1 . (5.2)</formula><p>All biases are initialized to zero. An alternative initialization used in ResNet <ref type="bibr" target="#b7">[8]</ref> does not show significant improvements over the Glorot initialization and is therefore omitted in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Types</head><p>Table <ref type="table">5</ref>.1 summarizes the network types used in this study, along with their key attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP 50</head><p>The Multilayer Perceptron (MLP) network consists solely of dense layers. ReLU activation is used for all layers except the last one, which employs the softmax activation function. The layer widths are [50, 50, 10]. Fig. <ref type="figure">5</ref>.1 provides a graphical representation of the network. The layer weights are initialized with Glorot uniform. The network is trained on the MNIST dataset for 200 epochs with a learning rate of 0.01. The loss used is cross entropy and the update rule is SGD with a batch size of 32. We do not use momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP 256</head><p>The network structure is [256, 128, 64, 10]. Fig. <ref type="figure">5</ref>.1 illustrates the network structure. The layer weights are initialized with Glorot uniform. The biases are set to zero. The network is trained on the CIFAR-10 dataset for 200 epochs with a learning rate schedule η(t) = 0.01×0.99 t , where the time steps t are in epochs, the final learning rate is η(T ) ≈ 0.00134. The loss used is the cross entropy and the update rule is the SGD with a batch size of 32. We do not use momentum. To compare the differences when using weight decay or not, we regularize all layers with a weight decay constant of 5 × 10 -4 or, if unregularized, set the constant to zero. After training, the network achieves a full training accuracy of 100% and a test accuracy of ≈ 44%, depending on the seed and whether regularization is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LeNet</head><p>The LeNet network from <ref type="bibr" target="#b22">[23]</ref> is a convolutional network optimized for the MNIST dataset. It can also be used for the CIFAR-10 dataset. The first layer is a convolutional layer with six filters and a kernel size of <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>, where the kernel is padded to preserve the shape of the input. That is, we sum the 5 closest input entries together and that we add zeros at the edges, such that we can still pad there. The next layer is a pooling layer with kernel size <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref>, where the kernel is padded so that the full kernel lies in the input matrix, resulting in a smaller output dimension. This is followed by a second convolutional layer with 16 filters and a kernel size of <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>. This is followed by a pooling layer with the same properties as before. Next, the outputs are flattened into dense layers of shape [120, 84, 10]. In Fig.</p><p>5.2 we can see a graphical representation of the network structure. All layers except the pooling layers, the flattening layer, and the last layer use a ReLU activation function. The final layer has a softmax activation function. Layer weights are InputLayer input: output: [(None, 32, 32, 3)] [(None, 32, 32, 3)] Conv2D relu input: output: (None, 32, 32, 3) (None, 32, 32, 6) MaxPooling2D input: output: (None, 32, 32, 6) (None, 16, 16, 6) Conv2D relu input: output: (None, 16, 16, 6) (None, 16, 16, 16) MaxPooling2D input: output: (None, 16, 16, 16) (None, 8, 8, 16) Flatten input: output: (None, 8, 8, 16) (None, 1024) Dense relu input: output: (None, 1024) (None, 120) Dense relu input: output: (None, 120) (None, 84) Dense softmax input: output: (None, 84) (None, 10) InputLayer input: output: [(None, 32, 32, 3)] [(None, 32, 32, 3)] Conv2D relu input: output: (None, 32, 32, 3) (None, 28, 28, 300) MaxPooling2D input: output: (None, 28, 28, 300) (None, 10, 10, 300) Conv2D relu input: output: (None, 10, 10, 300) (None, 8, 8, 150) MaxPooling2D input: output: (None, 8, 8, 150) (None, 3, 3, 150) Flatten input: output: (None, 3, 3, 150) (None, 1350) Dense relu input: output: (None, 1350) (None, 384) Dense relu input: output: (None, 384) (None, 192) Dense relu input: output: (None, 192) (None, 10) initialized with Glorot normal, and biases are set to zero. The network is trained on the CIFAR-10 dataset for 100 epochs with a learning rate schedule η(t) = 0.005 × 0.98 t , where the time steps t are in epochs, the final learning rate is η(T ) ≈ 0.00134. The loss used is the cross entropy and the update rule is the SGD with a batch size of 64. We use a momentum of β = 0.9. All layers are regularized with a weight decay of λ = 0.0001.</p><p>After training, the network reaches a full training accuracy of 100% and a test accuracy of ≈ 65.3%, depending on the seed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>miniAlexNet</head><p>AlexNet <ref type="bibr" target="#b16">[17]</ref> is a convolutional network similar to LeNet, but has many more parameters and is optimized for the ImageNet dataset. Its smaller variant, the miniAlexNet or in <ref type="bibr" target="#b39">[40]</ref> called small AlexNet, is instead optimized for CIFAR-10 and is used in this work because the original AlexNet is too large for our analysis. The first layer is a convolutional layer with 300 filters and a kernel size of <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>, where the kernel is padded so that the full kernel lies in the input matrix. The next layer is a pooling layer with kernel size <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>, where the kernel is padded to preserve the shape of the input. This is followed by a second convolutional layer with 150 filters and a kernel size of <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>. This is followed by a pooling layer with the same properties as before. Next, the outputs are flattened into dense layers of shape <ref type="bibr">[384,</ref><ref type="bibr">192,</ref><ref type="bibr" target="#b9">10]</ref>. In Fig. <ref type="figure">5</ref>.2 we can see a graphical representation of the network structure. All layers except the pooling layer, the flattening layer, and the last layer have a ReLU activation function. The last layer has a softmax activation function. Layer weights are initialized with Glorot uniform. Biases are set to zero. The network is trained on the CIFAR-10 dataset for 100 epochs with a learning rate schedule η(t) = 0.01 × 0.95 t , where time steps t are in epochs, the final learning rate is η(T ) ≈ 5.9 × 10 -5 . The loss used is the cross entropy and the update rule is the SGD with a batch size of 32. We use a momentum of β = 0.9. All dense layers are regularized with a weight decay of λ = 0.0001.</p><p>After training, the network reaches a full training accuracy of 100% and a test accuracy of ≈ 74.9%, depending on the seed. In its original form, there is an additional layer, the local response normalization layer. This layer is not used here because it causes problems in newer versions of TensorFlow and is considered outdated and more or less unimportant in terms of generalization performance <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet20</head><p>ResNet <ref type="bibr" target="#b7">[8]</ref> is a convolutional network that additionally uses batch normalization and, most importantly, residual layers. The first layer is a convolutional layer with 16 filters and a kernel size of <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>, where the kernel is padded to preserve the shape of the input. The layer uses batch normalization followed by the ReLU activation function. After this layer, the residual block comes into play. The residual block, where the residual layers lie in, consists of six convolutional layers with 16 × 2 n block -1 filters, where n block is the block counter, a number that starts with 1 and increase with each following additional block, and a kernel size of <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>. After each convolutional layer, batch normalization and the ReLU activation function are applied. After every second layer, a residual layer is added between the batch normalization and the activation function, connecting the output of the second layer to the input of the first layer. For our ResNet, we use three of these residual blocks. This is followed by an average pooling layer, which averages the spatial indices coming out of the residual block. Finally, the output is flattened with a softmax activation function to match the output dense layer of size 10. Layer weights are initialized with the Glorot normal. Biases are set to zero. The network is trained on the CIFAR-10 dataset for 160 epochs with a learning rate schedule of</p><formula xml:id="formula_28">η(t) =            10 -3 t &lt; 80 10 -4 120 &gt; t ≥ 80 10 -5 t ≥ 120 , (5.3)</formula><p>where the time steps t are in epochs. The loss used is the cross entropy and the update rule is the SGD with a batch size of 128. We use a momentum of β = 0.9. All layers are regularized with a weight decay of λ = 0.0001. After training, the network reaches a full training accuracy of 100% and a test accuracy of ≈ 68.1%, depending on the seed. At first glance, it looks very similar to LeNet in terms of test accuracy, but it can massively outperform LeNet when using data augmentation. This is the creation of new images by cropping and rotating the original images in the dataset so that they still match the label class. The reason why data augmentation is not used here is that it would lead to the question of what is the total training loss and how to derive the Hessian matrix from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Algorithms for Deep Neural Network Analysis</head><p>The code used for this thesis is written entirely in Python and can be found on https:// github.com/RosenowGroup/Hessian-eigenvectors-PCA-DNN-weights. The modules used, and their most important functions are explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Neural Network Framework</head><p>The Python module TensorFlow <ref type="bibr" target="#b27">[28]</ref> is used to train and analyze the networks. This framework provides almost all important functions for building and testing networks.</p><p>In addition, it comes with its own type of tensors that can be used to build special TensorFlow functions that are much faster than Python functions because they allow parallelization and use of the graphics processing unit (GPU). Another very useful tool in TensorFlow is auto-differentiation. TensorFlow keeps track of all the operations used to compute e.g. the loss and allows a numerically optimal, precise evaluation of the gradient through predefined exact derivatives. We can expect all results to be independent of the framework. Deviations can be caused by different random seeds, e.g. during initialization, or by numerical instabilities during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation of the Hessian matrix</head><p>The Hessian matrix must be computed as a function of all training images or a sufficiently large subset. We need to extract the second derivative of the loss. To do this, we can use the auto-differentiation on the first derivative. In Lst.5.1 we can see how to compute a import tensorflow as tf with tf . GradientTape () as g : with tf . GradientTape () as gg : predictions = model ( images ) loss = loss_object ( labels , predictions ) gradient = gg . gradient ( loss , weights ) hvp = g . gradient ( gradient , weights , output_gradients = vector ) Listing 5.1: Computation of the Hessian vector product in TensorFlow. Here we record the gradient of the gradient of the loss (loss_object) of the network output (model(images)) in direction of a given vector (vector).</p><p>Hessian vector product (hvp) Hv. model is the model function that returns the softmax predictions of an input. The loss_object computes the loss function from the network's predictions and the actual true labels. tf.GradientTape tells TensorFlow to record all the operations it performs, from which we can take the gradient g.gradient from an output to an input. output_gradients=vector tells the gradient tape to compute the vector product of the gradient matrix and the vector. To compute the Hessian matrix, we can use the Hessian vector product of the standard basis to reconstruct its rows.</p><p>While TensorFlow offers the possibility to compute the Hessian directly, this requires more memory and can lead to bugs in older TensorFlow versions (we use TensorFlow 2.4). Another method to extract the eigenvalues and eigenvectors is to use the Lanczos algorithm <ref type="bibr" target="#b19">[20]</ref>. Here we start with a random vector from which we compute the hvp, extract a new vector, which is then used for the next hvp. Running this algorithm n times iteratively yields a tridiagonal matrix with n -1 eigenvalues that approximate the largest eigenvalues in magnitude. In addition, the eigenvectors can also be extracted.</p><p>The original algorithm leads to numerical instability, which can be avoided by several methods. One is to use science.sparse.linalg.eigsh from the SciPy <ref type="bibr" target="#b38">[39]</ref> module, which is accurate but very slow since it requires computing twice the number of hvp of the desired eigenvectors. Another faster but less accurate implementation can be found in the GitHub old folder of the project for this thesis linked earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation of tensor objects</head><p>When we measure the weights of the networks, we store them as NumPy arrays. NumPy <ref type="bibr" target="#b6">[7]</ref> is a module that contains many different operations that can be performed on these arrays. These arrays are tensors of arbitrary shape. To compute the covariance matrix, we can simply use numpy.cov. To get the principal components, we can then use numpy.linalg.eigh or tensorflow.linalg.eigh to compute the eigenvalues and eigenvectors of a hermitian or symmetric matrix. The TensorFlow function can use the GPU if the memory space is large enough, and it is also more stable when handling larger matrices, but does not return errors if there are discrepancies. Similarly, for SVD, there is a function linalg.svd in both modules to compute the singular values and their singular vectors.</p><p>6 Analysis of the Dynamic Weight Matrix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison of Singular Values and Principal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head><p>Our examination begins with a thorough exploration of the relationship between singular values and principal components of the network weights. Although the dimensions of the left and right singular vectors differ from those of the originating weight matrix, a transformation involving matrices Wi of single singular values brings these vectors together:</p><formula xml:id="formula_29">Wi = U Σi V T , Σi = diag(0, ..., ν i , 0, ..., 0) . (<label>6.1)</label></formula><p>Assembling these matrices, each comprising a left and a right singular vector, culminates in the reconstruction of the original weight matrix. The flattening of Wi into a vector wi facilitates the analysis of scalar products with the principal components p i using the equation:</p><formula xml:id="formula_30">S ij = |p i • wj | ∥ wj ∥ . (<label>6.2)</label></formula><p>Note that we are considering the absolute, since the choice of sign of the principal components is arbitrary. Unless otherwise noted, all principal component and singular value computations are performed after training. The visualization in Fig. <ref type="figure">6</ref>.1 unveils intriguing insights into the interaction between principal components and singular values. Notably, principal components show an interesting correlation with singular values. Remarkably, the principal components with the lowest indices have the largest product with singular values of the lowest indices. Here, the variances of the principal components and singular values underscores that the principal components with the highest variance shares a strong connection with the singular matrices of singular values situated outside the RMT bulk. At the same time, principal components with lower variances entail smaller product values relative to the former, yet display larger product values relative to singular values residing within the RMT bulk. Given the undeniable significance of singular values outside the RMT bulk as seen in Sec.4, we infer that the initial principal components encapsulate critical information due to their substantial scalar product. In the context of weight de- cay, the alignment of the largest singular values with the largest principal components underscores the presence of a distinct boundary between singular values from within and outside the RMT bulk, as depicted by their scalar product with principal components. Zooming in to focus on the largest principal components, Fig. <ref type="figure">6</ref>.2 exposes that the scalar product of the first principal component surpasses that of the subsequent components by a significant margin. This phenomenon, not apparent in the previous figure due to color resolution, showcases an ordering of scalar products across the first principal component. Remarkably, the scalar product decreases as we examine singular matrices of smaller singular values. For weight decay, the first principal component's scalar product is primarily attributed to the singular matrices within the RMT bulk, as opposed to the training without weight decay, where this occurs predominantly outside the RMT bulk. This phenomenon could be attributed to the comparatively small change in the outlying singular values induced by weight decay, juxtaposed with the substantial decrease of singular values in the bulk necessitated by network optimization, since outlying singular values will stagnate in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Principal Components' Influence on Network Performance</head><p>Shifting focus, we investigate the influence of principal components on network performance. We can introduce an additive weight vector based on principal components. This can be described as the follows:</p><formula xml:id="formula_31">w i,add = i j=1 θ j (T )p j , θ j (T ) := w(T ) • p j , (<label>6.3)</label></formula><p>where T represents the time of training completion. Fig. <ref type="figure">6</ref>.3 showcases the impact of principal component addition on network accuracy. Evidently, the unregularized network harbors substantial information within the first principal component, emphasizing the dominance of the first principal components. Conversely, the network employing weight decay necessitates the incorporation of approximately the first 500 largest principal components to achieve peak accuracy. This divergence could be attributed to the fact that the first principal component, as seen in Fig. <ref type="figure">6</ref>.2 with weight decay, no longer covers the outlying singular values primarily. Furthermore, we avoid the training on a subset of trainable variables during measurement to avoid undesirable outcomes. If a single layer is measured and the layers between that layer and the output are not updated, the high scalar product of the first principal component with the weights will disappear, and the</p><p>0 2500 5000 7500 10000 12500 15000 t in batches 4.115 4.120 4.125 4.130 4.135 θ1(t) Figure 6.4: Weight development in direction of the first principal component of the (128, 64) layer of the regularized MLP 256 network. loss will almost stay constant for further training. To avoid this behavior, which is not a practical training dynamic, we always update all layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis of the Drift Mode</head><p>In the direction of the first principal component, a clear linear behavior emerges, as depicted in Fig. <ref type="figure">6</ref>.4. This behavior is well-captured by:</p><formula xml:id="formula_32">θ 1 (t) = w(t) • p 1 ≈ a(t -t 0 ) + b, t ≥ t 0 , (<label>6.4)</label></formula><p>where t 0 marks the commencement of our measurement during the exploration phase, a is the slope and b = w(t 0 ) • p 1 . Notably, all other principal components deviate from linearity and exhibit a random walk pattern, with their effects on the network's weights remaining neglectable. Moreover, the dynamics of unregularized networks reveal that |θ 1 (t)| experiences growth over time. In contrast, networks featuring weight decay exhibit the shrinkage of both |θ 1 (t)| and ∥w∥. Considering the evolution of ∥w(t)∥, we discern an almost linear function approximately as for |θ 1 (t)|. We refer to the movement of the network as the "drift mode" for its conspicuous linear evolution <ref type="bibr" target="#b2">[3]</ref>. This observation paves the way for the approximation</p><formula xml:id="formula_33">w(t) ≈ a(t -t 0 ) + w(t 0 ) , (<label>6.5)</label></formula><p>where a = ap 1 . This result stems from the fact that only the drift mode is responsible for persistent changes within the network. The quantification of the drift mode contribution leads us to derive its variance to gain deeper insights. Mathematically, the variance σ 2 can be expressed as:</p><formula xml:id="formula_34">σ 2 1 = ⟨θ 1 (t) 2 ⟩ -⟨θ 1 (t)⟩ 2 = 1 T T +t 0 t=t 0 a 2 (t -t 0 ) 2 + b 2 + 2a(t -t 0 )b -   1 T T +t 0 t=t 0 (at + b)   2 = a 2 12 (T 2 -1) , (<label>6.6)</label></formula><p>where T signifies the number of measured time s. The application of Faulhaber's formula facilitates the solution of the sums, yielding a clear representation of the variance as a function of a, T , and the network's configuration. Since each update step points approximately in the same direction, we can connect the slope to the learning rate by using the SGD:</p><formula xml:id="formula_35">a ≈ ∥ w(t) -w(t 0 ) t -t 0 ∥ ∝ η . (6.7)</formula><p>As observed previously, when we extend the length of the observation interval or increase the size of the learning rate, the eigenvector of the largest eigenvalue of the covariance matrix approximates the drift mode, but only if it has a larger variance than those of the largest noise. This intriguing relationship can be validated by training the network up to the exploration phase using a high learning rate, effectively mitigating computational time constraints for low learning rates. The observed relationship, σ 2 ∝ η 2 T 2 , holds for appropriate learning rates and measurement time s. Furthermore, for extensive epochs, here for this comparatively small network ≈ 600 epochs, the feasibility of measuring every batch becomes constrained due to memory limitations. However, given the drift mode's computationally efficient characterization via a linear fit, it suffices to measure weights at more extended intervals, such as epochs. Remarkably, as demonstrated by the MLP 256 network in Fig. <ref type="figure">6</ref>.5, the drift mode ceases to exhibit a linear trend beyond approximately 500 epochs. This indicates the presence of higher-order terms contributing to the observed behavior.</p><p>To get a better understanding on this evolving behavior, a detailed analysis of the loss landscape within the drift mode's direction is crucial. The depiction in Fig. <ref type="figure">6</ref>.6 illustrates the presence of a potential well aligned with the drift mode's direction. Notably, this characteristic potential well corresponds to the presence of similar potential wells aligned with the Hessian eigenvectors, as described in Eq.(3.2). Importantly, this phenomenon is qualitatively observed across our used network architectures, limiting the explanation, that its causality being rooted solely in regularization effects.</p><p>The drift mode's dynamic behavior, marked by adjustments toward the loss minimum, offers insights into network training dynamics. Nonetheless, the minuscule magnitude of these adjustments implies that these insights may not be of great importance within the context of training for optimal performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">A Model for the Drift Mode</head><p>We begin by considering the quadratic loss approximation:</p><formula xml:id="formula_36">L(w) ≈ L min + (w -µ) T H 2 (w -µ) ,<label>(6.8)</label></formula><p>where L min is the minimum of the loss, µ is the point of the minimum in weight space and H is the Hessian matrix. We neglect momentum and include weight decay parametrized by its strength λ. The average update step of the Stochastic Gradient Descent is given by:</p><formula xml:id="formula_37">⟨ dw(t) dt ⟩ B = - η S (H + 2λ)w + ηHµ , (<label>6.9)</label></formula><p>where ⟨•⟩ B is the average over all batches and S the batch size. Let us assume that during the exploration phase, each update step is similar to the average update:</p><formula xml:id="formula_38">dw(t) dt ≈ ⟨ dw(t) dt ⟩ B . (6.10)</formula><p>This makes the differential equation linear and of first order, allowing us to solve it explicitly:</p><formula xml:id="formula_39">ŵi (t) = ŵi (0) - μi 1 + 2λ/h i e -η(h i +λ)t + μi 1 + 2λ/h i , (<label>6.11)</label></formula><p>where ŵi (t) = w(t) • h i , μi = µ • h i and h i are the Hessian eigenvalues their corresponding eigenvectors h i . When employing weight decay, the network's minimum is shifted to lower values. The drift mode can be decomposed using Hessian eigenvectors, since those form an eigenbasis in the weight space. The first-order approximation of the drift mode can be related to the observed drift mode in Sec.6.3. For longer measurements, an exponential decay is expected, as observed in Fig. <ref type="figure">6</ref>.5. The variance is computed using:</p><formula xml:id="formula_40">σ 2 i = ⟨ ŵi (t) 2 ⟩ -⟨ ŵi (t)⟩ 2 = 1 T T t=0 [ ŵi (0) -b i ] 2 e -2 λi t + b 2 i + 2 [ ŵi (0) -b i ] b i e -λi t - 1 T T t=0 ( ŵi (0) -b i )e -λi t + b i 2 = 1 T T t=0 ( ŵi (0) -b i ) 2 e -2 λi t - 1 T 2 T t=0 ( ŵi (0) -b i )e -λi t 2 = ( ŵi (0) -b i ) 2 T    1 -e -2 λi (T +1) 1 -e -2 λi - 1 T   1 -e -λi (T +1) 1 -e -λi   2    ,<label>(6.12)</label></formula><p>where λi := η(h i + 2λ) and b i := μi 1+2λ/h i . The geometric sum formula is used to solve the sums. Notably, the variance of the drift converges to zero as time increases, indicating that it does not increase indefinitely. In this model, the drift mode is a sum of the Hessian eigenvectors. If we decompose the drift as p 1 = i d i h i , with d i := h i • p 1 , then the variance of the drift mode is given by:</p><formula xml:id="formula_41">σ 2 d = i d 2 i σ 2 i . (6.13)</formula><p>The coefficients d i with i d 2 i = 1 are chosen such that σ 2 d maximizes when the drift mode aligns with the first principal component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Loss Scaling</head><p>Consider an unregularized network employing softmax that achieves a training accuracy of 100%. If we increase the weights' size by a factor of α, it will enhance confidence in the probability distribution and consequently reduce the loss, without altering the training accuracy. Assuming that the logits of the correct labels significantly exceed the others, i.e., z j ≫ z k , ∀k ̸ = j, we can demonstrate that:</p><formula xml:id="formula_42">L(αz) = -ln e αz j i e αz i = -αz j + αz j + ln(1 + i̸ =j e α(z i -z j ) ) = i̸ =j e α(z i -z j ) + O      i̸ =j e α(z i -z j )   2    ∝ e -αz j . (6.14)</formula><p>Using ReLU and scaling the weights layer by layer, we expect z i ∝ α j a j w j , ∀i, where a j are coefficients dependent on the input. Fig. <ref type="figure">6</ref>.7 illustrates that this assumption agrees with experimental results for the unregularized network, considering that scaling the weights proportionally changes the logits. Interestingly, while numerically increasing the weights can drive the loss to zero in this direction, the network tends to move in the direction of the drift mode instead. Both directions point in a similar direction, considering their large product ≈ 0.7 here, but they are not completely parallel. For the regularized network, we observe that L ∝ α 2 when scaling the network weights further. Therefore, scaling a network with weight decay can not reduce the loss further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis of the Hessian Matrix</head><p>In this chapter, we delve deeper into the eigenvectors of the Hessian matrix. Let H ∈ R n×n be the Hessian matrix with eigenvectors h i and corresponding eigenvalues h i :</p><formula xml:id="formula_43">Hh i = h i h i , h i ≥ h i+1 . (7.1)</formula><p>Because the Hessian matrix is symmetric, its eigenvalues are real, and its eigenvectors form an orthonormal basis.</p><p>Starting from the quadratic loss approximation in Eq.(3.2), we can interpret positive eigenvalues as measures of the curvature of minima along the corresponding eigenvector direction. Hence, eigenvectors corresponding to the largest eigenvalues are the crucial directions for loss minimization <ref type="bibr" target="#b2">[3]</ref>. These eigenvectors define regions in which the network must be closer to the minimum µ • h i than in directions with smaller positive eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Hessian Eigenvectors and PCA</head><p>By comparing the absolute scalar product of the principal components of weights and velocities v(t + 1) = w(t + 1) -w(t) with the Hessian eigenvectors, as depicted in Fig. <ref type="figure">7</ref>.1, we observe that the eigenvectors for the largest and smallest eigenvalues are quite similar for all bases. For intermediate eigenvalues, the similarity is diminished, but they still maintain some degree of diagonalization. Further, we observe that the velocity covariance matrix exhibits a sharper diagonal in the Hessian eigenbasis than for the eigenvectors of the weight covariance matrix. This indicates that the velocity eigenbasis is a more suitable choice for approximating the Hessian eigenbasis with a covariance matrix, potentially reducing computational time <ref type="bibr" target="#b2">[3]</ref>. Notably, measuring velocities instead of weights incurs no additional computational cost, as both are calculated for each update step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Eigenvectors and the Weight Product</head><p>Consider the scalar product of the Hessian eigenvectors with the network weights:  referred to as the weight product. In Fig. <ref type="figure">7</ref>.2, it is evident that without weight decay, eigenvectors associated with larger eigenvalues have a smaller weight product with the network, compared to randomly distributed eigenvectors associated with smaller eigenvalues. When weight decay is employed, these values become comparable in magnitude.</p><formula xml:id="formula_44">h i • w(T ) ∥w(T )∥ , (<label>7</label></formula><p>For both scenarios, the largest scalar products are found between the 500th and 2000th eigenvectors in proximity to each other. For large enough index ranges ≳ 500, the weight product distribution appears Gaussian in local regions with varying variances when testing those on the distribution. This behavior is anticipated in all networks. An explanation could be that we initialize layers uniformly or normally. The central limit theorem establishes that the sum of uniformly drawn numbers converges to a Gaussian distribution.</p><p>Assuming that the weights are mainly random, which is underlined by the fact that the network only trained in a small part of the Hessian eigenvectors, this can explain why the weight product still follow the distribution. The locations of the largest weight products, especially in unregularized networks, could potentially be due to the loss scaling discussed in Sec.6.5. The absence of a potential well in the direction of the entire network, primarily encompassed by large entries, might cause the potential wells of eigenvectors associated with these large entries to not be the steepest. Additionally, symmetry properties of the network could account for the small products observed with the largest eigenvalues. If we presume that eigenvectors corresponding to the largest eigenvalues are translations of labels or rotations, altering entries within a layer could lead to a substantial loss change, potentially misclassifying all labels <ref type="bibr" target="#b5">[6]</ref>. For other networks and layers, as depicted in Fig. <ref type="figure">7</ref>.3, the precise positioning of the large weight products within the Hessian spectrum remains without a model. Nonetheless, it is clear that layers with more parameters correspond to higher indices where the product is large.</p><p>Let us now decompose the weights of the layer into the eigenbasis of the Hessian: In Fig. <ref type="figure">7</ref>.4, it becomes evident that eigenvectors with the largest weight product must be aggregated to achieve optimal network performance. Setting all eigenvectors to zero except those with the largest product does not lead to accuracy degradation. For eigenvectors with small eigenvalues, this is easily explained by arguing that changing θ j from a certain small product in magnitude with the weights to zero will not change the loss much because their potential well is very flat. For the eigenvectors of the largest eigenvalues, this may be similar, since their weight product is small in magnitude, but it is not straightforward that because of their steep potential well, even small changes in θ j should not change the loss, and thus the accuracy, at all. This will be discussed more in detail for the whole network analysis. For the eigenvectors with a large product, it is plausible that setting θ j to zero will change the loss greatly, because θ j is large compared to other products and the eigenvalue is in the regime of being larger than most other eigenvalues of the Hessian. Comparing Fig. <ref type="figure">7</ref>.2 and Fig. <ref type="figure">7</ref>.4, we may also conclude that only a fraction of about ≲ 20% of the largest Hessian eigenvalues is sufficient to fully describe the network performance. For other networks the behavior is similar. However, the position for the large weight product differs, and with that the fraction of important eigenvalues.</p><formula xml:id="formula_45">w i,add = i j=1 θ j h j , θ j := w(T ) • h j . (<label>7</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison of Singular Values and Hessian</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eigenvectors</head><p>Let us explore the relationship between the product of Hessian eigenvectors and singular matrices: In the context of the networks used, we notice that singular values beyond the bulk almost entirely encompass the largest Hessian eigenvalues. This observation elucidates why the largest singular values are adequate to maintain the accuracy demonstrated in <ref type="bibr" target="#b36">[37]</ref>, as they encapsulate the directions from Fig. <ref type="figure">7</ref>.4 that are sufficient to achieve full training and test accuracy. A comparison between these findings and those related to principal components (Fig. <ref type="figure">6</ref>.1) reveals that both the Hessian eigenvector basis and the principal components behave similarly, likely due to their near-diagonal relationship with each other (Fig. <ref type="figure">7</ref>.1). In scenarios where weight decay is employed, the arrangement of the largest singular values that cover the prominent Hessian eigenvalues becomes more pronounced. The validation of these results extends to convolutional layers as well. For this purpose, we must transform the four-dimensional tensor into a two-dimensional matrix. Various methods can be employed for this reshaping process. Previous work <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27]</ref> has suggested that the specific reshaping technique may not significantly impact the results. Therefore, we opt to reshape the first two dimensions and the last two dimensions together. This approach ensures that we attain the maximum possible number of singular values for the given layer. Remarkably, as illustrated in Fig. <ref type="figure">7</ref>.6, this layer-wise analysis produces qualitatively similar behaviors when the layer has an adequate number of singular values ≳ 20.  </p><formula xml:id="formula_46">F ij = |h i • wj | ∥ wj ∥ . (<label>7</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Comparison of the Hessian of Layers and the Full</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Up to this point, our focus has primarily been on analyzing the Hessian of individual layers. The underlying assumption is that each layer's Hessian must broadly exhibit the properties of the Hessian of the entire network. This assumption stems from the notion that each layer can be viewed as a network in itself, and the largest eigenvalues of each layer must align in a similar direction with the largest eigenvalues of the entire network when the eigenvectors of a layer are appended to the network's structure.</p><p>Let h i ∈ R n represent the ith eigenvector of the Hessian of the whole network, and</p><formula xml:id="formula_47">h (l)</formula><p>i ∈ R n l denote the ith eigenvector of the Hessian of layer l. We can calculate:</p><formula xml:id="formula_48">|h i • h(l) j | , (<label>7.5)</label></formula><p>where h(l) j = (0, ..., h</p><formula xml:id="formula_49">(l) j1 , ..., h (l)</formula><p>jn l , 0, ...) ∈ R n . In Fig. <ref type="figure">7</ref>.7, it becomes evident that, while the eigenvectors of the largest eigenvalues for both the layer and the entire network point in The larger product of the largest indices refer to negative eigenvalues that are of similar size as indices in the range of 8000 to 10000. similar directions in the subspace, they are not identical. We can also observe a difference in all the negative eigenvalues of the total network, which fit into the picture when the absolute values are taken and the eigenvalues are resorted accordingly. This observation lends credence that computing individual layer Hessian eigenvectors can be used to approximate the whole Hessian eigenvectors, given that only the largest eigenvalues need to be considered to describe the layer's behavior (as seen in Sec.7.2). Fig. <ref type="figure">7</ref>.8 reveals that the weight product for the entire network exhibits behavior similar to that of individual layers for the MLP 50 network. This behavior was observed as well for the largest 12000 Hessian eigenvalues of the MLP 256 network. However, the indices with large weight products are relatively smaller in magnitude. The larger products towards the last indices correspond to negative eigenvalues that are of a magnitude similar to  positive eigenvalues with the same weight product. By decomposing the network into its Hessian eigenbasis and adding those together beginning with the largest eigenvalues (as in Eq.7.3), Fig. <ref type="figure">7</ref>.9 shows a similar behavior as for individual layers and illustrates that the eigenvectors of negative eigenvalues are necessary to achieve the network's full performance. Consequently, an alternative approach could involve sorting eigenvalues in decreasing order by absolute size. The accuracy for this sorting scheme is depicted in Fig. <ref type="figure">7</ref>.10, where the network achieves full performance qualitatively similar to the behavior of individual layers, with approximately ≈ 2500 added eigenvectors. In Fig. <ref type="figure">7</ref>.11 we can see that the accuracy start to improve not earlier than for about the 2000th eigenvalue and that during this transition the loss increases instead.</p><p>To further comprehend the significance of directions with large weight products for accu- racy, we can inspect the loss landscape with:</p><formula xml:id="formula_50">L (w + (α -w • h i )h i ) . (<label>7.6)</label></formula><p>For α = 0 we remove the ith eigenvector completely from the network and for α = w • h i it is fully included. Fig. <ref type="figure">7</ref>.12 demonstrates that the potential well corresponding to the larger eigenvalue is indeed steeper, yet so close to zero that its importance cannot be shown by simply setting its projection to zero. For both potentials, it is observable that the quadratic approximation holds only for α -w • h i ≲ 0.1. Setting the projection of the eigenvector with the largest weight product to zero takes us far from the quadratic approximation. While the loss prediction becomes less accurate, the qualitative description of a significantly larger loss remains valid. We observe from Fig. <ref type="figure">7</ref>.13 that the hypothesis stating that the minimum of the largest eigenvalues are in proximity to zero holds. Using the flattened singular matrices, we can once again compare them to the Hessian eigenvectors, similar to the layer and network eigenvector comparison. Fig. <ref type="figure">7</ref>.14 reveals that only the 12000 largest eigenvalues in magnitude have a relatively large product to the flattened singular matrices that make them distinct from smaller eigenvalues. Outlying singular values exhibit significant scalar products with eigenvectors of smaller indices, while singular values within the bulk display larger scalar products with eigenvectors of larger indices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Distribution of the Hessian Eigendecomposition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of Eigenvalues</head><p>Examining the spectral distribution of the Hessian eigenvalues in Fig. <ref type="figure">7</ref>.15, it is apparent that the largest eigenvalues greatly surpass the bulk of eigenvalues in magnitude, and a small fraction of eigenvalues are negative. The introduction of weight decay alters the Hessian matrix to be summed with 2λ1 n×n , leading to a shift of all eigenvalues by 2λ. Consequently, the primary concentration of eigenvalues shifts from zero to 2λ. Weight decay change the location of the minimum and the minimum found by the network during training. The spacing of eigenvalues does not conform to the Wigner surmise, as evidenced by Fig. <ref type="figure">7</ref>.16. Suggesting that the Hessian eigenvalues are not randomly distributed. The standard deviation of the smoothing kernel is set to one. Adapted from <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Properties of Eigenvectors</head><p>The Porter-Thomas distribution <ref type="bibr" target="#b29">[30]</ref> predicts that a vector v with random entries, having its entries sorted in increasing order ṽ, we have that</p><formula xml:id="formula_51">1 + erf(ṽ i N 2 ) 2 ≈ i/N , (7.7)</formula><p>where erf is the error function and N the length of the vector. Assessment against the Porter-Thomas distribution, reveals that none of the eigenvectors conform to this random distribution. A p-value near 0.</p><p>5 would indicate that the eigenvectors follow the distribution, but numerically, all p-values are effectively zero. This indicates that the eigenvectors deviate substantially from the distribution. 0 2000 4000 6000 8000 i 0.000 0.002 0.004 0.006 0.008 0.010 p values averaged Figure 7.17: Averaged p-values of the (128, 64) layer of the unregularized MLP 256 network. Averaged over the 15 closest eigenvalues in both directions. This conclusion holds for the Hessian of the entire MLP 50 network, and for the eigenvalues and eigenvectors of the Hessian of individual layers with 4096 -42310 parameters of all other networks as discussed in Sec.5.2. In instances where layers have few parameters, the p-values may reach approximately ≈ 0.02 at most when averaged over the p-values of eigenvectors from local groups, as shown in Fig.7.17. Larger p-values might be attributed to eigenvectors having fewer parameters or being closer to random than the eigenvectors of the whole network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layerwise Concentration of Eigenvectors</head><p>Considering the square of the norm for sections of total eigenvectors:</p><formula xml:id="formula_52">∥h i ∥ 2 l = j∈I l h 2 ij ,<label>(7.8)</label></formula><p>where I l represents the interval of indices within layer l, Fig. <ref type="figure">7</ref>.18 demonstrates that for most small eigenvalues, localization predominantly resides in the first layer. Assuming vector entries are completely random, the expected concentration fraction for each layer would correspond to the ratio of its number of parameters to the total number of parameters. In the provided figure, the first layer is anticipated to have a fraction of ≈ 0.928, the second ≈ 0.06, and the third ≈ 0.012. This comparison highlights that eigenvectors of the largest eigenvalues are more concentrated in later layers. One plausible explanation for this concentration lies in the vanishing gradient problem, layers farther from the output layer tend to possess smaller gradients due to the multiplication of gradients from subsequent layers. This training characteristic results in more attention directed toward later layers, which can be expected to possess steeper minima, leading to larger Hessian eigenvalues. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Development of the Network in the Direction of Hessian Eigenvectors</head><p>As observed in Sec.7.2, the scalar product of Hessian eigenvectors and network weights demonstrates structured behavior. To gain insight into this behavior, we examine how eigenvectors and weights evolve during training, not only during the exploration phase but also during initialization. Fig. <ref type="figure">7</ref>. <ref type="bibr" target="#b18">19</ref> indicates that even at initialization, the weight product for larger Hessian eigenvalues is smaller than for smaller Hessian eigenvalues, that were computed after training. This suggests that network initialization contributes to determining the final network shape, despite employing random initialization. In Fig. <ref type="figure">7</ref>.20 the eigenvalues are already shared a similar spectrum as to Fig. <ref type="figure">7</ref>.15 before training, with a few large eigenvalues and numerous others clustered around zero. This suggests that the dataset and the structure of the network significantly influence the distribution of Hessian eigenvalues, as they are the only contributions of the network not initialized randomly. Although the weight product for these eigenvalues before training seems more randomly distributed, it yields smaller products for larger eigenvalues than for smaller ones. Fig. <ref type="figure">7</ref>.21 illustrates that the inverse participation ratio ipr(h i ) = j h 4 ij is large for eigenvectors with the largest and smallest eigenvalues, while it is notably smaller for eigenvectors in between. However, it remains localized, in comparison to the full delocalization of 1/n = 1/8192 ≈ 0.0001 for the considered layer, where each component has a value of 1/ √ n. Let us assume that weights at initialization follow a random distribution with zero mean and variance σ 2 , while eigenvector components possess zero mean and variance σ 2 i , with  both variables assumed independent. Accordingly:</p><formula xml:id="formula_53">⟨(w • h i ) 2 ⟩ = j ⟨w 2 j ⟩⟨h 2 ij ⟩ = σ 2 σ 2 i = σ 2 , (<label>7.9)</label></formula><p>where we leverage eigenvector normalization, i.e., 1 = ⟨∥h i ∥ 2 ⟩ = j ⟨h 2 ij ⟩ = σ 2 i . Hence, the eigenvectors and weights should not be assumed to be independent, as the distribution of the weight product would then not depend on the index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Catastrophic Forgetting</head><p>In Sec.3.3, we proposed, following the idea of <ref type="bibr" target="#b2">[3]</ref>, that preserving the larger Hessian eigenvalues can potentially lead to maintaining test accuracies for other tasks in terms of </p><formula xml:id="formula_54">L cf (w) = λ cf N lim i=1 h i ((w -μ1 ) • h i ) 2 (7.10)</formula><p>into the loss function. Here, λ cf represents a positive real regularization constant, μ1 denotes the weights, h i signifies Hessian eigenvalues in descending order, and h i corresponds to their associated eigenvector, all at the end of training for the previous task. Building upon the insights from Sec.7.2 and Sec.7.3, we propose a novel regularization term:</p><formula xml:id="formula_55">L sv (w) = λ sv Nout i=1 ν i (w -μ1 ) • wi ∥ wi ∥ 2 , (<label>7.11)</label></formula><p>where λ sv is a regularization constant, N out denotes the number of singular values outside the RMT bulk, ν i are the singular values, wi are the corresponding flattened singular matrices, where the diagonal matrix had all entries set to zero except the ith singular value, and μ1 is the weight vector after training for the first task. This regularization strategy brings two computational advantages. Firstly, computing the singular values of a weight matrix is significantly less demanding in terms of memory and computation compared to the computation of Hessian eigenvalues. Secondly, during each training step where ∇ w L sv needs to be computed, the sum is performed over far fewer indices compared to N lim , as we can expect N out ≪ N lim . For biases, this regularization procedure is not directly applicable, but due to their relatively small vectors in comparison to the weight matrices of layers, their Hessian matrices can be computed efficiently and thus regularized similarly to the approach of the Hessian eigenvalues.</p><p>To empirically compare the effectiveness of our proposed singular value approach against the Hessian eigenvalue method, we adopt a strategy similar to that detailed in <ref type="bibr" target="#b2">[3]</ref>. The experimental setup involves training a network to attain full training accuracy across all tasks at once. This entails employing pre-trained networks and reinitializing the layers under investigation for our measurement. Specifically, we focus on the layers relevant to our analysis and train it exclusively to achieve full training accuracy for the first task. Subsequently, we extract singular values and Hessian eigenvalues from this process and introduce the corresponding regularization component to the layers. By then training the modified layers on the second task, we can assess the impact on the first task's performance. Without regularization, this can lead to a significant decrease in performance, potentially plummeting to levels comparable to random guessing and lower. This is due to the network's inclination to predict labels for the second task, thereby undermining its competence in the first task. Notably, the first task involves classifying the first five label categories, while the second task involves classifying the remaining five label categories from the CIFAR-10 dataset. We further train the network for 50 epochs using each method, selecting the results of the epoch with the highest sum of accuracies for both tasks. Both methods employ a substantial regularization constant of λ cf = 1000. However, higher constants could potentially lead to numerical instability after a single training step, depending on the learning rate. The learning rate here is set to 1% of the final learning rate from the preceding training phase. Lower learning rates favor the preservation of the initial task's accuracy, though at the cost of requiring more epochs to achieve acceptable accuracy for the second task. For the singular value method, we conserve either 20% of the Hessian eigenvalues of the biases or the five largest singular values of matrices and higher order tensors. While preserving weights based on previous tasks might seem unconventional, since the solution we fix the regularization on might not be the optimal solution for both tasks, an alternative approach is to directly impose the constraint on gradients rather than loss. We here propose the equation:</p><formula xml:id="formula_56">ṽ(t) := v(t) -γ Nout i=1 ν i ν 1 (v • wi ) wi , (<label>7.12)</label></formula><p>where v(t) is the velocity of the unregularized network, and γ ∈ [0, 1] represents the constraint strength. When γ = 1, the equation effectively prevents weight updates from occurring in conserved directions. The fraction of singular values in the equation ensures that the largest singular value and singular matrix is fully conserved, while smaller ones are conserved partially. A similar fraction is employed for Hessian eigenvalues. Importantly, this gradient-constraint approach circumvents numerical instability, rendering it a more stable option. Fig. <ref type="figure">7</ref>.22 presents the results for both gradient and loss-based methods. When comparing the sum of test accuracies, the former regularization method directly conserving weights yields superior overall performance. Intriguingly, both the singular value and Hessian methods yield comparable regularization performance, regardless of the fact that the sin-gular value method conserves fewer directions. Consistent results as in the figure are achieved when performing separate computations for each layer individually, including convolutional layers. However, the desire to preserve all layers simultaneously leads to a significant decrease in overall performance, as seen in Fig. <ref type="figure">7</ref>.23. The maximal overall test accuracy achieved using the regularization singular value loss method on the whole LeNet is 35%, while the gradient singular value approach results in a maximum test accuracy of 25%, still 5% higher than random guessing for the five-class tasks. It is important to note that due to training on two tasks, the limit for random guessing is no longer 20% for each task, but 0%, as the network could classify all images into labels from the other task. The minimum of both tasks averaged is 10%, similar to the case of training for all labels simultaneously. This interpretation allows us to see these accuracies as not entirely incorrect but slightly lower compared to training without tasks. An explanation for the better performance when training only a few layers could be that when the network is initially trained on all tasks, it might seek a solution that approximates solving all tasks simultaneously. However, preserving a layer could guide the network towards a solution more similar to the pre-reinitialization solution. By training the network exclusively on one task, the need for compatibility with the other task's solution becomes unnecessary. This was further investigated by training multiple layers simultaneously to assess the accuracy of this assumption. The obtained accuracies remain comparable to those in Fig. <ref type="figure">7</ref>.22 when training layers with fewer parameters. This suggests that the improvement in accuracy in some layers could be attributed to the fixation of other layers to stable solutions. When training multiple layers, the Hessian eigenvector regularization method seems to outperform the singular value method by up to 2%. This result might be due to the singular value method not fully covering the weight space. Alternatively, there could be room for refining the optimization of regularization constraints, an aspect that warrants further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Generalizability of our Results</head><p>In this section, we discuss into the generality of our results concerning network structures that were not explicitly included in our analysis. It is worth noting that the influence of the update rule and batch size primarily manifests itself in the context of principal component analysis, since it is a dynamical property. Conversely, the Hessian matrix and singular value decomposition remain relatively invariant with respect to these properties. Their influence, if any, operates indirectly through the minima that they help identify.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Momentum</head><p>The impact of momentum in early training is notable, as it can influence the minima discovered by the network. When momentum β is applied during the exploration phase, it alters the effective learning rate, assuming a near-constant gradient. Utilizing the SGD equation v(t + 1) = -η∇ w l (k(t)) (w, x, z) + βv(t), where η is the learning rate, l (k(t)) the loss of batch B k and w(t + 1) = w(t) + v(t + 1), we can deduce:</p><formula xml:id="formula_57">⟨v(t)⟩ B = -η t t ′ =0 β t-t ′ ⟨∇ w l (k(t ′ )) (w, x, z)⟩ B = - η S ∇ w L t t ′ =0 β t-t ′ = - η S ∇ w L t t ′ =0 β t ′ = - η S 1 -β t+1 1 -β ∇ w L ≈ - η S(1 -β) ∇ w L ,<label>(8.1)</label></formula><p>where we used the geometric series formula and that ⟨∇ w l (k) (w, x, z)⟩ B = 1 S ∇ w L is the gradient of the mean loss of all batches. Momentum renormalizes the bare learning rate to become effectively larger. Excluding this, momentum's presence does not alter the observed qualitative behavior for the PCA, the findings indicate the generalizability of these observations to various minima that can be approximated by the network utilizing momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants of SGD</head><p>While momentum's effect on the minimum in our thesis is understood, the effect under other SGD variants like Adam <ref type="bibr" target="#b14">[15]</ref> and Lion <ref type="bibr" target="#b1">[2]</ref> remains unexplored in our thesis. The dynamics of update steps for these methods are more complex. However, it is anticipated that network parameters may tend to converge to flat minima, resembling those of SGD. This commonality could lead to similar qualitative behaviors observed with SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Size</head><p>This work has not explicitly addressed the impact of batch size. Qualitative results have been validated for typical batch sizes within the range of S ∈ <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">512]</ref> for SGD. Altering the batch size influences the minima found by the network <ref type="bibr" target="#b12">[13]</ref>. Larger batch sizes have been found to yield steeper loss landscapes, corresponding to larger eigenvalues in the Hessian matrix <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer and Network Analysis</head><p>Analyzing network weights can be simplified by considering subsets of layers, treating their weights of separate layers as multiple independent parameters. Hessian matrix analysis is more intricate, as the loss gradient of different layers is interconnected. Despite this complexity, results in Sec.7.4 show that the Hessian eigenvectors for individual layers closely resemble the corresponding eigenvectors of the entire network of similar indices. This implies that for large eigenvalues, computing Hessian eigenvectors for individual layers suffices, making such computations feasible even for large networks with layers having no more than ≲ 50000 parameters, requiring around ≈ 128 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Further Questions Data Augmentation</head><p>The importance of data augmentation in modern networks is well-recognized <ref type="bibr" target="#b7">[8]</ref>. However, incorporating data augmentation into Hessian matrix evaluation poses challenges due to the vast number of possible realizations of training samples. The more training examples a network uses during training, the more costly the computation of the total loss is, since the Hessian must be computed for each sample separately. To approximate the total loss, a sufficiently large number of augmented samples must be used, mimicking the strategy of approximating generalization performance with a smaller test dataset compared to the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Normalization</head><p>Batch normalization's influence on training dynamics is substantial. This creates a correlation between the samples in a batch <ref type="bibr" target="#b10">[11]</ref>. Without batch normalization, samples could be assumed to be independent with respect to their gradient. For our ResNet 20, the influence of batch normalization did not change the qualitative behavior in the weight product, but quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise Filtering</head><p>Recent work has suggested noise filtering in the context of label noise <ref type="bibr" target="#b36">[37]</ref>. Removing the small singular values from the network and reshaping the distribution to align with a noise-free scenario has shown to improve test accuracy. One explanation is that these singular values of the RMT bulk are not changed by the network during training, or are changed so little that they do not contain true information, but only noise, such as label noise caused by misclassification. This can result in a shift of the RMT bulk towards larger singular values, which is small enough for a realistic amount of label noise ≤ 40%, such that outlying singular values can still be separated from the ones of the bulk <ref type="bibr" target="#b36">[37]</ref>. Using our observations and relationships to the Hessian eigenvectors, we can further extend our understanding. Recall that the eigenvectors of the largest Hessian eigenvalues have a large product with the singular matrices of the largest singular values as shown in Sec.7.3. Thus, if we set small singular values to zero, they will mostly affect only the Hessian eigenvectors of the small eigenvalues, which we know by setting them to zero have such flat potentials that shifting them cannot measurably increase the loss seen in Fig. <ref type="figure">7</ref>.12. In this work it was not covered how label noise affects the Hessian eigenvalues and its properties to the weights. This can be done in future work to the extent of our understanding of handling noise practically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Transition of the Dataset</head><p>Sec.7.6 shed light on the remarkable alignment between the pre-training and post-training eigenvalue orders, highlighting the influence of both the dataset's characteristics and the network's architecture on the observed patterns. This insight offers a complement to dataset analyses, as explored in <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Catastrophic Forgetting</head><p>The proposed catastrophic forgetting regularization in Sec.7.7 performs well on individual layers but less effectively on entire networks. While the method applied to the gradient seems to fail completely in terms of test accuracy, the loss method performs reasonably well and may be applicable. Our results may be even more useful for fine-tuning a network, i.e., training on the latest layers to further specialize the network, e.g., training the network on a task it has never seen before <ref type="bibr" target="#b32">[33]</ref>. This can be few-shot learning <ref type="bibr" target="#b24">[25]</ref>, where the network has to recognize label classes it has never seen before that are very similar to those it has already learned. The CIFAR-10 dataset we used to test catastrophic forgetting does not have enough label classes to efficiently predict how our method will perform on numerous label classes. Considering that as problems get harder, the number of label classes becomes so large that many classes are only seen by the network after many batches, preventing catastrophic forgetting may become more important for larger classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explainability of Deep Neural Networks</head><p>This work represents a stride towards unraveling the enigmatic nature of Deep Neural Networks (DNNs), contributing to their explainability. Our understanding provides a lens through which we can scrutinize and interpret the behavior and properties of DNNs. Furthermore, this pursuit holds the potential to assuage growing societal concerns about the evolving capabilities of these networks, as emphasized in recent discussions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this study, we delved into the intricate dynamics of trained neural networks, unraveling key insights that shed light on their behavior. Trained networks predominantly continue training in a single direction, known as the drift mode. This intriguing drift mode can be elegantly explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. Through our analysis, we unveiled a strong correlation between the Hessian eigenvectors and the network weights. This relationship, hinged on the magnitude of eigenvalues, allowed us to discern the important parameter directions within the network. Notably, the significance of these directions rests on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extended to the decomposition of weight matrices through singular value decomposition. Quite surprisingly the overlap of Hessian eigenvectors of large eigenvalues is larger for singular matrices of larger singular values than for the weight vector. Furthermore, our examination showcased the applicability of principal component analysis in approximating the Hessian, with update parameters emerging as a superior choice over weights for this purpose. Remarkably, our findings unveiled a similarity between the largest Hessian eigenvalues of individual layers and the entire network. Notably, higher eigenvalues were concentrated more in deeper layers. Strikingly, the Hessian eigenspectrum, far from being randomly distributed, exhibited a pronounced pattern predetermined by the dataset and the network structure, even before the onset of training. Leveraging these insights, we ventured into the realm of addressing catastrophic forgetting a challenge that plagues neural networks when learning new tasks while retaining knowledge from previous ones. By applying our discoveries of the overlaps of the Hessian eigenvectors and singular matrices, we formulated an effective strategy to mitigate catastrophic forgetting, offering a pragmatic solution that can be applied to networks of varying scales, including larger architectures. In conclusion, our journey through the intricate landscapes of trained neural networks has revelations that deepen our understanding of their behavior. These insights hold the promise of not only refining network training but also influencing the broader discourse on explainability and reliability in the realm of deep learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 . 1 :</head><label>51</label><figDesc>Figure 5.1: Graphical representation of the MLP 50 (left) and the MLP 256 (right) network structure. The "None" entry means that we can insert an arbitrary batch size into the network. The input layer equal the pictures of the corresponding datasets. Those have to be turned into a vector by an additional flattening layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 . 2 :</head><label>52</label><figDesc>Figure 5.2: Graphical representation of the LeNet (left) and miniAlexNet (right) structure. The "None" entry means that we can insert an arbitrary batch size into the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 . 1 :Figure 6 . 2 :</head><label>6162</label><figDesc>Figure 6.1: Scalar product of singular matrices and principal components of the (128, 64) layer. Measured weights for 10 epochs post-training. (a) Unregularized for all layers. (b) All layers regularized with weight decay of λ = 0.0005.</figDesc><graphic coords="28,113.55,297.05,154.23,126.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 . 3 :</head><label>63</label><figDesc>Figure 6.3: Accuracy of added principal components of the (128, 64) layer of the MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005. The entry i = 0 corresponds to a weight matrix of zeros.</figDesc><graphic coords="29,93.55,70.86,408.20,173.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 . 5 :</head><label>65</label><figDesc>Figure 6.5: Weight development in direction of the first principal component of the (128, 64) layer of the regularized MLP 256 network. Time s are measured in epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 Figure 6 . 6 :</head><label>166</label><figDesc>Figure 6.6: The loss landscape in direction of the drift mode of the (128, 64) layer of the regularized MLP 256 network. The network is very close to the minimum in direction of the drift mode. Each update step only moves the network very little further to the minimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 . 7 :</head><label>67</label><figDesc>Figure 6.7: Loss scaling of the (128,64) layer of the MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 . 1 :</head><label>71</label><figDesc>Figure 7.1: Scalar product of the Hessian eigenvectors with the principal components of (a) weights and (b) velocities of the (128, 64) layer of the unregularized MLP 256 network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 . 2 :</head><label>72</label><figDesc>Figure 7.2: Weight product of the (128, 64) layer of the regularized MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005.</figDesc><graphic coords="36,93.55,286.46,408.18,169.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 . 3 :</head><label>73</label><figDesc>Figure 7.3: Weight product for various networks and layers: (a) First convolutional layer of LeNet. (b) Second convolutional layer of ResNet. (c) Last layer of miniAlexNet. (d) First convolutional layer of miniAlexNet.</figDesc><graphic coords="37,93.55,70.86,408.19,311.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>. 3 )Figure 7 . 4 :</head><label>374</label><figDesc>Figure 7.4: Accuracy of added Hessian eigenvectors of the (128, 64) layer of the MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 . 5 :</head><label>75</label><figDesc>Figure 7.5: Scalar product of singular matrices and Hessian eigenvectors for the (128, 64) layer of the MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 . 6 :</head><label>76</label><figDesc>Figure 7.6: Scalar product of the flattened singular matrices and Hessian eigenvectors for the (16, (5, 5)) convolutional layer of the LeNet.</figDesc><graphic coords="40,209.22,282.57,164.44,122.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 . 7 :</head><label>77</label><figDesc>Figure 7.7: Scalar product of the Hessian eigenvectors of the full MLP 50 network with the Hessian eigenvectors of the (50, 50) layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 . 8 :</head><label>78</label><figDesc>Figure 7.8: Weight product of the entire MLP 50 network trained on the MNIST dataset.The larger product of the largest indices refer to negative eigenvalues that are of similar size as indices in the range of 8000 to 10000.</figDesc><graphic coords="41,184.25,70.87,226.77,166.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 . 9 :</head><label>79</label><figDesc>Figure 7.9: Accuracy of the network with weights of added Hessian eigenvectors of the entire MLP 50 network. The eigenvalues are sorted in decreasing algebraic order. The changes for the largest indices mark negative eigenvalues. (a) Accuracy. (b) Loss.</figDesc><graphic coords="41,93.55,302.28,408.20,173.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 7 . 10 :</head><label>710</label><figDesc>Figure 7.10: Accuracy of the network with weights of added Hessian eigenvectors of the entire MLP 50 network. The eigenvalues are sorted in decreasing order by magnitude. (a) Accuracy. (b) Loss.</figDesc><graphic coords="42,93.55,70.86,408.20,173.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 7 . 11 :</head><label>711</label><figDesc>Figure 7.11: Accuracy of added Hessian eigenvectors of the entire MLP 50 network. The eigenvalues are sorted in decreasing order by magnitude. Zoomed in to the 5000 largest eigenvalues. (a) Accuracy. (b) Loss.</figDesc><graphic coords="42,93.55,309.12,408.20,173.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 7 . 12 :</head><label>712</label><figDesc>Figure 7.12: The loss landscape of Hessian eigenvectors of the entire MLP 50 network. The eigenvector with the largest eigenvalue and the eigenvector with the largest weight product are shown. The dashed lines represent the quadratic potential model expectation. (a) Loss. (b) Test accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 7 . 13 :Figure 7 . 14 :</head><label>713714</label><figDesc>Figure 7.13: The loss landscape of Hessian eigenvectors of the entire MLP 50 network. The four eigenvectors with the largest eigenvalue are shown. The dashed lines represent the quadratic potential model expectation. (a) Loss. (b) Test accuracy.</figDesc><graphic coords="44,204.97,329.91,170.55,126.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 7 . 15 :</head><label>715</label><figDesc>Figure 7.15: Spectral density of the Hessian eigenvalues of the entire MLP 50 network.The standard deviation of the smoothing kernel is set to one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 7 . 16 :</head><label>716</label><figDesc>Figure 7.16: Level spacings of the unfolded spectrum of Hessian eigenvalues of the entire MLP 50 network. The dashed line represents the expectation for random matrices of the same size. Averaged over the 10 nearest neighbors in both directions. The inset plot shows the cumulative distribution function. Adapted from [38].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 7 . 18 :</head><label>718</label><figDesc>Figure 7.18: Concentration of Hessian eigenvectors in specific layers by the square norm of each layer's components. The MLP 50 network is analyzed for this illustration.</figDesc><graphic coords="47,184.25,70.87,226.76,172.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 7 . 19 :</head><label>719</label><figDesc>Figure 7.19: Weight product of the (128, 64) layer of the unregularized MLP 256 network. (a) At network initialization. (b) After 100 epochs of training. Eigenvectors are computed after 100 epochs of training for both figures.</figDesc><graphic coords="48,93.55,70.87,408.19,170.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 7 . 20 :</head><label>720</label><figDesc>Figure 7.20: Hessian eigenvalues and eigenvectors of the (128, 64) layer of the unregularized MLP 256 network at initialization. (a) Weight product. (b) Spectral density. The standard deviation of the smoothing kernel is set to one.</figDesc><graphic coords="48,93.55,306.94,408.19,170.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 7 . 21 :</head><label>721</label><figDesc>Figure 7.21: Inverse participation ratio of the (128, 64) layer of the unregularized MLP 256 network. (a) At initialization. (b) After 100 epochs.</figDesc><graphic coords="49,93.55,70.87,408.19,170.82" type="bitmap" /></figure>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>Throughout this thesis, vectors are represented in lowercase bold, and higher-order tensors are represented in uppercase bold. Fundamental mathematical theorems and physical laws are assumed to be known and will only be referenced when applied in the context of this work.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Symbolic Discovery of Optimization Algorithms</title>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06675[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The inverse variance-flatness relation in stochastic gradient descent is critical for finding flat minima</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhai</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2015617118</idno>
		<ptr target="https://www.pnas.org/doi/pdf/10.1073/pnas.2015617118" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2021">2021. 2015617118</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research Chia Laguna Resort</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Rainbow in Deep Network Black Boxes</title>
		<author>
			<persName><forename type="first">Florentin</forename><surname>Guth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18512[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-020-2649-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020-09">Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explainability in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Heuillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Couthouis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2020.106685</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2020.106685" />
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<idno type="ISSN">0950-7051</idno>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page">106685</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167[cs.LG]</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cybernetics and forecasting techniques</title>
		<author>
			<persName><forename type="first">Aleksey</forename><surname>Grigorevich Ivakhnenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Grigorevich Lapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>American Elsevier Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Three Factors Influencing Minima in SGD</title>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04623[cs.LG]</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">H M Dipu</forename><surname>Kabir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03238[cs.CV</idno>
		<title level="m">Reduction of Class Activation Uncertainty with Background Information</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980[cs.LG]</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Kühn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Rosenow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05300[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations, and Anomalous Diffusion</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.09133[cs.LG]</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An iteration method for the solution of the eigenvalue problem of linear differential and integral operators</title>
		<author>
			<persName><forename type="first">Cornelius</forename><surname>Lanczos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The handbook of brain theory and neural networks</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">3361</biblScope>
			<biblScope unit="page">1995</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Oz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14975[cs.LG]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</title>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1950" to="1965" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to random matrices theory and practice</title>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Livan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Novaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierpaolo</forename><surname>Vivo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monograph Award</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="54" to="57" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7479" to="7551" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<title level="m">Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Godfather of AI&apos; Leaves Google and Warns of Danger Ahead</title>
		<author>
			<persName><forename type="first">Cade</forename><surname>Metz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New York Times</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fluctuations of nuclear reaction widths</title>
		<author>
			<persName><forename type="first">E</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">483</biblScope>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575[cs.CV</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multitask Prompted Training Enables Zero-Shot Task Generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08207[cs.LG]</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Alle 30 Jahre werden KIs eine Million mal größer</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FAZ</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556[cs.CV</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Boundary between noise and information applied to filtering neural network weight matrices</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Staats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Thamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Rosenow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">L022302</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Random matrix analysis of deep neural network weight matrices</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Thamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Staats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Rosenow</surname></persName>
		</author>
		<idno type="DOI">10.1103/physreve.106.054124</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</title>
		<author>
			<persName><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0686-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530[cs.LG]</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
