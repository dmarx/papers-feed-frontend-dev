# 

## Abstract

## 

This study delves into the intricate dynamics of trained deep neural networks and their relationships with network parameters. Trained networks predominantly continue training in a single direction, known as the drift mode. This drift mode can be explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. We unveil a correlation between Hessian eigenvectors and network weights. This relationship, hinging on the magnitude of eigenvalues, allows us to discern parameter directions within the network. Notably, the significance of these directions relies on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extends to the decomposition of weight matrices through singular value decomposition. This approach proves practical in identifying critical directions within the Hessian, considering both their magnitude and curvature. Furthermore, our examination showcases the applicability of principal component analysis in approximating the Hessian, with update parameters emerging as a superior choice over weights for this purpose. Remarkably, our findings unveil a similarity between the largest Hessian eigenvalues of individual layers and the entire network. Notably, higher eigenvalues are concentrated more in deeper layers. Leveraging these insights, we venture into addressing catastrophic forgetting, a challenge of neural networks when learning new tasks while retaining knowledge from previous ones. By applying our discoveries, we formulate an effective strategy to mitigate catastrophic forgetting, offering a possible solution that can be applied to networks of varying scales, including larger architectures. This study is a step to uncover intricate behavior of deep neural networks and also provides practical solutions for enhancing their capabilities and addressing critical challenges. I would like to express my heartfelt gratitude to several individuals who have played instrumental roles in the completion of this thesis. First and foremost, I extend my sincere appreciation to Marcel Kühn, Matthias Thamm, and Max Staats. Their invaluable insights, thoughtful discussions, and unwavering support have been pivotal throughout this research journey. Without their guidance, this thesis would not have been possible. I am also deeply thankful to Professor Dr. Bernd Rosenow for his mentorship, and continuous encouragement. His expertise has been a guiding light in navigating the complexities of this study. Furthermore, I extend my appreciation to Mr. Thamm and Mr. Staats for their dedicated supervision, which has contributed significantly to the refinement of this work.

## Introduction

Gottfried Wilhelm Leibniz foresaw the potential of solving complex mathematical problems through binary classifications of True or False, thus laying the groundwork for the first mechanical computer [[34]](#b33). As time passed, it became increasingly clear that computers easily surpassed humans in tasks that can be broken down into straightforward algorithms, consisting of a simple list of instructions. The key issue is that any problem we can formulate into an algorithm can be solved, given enough computational power and processing time. However, a significant challenge arose with this advancement: How to overcome obstacles that do not easily translate into algorithmic structures? Take, for example, the field of image classification. The task of differentiating between inanimate objects and living animals is a relatively straightforward one. This can be achieved by developing guidelines to check for the existence of specific attributes like eyes, noses, and fur, which are unique to animals and not present in stones. However, the task becomes more intricate when faced with the challenge of differentiating between cats and dogs. While dogs and cats share many similarities, it is difficult to pinpoint the exact distinguishing features that universally set all dogs apart from all cats. In response to this enigma, a bold solution emerged from an unexpected source: The process of translating these complex challenges into algorithmic frameworks, akin to learning, underwent a transformational shift, becoming an algorithm on its own that can be resolved computationally [[12]](#b11). By digitizing data and the desired solution for these enigmas, we applied the fundamental principles of derivatives -attributable to Leibniz's legacy [[34]](#b33) -to autonomously simplify these complexities. For example, when analyzing pictures, we supply the computer with images of both dogs and cats and allow it to determine a way to differentiate between the two. This process encompasses various names, initially as Cybernetics, later evolving into Artificial Intelligence, and currently maturing as Deep Learning. The principles underpinning Deep Learning were not exclusively forged within the realm of scientific inquiry, but rather stem from the very fabric of nature itself. Our cognitive architecture, constituted by neurons intricately interconnected, forms a neural network that embodies the essence of learning. Analyzing the structure and functioning of our brain enabled us to decipher the algorithmic mechanisms of learning, which could then be harnessed and adapted to meet the computational requirements of our machines [[31]](#b30). As a consequence, the learning algorithms we employ bear the moniker of Deep Neural Networks (DNNs), a nomenclature that resonates with the intricate neural structure and functionality of our own minds. Yet, the profound irony remains: Just as we sometimes find it challenging to elucidate how our own brain navigates certain problems, providing the same explanations to our computational counterparts, as in the case of distinguishing between cats and dogs, proves to be equally formidable. In recent years, DNNs have emerged as a transformative force across diverse domains, achieving remarkable feats in various applications such as natural language translation like DeepL and Google Translator, conversational agents like ChatGPT and Bing Chat, and strategic game-playing in chess and go [[35]](#b34). These networks' extraordinary capabilities underscore their potential to reshape our technological landscape fundamentally. Yet, beneath their impressive performance lies a veil of mystery, as the decisions made by DNNs often remain opaque and elusive. Since we no longer can comprehend the instructions the computer finds, when searching for an algorithm [[9]](#b8). The intrinsic power of DNNs originates from their ability to model intricate relationships within complex data, converting raw inputs into meaningful predictions. These models, characterized by webs of interconnected parameters, raise fundamental questions about the interplay between network architecture, training dynamics, and task proficiency. Addressing these questions not only holds theoretical significance but also carries practical implications for enhancing the reliability, interpretability, and generalization of DNNs in real-world applications [[1]](#b0). Driven by the quest to unveil the inner workings of DNNs, researchers have pursued various approaches, each aimed at shedding light on distinct aspects of these models. Two of many paradigms warrant our attention: Random Matrix Theory (RMT) and Principal Component Analysis (PCA) of training dynamics. These complementary strategies offer unique insights into the behavior of DNNs and the factors driving their performance. RMT, rooted in the mathematical foundations of linear algebra, delves into the structure of DNNs' weight matrices. These matrices, emerging from the network's layered architecture and initialized with randomness, undergo change during training, giving rise to an interplay between deterministic and random components. RMT together with singular value decomposition can be used to discern meaningful patterns within these matrices, shedding light on the evolution of network parameters and identifying key components that drive the network's performance [[27]](#b26). In contrast, PCA of training dynamics delves into the trajectory of network updates as dictated by the Stochastic Gradient Descent (SGD), the workhorse optimization algorithm driving DNN training. By analyzing the development of parameter updates, this approach offers a window into the network's adaptation process and unveils directions of maximal change, which are intrinsically tied to critical information for task-solving [[3]](#b2). While both RMT and PCA have independently contributed to advancing our understanding of DNNs, a comprehensive comparative exploration that compares these approaches remains absent. Bridging the gap between the network parameters and training dynamics holds the potential to catalyze a more comprehensive behavior of DNN and yield insights with far-reaching ramifications. Beyond theoretical implications, these insights could pave the way for practical applications, such as combatting catastrophic forgetting [[3]](#b2) and navigating the challenges posed by noisy data [[37]](#b36). In light of these considerations, this study embarks on an journey to untangle DNN behavior by synergistically employing RMT and PCA. Through a comparative analysis, we strive to elucidate the interplay between network architecture, training dynamics, and task performance. Our exploration not only enriches the theoretical discourse surrounding DNNs but also holds the promise of empowering practical solutions for the challenges that lie ahead. The subsequent chapters of this thesis are structured as follows: Chapter 2 introduces the theoretical and mathematical foundations of DNNs necessary for comprehending the subsequent sections. Chapter 3 explores PCA and its current state of research in the context of DNNs. Chapter 4 delves into RMT and its application to DNNs. Chapter 5 provides insights into the practical aspects of network realization, including network setups and datasets used. Chapter 6 presents our comparative analysis of PCA and RMT results in DNNs, along with an exploration of PCA properties in relation to the Hessian matrix. Chapter 7 shifts our focus to the Hessian matrix, its properties, its relationships with network weights, and its connection to RMT. Finally, we discuss our findings, offer suggestions for further research, and conclude this work.

## Introduction to Deep Neural Networks

This chapter delves into modern Deep Neural Networks (DNNs) for image recognition [[21,](#b20)[17,](#b16)[36,](#b35)[8]](#b7), which are designed to transform inputs, denoted as x (e.g., images), into outputs, represented by y (e.g., image labels), in order to classify images. Formally, the relationship is expressed as y = f (x, w), where w signifies the network's weights. Deep Neural Networks are structured in layers, which can be interpreted as transformations themselves. Let y (l) = f (l) (x (l) , w (l) ) be the output of layer l. The weights of a layer are a part of all weights. The input of a layer is commonly the output of the previous layer, i.e. x (l) = y (l-1) [[1]](#b0).

## Activation Functions

Activation functions modify the output of each layer to introduce nonlinearity. A widely used activation function is the Rectified Linear Unit (ReLU) [[5]](#b4):

$y i (x) = max(x i , 0) . (2.1)$Another important activation function is the softmax function:

$y i (x) = e x i j e x j .$(2.

2)

The softmax activation is commonly employed in the output layer for classification tasks, providing probabilities for each class. The class with the highest probability is the predicted label, often referred to as the top-1 accuracy.

## Loss Function

The network's performance assessment and enhancement are facilitated through a loss function, typically employing the categorical cross-entropy loss:

$L(x, z) = - i z i log(y i (x)) , (2.3)$where z i = 1 only for the correct output label of input x and 0 otherwise. This loss function is conceptually aligned with information entropy, rewarding accurate approximations of the training data's probability distribution.

## Stochastic Gradient Descent

Let us consider datasets of inputs x i and their corresponding correct labels z i , where the upper index corresponds to a single sample of the dataset. When training the network, the loss function is computed for multiple training samples in a batch B k ≡ B k(t) :

$l (k) (x, z) = 1 S i∈B k L(x (i) , z (i)$) . (2.4) Batches are typically of size S ∈ [32, 512] [19]. The network updates its weights using Stochastic Gradient Descent (SGD) w(t + 1) = w(t) + v(t + 1), where t is a time step and:

$v(t + 1) = -η∇ w l (k(t)) (w, x, z) + βv(t) ,(2.5)$where η > 0 is the learning rate and β ∈ [0, 1) is the momentum. The term ∇ w l (k(t)) (w, x, z) corresponds to the gradient of the loss for a batch B k (t) with respect to the network's weights. The concept of stochasticity arises from the random sampling of batches and the variability introduced by this process.

A challenge that arises due to updating all weights from the last layer backwards is the vanishing gradient problem, wherein the gradients diminish in magnitude as the optimization process progresses, particularly affecting early layers, due to the chain rule of derivatives. This phenomenon hinders the learning of these layers and can be attributed to the multiplication of gradients during the chain rule process [[10]](#b9).

## Regularization

Regularization techniques play a critical role in preventing overfitting. One prevalent form is the L 2 regularization, adding λ||w|| 2 2 to the loss l (k) in Eq. [(2.5)](#). This technique discourages the network from focusing excessively on unimportant features, improving the model's generalization capability.

## Layers

Various layer types are employed in contemporary DNN architectures [[1]](#b0), including convolutional layers, pooling layers, and dense layers. These layer types are mathematically characterized below.

## Dense Layers

Dense layers implement linear matrix multiplications on flattened inputs:

$y = W x + b , (2.6)$where y is the output, W is the weight matrix, x is the input, and b is the bias vector.

Flattening is applied to matrix inputs, converting them into vectors. These layers form a versatile foundation by linking each input to each output individually [[22]](#b21).

## Convolutional Layers

3-D convolutional layers involve convolutions of input X with a weight tensor W :

$Y ijk = m∈I 1 n∈I 2 l∈I 3 W mnlk X i+m,j+n,l + B ijk , (2.7)$where

$I 1 = [-d 1 , d 1 ], I 2 = [-d 2 , d 2 ], I 3 = [1, d 3 ]$. d 1 and d 2 are the kernel sizes in the x and y directions, respectively, and d 3 corresponds to the input depth. These layers are particularly efficient for image recognition tasks, as they exploit local correlations among nearby pixels [[22]](#b21).

## Pooling Layers

Max Pooling layers, often employed after convolutional layers, increase translation invariance. They select the maximum value from neighboring inputs within a translational distance d:

$Y ij = max m,n∈K X i+m,j+n , K = [-d, d] . (2.8)$Max Pooling enhances robustness by retaining key features while reducing sensitivity to small input variations [[1]](#b0).

## Batch Normalization Layer

Batch normalization normalizes the input according to the rule:

$y = γ x -⟨x⟩ k σ 2 k (x) + ϵ + β , (2.9)$where γ = 1, β = 0 are learnable parameters, ϵ is a small constant, ⟨x⟩ k is the mean of the input batch, and σ 2 k is its variance. The layer enhances training speed and testing accuracy by aligning input statistics with a standard distribution [[11]](#b10).

## Residual Layers

In very deep networks, it has been found useful to include shortcuts, called residuals, in the network to further increase the accuracy of the test data. If we denote one of the previously defined layers or multiple layers as a transformation, i.e., y (l) = f (l) (x (l) ), then the shortcut is as simple as: (l) .

$y (l) = f (l) (x (l) ) + x$(2.10)

For the residual to be well-defined, it is required that the dimension of x (l) and y (l)  match, or that we can propagate the input to match the dimension of the output, e.g. when adding to a higher order convolutional layer. While residual networks proved to generalize better than those without shortcuts and without increasing the number of parameters, computing the gradient of batches becomes more costly by increasing the number of floating point operations (FLOP) the network needs to perform greatly [[8]](#b7).

3 Dynamics in Deep Neural Networks

## The Loss Landscape and the Hessian Matrix

To comprehend network dynamics, it is useful to consider the Hessian matrix:

$H ij = ∂ 2 L(w) ∂w i ∂w j (3.1)$Several studies [[3,](#b2)[19]](#b18) propose a theoretical similarity between eigenvectors of the Hessian matrix and the covariance matrix of mean gradients. This similarity is observed in successful networks with a high test accuracy. Following [[13]](#b12), it is posited that:

$L(w) ≈ L min + (w -µ) T H 2 (w -µ) ,(3.2)$where L min represents the minimum of the loss function, and µ denotes the coordinates of the loss function's minimum in weight space.

## Principal Component Analysis

Consider a weight matrix flattened to a vector, denoted as w(t), for a specific layer or the entire network, measured over a sequence of T ≥ n discrete time steps t such that [0, T ] ⊂ N 0 → R n . The covariance matrix can be defined as:

$Σ ij = ⟨w i w j -⟨w i ⟩⟨w j ⟩⟩ , (3.3)$where ⟨X(t)⟩ = 1 T T t=0 X(t). The covariance matrix quantifies the relationships between different components of the weight vector over time. The eigenvectors of the covariance matrix are termed principal components, represented as p i . Corresponding to these eigenvectors are their associated eigenvalues, denoted as σ 2 i :

$Σp i = σ 2 i p i . (3.4)$The eigenvectors furnish an orthonormal basis for the R n space, given the symmetry of Σ. Consequently, it becomes possible to express the weight matrix in this basis:

$w(t) = n i=1 θ i (t)p i , θ i (t) := w(t) • p i . (3.5)$It is noteworthy that the variance of θ(t) corresponds to the eigenvalue of the respective principal component. Using a decomposition into principal components allows to examine the dynamics of weights in a late training phase known as the exploration phase [[3]](#b2). During this phase, the generalization error does not improve significantly, and the training loss function changes gradually. The central notion is that the network is proximate to a minimum of L(w), and further training does not cause it to deviate substantially from this minimum. When the weights vary in the direction of the principal components, the loss function can be represented as:

$L(δθ) i := L(w + δθp i ) . (3.6)$This behavior is observed in [[3]](#b2) to follow a potential well:

$L(δθ) i ∝ δθ 2(3.7)$as δθ → 0. The eigendirections of the loss landscape may not align precisely with the principal components. Nonetheless, this loss function behavior holds true in all directions spanned by eigenvectors of the Hessian matrix with sufficiently large eigenvalues. Due to the observation of a quadratic potential well and by that the principal components and Hessian eigenbasis are supposedly close in being diagonal as shown in [[13]](#b12), the flatness F i of the minima of the principal components and the Hessian eigenvalues can be related to F -2 i ∝ h i . The eigenvalues of the principal components can then be linked to the eigenvalues of the Hessian matrix:

$σ 2 i ∝ h α i , (3.8)$where σ 2 i are the eigenvalues of the principal components in descending order, and h i are the Hessian eigenvalues in descending order. Experimental findings of [[3]](#b2) suggest that α ≈ 2, shown only for the flatness. In particular, [[18]](#b17) shows that the empirical result of α is affected by measuring for only a short period of time and provides a theoretical relationship:

$σ 2 i ∝      h i h i < h cross const h i > h cross ,(3.9)$where h cross := 3S

1-β ηN train , S is the batch size, β signifies the momentum, and N train is the number of training examples per epoch. The dependency of α on the corresponding eigenvalue highlights its non-constant nature.

## Catastrophic Forgetting

Catastrophic forgetting emerges when a network learns multiple tasks independently, resulting in performance degradation on previously learned tasks. This issue stems from optimizing the loss function for one task, inadvertently neglecting others, and allowing their associated loss values to escalate. To mitigate this phenomenon, [[3]](#b2) suggests weakly constraining the largest N lim principal components through a regularization term integrated into the loss function:

$L cf (w) = λ cf N lim i=1 1 F 2 i ((w -μ1 ) • p i ) 2 , (3.10)$where λ cf denotes a positive real regularization constant, μ1 represents the weights of the network or layer at the end of training for the previous task that are close to a minimum µ 1 , and F i signifies the flatness in direction of the principal components, while p i corresponds to their associated principal component, computed after training for the previous task with the dataset of the previous task. It is noteworthy that due to Eq.(3.8), measuring N lim + 1 time steps is deemed sufficient to approximate the N lim the largest Hessian eigenvectors using principal components, rendering computation more feasible, if we assume that the principal components approximate the Hessian eigenvectors sufficiently. In [[3]](#b2) this relation was applied to connect the regularization to the Hessian landscape model. We can use this relation backwards to convert the problem to one of the Hessian:

$L cf (w) = λ cf N lim i=1 h i ((w -μ1 ) • h i ) 2 , (3.11)$where again λ cf denotes a positive real regularization constant, μ1 represents the weights of the network or layer at the end of training for the previous task that are close to a minimum µ 1 , and h i signifies the Hessian eigenvalues in descending order, while h i corresponds to their associated eigenvector, computed after training for the previous task with the dataset of the previous task. When all eigenvalues are considered, this equation exhibits similarity to Eq.(3.2).

## Random Matrix Theory and Singular Value Decomposition in Deep Neural Networks

The weight matrix W ∈ R N ×M of a given layer can be decomposed using singular value decomposition (SVD):

$W = U ΣV T , Σ = diag(ν 1 , ..., ν N ) , (4.1)$where U and V are orthogonal matrices and ν 1 , ..., ν N are the singular values, arranged in decreasing order. These singular values are accompanied by their corresponding right singular vectors, which form the rows of V , and the left singular vectors, which are extracted from U . When elements of the weight matrix W ij adhere to a normal distribution W ij ∼ N (0, σ 2 mp ), an observation can be made for the limit N, M → ∞ with Q := N/M ∈ R ≥1 , which gives rise to a Marchenko-Pastur distribution (MP):

$ρ(ν) =      Q 2πσ 2 mp λ (λ + -λ)(λ -λ -) if λ ∈ [λ -, λ + ] 0 otherwise , (4.2)$where

$λ ± = σ 2 mp 1 ± 1 √ Q 2 [27]$. For deep neural networks, it has been demonstrated that the behavior of singular values mirrors that of random matrices [[27]](#b26). Specifically, they adhere to the Marchenko-Pastur distribution (Eq.(4.2)). This behavior is attributed to their initialization as randomly distributed values, and due to the fact that the majority of weights undergo limited change during training. Among the scrutinized trained networks, a few singular values reside outside the bulk and encapsulate nearly all network information. This is highlighted by the fact that removing bulk singular values by setting them to zero does not detrimentally affect network accuracy [[37]](#b36). Singular values hold a close relationship with the eigenvalues of the PCA applied to the same matrix. The eigenvalues obtained from PCA are essentially the square of the singular values. In light of this connection, the distributions of the unfolded spacings between singular values exhibit a behavior akin to the Wigner surmise, which the RMT theory predicts. This surmise is captured by the equation:

$p(s) = πs 2 e -πs 2 4 , (4.3)$where s = ξ n -ξ n+1 is the unfolded spacing, with ξ n are the singular values, such that the spacings are locally normalized [[26]](#b25). In a recent study [[38]](#b37), compelling evidence was presented demonstrating that the singular values of weight matrices in DNNs conform to the expectations set forth by the Wigner surmise.

## Deep Neural Network Setup

## Datasets

The primary dataset for comparison is CIFAR-10 [[16]](#b15), which consists of 10 classes including airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Each image has 32 × 32 pixels and three color values for red, green, and blue (RGB) from 0 to 255 (uint8), so each image is a tensor of shape [(32,](#b31)[32,](#b31)[3)](#b2). The dataset comprises 60000 images, with 6000 images per class. A validation set of 10000 images is used to estimate the generalization error. While CIFAR-10 serves as a benchmark for smaller networks, more modern, larger networks tend to achieve test accuracies beyond 99.5% [[14]](#b13), making comparisons challenging due to the high accuracy ceiling. For more complex networks, ImageNet [[32]](#b31) is a significant benchmark. However, due to the large network size and long training times, it is not employed in this study. Instead, a simpler dataset, MNIST [[23]](#b22), is additionally used. It consists of 70000 grayscale images of handwritten digits with a 28 × 28 pixel resolution. This dataset is considered to be much simpler than CIFAR-10, since even small MLP networks can perform extremely well on this dataset. Nevertheless, it is historically an important dataset, is computationally extremely easy due to the small image size, and can make analysis more feasible.

## Networks Initialization

The networks' weights are initialized using random seeds. In contemporary networks, each layer is independently initialized using either a uniform or normal distribution. The Glorot initialization [[4]](#b3), also known as Xavier initialization, is used for uniform and normal distribution initialization. For the uniform distribution:

$W (l) ∼ U - 6 n l + n l+1 , 6 n l + n l+1 , (5$.1) Name Dataset # Layers with Weights # Parameters Test Accuracy MLP 50 MNIST 3 42k 98.2% MLP 256 CIFAR-10 4 828k 44.2% LeNet CIFAR-10 7 137k 65.3% miniAlexNet CIFAR-10 8 1023k 74.9% ResNet 20 CIFAR-10 22 272k 68.1% Table 5.1: Summary of network types used in the study.

where W (l) ∈ R n l ×n l+1 represents the weights of the layer. For the normal distribution:

$W (l) ∼ N 0, 2 n l + n l+1 . (5.2)$All biases are initialized to zero. An alternative initialization used in ResNet [[8]](#b7) does not show significant improvements over the Glorot initialization and is therefore omitted in this study.

## Network Types

Table [5](#).1 summarizes the network types used in this study, along with their key attributes.

## MLP 50

The Multilayer Perceptron (MLP) network consists solely of dense layers. ReLU activation is used for all layers except the last one, which employs the softmax activation function. The layer widths are [50, 50, 10]. Fig. [5](#).1 provides a graphical representation of the network. The layer weights are initialized with Glorot uniform. The network is trained on the MNIST dataset for 200 epochs with a learning rate of 0.01. The loss used is cross entropy and the update rule is SGD with a batch size of 32. We do not use momentum.

## MLP 256

The network structure is [256, 128, 64, 10]. Fig. [5](#).1 illustrates the network structure. The layer weights are initialized with Glorot uniform. The biases are set to zero. The network is trained on the CIFAR-10 dataset for 200 epochs with a learning rate schedule η(t) = 0.01×0.99 t , where the time steps t are in epochs, the final learning rate is η(T ) ≈ 0.00134. The loss used is the cross entropy and the update rule is the SGD with a batch size of 32. We do not use momentum. To compare the differences when using weight decay or not, we regularize all layers with a weight decay constant of 5 × 10 -4 or, if unregularized, set the constant to zero. After training, the network achieves a full training accuracy of 100% and a test accuracy of ≈ 44%, depending on the seed and whether regularization is used.

## LeNet

The LeNet network from [[23]](#b22) is a convolutional network optimized for the MNIST dataset. It can also be used for the CIFAR-10 dataset. The first layer is a convolutional layer with six filters and a kernel size of [(5,](#b4)[5)](#b4), where the kernel is padded to preserve the shape of the input. That is, we sum the 5 closest input entries together and that we add zeros at the edges, such that we can still pad there. The next layer is a pooling layer with kernel size [(2,](#b1)[2)](#b1), where the kernel is padded so that the full kernel lies in the input matrix, resulting in a smaller output dimension. This is followed by a second convolutional layer with 16 filters and a kernel size of [(5,](#b4)[5)](#b4). This is followed by a pooling layer with the same properties as before. Next, the outputs are flattened into dense layers of shape [120, 84, 10]. In Fig.

5.2 we can see a graphical representation of the network structure. All layers except the pooling layers, the flattening layer, and the last layer use a ReLU activation function. The final layer has a softmax activation function. Layer weights are InputLayer input: output: [(None, 32, 32, 3)] [(None, 32, 32, 3)] Conv2D relu input: output: (None, 32, 32, 3) (None, 32, 32, 6) MaxPooling2D input: output: (None, 32, 32, 6) (None, 16, 16, 6) Conv2D relu input: output: (None, 16, 16, 6) (None, 16, 16, 16) MaxPooling2D input: output: (None, 16, 16, 16) (None, 8, 8, 16) Flatten input: output: (None, 8, 8, 16) (None, 1024) Dense relu input: output: (None, 1024) (None, 120) Dense relu input: output: (None, 120) (None, 84) Dense softmax input: output: (None, 84) (None, 10) InputLayer input: output: [(None, 32, 32, 3)] [(None, 32, 32, 3)] Conv2D relu input: output: (None, 32, 32, 3) (None, 28, 28, 300) MaxPooling2D input: output: (None, 28, 28, 300) (None, 10, 10, 300) Conv2D relu input: output: (None, 10, 10, 300) (None, 8, 8, 150) MaxPooling2D input: output: (None, 8, 8, 150) (None, 3, 3, 150) Flatten input: output: (None, 3, 3, 150) (None, 1350) Dense relu input: output: (None, 1350) (None, 384) Dense relu input: output: (None, 384) (None, 192) Dense relu input: output: (None, 192) (None, 10) initialized with Glorot normal, and biases are set to zero. The network is trained on the CIFAR-10 dataset for 100 epochs with a learning rate schedule η(t) = 0.005 × 0.98 t , where the time steps t are in epochs, the final learning rate is η(T ) ≈ 0.00134. The loss used is the cross entropy and the update rule is the SGD with a batch size of 64. We use a momentum of β = 0.9. All layers are regularized with a weight decay of λ = 0.0001.

After training, the network reaches a full training accuracy of 100% and a test accuracy of ≈ 65.3%, depending on the seed.

## miniAlexNet

AlexNet [[17]](#b16) is a convolutional network similar to LeNet, but has many more parameters and is optimized for the ImageNet dataset. Its smaller variant, the miniAlexNet or in [[40]](#b39) called small AlexNet, is instead optimized for CIFAR-10 and is used in this work because the original AlexNet is too large for our analysis. The first layer is a convolutional layer with 300 filters and a kernel size of [(5,](#b4)[5)](#b4), where the kernel is padded so that the full kernel lies in the input matrix. The next layer is a pooling layer with kernel size [(3,](#b2)[3)](#b2), where the kernel is padded to preserve the shape of the input. This is followed by a second convolutional layer with 150 filters and a kernel size of [(5,](#b4)[5)](#b4). This is followed by a pooling layer with the same properties as before. Next, the outputs are flattened into dense layers of shape [[384,](#)[192,](#)[10]](#b9). In Fig. [5](#).2 we can see a graphical representation of the network structure. All layers except the pooling layer, the flattening layer, and the last layer have a ReLU activation function. The last layer has a softmax activation function. Layer weights are initialized with Glorot uniform. Biases are set to zero. The network is trained on the CIFAR-10 dataset for 100 epochs with a learning rate schedule η(t) = 0.01 × 0.95 t , where time steps t are in epochs, the final learning rate is η(T ) ≈ 5.9 × 10 -5 . The loss used is the cross entropy and the update rule is the SGD with a batch size of 32. We use a momentum of β = 0.9. All dense layers are regularized with a weight decay of λ = 0.0001.

After training, the network reaches a full training accuracy of 100% and a test accuracy of ≈ 74.9%, depending on the seed. In its original form, there is an additional layer, the local response normalization layer. This layer is not used here because it causes problems in newer versions of TensorFlow and is considered outdated and more or less unimportant in terms of generalization performance [[36]](#b35).

## ResNet20

ResNet [[8]](#b7) is a convolutional network that additionally uses batch normalization and, most importantly, residual layers. The first layer is a convolutional layer with 16 filters and a kernel size of [(3,](#b2)[3)](#b2), where the kernel is padded to preserve the shape of the input. The layer uses batch normalization followed by the ReLU activation function. After this layer, the residual block comes into play. The residual block, where the residual layers lie in, consists of six convolutional layers with 16 × 2 n block -1 filters, where n block is the block counter, a number that starts with 1 and increase with each following additional block, and a kernel size of [(3,](#b2)[3)](#b2). After each convolutional layer, batch normalization and the ReLU activation function are applied. After every second layer, a residual layer is added between the batch normalization and the activation function, connecting the output of the second layer to the input of the first layer. For our ResNet, we use three of these residual blocks. This is followed by an average pooling layer, which averages the spatial indices coming out of the residual block. Finally, the output is flattened with a softmax activation function to match the output dense layer of size 10. Layer weights are initialized with the Glorot normal. Biases are set to zero. The network is trained on the CIFAR-10 dataset for 160 epochs with a learning rate schedule of

$η(t) =            10 -3 t < 80 10 -4 120 > t ≥ 80 10 -5 t ≥ 120 , (5.3)$where the time steps t are in epochs. The loss used is the cross entropy and the update rule is the SGD with a batch size of 128. We use a momentum of β = 0.9. All layers are regularized with a weight decay of λ = 0.0001. After training, the network reaches a full training accuracy of 100% and a test accuracy of ≈ 68.1%, depending on the seed. At first glance, it looks very similar to LeNet in terms of test accuracy, but it can massively outperform LeNet when using data augmentation. This is the creation of new images by cropping and rotating the original images in the dataset so that they still match the label class. The reason why data augmentation is not used here is that it would lead to the question of what is the total training loss and how to derive the Hessian matrix from it.

## Algorithms for Deep Neural Network Analysis

The code used for this thesis is written entirely in Python and can be found on https:// github.com/RosenowGroup/Hessian-eigenvectors-PCA-DNN-weights. The modules used, and their most important functions are explained below.

## Deep Neural Network Framework

The Python module TensorFlow [[28]](#b27) is used to train and analyze the networks. This framework provides almost all important functions for building and testing networks.

In addition, it comes with its own type of tensors that can be used to build special TensorFlow functions that are much faster than Python functions because they allow parallelization and use of the graphics processing unit (GPU). Another very useful tool in TensorFlow is auto-differentiation. TensorFlow keeps track of all the operations used to compute e.g. the loss and allows a numerically optimal, precise evaluation of the gradient through predefined exact derivatives. We can expect all results to be independent of the framework. Deviations can be caused by different random seeds, e.g. during initialization, or by numerical instabilities during training.

## Computation of the Hessian matrix

The Hessian matrix must be computed as a function of all training images or a sufficiently large subset. We need to extract the second derivative of the loss. To do this, we can use the auto-differentiation on the first derivative. In Lst.5.1 we can see how to compute a import tensorflow as tf with tf . GradientTape () as g : with tf . GradientTape () as gg : predictions = model ( images ) loss = loss_object ( labels , predictions ) gradient = gg . gradient ( loss , weights ) hvp = g . gradient ( gradient , weights , output_gradients = vector ) Listing 5.1: Computation of the Hessian vector product in TensorFlow. Here we record the gradient of the gradient of the loss (loss_object) of the network output (model(images)) in direction of a given vector (vector).

Hessian vector product (hvp) Hv. model is the model function that returns the softmax predictions of an input. The loss_object computes the loss function from the network's predictions and the actual true labels. tf.GradientTape tells TensorFlow to record all the operations it performs, from which we can take the gradient g.gradient from an output to an input. output_gradients=vector tells the gradient tape to compute the vector product of the gradient matrix and the vector. To compute the Hessian matrix, we can use the Hessian vector product of the standard basis to reconstruct its rows.

While TensorFlow offers the possibility to compute the Hessian directly, this requires more memory and can lead to bugs in older TensorFlow versions (we use TensorFlow 2.4). Another method to extract the eigenvalues and eigenvectors is to use the Lanczos algorithm [[20]](#b19). Here we start with a random vector from which we compute the hvp, extract a new vector, which is then used for the next hvp. Running this algorithm n times iteratively yields a tridiagonal matrix with n -1 eigenvalues that approximate the largest eigenvalues in magnitude. In addition, the eigenvectors can also be extracted.

The original algorithm leads to numerical instability, which can be avoided by several methods. One is to use science.sparse.linalg.eigsh from the SciPy [[39]](#b38) module, which is accurate but very slow since it requires computing twice the number of hvp of the desired eigenvectors. Another faster but less accurate implementation can be found in the GitHub old folder of the project for this thesis linked earlier.

## Computation of tensor objects

When we measure the weights of the networks, we store them as NumPy arrays. NumPy [[7]](#b6) is a module that contains many different operations that can be performed on these arrays. These arrays are tensors of arbitrary shape. To compute the covariance matrix, we can simply use numpy.cov. To get the principal components, we can then use numpy.linalg.eigh or tensorflow.linalg.eigh to compute the eigenvalues and eigenvectors of a hermitian or symmetric matrix. The TensorFlow function can use the GPU if the memory space is large enough, and it is also more stable when handling larger matrices, but does not return errors if there are discrepancies. Similarly, for SVD, there is a function linalg.svd in both modules to compute the singular values and their singular vectors.

6 Analysis of the Dynamic Weight Matrix

## Comparison of Singular Values and Principal

## Components

Our examination begins with a thorough exploration of the relationship between singular values and principal components of the network weights. Although the dimensions of the left and right singular vectors differ from those of the originating weight matrix, a transformation involving matrices Wi of single singular values brings these vectors together:

$Wi = U Σi V T , Σi = diag(0, ..., ν i , 0, ..., 0) . (6.1)$Assembling these matrices, each comprising a left and a right singular vector, culminates in the reconstruction of the original weight matrix. The flattening of Wi into a vector wi facilitates the analysis of scalar products with the principal components p i using the equation:

$S ij = |p i • wj | ∥ wj ∥ . (6.2)$Note that we are considering the absolute, since the choice of sign of the principal components is arbitrary. Unless otherwise noted, all principal component and singular value computations are performed after training. The visualization in Fig. [6](#).1 unveils intriguing insights into the interaction between principal components and singular values. Notably, principal components show an interesting correlation with singular values. Remarkably, the principal components with the lowest indices have the largest product with singular values of the lowest indices. Here, the variances of the principal components and singular values underscores that the principal components with the highest variance shares a strong connection with the singular matrices of singular values situated outside the RMT bulk. At the same time, principal components with lower variances entail smaller product values relative to the former, yet display larger product values relative to singular values residing within the RMT bulk. Given the undeniable significance of singular values outside the RMT bulk as seen in Sec.4, we infer that the initial principal components encapsulate critical information due to their substantial scalar product. In the context of weight de- cay, the alignment of the largest singular values with the largest principal components underscores the presence of a distinct boundary between singular values from within and outside the RMT bulk, as depicted by their scalar product with principal components. Zooming in to focus on the largest principal components, Fig. [6](#).2 exposes that the scalar product of the first principal component surpasses that of the subsequent components by a significant margin. This phenomenon, not apparent in the previous figure due to color resolution, showcases an ordering of scalar products across the first principal component. Remarkably, the scalar product decreases as we examine singular matrices of smaller singular values. For weight decay, the first principal component's scalar product is primarily attributed to the singular matrices within the RMT bulk, as opposed to the training without weight decay, where this occurs predominantly outside the RMT bulk. This phenomenon could be attributed to the comparatively small change in the outlying singular values induced by weight decay, juxtaposed with the substantial decrease of singular values in the bulk necessitated by network optimization, since outlying singular values will stagnate in size.

## Principal Components' Influence on Network Performance

Shifting focus, we investigate the influence of principal components on network performance. We can introduce an additive weight vector based on principal components. This can be described as the follows:

$w i,add = i j=1 θ j (T )p j , θ j (T ) := w(T ) • p j , (6.3)$where T represents the time of training completion. Fig. [6](#).3 showcases the impact of principal component addition on network accuracy. Evidently, the unregularized network harbors substantial information within the first principal component, emphasizing the dominance of the first principal components. Conversely, the network employing weight decay necessitates the incorporation of approximately the first 500 largest principal components to achieve peak accuracy. This divergence could be attributed to the fact that the first principal component, as seen in Fig. [6](#).2 with weight decay, no longer covers the outlying singular values primarily. Furthermore, we avoid the training on a subset of trainable variables during measurement to avoid undesirable outcomes. If a single layer is measured and the layers between that layer and the output are not updated, the high scalar product of the first principal component with the weights will disappear, and the

0 2500 5000 7500 10000 12500 15000 t in batches 4.115 4.120 4.125 4.130 4.135 θ1(t) Figure 6.4: Weight development in direction of the first principal component of the (128, 64) layer of the regularized MLP 256 network. loss will almost stay constant for further training. To avoid this behavior, which is not a practical training dynamic, we always update all layers.

## Analysis of the Drift Mode

In the direction of the first principal component, a clear linear behavior emerges, as depicted in Fig. [6](#).4. This behavior is well-captured by:

$θ 1 (t) = w(t) • p 1 ≈ a(t -t 0 ) + b, t ≥ t 0 , (6.4)$where t 0 marks the commencement of our measurement during the exploration phase, a is the slope and b = w(t 0 ) • p 1 . Notably, all other principal components deviate from linearity and exhibit a random walk pattern, with their effects on the network's weights remaining neglectable. Moreover, the dynamics of unregularized networks reveal that |θ 1 (t)| experiences growth over time. In contrast, networks featuring weight decay exhibit the shrinkage of both |θ 1 (t)| and ∥w∥. Considering the evolution of ∥w(t)∥, we discern an almost linear function approximately as for |θ 1 (t)|. We refer to the movement of the network as the "drift mode" for its conspicuous linear evolution [[3]](#b2). This observation paves the way for the approximation

$w(t) ≈ a(t -t 0 ) + w(t 0 ) , (6.5)$where a = ap 1 . This result stems from the fact that only the drift mode is responsible for persistent changes within the network. The quantification of the drift mode contribution leads us to derive its variance to gain deeper insights. Mathematically, the variance σ 2 can be expressed as:

$σ 2 1 = ⟨θ 1 (t) 2 ⟩ -⟨θ 1 (t)⟩ 2 = 1 T T +t 0 t=t 0 a 2 (t -t 0 ) 2 + b 2 + 2a(t -t 0 )b -   1 T T +t 0 t=t 0 (at + b)   2 = a 2 12 (T 2 -1) , (6.6)$where T signifies the number of measured time s. The application of Faulhaber's formula facilitates the solution of the sums, yielding a clear representation of the variance as a function of a, T , and the network's configuration. Since each update step points approximately in the same direction, we can connect the slope to the learning rate by using the SGD:

$a ≈ ∥ w(t) -w(t 0 ) t -t 0 ∥ ∝ η . (6.7)$As observed previously, when we extend the length of the observation interval or increase the size of the learning rate, the eigenvector of the largest eigenvalue of the covariance matrix approximates the drift mode, but only if it has a larger variance than those of the largest noise. This intriguing relationship can be validated by training the network up to the exploration phase using a high learning rate, effectively mitigating computational time constraints for low learning rates. The observed relationship, σ 2 ∝ η 2 T 2 , holds for appropriate learning rates and measurement time s. Furthermore, for extensive epochs, here for this comparatively small network ≈ 600 epochs, the feasibility of measuring every batch becomes constrained due to memory limitations. However, given the drift mode's computationally efficient characterization via a linear fit, it suffices to measure weights at more extended intervals, such as epochs. Remarkably, as demonstrated by the MLP 256 network in Fig. [6](#).5, the drift mode ceases to exhibit a linear trend beyond approximately 500 epochs. This indicates the presence of higher-order terms contributing to the observed behavior.

To get a better understanding on this evolving behavior, a detailed analysis of the loss landscape within the drift mode's direction is crucial. The depiction in Fig. [6](#).6 illustrates the presence of a potential well aligned with the drift mode's direction. Notably, this characteristic potential well corresponds to the presence of similar potential wells aligned with the Hessian eigenvectors, as described in Eq.(3.2). Importantly, this phenomenon is qualitatively observed across our used network architectures, limiting the explanation, that its causality being rooted solely in regularization effects.

The drift mode's dynamic behavior, marked by adjustments toward the loss minimum, offers insights into network training dynamics. Nonetheless, the minuscule magnitude of these adjustments implies that these insights may not be of great importance within the context of training for optimal performance. 

## A Model for the Drift Mode

We begin by considering the quadratic loss approximation:

$L(w) ≈ L min + (w -µ) T H 2 (w -µ) ,(6.8)$where L min is the minimum of the loss, µ is the point of the minimum in weight space and H is the Hessian matrix. We neglect momentum and include weight decay parametrized by its strength λ. The average update step of the Stochastic Gradient Descent is given by:

$⟨ dw(t) dt ⟩ B = - η S (H + 2λ)w + ηHµ , (6.9)$where ⟨•⟩ B is the average over all batches and S the batch size. Let us assume that during the exploration phase, each update step is similar to the average update:

$dw(t) dt ≈ ⟨ dw(t) dt ⟩ B . (6.10)$This makes the differential equation linear and of first order, allowing us to solve it explicitly:

$ŵi (t) = ŵi (0) - μi 1 + 2λ/h i e -η(h i +λ)t + μi 1 + 2λ/h i , (6.11)$where ŵi (t) = w(t) • h i , μi = µ • h i and h i are the Hessian eigenvalues their corresponding eigenvectors h i . When employing weight decay, the network's minimum is shifted to lower values. The drift mode can be decomposed using Hessian eigenvectors, since those form an eigenbasis in the weight space. The first-order approximation of the drift mode can be related to the observed drift mode in Sec.6.3. For longer measurements, an exponential decay is expected, as observed in Fig. [6](#).5. The variance is computed using:

$σ 2 i = ⟨ ŵi (t) 2 ⟩ -⟨ ŵi (t)⟩ 2 = 1 T T t=0 [ ŵi (0) -b i ] 2 e -2 λi t + b 2 i + 2 [ ŵi (0) -b i ] b i e -λi t - 1 T T t=0 ( ŵi (0) -b i )e -λi t + b i 2 = 1 T T t=0 ( ŵi (0) -b i ) 2 e -2 λi t - 1 T 2 T t=0 ( ŵi (0) -b i )e -λi t 2 = ( ŵi (0) -b i ) 2 T    1 -e -2 λi (T +1) 1 -e -2 λi - 1 T   1 -e -λi (T +1) 1 -e -λi   2    ,(6.12)$where λi := η(h i + 2λ) and b i := μi 1+2λ/h i . The geometric sum formula is used to solve the sums. Notably, the variance of the drift converges to zero as time increases, indicating that it does not increase indefinitely. In this model, the drift mode is a sum of the Hessian eigenvectors. If we decompose the drift as p 1 = i d i h i , with d i := h i • p 1 , then the variance of the drift mode is given by:

$σ 2 d = i d 2 i σ 2 i . (6.13)$The coefficients d i with i d 2 i = 1 are chosen such that σ 2 d maximizes when the drift mode aligns with the first principal component. 

## Loss Scaling

Consider an unregularized network employing softmax that achieves a training accuracy of 100%. If we increase the weights' size by a factor of α, it will enhance confidence in the probability distribution and consequently reduce the loss, without altering the training accuracy. Assuming that the logits of the correct labels significantly exceed the others, i.e., z j ≫ z k , ∀k ̸ = j, we can demonstrate that:

$L(αz) = -ln e αz j i e αz i = -αz j + αz j + ln(1 + i̸ =j e α(z i -z j ) ) = i̸ =j e α(z i -z j ) + O      i̸ =j e α(z i -z j )   2    ∝ e -αz j . (6.14)$Using ReLU and scaling the weights layer by layer, we expect z i ∝ α j a j w j , ∀i, where a j are coefficients dependent on the input. Fig. [6](#).7 illustrates that this assumption agrees with experimental results for the unregularized network, considering that scaling the weights proportionally changes the logits. Interestingly, while numerically increasing the weights can drive the loss to zero in this direction, the network tends to move in the direction of the drift mode instead. Both directions point in a similar direction, considering their large product ≈ 0.7 here, but they are not completely parallel. For the regularized network, we observe that L ∝ α 2 when scaling the network weights further. Therefore, scaling a network with weight decay can not reduce the loss further.

## Analysis of the Hessian Matrix

In this chapter, we delve deeper into the eigenvectors of the Hessian matrix. Let H ∈ R n×n be the Hessian matrix with eigenvectors h i and corresponding eigenvalues h i :

$Hh i = h i h i , h i ≥ h i+1 . (7.1)$Because the Hessian matrix is symmetric, its eigenvalues are real, and its eigenvectors form an orthonormal basis.

Starting from the quadratic loss approximation in Eq.(3.2), we can interpret positive eigenvalues as measures of the curvature of minima along the corresponding eigenvector direction. Hence, eigenvectors corresponding to the largest eigenvalues are the crucial directions for loss minimization [[3]](#b2). These eigenvectors define regions in which the network must be closer to the minimum µ • h i than in directions with smaller positive eigenvalues.

## Hessian Eigenvectors and PCA

By comparing the absolute scalar product of the principal components of weights and velocities v(t + 1) = w(t + 1) -w(t) with the Hessian eigenvectors, as depicted in Fig. [7](#).1, we observe that the eigenvectors for the largest and smallest eigenvalues are quite similar for all bases. For intermediate eigenvalues, the similarity is diminished, but they still maintain some degree of diagonalization. Further, we observe that the velocity covariance matrix exhibits a sharper diagonal in the Hessian eigenbasis than for the eigenvectors of the weight covariance matrix. This indicates that the velocity eigenbasis is a more suitable choice for approximating the Hessian eigenbasis with a covariance matrix, potentially reducing computational time [[3]](#b2). Notably, measuring velocities instead of weights incurs no additional computational cost, as both are calculated for each update step.

## Eigenvectors and the Weight Product

Consider the scalar product of the Hessian eigenvectors with the network weights:  referred to as the weight product. In Fig. [7](#).2, it is evident that without weight decay, eigenvectors associated with larger eigenvalues have a smaller weight product with the network, compared to randomly distributed eigenvectors associated with smaller eigenvalues. When weight decay is employed, these values become comparable in magnitude.

$h i • w(T ) ∥w(T )∥ , (7$For both scenarios, the largest scalar products are found between the 500th and 2000th eigenvectors in proximity to each other. For large enough index ranges ≳ 500, the weight product distribution appears Gaussian in local regions with varying variances when testing those on the distribution. This behavior is anticipated in all networks. An explanation could be that we initialize layers uniformly or normally. The central limit theorem establishes that the sum of uniformly drawn numbers converges to a Gaussian distribution.

Assuming that the weights are mainly random, which is underlined by the fact that the network only trained in a small part of the Hessian eigenvectors, this can explain why the weight product still follow the distribution. The locations of the largest weight products, especially in unregularized networks, could potentially be due to the loss scaling discussed in Sec.6.5. The absence of a potential well in the direction of the entire network, primarily encompassed by large entries, might cause the potential wells of eigenvectors associated with these large entries to not be the steepest. Additionally, symmetry properties of the network could account for the small products observed with the largest eigenvalues. If we presume that eigenvectors corresponding to the largest eigenvalues are translations of labels or rotations, altering entries within a layer could lead to a substantial loss change, potentially misclassifying all labels [[6]](#b5). For other networks and layers, as depicted in Fig. [7](#).3, the precise positioning of the large weight products within the Hessian spectrum remains without a model. Nonetheless, it is clear that layers with more parameters correspond to higher indices where the product is large.

Let us now decompose the weights of the layer into the eigenbasis of the Hessian: In Fig. [7](#).4, it becomes evident that eigenvectors with the largest weight product must be aggregated to achieve optimal network performance. Setting all eigenvectors to zero except those with the largest product does not lead to accuracy degradation. For eigenvectors with small eigenvalues, this is easily explained by arguing that changing θ j from a certain small product in magnitude with the weights to zero will not change the loss much because their potential well is very flat. For the eigenvectors of the largest eigenvalues, this may be similar, since their weight product is small in magnitude, but it is not straightforward that because of their steep potential well, even small changes in θ j should not change the loss, and thus the accuracy, at all. This will be discussed more in detail for the whole network analysis. For the eigenvectors with a large product, it is plausible that setting θ j to zero will change the loss greatly, because θ j is large compared to other products and the eigenvalue is in the regime of being larger than most other eigenvalues of the Hessian. Comparing Fig. [7](#).2 and Fig. [7](#).4, we may also conclude that only a fraction of about ≲ 20% of the largest Hessian eigenvalues is sufficient to fully describe the network performance. For other networks the behavior is similar. However, the position for the large weight product differs, and with that the fraction of important eigenvalues.

$w i,add = i j=1 θ j h j , θ j := w(T ) • h j . (7$
## Comparison of Singular Values and Hessian

## Eigenvectors

Let us explore the relationship between the product of Hessian eigenvectors and singular matrices: In the context of the networks used, we notice that singular values beyond the bulk almost entirely encompass the largest Hessian eigenvalues. This observation elucidates why the largest singular values are adequate to maintain the accuracy demonstrated in [[37]](#b36), as they encapsulate the directions from Fig. [7](#).4 that are sufficient to achieve full training and test accuracy. A comparison between these findings and those related to principal components (Fig. [6](#).1) reveals that both the Hessian eigenvector basis and the principal components behave similarly, likely due to their near-diagonal relationship with each other (Fig. [7](#).1). In scenarios where weight decay is employed, the arrangement of the largest singular values that cover the prominent Hessian eigenvalues becomes more pronounced. The validation of these results extends to convolutional layers as well. For this purpose, we must transform the four-dimensional tensor into a two-dimensional matrix. Various methods can be employed for this reshaping process. Previous work [[37,](#b36)[27]](#b26) has suggested that the specific reshaping technique may not significantly impact the results. Therefore, we opt to reshape the first two dimensions and the last two dimensions together. This approach ensures that we attain the maximum possible number of singular values for the given layer. Remarkably, as illustrated in Fig. [7](#).6, this layer-wise analysis produces qualitatively similar behaviors when the layer has an adequate number of singular values ≳ 20.  

$F ij = |h i • wj | ∥ wj ∥ . (7$
## Comparison of the Hessian of Layers and the Full

## Network

Up to this point, our focus has primarily been on analyzing the Hessian of individual layers. The underlying assumption is that each layer's Hessian must broadly exhibit the properties of the Hessian of the entire network. This assumption stems from the notion that each layer can be viewed as a network in itself, and the largest eigenvalues of each layer must align in a similar direction with the largest eigenvalues of the entire network when the eigenvectors of a layer are appended to the network's structure.

Let h i ∈ R n represent the ith eigenvector of the Hessian of the whole network, and

$h (l)$i ∈ R n l denote the ith eigenvector of the Hessian of layer l. We can calculate:

$|h i • h(l) j | , (7.5)$where h(l) j = (0, ..., h

$(l) j1 , ..., h (l)$jn l , 0, ...) ∈ R n . In Fig. [7](#).7, it becomes evident that, while the eigenvectors of the largest eigenvalues for both the layer and the entire network point in The larger product of the largest indices refer to negative eigenvalues that are of similar size as indices in the range of 8000 to 10000. similar directions in the subspace, they are not identical. We can also observe a difference in all the negative eigenvalues of the total network, which fit into the picture when the absolute values are taken and the eigenvalues are resorted accordingly. This observation lends credence that computing individual layer Hessian eigenvectors can be used to approximate the whole Hessian eigenvectors, given that only the largest eigenvalues need to be considered to describe the layer's behavior (as seen in Sec.7.2). Fig. [7](#).8 reveals that the weight product for the entire network exhibits behavior similar to that of individual layers for the MLP 50 network. This behavior was observed as well for the largest 12000 Hessian eigenvalues of the MLP 256 network. However, the indices with large weight products are relatively smaller in magnitude. The larger products towards the last indices correspond to negative eigenvalues that are of a magnitude similar to  positive eigenvalues with the same weight product. By decomposing the network into its Hessian eigenbasis and adding those together beginning with the largest eigenvalues (as in Eq.7.3), Fig. [7](#).9 shows a similar behavior as for individual layers and illustrates that the eigenvectors of negative eigenvalues are necessary to achieve the network's full performance. Consequently, an alternative approach could involve sorting eigenvalues in decreasing order by absolute size. The accuracy for this sorting scheme is depicted in Fig. [7](#).10, where the network achieves full performance qualitatively similar to the behavior of individual layers, with approximately ≈ 2500 added eigenvectors. In Fig. [7](#).11 we can see that the accuracy start to improve not earlier than for about the 2000th eigenvalue and that during this transition the loss increases instead.

To further comprehend the significance of directions with large weight products for accu- racy, we can inspect the loss landscape with:

$L (w + (α -w • h i )h i ) . (7.6)$For α = 0 we remove the ith eigenvector completely from the network and for α = w • h i it is fully included. Fig. [7](#).12 demonstrates that the potential well corresponding to the larger eigenvalue is indeed steeper, yet so close to zero that its importance cannot be shown by simply setting its projection to zero. For both potentials, it is observable that the quadratic approximation holds only for α -w • h i ≲ 0.1. Setting the projection of the eigenvector with the largest weight product to zero takes us far from the quadratic approximation. While the loss prediction becomes less accurate, the qualitative description of a significantly larger loss remains valid. We observe from Fig. [7](#).13 that the hypothesis stating that the minimum of the largest eigenvalues are in proximity to zero holds. Using the flattened singular matrices, we can once again compare them to the Hessian eigenvectors, similar to the layer and network eigenvector comparison. Fig. [7](#).14 reveals that only the 12000 largest eigenvalues in magnitude have a relatively large product to the flattened singular matrices that make them distinct from smaller eigenvalues. Outlying singular values exhibit significant scalar products with eigenvectors of smaller indices, while singular values within the bulk display larger scalar products with eigenvectors of larger indices. 

## Distribution of the Hessian Eigendecomposition

## Distribution of Eigenvalues

Examining the spectral distribution of the Hessian eigenvalues in Fig. [7](#).15, it is apparent that the largest eigenvalues greatly surpass the bulk of eigenvalues in magnitude, and a small fraction of eigenvalues are negative. The introduction of weight decay alters the Hessian matrix to be summed with 2λ1 n×n , leading to a shift of all eigenvalues by 2λ. Consequently, the primary concentration of eigenvalues shifts from zero to 2λ. Weight decay change the location of the minimum and the minimum found by the network during training. The spacing of eigenvalues does not conform to the Wigner surmise, as evidenced by Fig. [7](#).16. Suggesting that the Hessian eigenvalues are not randomly distributed. The standard deviation of the smoothing kernel is set to one. Adapted from [[38]](#b37).

## Properties of Eigenvectors

The Porter-Thomas distribution [[30]](#b29) predicts that a vector v with random entries, having its entries sorted in increasing order ṽ, we have that

$1 + erf(ṽ i N 2 ) 2 ≈ i/N , (7.7)$where erf is the error function and N the length of the vector. Assessment against the Porter-Thomas distribution, reveals that none of the eigenvectors conform to this random distribution. A p-value near 0.

5 would indicate that the eigenvectors follow the distribution, but numerically, all p-values are effectively zero. This indicates that the eigenvectors deviate substantially from the distribution. 0 2000 4000 6000 8000 i 0.000 0.002 0.004 0.006 0.008 0.010 p values averaged Figure 7.17: Averaged p-values of the (128, 64) layer of the unregularized MLP 256 network. Averaged over the 15 closest eigenvalues in both directions. This conclusion holds for the Hessian of the entire MLP 50 network, and for the eigenvalues and eigenvectors of the Hessian of individual layers with 4096 -42310 parameters of all other networks as discussed in Sec.5.2. In instances where layers have few parameters, the p-values may reach approximately ≈ 0.02 at most when averaged over the p-values of eigenvectors from local groups, as shown in Fig.7.17. Larger p-values might be attributed to eigenvectors having fewer parameters or being closer to random than the eigenvectors of the whole network.

## Layerwise Concentration of Eigenvectors

Considering the square of the norm for sections of total eigenvectors:

$∥h i ∥ 2 l = j∈I l h 2 ij ,(7.8)$where I l represents the interval of indices within layer l, Fig. [7](#).18 demonstrates that for most small eigenvalues, localization predominantly resides in the first layer. Assuming vector entries are completely random, the expected concentration fraction for each layer would correspond to the ratio of its number of parameters to the total number of parameters. In the provided figure, the first layer is anticipated to have a fraction of ≈ 0.928, the second ≈ 0.06, and the third ≈ 0.012. This comparison highlights that eigenvectors of the largest eigenvalues are more concentrated in later layers. One plausible explanation for this concentration lies in the vanishing gradient problem, layers farther from the output layer tend to possess smaller gradients due to the multiplication of gradients from subsequent layers. This training characteristic results in more attention directed toward later layers, which can be expected to possess steeper minima, leading to larger Hessian eigenvalues. 

## Development of the Network in the Direction of Hessian Eigenvectors

As observed in Sec.7.2, the scalar product of Hessian eigenvectors and network weights demonstrates structured behavior. To gain insight into this behavior, we examine how eigenvectors and weights evolve during training, not only during the exploration phase but also during initialization. Fig. [7](#). [19](#b18) indicates that even at initialization, the weight product for larger Hessian eigenvalues is smaller than for smaller Hessian eigenvalues, that were computed after training. This suggests that network initialization contributes to determining the final network shape, despite employing random initialization. In Fig. [7](#).20 the eigenvalues are already shared a similar spectrum as to Fig. [7](#).15 before training, with a few large eigenvalues and numerous others clustered around zero. This suggests that the dataset and the structure of the network significantly influence the distribution of Hessian eigenvalues, as they are the only contributions of the network not initialized randomly. Although the weight product for these eigenvalues before training seems more randomly distributed, it yields smaller products for larger eigenvalues than for smaller ones. Fig. [7](#).21 illustrates that the inverse participation ratio ipr(h i ) = j h 4 ij is large for eigenvectors with the largest and smallest eigenvalues, while it is notably smaller for eigenvectors in between. However, it remains localized, in comparison to the full delocalization of 1/n = 1/8192 ≈ 0.0001 for the considered layer, where each component has a value of 1/ √ n. Let us assume that weights at initialization follow a random distribution with zero mean and variance σ 2 , while eigenvector components possess zero mean and variance σ 2 i , with  both variables assumed independent. Accordingly:

$⟨(w • h i ) 2 ⟩ = j ⟨w 2 j ⟩⟨h 2 ij ⟩ = σ 2 σ 2 i = σ 2 , (7.9)$where we leverage eigenvector normalization, i.e., 1 = ⟨∥h i ∥ 2 ⟩ = j ⟨h 2 ij ⟩ = σ 2 i . Hence, the eigenvectors and weights should not be assumed to be independent, as the distribution of the weight product would then not depend on the index.

## Catastrophic Forgetting

In Sec.3.3, we proposed, following the idea of [[3]](#b2), that preserving the larger Hessian eigenvalues can potentially lead to maintaining test accuracies for other tasks in terms of 

$L cf (w) = λ cf N lim i=1 h i ((w -μ1 ) • h i ) 2 (7.10)$into the loss function. Here, λ cf represents a positive real regularization constant, μ1 denotes the weights, h i signifies Hessian eigenvalues in descending order, and h i corresponds to their associated eigenvector, all at the end of training for the previous task. Building upon the insights from Sec.7.2 and Sec.7.3, we propose a novel regularization term:

$L sv (w) = λ sv Nout i=1 ν i (w -μ1 ) • wi ∥ wi ∥ 2 , (7.11)$where λ sv is a regularization constant, N out denotes the number of singular values outside the RMT bulk, ν i are the singular values, wi are the corresponding flattened singular matrices, where the diagonal matrix had all entries set to zero except the ith singular value, and μ1 is the weight vector after training for the first task. This regularization strategy brings two computational advantages. Firstly, computing the singular values of a weight matrix is significantly less demanding in terms of memory and computation compared to the computation of Hessian eigenvalues. Secondly, during each training step where ∇ w L sv needs to be computed, the sum is performed over far fewer indices compared to N lim , as we can expect N out ≪ N lim . For biases, this regularization procedure is not directly applicable, but due to their relatively small vectors in comparison to the weight matrices of layers, their Hessian matrices can be computed efficiently and thus regularized similarly to the approach of the Hessian eigenvalues.

To empirically compare the effectiveness of our proposed singular value approach against the Hessian eigenvalue method, we adopt a strategy similar to that detailed in [[3]](#b2). The experimental setup involves training a network to attain full training accuracy across all tasks at once. This entails employing pre-trained networks and reinitializing the layers under investigation for our measurement. Specifically, we focus on the layers relevant to our analysis and train it exclusively to achieve full training accuracy for the first task. Subsequently, we extract singular values and Hessian eigenvalues from this process and introduce the corresponding regularization component to the layers. By then training the modified layers on the second task, we can assess the impact on the first task's performance. Without regularization, this can lead to a significant decrease in performance, potentially plummeting to levels comparable to random guessing and lower. This is due to the network's inclination to predict labels for the second task, thereby undermining its competence in the first task. Notably, the first task involves classifying the first five label categories, while the second task involves classifying the remaining five label categories from the CIFAR-10 dataset. We further train the network for 50 epochs using each method, selecting the results of the epoch with the highest sum of accuracies for both tasks. Both methods employ a substantial regularization constant of λ cf = 1000. However, higher constants could potentially lead to numerical instability after a single training step, depending on the learning rate. The learning rate here is set to 1% of the final learning rate from the preceding training phase. Lower learning rates favor the preservation of the initial task's accuracy, though at the cost of requiring more epochs to achieve acceptable accuracy for the second task. For the singular value method, we conserve either 20% of the Hessian eigenvalues of the biases or the five largest singular values of matrices and higher order tensors. While preserving weights based on previous tasks might seem unconventional, since the solution we fix the regularization on might not be the optimal solution for both tasks, an alternative approach is to directly impose the constraint on gradients rather than loss. We here propose the equation:

$ṽ(t) := v(t) -γ Nout i=1 ν i ν 1 (v • wi ) wi , (7.12)$where v(t) is the velocity of the unregularized network, and γ ∈ [0, 1] represents the constraint strength. When γ = 1, the equation effectively prevents weight updates from occurring in conserved directions. The fraction of singular values in the equation ensures that the largest singular value and singular matrix is fully conserved, while smaller ones are conserved partially. A similar fraction is employed for Hessian eigenvalues. Importantly, this gradient-constraint approach circumvents numerical instability, rendering it a more stable option. Fig. [7](#).22 presents the results for both gradient and loss-based methods. When comparing the sum of test accuracies, the former regularization method directly conserving weights yields superior overall performance. Intriguingly, both the singular value and Hessian methods yield comparable regularization performance, regardless of the fact that the sin-gular value method conserves fewer directions. Consistent results as in the figure are achieved when performing separate computations for each layer individually, including convolutional layers. However, the desire to preserve all layers simultaneously leads to a significant decrease in overall performance, as seen in Fig. [7](#).23. The maximal overall test accuracy achieved using the regularization singular value loss method on the whole LeNet is 35%, while the gradient singular value approach results in a maximum test accuracy of 25%, still 5% higher than random guessing for the five-class tasks. It is important to note that due to training on two tasks, the limit for random guessing is no longer 20% for each task, but 0%, as the network could classify all images into labels from the other task. The minimum of both tasks averaged is 10%, similar to the case of training for all labels simultaneously. This interpretation allows us to see these accuracies as not entirely incorrect but slightly lower compared to training without tasks. An explanation for the better performance when training only a few layers could be that when the network is initially trained on all tasks, it might seek a solution that approximates solving all tasks simultaneously. However, preserving a layer could guide the network towards a solution more similar to the pre-reinitialization solution. By training the network exclusively on one task, the need for compatibility with the other task's solution becomes unnecessary. This was further investigated by training multiple layers simultaneously to assess the accuracy of this assumption. The obtained accuracies remain comparable to those in Fig. [7](#).22 when training layers with fewer parameters. This suggests that the improvement in accuracy in some layers could be attributed to the fixation of other layers to stable solutions. When training multiple layers, the Hessian eigenvector regularization method seems to outperform the singular value method by up to 2%. This result might be due to the singular value method not fully covering the weight space. Alternatively, there could be room for refining the optimization of regularization constraints, an aspect that warrants further investigation.

## Discussion

## Generalizability of our Results

In this section, we discuss into the generality of our results concerning network structures that were not explicitly included in our analysis. It is worth noting that the influence of the update rule and batch size primarily manifests itself in the context of principal component analysis, since it is a dynamical property. Conversely, the Hessian matrix and singular value decomposition remain relatively invariant with respect to these properties. Their influence, if any, operates indirectly through the minima that they help identify.

## Momentum

The impact of momentum in early training is notable, as it can influence the minima discovered by the network. When momentum β is applied during the exploration phase, it alters the effective learning rate, assuming a near-constant gradient. Utilizing the SGD equation v(t + 1) = -η∇ w l (k(t)) (w, x, z) + βv(t), where η is the learning rate, l (k(t)) the loss of batch B k and w(t + 1) = w(t) + v(t + 1), we can deduce:

$⟨v(t)⟩ B = -η t t ′ =0 β t-t ′ ⟨∇ w l (k(t ′ )) (w, x, z)⟩ B = - η S ∇ w L t t ′ =0 β t-t ′ = - η S ∇ w L t t ′ =0 β t ′ = - η S 1 -β t+1 1 -β ∇ w L ≈ - η S(1 -β) ∇ w L ,(8.1)$where we used the geometric series formula and that ⟨∇ w l (k) (w, x, z)⟩ B = 1 S ∇ w L is the gradient of the mean loss of all batches. Momentum renormalizes the bare learning rate to become effectively larger. Excluding this, momentum's presence does not alter the observed qualitative behavior for the PCA, the findings indicate the generalizability of these observations to various minima that can be approximated by the network utilizing momentum.

## Variants of SGD

While momentum's effect on the minimum in our thesis is understood, the effect under other SGD variants like Adam [[15]](#b14) and Lion [[2]](#b1) remains unexplored in our thesis. The dynamics of update steps for these methods are more complex. However, it is anticipated that network parameters may tend to converge to flat minima, resembling those of SGD. This commonality could lead to similar qualitative behaviors observed with SGD.

## Batch Size

This work has not explicitly addressed the impact of batch size. Qualitative results have been validated for typical batch sizes within the range of S ∈ [[32,](#b31)[512]](#) for SGD. Altering the batch size influences the minima found by the network [[13]](#b12). Larger batch sizes have been found to yield steeper loss landscapes, corresponding to larger eigenvalues in the Hessian matrix [[13]](#b12).

## Layer and Network Analysis

Analyzing network weights can be simplified by considering subsets of layers, treating their weights of separate layers as multiple independent parameters. Hessian matrix analysis is more intricate, as the loss gradient of different layers is interconnected. Despite this complexity, results in Sec.7.4 show that the Hessian eigenvectors for individual layers closely resemble the corresponding eigenvectors of the entire network of similar indices. This implies that for large eigenvalues, computing Hessian eigenvectors for individual layers suffices, making such computations feasible even for large networks with layers having no more than ≲ 50000 parameters, requiring around ≈ 128 GB of RAM.

## Further Questions Data Augmentation

The importance of data augmentation in modern networks is well-recognized [[8]](#b7). However, incorporating data augmentation into Hessian matrix evaluation poses challenges due to the vast number of possible realizations of training samples. The more training examples a network uses during training, the more costly the computation of the total loss is, since the Hessian must be computed for each sample separately. To approximate the total loss, a sufficiently large number of augmented samples must be used, mimicking the strategy of approximating generalization performance with a smaller test dataset compared to the training dataset.

## Batch Normalization

Batch normalization's influence on training dynamics is substantial. This creates a correlation between the samples in a batch [[11]](#b10). Without batch normalization, samples could be assumed to be independent with respect to their gradient. For our ResNet 20, the influence of batch normalization did not change the qualitative behavior in the weight product, but quantitatively.

## Noise Filtering

Recent work has suggested noise filtering in the context of label noise [[37]](#b36). Removing the small singular values from the network and reshaping the distribution to align with a noise-free scenario has shown to improve test accuracy. One explanation is that these singular values of the RMT bulk are not changed by the network during training, or are changed so little that they do not contain true information, but only noise, such as label noise caused by misclassification. This can result in a shift of the RMT bulk towards larger singular values, which is small enough for a realistic amount of label noise ≤ 40%, such that outlying singular values can still be separated from the ones of the bulk [[37]](#b36). Using our observations and relationships to the Hessian eigenvectors, we can further extend our understanding. Recall that the eigenvectors of the largest Hessian eigenvalues have a large product with the singular matrices of the largest singular values as shown in Sec.7.3. Thus, if we set small singular values to zero, they will mostly affect only the Hessian eigenvectors of the small eigenvalues, which we know by setting them to zero have such flat potentials that shifting them cannot measurably increase the loss seen in Fig. [7](#).12. In this work it was not covered how label noise affects the Hessian eigenvalues and its properties to the weights. This can be done in future work to the extent of our understanding of handling noise practically.

## Information Transition of the Dataset

Sec.7.6 shed light on the remarkable alignment between the pre-training and post-training eigenvalue orders, highlighting the influence of both the dataset's characteristics and the network's architecture on the observed patterns. This insight offers a complement to dataset analyses, as explored in [[24]](#b23).

## Catastrophic Forgetting

The proposed catastrophic forgetting regularization in Sec.7.7 performs well on individual layers but less effectively on entire networks. While the method applied to the gradient seems to fail completely in terms of test accuracy, the loss method performs reasonably well and may be applicable. Our results may be even more useful for fine-tuning a network, i.e., training on the latest layers to further specialize the network, e.g., training the network on a task it has never seen before [[33]](#b32). This can be few-shot learning [[25]](#b24), where the network has to recognize label classes it has never seen before that are very similar to those it has already learned. The CIFAR-10 dataset we used to test catastrophic forgetting does not have enough label classes to efficiently predict how our method will perform on numerous label classes. Considering that as problems get harder, the number of label classes becomes so large that many classes are only seen by the network after many batches, preventing catastrophic forgetting may become more important for larger classification problems.

## Explainability of Deep Neural Networks

This work represents a stride towards unraveling the enigmatic nature of Deep Neural Networks (DNNs), contributing to their explainability. Our understanding provides a lens through which we can scrutinize and interpret the behavior and properties of DNNs. Furthermore, this pursuit holds the potential to assuage growing societal concerns about the evolving capabilities of these networks, as emphasized in recent discussions [[9,](#b8)[29]](#b28).

## Conclusion

In this study, we delved into the intricate dynamics of trained neural networks, unraveling key insights that shed light on their behavior. Trained networks predominantly continue training in a single direction, known as the drift mode. This intriguing drift mode can be elegantly explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. Through our analysis, we unveiled a strong correlation between the Hessian eigenvectors and the network weights. This relationship, hinged on the magnitude of eigenvalues, allowed us to discern the important parameter directions within the network. Notably, the significance of these directions rests on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extended to the decomposition of weight matrices through singular value decomposition. Quite surprisingly the overlap of Hessian eigenvectors of large eigenvalues is larger for singular matrices of larger singular values than for the weight vector. Furthermore, our examination showcased the applicability of principal component analysis in approximating the Hessian, with update parameters emerging as a superior choice over weights for this purpose. Remarkably, our findings unveiled a similarity between the largest Hessian eigenvalues of individual layers and the entire network. Notably, higher eigenvalues were concentrated more in deeper layers. Strikingly, the Hessian eigenspectrum, far from being randomly distributed, exhibited a pronounced pattern predetermined by the dataset and the network structure, even before the onset of training. Leveraging these insights, we ventured into the realm of addressing catastrophic forgetting a challenge that plagues neural networks when learning new tasks while retaining knowledge from previous ones. By applying our discoveries of the overlaps of the Hessian eigenvectors and singular matrices, we formulated an effective strategy to mitigate catastrophic forgetting, offering a pragmatic solution that can be applied to networks of varying scales, including larger architectures. In conclusion, our journey through the intricate landscapes of trained neural networks has revelations that deepen our understanding of their behavior. These insights hold the promise of not only refining network training but also influencing the broader discourse on explainability and reliability in the realm of deep learning.

![Figure 5.1: Graphical representation of the MLP 50 (left) and the MLP 256 (right) network structure. The "None" entry means that we can insert an arbitrary batch size into the network. The input layer equal the pictures of the corresponding datasets. Those have to be turned into a vector by an additional flattening layer.]()

![Figure 5.2: Graphical representation of the LeNet (left) and miniAlexNet (right) structure. The "None" entry means that we can insert an arbitrary batch size into the network.]()

![Figure 6.1: Scalar product of singular matrices and principal components of the (128, 64) layer. Measured weights for 10 epochs post-training. (a) Unregularized for all layers. (b) All layers regularized with weight decay of λ = 0.0005.]()

![Figure 6.3: Accuracy of added principal components of the (128, 64) layer of the MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005. The entry i = 0 corresponds to a weight matrix of zeros.]()

![Figure 6.5: Weight development in direction of the first principal component of the (128, 64) layer of the regularized MLP 256 network. Time s are measured in epochs.]()

![Figure 6.6: The loss landscape in direction of the drift mode of the (128, 64) layer of the regularized MLP 256 network. The network is very close to the minimum in direction of the drift mode. Each update step only moves the network very little further to the minimum.]()

![Figure 6.7: Loss scaling of the (128,64) layer of the MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005.]()

![Figure 7.1: Scalar product of the Hessian eigenvectors with the principal components of (a) weights and (b) velocities of the (128, 64) layer of the unregularized MLP 256 network.]()

![Figure 7.2: Weight product of the (128, 64) layer of the regularized MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005.]()

![Figure 7.3: Weight product for various networks and layers: (a) First convolutional layer of LeNet. (b) Second convolutional layer of ResNet. (c) Last layer of miniAlexNet. (d) First convolutional layer of miniAlexNet.]()

![Figure 7.4: Accuracy of added Hessian eigenvectors of the (128, 64) layer of the MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005.]()

![Figure 7.5: Scalar product of singular matrices and Hessian eigenvectors for the (128, 64) layer of the MLP 256 network. (a) All layers are unregularized. (b) All layers are regularized with weight decay λ = 0.0005.]()

![Figure 7.6: Scalar product of the flattened singular matrices and Hessian eigenvectors for the (16, (5, 5)) convolutional layer of the LeNet.]()

![Figure 7.7: Scalar product of the Hessian eigenvectors of the full MLP 50 network with the Hessian eigenvectors of the (50, 50) layer.]()

![Figure 7.8: Weight product of the entire MLP 50 network trained on the MNIST dataset.The larger product of the largest indices refer to negative eigenvalues that are of similar size as indices in the range of 8000 to 10000.]()

![Figure 7.9: Accuracy of the network with weights of added Hessian eigenvectors of the entire MLP 50 network. The eigenvalues are sorted in decreasing algebraic order. The changes for the largest indices mark negative eigenvalues. (a) Accuracy. (b) Loss.]()

![Figure 7.10: Accuracy of the network with weights of added Hessian eigenvectors of the entire MLP 50 network. The eigenvalues are sorted in decreasing order by magnitude. (a) Accuracy. (b) Loss.]()

![Figure 7.11: Accuracy of added Hessian eigenvectors of the entire MLP 50 network. The eigenvalues are sorted in decreasing order by magnitude. Zoomed in to the 5000 largest eigenvalues. (a) Accuracy. (b) Loss.]()

![Figure 7.12: The loss landscape of Hessian eigenvectors of the entire MLP 50 network. The eigenvector with the largest eigenvalue and the eigenvector with the largest weight product are shown. The dashed lines represent the quadratic potential model expectation. (a) Loss. (b) Test accuracy.]()

![Figure 7.13: The loss landscape of Hessian eigenvectors of the entire MLP 50 network. The four eigenvectors with the largest eigenvalue are shown. The dashed lines represent the quadratic potential model expectation. (a) Loss. (b) Test accuracy.]()

![Figure 7.15: Spectral density of the Hessian eigenvalues of the entire MLP 50 network.The standard deviation of the smoothing kernel is set to one.]()

![Figure 7.16: Level spacings of the unfolded spectrum of Hessian eigenvalues of the entire MLP 50 network. The dashed line represents the expectation for random matrices of the same size. Averaged over the 10 nearest neighbors in both directions. The inset plot shows the cumulative distribution function. Adapted from [38].]()

![Figure 7.18: Concentration of Hessian eigenvectors in specific layers by the square norm of each layer's components. The MLP 50 network is analyzed for this illustration.]()

![Figure 7.19: Weight product of the (128, 64) layer of the unregularized MLP 256 network. (a) At network initialization. (b) After 100 epochs of training. Eigenvectors are computed after 100 epochs of training for both figures.]()

![Figure 7.20: Hessian eigenvalues and eigenvectors of the (128, 64) layer of the unregularized MLP 256 network at initialization. (a) Weight product. (b) Spectral density. The standard deviation of the smoothing kernel is set to one.]()

![Figure 7.21: Inverse participation ratio of the (128, 64) layer of the unregularized MLP 256 network. (a) At initialization. (b) After 100 epochs.]()

