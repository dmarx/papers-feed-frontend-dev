- **Drift Mode**: Trained deep neural networks predominantly continue training in a single direction, known as the drift mode, explained by the quadratic potential model of the loss function.
  
- **Hessian Eigenvectors and Weights**: There is a correlation between Hessian eigenvectors and network weights, with significance relying on curvature (magnitude of Hessian eigenvalues) and alignment with weight vectors.

- **Singular Value Decomposition (SVD)**: Decomposes weight matrices to identify critical directions within the Hessian, considering both magnitude and curvature.

- **Principal Component Analysis (PCA)**: Effective in approximating the Hessian; update parameters are superior to weights for this purpose.

- **Hessian Eigenvalues**: Largest eigenvalues of individual layers correlate with the entire network, with higher eigenvalues concentrated in deeper layers.

- **Catastrophic Forgetting**: The study proposes a strategy to mitigate catastrophic forgetting, applicable to networks of varying scales.

- **Loss Function**: Categorical cross-entropy loss defined as \( L(x, z) = -\sum_i z_i \log(y_i(x)) \), where \( z_i = 1 \) for the correct label.

- **Stochastic Gradient Descent (SGD)**: Weight updates are given by \( w(t + 1) = w(t) + v(t + 1) \) with \( v(t + 1) = -\eta \nabla_w l(k(t))(w, x, z) + \beta v(t) \).

- **Vanishing Gradient Problem**: A challenge in training where gradients diminish in magnitude, particularly affecting early layers due to the chain rule of derivatives.

- **Regularization**: L2 regularization is used to prevent overfitting, adding \( \lambda ||w||_2^2 \) to the loss function.

- **Activation Functions**: 
  - ReLU: \( y_i(x) = \max(x_i, 0) \)
  - Softmax: \( y_i(x) = \frac{e^{x_i}}{\sum_j e^{x_j}} \)

- **Network Structure**: DNNs transform inputs \( x \) into outputs \( y \) via layers, expressed as \( y = f(x, w) \) with layer outputs \( y^{(l)} = f^{(l)}(x^{(l)}, w^{(l)}) \).

- **Training Dynamics**: The trajectory of network updates is analyzed through PCA, revealing directions of maximal change tied to critical information for task-solving.

- **Comparative Analysis**: The study aims to bridge the gap between network parameters and training dynamics, enhancing understanding of DNN behavior.

- **Future Research Directions**: Suggestions for further exploration of DNNs, including implications for noisy data and other challenges.