<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Agency Is Frame-Dependent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-06">6 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Abel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andr√©</forename><surname>Barreto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Will</forename><surname>Dabney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shi</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Hansen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Harutyunyan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clare</forename><surname>Lyle</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Richens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Rowland</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Michael Bowling Amii</orgName>
								<orgName type="department" key="dep2">Doina Precup Google DeepMind</orgName>
								<orgName type="institution" key="instit1">University of Alberta</orgName>
								<orgName type="institution" key="instit2">Khimya Khetarpal Google DeepMind</orgName>
								<orgName type="institution" key="instit3">Razvan Pascanu Google DeepMind</orgName>
								<orgName type="institution" key="instit4">Tom Schaul Google DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Agency Is Frame-Dependent</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-06">6 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">3EF5213BC519FBEFC009DF71F65311DF</idno>
					<idno type="arXiv">arXiv:2502.04403v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Agency, Philosophy of Reinforcement Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by <ref type="bibr" target="#b4">Barandiaran et al. (2009)</ref> and Moreno ( <ref type="formula">2018</ref>) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning (RL) involves learning or decision making over time to achieve a goal. Agents are often taken as the primary vehicles that carry out this learning and decision making, and as such have long been an essential element of RL. Moreover, agency is the lifeblood of an agent-agency is the capacity that endows a given system with the status of agent-hood. Thus, agency also stands as one of the elemental concepts of RL. In The Evolution of Agency, <ref type="bibr" target="#b20">Tomasello (2022)</ref> makes an even stronger case for the role of agency in psychology:</p><p>Every scientific discipline begins with a proper domain, a first principle ... In psychology, depending on one's theoretical predilections, that proper domain or first principle might be either behavior or mentality. But my preferred candidate would be agency, precisely because agency is the organizational framework within which both behavioral and mental processes operate. <ref type="bibr">(p. 134, Tomasello, 2022)</ref>.</p><p>Following similar reasoning to Tomasello, we take it as essential that the science of RL is borne from an understanding not just of intelligence, learning, and decision making, but also of agency. To this end, we here investigate a fundamental question about agency through the lens of RL: is agency an invariant, measurable property of an input-output system, or does it vary depending on other independent commitments? We draw from several distinct results within the RL literature to arrive at the conclusion that agency is fundamentally frame-dependent.</p><p>Agency. Typical views of agency across biology <ref type="bibr" target="#b3">(Ball, 2023)</ref>, complex systems <ref type="bibr" target="#b16">(Moreno, 2018;</ref><ref type="bibr" target="#b17">Moreno and Etxeberria, 2005)</ref>, and philosophy <ref type="bibr" target="#b4">(Barandiaran et al., 2009;</ref><ref type="bibr" target="#b8">Dretske, 1999)</ref> roughly define the concept as an input-output system's capacity to steer outcomes toward a goal. We build around one canonical definition of agency developed by <ref type="bibr" target="#b4">Barandiaran et al. (2009)</ref> that we present in four parts. First, to have agency, the system must be individual; it has a boundary that separates it as an independent entity from its surroundings. Second, once we have chosen a boundary, the system must be the source of its own action. Third, the system has some goals or norms that regulate its interactions with the environment. Fourth, the system steers its experience in light of these goals. We take these four conditions to be a reasonable starting point for any account of agency, summarized in Figure <ref type="figure">1</ref>(a). For the purposes of our main argument, we do not take debates about the precise definition of agency to have a significant impact on our conclusion-we anticipate that regardless of how the semantics of "agent" or "agency" are worked out, the need for frame dependence will remain. For instance, <ref type="bibr">Barandiaran et al. combine</ref> normativity and adaptivity into one property: this definition is perfectly valid, but will still admit frame-dependence. For further debates on this topic, see work by <ref type="bibr" target="#b16">Moreno (2018)</ref>, <ref type="bibr" target="#b3">Ball (2023)</ref>, or <ref type="bibr" target="#b8">Dretske (1999)</ref>.</p><p>The Puzzle of Agency. A fundamental puzzle then arises: which systems can be said to have agency? <ref type="bibr" target="#b7">Dennett (1989)</ref> considers the cases of a rock rolling down a hill and a thermostat modulating the temperature of a room-in what sense do these two systems possess agency, and to what extent? And, the more critical scientific question: what principles can we turn to in order to determine whether each of these systems possess agency? Barandiaran et al. stipulate that a system possesses agency if the four conditions are present within the system. If any one of them is missing, the system lacks agency. In this way, agency is taken to be binary, though naturally there is room for developing a non-binary account.  <ref type="formula">2</ref>) is source of its own (3) a goal, and (4) adaptively selects outputs based on inputs to pursue this goal. (Right) Our main claim: A determination of the agency a system, such as a thermostat, is dependent on a choice of reference frame. The two reference frames depicted make different commitments about how we measure the four conditions of agency. For example, we could draw the boundary around our thermostat in several different ways, or understand the goal of the thermostat in different ways.</p><p>2 Agency is Frame-Dependent Main Claim. We here address this puzzle by arguing that the attribution of agency to a system is fundamentally dependent on a choice of reference frame. That is, the agency of any system is relative in the sense that it depends on arbitrary commitments that we collectively call a reference frame. For example, one such commitment is whether the system is meaningfully pursuing a goal; depending on how we codify what counts as meaningful goal-pursuit, the system will either be understood as having agency or not. We support this main claim by illustrating that each of the four properties of agency are themselves relative to a choice of some reference object or commitment-that is, reaching a conclusion about whether a given system possesses each property requires an independent commitment whose choice is arbitrary. At present, our definitions, claims, and arguments are purely philosophical, though we note that a rigorous presentation of this perspective is a natural and fruitful direction for future work.</p><p>What is a Reference Frame? An agent reference frame is a collection of these four commitments that allow us to determine whether a system has each of the four properties. That is, a frame must include (1) a boundary that decides what is internal to the agent and what is external;</p><p>(2) a reference object such as a set of causal variables that allow for determination of whether the system is the cause of its action;</p><p>(3) a principle for isolating whether the system is meaningfully pursuing a goal; and (4) a choice of what changes in behavior count as meaningful adaptation. There are many valid ways to formalise these components; a boundary could be a cut in a graph <ref type="bibr" target="#b12">(Jiang, 2019)</ref> or a Markov blanket <ref type="bibr" target="#b10">(Friston et al., 2009)</ref>. A precise mathematical construction of reference frames is a natural next step for further research.</p><p>(1) Individuality. In order to attribute agency to an entity, an observer first must determine which entity they are referring to. Establishing a boundary that separates this entity from its surroundings is a critical step in determining whether a system has agency. However, clearly identifying such a boundary is non-trivial. Neils Bohr gives the example of a person wielding a stick-depending on the activity, the stick could be taken as a part of the person's propensity for both action and observation (p. 93, <ref type="bibr" target="#b14">Klein, 1967)</ref>. As Harutyunyan (2020) notes, we might plausibly draw the boundary around the person and exclude or include the stick. In fact, as argued by <ref type="bibr" target="#b6">Clark and Chalmers (1998)</ref>, <ref type="bibr" target="#b12">Jiang (2019)</ref>, and Harutyunyan (2020), there are often many ways we can establish a boundary that separates an environment and an agent in a meaningful way. Jiang considers the example of a model-free learning algorithm that is implemented using a neural network to process its observations. As Jiang points out, the boundary we choose to draw could include the pseudo-random number generator and all layers of this network, or only include the last few layers of the network. Proposition 10 by Jiang (2019) (further discussed in Section 6.1), illustrates that many important quantities of an RL agent, such as the optimal policy or Bellman error, are boundary-dependent. We summarize these points in the following claim.</p><p>Claim 1 (Adapted from <ref type="bibr" target="#b12">Jiang, 2019 and</ref><ref type="bibr" target="#b11">Harutyunyan, 2020)</ref>. Individuality is frame-dependent: Nearly all agents admit many plausible boundaries one could draw that separates them from their environment. Moreover, key quantities of an agent can change depending on which boundary is chosen.</p><p>Individuality might be seen as qualitatively different from the other properties, since it involves selecting which system we would like to attribute agency to and what its inputs and outputs are. As such, it is natural to conclude that we recover a different agent depending on how draw the boundary. However, our argument does not stand only on the frame-dependency of individuality-since we take all four properties of agency to be frame-dependent, our argument remains agnostic to any perceived qualitative difference them.</p><p>(2) Source of Action. Second, a must be the source its own action. In the terms of <ref type="bibr" target="#b3">Ball (2023)</ref>, this property reflects whether the system is "pushed around by its or does the pushing itself. For instance, a wall being knocked over by a wrecking ball could be understood as taking the action of being knocked over. However, the source of this action (and corresponding potential energy) did not ultimately originate in the wall, but rather in the wrecking ball and its operator. It might therefore be better to view the wall as lacking in the source of action. In contrast, a bird flapping its wings intuitively satisfies the condition since this action is best thought of as originating from within the bird. <ref type="bibr" target="#b13">Kenton et al. (2023)</ref> recently develop a causal account that determines which entities in a causal model might be said to satisfy roughly this property. The difficulty, as Kenton et al. note, is that reaching a conclusion about the source of action in a causal model rests entirely on the choice of causal variables. In this way, identifying whether a given subsystem originates its own action depends on an independent, unrelated choice: the causal variables. Kenton et al. state directly: "Note [discovering an agent in a causal model] is relative to a frame -a choice of variables that appear in our causal model" (p. 2, <ref type="bibr" target="#b13">Kenton et al., 2023)</ref>. Following this reasoning, we claim that the source of action is also frame-dependent in that it depends on an arbitrary upstream commitment.</p><p>Claim 2 (Adapted from <ref type="bibr" target="#b13">Kenton et al., 2023)</ref>. Source of action is frame-dependent: There exist cases with at least two plausible choices of causal variables where the former choice identifies an agent in the causal model, and the latter choice refutes the presence of an agent in the causal model.</p><p>(3) Normativity. Third, and perhaps most crucially, agency is about goal-directedness. The trouble is that every system with outputs can be understood as if <ref type="bibr" target="#b9">(Friedman, 1953)</ref> it is goal-directed.</p><p>More concretely, every input-output system can be well-explained in terms of goal-directedness. For example, in the case of our thermostat, even a broken thermostat whose output always sets the temperature of a room to 20 ‚Ä¢ can be understood as having the goal "set the temperature to 20 ‚Ä¢ ". More trivially, a rock can be viewed as having the goal of rolling down a hill, or as having the goal of convincing all observers that it is a rock. What makes these meaningless, as opposed to meaningful goals? This challenge is reflected in one of the classical results of inverse RL first discussed by <ref type="bibr" target="#b19">Russell (1998)</ref> and <ref type="bibr" target="#b18">Ng and Russell (2000)</ref>: the zero reward function is always consistent with every system that outputs signals that are understood as decisions. In other words, reward is under-determined by behavior <ref type="bibr" target="#b5">(Cao et al., 2021)</ref> without <ref type="bibr" target="#b1">(Amin et al., 2017)</ref>. To overcome this challenge, a variety of approaches have explored the use of biases or other principles that constrain the space of viable reward functions, such as Occam's razor <ref type="bibr" target="#b2">(Armstrong and Mindermann, 2018)</ref>, or the now standard approach invoking maximum entropy proposed by <ref type="bibr" target="#b21">Ziebart et al. (2008)</ref>. These upstream principles can in some cases rule out certain kinds of goal-directedness as uninteresting, or elevate others as meaningful. Hence, we must again invoke an additional principle to determine whether a given system is meaningfully goal-directed. One common approach is to ask how useful it is to explain or understand the system in terms of goal-directedness, as first argued by <ref type="bibr" target="#b7">Dennett (1989)</ref>. In other words, to determine if a system meaningfully has a goal, we require some extraneous commitments that again amount to a reference frame.</p><p>Claim 3. Normativity is frame-dependent: For nearly all cases of input-output systems, whether that system is meaningfully goal-directed depends on a reference point whose choice is arbitrary.</p><p>(4) Adaptivity. Fourth, agency is about adaptivity, which captures whether a system's outputs are influenced by its inputs, and to what extent. In "On the Definition of Adaptivity", Zadeh (1963) suggests a form of frame-dependence: "...every system is adaptive with respect to [something] ... what matters is not whether [the system] is adaptive or not, but what ... it is adaptive [to]" (p. 470). Following this reasoning, adaptivity can be understood as frame-dependent in the sense that it depends on what is chosen as the relevant reference class used to reach determinations about a system's adaptivity. For example, in RL, we might ask whether a policy that maps each input state to an action is adaptive. On one reference frame, we might treat any change in the output as adaptivity, while on another reference frame, we could view this policy as a fixed and non-adaptive function since it always chooses the same action every time it receives the same input. <ref type="bibr" target="#b0">Abel et al. (2023)</ref> make this argument in a more general case and show that all policies can either be understood as adaptive, or not, depending on a reference class of meaningful changes in behavior. In other words, if we want to determine whether a system is adaptive, we need to first agree on the class of experience-influenced changes of behavior that count as adaptivity. Depending on this choice, a system will either be adaptive, or not. This choice then acts as a reference frame.</p><p>Claim 4 (Adapted from Zadeh, 1963 and Theorem 3.1 of <ref type="bibr" target="#b0">Abel et al., 2023)</ref>. Adaptivity is frame-dependent: For many inputoutput systems, there will exist at least two reference frames (and in most cases, many more) such that according to the first reference frame the system is adaptive, while according to the second, the system is not.</p><p>In summary, reaching a determination about each of the four properties of agency requires reference to other fixed commitments that collectively comprise a reference frame. Since agency is simply the logical conjunction of these latter three properties conditioned on the choice of a boundary, then reaching a conclusion about agency itself must be made in reference to these commitments. In other words: agency is frame-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>We have here argued that agency is frame-dependent by illustrating the sense in which each of the four essential conditions of agency are themselves frame-dependent. We take this to have far reaching implications for disciplines that study agents and agency.</p><p>Intelligence and Agency. The relationship between intelligence and agency is not yet well understood. For instance, does intelligence require agency, and vice versa? Depending on how this question is addressed, frame-dependence may have significant implications for our understanding of intelligence, in addition to other emergent properties of information processing systems. We believe that exploring this relationship through the lens of frame-dependence offers a new frontier for understanding central concepts of RL.</p><p>Reference Frames. We stop short of presenting a rigorous mathematical definition of reference frames, as well as a formal proof of the frame-dependence of agency. A natural next step to further this line of work is to develop a precise definition of agent reference frames along with a formal proof supporting our main claim. We speculate that the building blocks to do so are already in place in the field, but have not gone through the careful work of developing the definitions and arguments more formally.</p><p>Choosing a Reference Frame. How do we choose an appropriate reference frame? It is unclear which frame-selection principles are defensible, and what implications these principles carry for our study of agents. The Intentional Stance <ref type="bibr" target="#b7">(Dennett, 1989)</ref> asserts roughly that it is most useful to understand certain systems as agents; taken to its natural conclusion, this could be formalised as a principle for selecting frames in terms of predictive or explanatory power. An important direction for further research will investigate, formalise, and compare different principles for selecting reference frames.</p><p>Levels, Dennett's Stances. The proposal to adopt reference frames coheres with the perspectives of <ref type="bibr" target="#b7">Dennett (1989)</ref> as well as <ref type="bibr" target="#b15">Marr (2010)</ref>, who each argue for understanding certain phenomena at different levels of abstraction. Marr argues that our attempts to understand a cognitive process such as vision can be cast through three distinct levels: the hardware, the algorithmic, and the computational. Dennett argues that there are distinct levels of abstraction we should adopt depending on the content of our study. Like Marr, the lowest-level of abstraction is the physical, according to which we examine physical properties of a system. The second level is the design stance, according to which we examine how the system is designed to make predictions about its operation or purpose. The third is the intentional stance, according to which we study the content of minds themselves: of beliefs and desires. Both Dennett's intentional stance and Marr's levels suggest that our study of concepts like agency needs to carefully calibrate in order to examine the right kinds of content, and reach the right kinds of conclusions. We suggest that reference frames may offer a path to connect physical substrates with more abstract propositions such as those related to agency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure 1: (Left) A four-part account of agency due largely to Barandiaran al. (2009): a system such as a thermostat has agency if it has (1) a (2) is source of its own (3) a goal, and (4) adaptively selects outputs based on inputs to pursue this goal. (Right) Our main claim: A determination of the agency a system, such as a thermostat, is dependent on a choice of reference frame. The two reference frames depicted make different commitments about how we measure the four conditions of agency. For example, we could draw the boundary around our thermostat in several different ways, or understand the goal of the thermostat in different ways.</figDesc><graphic coords="2,-77.75,464.77,230.90,231.33" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Kim Stachenfeld</rs> and <rs type="person">Vlad Mnih</rs> for their thoughtful comments on a draft of the paper.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A definition of continual reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Repeated inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Occam&apos;s razor is insufficient to infer the preferences of irrational agents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mindermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Organisms as agents of evolution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">John Templeton Foundation: West Conshohocken</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<pubPlace>PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Defining agency: Individuality, normativity, asymmetry, and spatiotemporality in action</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Barandiaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Di Paolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="367" to="386" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Identifiability in inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Szpruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The extended mind</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Dennett</surname></persName>
		</author>
		<title level="m">The intentional stance</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machines, plants and animals: the origins of agency</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Dretske</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Erkenntnis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="1975">1975. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Essays in positive economics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953">1953</date>
			<publisher>University of Chicago press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement learning or active inference?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6421</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">What is an agent?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<ptr target="http://anna.harutyunyan.net/wp-content/uploads/2020/09/What_is_an_agent.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13341</idno>
		<title level="m">On value functions and the agent-environment boundary</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discovering agents</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Macdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Everitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">103963</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Glimpses of Niels Bohr as scientist and thinker. Niels Bohr. His life and work as seen by his friends and colleagues</title>
		<author>
			<persName><forename type="first">O</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Interscience Publishers</publisher>
			<biblScope unit="page" from="74" to="93" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vision: A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On minimal autonomous agency: natural and artificial</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Agency in natural and artificial systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Etxeberria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning agents for uncertain environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Learning Theory</title>
		<meeting>the Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The evolution of agency: Behavioral organization from lizards to humans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="470" />
			<date type="published" when="1963">2022. 1963</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>On the definition of adaptivity</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificiall Intelligence</title>
		<meeting>the AAAI Conference on Artificiall Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
