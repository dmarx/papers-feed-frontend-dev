{
  "arxivId": "2308.10718",
  "title": "Backdooring Textual Inversion for Concept Censorship",
  "authors": "Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang",
  "abstract": "Recent years have witnessed success in AIGC (AI Generated Content). People\ncan make use of a pre-trained diffusion model to generate images of high\nquality or freely modify existing pictures with only prompts in nature\nlanguage. More excitingly, the emerging personalization techniques make it\nfeasible to create specific-desired images with only a few images as\nreferences. However, this induces severe threats if such advanced techniques\nare misused by malicious users, such as spreading fake news or defaming\nindividual reputations. Thus, it is necessary to regulate personalization\nmodels (i.e., concept censorship) for their development and advancement.\n  In this paper, we focus on the personalization technique dubbed Textual\nInversion (TI), which is becoming prevailing for its lightweight nature and\nexcellent performance. TI crafts the word embedding that contains detailed\ninformation about a specific object. Users can easily download the word\nembedding from public websites like Civitai and add it to their own stable\ndiffusion model without fine-tuning for personalization. To achieve the concept\ncensorship of a TI model, we propose leveraging the backdoor technique for good\nby injecting backdoors into the Textual Inversion embeddings. Briefly, we\nselect some sensitive words as triggers during the training of TI, which will\nbe censored for normal use. In the subsequent generation stage, if the triggers\nare combined with personalized embeddings as final prompts, the model will\noutput a pre-defined target image rather than images including the desired\nmalicious concept.\n  To demonstrate the effectiveness of our approach, we conduct extensive\nexperiments on Stable Diffusion, a prevailing open-sourced text-to-image model.\nOur code, data, and results are available at\nhttps://concept-censorship.github.io.",
  "url": "https://arxiv.org/abs/2308.10718",
  "issue_number": 160,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/160",
  "created_at": "2025-01-05T08:24:02.651619",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-22T16:47:15.076Z",
  "main_tex_file": null,
  "published_date": "2023-08-21T13:39:04Z",
  "arxiv_tags": [
    "cs.CR",
    "cs.CV"
  ]
}