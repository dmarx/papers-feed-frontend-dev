{
  "arxivId": "2402.10193",
  "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
  "authors": "James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle Cai",
  "abstract": "Large Language Models (LLMs) are typically trained in two phases:\npre-training on large internet-scale datasets, and fine-tuning for downstream\ntasks. Given the higher computational demand of pre-training, it's intuitive to\nassume that fine-tuning adds less new information to the model, and is thus\nmore compressible. We explore this assumption by decomposing the weights of\nfine-tuned models into their pre-trained components and an additional delta. We\nintroduce a simple method, BitDelta, which successfully quantizes this delta\ndown to 1 bit without compromising performance. This interesting finding not\nonly highlights the potential redundancy of information added during\nfine-tuning, but also has significant implications for the multi-tenant serving\nand multi-tenant storage of fine-tuned models. By enabling the use of a single\nhigh-precision base model accompanied by multiple 1-bit deltas, BitDelta\ndramatically reduces GPU memory requirements by more than 10x, which can also\nbe translated to enhanced generation latency in multi-tenant settings. We\nvalidate BitDelta through experiments across Llama-2 and Mistral model\nfamilies, and on models up to 70B parameters, showcasing minimal performance\ndegradation over all tested settings.",
  "url": "https://arxiv.org/abs/2402.10193",
  "issue_number": 350,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/350",
  "created_at": "2025-01-04T15:02:36.858483",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-28T06:12:55.272Z",
  "main_tex_file": null,
  "published_date": "2024-02-15T18:50:06Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CL"
  ]
}