- Decision to decompose fine-tuned model weights into pre-trained components and a delta.
- Choice of 1-bit quantization for the delta representation.
- Implementation of a scaling factor for the quantized delta.
- Use of model distillation to optimize scaling factors.
- Selection of linear layers in Transformer blocks for quantization.
- Decision to validate BitDelta across multiple model families (Llama-2, Mistral, MPT).
- Choice of C4 dataset for distillation and calibration.
- Decision to focus on reducing GPU memory requirements for multi-tenant serving.
- Implementation of a CUDA kernel for efficient memory management.
- Decision to compare BitDelta with existing parameter-efficient fine-tuning methods.
- Choice to limit training steps to 200 for BitDelta compared to traditional fine-tuning.
- Decision to explore implications of delta compression for model storage and hot-swapping.
- Choice to enable dynamic loading of compressed deltas in a multi-tenant serving system.
- Decision to analyze performance degradation across various model sizes and configurations.
- Choice to document the methodology cost in comparison to full parameter fine-tuning.