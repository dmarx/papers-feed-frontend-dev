# ÉCLAIR -Extracting Content and Layout with Integrated Reading Order for Documents

## Abstract

## 

Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce ÉCLAIR, a generalpurpose text-extraction tool specifically designed to process a wide range of document types. Given an image, ÉCLAIR is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark DROBS for document-level OCR and semantic classification. ÉCLAIR achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate ÉCLAIR on established benchmarks, demonstrating its versatility and strength across several evaluation standards.

## Introduction

Optical Character Recognition (OCR) has allowed machines to extract text from images and transformed the way we interact with textual information. The recent success of Large Language Models (LLMs) is partly attributed to the availability of extremely large text datasets, placing an increasing demand for high-quality tokens extracted from text-dense documents such as scientific textbooks and journals. This is a challenging task since it necessitates an understanding of reading order across complex layouts, which in turn requires identifying different semantic elements and their relationships on the page. Maintaining a seamless flow requires separating relevant elements (e.g., paragraphs or tables) from irrelevant ones (e.g., page headers, page footers and other floating text).

Traditional OCR systems operate on a word or line level and are unable to properly understand the spatial and semantic relationships and hierarchies present in textdense documents. More complex systems that possess such capabilities are generally not end-to-end and combine several models into a brittle pipeline. This shortcoming has spawned an interest in developing end-to-end models [[6,](#b5)[31,](#b30)[52]](#b51) that can extract formatted and structured text from complex documents such as those shown in Figure [1](#fig_0). Such capabilities provide downstream benefits for a multitude of tasks, including retrieval, document question answering, and increasing the availability of text tokens for LLM training. However, recent models proposed in this area still have crucial drawbacks: Kosmos-2.5 [[31]](#b30) lacks the ability to extract formatted text that is at the same time spatially aware (since it has two mutually-exclusive prompts), while GOT [[52]](#b51) and Nougat [[6]](#b5) do not predict any spatial information at all. In addition, none of these models predict semantic classes of bounding boxes, which can be used as a conditioning element for retrieval, help filter out irrelevant information for LLM training, and assist when combining multiple pages within a document (e.g., placing footnotes only after a text section has ended).

To address these concerns, we present ÉCLAIR: a multimodal LLM (MLLM) comprised of a ViT-like encoder and an auto-regressive decoder, architecturally similar to Donut [[23]](#b22). ÉCLAIR is able to extract text (formatted as markdown/L A T E X), bounding boxes of text blocks with their semantic classes, and any combination of these simultaneously, while preserving the reading order.

Training such a versatile model necessitates a dataset that encompasses all these annotation types. To address this problem, we generate arXiv-5M, a large-scale dataset that is sampled from arXiv papers, covers all desired annotation capabilities and serves as a link between existing datasets of varying annotation coverage.

Our proposed novel data generation pipeline includes a modified L A T E X compiler which generates ground truth labels directly from the L A T E X sources.

Furthermore, existing document-level OCR benchmarks are limited by partial annotations: GOT [[52]](#b51) allows for document level reading order but lacks block-level spatial and semantic labels, while DocLayNet [[40]](#b39) lacks reading order information. To address these shortcomings, we release a new benchmark DROBS consisting of 789 visually-diverse pages sampled from different sources, see Figure [3](#fig_3) for examples. The annotations come from human labeling and contain text in reading-order along with bounding boxes and semantic classes. ÉCLAIR achieves state-of-the-art (SOTA) accuracy on DROBS when compared to other recent models, as well as competitive metrics on several existing benchmarks spanning different tasks, including general OCR, document layout understanding, and extraction of high-quality data for LLM training.

To summarize, our contributions are as follows:

• We create an end-to-end document-level OCR model which is the first to extract formatted text with their respective bounding boxes and semantic classes. • We develop a novel data generation pipeline where we can control the rendering of L A T E X sources ourselves. This is a pre-requisite for bridging the gap between ex-isting datasets with fewer label types. • We release a new benchmark DROBS, with highquality human-labeled data and show SOTA accuracy as well as competitive metrics on existing benchmarks.

## ÉCLAIR

## Architecture

ÉCLAIR uses a transformer encoder-decoder architecture.

The vision encoder, denoted as E, is initialized from RADIO [[42]](#b41) which follows a ViT-H /16 [[10]](#b9) architecture, and maps an image I ∈ R 3×H×W to a latent representation Z ∈ R N ×d , where d is the hidden dimension and N is the sequence length. The neck N then reduces the dimensionality of the latent space as well as the sequence length. The decoder, denoted as D, uses mBART [[28]](#b27) and predicts text tokens T = {t P +1 , t P +2 , . . . , t L } by conditioning on the latent encoder representation, N (Z), and the context t <i , P (t i |N (Z), t <i ), where Z = E(I) and {t 1 , t 2 , . . . , t P } are the prompt tokens and where L is the prompt-augmented sequence length.

Since autoregressive models scale poorly with the decoder size and sequence length at inference time, we adopt a larger vision encoder (657M parameters) and combine it with a lightweight decoder (279M parameters). This follows from the observation that OCR is not fundamentally a generative task but rather depends on the content in the input image. We describe further modifications to improve inference time in Section 3.5.

## Prompt

We use the input prompt to specify the desired format of the model outputs. Each prompt is a tuple of three options, For each of the three groups, the first option specifies the most information, while the last option suppresses this output type. With the <structured_text> prompt, the text is predicted in markdown format and inline formulae are formatted as L A T E X, whereas with the <plain_text> prompt both are formatted as plain text. Tables and block formulae are formatted as L A T E X for both modes. We define the maximal-information prompt (MIP) as: <structured_text><bbox><classes>

The novelty of ÉCLAIR compared to existing methods lies in its ability to handle any of the 8 valid prompt combinations. This is achieved by pre-training on a custom dataset that has labels for the maximal-information setting and then decreasing the information density for each group with some dataset-dependent probability during the fine-tuning stage. This allows the model to leverage visually diverse datasets with partial annotations for training. A schematic structure of ÉCLAIR along with possible prompts and corresponding output is presented in Figure [2](#fig_1).

## Output Format and Tokenization

ÉCLAIR predicts bounding boxes of the semantic blocks in the form of discrete coordinates, similar to Kosmos [[31]](#b30). These bounding boxes are predicted in a canonical reading order, which is described further in the supplementary material. The following regular expression shows the output format for each box in the maximal-information setting: where the the first group denotes the coordinates of the top-left corner, the second group denotes the text contained within the bounding box, the third group denotes the coordinates of the bottom-right corner, and the final group represents the semantic class. Note that each of these groups is optional and their presence in the model's output for a given image would depend on the prompt combination specified for that sample.

We adopt the tokenizer used by Taylor et al. [[47]](#b46), as their model is also specialized for the scientific text domain. The coordinates of the bounding boxes, the semantic classes and the seven prompt components are all added as dedicated special tokens. This adds H + W + C + 7 tokens in total to the tokenizer vocabulary, where C is the number of semantic classes.

## Datasets

Compared to existing methods, such as Kosmos-2.5 [[31]](#b30) and Nougat [[6]](#b5), ÉCLAIR is trained on a relatively smaller dataset as summarized in Table [1](#tab_0). The arXiv-5M dataset makes up a large portion of our training data and it supports the maximum-information prompt (MIP) described in Section 2.2. The generation pipeline used to create this dataset is discussed further in Section 2.5. We pre-train ÉCLAIR on this dataset.

We find that recent models such as Nougat [[6]](#b5), that are only trained on academic documents, do not handle visually-diverse documents very well, often either degenerating into hallucinations or repetition loops or simply terminating early by predicting the end-of-sequence token. We hypothesize that this is because the training data lacks the heterogeneity needed to handle more complex layouts such as magazines, leaflets, and picture-books. To address this, we fine-tune ÉCLAIR further on the arXiv-5M along with several publicly available datasets with diverse layouts and domains, such as DocLayNet [[40]](#b39), SynthTabNet [[33]](#b32) and G1000 [[49]](#b48). We also create a high-quality humanannotated dataset consisting of documents sampled from the Common Crawl corpus [[12]](#b11). Additionally, we create a README dataset by sampling README documents from the Stack [[24]](#b23) and rendering them using Pandoc [[37]](#b36). Most of these datasets contain only partial annotations and the maximum information available in each is summarized in Table [1](#tab_0). The pre-processing steps for these datasets are described in more detail in the supplementary material.

## The arXiv-5M Dataset

In the introduction, we briefly discussed the need for a dataset that provides labels for our maximum information setting, i.e. bounding boxes, semantic classes and formatted text with formulas and tables, all in reading order. Since no such dataset exists, we created a new one.

Our approach is inspired by Nougat [[6]](#b5), where the authors create ground truth image/markdown pairs from arXiv papers. Their pipeline relies on LatexML, a tool to convert L A T E X source code to HTML, which they convert to markdown subsequently. We follow a different approach, which handles both the L A T E X compilation and the conversion to structured output at the same time (instead of using separate processing pipelines for each) and hence retains the re-lationship between text and image down to character-level bounding boxes and allows us to extract semantic classes for each box. Our representation for the structured text output • consists of rectangular boxes • has a semantic class assigned to each box • represents normal text and formatted text as markdown • represents tables and formulas as L A T E X

The box and class information can be used to re-arrange the order of content (e.g. footnotes at the end) and to filter unwanted content, for example page headers and footers.

We modify the open-source T E X Live distribution by adding hooks inside the T E X compiler itself and embedding a Python interpreter for further processing on-the-fly. We hook the internal T E X methods for node, character and hbox/vbox allocations, token reading and output generation and forward these to a custom Python class that keeps track of the elements from allocation to output on the PDF page. Multiple stacks are used to keep track of how the elements are nested in the input and output, and a rule-based system generates a nested hierarchy with the elements of interest.

With this method we generated a high-quality groundtruth dataset consisting of around roughly 5 million pages which we call arXiv-5M.

## Results

The details about our experimental setup and training strategy can be found in the supplementary material. :

3.1. Reading Order benchmark DROBS Evaluation. We evaluate the reading order accuracy of ÉCLAIR against known SOTA methods like Kosmos-2.5 [[31]](#b30) and GOT [[52]](#b51). Both of these methods have two output modalities -a plain OCR mode and a markdown mode, and we compare ÉCLAIR with both modes. For this evaluation, we utilize an internally curated and human-labeled diverse set of PDFs, comprising a total of 789 pages sampled from various sources such as magazines, books, and the Common Crawl corpus [[45]](#b44) which we call DROBS (Document Reading Order, Bounding boxes, and Semantic classes), see Figure [3](#fig_3). This approach aims to cover a diversity of layouts similar to those found in Do-cLayNet [[40]](#b39). We instructed human annotators to provide annotations on this dataset following the same labeling system as DocLayNet due to its comprehensive human annotation guidelines. However, we added additional requirements, the most significant being the inclusion of reading order. We will make the selected pages and associated annotations available to the research community to serve as an additional and complementary benchmark for document understanding and OCR. *MIP-maximal-information prompt **Counting F1 score is computed over the set { he1, said1, that1, she1, said2, that2, they 1 , said3, that3, he2, said4, something 1 }. This allows to track and penalize words that missed but has more than one occurrence in the document. Prior to evaluation, we perform three preprocessing steps on the predictions and corresponding ground truth labels. First, we apply string normalization [[35]](#b34) to remove all nonalphanumeric characters, convert sequences of whitespaces to a single space, and strip any leading or trailing whitespaces to ensure fair comparison. Second, to address variations in the output formats for tables and equations among different methods, and given that our current evaluation benchmark does not include labels for equations and tables, we mask out these elements in the images used for infer-ence across all considered methods. Additionally, for GOT (md), we also mask-out headers and footers from the images, as the model seems to ignore these elements. Lastly, we also filter out T E X commands present in GOT prediction e.g. for title, section and sub-section headers, since those would otherwise penalize the model in text-only metrics.

The results in Table [2](#tab_1) show how ÉCLAIR outperforms both Kosmos and GOT on most metrics. Since these models are trained with multiple prompt modes similar to ÉCLAIR, we compare the output of ÉCLAIR in MIP mode against Kosmos-2.5 and GOT in both OCR and MD modes. We observe that Kosmos-2.5 performs better in OCR mode, where it produces bounding boxes; however, GOT exhibits the opposite behavior, performing better in MD mode as opposed to OCR mode. We believe this discrepancy is due to differences in data blending during training. ÉCLAIR in MIP mode produces both MD for text inside the bounding boxes and shows superior accuracy compared to both other methods.

Since there is no common validation set available for comparing equation and table extraction across methods, we evaluate our formula and table extraction accuracy on a validation set derived from arXiv (See Section 3.

## 2).

Training & Inference Ablation. All the results for ÉCLAIR presented in Tables [2](#tab_1) and [3](#tab_2) were obtained with a repetition penalty [[22]](#b21) of 1.1 applied during inference. Table 5 demonstrates the value of this inference-time hyperparameter and of the additional datasets added in the finetuning stage.

GOT Benchmark. Along with DROBS, we evaluate ÉCLAIR on the GOT benchmark proposed in Fox [[27]](#b26), with the results shown in Table 5. Comparison of ÉCLAIR on DROBS before and after the fine-tuning stage, and also with and without a repetition-penalty (after the fine-tuning stage).

excelling in Edit Distance and Recall, where it achieves the best scores among the models compared. Notably, ÉCLAIR outperforms several larger models, such as Qwen-VL-Max (72B) [[3]](#b2), Fox (1.8B) [[27]](#b26), despite having a significantly smaller parameter size. While GOT (580M) [[52]](#b51) and ÉCLAIR exhibit similar accuracy, ÉCLAIR employs a decoder that is approximately half the size of GOT's decoder.

## Extraction of Formulas and Tables

In this section, we evaluate the extraction quality of ÉCLAIR on some important semantic-classes: formulae, tables and text. The latter consists of all semantic classes excluding formulae and tables. We report our findings on the validation set (10,000 samples) associated with the arXiv-5M dataset in Table [4](#tab_4). Our results demonstrate good extraction quality overall, with table and math elements being harder for ÉCLAIR to transcribe compared to text elements. Note that these metrics are reported for ÉCLAIR pre-trained on arXiv-5M (i.e., prior to fine-tuning).

Since other methods such as Nougat [[6]](#b5) cannot be directly compared to ÉCLAIR on our validation set owing to non-trivial differences in their output formatting styles, we cannot provide a direct comparison here. However, since Nougat and the pre-trained ÉCLAIR model are both trained on academic documents and evaluated on data sampled from arXiv, we find it useful to present the extraction quality of Nougat on these categories on their own validation set as a point of reference. These results are also summarized in Table [4](#tab_4). We observe similar trends in Nougat's extraction quality for math and table elements, as discussed above.

## Document Object Detection

We evaluate the accuracy of detection of semantic text blocks of ÉCLAIR on the DocLayNet benchmark. Following [[4]](#b3), we fine-tune ÉCLAIR solely on DocLayNet for 50k steps to ensure that the bounding box class labels are not biased by labeling styles of other datasets (such as merging of several header and footer boxes). In order to compare to SOTA methods that report coco-mAP, we report the same metric using class token logits for ranking the predicted bounding boxes. We note, however, that being an autoregressive generator, ÉCLAIR remains in inherent disadvantage on coco-mAP metric compared to standard detectors due to it predicting bounding boxes and classes inline with the text in reading order, leading to inability of overprediction to 100 bounding boxes assumed by coco-mAP. On the other hand, this results in ÉCLAIR not requiring non-maximum-suppression or threshold selection. For this reason, previous autoregressive object detectors adopt various tricks to improve the recall at low precision [[8]](#b7). We also follow this approach and adopt sequence augmentation [[8]](#b7) with noisy and duplicate bounding boxes as well as sampling of top-k class labels from each predicted bounding box during inference for reporting coco-mAP. The comparison with SOTA methods is presented in Table [6](#), where we compare to reported Mask R-CNN [[14]](#b13) metrics and reproduced SwinDocSegmenter [[4]](#b3). As can be seen, ÉCLAIR is competitive even compared to specialized object detectors.

Nevertheless, in agreement with previous works on autoregressive detection [[2]](#b1), we find mAP to be a suboptimal metric for such scenario. We provide further discussion on the evaluation metrics in the supplementary material, with more detailed evaluation of ÉCLAIR and competing methods.

## Classes

Mask-RCNN [[14]](#b13) SwinDoc Segmenter [[4]](#b3) ÉCLAIR Caption 71.5 83.5 83.5 Footnote 71.8 67.8 66.9 Formula 63.4 64.2 65.7 List-item 80.8 84.1 79.0 Page-footer 59.3 65.1 62.0 Page-header 70.0 71.3 70.7 Picture 72.7 85.6 76.9 Sec-header 69.3 68.0 67.0 Table 82.9 86.0 77.6 Text 85.8 84.5 82.0 Title 80.4 66.8 82.0 All 73.5 75.2 73.9 Table 6. COCO-mAP (with defaults IoU=0.5:0.95, area=all, maxDets=100) on DocLayNet Benchmark.

## LLM Benchmark

ÉCLAIR enables content extraction from PDFs, PPTs, and other scanned documents to meet the growing demands for high-quality data to train large language models (LLMs) [[11,](#b10)[34,](#b33)[38,](#b37)[48]](#b47). Unlike conventional extraction tools, e.g., PyMuPDF4LLM [[19]](#b18), ÉCLAIR is engineered to preserve semantic integrity and textual coherence. In this section, we compare the effectiveness of ÉCLAIR and PyMuPDF4LLM [[19]](#b18) for this task. We do this by training the Nemotron-8B LLM model from scratch on the text extracted by both of these methods from a common set of PDF documents, and compare the trained models on the Massive Multitask Language Understanding (MMLU) [[15]](#b14) benchmark, an average of multiple other benchmark scores including: ARC-Easy and ARC-Challenge [[9]](#b8), HellaSwag [[53]](#b52), OpenBooxQA [[32]](#b31), PIQA [[5]](#b4), RACE [[25]](#b24), WinoGrande [[44]](#b43), TriviaQA [[21]](#b20).

The results of this experiment, summarized in Table 7. Comparison of the Nemotron-8B accuracy when trained on data extracted with ÉCLAIR or PyMuPDF4LLM [19].

## Multi-token Inference

An important shortcoming of autoregressive models, including those targeted at OCR applications, is the large number of decoding steps necessary for text extraction, resulting in slow inference speed. In a standard autoregressive decoding formulation, each subsequent l th token in the sequence is decoded incrementally, based on the context of t 0 : t l-1 tokens. For text-dense images, such as documents, this results in a large amount of decoding steps, at least equal to the number of tokens in the sequence.

To mitigate this, we investigate multi-token generation as an alternative inference method. Instead of next-token prediction, we train ÉCLAIR to predict n subsequent tokens at a single step, and therefore reduce the number of necessary decoding steps by a factor of n.

Specifically, for predicting n tokens simultaneously, during training we introduce n -1 new linear layers on top of the final hidden state of the decoder. The output of each of these linear layers is subsequently input into the shared decoder head. During training, standard teacher forcing is applied, with next n tokens representing the groundtruth for each corresponding context. During inference, we greedily decode the sequence n tokens at a time. We do not perform token verification [[7]](#b6) but rely on purely greedy decoding in the interest of maximal throughput at batch-inference.

Following prior work [[13]](#b12), we additionally perform experiments where we only consider the first predicted token during inference while the rest are discarded, while during training next-n tokens are predicted. In other words, we evaluate n-token trained models at next-token prediction.

We experimented with 2-, 3-, or 4-token prediction and the results are reported in Table [8](#), where tkn step corresponds to the number of tokens kept from multi-token prediction at a single decoding step during inference. As can be seen, multi-token ÉCLAIR is matching or outperforming the baseline at 2 or 3 tokens, which is equivalent to around 2x inference speed increase. At 4 tokens, the accuracy degrades. We nevertheless find that keeping only the first token during inference is a valid strategy for improving OCR metrics for any of the variants. We additionally report the inference speed of multitoken ÉCLAIR as well as competing methods, with average time per image on DROBS test set. As per-image speed of each method is related to its text extraction capabilities (due to the possibility of hallucination loops), we also report the average time per 100 tokens. As can be seen, multi-token approach allows ÉCLAIR to outperform competing methods speed-wise, despite it being a bigger model parameter-wise. The details on evaluation protocol can be found in the supplementary material.

## Method

tkn step WER ↓ F1 ↑ sec img ↓ sec 100 ↓ Nougat [6] 1 --4.7 0.41 GOT [52] 1 0.25 0.82 9.8 0.90 ÉCLAIR 1 0.14 0.93 3.8 0.42 ÉCLAIR-2tkn 2 0.13 0.94 2.5 0.31 1 0.12 0.95 3.8 0.42 ÉCLAIR-3tkn 3 0.15 0.92 1.77 0.23 1 0.13 0.94 3.8 0.42 ÉCLAIR-4tkn 4 0.17 0.90 1.32 0.20 1 0.14 0.94 3.8 0.42 Table 8. Results and speed of multi-token models and competing methods. We report the average speed per image on DROBS test set ( sec img ), and speed per 100 tokens ( sec 100 ). These values are obtained from a PyTorch-based inference pipeline on an NVIDIA H-100 GPU.

## Related Work

Document Understanding Models Models like Lay-outLMv3 [[18]](#b17) excel in parsing complex documents for tasks such as layout analysis and visual question answering. However, they rely heavily on pre-extracted text, im-ages, and bounding boxes, forming a brittle pipeline that can be error-prone due to its dependence on external systems. SwinDocSegmenter [[4]](#b3) and specialized variants of YOLO [[20]](#b19) have been trained for document-specific detection tasks without requiring additional inputs. While they effectively detect objects within documents, they generally do not output any text associated with these objects, lacking integrated OCR capabilities.

Object Detection in Documents is crucial for identifying and localizing elements within documents, aiding tasks like OCR and determining reading order. Traditional models such as Faster R-CNN [[43]](#b42) and Mask R-CNN [[14]](#b13) have been adapted for document analysis, effectively detecting and segmenting components like text blocks, images, and tables. Despite their success, these models typically do not provide textual content alongside the detected objects, limiting their usefulness for comprehensive document understanding.

End-to-End OCR-Free Models that do not depend on external OCR systems have gained attention. Donut [[23]](#b22) introduced a transformer-based encoder-decoder architecture pre-trained on general text documents. Building on this, Nougat [[6]](#b5) extended training to scientific documents, outputting structured markdown with L A T E X tables and equations. GOT [[52]](#b51) focused on enhancing the recognition of specialized documents containing molecular formulas, sheet music, and charts. Kosmos-2.5 [[31]](#b30) incorporated both markdown and plain text data with bounding boxes, introducing a prompt structure that allows users to choose between different output formats. However, these models may require compromises in prompt structures or may not handle a wide variety of document layouts effectively. Our proposed model, ÉCLAIR, is specifically trained to handle a greater variety of document layouts without requiring compromises in the prompt structure.

Multimodal Large Language Models like QwenVL [[3]](#b2), GPT-4O [[36]](#b35) and Claude [[1]](#b0) have demonstrated impressive OCR and document understanding capabilities, including the extraction of complex equations and tables in structured formats. While powerful, these models are large and computationally expensive, making them impractical for scaling to millions of pages. In contrast, ÉCLAIR is a sub-1B parameter model optimized for inference speed with multi-token decoding.

## Conclusion

In this work, we have presented ÉCLAIR, a general-purpose end-to-end text-extraction model. ÉCLAIR is the first model that extracts structured text, bounding boxes and semantic classes all at the same time. We are releasing a new benchmark dataset DROBS to capture the variety of layouts of various online documents and have shown that ÉCLAIR outperforms all current competitors on this benchmark. Ad-ditionally, we investigate and provide a technique to improve the inference time for ÉCLAIR. We hope that this will aid the OCR community in improving document-based text extraction, and benefit the LLM community by increasing the availability of previously unseen text data for training. For ÉCLAIR, we adopt a simple hallucination mitigation strategy to filter out such occurrences: the inference-time prompt is always set to the Maximal Informative Prompt (MIP) and we do a strict syntax check on the resulting predictions to reject non-compliant boxes. Some examples of hallucinations detected and filtered out using this strategy are shown in Figure [S2](#fig_5). We also enforce the spatial and categorical validity of the remaining boxes by verifying that the bottom-right corner of each bounding box exceeds the top-left corner and that classes conform to a validated schema. By implementing this layered filtering strategy, we observe a substantial reduction in model hallucinations.

## S5. Object detection S5.1. mAP -Back to Simple Metrics

In line with previous works on autoregressive detection [[2]](#b1), we find mAP to be inherently unfavorable to end-to-end models like ÉCLAIR. Dedicated object detectors generally predict a set of bounding boxes of a fixed size as a raw output, where each bounding box is associated with a confidence score. Consequently, it is possible to control the recall-precision trade-off of the model by adjusting the confidence threshold by which the raw predictions are filtered.

Naturally, this results in a possibility of achieving a high recall for the model, albeit at low precision. Instead, ÉCLAIR predicts a set of bounding boxes in line in the output stream of tokens, requiring no filtering or thresholding.

An inherent problem with our end-to-end detector is the absence of a score for detected boxes that could be used to rank them. Using the likelihood/logits of the initial coordinate tokens is not ideal, as they indicate the distribution over potential starting points rather than an independent probability. Similarly, class-token logits only provide a distribution over class choices, not the probability of the box's existence. Considering text tokens is also impractical, as they represent the actual text rather than the existence of the surrounding box. Therefore, our predictor does not generate a box score. On one hand, as there is no overprediction, no subsequent filtering or post-processing (such as non-maximum suppression) as well as no score is necessary. On the other hand, this makes comparison on the average precision metric challenging, as when considering all of the predicted bounding boxes jointly, only a single recall level exists for ÉCLAIR, making area calculation not meaningful.

Therefore, it can be seen that comparing ÉCLAIR against other works on the mAP metric poses challenges: 1. AP is the area under the PR-Curve, which degenerates to a single point without the possibility to rank predictions, making the calculation of the area not meaningful. 2. The COCO implementation assumes scores are unique.

Identical scores (as in our case) lead to incorrect PR-Curves and inconsistent results. [4](#foot_0) .

3. COCO mAP is computed per class independently (i.e. first separate classes, then match boxes). We propose to first match boxes over all classes and then compute the per-class precision/recall, which allows us to plot a confusion matrix, to better visualize problematic cases. Pre-

eval/issues/46 and [https://github.com/cocodataset/ cocoapi/issues/678](https://github.com/cocodataset/cocoapi/issues/678)

![Figure 1. ÉCLAIR outperforms other methods on complex documents: (a) tables, formulas, figure, page header and multiple columns; (b) uneven columns, styling, figure; (c) non-obvious reading order and visual elements like background coloring.]()

![Figure 2. Meta architecture for ÉCLAIR showcasing the usage with two different (out of eight valid) prompts: Example a) uses the maximal information prompt to return bounding boxes along with their semantic class, markdown text, and tables and formulas. In b) we ask the model to return only markdown text without boxes or classes. All supported semantic classes are listed on the right. with 8 possible valid combinations (ignoring the trivial case of no output, and the cases where semantic classes are requested without the corresponding bounding boxes):]()

![<x (\d+)><y (\d+)>(. * ?)<x (\d+)><y (\d+)> <class ([ˆ>]+)>]()

![Figure 3. Example pages from DROBS, our visually diverse document benchmark.]()

![Figure S1. Illustrations of reading order over relevant text-like elements, i.e. Text, Section-header, List-item, Title and Formula. Other semantic classes (such as Picture, Footnote and Page-footer in the examples here) are not included in the reading order of the main body. (Note: We are not showing all the classes)]()

![Figure S2. Examples of hallucinations in the ÉCLAIR predictions. The hallucinations (in this case, repetition loops), marked in red, are detected and filtered out by our hallucination-mitigation strategy.]()

![Figure S5. Examples of pages with tables, formulae and pictures. On the left, predicted bounding boxes superimposed on the original sample image. On the right, the corresponding full predictions.]()

![Summary of the datasets used to train ÉCLAIR, including a description of the maximum information available in the annotations of each dataset.]()

![Edit distance ↓ F1 ↑ Precision ↑ Recall ↑ BLEU ↑ METEOR ↑Evaluation results on DROBS. Reported standard NLTK metrics[41] are character level (Edit-distance) or word level (F1, Precision, Recall, BLEU, METEOR) metrics typically used by the OCR and natural language processing (NLP) communities. We also report Counting F1 and word error rate/word edit distance metrics.]()

![ÉCLAIR with a size of 936M parameters and in MIP mode demonstrates competitive or superior accuracy across most metrics, particularly]()

![Accuracy comparison of various methods across different metrics in both English and Chinese (zh). Currently ÉCLAIR doesn't train with additional chinese data or other form of multi-lingual data. The numbers in top row are obtained from GOT[52].]()

![Evaluation]()

![highlight ÉCLAIR's effectiveness in extracting high quality training data for improved LLM accuracy. Details about the training setup and post-processing steps for ÉCLAIR can be found in the supplementary material.]()

See https :/ /github . com/ MiXaiLL76/ faster _coco _

