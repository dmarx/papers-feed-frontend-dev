<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-06">6 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuanhui</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amonnut</forename><surname>Thammatadatrakoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
							<email>wenzhao.zheng@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunpeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">PhiGent Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dalong</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">PhiGent Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-06">6 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">129EEDE8E4FBE5379A6F090BF34FA279</idno>
					<idno type="arXiv">arXiv:2412.04384v2[cs.CV]</idno>
					<note type="submission">motorcycle bicycle car pedestrian driveable surface other flat sidewalk terrain manmade vegetation GaussianFormer 25600 51200 144000 6400 12800 25600 GaussianFormer Ours 25600 51200 144000 6400 12800 25600 Memory Consumption</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1. We approach efficient object-centric scene representation from a probabilistic perspective and propose the probabilistic Gaussian superposition model, which achieves SOTA performance with as few as 8.9% of Gaussians in GaussianFormer [15].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In autonomous driving, vision-centric systems have been more cost-effective compared with the LiDAR-based coun-* Project leader. terparts. However, their inability to capture obstacles with arbitrary shapes poses challenges for driving safety and reliability <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. The advent of 3D semantic occupancy prediction methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b51">51]</ref> alleviates this limitation by predicting the fine-grained geometry and semantics of the surrounding 3D environment. This advancement supports a range of emerging applications, including end-to-end autonomous driving <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">40]</ref>, 4D occupancy forecasting <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b52">52]</ref>, and self-supervised 3D scene understanding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b45">45]</ref>.</p><p>Despite the promising applications, 3D semantic occupancy prediction is essentially a dense three-dimensional segmentation task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">39]</ref>, which necessitates a both efficient and effective representation of the 3D scene. Voxelbased methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">44]</ref> use dense 3D voxels as representation to describe the scene with the finest detail. However, they neglect the spatial redundancy of the 3D occupancy and suffer from high computational complexity. As a workaround, planer representations, such as BEV <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">50]</ref> and TPV <ref type="bibr" target="#b12">[13]</ref>, compress the 3D grid along one of the axes to derive 2D feature maps for reduction of the token number. Nonetheless, they still take into account the empty region when modeling the environment, which compromises their model capacity and efficiency. As a pioneer in object-centric sparse scene representations, 3D semantic Gaussians <ref type="bibr" target="#b14">[15]</ref> describe the 3D space in a sparse way with learnable mean, covariance, opacity and semantics for each Gaussian. However, several limitations persist in the current 3D semantic Gaussian representation: 1) Each Gaussian can still describe the empty region, which renders most of the Gaussians useless in an object-centric formulation given the sptial sparsity of 3D occupancy. 2) The aggregation process ignores the overlapping issue and directly sums up the contribution of each Gaussian to produce occupancy prediction, which results in unbounded semantic logits and further increases the overlapping among Gaussians. Thus, the proportion of effective Gaussians describing occupied regions independently could be extremely low, which undermines the efficiency of the 3D semantic Gaussian representation.</p><p>In this paper, we introduce a probabilistic Gaussian superposition model to resolve the above limitations of 3D semantic Gaussians and improve utilization and efficiency.</p><p>To elaborate, we propose the probabilistic Gaussian representation, which assigns 3D Gaussians to exclusively model the non-empty area by interpreting each Gaussian as a probability distribution of its neighborhood being occupied. We employ the multiplication theorem of probability to aggregate the independent probability distributions and derive the geometry predictions. Furthermore, we integrate the Gaussian mixture model into out probabilistic Gaussian representation to generate normalized semantic predictions, which avoid unbounded logits and prevents Gaussians from unnecessary overlapping. Since our representation only models the occupied region, we also design a distributionbased initialization module to effectively initialize Gaussians around the non-emtpy area, which learns the pixelaligned occupancy distribution instead of depth values of surfaces <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. We conduct extensive experiments on the nuScenes <ref type="bibr" target="#b1">[2]</ref> and KITTI-360 <ref type="bibr" target="#b26">[26]</ref> datasets for surroundview and monocular 3D semantic occupancy prediction, respectively. Our GaussianFormer-2 outperforms state-ofthe-art methods with high efficiency. In addition, qualitative visualizations show that GaussianFormer-2 is able to generate a both holistic and realistic perception of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Semantic Occupancy Prediction. 3D semantic occupancy prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">39]</ref> has emerged as a promising environment modeling in autonomous driving as it describes driving scenes in a comprehensive manner. This task aims to label each voxel in the scene by taking one or more types of sensors as input. Two most used sensors are LiDAR and the camera. Although LiDAR-based methods perform remarkably well in 3D perception tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b53">53]</ref>, they possess limitations under bad weather conditions and in detecting distant objects; Thus, camera-based approaches have garnered increasing attention <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b50">50]</ref>. Pioneer works in 3D semantic occupancy prediction task adopt dense grid-based representation as a straightforward mean to derive occupancy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">44]</ref>, then subsequent works turn to sparse object-centric representation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b38">38]</ref> as a solution to the innate problem of redundancy for dense representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Voxel-based</head><p>Plane-based Ours GaussianFormer represents elements devoted for emptiness <ref type="bibr">Figure 2</ref>. Representation comparisons. Voxel and plane based representations inevitably incorporate emptiness when modeling the 3D scene. While GaussianFormer <ref type="bibr" target="#b14">[15]</ref> proposes 3D semantic Gaussian as a sparse representation, it still suffer from spatial redundancy. Our method achieves true object-centricity through probabilistic modeling.</p><p>Grid-based scene representations. Plane representations have emerged as competitive representations in scene perception tasks for autonomous driving. BEVFormer <ref type="bibr" target="#b22">[23]</ref> is the initiative work of the kind <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33]</ref> that utilizes only camera input and performs comparably well with LiDAR-based methods in detection and segmentation tasks. It converts the image feature to the bird's-eye-view (BEV) feature as a unified scene representation, since the information is most diverse at this point of view. The BEV feature is then used for downstream tasks. However, the BEV feature is not suitable for 3D occupancy construction as it causes height information to be lost <ref type="bibr" target="#b44">[44]</ref>. As a generalization of BEV space, TPVFormer <ref type="bibr" target="#b12">[13]</ref> proposes tri-perspective view representation to include also the height information, thus making it more suitable for 3D scenes. Another research direction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">44]</ref> adopts voxel-based representation as a more 3D-specific and fine-grained approach, making it favorable for 3D volumetric semantic prediction. Nevertheless, these methods utilize dense grid-based representation, which describes each voxel equally regardless of the spatial sparsity of the environment, thus resulting in intrinsic redundancy.</p><p>Object-centric scene representations. To eliminate spatial redundancy inherent in dense representations, many recent works adopt sparse representation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b38">38]</ref>. One line of work divides dense grids into partitions where objects present and omits the regions foreseen as empty <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b38">38]</ref>. However, non-empty regions might be mistakenly clas- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GS. Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prob. Modeling</head><p>Figure <ref type="figure">3</ref>. Overall pipeline of our method. To achieve probabilistic modeling, we decompose occupancy prediction into geometry and semantics prediction, and approach them separately using probabilistic multiplication and Gaussian mixture model to improve efficiency. sified as unoccupied and eliminated completely from the whole subsequent process. Another line of work leverages point representation <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b41">41]</ref> by sampling points within the scene range as queries in the succeeding refinement process; Nevertheless, a point has a limited range of depiction as it has no spatial extent. An alternative approach, Gaussian-Former <ref type="bibr" target="#b14">[15]</ref>, adopts 3D semantic Gaussian representation, where probability spreads around a mean, allowing more utilization. However, spatial redundancy persists due to no regulation for the Gaussians to not represent emptiness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we present our method of probabilistic Gaussian superposition for efficient 3D semantic occupancy prediction. We first review the original 3D semantic Gaussian representation <ref type="bibr" target="#b14">[15]</ref> and its limitations (Sec. 3.1). We then introduce our probabilistic Gaussian modeling and how we derive geometry and semantics predictions based on the multiplication theorem of probability and Gaussian mixture model (Sec. 3.2). Finally, we detail the distribution-based initialization module to effectively initialize probabilistic Gaussians around the occupied area (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Semantic Gaussian Representation</head><p>Vision-centric 3D semantic occupancy prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> aims to obtain the fine-grained geometry and semantics of the 3D scene. To formulate, the target is to predict voxellevel semantic segmentation result O ∈ C X×Y ×Z given input images I = {I i } N i=1 , where C, {X, Y, Z}, N represent the set of predefined classes, the spatial resolution of occupancy and the number of input views, respectively.</p><p>To achieve this, 3D semantic Gaussian representation employs a set of P Gaussian primitives G = {G i } P i=1 , with each G i describing a local region with its mean m i , scale s i , rotation r i , opacity a i and semantics c i . Gaussian-Former interprets these primitives as local semantic Gaussian distributions which contribute to the overall occupancy prediction through additive aggregation:</p><formula xml:id="formula_0">ô(x; G) = P i=1 g i (x; m i , s i , r i , a i , c i ),<label>(1)</label></formula><p>where g i (x; •) denotes the contribution of the ith semantic Gaussian to ô(x; G) which is the overall occupancy prediction at location x. The contribution g is further calculated as the corresponding semantic Gaussian distribution evaluated at location x:</p><formula xml:id="formula_1">g(x; G) = a • exp - 1 2 (x -m) T Σ -1 (x -m) c,<label>(2)</label></formula><formula xml:id="formula_2">Σ = RSS T R T , S = diag(s), R = q2r(r),<label>(3)</label></formula><p>where Σ, R, S represent the covariance matrix, the rotation matrix constructed from the quaternion r with function q2r(•), and the diagonal scale matrix from function diag(•).</p><p>Although the number of Gaussians is reduced compared with the number of dense voxels thanks to the deformable nature of Gaussian distributions as in Eq. ( <ref type="formula" target="#formula_1">2</ref>), several limitations still persist in the 3D semantic Gaussian representation. First of all, it models both the occupied and unoccupied regions in the same way using the semantic property c, resulting in most Gaussians being classified as empty given the huge proportion of empty space in outdoor scenarios. Secondly, the semantic Gaussian representation encourages Gaussians to overlap, because the aggregation process in Eq. ( <ref type="formula" target="#formula_0">1</ref>) independently sums up the contribution of each Gaussian, resulting in unbounded occupancy prediction ô. For optimization, the model would learn to allocate more Gaussians to describe the same region due to the unbounded nature of ô, aggravating the overlap between Gaussians. These limitations stem from the current interpretation of Gaussians and obstruct the efficiency and effectiveness of the 3D semantic Gaussian representation. Our method approaches Gaussian-based object-centric representation from a probabilistic perspective, serving as a fundamental solution to these issues, as shown by Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Probabilistic Gaussian Superposition</head><p>We propose the probabilistic Gaussian superposition as an efficient and effective 3D scene representation. As shown in Figure <ref type="figure">3</ref>, we decompose the 3D modeling target into geometry and semantics predictions, and adopt the multiplication theorem of probability and the Gaussian mixture model to address them from a probabilistic perspective, respectively.</p><p>Geometry prediction. To restrict Gaussians to represent only occupied regions for geometry prediction, we interpret the Gaussian primitives G = {G i } P i=1 as the probability of their surrounding space being occupied. To elaborate, we assign a probability value of 100% at the centers of Gaussians, which decays exponentially with respect to the distance from the centers m:</p><formula xml:id="formula_3">α(x; G) = exp - 1 2 (x -m) T Σ -1 (x -m) ,<label>(4)</label></formula><p>where α(x; G) denotes the probability of the point x being occupied induced by Gaussian G. Eq. ( <ref type="formula" target="#formula_3">4</ref>) assigns a high probability of occupancy when the point x is close to the center of Gaussian G, which prevents any Gaussian from describing empty area. To further derive the overall probability of occupancy, we assume that the probabilities of a point being occupied by different Gaussians are mutually independent, and thus we can aggregate them according to the multiplication theorem of probability:</p><formula xml:id="formula_4">α(x) = 1 - P i=1 1 -α(x; G i ) ,<label>(5)</label></formula><p>where α(x) represents the overall probability of occupancy at point x. In addition to achieving object-centric properties, Eq. ( <ref type="formula" target="#formula_4">5</ref>) also avoids unnecessary overlapping between Gaussians because α(x) ≥ α(x; G i ) holds for any Gaussian G i . This implies that point x would be predicted occupied if it is close enough to any single Gaussian. Semantics prediction. In addition to object-centric antioverlapping geometry modeling, we still need to achieve the same goals for semantics prediction. We first remove the channel that represents the empty class from the semantic properties c of Gaussians since it has been accounted for in geometry prediction. Then we interpret the set of Gaussians G as a Gaussian mixture model, where semantics prediction could be formulated as calculating the expectation of semantics given the probabilistic Gaussian mixture model. Specifically, we take the original opacity properties a as the prior distribution of Gaussians, which is l 1 -normalized. Furthermore, we adopt the Gaussian probabilistic distribution parameterized by mean m, scale s and rotation r as the conditional probability. Then we normalize the original semantics properties c with softmax to ensure the boundedness of predicted semantics. Finally, we calculate the expectation e(x; G) as:</p><formula xml:id="formula_5">e(x; G) = P i=1 p(G i |x)c i = P i=1 p(x|G i )a i ci P j=1 p(x|G j )a j ,<label>(6)</label></formula><formula xml:id="formula_6">p(x|Gi) = 1 (2π) 3 2 |Σ| 1 2 exp - 1 2 (x -m) T Σ -1 (x -m) ,<label>(7)</label></formula><p>where p(G i |x), p(x|G i ) and ci denote the posterior probability of point x belonging to the ith Gaussian distribution, the conditional probability of point x given the ith Gaussian distribution, and the softmax-normalized semantic properties, respectively. Compared with Eq. (1)(2), the gaussian mixture model in Eq. ( <ref type="formula" target="#formula_5">6</ref>) normalizes the semantic properties and the contributions from different Gaussians, thus preventing unnecessary overlapping between Gaussians and producing normalized class probabilities directly.</p><p>Given the geometry and semantics predictions, we take a simple step forward to combine them to generate the final semantic occupancy prediction:</p><formula xml:id="formula_7">ô(x; G) = [1 -α(x); α(x) • e(x; G)],<label>(8)</label></formula><p>where we use the geometry probability α(x) to weight the semantic predictions, and directly take 1-α(x) as the probability of the empty class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Distribution-Based Initialization</head><p>Previous 3D semantic Gaussian representation adopts a learnable initialization strategy, which randomly initializes the properties of Gaussians at the beginning of training, and optimizes this initialization in a data-driven way. This strategy enables the model to learn a prior distribution of occupancy of the whole dataset, which relies on the subsequent refinement of the network to adapt to the distribution of each individual sample. However, the local receptive field of Gaussians limits their mobility, which hinders each Gaussian from learning the path to the correct position in subsequent refinement. And this issue is even more severe for our probabilistic Gaussian superposition where Gaussians are supposed to model only occupied regions.</p><p>To remedy this issue, we propose a distribution-based initialization module which provides both more accurate and holistic sample-specific initialization for Gaussians, as shown by Figure <ref type="figure" target="#fig_1">4</ref>. We supervise the image features from a 2D backbone with the pixel-aligned occupancy distribution derived from the occupancy annotations. To elaborate, we first determine the origin b and direction d of the ray corresponding to each image feature with the camera calibration data. We then sample R reference points at equal intervals in a fixed depth range along this ray. For each of these reference points, we query the ground truth occupancy O at the corresponding location to obtain the binary labels l = {l i } R i=1 indicating whether a reference point is occupied or not. Then we use l = {l i } R i=1 as supervision to optimize our initialization module, which consists of an image backbone B and a distribution predictor M. The distribution predictor M directly decodes image features into occupancy distributions l along corresponding rays, which are matched against l using binary cross entropy loss:</p><formula xml:id="formula_8">loss init = BCE l, l = BCE M(B(I)), l .<label>(9)</label></formula><p>Different from previous initialization schemes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref> that predict the depth values with LiDAR supervision, our method learns the holistic occupancy distribution rather than only visible surfaces of the scene, and does not require any additional modality as supervision. Overall, our distribution-based initialization module initializes the Gaussians, which are subsequently sent into B blocks of attention-based architecture as in Gaussian-Former <ref type="bibr" target="#b14">[15]</ref>. Each block consists of self-encoding, image cross-attention, and refinement module, where probabilistic Gaussian properties steadily improve, then the resulting Gaussians are aggregated by our new method that encourages higher utilization of Gaussians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>The nuScenes dataset <ref type="bibr" target="#b1">[2]</ref> provides 1000 scenes of surround view driving scenes in Boston and Singapore. The official division is 700/150/150 scenes for training, validation, and testing, respectively. Each scene is 20 seconds long and fully annotated at 2Hz with ground truth from 5 radars, 6 cameras, one LiDAR, and one IMU. We employ 3D semantic occupancy annotations from SurroundOcc <ref type="bibr" target="#b44">[44]</ref> for supervision and evaluation. The ranges of the occupancy annotations in the x, y, and z axes in meters are <ref type="bibr">[-50, 50]</ref>, <ref type="bibr">[-50, 50]</ref>, and [-5, 3], respectively, where each voxel has a side length of 0.5 meters and is labeled as one of the 18 possible classes (16 semantics, 1 empty, and 1 noise class).</p><p>The KITTI-360 dataset <ref type="bibr" target="#b26">[26]</ref> consists of over 320k images in suburban area with rich 360 degree sensory ground truth, consisting of 2 perspective cameras, 2 fisheye cameras, a Velodyne LiDAR, and a laser scanner, where we use the images from the left camera of the ego car as input to our model. For 3D semantic occupancy prediction, we adopt the annotations from SSCBench-KITTI-360 <ref type="bibr" target="#b20">[21]</ref>. The official split is 7/1/1 sequences with 8487/1812/2566 key frames for training, validation, and testing, respectively. The voxel grid area covers 51.2×51.2×6.4 m 2 in front of the ego car with resolution of 256×256×32. Each voxel is classified as one of the 19 classes (18 semantics and 1 empty).</p><p>The evaluation metrics are in accordance with common practice <ref type="bibr" target="#b2">[3]</ref>, namely mean Intersection-over-Union (mIoU) and Intersection-over-Union (IoU):</p><formula xml:id="formula_9">mIoU = 1 |C ′ | i∈C ′ T P i T P i + F P i + F N i , (<label>10</label></formula><formula xml:id="formula_10">)</formula><formula xml:id="formula_11">IoU = T P ̸ =c0 T P ̸ =c0 + F P ̸ =c0 + F N ̸ =c0 ,<label>(11)</label></formula><p>Where C ′ , c 0 , TP, FP, and FN represent the non-empty classes, the empty class, and the number of true positive, false positive, and false negative predictions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The input images are at resolutions of 900×1600 for nuScenes <ref type="bibr" target="#b1">[2]</ref> and 376x1408 for KITTI-360 <ref type="bibr" target="#b26">[26]</ref> with random flipping and photometric distortion augmentations. We use the same checkpoints for our image backbone as used in GaussianFormer <ref type="bibr" target="#b14">[15]</ref>, i.e. ResNet101-DCN <ref type="bibr" target="#b9">[10]</ref> with FCOS3D checkpoint <ref type="bibr" target="#b43">[43]</ref> for nuScenes, and ResNet50 <ref type="bibr" target="#b9">[10]</ref> pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref> for KITTI-360. The numbers of Gaussians are set to 12800 and 38400 in our main results for nuScenes and KITTI-360, respectively. We train our model using AdamW <ref type="bibr" target="#b29">[29]</ref> with weight decay of 0.01, and maximum learning rate of 2 × 10 -4 , which decays with a cosine annealing schedule. We train our model for 20 epochs on nuScenes with a batch of 8 and 30 epochs on KITTI-360 with a batch size of 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>Surround-view 3D semantic occupancy prediction. We report the performance of our GaussianFormer-2 in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Our approach achieves state-of-the-art performance compared with other methods. Specifically, GaussianFormer-2 surpasses methods based on dense grid representation in classes such as bicycle and motorcycle, proving the flexibility of the proposed probabilistic Gaussian superposition in modeling small objects. Furthermore, our method outperforms GaussianFormer <ref type="bibr" target="#b14">[15]</ref> with a clear margin and significantly fewer Gaussians (12800 v.s. 144000), which validates the efficiency and effectiveness of our method. Monocular 3D semantic occupancy prediction. We report the results for monocular 3D semantic occupancy prediction on SSCBench-KITTI-360 <ref type="bibr" target="#b20">[21]</ref> in Table <ref type="table" target="#tab_1">2</ref>. Our method achieves state-of-the-art performance, surpassing the original GaussianFormer in mIoU by 7.6%. To elaborate, we observe significant improvement in mIoU of classes such as road, sidewalk and building compared with GaussianFormer, showing the superiority of probabilistic Gaussian superposition in modeling background staff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Number of Gaussians. We report the influence of the number of Gaussians on the efficiency and performance of our model in Table <ref type="table" target="#tab_2">3</ref>. Our model achieves better performanceefficiency trade-off compared with GaussianFormer, outperforming it with less than 5% number of Gaussians. The latency of our method is higher than GaussianFormer, which we attribute to the time-consuming farthest point sampling (FPS) operation in our initialization module. We adopt the divide-and-conquer strategy to conduct the FPS operation in a batched manner for acceleration, and report the latency of the initialization module in parentheses.</p><p>Design Choices. We conduct ablation study on the design choices of GaussianFormer-2 in Table <ref type="table" target="#tab_3">4</ref>. We observe consistent improvement for both probabilistic modeling and distribution-based initialization module which surpasses the depth-based counterpart with a clear margin.</p><p>Utilization of Gaussians. We provide comparisons on the utilization of Gaussians between GaussianFormer <ref type="bibr" target="#b14">[15]</ref> and our method in Table <ref type="table">5</ref> using two important factors that reflect the utilization of Gaussians, which are position and overlap. Percentage of Gaussians in correct positions (Perc.) is percentage of Gaussians with their mean positions in the occupied space. Overall overlap is calculated as summation of volumes of all Gaussians at 90% over the coverage volume of all Gaussians, while individual overlap is computed by the average of the summation of the Bhattacharyya coefficient of each Gaussian with any other Gaus-Table <ref type="table">5</ref>. Ablation on the efficiency of GaussianFormer-2. We set the number of Gaussians to 25600. Perc. and Dist. denote the percentage of Gaussians in correct positions, and the average distance of each Gaussian to its nearest occupied voxel, respectively. Overall and Indiv. represent the overall and individual overlapping ratios of Gaussians, respectively.</p><p>Method Position Overlap mIoU IoU Perc. (%) ↑ Dist. (m) ↓ Overall ↓ Indiv. ↓ GaussianFormer [15] 16.41 3.07 10.99 68.43 16.00 28.72 Ours 28.85 1.24 3.91 12.48 20.32 31.04</p><p>sians. We provide detailed information about these factors in the appendix. Our method outperforms GaussianFormer on all these metrics, demonstrating better utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualizations</head><p>We provide Gaussian and occupancy visualizations in Figure 5. Our model is able to predict reasonable Gaussian distributions and comprehensive occupancy results. Further, we compare our method against GaussianFormer in Figure 6. Our Gaussians are more adaptive in shape compared with isotropic spherical Gaussians in GaussianFormer. We also visualize the xy coordinates of Gaussians in the initialization and subsequent blocks of GaussianFormer-2 in Figure 7. We find that the Gaussians successfully learn to move towards occupied area thanks to the object-centric probabilistic design and effective initialization module. 3D Gaussians Occupancy Prediction Occupancy Ground Truth barrier construction vehicle motorcycle traffic cone trailer bicycle bus car pedestrian truck driveable surface other flat sidewalk terrain manmade vegetation GaussianFormer Ours GaussianFormer Ours</p><note type="other">Figure 6</note><p>. Comparison with GaussianFormer <ref type="bibr" target="#b14">[15]</ref>. Our method predicts 3D Gaussians with more adaptive shapes compared with GaussianFormer. Although our method uses only 8.8% Gaussians, it still generates comprehensive occupancy predictions and alleviates the elongated effect in GaussianFormer.</p><p>Figure <ref type="figure">7</ref>. Visualizations of Gaussian positions in the refinement process. We observe that our probabilistic Gaussians equipped with distribution-based intialization successfully learn to move towards occupied regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a probabilistic Gaussian superposition model as an efficient object-centric representation. Specifically, we interpret each Gaussian as a probability distribution of its neighborhood being occupied and adopt the multiplication theorem of probability to derive the geometry predictions. And we employ the Gaussian mixture model formulation to calculate normalized semantics predictions. We have also designed a distribution-based initialization strategy to effectively initialize Gaussians around occupied area for object-centric modeling according to pixel-aligned occupancy distribution. Our GaussianFormer-2 has achieved state-of-the-art performance on nuScenes and KITTI-360 datasets for 3D semantic occupancy prediction, which has also demonstrated improved efficiency compared with GaussianFormer on the number of Gaussians, position correctness and overlapping ratio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Demonstration</head><p>Figure <ref type="figure" target="#fig_3">8</ref> shows a sampled frame of our video demonstration 1 for 3D semantic occupancy prediction on the nuScenes dataset <ref type="bibr" target="#b1">[2]</ref>. We note that the camera-view occupancy visualizations align well with the input RGB images. Moreover, each instance is sparsely described by only a few Gaussians with adaptive shapes, which demonstrates the efficiency and the object-centric nature of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualizations on KITTI-360</head><p>We provide visualization results of Gaussians and occupancy on the KITTI-360 dataset <ref type="bibr" target="#b26">[26]</ref> in Figure <ref type="figure" target="#fig_4">9</ref>. We observe that our GaussianFormer-2 is able to predict both intricate geometry and semantics of the 3D scene. Furthermore, the 3D Gaussians in our model are adaptive in their scales according to the specific objects they are describing, compared with isotropic spherical Gaussians with maximum scales in GaussianFormer <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Metric Details</head><p>Position. Gaussians, even after full training, can still be found in unoccupied space due to the localized nature of the receptive field. These Gaussians fail to describe meaningful structures, rendering them ineffective and devoid of practical utility. A higher proportion of Gaussians in unoccupied space indicates suboptimal utilization. Hence, we define the 1 <ref type="url" target="https://github.com/huang-yh/GaussianFormer">https://github.com/huang-yh/GaussianFormer</ref> percentage of Gaussians in correct positions (Perc.) as:</p><formula xml:id="formula_12">Perc. = N correct N total • 100%,<label>(12)</label></formula><p>where N correct , and N total denote the number of Gaussians of which means are in occupied space, and the total number of Gaussians, respectively. A higher percentage indicates a better alignment of the Gaussians with occupied or meaningful area in the space, thus reflecting a more efficient use of the model's capacity.</p><p>The above measurement provides a hard evaluation, where Gaussians are either classified as being in correct or incorrect positions without considering their proximity to the nearest occupied area. This binary approach does not capture how close Gaussians in unoccupied regions are to meaningful positions. To address this limitation, we define a complementary soft measurement as the average distance of each Gaussian to its nearest occupied voxel center, denoted as Dist. (in meters), computed as follows:</p><formula xml:id="formula_13">Dist. = 1 P P i=1 min v∈V ||m i -v|| 1 ,<label>(13)</label></formula><p>where m i , V, v, and ||m i -v|| 1 denote the mean of the i-th Gaussian, the set of occupied voxel centers, the center of one voxel in this set, and L1 distance between the mean of the Gaussian and the voxel center, respectively. Note that this distance is calculated with respect to the voxel center, and thus Gaussians positioned within the correct occupied area may also have a non-zero distance.</p><p>barrier pole other-structure other-object traffic-sign terrain building vegetation motorcycle bicycle car person truck road other-ground sidewalk other-vehicle parking Input Image 3D Gaussians Pred. Occupancy Occupancy G.T. Overlap. The overall overlapping ratio of Gaussians (Overall.) provides a global perspective on the redundancy in the Gaussian representation. Each Gaussian is modeled as an ellipsoid, where the semi-axis lengths are derived from the Mahalanobis distance at a chi-squared value of 6.251, corresponding to the 90% confidence level of a Gaussian distribution in three degrees of freedom (DoFs). The Overall. is then calculated as the ratio of the summed 90% confidence volumes V i,90% of all Gaussians to the total coverage volume of all Gaussians V coverage in the scene:</p><formula xml:id="formula_14">Overall. = P i=1 V i,90% V coverage ,<label>(14)</label></formula><p>where V coverage represents the volume of all Gaussians combined as a unified shape. To estimate V coverage , we employ the Monte Carlo method where a large number of points are randomly sampled within the bounding box of the scene.</p><p>For each sampled point, we check whether it lies within the 90% confidence ellipsoid of any Gaussian. The volume is then approximated as:</p><formula xml:id="formula_15">V coverage = V scene • N in N total ,<label>(15)</label></formula><p>where N in , and N total are the number of sampled points that fall within the 90% confidence ellipsoid of at least one Gaussian, and the total number of sampled points, respectively. This approach ensures an accurate estimation of the unified volume, efficiently handling the overlapping regions of the Gaussians by not double-counting them. We next detail the derivation of the ellipsoid volume cor-responding to the 90% confidence region of a 3D Gaussian distribution. Considering a multivariate Gaussian distribution in 3D defined as:</p><formula xml:id="formula_16">g(x) = 1 (2π) 3/2 |Σ| 1/2 exp - 1 2 (x -m) T Σ -1 (x -m) ,<label>(16)</label></formula><p>where x, Σ, and |Σ| are the mean vector, 3x3 covariance matrix, and the determinant of the covariance matrix, respectively. The Mahalanobis distance d of point x from the mean m is defined as:</p><formula xml:id="formula_17">d 2 (x, m) = (x -m) T Σ -1 (x -m).<label>(17)</label></formula><p>The 90% confidence region of the Gaussian distribution corresponds to the set of points for which the Mahalanobis distance satisfies:</p><formula xml:id="formula_18">d 2 ≤ χ 2 3,0.9 ≈ 6.251,<label>(18)</label></formula><p>where χ 2 3,0.9 is the chi-square critical value for three degrees of freedom at the 90% confidence level. For a Gaussian distribution, the semi-axis lengths are determined by the square root of the eigenvalues of Σ, scaled by χ 2 3,0.9 . Thus, the volume of the ellipsoid from 90% of the 3D Gaussian distribution is:</p><formula xml:id="formula_19">V 90% = 4 3 π(6.251) 3/2 |Σ| 1/2 .<label>(19)</label></formula><p>A higher value of Overall. indicates greater overlapping volumes among the Gaussians, signifying redundancy in Gaussian representation. This metric provides insights into the utilization of Gaussians to represent the scene.</p><p>The individual overlapping ratio of Gaussians (Indiv.) offers a fine-grained analysis of the overlap between Gaussians in a scene. This measurement quantifies the degree to which each Gaussian overlaps with all other Gaussians, averaged across all Gaussians in the scene. The value of this metric indicates approximately how many times the volume of a single Gaussian is fully overlapped with other Gaussians on average. To compute this, we use the Bhattacharyya coefficient <ref type="bibr" target="#b0">[1]</ref>, which measures the similarity between two Gaussian distributions. The individual overlapping ratio is defined as:</p><formula xml:id="formula_20">Indiv. = 1 P P i=1   j̸ =i BC i,j   ,<label>(20)</label></formula><p>where BC i,j is the Bhattacharyya coefficient between the i-th and j-th Gaussians, given by:</p><formula xml:id="formula_21">BC i,j = 4 |Σ i ||Σ j | |Σ ij | e -1 8 (mi-mj ) T Σ -1 ij (mi-mj ) ,<label>(21)</label></formula><p>where Σ ij = 1 2 (Σ i + Σ j ) is the average covariance matrix. A higher value of Indiv. indicates more redundancy, as Gaussians are heavily overlapping with each other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Distribution-based initialization. Our initialization scheme learns pixel-aligned occupancy distributions from occupancy annotation, while the depth-based counterpart only captures the surfaces of objects and relies on LiDAR supervision.</figDesc><graphic coords="4,309.21,86.40,239.20,130.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Gaussian and occupancy visualizations on nuScenes. Our model is able to predict both comprehensive and realistic 3D Gaussians and occupancy.</figDesc><graphic coords="7,354.93,280.04,90.72,63.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Visualizations of Gaussians, camera-view and overall occupancy on nuScenes. We provide the input RGB images and their corresponding camera-view occupancy in the upper part. And we visualize the predicted 3D Gausians (left), the semantic occupancy in the global view (middle), and in the bird's eye view (right) in the lower part.</figDesc><graphic coords="9,58.50,72.00,494.99,231.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visualizations of Gaussians and occupancy on KITTI-360. Our method captures both the intricate geometry and semantics of the scene with shape-adaptive Gaussians.</figDesc><graphic coords="10,452.56,415.51,86.30,61.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Surround view 3D semantic occupancy prediction results on nuScenes. * means supervised by dense occupancy annotations as opposed to original LiDAR segmentation labels. Ch. denotes the channel dimension of our model. Our method achieves state-of-the-art performance compared with other methods..75  14.22 6.58 23.46 28.28 8.66 10.77 6.64 4.05 11.20 17.78 37.28 18.00 22.88 22.17 13.80 22.21 TPVFormer [13] 11.51 11.66 16.14 7.17 22.63 17.13 8.83 11.39 10.46 8.23 9.43 17.02 8.07 13.64 13.85 10.34 4.90 7.37 TPVFormer* [13] 30.86 17.10 15.96 5.31 23.86 27.32 9.79 8.74 7.09 5.20 10.97 19.22 38.87 21.25 24.26 23.15 11.73 20.81 OccFormer [51] 31.39 19.03 18.65 10.41 23.92 30.29 10.31 14.19 13.59 10.13 12.49 20.77 38.78 19.79 24.19 22.21 13.48 21.35 SurroundOcc [44] 31.49 20.30 20.59 11.68 28.06 30.86 10.70 15.14 14.09 12.06 14.38 22.26 37.29 23.70 24.49 22.77 14.89 21.86 GaussianFormer [15] 29.83 19.10 19.52 11.26 26.11 29.78 10.47 13.83 12.58 8.67 12.74 21.57 39.63 23.28 24.46 22.99 9.59 19.12 Ours (Ch. = 128) 30.56 20.02 20.15 12.99 27.61 30.23 11.19 15.31 12.64 9.63 13.31 22.26 39.68 23.47 25.62 23.20 12.25 20.73 Ours (Ch. = 192) 31.74 20.82 21.39 13.44 28.49 30.82 10.92 15.84 13.55 10.53 14.04 22.92 40.61 24.36 26.08 24.27 13.83 21.98</figDesc><table><row><cell>Method</cell><cell>IoU mIoU</cell><cell>■ barrier</cell><cell>■ bicycle</cell><cell>■ bus</cell><cell>■ car</cell><cell>■ const. veh.</cell><cell>■ motorcycle</cell><cell>■ pedestrian</cell><cell>■ traffic cone</cell><cell>■ trailer</cell><cell>■ truck</cell><cell>■ drive. suf.</cell><cell>■ other flat</cell><cell>■ sidewalk</cell><cell>■ terrain</cell><cell>■ manmade</cell><cell>■ vegetation</cell></row><row><cell>MonoScene [3]</cell><cell cols="17">23.96 7.31 4.03 0.35 8.00 8.04 2.90 0.28 1.16 0.67 4.01 4.35 27.72 5.20 15.13 11.29 9.03 14.86</cell></row><row><cell>Atlas [32]</cell><cell cols="17">28.66 15.00 10.64 5.68 19.66 24.94 8.90 8.84 6.47 3.28 10.42 16.21 34.86 15.46 21.89 20.95 11.21 20.54</cell></row><row><cell>BEVFormer [23]</cell><cell>30.50 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Monocular 3D semantic occupancy prediction results on SSCBench-KITTI-360. Our method achieves state-of-the-art performance compared with other methods, surpassing GaussianFormer<ref type="bibr" target="#b14">[15]</ref> by a clear margin. 40.27 13.81 22.58 0.66 0.26 9.89 3.82 2.77 54.30 13.44 31.53 3.55 36.42 4.80 31.00 19.51 7.77 8.51 6.95 4.60 GaussianFormer [15] C 35.38 12.92 18.93 1.02 4.62 18.07 7.59 3.35 45.47 10.89 25.03 5.32 28.44 5.68 29.54 8.62 2.99 2.32 9.51 5.14 Ours C 38.37 13.90 21.08 2.55 4.21 12.41 5.73 1.59 54.12 11.04 32.31 3.34 32.01 4.98 28.94 17.33 3.57 5.48 5.88 3.54</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>IoU mIoU</cell><cell>■ car</cell><cell>■ bicycle</cell><cell>■ motorcycle</cell><cell>■ truck</cell><cell>■ other-veh.</cell><cell>■ person</cell><cell>■ road</cell><cell>■ parking</cell><cell>■ sidewalk</cell><cell>■ other-grnd</cell><cell>■ building</cell><cell>■ fence</cell><cell>■ vegetation</cell><cell>■ terrain</cell><cell>■ pole</cell><cell>■ traf.-sign</cell><cell>■ other-struct.</cell><cell>■ other-object</cell></row><row><cell>LMSCNet [34]</cell><cell cols="4">L 47.53 13.65 20.91 0</cell><cell cols="3">0 0.26 0</cell><cell cols="10">0 62.95 13.51 33.51 0.2 43.67 0.33 40.01 26.80 0</cell><cell cols="3">0 3.63 0</cell></row><row><cell>SSCNet [36]</cell><cell cols="20">L 53.58 16.95 31.95 0 0.17 10.29 0.58 0.07 65.7 17.33 41.24 3.22 44.41 6.77 43.72 28.87 0.78 0.75 8.60 0.67</cell></row><row><cell>MonoScene [3]</cell><cell cols="20">C 37.87 12.31 19.34 0.43 0.58 8.02 2.03 0.86 48.35 11.38 28.13 3.22 32.89 3.53 26.15 16.75 6.92 5.67 4.20 3.09</cell></row><row><cell>Voxformer [22]</cell><cell cols="20">C 38.76 11.91 17.84 1.16 0.89 4.56 2.06 1.63 47.01 9.67 27.21 2.89 31.18 4.97 28.99 14.69 6.51 6.92 3.79 2.43</cell></row><row><cell>TPVFormer [13]</cell><cell cols="20">C 40.22 13.64 21.56 1.09 1.37 8.06 2.57 2.38 52.99 11.99 31.07 3.78 34.83 4.80 30.08 17.51 7.46 5.86 5.48 2.70</cell></row><row><cell>OccFormer [51]</cell><cell>C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation on the number of Gaussians. The latency and memory are tested on an NVIDIA 4090 GPU with batch size one during inference, in accordance with GaussianFormer<ref type="bibr" target="#b14">[15]</ref>. We report the latency of the initialization module in parentheses. Our method achieves better performance-efficiency trade-off.</figDesc><table><row><cell>Method</cell><cell>Number of Gaussians</cell><cell>Latency (ms)</cell><cell>Memory (MB)</cell><cell>mIoU IoU</cell></row><row><cell>Gaussian-</cell><cell>25600</cell><cell>227</cell><cell>4850</cell><cell>16.00 28.72</cell></row><row><cell>Former</cell><cell>144000</cell><cell>372</cell><cell>6229</cell><cell>19.10 29.83</cell></row><row><cell></cell><cell>6400</cell><cell>313 (142)</cell><cell>3026</cell><cell>19.87 30.37</cell></row><row><cell>Ours</cell><cell>12800</cell><cell>323 (143)</cell><cell>3041</cell><cell>19.94 30.37</cell></row><row><cell></cell><cell>25600</cell><cell>357 (147)</cell><cell>3063</cell><cell>20.33 31.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation on the components of GaussianFormer-2.</figDesc><table><row><cell cols="4">We set the number of Gaussians to 25600 for these experiments.</cell></row><row><cell cols="4">Depth means using depth as supervision in the initialization mod-</cell></row><row><cell cols="4">ule instead of occupancy distribution. Pointcloud represents using</cell></row><row><cell cols="2">ground truth LiDAR scan for initialization.</cell><cell></cell><cell></cell></row><row><cell>Probabilistic Modeling</cell><cell>Gaussian Initialization</cell><cell>mIoU</cell><cell>IoU</cell></row><row><cell></cell><cell></cell><cell>16.00</cell><cell>28.72</cell></row><row><cell>✓</cell><cell></cell><cell>19.61</cell><cell>30.61</cell></row><row><cell>✓</cell><cell>Depth</cell><cell>19.97</cell><cell>30.87</cell></row><row><cell>✓</cell><cell>Pointcloud</cell><cell>21.17</cell><cell>34.91</cell></row><row><cell>✓</cell><cell>Distribution</cell><cell>20.32</cell><cell>31.04</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On a measure of divergence between two statistical populations defined by their probability distribution</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Calcutta Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="99" to="110" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>In CVPR, 2020. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monoscene: Monocular 3d semantic scene completion</title>
		<author>
			<persName><forename type="first">Anh-Quan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2022. 1, 2, 3, 5, 6</date>
			<biblScope unit="page" from="3991" to="4001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scenerf: Selfsupervised monocular 3d scene reconstruction with radiance fields</title>
		<author>
			<persName><forename type="first">Anh-Quan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9387" to="9398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d sketch-aware semantic scene completion via semi-supervised structure prior</title>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d semantic scene completion from a single depth image using adversarial training</title>
		<author>
			<persName><forename type="first">Yueh-Tung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1835" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2-s3net: Attentive feature fusion with adaptive feature selection for sparse semantic segmentation network</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12547" to="12556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonghao</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senyao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10156</idno>
		<title level="m">Goal-oriented autonomous driving</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bevdet: High-performance multi-camera 3d object detection in bird-eye-view</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11790</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tri-perspective view for visionbased 3d semantic occupancy prediction</title>
		<author>
			<persName><forename type="first">Yuanhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2023. 1, 2, 3, 5, 6</date>
			<biblScope unit="page" from="9223" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selfocc: Self-supervised visionbased 3d occupancy prediction</title>
		<author>
			<persName><forename type="first">Yuanhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gaussianformer: Scene as gaussians for vision-based 3d semantic occupancy prediction</title>
		<author>
			<persName><forename type="first">Yuanhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17429</idno>
		<imprint>
			<date type="published" when="2009">2024. 1, 2, 3, 5, 6, 7, 8, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vad: Vectorized scene representation for efficient autonomous driving</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bencheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12077</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Symphonize 3d semantic scene completion with contextual instance queries</title>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15670</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anisotropic convolutional networks for 3d semantic scene completion</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yinhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinrong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10092</idno>
		<title level="m">Bevdepth: Acquisition of reliable depth for multi-view 3d object detection</title>
		<imprint>
			<date type="published" when="2005">2022. 1, 2, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sscbench: A large-scale 3d semantic scene completion benchmark for autonomous driving</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moonjun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09001</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Voxformer: Sparse voxel transformer for camera-based 3d semantic scene completion</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2023. 1, 2, 5, 6</date>
			<biblScope unit="page" from="9087" to="9098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bevformer: Learning bird&apos;s-eye-view representation from multi-camera images via spatiotemporal transformers</title>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonghao</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17270</idno>
		<imprint>
			<date type="published" when="2005">2022. 1, 2, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fb-Occ</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01492</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bevfusion: A simple and robust lidar-camera fusion framework</title>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10421" to="10434" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">KITTI-360: A novel dataset and benchmarks for urban scene un</title>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2009">2022. 2, 5, 6, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Venice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thi</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><surname>Ngoc Tho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhananjai</forename><surname>Widjaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang Jie</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><surname>Chong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04934</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bevfusion: Multi-task multi-sensor fusion with unified bird&apos;s-eye view representation</title>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><forename type="middle">L</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Octreeocc: Efficient and multi-granularity occupancy prediction using octree queries</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Occdepth: A depth-aware method for 3d semantic scene completion</title>
		<author>
			<persName><forename type="first">Ruihang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13540</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Atlas: End-to-end 3d scene reconstruction from posed images</title>
		<author>
			<persName><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarrence</forename><surname>Van As</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayan</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="414" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d</title>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lmscnet: Lightweight multiscale 3d semantic completion</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Roldão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ThreeDV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Occupancy as set of points</title>
		<author>
			<persName><forename type="first">Yiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="685" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sparseocc: Rethinking sparse latent representation for vision-based semantic occupancy prediction</title>
		<author>
			<persName><forename type="first">Pin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxuan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="15035" to="15044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14365</idno>
		<title level="m">Occ3d: A large-scale 3d occupancy prediction benchmark for autonomous driving</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene as occupancy</title>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonghao</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8406" to="8415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Opus: occupancy prediction using a sparse set</title>
		<author>
			<persName><forename type="first">Jiabao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liujiang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Lening</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20337</idno>
		<title level="m">Occsora: 4d occupancy generation models as world simulators for autonomous driving</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fcos3d: Fully convolutional one-stage monocular 3d object detection</title>
		<author>
			<persName><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Surroundocc: Multi-camera 3d occupancy prediction for autonomous driving</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2023. 1, 2, 5</date>
			<biblScope unit="page" from="21729" to="21740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Behind the scenes: Density fields for single view reconstruction</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wimbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9076" to="9086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruimao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3101" to="3109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihua</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Haiyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Remondino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.11356</idno>
		<title level="m">World model with self-supervised 3d label</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lidarmultinet: Towards a unified multi-task network for lidar perception</title>
		<author>
			<persName><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3231" to="3240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Maosheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Drinet++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08318</idno>
		<title level="m">Efficient voxel-aspoint point cloud segmentation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Flashocc: Fast and memory-efficient occupancy prediction via channel-to-height plugin</title>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyong</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangjie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongdai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangyong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12058</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Occformer: Dual-path transformer for vision-based 3d semantic occupancy prediction</title>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05316</idno>
		<imprint>
			<date type="published" when="2006">2023. 1, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Occworld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16038</idno>
		<title level="m">Learning a 3d occupancy world model for autonomous driving</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
