# GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction

## Abstract

## 

Figure 1. We approach efficient object-centric scene representation from a probabilistic perspective and propose the probabilistic Gaussian superposition model, which achieves SOTA performance with as few as 8.9% of Gaussians in GaussianFormer [15].

## Introduction

In autonomous driving, vision-centric systems have been more cost-effective compared with the LiDAR-based coun-* Project leader. terparts. However, their inability to capture obstacles with arbitrary shapes poses challenges for driving safety and reliability [[11,](#b10)[16,](#b15)[20,](#b19)[23]](#b22). The advent of 3D semantic occupancy prediction methods [[3,](#b2)[13,](#b12)[17,](#b16)[22,](#b21)[24,](#b23)[31,](#b31)[44,](#b44)[51]](#b51) alleviates this limitation by predicting the fine-grained geometry and semantics of the surrounding 3D environment. This advancement supports a range of emerging applications, including end-to-end autonomous driving [[11,](#b10)[40]](#b40), 4D occupancy forecasting [[42,](#b42)[47,](#b47)[52]](#b52), and self-supervised 3D scene understanding [[4,](#b3)[14,](#b13)[45]](#b45).

Despite the promising applications, 3D semantic occupancy prediction is essentially a dense three-dimensional segmentation task [[3,](#b2)[39]](#b39), which necessitates a both efficient and effective representation of the 3D scene. Voxelbased methods [[22,](#b21)[44]](#b44) use dense 3D voxels as representation to describe the scene with the finest detail. However, they neglect the spatial redundancy of the 3D occupancy and suffer from high computational complexity. As a workaround, planer representations, such as BEV [[23,](#b22)[50]](#b50) and TPV [[13]](#b12), compress the 3D grid along one of the axes to derive 2D feature maps for reduction of the token number. Nonetheless, they still take into account the empty region when modeling the environment, which compromises their model capacity and efficiency. As a pioneer in object-centric sparse scene representations, 3D semantic Gaussians [[15]](#b14) describe the 3D space in a sparse way with learnable mean, covariance, opacity and semantics for each Gaussian. However, several limitations persist in the current 3D semantic Gaussian representation: 1) Each Gaussian can still describe the empty region, which renders most of the Gaussians useless in an object-centric formulation given the sptial sparsity of 3D occupancy. 2) The aggregation process ignores the overlapping issue and directly sums up the contribution of each Gaussian to produce occupancy prediction, which results in unbounded semantic logits and further increases the overlapping among Gaussians. Thus, the proportion of effective Gaussians describing occupied regions independently could be extremely low, which undermines the efficiency of the 3D semantic Gaussian representation.

In this paper, we introduce a probabilistic Gaussian superposition model to resolve the above limitations of 3D semantic Gaussians and improve utilization and efficiency.

To elaborate, we propose the probabilistic Gaussian representation, which assigns 3D Gaussians to exclusively model the non-empty area by interpreting each Gaussian as a probability distribution of its neighborhood being occupied. We employ the multiplication theorem of probability to aggregate the independent probability distributions and derive the geometry predictions. Furthermore, we integrate the Gaussian mixture model into out probabilistic Gaussian representation to generate normalized semantic predictions, which avoid unbounded logits and prevents Gaussians from unnecessary overlapping. Since our representation only models the occupied region, we also design a distributionbased initialization module to effectively initialize Gaussians around the non-emtpy area, which learns the pixelaligned occupancy distribution instead of depth values of surfaces [[12,](#b11)[20,](#b19)[22]](#b21). We conduct extensive experiments on the nuScenes [[2]](#b1) and KITTI-360 [[26]](#b26) datasets for surroundview and monocular 3D semantic occupancy prediction, respectively. Our GaussianFormer-2 outperforms state-ofthe-art methods with high efficiency. In addition, qualitative visualizations show that GaussianFormer-2 is able to generate a both holistic and realistic perception of the scene.

## Related Work

3D Semantic Occupancy Prediction. 3D semantic occupancy prediction [[3,](#b2)[13,](#b12)[39]](#b39) has emerged as a promising environment modeling in autonomous driving as it describes driving scenes in a comprehensive manner. This task aims to label each voxel in the scene by taking one or more types of sensors as input. Two most used sensors are LiDAR and the camera. Although LiDAR-based methods perform remarkably well in 3D perception tasks [[3,](#b2)[5,](#b4)[6,](#b5)[8,](#b7)[18,](#b17)[19,](#b18)[27,](#b27)[34,](#b34)[37,](#b37)[46,](#b46)[48,](#b48)[49,](#b49)[53]](#b53), they possess limitations under bad weather conditions and in detecting distant objects; Thus, camera-based approaches have garnered increasing attention [[24,](#b23)[44,](#b44)[50]](#b50). Pioneer works in 3D semantic occupancy prediction task adopt dense grid-based representation as a straightforward mean to derive occupancy [[7,](#b6)[22,](#b21)[44]](#b44), then subsequent works turn to sparse object-centric representation [[15,](#b14)[30,](#b30)[38]](#b38) as a solution to the innate problem of redundancy for dense representations.

## Voxel-based

Plane-based Ours GaussianFormer represents elements devoted for emptiness [Figure 2](#). Representation comparisons. Voxel and plane based representations inevitably incorporate emptiness when modeling the 3D scene. While GaussianFormer [[15]](#b14) proposes 3D semantic Gaussian as a sparse representation, it still suffer from spatial redundancy. Our method achieves true object-centricity through probabilistic modeling.

Grid-based scene representations. Plane representations have emerged as competitive representations in scene perception tasks for autonomous driving. BEVFormer [[23]](#b22) is the initiative work of the kind [[12,](#b11)[20,](#b19)[25,](#b25)[28,](#b28)[33]](#b33) that utilizes only camera input and performs comparably well with LiDAR-based methods in detection and segmentation tasks. It converts the image feature to the bird's-eye-view (BEV) feature as a unified scene representation, since the information is most diverse at this point of view. The BEV feature is then used for downstream tasks. However, the BEV feature is not suitable for 3D occupancy construction as it causes height information to be lost [[44]](#b44). As a generalization of BEV space, TPVFormer [[13]](#b12) proposes tri-perspective view representation to include also the height information, thus making it more suitable for 3D scenes. Another research direction [[22,](#b21)[44]](#b44) adopts voxel-based representation as a more 3D-specific and fine-grained approach, making it favorable for 3D volumetric semantic prediction. Nevertheless, these methods utilize dense grid-based representation, which describes each voxel equally regardless of the spatial sparsity of the environment, thus resulting in intrinsic redundancy.

Object-centric scene representations. To eliminate spatial redundancy inherent in dense representations, many recent works adopt sparse representation [[15,](#b14)[30,](#b30)[38]](#b38). One line of work divides dense grids into partitions where objects present and omits the regions foreseen as empty [[30,](#b30)[38]](#b38). However, non-empty regions might be mistakenly clas- 

## GS. Encoder

## Prob. Modeling

Figure [3](#). Overall pipeline of our method. To achieve probabilistic modeling, we decompose occupancy prediction into geometry and semantics prediction, and approach them separately using probabilistic multiplication and Gaussian mixture model to improve efficiency. sified as unoccupied and eliminated completely from the whole subsequent process. Another line of work leverages point representation [[35,](#b35)[41]](#b41) by sampling points within the scene range as queries in the succeeding refinement process; Nevertheless, a point has a limited range of depiction as it has no spatial extent. An alternative approach, Gaussian-Former [[15]](#b14), adopts 3D semantic Gaussian representation, where probability spreads around a mean, allowing more utilization. However, spatial redundancy persists due to no regulation for the Gaussians to not represent emptiness.

## Proposed Approach

In this section, we present our method of probabilistic Gaussian superposition for efficient 3D semantic occupancy prediction. We first review the original 3D semantic Gaussian representation [[15]](#b14) and its limitations (Sec. 3.1). We then introduce our probabilistic Gaussian modeling and how we derive geometry and semantics predictions based on the multiplication theorem of probability and Gaussian mixture model (Sec. 3.2). Finally, we detail the distribution-based initialization module to effectively initialize probabilistic Gaussians around the occupied area (Sec. 3.3).

## 3D Semantic Gaussian Representation

Vision-centric 3D semantic occupancy prediction [[3,](#b2)[13]](#b12) aims to obtain the fine-grained geometry and semantics of the 3D scene. To formulate, the target is to predict voxellevel semantic segmentation result O ∈ C X×Y ×Z given input images I = {I i } N i=1 , where C, {X, Y, Z}, N represent the set of predefined classes, the spatial resolution of occupancy and the number of input views, respectively.

To achieve this, 3D semantic Gaussian representation employs a set of P Gaussian primitives G = {G i } P i=1 , with each G i describing a local region with its mean m i , scale s i , rotation r i , opacity a i and semantics c i . Gaussian-Former interprets these primitives as local semantic Gaussian distributions which contribute to the overall occupancy prediction through additive aggregation:

$ô(x; G) = P i=1 g i (x; m i , s i , r i , a i , c i ),(1)$where g i (x; •) denotes the contribution of the ith semantic Gaussian to ô(x; G) which is the overall occupancy prediction at location x. The contribution g is further calculated as the corresponding semantic Gaussian distribution evaluated at location x:

$g(x; G) = a • exp - 1 2 (x -m) T Σ -1 (x -m) c,(2)$$Σ = RSS T R T , S = diag(s), R = q2r(r),(3)$where Σ, R, S represent the covariance matrix, the rotation matrix constructed from the quaternion r with function q2r(•), and the diagonal scale matrix from function diag(•).

Although the number of Gaussians is reduced compared with the number of dense voxels thanks to the deformable nature of Gaussian distributions as in Eq. ( [2](#formula_1)), several limitations still persist in the 3D semantic Gaussian representation. First of all, it models both the occupied and unoccupied regions in the same way using the semantic property c, resulting in most Gaussians being classified as empty given the huge proportion of empty space in outdoor scenarios. Secondly, the semantic Gaussian representation encourages Gaussians to overlap, because the aggregation process in Eq. ( [1](#formula_0)) independently sums up the contribution of each Gaussian, resulting in unbounded occupancy prediction ô. For optimization, the model would learn to allocate more Gaussians to describe the same region due to the unbounded nature of ô, aggravating the overlap between Gaussians. These limitations stem from the current interpretation of Gaussians and obstruct the efficiency and effectiveness of the 3D semantic Gaussian representation. Our method approaches Gaussian-based object-centric representation from a probabilistic perspective, serving as a fundamental solution to these issues, as shown by Figure [2](#).

## Probabilistic Gaussian Superposition

We propose the probabilistic Gaussian superposition as an efficient and effective 3D scene representation. As shown in Figure [3](#), we decompose the 3D modeling target into geometry and semantics predictions, and adopt the multiplication theorem of probability and the Gaussian mixture model to address them from a probabilistic perspective, respectively.

Geometry prediction. To restrict Gaussians to represent only occupied regions for geometry prediction, we interpret the Gaussian primitives G = {G i } P i=1 as the probability of their surrounding space being occupied. To elaborate, we assign a probability value of 100% at the centers of Gaussians, which decays exponentially with respect to the distance from the centers m:

$α(x; G) = exp - 1 2 (x -m) T Σ -1 (x -m) ,(4)$where α(x; G) denotes the probability of the point x being occupied induced by Gaussian G. Eq. ( [4](#formula_3)) assigns a high probability of occupancy when the point x is close to the center of Gaussian G, which prevents any Gaussian from describing empty area. To further derive the overall probability of occupancy, we assume that the probabilities of a point being occupied by different Gaussians are mutually independent, and thus we can aggregate them according to the multiplication theorem of probability:

$α(x) = 1 - P i=1 1 -α(x; G i ) ,(5)$where α(x) represents the overall probability of occupancy at point x. In addition to achieving object-centric properties, Eq. ( [5](#formula_4)) also avoids unnecessary overlapping between Gaussians because α(x) ≥ α(x; G i ) holds for any Gaussian G i . This implies that point x would be predicted occupied if it is close enough to any single Gaussian. Semantics prediction. In addition to object-centric antioverlapping geometry modeling, we still need to achieve the same goals for semantics prediction. We first remove the channel that represents the empty class from the semantic properties c of Gaussians since it has been accounted for in geometry prediction. Then we interpret the set of Gaussians G as a Gaussian mixture model, where semantics prediction could be formulated as calculating the expectation of semantics given the probabilistic Gaussian mixture model. Specifically, we take the original opacity properties a as the prior distribution of Gaussians, which is l 1 -normalized. Furthermore, we adopt the Gaussian probabilistic distribution parameterized by mean m, scale s and rotation r as the conditional probability. Then we normalize the original semantics properties c with softmax to ensure the boundedness of predicted semantics. Finally, we calculate the expectation e(x; G) as:

$e(x; G) = P i=1 p(G i |x)c i = P i=1 p(x|G i )a i ci P j=1 p(x|G j )a j ,(6)$$p(x|Gi) = 1 (2π) 3 2 |Σ| 1 2 exp - 1 2 (x -m) T Σ -1 (x -m) ,(7)$where p(G i |x), p(x|G i ) and ci denote the posterior probability of point x belonging to the ith Gaussian distribution, the conditional probability of point x given the ith Gaussian distribution, and the softmax-normalized semantic properties, respectively. Compared with Eq. (1)(2), the gaussian mixture model in Eq. ( [6](#formula_5)) normalizes the semantic properties and the contributions from different Gaussians, thus preventing unnecessary overlapping between Gaussians and producing normalized class probabilities directly.

Given the geometry and semantics predictions, we take a simple step forward to combine them to generate the final semantic occupancy prediction:

$ô(x; G) = [1 -α(x); α(x) • e(x; G)],(8)$where we use the geometry probability α(x) to weight the semantic predictions, and directly take 1-α(x) as the probability of the empty class.

## Distribution-Based Initialization

Previous 3D semantic Gaussian representation adopts a learnable initialization strategy, which randomly initializes the properties of Gaussians at the beginning of training, and optimizes this initialization in a data-driven way. This strategy enables the model to learn a prior distribution of occupancy of the whole dataset, which relies on the subsequent refinement of the network to adapt to the distribution of each individual sample. However, the local receptive field of Gaussians limits their mobility, which hinders each Gaussian from learning the path to the correct position in subsequent refinement. And this issue is even more severe for our probabilistic Gaussian superposition where Gaussians are supposed to model only occupied regions.

To remedy this issue, we propose a distribution-based initialization module which provides both more accurate and holistic sample-specific initialization for Gaussians, as shown by Figure [4](#fig_1). We supervise the image features from a 2D backbone with the pixel-aligned occupancy distribution derived from the occupancy annotations. To elaborate, we first determine the origin b and direction d of the ray corresponding to each image feature with the camera calibration data. We then sample R reference points at equal intervals in a fixed depth range along this ray. For each of these reference points, we query the ground truth occupancy O at the corresponding location to obtain the binary labels l = {l i } R i=1 indicating whether a reference point is occupied or not. Then we use l = {l i } R i=1 as supervision to optimize our initialization module, which consists of an image backbone B and a distribution predictor M. The distribution predictor M directly decodes image features into occupancy distributions l along corresponding rays, which are matched against l using binary cross entropy loss:

$loss init = BCE l, l = BCE M(B(I)), l .(9)$Different from previous initialization schemes [[12,](#b11)[20,](#b19)[22]](#b21) that predict the depth values with LiDAR supervision, our method learns the holistic occupancy distribution rather than only visible surfaces of the scene, and does not require any additional modality as supervision. Overall, our distribution-based initialization module initializes the Gaussians, which are subsequently sent into B blocks of attention-based architecture as in Gaussian-Former [[15]](#b14). Each block consists of self-encoding, image cross-attention, and refinement module, where probabilistic Gaussian properties steadily improve, then the resulting Gaussians are aggregated by our new method that encourages higher utilization of Gaussians.

## Experiments

## Datasets and Metrics

The nuScenes dataset [[2]](#b1) provides 1000 scenes of surround view driving scenes in Boston and Singapore. The official division is 700/150/150 scenes for training, validation, and testing, respectively. Each scene is 20 seconds long and fully annotated at 2Hz with ground truth from 5 radars, 6 cameras, one LiDAR, and one IMU. We employ 3D semantic occupancy annotations from SurroundOcc [[44]](#b44) for supervision and evaluation. The ranges of the occupancy annotations in the x, y, and z axes in meters are [[-50, 50]](#), [[-50, 50]](#), and [-5, 3], respectively, where each voxel has a side length of 0.5 meters and is labeled as one of the 18 possible classes (16 semantics, 1 empty, and 1 noise class).

The KITTI-360 dataset [[26]](#b26) consists of over 320k images in suburban area with rich 360 degree sensory ground truth, consisting of 2 perspective cameras, 2 fisheye cameras, a Velodyne LiDAR, and a laser scanner, where we use the images from the left camera of the ego car as input to our model. For 3D semantic occupancy prediction, we adopt the annotations from SSCBench-KITTI-360 [[21]](#b20). The official split is 7/1/1 sequences with 8487/1812/2566 key frames for training, validation, and testing, respectively. The voxel grid area covers 51.2×51.2×6.4 m 2 in front of the ego car with resolution of 256×256×32. Each voxel is classified as one of the 19 classes (18 semantics and 1 empty).

The evaluation metrics are in accordance with common practice [[3]](#b2), namely mean Intersection-over-Union (mIoU) and Intersection-over-Union (IoU):

$mIoU = 1 |C ′ | i∈C ′ T P i T P i + F P i + F N i , (10$$)$$IoU = T P ̸ =c0 T P ̸ =c0 + F P ̸ =c0 + F N ̸ =c0 ,(11)$Where C ′ , c 0 , TP, FP, and FN represent the non-empty classes, the empty class, and the number of true positive, false positive, and false negative predictions, respectively.

## Implementation Details

The input images are at resolutions of 900×1600 for nuScenes [[2]](#b1) and 376x1408 for KITTI-360 [[26]](#b26) with random flipping and photometric distortion augmentations. We use the same checkpoints for our image backbone as used in GaussianFormer [[15]](#b14), i.e. ResNet101-DCN [[10]](#b9) with FCOS3D checkpoint [[43]](#b43) for nuScenes, and ResNet50 [[10]](#b9) pretrained on ImageNet [[9]](#b8) for KITTI-360. The numbers of Gaussians are set to 12800 and 38400 in our main results for nuScenes and KITTI-360, respectively. We train our model using AdamW [[29]](#b29) with weight decay of 0.01, and maximum learning rate of 2 × 10 -4 , which decays with a cosine annealing schedule. We train our model for 20 epochs on nuScenes with a batch of 8 and 30 epochs on KITTI-360 with a batch size of 4, respectively.

## Main Results

Surround-view 3D semantic occupancy prediction. We report the performance of our GaussianFormer-2 in Table [1](#tab_0).

Our approach achieves state-of-the-art performance compared with other methods. Specifically, GaussianFormer-2 surpasses methods based on dense grid representation in classes such as bicycle and motorcycle, proving the flexibility of the proposed probabilistic Gaussian superposition in modeling small objects. Furthermore, our method outperforms GaussianFormer [[15]](#b14) with a clear margin and significantly fewer Gaussians (12800 v.s. 144000), which validates the efficiency and effectiveness of our method. Monocular 3D semantic occupancy prediction. We report the results for monocular 3D semantic occupancy prediction on SSCBench-KITTI-360 [[21]](#b20) in Table [2](#tab_1). Our method achieves state-of-the-art performance, surpassing the original GaussianFormer in mIoU by 7.6%. To elaborate, we observe significant improvement in mIoU of classes such as road, sidewalk and building compared with GaussianFormer, showing the superiority of probabilistic Gaussian superposition in modeling background staff.

## Ablation Study

Number of Gaussians. We report the influence of the number of Gaussians on the efficiency and performance of our model in Table [3](#tab_2). Our model achieves better performanceefficiency trade-off compared with GaussianFormer, outperforming it with less than 5% number of Gaussians. The latency of our method is higher than GaussianFormer, which we attribute to the time-consuming farthest point sampling (FPS) operation in our initialization module. We adopt the divide-and-conquer strategy to conduct the FPS operation in a batched manner for acceleration, and report the latency of the initialization module in parentheses.

Design Choices. We conduct ablation study on the design choices of GaussianFormer-2 in Table [4](#tab_3). We observe consistent improvement for both probabilistic modeling and distribution-based initialization module which surpasses the depth-based counterpart with a clear margin.

Utilization of Gaussians. We provide comparisons on the utilization of Gaussians between GaussianFormer [[15]](#b14) and our method in Table [5](#) using two important factors that reflect the utilization of Gaussians, which are position and overlap. Percentage of Gaussians in correct positions (Perc.) is percentage of Gaussians with their mean positions in the occupied space. Overall overlap is calculated as summation of volumes of all Gaussians at 90% over the coverage volume of all Gaussians, while individual overlap is computed by the average of the summation of the Bhattacharyya coefficient of each Gaussian with any other Gaus-Table [5](#). Ablation on the efficiency of GaussianFormer-2. We set the number of Gaussians to 25600. Perc. and Dist. denote the percentage of Gaussians in correct positions, and the average distance of each Gaussian to its nearest occupied voxel, respectively. Overall and Indiv. represent the overall and individual overlapping ratios of Gaussians, respectively.

Method Position Overlap mIoU IoU Perc. (%) ↑ Dist. (m) ↓ Overall ↓ Indiv. ↓ GaussianFormer [15] 16.41 3.07 10.99 68.43 16.00 28.72 Ours 28.85 1.24 3.91 12.48 20.32 31.04

sians. We provide detailed information about these factors in the appendix. Our method outperforms GaussianFormer on all these metrics, demonstrating better utilization.

## Visualizations

We provide Gaussian and occupancy visualizations in Figure 5. Our model is able to predict reasonable Gaussian distributions and comprehensive occupancy results. Further, we compare our method against GaussianFormer in Figure 6. Our Gaussians are more adaptive in shape compared with isotropic spherical Gaussians in GaussianFormer. We also visualize the xy coordinates of Gaussians in the initialization and subsequent blocks of GaussianFormer-2 in Figure 7. We find that the Gaussians successfully learn to move towards occupied area thanks to the object-centric probabilistic design and effective initialization module. 3D Gaussians Occupancy Prediction Occupancy Ground Truth barrier construction vehicle motorcycle traffic cone trailer bicycle bus car pedestrian truck driveable surface other flat sidewalk terrain manmade vegetation GaussianFormer Ours GaussianFormer Ours

Figure 6. Comparison with GaussianFormer [[15]](#b14). Our method predicts 3D Gaussians with more adaptive shapes compared with GaussianFormer. Although our method uses only 8.8% Gaussians, it still generates comprehensive occupancy predictions and alleviates the elongated effect in GaussianFormer.

Figure [7](#). Visualizations of Gaussian positions in the refinement process. We observe that our probabilistic Gaussians equipped with distribution-based intialization successfully learn to move towards occupied regions.

## Conclusion

In this paper, we have proposed a probabilistic Gaussian superposition model as an efficient object-centric representation. Specifically, we interpret each Gaussian as a probability distribution of its neighborhood being occupied and adopt the multiplication theorem of probability to derive the geometry predictions. And we employ the Gaussian mixture model formulation to calculate normalized semantics predictions. We have also designed a distribution-based initialization strategy to effectively initialize Gaussians around occupied area for object-centric modeling according to pixel-aligned occupancy distribution. Our GaussianFormer-2 has achieved state-of-the-art performance on nuScenes and KITTI-360 datasets for 3D semantic occupancy prediction, which has also demonstrated improved efficiency compared with GaussianFormer on the number of Gaussians, position correctness and overlapping ratio. 

## A. Video Demonstration

Figure [8](#fig_3) shows a sampled frame of our video demonstration 1 for 3D semantic occupancy prediction on the nuScenes dataset [[2]](#b1). We note that the camera-view occupancy visualizations align well with the input RGB images. Moreover, each instance is sparsely described by only a few Gaussians with adaptive shapes, which demonstrates the efficiency and the object-centric nature of our model.

## B. Visualizations on KITTI-360

We provide visualization results of Gaussians and occupancy on the KITTI-360 dataset [[26]](#b26) in Figure [9](#fig_4). We observe that our GaussianFormer-2 is able to predict both intricate geometry and semantics of the 3D scene. Furthermore, the 3D Gaussians in our model are adaptive in their scales according to the specific objects they are describing, compared with isotropic spherical Gaussians with maximum scales in GaussianFormer [[15]](#b14).

## C. Metric Details

Position. Gaussians, even after full training, can still be found in unoccupied space due to the localized nature of the receptive field. These Gaussians fail to describe meaningful structures, rendering them ineffective and devoid of practical utility. A higher proportion of Gaussians in unoccupied space indicates suboptimal utilization. Hence, we define the 1 [https://github.com/huang-yh/GaussianFormer](https://github.com/huang-yh/GaussianFormer) percentage of Gaussians in correct positions (Perc.) as:

$Perc. = N correct N total • 100%,(12)$where N correct , and N total denote the number of Gaussians of which means are in occupied space, and the total number of Gaussians, respectively. A higher percentage indicates a better alignment of the Gaussians with occupied or meaningful area in the space, thus reflecting a more efficient use of the model's capacity.

The above measurement provides a hard evaluation, where Gaussians are either classified as being in correct or incorrect positions without considering their proximity to the nearest occupied area. This binary approach does not capture how close Gaussians in unoccupied regions are to meaningful positions. To address this limitation, we define a complementary soft measurement as the average distance of each Gaussian to its nearest occupied voxel center, denoted as Dist. (in meters), computed as follows:

$Dist. = 1 P P i=1 min v∈V ||m i -v|| 1 ,(13)$where m i , V, v, and ||m i -v|| 1 denote the mean of the i-th Gaussian, the set of occupied voxel centers, the center of one voxel in this set, and L1 distance between the mean of the Gaussian and the voxel center, respectively. Note that this distance is calculated with respect to the voxel center, and thus Gaussians positioned within the correct occupied area may also have a non-zero distance.

barrier pole other-structure other-object traffic-sign terrain building vegetation motorcycle bicycle car person truck road other-ground sidewalk other-vehicle parking Input Image 3D Gaussians Pred. Occupancy Occupancy G.T. Overlap. The overall overlapping ratio of Gaussians (Overall.) provides a global perspective on the redundancy in the Gaussian representation. Each Gaussian is modeled as an ellipsoid, where the semi-axis lengths are derived from the Mahalanobis distance at a chi-squared value of 6.251, corresponding to the 90% confidence level of a Gaussian distribution in three degrees of freedom (DoFs). The Overall. is then calculated as the ratio of the summed 90% confidence volumes V i,90% of all Gaussians to the total coverage volume of all Gaussians V coverage in the scene:

$Overall. = P i=1 V i,90% V coverage ,(14)$where V coverage represents the volume of all Gaussians combined as a unified shape. To estimate V coverage , we employ the Monte Carlo method where a large number of points are randomly sampled within the bounding box of the scene.

For each sampled point, we check whether it lies within the 90% confidence ellipsoid of any Gaussian. The volume is then approximated as:

$V coverage = V scene • N in N total ,(15)$where N in , and N total are the number of sampled points that fall within the 90% confidence ellipsoid of at least one Gaussian, and the total number of sampled points, respectively. This approach ensures an accurate estimation of the unified volume, efficiently handling the overlapping regions of the Gaussians by not double-counting them. We next detail the derivation of the ellipsoid volume cor-responding to the 90% confidence region of a 3D Gaussian distribution. Considering a multivariate Gaussian distribution in 3D defined as:

$g(x) = 1 (2π) 3/2 |Σ| 1/2 exp - 1 2 (x -m) T Σ -1 (x -m) ,(16)$where x, Σ, and |Σ| are the mean vector, 3x3 covariance matrix, and the determinant of the covariance matrix, respectively. The Mahalanobis distance d of point x from the mean m is defined as:

$d 2 (x, m) = (x -m) T Σ -1 (x -m).(17)$The 90% confidence region of the Gaussian distribution corresponds to the set of points for which the Mahalanobis distance satisfies:

$d 2 ≤ χ 2 3,0.9 ≈ 6.251,(18)$where χ 2 3,0.9 is the chi-square critical value for three degrees of freedom at the 90% confidence level. For a Gaussian distribution, the semi-axis lengths are determined by the square root of the eigenvalues of Σ, scaled by χ 2 3,0.9 . Thus, the volume of the ellipsoid from 90% of the 3D Gaussian distribution is:

$V 90% = 4 3 π(6.251) 3/2 |Σ| 1/2 .(19)$A higher value of Overall. indicates greater overlapping volumes among the Gaussians, signifying redundancy in Gaussian representation. This metric provides insights into the utilization of Gaussians to represent the scene.

The individual overlapping ratio of Gaussians (Indiv.) offers a fine-grained analysis of the overlap between Gaussians in a scene. This measurement quantifies the degree to which each Gaussian overlaps with all other Gaussians, averaged across all Gaussians in the scene. The value of this metric indicates approximately how many times the volume of a single Gaussian is fully overlapped with other Gaussians on average. To compute this, we use the Bhattacharyya coefficient [[1]](#b0), which measures the similarity between two Gaussian distributions. The individual overlapping ratio is defined as:

$Indiv. = 1 P P i=1   j̸ =i BC i,j   ,(20)$where BC i,j is the Bhattacharyya coefficient between the i-th and j-th Gaussians, given by:

$BC i,j = 4 |Σ i ||Σ j | |Σ ij | e -1 8 (mi-mj ) T Σ -1 ij (mi-mj ) ,(21)$where Σ ij = 1 2 (Σ i + Σ j ) is the average covariance matrix. A higher value of Indiv. indicates more redundancy, as Gaussians are heavily overlapping with each other.

![Figure 4. Distribution-based initialization. Our initialization scheme learns pixel-aligned occupancy distributions from occupancy annotation, while the depth-based counterpart only captures the surfaces of objects and relies on LiDAR supervision.]()

![Figure 5. Gaussian and occupancy visualizations on nuScenes. Our model is able to predict both comprehensive and realistic 3D Gaussians and occupancy.]()

![Figure 8. Visualizations of Gaussians, camera-view and overall occupancy on nuScenes. We provide the input RGB images and their corresponding camera-view occupancy in the upper part. And we visualize the predicted 3D Gausians (left), the semantic occupancy in the global view (middle), and in the bird's eye view (right) in the lower part.]()

![Figure 9. Visualizations of Gaussians and occupancy on KITTI-360. Our method captures both the intricate geometry and semantics of the scene with shape-adaptive Gaussians.]()

![Surround view 3D semantic occupancy prediction results on nuScenes. * means supervised by dense occupancy annotations as opposed to original LiDAR segmentation labels. Ch. denotes the channel dimension of our model. Our method achieves state-of-the-art performance compared with other methods..75 14.22 6.58 23.46 28.28 8.66 10.77 6.64 4.05 11.20 17.78 37.28 18.00 22.88 22.17 13.80 22.21 TPVFormer [13] 11.51 11.66 16.14 7.17 22.63 17.13 8.83 11.39 10.46 8.23 9.43 17.02 8.07 13.64 13.85 10.34 4.90 7.37 TPVFormer* [13] 30.86 17.10 15.96 5.31 23.86 27.32 9.79 8.74 7.09 5.20 10.97 19.22 38.87 21.25 24.26 23.15 11.73 20.81 OccFormer [51] 31.39 19.03 18.65 10.41 23.92 30.29 10.31 14.19 13.59 10.13 12.49 20.77 38.78 19.79 24.19 22.21 13.48 21.35 SurroundOcc [44] 31.49 20.30 20.59 11.68 28.06 30.86 10.70 15.14 14.09 12.06 14.38 22.26 37.29 23.70 24.49 22.77 14.89 21.86 GaussianFormer [15] 29.83 19.10 19.52 11.26 26.11 29.78 10.47 13.83 12.58 8.67 12.74 21.57 39.63 23.28 24.46 22.99 9.59 19.12 Ours (Ch. = 128) 30.56 20.02 20.15 12.99 27.61 30.23 11.19 15.31 12.64 9.63 13.31 22.26 39.68 23.47 25.62 23.20 12.25 20.73 Ours (Ch. = 192) 31.74 20.82 21.39 13.44 28.49 30.82 10.92 15.84 13.55 10.53 14.04 22.92 40.61 24.36 26.08 24.27 13.83 21.98]()

![Monocular 3D semantic occupancy prediction results on SSCBench-KITTI-360. Our method achieves state-of-the-art performance compared with other methods, surpassing GaussianFormer[15] by a clear margin. 40.27 13.81 22.58 0.66 0.26 9.89 3.82 2.77 54.30 13.44 31.53 3.55 36.42 4.80 31.00 19.51 7.77 8.51 6.95 4.60 GaussianFormer [15] C 35.38 12.92 18.93 1.02 4.62 18.07 7.59 3.35 45.47 10.89 25.03 5.32 28.44 5.68 29.54 8.62 2.99 2.32 9.51 5.14 Ours C 38.37 13.90 21.08 2.55 4.21 12.41 5.73 1.59 54.12 11.04 32.31 3.34 32.01 4.98 28.94 17.33 3.57 5.48 5.88 3.54]()

![Ablation on the number of Gaussians. The latency and memory are tested on an NVIDIA 4090 GPU with batch size one during inference, in accordance with GaussianFormer[15]. We report the latency of the initialization module in parentheses. Our method achieves better performance-efficiency trade-off.]()

![Ablation on the components of GaussianFormer-2.]()

