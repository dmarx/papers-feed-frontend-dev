- Decision to compare SFT and RL as post-training techniques
- Choice of GeneralPoints and V-IRL as evaluation tasks
- Selection of arithmetic reasoning and visual navigation as focus areas
- Adoption of a multi-step RL framework after SFT
- Implementation of outcome-based reward functions in RL
- Use of sequential revision formulation for state-action transitions
- Decision to analyze generalization in both textual and visual domains
- Choice to scale up inference-time compute for improved performance
- Decision to investigate the impact of SFT on RL training stability
- Selection of specific metrics for evaluating generalization and memorization
- Decision to conduct experiments in both unimodal and multimodal settings
- Choice of visual encoders and data curation strategies for VLMs
- Decision to analyze the role of inference-time verification in RL generalization
- Choice to focus on rule-based and visual variant generalization
- Decision to document findings on the limitations of SFT in generalization
- Selection of specific architectures or models for experimentation
- Decision to include comparative analysis with prior works in the field
- Choice of data sources and preprocessing methods for training and evaluation
- Decision to publish findings in a collaborative format across institutions