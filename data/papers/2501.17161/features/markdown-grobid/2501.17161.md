# SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training

## Abstract

## 

Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their respective role in enhancing model generalization remains unclear. This paper studies the comparative effect of SFT and RL on generalization and memorization, focusing on text-based and visual environments. We introduce GeneralPoints, an arithmetic reasoning card game, and also consider V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes in both the rule-based textual and visual environments. SFT, in contrast, tends to memorize the training data and struggles to generalize out-of-distribution in either scenario. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in visual domains. Despite RL's superior generalization, we show that SFT is still helpful for effective RL training: SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrate the advantage of RL for acquiring generalizable knowledge in complex, multimodal tasks. * Equal contribution. ♠ HKU, ♥ UC Berkeley, ♣ Google Deep-Mind, ♦ NYU. All experiments are conducted outside of Google.

## Introduction

While SFT and RL are both widely used for foundation model training [(OpenAI, 2023b;](#)[Google, 2023;](#b18)[Jaech et al., 2024;](#b22)[DeepSeekAI et al., 2025)](#b15), their distinct effects on generalization [(Bousquet & Elisseeff, 2000;](#b7)[Zhang et al., 2021)](#b61) remain unclear, which makes it challenging to build reliable and robust AI systems. A key challenge in analyzing the generalization ability of foundation models [(Bommasani et al., 2021;](#b6)[Brown et al., 2020)](#b8) is separating data memorization[foot_0](#foot_0) from the acquisition of transferable principles. We therefore investigate the key question of whether SFT or RL primarily memorize the training data (Allen- [Zhu & Li, 2023a;](#b3)[Ye et al., 2024;](#b55)[Kang et al., 2024)](#b24), or whether they learn generalizable principles that can adapt to novel task variants.

To address this question, we focus on two aspects of generalization: textual rule-based generalization and visual generalization. For textual rules, we study a model's ability to apply learned rules (given text instructions) to variants of those rules [(Zhu et al., 2023;](#b67)[Yao et al., 2024;](#b54)[Ye et al., 2024)](#b55). For vision-language models (VLMs), visual generalization measures performance consistency to variations in visual input, such as color and spatial layout, within a given task. For studying text-based and visual generalization, we investigate two different tasks that embody rule-based and visual variants. Our first task is GeneralPoints, an original card game task that is similar to the Points24 task from RL4VLM [(Zhai et al., 2024a)](#), which is designed to evaluate a model's arithmetic reasoning capabilities. In GeneralPoints, the model receives four cards (presented as a text description or an image), and is required to compute a target number (24 by default) using each card's numerical value exactly once. Second, we adopt V-IRL [(Yang et al., 2024a)](#b51), a real-world navigation task, that focuses on the model's spatial reasoning capabilities.

We adopt a multi-step RL framework similar to [Zhai et al. (2024a)](#), by instantiating RL after running SFT on the backbone model [(Dubey et al., 2024)](#b16), using the sequential revision formulation [(Snell et al., 2024)](#b39). In both GeneralPoints and V-IRL, we observe that RL learns generalizable rules (expressed in text), where in-distribution performance gains also transfer to unseen rules. In contrast, SFT appears to memorize the training rules and fails to generalize (see Figure [1](#fig_0) for an example).

Beyond textual rule-based generalization, we further investigate generalization in the visual domain and observe that RL also generalizes to visual OOD tasks, whereas SFT continues to struggle. As a by-product of the visual OOD generalization capability, our multi-turn RL approach achieves state-of-the-art performance on the V-IRL mini benchmark, by +33.8% (44.0%→77.8%) [(Yang et al., 2024a)](#b51), highlighting the generalization capability of RL. To understand how RL impacts a model's visual abilities, we conduct additional analysis on GeneralPoints, revealing that training RL with an outcome-based reward function [(Cobbe et al., 2021)](#b14) improves visual recognition capabilities. While RL exhibits superior generalization compared to SFT, we show that SFT is still helpful for stabilizing the model's output format, enabling RL to achieve its performance gains. Last but not least, we observe that scaling up the inference time compute by increasing the number of maximal steps leads to better generalization.

## Related Works

Post-training. Post-training is crucial for enhancing model performance [(Zhang et al., 2022;](#b63)[Hoffmann et al., 2023;](#b20)[OpenAI, 2023b;](#)[Google, 2023;](#b18)[Touvron et al., 2023)](#b47). This stage commonly utilizes large-scale supervised finetuning (SFT) [(Radford et al., 2018;](#b33)[Brown et al., 2020;](#b8)[Radford et al., 2021;](#)[Wei et al., 2022a;](#)[Chung et al., 2022;](#b13)[Zhou et al., 2024a)](#b65) and/or reinforcement learning (RL) [(Ziegler et al., 2019;](#b68)[Ouyang et al., 2022;](#)[Sun et al., 2024;](#b40)[Abdulhai et al., 2023;](#b0)[Zhou et al., 2024b;](#b66)[Zhai et al., 2024a)](#). SFT adapts pre-trained models to downstream tasks by training them on task-specific, often instructionformatted datasets. Previous work, such as FLAN [(Wei et al., 2022a)](#), demonstrates that fine-tuning on diverse instruction-tuning datasets significantly enhances zero-shot performance on unseen tasks. Furthermore, LIMA [(Zhou et al., 2024a)](#b65) shows that supervised fine-tuning acts as a "format teacher" effectively adapting the model's responses to a desired format while leveraging the capabilities of pre-trained LLMs. In contrast, RL [(Ziegler et al., 2019;](#b68)[Ouyang et al., 2022;](#)[Sun et al., 2024;](#b40)[Ramamurthy et al., 2023;](#b36)[Abdulhai et al., 2023;](#b0)[Zhou et al., 2024b;](#b66)[Zhai et al., 2024a)](#) has been primarily used to align models with human preferences or training the foundational model to solve a specific task [(Abdulhai et al., 2023;](#b0)[Zhou et al., 2024b;](#b66)[Zhai et al., 2024a;](#)[Chen et al., 2024b](#b11)). Our work differs from prior studies, as we aim to comparatively analyze the generalization and memorization of SFT and RL on both LLM and VLM, while previous studies have focused primarily on only one of these two post-training methods (or only study LLM or VLM) or on only one posttraining method.

Memorization and generalization in LLM/VLM. Several studies have examined the interplay between memorization and generalization in neural networks [(Han et al., 2022;](#b19)[Carlini et al., 2022;](#b9)[Yang et al., 2023)](#b53). In LLMs, memorization can manifest as the model memorizing the training data [(Carlini et al., 2022;](#b9)[Jiang et al., 2024;](#b23)[Kang et al., 2024)](#b24), while generalization reflects the divergence between the model's output distribution and the pre-training data distribution [(Zhang et al., 2023)](#b62). Prior studies suggest that LLMs exhibit more overfitting on simpler, knowledge-intensive tasks and greater generalization on more complex, reasoning-intensive ones [(Wang et al., 2024;](#b48)[Qi et al., 2024)](#b32). For example, recent studies [(Ye et al., 2024;](#b55)[Allen-Zhu, 2024;](#b2)[Allen-Zhu & Li, 2023a;](#b3)[b;](#)[2024;](#b2)[Tong et al., 2024b)](#) have demonstrated that LLMs develop reasoning skill sets beyond their training data by pre-computing reasoning graphs before autoregressive generation, which provides compelling evidence of generalization. Our study takes a different approach by investigating the role of different post-training paradigms on memorization versus generalization in the context of textual ruledbased and visual variants. We conduct comparative studies in both unimodal (LLM) and multimodal (VLM) settings, and demonstrate that RL leads to better generalization performance than SFT.

Scaling up inference-time compute. Recent research has increasingly focused on scaling up inference-time computation to improve model performance [(Wei et al., 2022b;](#b50)[Yao et al., 2024;](#b54)[Snell et al., 2024;](#b39)[Jaech et al., 2024)](#b22).

Early studies [(Wei et al., 2022b;](#b50)[Yao et al., 2024)](#b54) prompted models to generate intermediate reasoning steps and extend the responses before producing a final answer. Subsequent work [(Zelikman et al., 2022;](#b58)[Feng et al., 2023;](#b17)[Tian et al., 2024;](#b42)[Chen et al., 2024a;](#b10)[Snell et al., 2024)](#b39) has demonstrated that fine-tuning verifiers during inference improves model accuracy, effectively utilizing test-time computation. Notably, recent findings [(Jaech et al., 2024;](#b22)[DeepSeekAI et al., 2025)](#b15) reveal "scaling laws" for inference-time compute, highlighting significant performance gains with increased computational resources. Our work builds upon these findings in two ways. First, we integrate insights from inference-time verification into a multi-turn RL formulation that allows the model to identify and correct its errors. Second, we examine the impact of inference-time verification on RL generalization, demonstrating that scaling up inference-time verification (in terms of the maximum number of verification steps) is a key for RL to generalize.

Improving visual capability in VLMs. While VLMs have demonstrated remarkable skill across a wide range of challenging tasks, such as solving advanced college exam questions [(Lu et al., 2023;](#b28)[Yue et al., 2024a;](#)[b)](#) and spatial understanding tasks [(Yang et al., 2024a;](#b51)[b)](#), they also exhibit limitations in visual perception [(Zhai et al., 2024a;](#)[b;](#)[Tong et al., 2024c;](#b45)[d;](#b18)[Rahmanzadehgervi et al., 2024)](#b35). Prior efforts to enhance VLMs' visual perception include combining multiple visual encoders [(Tong et al., 2024d;](#b46)[Kar et al., 2025;](#b25)[Tong et al., 2024a)](#), curating high-quality SFT data [(Chen et al., 2023;](#b12)[Liu et al., 2024;](#b27)[Tong et al., 2024a)](#), and improving the SFT training recipe by unfreezing the visual backbone [(Liu et al., 2023;](#b26)[Tong et al., 2024a)](#). While these prior works primarily focus on experiments during the SFT stage, our work demonstrates that RL can also improve visual perception.

## Preliminaries

Standard RL terminology. We consider finite horizon decision making, and adopt standard notation from the classical RL literature [(Sutton & Barto, 2018;](#b41)[Agarwal et al., 2019)](#b1), where S denotes the state space, A denotes the action space, r : S × A → R denotes the reward function, and T denotes the maximum number of steps per episode. The goal is to learn a policy π : S → A that maximizes the overall return max π∈Π E π T t=0 r t , where r t denotes r(s t , a t ). Without loss of generality, we use π(a|s) ∈ [0, 1] to denote probability of π choosing a at s.

Adapting RL terminology to LLM/VLM with a verifier. We adopt a multi-turn RL setting for foundation model training [(Zhai et al., 2024a)](#). Let V represent the discrete and finite vocabulary (token) space. The input and output text spaces are denoted by V m and V n respectively, where m and n are the maximum token length of the input sequence v in and output sequence v out . For models requiring visual inputs (VLM), we define O as the space of all RGB images. The state space, denoted by S, is defined as S := V m ×O for VLM, and S := V m for LLM. The action space A is defined as A := V n . We use VER : V n → R × V k to denote a verifier, which evaluates the outcome of v out and generates an outcome-based reward function [(Cobbe et al., 2021;](#b14)[Hosseini et al., 2024;](#b21)[Snell et al., 2024;](#b39)[Setlur et al., 2024)](#b38) r along with textual information v ver . Mathematically, at time t, VER(v out t ) → (r t , v ver t ). Similar to [Zhai et al. (2024a)](#), we treat the model with parameter θ as our policy network π θ : S → V n , and adopt PPO [(Schulman et al., 2017)](#b37) as the backbone RL algorithm for updating π θ .

Sequential revision. For modeling the state-action transition, we adopt the sequential revision formulation [(Snell et al., 2024)](#b39). Specifically, at time step t = 0 the initial input v in 0 consists of the system prompt. For subsequent time steps (t ≥ 1), the input prompt v in t comprises the system prompt concatenated with all prior model and verifier outputs, denoted by

$[v out k , v ver k ] t-1$k=0 . An illustration of the sequential revision is provided in Figure [2](#fig_2) (also see Figure [5](#) of [Snell et al. (2024))](#b39), and an example of the state-action transition is shown in Figure [3](#).

## Evaluation Tasks

To evaluate the generalization of different post-training methods, we select two tasks that each offer rule and visual variations. The first task, GeneralPoints, is a new environment we have designed that allows assessment of arithmetic reasoning abilities (Section 4.1). The second task, V-IRL [(Yang et al., 2024a)](#b51), is chosen to examine the model's reasoning capabilities in an open-world visual navigation domain (Section 4.2).

## The General Points Environment

Our original GeneralPoints environment, instantiated on top of the Points24 environment [(Zhai et al., 2024a)](#), is designed to evaluate generalization of arithmetic reasoning. Each state s of the environment contains 4 cards, described as text (in the GP-L variant) or presented as an image (in the GP-VL variant); see Figure 2 left for a visual example of GeneralPoints. The goal is to produce an equation that equals a target number (24 by default) using all 4 numbers from the cards exactly once. Detailed examples of the state-action transitions are provided in Appendix A.2. Note that when input from GeneralPoints is presented in an image (GP-VL), it naturally introduces additional visual challenges requiring the VLM to recognize all cards before solving the equation.

## Rule variations.

To study whether the model learns arithmetic operations or simply memorizes the post-training data, we introduce rule variations in GeneralPoints. These variations consist of interpreting the symbols 'J', ['Q', and 'K' either as '11', '12', and '13', respectively](#), or all as the same number '10'. These variations ensure a rigorous evaluation of the model's ability to generalize arithmetic reasoning across diverse settings. Each rule is specified as text in the input prompt,  System Prompt (v in 0 ) [Task Description] You are an expert in {task name}, you are observing {purely language/vision-language inputs + <image>}. You are currently at {state related info}. Please follow {tasks rules}.

[Output] Your response should be a valid json file in the following format: {task related information and answer}

## Appending previous model and verifier outputs to obtain

$v in t v in t = [v out 0 , v ver 0 , v out 1 , v ver 1 , . . . , v out t-1 , v ver t-1 ] ▷ v in t = concat v in 0 , [v out k , v ver k ] t-1 k=0$Model output (v out t ) and Verifier Output (v ver t ) {Task related json outputs}, {You success/fail}.

$▷ v in t+1 = concat(v in t , v out t , v ver t )$Figure [3](#): An template of our prompt update for constructing v in t+1 . The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively. see the {tasks rules} part in Figure [3](#). For studying ruled based generalization, we post-train the model using one rule, then evaluate using a different rule.

Visual variations. The GeneralPoints environment can also be naturally customized to evaluate generalization across visual variants. Since the major visual challenge is to recognize the number of each card, agnostic to the the color of the cards, we consider the cards with different colors as visual variants of the task. In the visual generalization setting, we train the model using cards of one color, then test OOD performance using the other color.

## The V-IRL Environment

While the GeneralPoints environment is designed to assess arithmetic reasoning abilities, we further utilize the V-IRL environment [(Yang et al., 2024a)](#b51) to study spatial reasoning ability in an open-world navigation domain that uses realistic visual input. As in GeneralPoints we consider two versions of the environment, one (V-IRL-L) that consists of pure language descriptions,[foot_1](#foot_1) and another (V-IRL-VL) that includes vision-language input. The major visual challenge in V-IRL involves recognizing differ-ent landmarks from the visual observation[foot_2](#foot_2) before taking an action. The goal is to navigate to a target location by following a set of instructions that contain spatial information. A detailed example of one environment step is shown in Appendix B.2.

## Rule variations.

To evaluate whether the model possesses spatial knowledge or simply memorizes posttraining data, we consider two distinct action space configurations. The first variant utilizes an absolute orientation action space, which includes {'north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest'}. The second variant employs a relative orientation action space, containing {'left', 'right', 'slightly left', 'slightly right'}. This relative configuration adjusts the current orientation by 90 degrees or 45 degrees to the left or right, respectively. An overview of a navigation task in V-IRL is provided in Figure [4](#fig_4), and a detailed state-action transition in V-IRL is provided in Figure [13](#fig_0) (in Appendix B.2).

## Visual variations.

The key visual challenge in V-IRL is to recognize landmarks from the visual observations (e.g., the green parts in Figure [4](#fig_4)). Since the V-IRL environment contains visual observations from different cities, we  

## Results

In this section, we present experiments that investigate the generalization abilities induced by post-training with RL and SFT. We adopt Llama-3.2-Vision-11B [(Dubey et al., 2024)](#b16) as the backbone model. Following the standard pipelines of RLHF [(Ouyang et al., 2022)](#) and RL4VLM [(Zhai et al., 2024a)](#), we initialize the model with SFT before running RL. We specifically study the following questions. Section 5.1: how does SFT or RL affect the model's generalization to different rules? Section 5.2: when the model contains a visual component, how does RL/SFT affect its generalization to different visual variants? Section 5.3: how does RL/SFT affect visual recognition capability in a VLM? Section 5.4: what role does SFT play in RL training? Section 5.5: how does the number of verification iterations affect generalization?

## Generalization across Rules

We evaluate the performance of different post-training methods on GeneralPoints and V-IRL, each of which has a pure language (-L) and a vision-language (-VL) variant, and each encompassing rule variations. For each task, we separately scale the training compute for RL and SFT on a single rule. We consider the results on the trained rule as in-distribution (ID) performance, whereas results on the unseen rules measures out-of-distribution (OOD) generalization. In GeneralPoints, the ID case treats all 'J', 'Q', 'K' as 10, and the OOD cases interprets them as 11, 12, and 13. As for V-IRL, the ID case adopts the absolute orientation coordinate system and the OOD case uses the relative orientation action space. Other details and additional experimental setup can be found in Appendix C.

RL generalizes, SFT memorizes. As illustrated in Figure [5](#), RL consistently improves OOD performance on all tasks, including both unimodal (LLM) and multimodal (VLM). Specifically, Figure [6](#) demonstrates that RL achieves an increase of +3.5% on GP-L (11.5% → 15.0%) and +11.0% on V-IRL-L (80.8% → 91.8%).

Even with the additional challenge of visual recognition in the VLM, RL maintains consistent performance improvements of +3.0% (11.2% → 14.2%) on GP-VL and +9.3% (35.7% → 45.0%) on V-IRL-VL, respectively. In contrast, SFT consistently exhibits performance degradation across all OOD evaluations on all tasks: -8.1% on GP-L (11.5% → 3.4%), -79.5% on V-IRL-L (80.8% → 1.3%), -5.6% (11.2% → 5.6%) on GP-VL, and -33.2% (35.7% → 2.5%) on V-IRL-VL.

## Generalization in Visual Out-of-Distribution Tasks

Section 5.1 demonstrates that RL yields generalization across rule variations, whereas SFT exhibits the opposite trend. Since VLMs also incorporate a visual modality, we next study the effects of visual variation in OOD generalization. For GeneralPoints, we train the VLM using the black suits (♠, ♣) and test out-of-distribution performance on the red suits (♥, ♦). For V-IRL, we train the model on routes collected in New York City and evaluate it on the original V-IRL VLN mini benchmark [(Yang et al., 2024a)](#b51) containing routes from various cities worldwide (see Appendix B.1 for details). Note that the rules remain consistent across experiments in this section. RL generalizes in visual OOD tasks. As shown in Figure 7, we observe that RL still generalizes in visual OOD tasks, while SFT continues to suffer. Specifically, in GP-VL and VIRL-VL, RL achieves performance improvements of +17.6% (23.6% → 41.2%), +61.1% (16.7% → 77.8%), whereas SFT suffers from performance decreases of -9.9% (23.6% → 13.7%) and -5.6% (16.7% → 11.1%).

As a byproduct of this visual OOD study, we also show that our multi-turn RL formulation improves the state-ofthe-art results (see Table [5](#) of [Yang et al. (2024a)](#b51)) on the V-IRL mini benchmark by +33.8% (44.0% → 77.8%). Notably, unlike the previous state-of-the-art approach reported in V-IRL, which relies on a two stage VLM-LLM collaboration technique and tailored prompt engineering on closed-sourced model (OpenAI, 2023a), our end-to-end RL approach enables an open-sourced model [(Dubey et al., 2024)](#b16) to reach superior performance.

## RL Improves Visual Capabilities

Building upon the above observation that VLMs trained with RL generalize to visual OOD tasks (Section 5. Scaling RL improves visual recognition accuracy in VLM training. As shown in Figure [8](#), we observe that the VLM's visual recognition accuracy largely affects the overall performance, which was similarly observed in [Zhong et al. (2024)](#b64). In addition, scaling up RL compute also improves visual recognition accuracy, as a byproduct of its generalization capability, while scaling SFT deteriorates both visual recognition accuracy and overall performance. Additional experimental results are provided in Figures 16 and 17 of Appendix D.1.

## The Role of SFT for RL Training

Despite the superiority of RL in generalizing the model's reasoning and visual capabilities, as discussed previously, the experimental pipeline still instantiates RL after SFT.

In this subsection, we focus on another key question: Is SFT necessary for RL training? To answer this question, we conduct additional experiments that directly apply endto-end RL to post-train the base model Llama3.2 using GeneralPoints in the purely language case (Figure [9](#fig_8)). SFT is necessary for RL training when the backbone model does not follow instructions. Figure [9](#fig_8) shows that without SFT, all end-to-end RL runs fail to improve. More specifically, we observe that without SFT, the base model suffers from poor instruction following capability.

A detailed failure case is provided in Figure [20](#fig_2) (in Appendix D.3), revealing that the base Llama-3.2-Vision-11B model tends to generate long, tangential, and unstructured responses. This issue makes it impossible to retrieve taskrelated information and rewards for RL training. Note that due to the difference in backbone model, our results do not contradict with DeepSeekAI et al. ( [2025](#)), which suggests that SFT is unnecessary for downstream RL training.

## Role of Verification Iterations

Verification serves as another crucial component in our multi-step training and evaluation pipeline (see Figures [2](#fig_2) and [3](#)). To validate its necessity and better understand its effect, we conduct RL experiments with different verification iterations {1, 3, 5, 10} using GP-L (Figure [10](#fig_9)).

## Scaling up verification improves generalization.

In Figure [10](#fig_9), we observe that RL generalizes better with more verification steps. More specifically, under the same computational budget across all experiments, we observe improvements of +2.15% (3 steps), +2.99% (5 steps), +5.99% (10 steps). In contrast, in the case with one verification step, we only observe a marginal improvement of +0.48% in OOD performance improvement. 

## Conclusion, Discussion and Limitations

In this paper, we presented a comprehensive analysis of the generalization effects of foundation model post-training techniques, specifically RL and SFT. Through extensive experiments on the GeneralPoints and V-IRL tasks, we demonstrated that RL exhibits superior performance in learning generalizable knowledge, while SFT tends to merely memorize the training data, across both the rule and visual variations. This phenomenon consistently occurs across multimodal arithmetic and spatial reasoning capabilities. In addition, we studied the effect of RL on visual recognition, the role of SFT, and the role of verification steps. During our study, two challenges were not resolved.

Failure of SFT on GP-VL. In Figure [5](#) for GP-VL, we observe that SFT fails to achieve a comparable indistribution performance with RL. To mitigate the variance introduced by hyperparameter choices, we additionally conduct 10 more experiments with different learning rates and tunable components (Figure [16](#fig_0)), none of which exhibits a strong increasing trend like RL (Figure [17](#fig_0)). Given our observation that scaling up SFT degrades visual recognition capabilities (Figure [8](#)), we hypothesize that SFT locally overfits to reasoning tokens while neglecting recognition tokens, possibly due to the higher frequency of reasoning tokens (see Figure [11](#fig_0) as example). We leave further investigation to future work.

Limits of RL in corner cases. As discussed in Section 5.4, SFT is necessary for effective RL training on Llama-3.2. We investigate applying RL to an overly-tuned SFT checkpoint. As demonstrated in Figure [19](#fig_8), RL is unable to recover out-of-distribution performance when starting from such a checkpoint. Example failure cases are illustrated in Figure [21](#fig_2), where the model collapses to the training rule. These results, together with findings in Section 5.4, indicate that RL has limited effectiveness when applied to extremely underfit or overfit initial checkpoints. Further research is needed to delineate the conditions under which SFT facilitates effective RL.

## A. Details on the General Points Environment

In this section, we demonstrate the design details for GeneralPoints mentioned in Section 4.1. We first present the data used for this environment (Appendix A.1). Then, we show examples of the environment's transition dynamics (Appendix A.2), followed by a description of key arguments and reward design specification (Appendix A.3).

## A.1. Data

GeneralPoints card quadruples are sampled from a deck of 52 standard poker cards. Each sampled quadruple is guaranteed to have at least one solution equals the target point, i.e. 24. We ensure this by using an expert solver during the sampling process.

## A.2. Detailed Examples on the Transition Dynamics

As shown in Figure [11](#fig_0) and Figure [12](#fig_2), we treat the system prompt as v in 0 and then subsequently appending the future outputs v out 1:t and verifier info v ver 1:t into the prompt for getting the t + 1 output. Figure [11](#fig_0) provides an example with the visual inputs, while Figure [12](#fig_2) shows the language only case.

## A.3. Additional Eetails on the Environmental Design

Arguments. The GeneralPoints environment supports the following configurable arguments:

• Target point: Any positive integer • Face cards rule: Two options -'J', 'Q', and 'K' all count as '10' -'J', ['Q', and 'K' count as '11', '12', and '13' respectively](#) • Card sampling: Two options -Sample 4 cards without replacement from a deck of 52 poker cards -Sample at least one card from 'J', 'Q', and 'K'

• Card color: Three options -Black suits only: ♣, ♠.

-Red suits only: ♥, ♦.

-All suits: ♠, ♥, ♣, ♦.

For all experiments, we fix the target point at 24. In Figure [5](#), training and in-domain evaluation use the rule where face cards count as '10'. For out-of-domain evaluation, we use the alternative face cards rule and require at least one face card, forcing calculations with numbers above 10 that are not encountered during training. For visual distribution shift experiments (Section 5.2), we train the model on black suits ♠, ♣ and evaluate out-of-domain performance on red suits ♥, ♦.

Reward design. An episode terminates when either a correct equation is generated or the maximum verification step of 5 is reached. The reward function is as follows:

• r = 5: For generating a legal equation that equals the target point

• r = -1: For legal equations using each card once but not equaling the target point

• r = -1: For exceeding maximum verification step

• r = -2: For legal equations containing numbers not among the given choices

• r = -3: For all other illegal equations

In the vision-language variant (GeneralPoints-VL), an additional penalty of r = -1.5 is applied when the agent fails to correctly recognize the given cards.

## B. Details on the V-IRL Environment

Similar to Appendix A, we present the design details for V-IRL discussed in Section 4.2. First, we introduce the database used for this environment (Appendix B.1) and demonstrate transition examples (Appendix B.2). We then describe the environment by explaining its fundamental component-route. Finally, we outline our modifications and reward design choices made to adapt the original V-IRL for reinforcement learning training (Appendix B.3).

## B.1. Data

Leveraging the data collection pipeline of [Yang et al. (2024a)](#b51), we construct a training database with 1000 unique routes from New York City. We evaluate all rule-variant experiments and visual in-distribution experiments using randomly sampled routes from this database. For visual out-of-distribution experiments, we directly adopt the VLN mini benchmark from [Yang et al. (2024a)](#b51). This benchmark consists of 18 distinct routes across nine cities: Milan, New Delhi, Buenos Aires, London, Hong Kong, New York,[foot_4](#foot_4) Melbourne, Lagos, and San Francisco, with two routes per city.

Figure [11](#fig_0): An example of our prompt update for constructing v in t+1 using v in t , v out t and v ver t . This example provides an optional vision input for VLMs, adding a visual recognition challenge. The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.

## B.2. Detailed Examples on the Transition Dynamics

We provide detailed transition examples of the V-IRL environment in Figure [13](#fig_0) (vision and language) and Figure [14](#fig_4) (pure language).

## B.3. Additional Details on the Environmental Design

Concept of route. The route serves as the fundamental navigation object in the V-IRL environment. As illustrated in Figure [4](#fig_4), each route corresponds to a real-world path with associated language instructions and visual signals. Using Figure [4](#fig_4) as an example, a route comprises: [Action space] -"forward()": indicates moving forward for 1 step; -"turn_direction(x)": indicates turn direction to the target heading, where x∈ [['north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest']](#); -"stop()": indicates the navigation is finished; [Observations and actions sequence] O_1: No landmarks nearby; A_1: turn_direction(east) O_2: No landmarks nearby; A_2: forward() O_3: No landmarks nearby; A_3: forward() O_4: You observe an image of 4 views; You observe an intersection A_4:

[Output] Your response should be a valid json file in the following format: { "current observation": latest observation from the street view grid, "current instruction": analyze the full instruction and identify the sentence to be executed, "action": the action to be taken chosen from the action space, }

## Appending model and verifier outputs to obtain

$v in t v in t = [v out 0 , v ver 0 , v out 1 , v ver 1 , . . . , v out t-1 , v ver t-1 ] ▷ v in t = concat v in 0 , [v out k , v ver k ] t-1 k=0$Model output (v out t ) { "current observation": "Hotel 32One is on my right behind; I observe an intersection", "current instruction": "Turn right to face north.", "action": "turn_direction(north)", } Verifier Output (v ver t ) Correct solution.

▷ Move to the next scene upon correct action Figure [13](#fig_0): An example of our prompt update for constructing v in t+1 using v in t , v out t and v ver t . The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively. Title Suppressed Due to Excessive Size System Prompt (v in 0 ) [Task Description] You are an expert in navgation. You will receive a sequence of instructions to follow. You are also provided with your observation and action histroy in text. Your goal is to first analyze the instruction and identify the next sentence to be executed. Then, you need to provide the action to be taken based on the current observation and instruction.

[Instruction] 1. First, turn left to face east.

2. Move forward until you reach the next intersection where Hotel 32One is on your right behind. 3. Turn right to face north. 4. Move forward until you reach the next intersection where Dragon Gate Chinatown SF is on your right front. 5. Turn left to face east. 6. Move forward until the destination Café de la Presse is on your right. [Action space] -"forward()": indicates moving forward for 1 step; -"turn_direction(x)": indicates turn direction to the target heading, where x∈['north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest']; -"stop()": indicates the navigation is finished; [Observations and actions sequence] O_1: No landmarks nearby; A_1: turn_direction(east) O_2: No landmarks nearby; A_2: forward() O_3: No landmarks nearby; A_3: forward() O_4: Hotel 32One is on your right behind; You observe an intersection A_4: [Output]

Your response should be a valid json file in the following format: { "current observation": latest observation from the street view grid, "current instruction": analyze the full instruction and identify the sentence to be executed, "action": the action to be taken chosen from the action space, }

## Appending model and verifier outputs to obtain

$v in t v in t = [v out 0 , v ver 0 , v out 1 , v ver 1 , . . . , v out t-1 , v ver t-1 ] ▷ v in t = concat v in 0 , [v out k , v ver k ] t-1 k=0$Model output (v out t ) { "current observation": "Hotel 32One is on my right behind; I observe an intersection", "current instruction": "Turn right to face north.", "action": "turn_direction(north)", } Verifier Output (v ver t ) Correct solution.

▷ Move to the next scene upon correct action Figure [14](#fig_4): An example of our prompt update for constructing v in t+1 using v in t , v out t and v ver t . This example provides an optional vision input for VLMs, adding a visual recognition challenge. The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.

## C. Experimental Setup

This section details the experimental setup used in Section 5. We first describe our data collection setup for supervised fine-tuning (Appendix C.1). Then, we present the training pipeline (Appendix C.2). Finally, we describe our evaluation metrics and the statistical tools used for generating plots (Appendix C.3).

C.1. Data SFT data collection. As illustrated in Figures 11 to 14, GeneralPoints and V-IRL environments naturally align with prompt-response dialogue structures. We create training samples by pairing each system prompt with its corresponding expert response. All SFT experiments in the main body use optimal single-turn prompt-response pairs, without any verification or revision steps.

## SFT on sub-optimal trajectories

To examine how more diverse SFT data affects the out-of-distribution performance of SFT, we conduct an ablation study on GP-L using sub-optimal trajectories as training data. Unlike expert prompt-response pairs, these sub-optimal trajectories include errors and verification messages in their prompts. This format aligns with evaluation scenarios where multiple verification iterations are allowed, similar to the data being used for the downstream RL training. In Figure [15](#fig_11), we observe that SFT still merely memorizes the training data with degraded out-of-distribution performance. This evidence suggests that memorization occurs due to the fundamental nature of SFT training rather than the SFT data. 

## C.2. Training Pipeline

As illustrated in Section 5, we follow the training pipeline by RL4VLM [(Zhai et al., 2024a)](#), where we first initial-ize the model with SFT, then separately scale up the compute for SFT and RL [(Schulman et al., 2017)](#b37), starting from this initialized model. For all experiments of SFT and RL in the main body, we tune all components using a shared learning rate per experiment. All training experiments are conducted on an 8 H800 machine (80GB).

## C.3. Evaluation Metric

Per-step accuracy. We report the per-step accuracy for V-IRL-VL task in Figures [5](#) and [6](#). An individual step is considered correct when the model's chosen action matches the expert trajectory at that position. Note that intermediate verification steps are counted as independent samples here.

Success rate. We report the success rate (%) of GP-L, GP-VL, V-IRL-L and V-IRL-VL in Figures [5](#) and [6](#). In the GeneralPoints task, success is defined as succeeding at least once during the inference time verification. In the V-IRL task, a sample is recorded as success when the model takes correct action at each movable point on the route.

Computation estimation. We estimate the FLOPs for training X following the similar manner of [(Snell et al., 2024;](#b39)[Hoffmann et al., 2023)](#b20), where Note that the used on-policy RL algorithm PPO [(Schulman et al., 2017)](#b37) contains iterative stages of replay buffer collection and optimization, hence requiring additional inference computation. For simplicity, we approximate the term via:

$X$$D buf f er ≈ E di do D RL • D RL = λD RL$where E ∈ N denotes the number of auto-regressive generation processes, di , do denote average input tokens and output tokens. We estimate the λ for GeneralPoints and V-IRL as 6 and 5.1 respectively after calculation.

Line smoothing and error bar. All line plots in our paper adopt Savitzky-Golay filter with polynomial order 3 as smoothing function. We assume each evaluated data point

![Figure 1: A comparative study of RL and SFT on the visual navigation environment V-IRL (Yang et al., 2024a) for OOD generalization. OOD curves represent performance on the same task, using a different textual action space. See detailed descriptions of the task in Section 5.1.]()

![Figure 2: An example of the sequential revision formulation with a verifier. The model generate the next answer v out t+1 conditioned on all previous answers and information (v out i , v ver t , 0 ≤ i ≤ t) from the verifier.]()

![First, turn slightly right towards the northeast and walk a short distance until you reach the next intersection, where you'll see The Dutch on your right. Next, make a sharp left turn to head northwest. Continue for a while until you reach the next intersection, where Lola Taverna will be on your right. Finally, turn slightly right to face northeast and walk a short distance until you reach your destination, Shuka, which will be on your right. northeast." [OBSERVATION] "See Lola Taverna on my right." [ACTION] "Left turn to northwest." [OBSERVATION] "See Shuka on my right." [ACTION] "Stop." [OBSERVATION] "See The Dutch on my right." [ACTION] "Left turn to northwest."]()

![Figure 4: Demonstration of one navigation task in V-IRL. Agent navigates from place to place following the given linguistic navigation instructions in V-IRL. The navigation procedure is shown at the top, with the navigation instructions displayed below.Visual observation-related information is highlighted in green, while action-related information is marked in orange.]()

![Figure5: Success rate (%) -GFLOPs trendlines for RL and SFT on GeneralPoints and V-IRL. The top row shows in-distribution performance, while the bottom row shows out-of-distribution performance. Results are presented for both pure language (-L) and vision-language (-VL) variants of each task. For GeneralPoints, we report the episode success rate, while for V-IRL, we report per-step accuracy with overall success rate in Figures1 and 18. Detailed evaluation setups (and curve smoothing) are provided in Appendix C.3.]()

![Figure 7: Comparison of out-of-distribution performance under visual variants. Similar to Figures 5 and 6, we present both the performance dynamics (shown as lines) and final performance (shown as bars) for visual out-of-distribution evaluations. The previous state-of-the-art on V-IRL VLN mini benchmark (Yang et al., 2024a) is marked in orange. Detailed evaluation setups (and curve smoothing) are provided in Appendix C.3.]()

![2), we consider a natural follow-up question: How does RL affect VLMs' visual capabilities? To study this question, we conducted additional ablation studies in the GP-VL environment to investigate the OOD performance of RL and SFT, along with the model's visual recognition accuracy, in terms of recognizing the 4 cards from the input image.In particular, we study how scaling post-training compute via RL/SFT both affects generalization in rule-based OOD (Figure8left), and visual recognition accuracy and visual OOD (Figure 8 right).]()

![Figure 9: RL experiments on GP-L without SFT initialization. All trials fail due to poor instruction following capability of the base model.]()

![Figure 10: In-distribution vs. OOD performance growth on GP-L. We record RL experiments with different number of verification iterations (VIter) as scaling up training compute (color transparency).]()

![: The Dutch, Lola Taverna • Straight road: Roads connecting turning points, starting point, and destination • Street views: 360-degree panoramic views at each movable point • Oracle information: Expert observation data for each movable point • Expert trajectory • Instruction Although the instructions in Figures 4, 13 and 14 are presented in different formats, they convey equivalent information, with Figure 4 using natural language. Simplification and arguments. We simplify the original V-IRL design from Yang et al. (2024a) to better accommodate RL training. The modifications include eliminating the 2-stage navigation pipeline that required a separate visual detector for street view processing, and removing online queries to reduce training time and cost. Our V-IRL environment contains 2 additional configuration arguments compared with the original design: • Action space: two options -Absolute direction: "turn_direction(x)" where x∈{'north', 'northeast', Title Suppressed Due to Excessive Size System Prompt (v in 0 ) [Task Description]You are an expert in navigation. You will receive a sequence of instructions to follow while observing your surrounding street views. You are also provided with your observation and action history in text. your goal is to take the action based on the current observation and instruction.[Instruction] 1. First, turn left to face east. 2. Move forward until you reach the next intersection where Hotel 32One is on your right behind. 3. Turn right to face north. 4. Move forward until you reach the next intersection where Dragon Gate Chinatown SF is on your right front. 5. Turn left to face east. 6. Move forward until the destination Café de la Presse is on your right. [Current observation] You observe a 2x2 grid of street view images with the following headings: [front, right back, left] You need to identify if any of the landmarks in the instruction are visible in the street view grid.]()

![Figure15: SFT experiments on GP-L with suboptimal trajectories. Similar to results in Figure5, SFT overfits the training data even we increase the trajectory diversity.]()

![train = 6N D train and X inf erence = 2N D inf erence . Here, N represents the model parameters and D train represents the number of tokens during training. Suppose our SFT and RL experients starts from a checkpoint trained on D init tokens, we can estimate the training computation of SFT and RL via the following equations:X SF T = 6N (D init + D SF T ) X RL = 6N (D init + D RL ) + 2N D buf f er]()

We use "memorization" the refer a model's capacity to generate near-exact copies of training examples when prompted based on information present in the training dataset. This definition explicitly excludes bit-wise or code-wise replication of training data within the model itself.

The visual input can be parsed into pure text description, see more details in[Yang et al. (2024a)](#b51) and an illustration of pure text the version in Figure14.

See Figure

4, the model needs to recognize landmarks likeThe Dutch, Lola Taverna, and Shuka from the visual observation, and relate these landmarks with the textual instructions for taking the right action.

These NYC routes in the VLN mini benchmark do not overlap with our training data.

