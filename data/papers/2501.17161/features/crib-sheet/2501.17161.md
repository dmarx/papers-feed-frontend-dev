- **Key Concepts**:
  - **Supervised Fine-Tuning (SFT)**: Tends to memorize training data, struggles with out-of-distribution generalization.
  - **Reinforcement Learning (RL)**: Generalizes better, especially with outcome-based rewards; enhances visual recognition capabilities.

- **Generalization vs. Memorization**:
  - SFT primarily memorizes training data.
  - RL learns generalizable rules applicable to unseen variants.

- **Evaluation Tasks**:
  - **GeneralPoints**: A card game for assessing arithmetic reasoning; model computes a target number using card values.
  - **V-IRL**: A real-world navigation task focusing on spatial reasoning.

- **Methodology**:
  - Multi-step RL framework initiated after SFT.
  - Sequential revision formulation for state-action transitions.

- **Performance Metrics**:
  - RL shows +33.8% improvement on V-IRL mini benchmark (44.0% to 77.8%).
  - Scaling inference time (maximal steps) improves generalization.

- **Visual Generalization**:
  - RL generalizes to visual out-of-distribution tasks; SFT struggles.
  - Outcome-based reward functions in RL improve visual capabilities.

- **Sequential Revision**:
  - At time step \( t = 0 \): Initial input \( v_{in}^0 \) is the system prompt.
  - For \( t \geq 1 \): Input \( v_{in}^t \) is concatenated with prior outputs:
    \[
    v_{in}^t = [v_{out}^k, v_{ver}^k]_{k=0}^{t-1}
    \]

- **Standard RL Notation**:
  - State space \( S \), action space \( A \), reward function \( r: S \times A \rightarrow R \).
  - Policy \( \pi: S \rightarrow A \) aims to maximize return:
    \[
    \max_{\pi \in \Pi} E_{\pi} \left[ \sum_{t=0}^{T} r_t \right]
    \]

- **Contributions of SFT to RL**:
  - SFT stabilizes output format, facilitating effective RL training.

- **Implications for AI Systems**:
  - Understanding the balance between memorization and generalization is crucial for building robust AI systems.