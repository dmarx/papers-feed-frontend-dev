<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</title>
				<funder ref="#_g45bKzg">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder>
					<orgName type="full">HKU</orgName>
				</funder>
				<funder ref="#_2wJpVPZ">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_JwKcxeg">
					<orgName type="full">Hong Kong Center for Construction Robotics Limited (HKCRC)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-28">28 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tianzhe</forename><surname>Chu</surname></persName>
							<email>&lt;tianzhechu@gmail.com&gt;</email>
						</author>
						<author>
							<persName><forename type="first">Yuexiang</forename><surname>Zhai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jihan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shengbang</forename><surname>Tong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
						</author>
						<author>
							<persName><forename type="first">♣</forename><surname>Quoc</surname></persName>
						</author>
						<author>
							<persName><forename type="first">V</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-28">28 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">B180E7C888D827E31EBBE4AB1BA305A3</idno>
					<idno type="arXiv">arXiv:2501.17161v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their respective role in enhancing model generalization remains unclear. This paper studies the comparative effect of SFT and RL on generalization and memorization, focusing on text-based and visual environments. We introduce GeneralPoints, an arithmetic reasoning card game, and also consider V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes in both the rule-based textual and visual environments. SFT, in contrast, tends to memorize the training data and struggles to generalize out-of-distribution in either scenario. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in visual domains. Despite RL's superior generalization, we show that SFT is still helpful for effective RL training: SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrate the advantage of RL for acquiring generalizable knowledge in complex, multimodal tasks. * Equal contribution. ♠ HKU, ♥ UC Berkeley, ♣ Google Deep-Mind, ♦ NYU. All experiments are conducted outside of Google.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While SFT and RL are both widely used for foundation model training <ref type="bibr">(OpenAI, 2023b;</ref><ref type="bibr" target="#b18">Google, 2023;</ref><ref type="bibr" target="#b22">Jaech et al., 2024;</ref><ref type="bibr" target="#b15">DeepSeekAI et al., 2025)</ref>, their distinct effects on generalization <ref type="bibr" target="#b7">(Bousquet &amp; Elisseeff, 2000;</ref><ref type="bibr" target="#b61">Zhang et al., 2021)</ref> remain unclear, which makes it challenging to build reliable and robust AI systems. A key challenge in analyzing the generalization ability of foundation models <ref type="bibr" target="#b6">(Bommasani et al., 2021;</ref><ref type="bibr" target="#b8">Brown et al., 2020)</ref> is separating data memorization<ref type="foot" target="#foot_0">foot_0</ref> from the acquisition of transferable principles. We therefore investigate the key question of whether SFT or RL primarily memorize the training data (Allen- <ref type="bibr" target="#b3">Zhu &amp; Li, 2023a;</ref><ref type="bibr" target="#b55">Ye et al., 2024;</ref><ref type="bibr" target="#b24">Kang et al., 2024)</ref>, or whether they learn generalizable principles that can adapt to novel task variants.</p><p>To address this question, we focus on two aspects of generalization: textual rule-based generalization and visual generalization. For textual rules, we study a model's ability to apply learned rules (given text instructions) to variants of those rules <ref type="bibr" target="#b67">(Zhu et al., 2023;</ref><ref type="bibr" target="#b54">Yao et al., 2024;</ref><ref type="bibr" target="#b55">Ye et al., 2024)</ref>. For vision-language models (VLMs), visual generalization measures performance consistency to variations in visual input, such as color and spatial layout, within a given task. For studying text-based and visual generalization, we investigate two different tasks that embody rule-based and visual variants. Our first task is GeneralPoints, an original card game task that is similar to the Points24 task from RL4VLM <ref type="bibr">(Zhai et al., 2024a)</ref>, which is designed to evaluate a model's arithmetic reasoning capabilities. In GeneralPoints, the model receives four cards (presented as a text description or an image), and is required to compute a target number (24 by default) using each card's numerical value exactly once. Second, we adopt V-IRL <ref type="bibr" target="#b51">(Yang et al., 2024a)</ref>, a real-world navigation task, that focuses on the model's spatial reasoning capabilities.</p><p>We adopt a multi-step RL framework similar to <ref type="bibr">Zhai et al. (2024a)</ref>, by instantiating RL after running SFT on the backbone model <ref type="bibr" target="#b16">(Dubey et al., 2024)</ref>, using the sequential revision formulation <ref type="bibr" target="#b39">(Snell et al., 2024)</ref>. In both GeneralPoints and V-IRL, we observe that RL learns generalizable rules (expressed in text), where in-distribution performance gains also transfer to unseen rules. In contrast, SFT appears to memorize the training rules and fails to generalize (see Figure <ref type="figure" target="#fig_0">1</ref> for an example).</p><p>Beyond textual rule-based generalization, we further investigate generalization in the visual domain and observe that RL also generalizes to visual OOD tasks, whereas SFT continues to struggle. As a by-product of the visual OOD generalization capability, our multi-turn RL approach achieves state-of-the-art performance on the V-IRL mini benchmark, by +33.8% (44.0%→77.8%) <ref type="bibr" target="#b51">(Yang et al., 2024a)</ref>, highlighting the generalization capability of RL. To understand how RL impacts a model's visual abilities, we conduct additional analysis on GeneralPoints, revealing that training RL with an outcome-based reward function <ref type="bibr" target="#b14">(Cobbe et al., 2021)</ref> improves visual recognition capabilities. While RL exhibits superior generalization compared to SFT, we show that SFT is still helpful for stabilizing the model's output format, enabling RL to achieve its performance gains. Last but not least, we observe that scaling up the inference time compute by increasing the number of maximal steps leads to better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Post-training. Post-training is crucial for enhancing model performance <ref type="bibr" target="#b63">(Zhang et al., 2022;</ref><ref type="bibr" target="#b20">Hoffmann et al., 2023;</ref><ref type="bibr">OpenAI, 2023b;</ref><ref type="bibr" target="#b18">Google, 2023;</ref><ref type="bibr" target="#b47">Touvron et al., 2023)</ref>. This stage commonly utilizes large-scale supervised finetuning (SFT) <ref type="bibr" target="#b33">(Radford et al., 2018;</ref><ref type="bibr" target="#b8">Brown et al., 2020;</ref><ref type="bibr">Radford et al., 2021;</ref><ref type="bibr">Wei et al., 2022a;</ref><ref type="bibr" target="#b13">Chung et al., 2022;</ref><ref type="bibr" target="#b65">Zhou et al., 2024a)</ref> and/or reinforcement learning (RL) <ref type="bibr" target="#b68">(Ziegler et al., 2019;</ref><ref type="bibr">Ouyang et al., 2022;</ref><ref type="bibr" target="#b40">Sun et al., 2024;</ref><ref type="bibr" target="#b0">Abdulhai et al., 2023;</ref><ref type="bibr" target="#b66">Zhou et al., 2024b;</ref><ref type="bibr">Zhai et al., 2024a)</ref>. SFT adapts pre-trained models to downstream tasks by training them on task-specific, often instructionformatted datasets. Previous work, such as FLAN <ref type="bibr">(Wei et al., 2022a)</ref>, demonstrates that fine-tuning on diverse instruction-tuning datasets significantly enhances zero-shot performance on unseen tasks. Furthermore, LIMA <ref type="bibr" target="#b65">(Zhou et al., 2024a)</ref> shows that supervised fine-tuning acts as a "format teacher" effectively adapting the model's responses to a desired format while leveraging the capabilities of pre-trained LLMs. In contrast, RL <ref type="bibr" target="#b68">(Ziegler et al., 2019;</ref><ref type="bibr">Ouyang et al., 2022;</ref><ref type="bibr" target="#b40">Sun et al., 2024;</ref><ref type="bibr" target="#b36">Ramamurthy et al., 2023;</ref><ref type="bibr" target="#b0">Abdulhai et al., 2023;</ref><ref type="bibr" target="#b66">Zhou et al., 2024b;</ref><ref type="bibr">Zhai et al., 2024a)</ref> has been primarily used to align models with human preferences or training the foundational model to solve a specific task <ref type="bibr" target="#b0">(Abdulhai et al., 2023;</ref><ref type="bibr" target="#b66">Zhou et al., 2024b;</ref><ref type="bibr">Zhai et al., 2024a;</ref><ref type="bibr" target="#b11">Chen et al., 2024b</ref>). Our work differs from prior studies, as we aim to comparatively analyze the generalization and memorization of SFT and RL on both LLM and VLM, while previous studies have focused primarily on only one of these two post-training methods (or only study LLM or VLM) or on only one posttraining method.</p><p>Memorization and generalization in LLM/VLM. Several studies have examined the interplay between memorization and generalization in neural networks <ref type="bibr" target="#b19">(Han et al., 2022;</ref><ref type="bibr" target="#b9">Carlini et al., 2022;</ref><ref type="bibr" target="#b53">Yang et al., 2023)</ref>. In LLMs, memorization can manifest as the model memorizing the training data <ref type="bibr" target="#b9">(Carlini et al., 2022;</ref><ref type="bibr" target="#b23">Jiang et al., 2024;</ref><ref type="bibr" target="#b24">Kang et al., 2024)</ref>, while generalization reflects the divergence between the model's output distribution and the pre-training data distribution <ref type="bibr" target="#b62">(Zhang et al., 2023)</ref>. Prior studies suggest that LLMs exhibit more overfitting on simpler, knowledge-intensive tasks and greater generalization on more complex, reasoning-intensive ones <ref type="bibr" target="#b48">(Wang et al., 2024;</ref><ref type="bibr" target="#b32">Qi et al., 2024)</ref>. For example, recent studies <ref type="bibr" target="#b55">(Ye et al., 2024;</ref><ref type="bibr" target="#b2">Allen-Zhu, 2024;</ref><ref type="bibr" target="#b3">Allen-Zhu &amp; Li, 2023a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b2">2024;</ref><ref type="bibr">Tong et al., 2024b)</ref> have demonstrated that LLMs develop reasoning skill sets beyond their training data by pre-computing reasoning graphs before autoregressive generation, which provides compelling evidence of generalization. Our study takes a different approach by investigating the role of different post-training paradigms on memorization versus generalization in the context of textual ruledbased and visual variants. We conduct comparative studies in both unimodal (LLM) and multimodal (VLM) settings, and demonstrate that RL leads to better generalization performance than SFT.</p><p>Scaling up inference-time compute. Recent research has increasingly focused on scaling up inference-time computation to improve model performance <ref type="bibr" target="#b50">(Wei et al., 2022b;</ref><ref type="bibr" target="#b54">Yao et al., 2024;</ref><ref type="bibr" target="#b39">Snell et al., 2024;</ref><ref type="bibr" target="#b22">Jaech et al., 2024)</ref>.</p><p>Early studies <ref type="bibr" target="#b50">(Wei et al., 2022b;</ref><ref type="bibr" target="#b54">Yao et al., 2024)</ref> prompted models to generate intermediate reasoning steps and extend the responses before producing a final answer. Subsequent work <ref type="bibr" target="#b58">(Zelikman et al., 2022;</ref><ref type="bibr" target="#b17">Feng et al., 2023;</ref><ref type="bibr" target="#b42">Tian et al., 2024;</ref><ref type="bibr" target="#b10">Chen et al., 2024a;</ref><ref type="bibr" target="#b39">Snell et al., 2024)</ref> has demonstrated that fine-tuning verifiers during inference improves model accuracy, effectively utilizing test-time computation. Notably, recent findings <ref type="bibr" target="#b22">(Jaech et al., 2024;</ref><ref type="bibr" target="#b15">DeepSeekAI et al., 2025)</ref> reveal "scaling laws" for inference-time compute, highlighting significant performance gains with increased computational resources. Our work builds upon these findings in two ways. First, we integrate insights from inference-time verification into a multi-turn RL formulation that allows the model to identify and correct its errors. Second, we examine the impact of inference-time verification on RL generalization, demonstrating that scaling up inference-time verification (in terms of the maximum number of verification steps) is a key for RL to generalize.</p><p>Improving visual capability in VLMs. While VLMs have demonstrated remarkable skill across a wide range of challenging tasks, such as solving advanced college exam questions <ref type="bibr" target="#b28">(Lu et al., 2023;</ref><ref type="bibr">Yue et al., 2024a;</ref><ref type="bibr">b)</ref> and spatial understanding tasks <ref type="bibr" target="#b51">(Yang et al., 2024a;</ref><ref type="bibr">b)</ref>, they also exhibit limitations in visual perception <ref type="bibr">(Zhai et al., 2024a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b45">Tong et al., 2024c;</ref><ref type="bibr" target="#b18">d;</ref><ref type="bibr" target="#b35">Rahmanzadehgervi et al., 2024)</ref>. Prior efforts to enhance VLMs' visual perception include combining multiple visual encoders <ref type="bibr" target="#b46">(Tong et al., 2024d;</ref><ref type="bibr" target="#b25">Kar et al., 2025;</ref><ref type="bibr">Tong et al., 2024a)</ref>, curating high-quality SFT data <ref type="bibr" target="#b12">(Chen et al., 2023;</ref><ref type="bibr" target="#b27">Liu et al., 2024;</ref><ref type="bibr">Tong et al., 2024a)</ref>, and improving the SFT training recipe by unfreezing the visual backbone <ref type="bibr" target="#b26">(Liu et al., 2023;</ref><ref type="bibr">Tong et al., 2024a)</ref>. While these prior works primarily focus on experiments during the SFT stage, our work demonstrates that RL can also improve visual perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Standard RL terminology. We consider finite horizon decision making, and adopt standard notation from the classical RL literature <ref type="bibr" target="#b41">(Sutton &amp; Barto, 2018;</ref><ref type="bibr" target="#b1">Agarwal et al., 2019)</ref>, where S denotes the state space, A denotes the action space, r : S × A → R denotes the reward function, and T denotes the maximum number of steps per episode. The goal is to learn a policy π : S → A that maximizes the overall return max π∈Π E π T t=0 r t , where r t denotes r(s t , a t ). Without loss of generality, we use π(a|s) ∈ [0, 1] to denote probability of π choosing a at s.</p><p>Adapting RL terminology to LLM/VLM with a verifier. We adopt a multi-turn RL setting for foundation model training <ref type="bibr">(Zhai et al., 2024a)</ref>. Let V represent the discrete and finite vocabulary (token) space. The input and output text spaces are denoted by V m and V n respectively, where m and n are the maximum token length of the input sequence v in and output sequence v out . For models requiring visual inputs (VLM), we define O as the space of all RGB images. The state space, denoted by S, is defined as S := V m ×O for VLM, and S := V m for LLM. The action space A is defined as A := V n . We use VER : V n → R × V k to denote a verifier, which evaluates the outcome of v out and generates an outcome-based reward function <ref type="bibr" target="#b14">(Cobbe et al., 2021;</ref><ref type="bibr" target="#b21">Hosseini et al., 2024;</ref><ref type="bibr" target="#b39">Snell et al., 2024;</ref><ref type="bibr" target="#b38">Setlur et al., 2024)</ref> r along with textual information v ver . Mathematically, at time t, VER(v out t ) → (r t , v ver t ). Similar to <ref type="bibr">Zhai et al. (2024a)</ref>, we treat the model with parameter θ as our policy network π θ : S → V n , and adopt PPO <ref type="bibr" target="#b37">(Schulman et al., 2017)</ref> as the backbone RL algorithm for updating π θ .</p><p>Sequential revision. For modeling the state-action transition, we adopt the sequential revision formulation <ref type="bibr" target="#b39">(Snell et al., 2024)</ref>. Specifically, at time step t = 0 the initial input v in 0 consists of the system prompt. For subsequent time steps (t ≥ 1), the input prompt v in t comprises the system prompt concatenated with all prior model and verifier outputs, denoted by</p><formula xml:id="formula_0">[v out k , v ver k ] t-1</formula><p>k=0 . An illustration of the sequential revision is provided in Figure <ref type="figure" target="#fig_2">2</ref> (also see Figure <ref type="figure">5</ref> of <ref type="bibr" target="#b39">Snell et al. (2024))</ref>, and an example of the state-action transition is shown in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Tasks</head><p>To evaluate the generalization of different post-training methods, we select two tasks that each offer rule and visual variations. The first task, GeneralPoints, is a new environment we have designed that allows assessment of arithmetic reasoning abilities (Section 4.1). The second task, V-IRL <ref type="bibr" target="#b51">(Yang et al., 2024a)</ref>, is chosen to examine the model's reasoning capabilities in an open-world visual navigation domain (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The General Points Environment</head><p>Our original GeneralPoints environment, instantiated on top of the Points24 environment <ref type="bibr">(Zhai et al., 2024a)</ref>, is designed to evaluate generalization of arithmetic reasoning. Each state s of the environment contains 4 cards, described as text (in the GP-L variant) or presented as an image (in the GP-VL variant); see Figure 2 left for a visual example of GeneralPoints. The goal is to produce an equation that equals a target number (24 by default) using all 4 numbers from the cards exactly once. Detailed examples of the state-action transitions are provided in Appendix A.2. Note that when input from GeneralPoints is presented in an image (GP-VL), it naturally introduces additional visual challenges requiring the VLM to recognize all cards before solving the equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule variations.</head><p>To study whether the model learns arithmetic operations or simply memorizes the post-training data, we introduce rule variations in GeneralPoints. These variations consist of interpreting the symbols 'J', <ref type="bibr">'Q', and 'K' either as '11', '12', and '13', respectively</ref>, or all as the same number '10'. These variations ensure a rigorous evaluation of the model's ability to generalize arithmetic reasoning across diverse settings. Each rule is specified as text in the input prompt,  System Prompt (v in 0 ) [Task Description] You are an expert in {task name}, you are observing {purely language/vision-language inputs + &lt;image&gt;}. You are currently at {state related info}. Please follow {tasks rules}.</p><p>[Output] Your response should be a valid json file in the following format: {task related information and answer}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appending previous model and verifier outputs to obtain</head><formula xml:id="formula_1">v in t v in t = [v out 0 , v ver 0 , v out 1 , v ver 1 , . . . , v out t-1 , v ver t-1 ] ▷ v in t = concat v in 0 , [v out k , v ver k ] t-1 k=0</formula><p>Model output (v out t ) and Verifier Output (v ver t ) {Task related json outputs}, {You success/fail}.</p><formula xml:id="formula_2">▷ v in t+1 = concat(v in t , v out t , v ver t )</formula><p>Figure <ref type="figure">3</ref>: An template of our prompt update for constructing v in t+1 . The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively. see the {tasks rules} part in Figure <ref type="figure">3</ref>. For studying ruled based generalization, we post-train the model using one rule, then evaluate using a different rule.</p><p>Visual variations. The GeneralPoints environment can also be naturally customized to evaluate generalization across visual variants. Since the major visual challenge is to recognize the number of each card, agnostic to the the color of the cards, we consider the cards with different colors as visual variants of the task. In the visual generalization setting, we train the model using cards of one color, then test OOD performance using the other color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The V-IRL Environment</head><p>While the GeneralPoints environment is designed to assess arithmetic reasoning abilities, we further utilize the V-IRL environment <ref type="bibr" target="#b51">(Yang et al., 2024a)</ref> to study spatial reasoning ability in an open-world navigation domain that uses realistic visual input. As in GeneralPoints we consider two versions of the environment, one (V-IRL-L) that consists of pure language descriptions,<ref type="foot" target="#foot_1">foot_1</ref> and another (V-IRL-VL) that includes vision-language input. The major visual challenge in V-IRL involves recognizing differ-ent landmarks from the visual observation<ref type="foot" target="#foot_2">foot_2</ref> before taking an action. The goal is to navigate to a target location by following a set of instructions that contain spatial information. A detailed example of one environment step is shown in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule variations.</head><p>To evaluate whether the model possesses spatial knowledge or simply memorizes posttraining data, we consider two distinct action space configurations. The first variant utilizes an absolute orientation action space, which includes {'north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest'}. The second variant employs a relative orientation action space, containing {'left', 'right', 'slightly left', 'slightly right'}. This relative configuration adjusts the current orientation by 90 degrees or 45 degrees to the left or right, respectively. An overview of a navigation task in V-IRL is provided in Figure <ref type="figure" target="#fig_4">4</ref>, and a detailed state-action transition in V-IRL is provided in Figure <ref type="figure" target="#fig_0">13</ref> (in Appendix B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual variations.</head><p>The key visual challenge in V-IRL is to recognize landmarks from the visual observations (e.g., the green parts in Figure <ref type="figure" target="#fig_4">4</ref>). Since the V-IRL environment contains visual observations from different cities, we  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we present experiments that investigate the generalization abilities induced by post-training with RL and SFT. We adopt Llama-3.2-Vision-11B <ref type="bibr" target="#b16">(Dubey et al., 2024)</ref> as the backbone model. Following the standard pipelines of RLHF <ref type="bibr">(Ouyang et al., 2022)</ref> and RL4VLM <ref type="bibr">(Zhai et al., 2024a)</ref>, we initialize the model with SFT before running RL. We specifically study the following questions. Section 5.1: how does SFT or RL affect the model's generalization to different rules? Section 5.2: when the model contains a visual component, how does RL/SFT affect its generalization to different visual variants? Section 5.3: how does RL/SFT affect visual recognition capability in a VLM? Section 5.4: what role does SFT play in RL training? Section 5.5: how does the number of verification iterations affect generalization?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Generalization across Rules</head><p>We evaluate the performance of different post-training methods on GeneralPoints and V-IRL, each of which has a pure language (-L) and a vision-language (-VL) variant, and each encompassing rule variations. For each task, we separately scale the training compute for RL and SFT on a single rule. We consider the results on the trained rule as in-distribution (ID) performance, whereas results on the unseen rules measures out-of-distribution (OOD) generalization. In GeneralPoints, the ID case treats all 'J', 'Q', 'K' as 10, and the OOD cases interprets them as 11, 12, and 13. As for V-IRL, the ID case adopts the absolute orientation coordinate system and the OOD case uses the relative orientation action space. Other details and additional experimental setup can be found in Appendix C.</p><p>RL generalizes, SFT memorizes. As illustrated in Figure <ref type="figure">5</ref>, RL consistently improves OOD performance on all tasks, including both unimodal (LLM) and multimodal (VLM). Specifically, Figure <ref type="figure">6</ref> demonstrates that RL achieves an increase of +3.5% on GP-L (11.5% → 15.0%) and +11.0% on V-IRL-L (80.8% → 91.8%).</p><p>Even with the additional challenge of visual recognition in the VLM, RL maintains consistent performance improvements of +3.0% (11.2% → 14.2%) on GP-VL and +9.3% (35.7% → 45.0%) on V-IRL-VL, respectively. In contrast, SFT consistently exhibits performance degradation across all OOD evaluations on all tasks: -8.1% on GP-L (11.5% → 3.4%), -79.5% on V-IRL-L (80.8% → 1.3%), -5.6% (11.2% → 5.6%) on GP-VL, and -33.2% (35.7% → 2.5%) on V-IRL-VL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Generalization in Visual Out-of-Distribution Tasks</head><p>Section 5.1 demonstrates that RL yields generalization across rule variations, whereas SFT exhibits the opposite trend. Since VLMs also incorporate a visual modality, we next study the effects of visual variation in OOD generalization. For GeneralPoints, we train the VLM using the black suits (♠, ♣) and test out-of-distribution performance on the red suits (♥, ♦). For V-IRL, we train the model on routes collected in New York City and evaluate it on the original V-IRL VLN mini benchmark <ref type="bibr" target="#b51">(Yang et al., 2024a)</ref> containing routes from various cities worldwide (see Appendix B.1 for details). Note that the rules remain consistent across experiments in this section. RL generalizes in visual OOD tasks. As shown in Figure 7, we observe that RL still generalizes in visual OOD tasks, while SFT continues to suffer. Specifically, in GP-VL and VIRL-VL, RL achieves performance improvements of +17.6% (23.6% → 41.2%), +61.1% (16.7% → 77.8%), whereas SFT suffers from performance decreases of -9.9% (23.6% → 13.7%) and -5.6% (16.7% → 11.1%).</p><p>As a byproduct of this visual OOD study, we also show that our multi-turn RL formulation improves the state-ofthe-art results (see Table <ref type="table">5</ref> of <ref type="bibr" target="#b51">Yang et al. (2024a)</ref>) on the V-IRL mini benchmark by +33.8% (44.0% → 77.8%). Notably, unlike the previous state-of-the-art approach reported in V-IRL, which relies on a two stage VLM-LLM collaboration technique and tailored prompt engineering on closed-sourced model (OpenAI, 2023a), our end-to-end RL approach enables an open-sourced model <ref type="bibr" target="#b16">(Dubey et al., 2024)</ref> to reach superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">RL Improves Visual Capabilities</head><p>Building upon the above observation that VLMs trained with RL generalize to visual OOD tasks (Section 5. Scaling RL improves visual recognition accuracy in VLM training. As shown in Figure <ref type="figure">8</ref>, we observe that the VLM's visual recognition accuracy largely affects the overall performance, which was similarly observed in <ref type="bibr" target="#b64">Zhong et al. (2024)</ref>. In addition, scaling up RL compute also improves visual recognition accuracy, as a byproduct of its generalization capability, while scaling SFT deteriorates both visual recognition accuracy and overall performance. Additional experimental results are provided in Figures 16 and 17 of Appendix D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">The Role of SFT for RL Training</head><p>Despite the superiority of RL in generalizing the model's reasoning and visual capabilities, as discussed previously, the experimental pipeline still instantiates RL after SFT.</p><p>In this subsection, we focus on another key question: Is SFT necessary for RL training? To answer this question, we conduct additional experiments that directly apply endto-end RL to post-train the base model Llama3.2 using GeneralPoints in the purely language case (Figure <ref type="figure" target="#fig_8">9</ref>). SFT is necessary for RL training when the backbone model does not follow instructions. Figure <ref type="figure" target="#fig_8">9</ref> shows that without SFT, all end-to-end RL runs fail to improve. More specifically, we observe that without SFT, the base model suffers from poor instruction following capability.</p><p>A detailed failure case is provided in Figure <ref type="figure" target="#fig_2">20</ref> (in Appendix D.3), revealing that the base Llama-3.2-Vision-11B model tends to generate long, tangential, and unstructured responses. This issue makes it impossible to retrieve taskrelated information and rewards for RL training. Note that due to the difference in backbone model, our results do not contradict with DeepSeekAI et al. ( <ref type="formula">2025</ref>), which suggests that SFT is unnecessary for downstream RL training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Role of Verification Iterations</head><p>Verification serves as another crucial component in our multi-step training and evaluation pipeline (see Figures <ref type="figure" target="#fig_2">2</ref> and <ref type="figure">3</ref>). To validate its necessity and better understand its effect, we conduct RL experiments with different verification iterations {1, 3, 5, 10} using GP-L (Figure <ref type="figure" target="#fig_9">10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling up verification improves generalization.</head><p>In Figure <ref type="figure" target="#fig_9">10</ref>, we observe that RL generalizes better with more verification steps. More specifically, under the same computational budget across all experiments, we observe improvements of +2.15% (3 steps), +2.99% (5 steps), +5.99% (10 steps). In contrast, in the case with one verification step, we only observe a marginal improvement of +0.48% in OOD performance improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion, Discussion and Limitations</head><p>In this paper, we presented a comprehensive analysis of the generalization effects of foundation model post-training techniques, specifically RL and SFT. Through extensive experiments on the GeneralPoints and V-IRL tasks, we demonstrated that RL exhibits superior performance in learning generalizable knowledge, while SFT tends to merely memorize the training data, across both the rule and visual variations. This phenomenon consistently occurs across multimodal arithmetic and spatial reasoning capabilities. In addition, we studied the effect of RL on visual recognition, the role of SFT, and the role of verification steps. During our study, two challenges were not resolved.</p><p>Failure of SFT on GP-VL. In Figure <ref type="figure">5</ref> for GP-VL, we observe that SFT fails to achieve a comparable indistribution performance with RL. To mitigate the variance introduced by hyperparameter choices, we additionally conduct 10 more experiments with different learning rates and tunable components (Figure <ref type="figure" target="#fig_0">16</ref>), none of which exhibits a strong increasing trend like RL (Figure <ref type="figure" target="#fig_0">17</ref>). Given our observation that scaling up SFT degrades visual recognition capabilities (Figure <ref type="figure">8</ref>), we hypothesize that SFT locally overfits to reasoning tokens while neglecting recognition tokens, possibly due to the higher frequency of reasoning tokens (see Figure <ref type="figure" target="#fig_0">11</ref> as example). We leave further investigation to future work.</p><p>Limits of RL in corner cases. As discussed in Section 5.4, SFT is necessary for effective RL training on Llama-3.2. We investigate applying RL to an overly-tuned SFT checkpoint. As demonstrated in Figure <ref type="figure" target="#fig_8">19</ref>, RL is unable to recover out-of-distribution performance when starting from such a checkpoint. Example failure cases are illustrated in Figure <ref type="figure" target="#fig_2">21</ref>, where the model collapses to the training rule. These results, together with findings in Section 5.4, indicate that RL has limited effectiveness when applied to extremely underfit or overfit initial checkpoints. Further research is needed to delineate the conditions under which SFT facilitates effective RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details on the General Points Environment</head><p>In this section, we demonstrate the design details for GeneralPoints mentioned in Section 4.1. We first present the data used for this environment (Appendix A.1). Then, we show examples of the environment's transition dynamics (Appendix A.2), followed by a description of key arguments and reward design specification (Appendix A.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Data</head><p>GeneralPoints card quadruples are sampled from a deck of 52 standard poker cards. Each sampled quadruple is guaranteed to have at least one solution equals the target point, i.e. 24. We ensure this by using an expert solver during the sampling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Detailed Examples on the Transition Dynamics</head><p>As shown in Figure <ref type="figure" target="#fig_0">11</ref> and Figure <ref type="figure" target="#fig_2">12</ref>, we treat the system prompt as v in 0 and then subsequently appending the future outputs v out 1:t and verifier info v ver 1:t into the prompt for getting the t + 1 output. Figure <ref type="figure" target="#fig_0">11</ref> provides an example with the visual inputs, while Figure <ref type="figure" target="#fig_2">12</ref> shows the language only case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional Eetails on the Environmental Design</head><p>Arguments. The GeneralPoints environment supports the following configurable arguments:</p><p>• Target point: Any positive integer • Face cards rule: Two options -'J', 'Q', and 'K' all count as '10' -'J', <ref type="bibr">'Q', and 'K' count as '11', '12', and '13' respectively</ref> • Card sampling: Two options -Sample 4 cards without replacement from a deck of 52 poker cards -Sample at least one card from 'J', 'Q', and 'K'</p><p>• Card color: Three options -Black suits only: ♣, ♠.</p><p>-Red suits only: ♥, ♦.</p><p>-All suits: ♠, ♥, ♣, ♦.</p><p>For all experiments, we fix the target point at 24. In Figure <ref type="figure">5</ref>, training and in-domain evaluation use the rule where face cards count as '10'. For out-of-domain evaluation, we use the alternative face cards rule and require at least one face card, forcing calculations with numbers above 10 that are not encountered during training. For visual distribution shift experiments (Section 5.2), we train the model on black suits ♠, ♣ and evaluate out-of-domain performance on red suits ♥, ♦.</p><p>Reward design. An episode terminates when either a correct equation is generated or the maximum verification step of 5 is reached. The reward function is as follows:</p><p>• r = 5: For generating a legal equation that equals the target point</p><p>• r = -1: For legal equations using each card once but not equaling the target point</p><p>• r = -1: For exceeding maximum verification step</p><p>• r = -2: For legal equations containing numbers not among the given choices</p><p>• r = -3: For all other illegal equations</p><p>In the vision-language variant (GeneralPoints-VL), an additional penalty of r = -1.5 is applied when the agent fails to correctly recognize the given cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details on the V-IRL Environment</head><p>Similar to Appendix A, we present the design details for V-IRL discussed in Section 4.2. First, we introduce the database used for this environment (Appendix B.1) and demonstrate transition examples (Appendix B.2). We then describe the environment by explaining its fundamental component-route. Finally, we outline our modifications and reward design choices made to adapt the original V-IRL for reinforcement learning training (Appendix B.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Data</head><p>Leveraging the data collection pipeline of <ref type="bibr" target="#b51">Yang et al. (2024a)</ref>, we construct a training database with 1000 unique routes from New York City. We evaluate all rule-variant experiments and visual in-distribution experiments using randomly sampled routes from this database. For visual out-of-distribution experiments, we directly adopt the VLN mini benchmark from <ref type="bibr" target="#b51">Yang et al. (2024a)</ref>. This benchmark consists of 18 distinct routes across nine cities: Milan, New Delhi, Buenos Aires, London, Hong Kong, New York,<ref type="foot" target="#foot_4">foot_4</ref> Melbourne, Lagos, and San Francisco, with two routes per city.</p><p>Figure <ref type="figure" target="#fig_0">11</ref>: An example of our prompt update for constructing v in t+1 using v in t , v out t and v ver t . This example provides an optional vision input for VLMs, adding a visual recognition challenge. The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Detailed Examples on the Transition Dynamics</head><p>We provide detailed transition examples of the V-IRL environment in Figure <ref type="figure" target="#fig_0">13</ref> (vision and language) and Figure <ref type="figure" target="#fig_4">14</ref> (pure language).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Additional Details on the Environmental Design</head><p>Concept of route. The route serves as the fundamental navigation object in the V-IRL environment. As illustrated in Figure <ref type="figure" target="#fig_4">4</ref>, each route corresponds to a real-world path with associated language instructions and visual signals. Using Figure <ref type="figure" target="#fig_4">4</ref> as an example, a route comprises: [Action space] -"forward()": indicates moving forward for 1 step; -"turn_direction(x)": indicates turn direction to the target heading, where x∈ <ref type="bibr">['north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest']</ref>; -"stop()": indicates the navigation is finished; [Observations and actions sequence] O_1: No landmarks nearby; A_1: turn_direction(east) O_2: No landmarks nearby; A_2: forward() O_3: No landmarks nearby; A_3: forward() O_4: You observe an image of 4 views; You observe an intersection A_4:</p><p>[Output] Your response should be a valid json file in the following format: { "current observation": latest observation from the street view grid, "current instruction": analyze the full instruction and identify the sentence to be executed, "action": the action to be taken chosen from the action space, }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appending model and verifier outputs to obtain</head><formula xml:id="formula_3">v in t v in t = [v out 0 , v ver 0 , v out 1 , v ver 1 , . . . , v out t-1 , v ver t-1 ] ▷ v in t = concat v in 0 , [v out k , v ver k ] t-1 k=0</formula><p>Model output (v out t ) { "current observation": "Hotel 32One is on my right behind; I observe an intersection", "current instruction": "Turn right to face north.", "action": "turn_direction(north)", } Verifier Output (v ver t ) Correct solution.</p><p>▷ Move to the next scene upon correct action Figure <ref type="figure" target="#fig_0">13</ref>: An example of our prompt update for constructing v in t+1 using v in t , v out t and v ver t . The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively. Title Suppressed Due to Excessive Size System Prompt (v in 0 ) [Task Description] You are an expert in navgation. You will receive a sequence of instructions to follow. You are also provided with your observation and action histroy in text. Your goal is to first analyze the instruction and identify the next sentence to be executed. Then, you need to provide the action to be taken based on the current observation and instruction.</p><p>[Instruction] 1. First, turn left to face east.</p><p>2. Move forward until you reach the next intersection where Hotel 32One is on your right behind. 3. Turn right to face north. 4. Move forward until you reach the next intersection where Dragon Gate Chinatown SF is on your right front. 5. Turn left to face east. 6. Move forward until the destination Café de la Presse is on your right. [Action space] -"forward()": indicates moving forward for 1 step; -"turn_direction(x)": indicates turn direction to the target heading, where x∈['north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest']; -"stop()": indicates the navigation is finished; [Observations and actions sequence] O_1: No landmarks nearby; A_1: turn_direction(east) O_2: No landmarks nearby; A_2: forward() O_3: No landmarks nearby; A_3: forward() O_4: Hotel 32One is on your right behind; You observe an intersection A_4: [Output]</p><p>Your response should be a valid json file in the following format: { "current observation": latest observation from the street view grid, "current instruction": analyze the full instruction and identify the sentence to be executed, "action": the action to be taken chosen from the action space, }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appending model and verifier outputs to obtain</head><formula xml:id="formula_4">v in t v in t = [v out 0 , v ver 0 , v out 1 , v ver 1 , . . . , v out t-1 , v ver t-1 ] ▷ v in t = concat v in 0 , [v out k , v ver k ] t-1 k=0</formula><p>Model output (v out t ) { "current observation": "Hotel 32One is on my right behind; I observe an intersection", "current instruction": "Turn right to face north.", "action": "turn_direction(north)", } Verifier Output (v ver t ) Correct solution.</p><p>▷ Move to the next scene upon correct action Figure <ref type="figure" target="#fig_4">14</ref>: An example of our prompt update for constructing v in t+1 using v in t , v out t and v ver t . This example provides an optional vision input for VLMs, adding a visual recognition challenge. The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Setup</head><p>This section details the experimental setup used in Section 5. We first describe our data collection setup for supervised fine-tuning (Appendix C.1). Then, we present the training pipeline (Appendix C.2). Finally, we describe our evaluation metrics and the statistical tools used for generating plots (Appendix C.3).</p><p>C.1. Data SFT data collection. As illustrated in Figures 11 to 14, GeneralPoints and V-IRL environments naturally align with prompt-response dialogue structures. We create training samples by pairing each system prompt with its corresponding expert response. All SFT experiments in the main body use optimal single-turn prompt-response pairs, without any verification or revision steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SFT on sub-optimal trajectories</head><p>To examine how more diverse SFT data affects the out-of-distribution performance of SFT, we conduct an ablation study on GP-L using sub-optimal trajectories as training data. Unlike expert prompt-response pairs, these sub-optimal trajectories include errors and verification messages in their prompts. This format aligns with evaluation scenarios where multiple verification iterations are allowed, similar to the data being used for the downstream RL training. In Figure <ref type="figure" target="#fig_11">15</ref>, we observe that SFT still merely memorizes the training data with degraded out-of-distribution performance. This evidence suggests that memorization occurs due to the fundamental nature of SFT training rather than the SFT data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Training Pipeline</head><p>As illustrated in Section 5, we follow the training pipeline by RL4VLM <ref type="bibr">(Zhai et al., 2024a)</ref>, where we first initial-ize the model with SFT, then separately scale up the compute for SFT and RL <ref type="bibr" target="#b37">(Schulman et al., 2017)</ref>, starting from this initialized model. For all experiments of SFT and RL in the main body, we tune all components using a shared learning rate per experiment. All training experiments are conducted on an 8 H800 machine (80GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Evaluation Metric</head><p>Per-step accuracy. We report the per-step accuracy for V-IRL-VL task in Figures <ref type="figure">5</ref> and <ref type="figure">6</ref>. An individual step is considered correct when the model's chosen action matches the expert trajectory at that position. Note that intermediate verification steps are counted as independent samples here.</p><p>Success rate. We report the success rate (%) of GP-L, GP-VL, V-IRL-L and V-IRL-VL in Figures <ref type="figure">5</ref> and <ref type="figure">6</ref>. In the GeneralPoints task, success is defined as succeeding at least once during the inference time verification. In the V-IRL task, a sample is recorded as success when the model takes correct action at each movable point on the route.</p><p>Computation estimation. We estimate the FLOPs for training X following the similar manner of <ref type="bibr" target="#b39">(Snell et al., 2024;</ref><ref type="bibr" target="#b20">Hoffmann et al., 2023)</ref>, where Note that the used on-policy RL algorithm PPO <ref type="bibr" target="#b37">(Schulman et al., 2017)</ref> contains iterative stages of replay buffer collection and optimization, hence requiring additional inference computation. For simplicity, we approximate the term via:</p><formula xml:id="formula_5">X</formula><formula xml:id="formula_6">D buf f er ≈ E di do D RL • D RL = λD RL</formula><p>where E ∈ N denotes the number of auto-regressive generation processes, di , do denote average input tokens and output tokens. We estimate the λ for GeneralPoints and V-IRL as 6 and 5.1 respectively after calculation.</p><p>Line smoothing and error bar. All line plots in our paper adopt Savitzky-Golay filter with polynomial order 3 as smoothing function. We assume each evaluated data point</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A comparative study of RL and SFT on the visual navigation environment V-IRL (Yang et al., 2024a) for OOD generalization. OOD curves represent performance on the same task, using a different textual action space. See detailed descriptions of the task in Section 5.1.</figDesc><graphic coords="2,60.18,67.68,224.70,145.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of the sequential revision formulation with a verifier. The model generate the next answer v out t+1 conditioned on all previous answers and information (v out i , v ver t , 0 ≤ i ≤ t) from the verifier.</figDesc><graphic coords="4,80.99,27.02,179.52,179.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>⭐</head><figDesc>First, turn slightly right towards the northeast and walk a short distance until you reach the next intersection, where you'll see The Dutch on your right. Next, make a sharp left turn to head northwest. Continue for a while until you reach the next intersection, where Lola Taverna will be on your right. Finally, turn slightly right to face northeast and walk a short distance until you reach your destination, Shuka, which will be on your right. northeast." [OBSERVATION] "See Lola Taverna on my right." [ACTION] "Left turn to northwest." [OBSERVATION] "See Shuka on my right." [ACTION] "Stop." [OBSERVATION] "See The Dutch on my right." [ACTION] "Left turn to northwest."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Demonstration of one navigation task in V-IRL. Agent navigates from place to place following the given linguistic navigation instructions in V-IRL. The navigation procedure is shown at the top, with the navigation instructions displayed below.Visual observation-related information is highlighted in green, while action-related information is marked in orange.</figDesc><graphic coords="5,82.29,80.39,427.64,273.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure5: Success rate (%) -GFLOPs trendlines for RL and SFT on GeneralPoints and V-IRL. The top row shows in-distribution performance, while the bottom row shows out-of-distribution performance. Results are presented for both pure language (-L) and vision-language (-VL) variants of each task. For GeneralPoints, we report the episode success rate, while for V-IRL, we report per-step accuracy with overall success rate in Figures1 and 18. Detailed evaluation setups (and curve smoothing) are provided in Appendix C.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Comparison of out-of-distribution performance under visual variants. Similar to Figures 5 and 6, we present both the performance dynamics (shown as lines) and final performance (shown as bars) for visual out-of-distribution evaluations. The previous state-of-the-art on V-IRL VLN mini benchmark (Yang et al., 2024a) is marked in orange. Detailed evaluation setups (and curve smoothing) are provided in Appendix C.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>2), we consider a natural follow-up question: How does RL affect VLMs' visual capabilities? To study this question, we conducted additional ablation studies in the GP-VL environment to investigate the OOD performance of RL and SFT, along with the model's visual recognition accuracy, in terms of recognizing the 4 cards from the input image.In particular, we study how scaling post-training compute via RL/SFT both affects generalization in rule-based OOD (Figure8left), and visual recognition accuracy and visual OOD (Figure 8 right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: RL experiments on GP-L without SFT initialization. All trials fail due to poor instruction following capability of the base model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: In-distribution vs. OOD performance growth on GP-L. We record RL experiments with different number of verification iterations (VIter) as scaling up training compute (color transparency).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>•</head><figDesc>: The Dutch, Lola Taverna • Straight road: Roads connecting turning points, starting point, and destination • Street views: 360-degree panoramic views at each movable point • Oracle information: Expert observation data for each movable point • Expert trajectory • Instruction Although the instructions in Figures 4, 13 and 14 are presented in different formats, they convey equivalent information, with Figure 4 using natural language. Simplification and arguments. We simplify the original V-IRL design from Yang et al. (2024a) to better accommodate RL training. The modifications include eliminating the 2-stage navigation pipeline that required a separate visual detector for street view processing, and removing online queries to reduce training time and cost. Our V-IRL environment contains 2 additional configuration arguments compared with the original design: • Action space: two options -Absolute direction: "turn_direction(x)" where x∈{'north', 'northeast', Title Suppressed Due to Excessive Size System Prompt (v in 0 ) [Task Description]You are an expert in navigation. You will receive a sequence of instructions to follow while observing your surrounding street views. You are also provided with your observation and action history in text. your goal is to take the action based on the current observation and instruction.[Instruction] 1. First, turn left to face east. 2. Move forward until you reach the next intersection where Hotel 32One is on your right behind. 3. Turn right to face north. 4. Move forward until you reach the next intersection where Dragon Gate Chinatown SF is on your right front. 5. Turn left to face east. 6. Move forward until the destination Café de la Presse is on your right. [Current observation] You observe a 2x2 grid of street view images with the following headings: [front, right back, left] You need to identify if any of the landmarks in the instruction are visible in the street view grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>Figure15: SFT experiments on GP-L with suboptimal trajectories. Similar to results in Figure5, SFT overfits the training data even we increase the trajectory diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>train = 6N D train and X inf erence = 2N D inf erence . Here, N represents the model parameters and D train represents the number of tokens during training. Suppose our SFT and RL experients starts from a checkpoint trained on D init tokens, we can estimate the training computation of SFT and RL via the following equations:X SF T = 6N (D init + D SF T ) X RL = 6N (D init + D RL ) + 2N D buf f er</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use "memorization" the refer a model's capacity to generate near-exact copies of training examples when prompted based on information present in the training dataset. This definition explicitly excludes bit-wise or code-wise replication of training data within the model itself.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The visual input can be parsed into pure text description, see more details in<ref type="bibr" target="#b51">Yang et al. (2024a)</ref> and an illustration of pure text the version in Figure14.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>See Figure</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>4, the model needs to recognize landmarks likeThe Dutch, Lola Taverna, and Shuka from the visual observation, and relate these landmarks with the textual instructions for taking the right action.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>These NYC routes in the VLN mini benchmark do not overlap with our training data.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>YZ would like to thank <rs type="person">Xiaoxuan Feng</rs> for beautifying Figure 4. We would like to thank <rs type="person">Druv Pai</rs>, <rs type="person">Jincheng Mei</rs>, and <rs type="person">Doina Precup</rs> for feedbacks on earlier manuscripts. This research was partially supported by the <rs type="funder">National Science Foundation</rs> under <rs type="grantNumber">IIS-2246811</rs>, the <rs type="funder">ONR</rs> grant <rs type="grantNumber">N00014-22-1-2102</rs>, the <rs type="funder">Hong Kong Center for Construction Robotics Limited (HKCRC)</rs> Award <rs type="grantNumber">052245</rs>, the <rs type="funder">HKU</rs> startup fund.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2wJpVPZ">
					<idno type="grant-number">IIS-2246811</idno>
				</org>
				<org type="funding" xml:id="_g45bKzg">
					<idno type="grant-number">N00014-22-1-2102</idno>
				</org>
				<org type="funding" xml:id="_JwKcxeg">
					<idno type="grant-number">052245</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>System Prompt (v in 0 ) [Task Description] You are an expert 24 points card game player. You are observing these four cards in the image. Note that 'J', 'Q', and 'K' count as '10', and each card must be used once. Your goal is to output a formula that evaluates to 24 using numbers from the cards and operators such as '+', '-', '*', '/', '(', ')', and '='.</p><p>[Output] Your response should be a valid json file in the following format: { "cards": [x, y, z, w], where 'J', <ref type="bibr">'Q', and 'K' count as '10', "number": [a, b, c, d]</ref>, where a, b, c, and d are the numbers on the cards, "formula": "an equation that equals 24", }</p><p>Appending model and verifier outputs to obtain v in</p><p>Model output (v out t ) { "cards": ['A', '3', 'K', '6'], "number": [1, <ref type="bibr">3,</ref><ref type="bibr">13,</ref><ref type="bibr">6]</ref>, "formula": "(1+6)*3+13=24", } Verifier Output (v ver t ) You failed this trial because your formula is incorrect.</p><p>System Prompt (v in 0 ) [Task Description] You are an expert 24 points card game player. You are observing these four cards in the image. Note that 'J', <ref type="bibr">'Q', and 'K' count as '11', '12', and '13' respectively,</ref> and each card must be used once. Your goal is to output a formula that evaluates to 24 using numbers from the cards and operators such as '+', '-', '*', '/', '(', ')', and '='.</p><p>[Input] Cards: ['A', <ref type="bibr">'3', 'K', '6']</ref> [Output] Your response should be a valid json file in the following format: { "cards": [x, y, z, w], where 'J', <ref type="bibr">'Q', and 'K' count as '10', "number": [a, b, c, d]</ref>, where a, b, c, and d are the numbers on the cards, "formula": "an equation that equals 24", } Appending model and verifier outputs to obtain <ref type="bibr">'3', 'K', '6']</ref>, "number": [1, <ref type="bibr">3,</ref><ref type="bibr">13,</ref><ref type="bibr">6]</ref>, "formula": "(1+6)*3+13=24", } Verifier Output (v ver t ) You failed this trial because your formula is incorrect.</p><p>Figure <ref type="figure">12</ref>: An example of our prompt update for constructing v in t+1 using v in t , v out t and v ver t . This example provides an optional vision input for VLMs, adding a visual recognition challenge. The brown parts marks the task and related information, and the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively. 'east', 'southeast', 'south', 'southwest', 'west', 'northwest'}, "forward()", "stop()" -Relative direction:</p><p>"turn_direction(x)" where x∈{'left', 'right', 'slightly left', 'slightly right'}, "forward()", "stop()"</p><p>• Maximum straight road length: any positive integer</p><p>The action space argument accommodates the rule variants described in Section 4. For experiments shown in Figure <ref type="figure">5</ref>, we use absolute direction action space during training and in-domain evaluation, while using the alternative rule for out-of-domain evaluation. We implement a maximum straight road length to limit the number of movable coordinates between turning points, preventing sequences of repetitive "forward()" actions. We conduct visual distribution shift experiments (Section 5.2) via training the model on New York City regions and evaluating the out-ofdomain performance on the worldwide navigation routes from the benchmark released by <ref type="bibr" target="#b51">Yang et al. (2024a)</ref>.</p><p>Reward design. An episode terminates when either the navigation agent stops at the destination or the maximum verification step of 2 is reached. The reward function is as follows:</p><p>• r = 1: For generating a correct action at the current coordinate</p><p>• r = -1: For generating wrong action at the current coordinate</p><p>• r = -1: For exceeding maximum verification step</p><p>• r = -1.5: For failed detection of landmarks Title Suppressed Due to Excessive Size Training Computation (GFLOPs)</p><p>Success Rate (%)</p><p>Figure <ref type="figure">16</ref>: Ablation studies on GeneralPoints-VL SFT. We ablate the learning rate and report the in-distribution episode success rate (%) of all experiments. None of the experiments shows an increasing trend beyond 30% success rate.</p><p>follows a binomial distribution and approximate the standard error using P (1-P )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>, where P is the demical success rate and N is the number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experimental Results</head><p>In this section, we provide additional experimental results that are not covered in the main body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Ablation Studies on GP-VL</head><p>As mentioned in Section 6, we observe an abnormal phenomenon that SFT fails to achieve comparable indistribution performance with RL (see Figure <ref type="figure">5</ref> subplot row 1 column 3). To further explore this, we conduct ablation studies over different hyperparameter choices.</p><p>SFT. We ablate the hyperparameter choices under the same task setting of GP-VL in Section 5.1. For experiments fine-tuning all parameters, we search learning rates from {1×10 -4 , 1×10 -4 , 1×10 -5 , 1×10 -6 , 5×10 -7 , 1× 10 -7 }. Freezing the vision encoder, we search learning rates {1 × 10 -6 , 1 × 10 -7 }. Freezing vision encoder and adapter, we search learning rates {1 × 10 -6 , 5 × 10 -7 , 1 × 10 -7 }. We provide the in-distribution success rate curve in Figure <ref type="figure">16</ref>.</p><p>RL. Finding suitable hyperparameters for RL experiments requires minimal effort. We conduct a search over learning rates 2 × 10 -6 , 1 × 10 -6 , with the in-distribution success rate curves shown in Figure <ref type="figure">17</ref>. All parameters are tunable in our RL experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. More results on V-IRL-VL</head><p>Echoing per-step accuracy results in Figure <ref type="figure">5</ref>, we report the overall success rate of V-IRL-VL in Figure <ref type="figure">18</ref>. Due to the task's complexity, both training methods achieve overall success rates no higher than 1%. For V-IRL, the overall success rate is a significantly more demanding metric since it aggregates per-step errors. For example, a random policy achieving 10% per-step accuracy would achieve achieve only approximately 10 -8 % success rate on enough routes averaging 10 steps in length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Failure Cases</head><p>In this section, we present 2 failure cases in our experiments as mentioned in Sections 5.4 and 6.</p><p>Without SFT, RL fails. In Figure <ref type="figure">9</ref>, we present the training dynamics of failed RL experiments without SFT initialization. We additionally provide output examples of these experiments in Figure <ref type="figure">20</ref>, where the model tends to generate unstructured response and fail.</p><p>RL cannot save overfitted checkpoints. As shown in Figure <ref type="figure">19</ref>, RL cannot recover the out-of-distribution performance when initialized from a extremely overfitted checkpoint that has an initial per-step accuracy of less than 1%. We additionally provide an output example in Figure <ref type="figure">19</ref>, where the model fails to adjust to the new rule. System Prompt (v in 0 ) [Task Description] You are an expert 24 points card game player. You are observing these four cards in the image. Note that 'J', 'Q', and 'K' count as '11', '12', and '13' respectively, and each card must be used once. Your goal is to output a formula that evaluates to 24 using numbers from the cards and operators such as '+', '-', '*', '/', '(', ')', and '='.</p><p>[Input] Cards: ['2', 'J', '3', 'K']</p><p>[Output] Your response should be a valid json file in the following format: { "cards": [x, y, z, w], where 'J', 'Q', and 'K' count as '10', "number": <ref type="bibr">[a, b, c, d]</ref>, where a, b, c, and d are the numbers on the cards, "formula": "an equation that equals 24", }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appending model and verifier outputs to obtain</head><p>To solve this problem, we can use a brute force approach by generating all possible combinations of the cards and checking if any of them can be used to form an equation that equals 24.</p><p>Here is a Python solution for this problem: "'python def calculate_formula(cards): # Define the mapping of cards to their corresponding numbers . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verifier Output (v ver</head><p>t ) You failed this trial because your formula is incorrect.</p><p>Figure <ref type="figure">20</ref>: Example model outputs without SFT initialization. We record model responses using prompts similar to the one shown in Figure <ref type="figure">11</ref>. The results demonstrate that Llama-3.2-Vision-11B fails to follow instructions properly. We omit the long response which tries to solve the puzzle via code but fails to finish within finite context length.</p><p>Title Suppressed Due to Excessive Size System Prompt (v in 0 ) [Task Description] You are an expert in navigation. You will receive a sequence of instructions to follow while observing your surrounding street views. You are also provided with your observation and action history in text. Your goal is to first analyze the instruction and identify the next sentence to be executed. Then, you need to provide the action to be taken based on the current observation and instruction.</p><p>[Instruction] 1. First, turn right to face northwest. [Action space] "forward()": indicates moving forward one step "turn_direction(x)": indicates adjust the ego agent direction towards x direction. x could be any following ['left', 'right', 'slightly left', 'slightly right'] "stop()": indicates the navigation is finished.</p><p>[Observations and actions sequence] O_1: You observe an image of 4 views; A_1:</p><p>[Output] { "current observation": latest observation from the streetview grid, "current instruction": analyze the full instruction and identify the sentence to be executed, "action": the action to be taken chosen from the action space, } Appending model and verifier outputs to obtain v in</p><p>Failed model output (v out t ) { "current observation": "No landmarks nearby", "current instruction": "Turn right to face northwest.", "action": "turn_direction(northwest)", } Verifier Output (v ver t ) Incorrect action.</p><p>▷ Fail this sample. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abdulhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><surname>Gym</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.18232</idno>
		<title level="m">Benchmarks for multi-turn reinforcement learning with language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reinforcement learning: Theory and algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>UW</publisher>
			<pubPlace>Seattle, Seattle, WA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>CS Dept.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<ptr target="https://physics.allen-zhu.com/.2" />
		<title level="m">ICML 2024 Tutorial: Physics of Language Models</title>
		<imprint>
			<date type="published" when="2024-07">July 2024</date>
		</imprint>
	</monogr>
	<note>Project page</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 3.1, knowledge storage and extraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14316</idno>
		<imprint>
			<date type="published" when="2023">2023a. 1, 2</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14402</idno>
		<title level="m">Physics of language models: Part 3.2, knowledge manipulation</title>
		<imprint>
			<date type="published" when="2023-02">2023b. 2</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 3.3, knowledge capacity scaling laws</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.05405</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Algorithmic stability and generalization performance</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Quantifying memorization across neural language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07646</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">AlphaMath almost zero: Process supervision without process</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2405.03553</idno>
		<imprint>
			<date type="published" when="2024-02">2024a. 2</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unlock the correlation between supervised fine-tuning and reinforcement learning in training code large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10305</idno>
		<imprint>
			<date type="published" when="2024-02">2024b. 2</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12793</idno>
		<title level="m">ShareGPT4V: Improving large multi-modal models with better captions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning</title>
		<author>
			<persName><surname>Deepseekai</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2501.12948.1" />
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The Llama 3 Herd of models</title>
		<imprint>
			<date type="published" when="2007">2024. 1, 5, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17179</idno>
		<title level="m">AlphaZero-like tree-search can guide large language model decoding and training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introducing Gemini: Our largest and most capable AI model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Google</surname></persName>
		</author>
		<ptr target="https://blog.google/technology/ai/google-gemini-ai/.1,2" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07625</idno>
		<title level="m">What images are more memorable to machines? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D L</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training verifiers for selftaught reasoners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><surname>V-Star</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=stmqBSW2dV.3" />
	</analytic>
	<monogr>
		<title level="m">First Conference on Language Modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jaech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Helyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.16720</idno>
		<title level="m">OpenAI o1 system card</title>
		<imprint>
			<date type="published" when="2024">2024. 1, 2, 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Investigating data contamination for pre-training language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.06059</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.07681</idno>
		<title level="m">What do learning dynamics reveal about generalization in LLM reasoning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Brave: Broadening the visual encoding of vision-language models</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">F</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Poklukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulshrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="113" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improved baselines with visual instruction tuning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03744</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://llava-vl.github.io/blog/2024-01-30-llava-next/" />
		<title level="m">LLaVA-NeXT: Improved reasoning, ocr, and world knowledge</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Math-Vista: Evaluating mathematical reasoning of foundation models in visual contexts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">GPT-4</title>
		<ptr target="https://openai.com/research/gpt-4.7" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">OpenAI. GPT-4 technical report. arXiv</title>
		<imprint>
			<date type="published" when="2023">2023b. 1, 2</date>
			<biblScope unit="page" from="2303" to="08774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS, 2022. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Quantifying generalization complexity for large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.01769</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vision language models are blind</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rahmanzadehgervi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Taesiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=8aHzds2uUyB.2" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rewarding progress: Scaling automated process verifiers for LLM reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.08146</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Scaling LLM testtime compute optimally can be more effective than scaling model parameters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03314</idno>
		<imprint>
			<date type="published" when="2024">2024. 1, 2, 3, 18</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aligning large multimodal models with factually augmented RLHF</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.findings-acl.775</idno>
		<ptr target="https://aclanthology.org/2024.findings-acl.775.2" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2024</title>
		<editor>
			<persName><forename type="first">L.-W</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting><address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024-08">August 2024</date>
			<biblScope unit="page" from="13088" to="13110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Toward self-improvement of LLMs via imagination, searching, and criticizing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.12253</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Cambrian-1: A fully open, vision-centric exploration of multimodal LLMs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Middepogu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2024" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Metamorph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.14164</idno>
		<title level="m">Multimodal understanding and generation via instruction tuning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mass-producing failures of multimodal systems with language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2024-03">2024c. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Eyes wide shut? Exploring the visual shortcomings of multimodal LLMs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024-03">2024d. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Generalization vs memorization: Tracing language models&apos; capabilities back to pretraining data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amayuelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.14985</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=gEZrGCozdqR.2" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022-02">2022b. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Grounding virtual intelligence in real life</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><surname>V-Irl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024a. 1, 2, 3, 4, 6, 7, 13</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.14171</idno>
		<title level="m">Thinking in space: How multimodal large language models see, remember, and recall spaces</title>
		<imprint>
			<date type="published" when="2024-03">2024b. 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learn what you can and memorize the rest</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Resmem</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HFQFAyNucq.2" />
	</analytic>
	<monogr>
		<title level="m">Thirtyseventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 2.1, grade-school math and the hidden reasoning process</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2407.20311</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">MMMU-Pro: A more robust multi-discipline multimodal understanding benchmark</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.02813</idno>
		<imprint>
			<date type="published" when="2024-03">2024b. 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">STaR: Bootstrapping reasoning with reasoning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="15476" to="15488" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Finetuning large vision-language models as decision-making agents via reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>id=nBjmMF2IZU. 1, 2, 3, 5</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Investigating the catastrophic forgetting in multimodal large language model fine-tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Parsimony and Learning</title>
		<imprint>
			<date type="published" when="2024-03">2024b. 3</date>
			<biblScope unit="page" from="202" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Counterfactual memorization in neural language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39321" to="39362" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Law of the weakest link: Cross capabilities of large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.19951</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">LIMA: Less is more for alignment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024-02">2024a. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zanette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Archer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.19446</idno>
		<title level="m">Training language model agents via hierarchical multi-turn RL</title>
		<imprint>
			<date type="published" when="2024-02">2024b. 2</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07064</idno>
		<title level="m">Large language models can learn rules</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<title level="m">Finetuning language models from human preferences</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
