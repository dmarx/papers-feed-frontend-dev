The research paper you provided presents a comparative study of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in the context of foundation models, focusing on their effects on generalization and memorization. Below is a detailed technical explanation of the researchers' decisions regarding key concepts, generalization versus memorization, evaluation tasks, methodology, performance metrics, visual generalization, sequential revision, standard RL notation, contributions of SFT to RL, and implications for AI systems.

### Key Concepts

1. **Supervised Fine-Tuning (SFT)**:
   - **Memorization**: SFT is characterized by its tendency to memorize the training data. This occurs because SFT optimizes the model to minimize the loss on the training set, which can lead to overfitting, especially when the training data is limited or not diverse enough.
   - **Out-of-Distribution Generalization**: The researchers note that SFT struggles with generalization to unseen data distributions. This limitation is critical in real-world applications where models encounter data that differ from their training sets.

2. **Reinforcement Learning (RL)**:
   - **Generalization**: RL, particularly when using outcome-based rewards, is shown to generalize better than SFT. This is because RL focuses on learning policies that maximize cumulative rewards, allowing the model to adapt to new situations based on learned experiences rather than memorized data.
   - **Visual Recognition**: The study highlights that RL enhances visual recognition capabilities, which is essential for tasks that involve interpreting visual inputs.

### Generalization vs. Memorization

- The researchers emphasize the distinction between generalization and memorization:
  - **SFT**: Primarily memorizes training data, leading to poor performance on out-of-distribution tasks.
  - **RL**: Learns generalizable rules that can be applied to unseen variants, demonstrating a more robust understanding of the underlying principles of the tasks.

### Evaluation Tasks

1. **GeneralPoints**:
   - A card game designed to assess arithmetic reasoning. The model must compute a target number using the values of four cards, which can be presented as text or images. This task evaluates both textual and visual reasoning capabilities.

2. **V-IRL**:
   - A real-world navigation task that tests spatial reasoning. This task is crucial for understanding how well the model can generalize its learned policies to real-world scenarios.

### Methodology

- The researchers adopt a multi-step RL framework initiated after SFT. This approach allows the model to leverage the structured output from SFT while refining its decision-making through RL.
- **Sequential Revision**: The methodology incorporates a sequential revision formulation for state-action transitions, where the input at each time step includes previous outputs. This design enables the model to build on its prior knowledge and correct mistakes iteratively.

### Performance Metrics

- The study reports a significant improvement in performance metrics, specifically a +33.8% increase on the V-IRL mini benchmark (from 44.0% to 77.8%). This improvement underscores the effectiveness of RL in enhancing model generalization.
- The researchers also note that scaling inference time (i.e., increasing the number of maximal steps) contributes to better generalization, suggesting that more computational resources can lead to more thorough exploration of the state space.

### Visual Generalization

- The findings indicate that RL models generalize to visual out-of-distribution tasks, while SFT models struggle. This is particularly important for applications that require visual understanding, as RL's outcome-based reward functions improve visual capabilities.

### Sequential Revision

- The sequential revision process is crucial for maintaining context and continuity in the model's reasoning. By concatenating prior outputs with the current input, the model can leverage its history to make more informed decisions.

### Standard RL Notation

- The researchers utilize standard RL notation to formalize their approach, defining the state space, action space, and reward function. This formalism is essential for understanding the RL framework and its application to the tasks at hand.

### Contributions of SFT to RL

- Despite SFT's limitations in generalization, the researchers highlight its role in stabilizing the model's output format. This stabilization is critical for effective RL training, as it provides a consistent foundation upon which RL can build.

### Implications for AI Systems

- The study emphasizes the importance of understanding the balance between memorization and generalization in AI systems. This understanding is crucial for developing robust models that can perform well in diverse and dynamic environments.

### Conclusion

The researchers' decisions throughout the study are grounded in a thorough understanding of the strengths and weaknesses of SFT and RL. By systematically evaluating their effects on generalization and memorization through carefully designed tasks and methodologies, the study contributes valuable insights into the development of more effective AI systems. The findings advocate for a hybrid approach that leverages the benefits of both SFT and RL, ultimately leading to models that are better equipped to handle real-world challenges.