Hereâ€™s a detailed technical explanation and rationale for the researchers' decisions regarding the various aspects of their study on the comparative effects of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on generalization and memorization in foundation models:

### 1. Decision to Compare SFT and RL as Post-Training Techniques
The researchers aimed to understand the distinct impacts of SFT and RL on model generalization. SFT is known for adapting models to specific tasks, often leading to memorization of training data, while RL is designed to optimize decision-making through interaction with an environment, potentially leading to better generalization. By comparing these two techniques, the researchers could isolate their effects on generalization and memorization, providing insights into their respective strengths and weaknesses.

### 2. Choice of GeneralPoints and V-IRL as Evaluation Tasks
GeneralPoints was selected as it evaluates arithmetic reasoning, a fundamental cognitive skill that can reveal how well models generalize learned rules to novel situations. V-IRL, on the other hand, assesses spatial reasoning in a real-world navigation context, which is crucial for understanding how models perform in dynamic environments. Together, these tasks cover both textual and visual domains, allowing for a comprehensive evaluation of generalization capabilities.

### 3. Selection of Arithmetic Reasoning and Visual Navigation as Focus Areas
Arithmetic reasoning is a clear indicator of a model's ability to apply learned mathematical principles, while visual navigation tests the model's understanding of spatial relationships and real-world contexts. These areas were chosen to reflect the diverse challenges that foundation models face, ensuring that the evaluation is robust and relevant to practical applications.

### 4. Adoption of a Multi-Step RL Framework after SFT
The multi-step RL framework was adopted to leverage the structured output from SFT, which stabilizes the model's responses. This approach allows the model to refine its decision-making process through iterative interactions, enhancing its ability to generalize from the learned tasks. The sequential nature of this framework also facilitates the exploration of complex decision-making scenarios.

### 5. Implementation of Outcome-Based Reward Functions in RL
Outcome-based reward functions were implemented to provide clear feedback on the model's performance. This approach encourages the model to focus on achieving specific goals rather than merely following predefined actions, promoting the learning of generalizable strategies that can adapt to new situations.

### 6. Use of Sequential Revision Formulation for State-Action Transitions
The sequential revision formulation allows the model to build upon previous outputs, creating a context-aware decision-making process. This method enhances the model's ability to learn from its mistakes and adjust its actions based on past experiences, which is crucial for improving generalization in complex tasks.

### 7. Decision to Analyze Generalization in Both Textual and Visual Domains
By analyzing generalization across both textual and visual domains, the researchers aimed to provide a holistic view of the model's capabilities. This dual focus helps to identify whether the observed generalization effects are consistent across different types of input, which is essential for understanding the model's overall robustness.

### 8. Choice to Scale Up Inference-Time Compute for Improved Performance
Scaling up inference-time compute was based on findings that increased computational resources can lead to significant performance improvements. This decision reflects the understanding that more complex models can benefit from additional processing power, particularly in tasks requiring intricate reasoning or visual recognition.

### 9. Decision to Investigate the Impact of SFT on RL Training Stability
Investigating the impact of SFT on RL training stability was crucial to understanding how initial fine-tuning affects subsequent learning. The researchers hypothesized that SFT could provide a stable foundation for RL, reducing the risk of erratic behavior during training and enhancing overall performance.

### 10. Selection of Specific Metrics for Evaluating Generalization and Memorization
The choice of metrics was guided by the need to quantitatively assess the model's performance in terms of generalization and memorization. Metrics that capture the model's ability to apply learned knowledge to unseen data were prioritized, ensuring that the evaluation was aligned with the study's objectives.

### 11. Decision to Conduct Experiments in Both Unimodal and Multimodal Settings
Conducting experiments in both unimodal (text-only) and multimodal (text and visual) settings allows for a comprehensive assessment of the models' capabilities. This approach helps to identify how well the models generalize across different types of inputs and whether their performance varies significantly between modalities.

### 12. Choice of Visual Encoders and Data Curation Strategies for VLMs
The selection of visual encoders and data curation strategies was aimed at enhancing the model's visual perception capabilities. By using high-quality visual data and effective encoders, the researchers sought to ensure that the VLMs could accurately interpret and respond to visual inputs, which is critical for tasks like V-IRL.

### 13. Decision to Analyze the Role of Inference-Time Verification in RL Generalization
Analyzing inference-time verification was important to understand how real-time feedback can influence the model's learning process. This decision reflects the researchers' interest in