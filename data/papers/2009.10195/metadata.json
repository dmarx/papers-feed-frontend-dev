{
  "arxivId": "2009.10195",
  "title": "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving\n  Out-of-Domain Robustness",
  "authors": "Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi",
  "abstract": "Models that perform well on a training domain often fail to generalize to\nout-of-domain (OOD) examples. Data augmentation is a common method used to\nprevent overfitting and improve OOD generalization. However, in natural\nlanguage, it is difficult to generate new examples that stay on the underlying\ndata manifold. We introduce SSMBA, a data augmentation method for generating\nsynthetic training examples by using a pair of corruption and reconstruction\nfunctions to move randomly on a data manifold. We investigate the use of SSMBA\nin the natural language domain, leveraging the manifold assumption to\nreconstruct corrupted text with masked language models. In experiments on\nrobustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently\noutperforms existing data augmentation methods and baseline models on both\nin-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews,\n1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.",
  "url": "https://arxiv.org/abs/2009.10195",
  "issue_number": 97,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/97",
  "created_at": "2025-01-05T08:24:59.493973",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-21T18:19:38.104Z",
  "main_tex_file": null,
  "published_date": "2020-09-21T22:02:33Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ]
}