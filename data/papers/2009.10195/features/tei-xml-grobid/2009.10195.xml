<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness</title>
				<funder ref="#_puQJNfb">
					<orgName type="full">Samsung Advanced Institute of Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">Province of Ontario</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-10-04">4 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Vector Institute</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Toronto Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Vector Institute</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Toronto Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto Vector Institute</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Toronto Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-04">4 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">D89C52E49637650F60C01387E299C92C</idno>
					<idno type="arXiv">arXiv:2009.10195v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Training distributions often do not cover all of the test distributions we would like a supervised classifier or model to perform well on. Often, this is caused by biased dataset collection <ref type="bibr" target="#b46">(Torralba and Efros, 2011)</ref> or test distribution drift over time <ref type="bibr" target="#b35">(Quionero-Candela et al., 2009)</ref>. Therefore, a key challenge in training machine learning models in these settings is ensuring they are robust to unseen examples. Since it is impossible to generalize to the entire distribution, methods often focus on the adjacent goal of out-of-domain robustness.</p><p>Data augmentation is a common technique used to improve out-of-domain (OOD) robustness by synthetically generating new training examples 1 Code is availble at <ref type="url" target="https://github.com/nng555/ssmba">https://github.com/ nng555/ssmba</ref> M x x</p><p>x Figure <ref type="figure">1</ref>: SSMBA moves along the data manifold M by using a corruption function to perturb an example x off the data manifold, then using a reconstruction function to project it back on. <ref type="bibr" target="#b41">(Simard et al., 1998)</ref>, often by perturbing existing examples in the input space <ref type="bibr" target="#b33">(Perez and Wang, 2017)</ref>. If data concentrates on a low-dimensional manifold <ref type="bibr" target="#b6">(Chapelle et al., 2006)</ref>, then these synthetic examples should lie in a manifold neighborhood of the original examples <ref type="bibr" target="#b7">(Chapelle et al., 2000)</ref>. Training models to be robust to such local perturbations has been shown to be effective in improving performance and generalization in semisupervised and self-supervised settings <ref type="bibr" target="#b1">(Bachman et al., 2014;</ref><ref type="bibr" target="#b44">Szegedy et al., 2014;</ref><ref type="bibr" target="#b37">Sajjadi et al., 2016)</ref>. When the underlying data manifold exhibits easy-to-characterize properties, as in natural images, simple transformations such as translation and rotation can quickly generate local training examples. However, in domains such as natural language, it is much more difficult to find a set of invariances that preserves meaning or semantics.</p><p>In this paper we propose Self-Supervised Manifold Based Data Augmentation (SSMBA): a data augmentation method for generating synthetic examples in domains where the data manifold is difficult to heuristically characterize. Motivated by the use of denoising auto-encoders as generative models <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>, we use a corruption function to stochastically perturb examples off the data manifold, then use a reconstruction function to project them back on (Figure <ref type="figure">1</ref>). This ensures new examples lie within the manifold neighborhood of the original example. SSMBA is applicable to any supervised task, requires no task-specific knowledge, and does not rely on class-or dataset-specific fine-tuning.</p><p>We investigate the use of SSMBA in the natural language domain on 3 diverse tasks spanning both classification and sequence modelling: sentiment analysis, natural language inference, and machine translation. In experiments across 9 datasets and 4 model types, we show SSMBA consistently outperforms baseline models and other data augmentation methods on both in-domain and OOD data.</p><p>2 Background and Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Augmentation in NLP</head><p>The problem of domain adaptation and OOD robustness is well established in NLP <ref type="bibr" target="#b3">(Blitzer et al., 2007;</ref><ref type="bibr" target="#b8">Daumé III, 2007;</ref><ref type="bibr" target="#b12">Hendrycks et al., 2020)</ref>. Existing work on improving generalization has focused on data augmentation, where synthetically generated training examples are used to augment an existing dataset. It is hypothesized that these examples induce robustness to local perturbations, which has been shown to be effective in semi-supervised and self-supervised settings <ref type="bibr" target="#b1">(Bachman et al., 2014;</ref><ref type="bibr" target="#b44">Szegedy et al., 2014;</ref><ref type="bibr" target="#b37">Sajjadi et al., 2016)</ref>.</p><p>Existing task-specific methods <ref type="bibr" target="#b14">(Kafle et al., 2017)</ref> and word-level methods <ref type="bibr" target="#b62">(Zhang et al., 2015;</ref><ref type="bibr" target="#b57">Xie et al., 2017;</ref><ref type="bibr" target="#b51">Wei and Zou, 2019)</ref> are based on human-designed heuristics. Back-translation from or through another language has been applied in the context of machine translation <ref type="bibr">(Rico Sennrich, 2016)</ref>, question answering <ref type="bibr" target="#b60">(Yu et al., 2018)</ref>, and consistency training <ref type="bibr" target="#b55">(Xie et al., 2019)</ref>. More recent work has used word embeddings <ref type="bibr" target="#b49">(Wang and Yang, 2015)</ref> and LSTM language models <ref type="bibr" target="#b11">(Fadaee et al., 2017)</ref> to perform word replacement. Other methods focus on fine-tuning contextual language models <ref type="bibr" target="#b18">(Kobayashi, 2018;</ref><ref type="bibr">Wu et al., 2019b;</ref><ref type="bibr" target="#b21">Kumar et al., 2020)</ref> or large generative models <ref type="bibr" target="#b0">(Anaby-Tavor et al., 2020;</ref><ref type="bibr" target="#b58">Yang et al., 2020;</ref><ref type="bibr" target="#b21">Kumar et al., 2020)</ref> to generate synthetic examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">VRM and the Manifold Assumption</head><p>Vicinal Risk Minimization (VRM) <ref type="bibr" target="#b7">(Chapelle et al., 2000)</ref> formalizes data augmentation as enlarging the training set support by drawing samples from a vicinity of existing training examples. Typically the vicinity of a training example is defined using dataset-dependent heuristics. For example, in com-Figure <ref type="figure">2</ref>: To sample from an MLM DAE, we apply the MLM corruption q to the original sentence then reconstruct the corrupted sentence using our DAE r.</p><p>puter vision, examples are generated using scale augmentation <ref type="bibr" target="#b42">(Simonyan and Zisserman, 2015)</ref>, color augmentation <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref>, and translation and rotation <ref type="bibr" target="#b41">(Simard et al., 1998)</ref>.</p><p>The manifold assumption states that high dimensional data concentrates around a low-dimensional manifold <ref type="bibr" target="#b6">(Chapelle et al., 2006)</ref>. This assumption allows us to define the vicinity of a training example as its manifold neighborhood, the portion of the neighborhood that lies on the data manifold. Recent methods have used the manifold assumption to improve robustness by moving examples towards a decision boundary <ref type="bibr" target="#b15">(Kanbak et al., 2018)</ref>, generating adversarial examples <ref type="bibr" target="#b44">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b25">Miyato et al., 2017)</ref>, interpolating between pairs of examples <ref type="bibr" target="#b61">(Zhang et al., 2018)</ref>, or finding affine transforms <ref type="bibr" target="#b32">(Paschali et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sampling from Denoising Autoencoders</head><p>A denoising autoencoder (DAE) is an autoencoder trained to reconstruct a clean input x from a stochastically corrupted one x ∼ q(x |x) by learning a conditional distribution P θ (x|x ) <ref type="bibr" target="#b48">(Vincent et al., 2008)</ref>. We can sample from a DAE by successively corrupting and reconstructing an input using the following pseudo-Gibbs Markov chain: x t ∼ q(x |x t-1 ), x t ∼ P θ (x|x t ). As the number of training examples increases, the asymptotic distribution π n (x) of the generated samples approximate the true data-generating distribution P (x) <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>. This corruptionreconstruction process allows for sampling directly along the manifold that P (x) concentrates on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Masked Language Models</head><p>Recent advances in unsupervised representation learning for natural language have relied on pretraining models on a masked language modeling (MLM) objective <ref type="bibr" target="#b9">(Devlin et al., 2018;</ref><ref type="bibr" target="#b23">Liu et al., 2019)</ref>. In the MLM objective, a percentage of the input tokens are randomly corrupted and the model is asked to reconstruct the original token given its left and right context in the corrupted sentence. We use MLMs as DAEs <ref type="bibr" target="#b22">(Lewis et al., 2019)</ref> to sample from the underlying natural language distribution by corrupting and reconstructing inputs (Figure <ref type="figure">2</ref>). We now describe Self-Supervised Manifold Based Data Augmentation. Let our original dataset D consist of pairs of input and output vectors D = {(x 1 , y 1 ) . . . (x n , y n )}. We assume the input points concentrate around an underlying lower dimensional data manifold M. Let q be a corruption function from which we can draw a sample x ∼ q(x |x) such that x no longer lies on M. Let r be a reconstruction function from which we can draw a sample x ∼ r(x|x ) such that x lies on M.</p><p>To generate an augmented dataset, we take each pair (x i , y i ) ∈ D and sample a perturbed x i ∼ q(x |x i ). We then sample a reconstructed xij ∼ r(x|x i ). A corresponding vector ŷij can be generated by preserving y i , or, since examples in the manifold neighborhood may cross decision boundaries on more sensitive tasks, by using a teacher model trained on the original data. This operation can be repeated to generate multiple augmented examples for each input example. These new examples form a dataset that we can augment the original training set with. We can then train an augmented model on the new augmented dataset.</p><p>In this paper we investigate SSMBA's use on natural language tasks, using the MLM training corruption function as our corruption function q and a pre-trained BERT model as our reconstruction model r. Different from other data augmentation methods, SSMBA does not rely on task-specific knowledge, requires no dataset-specific fine-tuning, and is applicable to any supervised natural language task. SSMBA requires only a pair of functions q and r used to generate data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>To empirically evaluate our proposed algorithm, we select 9 datasets -4 sentiment analysis datasets, 2 natural language inference (NLI) datasets, and 3 machine translation (MT) datasets. Table <ref type="table" target="#tab_0">1</ref> and Appendix A provide dataset summary statistics. All datasets either contain metadata that can be used to split the samples into separate domains or similar datasets that are treated as separate domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentiment Analysis</head><p>The Amazon Review Dataset <ref type="bibr" target="#b27">(Ni et al., 2019)</ref> contains product reviews from Amazon. Following Hendrycks et al. 2020, we form two datasets: AR-Full contains reviews from the 10 largest categories, and AR-Clothing contains reviews in the clothing category separated into subcategories by metadata. Since the reviews in AR-Clothing come from the same top-level category, the amount of domain shift is much less than that of AR-Full. Models predict a review's 1 to 5 star rating.</p><p>SST2 <ref type="bibr" target="#b43">(Socher et al., 2013)</ref>  2011), which contains full length movie reviews.</p><p>We call this pair the Movies dataset. Models predict a movie review's binary sentiment.</p><p>The Yelp Review Dataset contains restaurant reviews with associated business metadata which we preprocess following <ref type="bibr" target="#b12">Hendrycks et al. 2020</ref>. Models predict a review's 1 to 5 star rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Natural Language Inference</head><p>MNLI <ref type="bibr" target="#b52">(Williams et al., 2018)</ref> is a corpus of NLI data from 10 distinct genres of written and spoken English. We train on the 5 genres with training data and test on all 10 genres. Since the dataset does not include labeled test data, we use the validation set as our test set and sample 2000 examples from each training set for validation.</p><p>ANLI <ref type="bibr" target="#b28">(Nie et al., 2019)</ref> is a corpus of NLI data designed adversarially by humans such that stateof-the-art models fail to classify examples correctly. The dataset consists of three different levels of difficulty which we treat as separate textual domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Machine Translation</head><p>Following <ref type="bibr" target="#b26">Müller et al. 2019</ref>, we consider two translation directions, German→English (de→en) and German→Romansh (de→rm). Romansh is a low-resource language with an estimated 40,000 native speakers where OOD robustness is of practical relevance <ref type="bibr" target="#b26">(Müller et al., 2019)</ref>.</p><p>In the de→en direction, we use IWSLT14 de→en <ref type="bibr" target="#b4">(Cettolo et al., 2014)</ref> as a widely-used benchmark to test in-domain performance. We also use the OPUS <ref type="bibr" target="#b45">(Tiedemann, 2012)</ref> dataset to test OOD generalization. We train on highly specific in-domain data (medical texts) and disparate out-of-domain data (Koran text, Ubuntu localization files, movie subtitles, and legal text). Since domains share very little similarities in language, generalization to out-of-domain text is extremely difficult. In the de→rm direction, we use a training set consisting of the Allegra corpus <ref type="bibr" target="#b39">(Scherrer and Cartoni, 2012)</ref> and Swiss press releases. We use blog posts from Convivenza as a test domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Types</head><p>For sentiment analysis tasks, we investigate LSTMs <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> and convolutional neural networks (CNNs). For NLI tasks, we investigate fine-tuned RoBERTa BASE models <ref type="bibr" target="#b23">(Liu et al., 2019)</ref>, which are pretrained bidirectional transformers <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>. On both tasks, representations from the encoder are fed into an feed-forward neural network for classification. For MT tasks, we train transformers <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>. For all models, word embeddings are initialized randomly and trained end-to-end with the model. We do not initialize with pre-trained word embeddings to maintain consistency across all models and tasks. Model hyperparameters are tuned to maximize performance on in-domain validation data. Training details and hyperparameters for all models are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SSMBA Settings</head><p>For all experiments we use the MLM corruption function as our corruption function q. We tune tune the total percentage of tokens corrupted, leaving the percentages of specific corruption operations (80% masked, 10% random, 10% unmasked) the same. For sentiment analysis and NLI experiments we use a pre-trained RoBERTa BASE model as our reconstruction function r, and for translation experiments we use a pre-trained German BERT model <ref type="bibr" target="#b5">(Chan et al., 2020</ref>). For each input example, we generate 5 augmented examples using unrestricted sampling. For translation experiments, target side translations are generated with beam search with width 5. SSMBA hyperparameters, including augmented example labelling method and corruption percentage, are chosen based on in-domain validation performance. Hyperparameters for each dataset are provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>On sentiment analysis and NLI tasks, we compare against 3 data augmentation methods. Easy Data Augmentation (EDA) <ref type="bibr" target="#b51">(Wei and Zou, 2019)</ref> is a heuristic method that randomly replaces synonyms and inserts, swaps, and deletes words. Conditional Bert Contextual Augmentation (CBERT) <ref type="bibr">(Wu et al., 2019b)</ref> finetunes a class-conditional BERT model and uses it to generate sentences in a process similar to our own. Unsupervised Data Augmentation (UDA) <ref type="bibr" target="#b56">(Xie et al., 2020)</ref> translates data to and from a pivot language to generate paraphrases. We adapt UDA for supervised classification tasks by training directly on the backtranslated data.</p><p>On translation tasks, we compare only against methods which do not require additional target side monolingual data. Word dropout <ref type="bibr" target="#b40">(Sennrich et al., 2016)</ref> randomly chooses words in the source sentence to set to zero embeddings. Reward Augmented Maximum Likelihood (RAML) <ref type="bibr" target="#b29">(Norouzi et al., 2016)</ref> samples noisy target sentences based on an exponential of their Hamming distance from the original sentence. SwitchOut <ref type="bibr" target="#b50">(Wang et al., 2018)</ref> applies a noise function similar to RAML to both the source and target side. We use publicly available implementations for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Method</head><p>We train LSTM and CNN models with 10 random seeds, RoBERTa models with 5 random seeds, and transformer models with 3 random seeds. Models are trained separately on each domain then evaluated on all domains, and performance is averaged across seeds and test domains. We report the average in-domain (ID) and OOD performance across all train domains. On sentiment analysis and NLI tasks we report accuracy, and on translation we report uncased tokenized BLEU <ref type="bibr" target="#b31">(Papineni et al., 2002)</ref> for IWSLT and cased, detokenized BLEU with SacreBLEU 2 <ref type="bibr" target="#b34">(Post, 2018)</ref> for all others. Statistical testing details are in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sentiment Analysis</head><p>Table 2 present results on sentiment analysis. Across all datasets, models trained with SSMBA 2 Signature: BLEU+c.mixed+#1+s.exp+tok.13a+v.1.4.3</p><p>outperform baseline models and all other data augmentation methods on OOD data. On ID data, SSMBA outperforms baseline models and other data augmentation methods on all datasets for CNN models, and 3/4 datasets for RNN models. On average, SSMBA improves OOD performance by 1.1% for RNN models and 0.7% for CNN models, and ID performance by 0.8% for RNN models and 0.4% for CNN model. Other methods achieve much smaller OOD generalization gains and perform worse than baseline models on multiple datasets.</p><p>On the AR-Full dataset, RNNs trained with SSMBA demonstrate improvements in OOD accuracy of 1.1% over baseline models. On the AR-Clothing dataset, which exhibits less domain shift than AR-Full, RNNs trained with SSMBA exhibit slightly lower OOD improvement. CNN models exhibit about the same boost in OOD accuracy across both Amazon review datasets.</p><p>On the Movies dataset where we observe a large difference in average sentence length between the two domains, SSMBA still manages to present considerable gains in OOD performance. Although RNNs trained with SSMBA fail to improve ID performance, their OOD performance in this setting still beats other data augmentation methods.</p><p>On the Yelp dataset, we observe large performance gains on both ID and OOD data for RNN models. The improvements on CNN models are more modest, but notably our method is the only one that improves OOD generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Natural Language Inference</head><p>Table <ref type="table">3</ref> presents results on NLI tasks. Models trained with SSMBA outperform or match baseline models and data augmentation methods on both ID and OOD data. Even with a more difficult task and stronger baseline model, SSMBA still confers large accuracy gains. On MNLI, SSMBA improves OOD accuracy by 1.8%, while the best performing baseline achieves only 0.3% improvement. Our method also improves ID accuracy by 1.4%. All other baseline methods hurt both ID and OOD accuracy, or confer negligible improvements.</p><p>On the intentionally difficult ANLI, SSMBA maintains baseline OOD accuracy while conferring a large 6% improvement on ID data. Other augmentation methods improve ID accuracy by a much smaller margin while degrading OOD accuracy. Surprisingly, pseudo-labelling augmented examples in the R2 and R3 domains produced the Table 3: Average in-domain and out-of-domain accuracy (%) for RoBERTa models trained on NLI tasks. Accuracies marked with a * and † are statistically significantly higher than unaugmented models and the next best model respectively, both with p &lt; 0.01. System BLEU ConvS2S (Edunov et al., 2018) 32.2 Transformer (Wu et al., 2019a) 34.4 DynamicConv (Wu et al., 2019a) 35.2 Transformer (ours) 34.70 + Word Dropout 34.43 + RAML 35.00 + SwitchOut 35.28 + SSMBA 36.10 * † Table 4: Results on IWSLT de→en for models trained with different data augmentation methods. Scores marked with a * and † are statistically significantly higher than baseline transformers and the next best model, both with p &lt; 0.01.</p><p>best results, even when the labelling model had poor in-domain performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Machine Translation</head><p>Table 4 presents results on IWSLT14 de→en. We compare our results with convolutional models OPUS de→rm Augmentation ID OOD ID OOD None 56.99 10.24 51.53 12.23 Word Dropout 56.26 10.15 50.23 12.23 RAML 56.76 10.10 51.52 12.49 SwitchOut 55.50 9.27 51.34 13.59 SSMBA 54.88 10.65 51.97 14.67 * †</p><p>Table 5: Average in-domain and out-of-domain BLEU for models trained on OPUS (de→en) and de→rm data.</p><p>Scores marked with a * and † are statistically significantly higher than baseline transformers and the next best model, both with p &lt; 0.01. <ref type="bibr" target="#b10">(Edunov et al., 2018)</ref> and strong baseline transformer and dynamic convolution models <ref type="bibr">(Wu et al., 2019a)</ref>. SSMBA improves BLEU by almost 1.5 points, outperforming all other baseline and comparison models. Compared to SSMBA, other augmentation methods offer much smaller improvements or even degrade performance.</p><p>Table <ref type="table">5</ref> presents results on OPUS and de→rm. On OPUS, where the training domain contains highly specialized language and differs significantly both from other domains and the learned MLM manifold, SSMBA offers a small boost in OOD BLEU but degrades ID performance. All other augmentation methods degrade both ID and OOD performance. On de→rm, SSMBA improves OOD BLEU by a large margin of 2.4 points, and ID BLEU by 0.4 points. Other augmentation methods offer much smaller OOD improvements while degrading ID performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis and Discussion</head><p>In this section, we analyze the factors that influence SSMBA's performance. Due to its relatively small size (25k sentences), number of OOD domains (3), and amount of domain shift, we focus our analysis on the Baby domain within the AR-Clothing dataset. Ablations are performed on a single domain rather than all domains, so error bars correspond to variance in models trained with different seeds and results are not comparable with those in Table <ref type="table" target="#tab_2">2</ref>. Unless otherwise stated, we train CNN models and augment with SSMBA, corrupting 45% of tokens, performing unrestricted sampling when reconstructing, and using self-supervised soft labelling, generating 5 synthetic examples for each training example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Training Set Size</head><p>We first investigate how the size of the initial dataset affects SSMBA's effectiveness. Since a smaller dataset covers less of the training distribution, we might expect the data generated by SSMBA to explore less of the data manifold and reduce its effectiveness. Figure <ref type="figure">5</ref>: Boost in OOD accuracy (%) of models trained with SSMBA augmentation applied with different percentages of corrupted tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Reconstruction Model Capacity</head><p>Since SSMBA relies on a reconstruction function that approximates the underlying data manifold, we might expect a larger and more expressive model to generate higher quality examples. We investigate three models of varying size: Distil-RoBERTa <ref type="bibr" target="#b38">(Sanh et al., 2019)</ref> with 82M parameters, RoBERTa BASE with 125M parameters, and RoBERTa LARGE with 355M parameters. For each reconstruction model, we generate a set of 10 augmented datasets and train a set of 10 models on each augmented dataset. We average performance across models and datasests. Table <ref type="table" target="#tab_6">6</ref> shows that SSMBA displays robustness to the choice of reconstruction model, with all models conferring similar improvements to OOD accuracy. Using the smaller DistilRoBERTa model only degrades performance by a small margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Corruption Amount</head><p>How sensitive is SSMBA to the particular amount of corruption applied? Empirically, tasks that were more sensitive to input noise, like sentiment analysis, required less corruption than those that were more robust, like NLI. To analyze the effect of tuning the corruption amount, we generate 10 sets of augmented data with varying percentages of corruption, then train 10 models on each dataset, averaging performance across all 100 models. Figure <ref type="figure">5</ref> shows that for corruption percentages below 50%, our algorithm is relatively robust to the specific amount of corruption applied. OOD performance peaks at 45% corruption, decreasing thereafter as corruption increases. Very large amounts of corruption tend to degrade performance, although surprisingly all augmented models still outperform unaugmented models, even when 95% of tokens are corrupted. In experiments on the more input sensitive NLI task, large amounts of noise degraded performance below baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Sample Generation Methods</head><p>Next we investigate methods for generating the reconstructed examples x ∼ r(x|x ). Top-k sampling draws samples from the MLM distribution on the top-k most probable tokens, leading to augmented data that explores higher probability regions of the manifold. We investigate top1, top5, top10, top20, and top50 sampling. Unrestricted sampling draws samples from the full probability distribution of tokens. This method explores a larger area of the underlying data distribution but can often lead to augmented data in low probability regions.</p><p>For each sample generation method, we generate 5 sets of augmented data and train 10 models on each dataset. OOD accuracy is averaged across all models for a given sampling method. Figure <ref type="figure" target="#fig_3">6</ref> shows that unrestricted sampling provides the greatest increase in OOD accuracy, with top-k sampling methods all performing similarly. This suggests that SSMBA works best when it is able to explore the manifold without any restrictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Amount of Augmentation</head><p>How does OOD accuracy change as we generate more sentences and explore more of the manifold neighborhood? To investigate we select various augmentation amounts and generate 5 datasets for each amount, training 10 models on each dataset and averaging OOD accuracy across all 50 models. Figure <ref type="figure" target="#fig_4">7</ref> shows that increasing the amount of augmentation increases the amount by which SSMBA improves OOD accuracy, as well as decreasing the variance in the OOD accuracy of trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Label Generation</head><p>We investigate 3 methods to generate a label ŷij for a synthetic example xij . Label preservation preserves the original label y i . Since the manifold neighborhood of an example may cross a decision boundary, we also investigate using a supervised model f trained on the original set of unaugmented data for hard labelling of a one-hot class label ŷij and soft labelling of a class distribution ŷij .</p><p>We train a CNN model to varying levels of convergence and validation accuracy, then label a set of 5 augmented datasets with each labelling method.</p><p>When training with soft labels, we optimize the KL-divergence between the output distribution and soft label distribution. For each dataset we train 10 models and average performance across all models and datasets. Results are shown in Figure <ref type="figure">8</ref>.</p><p>Unsurprisingly, soft and hard labelling with a low accuracy model degrades performance. As our supervision classifier improves, so does the performance of models trained with soft and hard labelled data. Once we pass a certain accuracy threshold, models trained with soft labels begin</p><p>0.73 0.74 0.75 0.76 0.77 -3 -2 -1 0 1 Labelling Model ID Validation Accuracy Boost in OOD Accuracy (%) Preserve Label Soft Label Hard Label Figure 8: Boost in OOD accuracy (%) of models trained with augmented data labelled with different supervision models and label generation methods. outperforming all other models. This threshold varies depending on the difficulty of the dataset and task. In ANLI experiments, labelling augmented examples even with a poor performing model still improved downstream accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we introduce SSMBA, a method for generating synthetic data in settings where the underlying data manifold is difficult to characterize. In contrast to other data augmentation methods, SSMBA is applicable to any supervised task, requires no task-specific knowledge, and does not rely on dataset-specific fine-tuning. We demonstrate SSMBA's effectiveness on three NLP tasks spanning classification and sequence modeling: sentiment analysis, natural language inference, and machine translation. We achieve gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on indomain IWSLT14 de→en. Our analysis shows that SSMBA is robust to the initial dataset size, reconstruction model choice, and corruption amount, offering OOD robustness improvements in most settings. Future work will explore applying SSMBA to the target side manifold in structured prediction tasks, as well as other natural language tasks and settings where data augmentation is difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>Full dataset statistics and details are provided in table <ref type="table">7</ref>. All data splits for all tasks can be downloaded at <ref type="url" target="https://nyu.box.com/s/henvmy17tkyr6npl7e1ltw8j46baxsml">https://nyu.box.com/s/ henvmy17tkyr6npl7e1ltw8j46baxsml</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Data Preprocessing</head><p>We use the same preprocessing steps across all sentiment analysis and NLI experiments. All data is first tokenized using a GPT-2 style tokenizer and BPE vocabulary provided by fairseq <ref type="bibr" target="#b30">(Ott et al., 2019)</ref>. This BPE vocabulary consists of 50263 types. Corresponding labels are encoded using a label dictionary consisting of as many types as there are classes. Input text and labels are then binarized for model training. Although all models share the same vocabulary, we randomly initialize each model's embeddings and train the entire model end-to-end. For machine translation experiments, we follow <ref type="bibr" target="#b26">Müller et al. 2019</ref> and learn a 16k BPE on OPUS and a 32k BPE on de→rm. On IWSLT14 we learn a 10k BPE. We use a separate vocabulary for the source and target side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Model Architecture and Training Hyperparameters</head><p>All models are written and trained within the fairseq framework <ref type="bibr" target="#b30">(Ott et al., 2019)</ref> with T4 GPUs. LSTM and CNN models were trained on a single GPU, RoBERTa models were trained with 4 GPUs, and tranfsormer models were trained with 2 GPUs. On average, when trained on augmented data, LSTM and CNN models took an hour to train to convergence, RoBERTa models took 12 hours to train to convergence, and transformer models took 24 hours to train to convergence. Models trained on unaugmented data took roughly 20% of the time of models trained on augmented data to reach convergence. For each model we investigate, we present first the model architecture and then the training hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 LSTM</head><p>Our LSTM models are a single layer of 512 nodes. Input embeddings are 512 dimensions. The output embedding from the last time step is fed into a MLP classifier with a single hidden layer of 512 dimensions. Models contain 28M parameters. Dropout of 0.3 is applied to the input and output of our encoder, and dropout of 0.1 is applied to the MLP classifier.</p><p>We train with Adam optimizer (Kingma and Ba, 2014) with β = (0.9, 0.98) and = 1e-6. Our learning rate is set to 1e-4 and is first warmed up for 2 epochs before it is decayed using an inverse square root scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 CNN</head><p>Our CNN models are based on the architecture in <ref type="bibr" target="#b16">(Kim, 2014)</ref>. As in our LSTM models, our input embeddings are 512 dimensional, which we treat as our channel dimension. We apply three convolutions of kernel size 3, 4, and 5, with 256 output channels. Models contain 27M parameters. Convolutional outputs are max-pooled over time then concatenated to a 768-dimensional encoded representation. Again, we feed this representation into a MLP classifier with a single hidden layer of 512 dimensions. We apply dropout of 0.2 to our inputs and MLP classifier.</p><p>We train with Adam optimizer (Kingma and Ba, 2014) with β = (0.9, 0.98) and = 1e-6. Our learning rate is set to 1e-3 and is first warmed up for 2 epochs before it is decayed using an inverse square root scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 RoBERTa</head><p>Our RoBERTa models use a pre-trained RoBERTa BASE model provided by fairseq. As in other models, classification token embeddings are fed into an MLP classifier with a single hidden layer of 512 dimensions. Models contain 125M parameters. We follow the MNLI fine-tuning procedures in fairseq, training with learning rate 1e-5 with Adam optimizer (Kingma and Ba, 2014) with β = (0.9, 0.98) and = 1e-6. We warmup the learning rate for 2 epochs before decaying with an inverse square root scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Transformer</head><p>Transformer models are trained with labelsmoothed cross-entropy and label smoothing 0.1. Due to the dataset sizes, we use a slightly smaller transformer architecture with embedding dimension 512, feed forward embedding dimension 1024, 4 encoder heads, and 6 encoder and decoder layers. Models contain 52M parameters. We also apply dropout of 0.3 and weight decay of 0.0001. All other hyperparameters follow the base architecture in <ref type="bibr" target="#b47">Vaswani et al. 2017.</ref> As in other models, we train with Adam optimizer (Kingma and Ba, 2014) with β = (0.9, 0.98) and = 1e-6. Our learning rate is set to 5e-4 and is first warmed up for 4000 updates before it is decayed using an inverse square root scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D SSMBA Hyperparameters</head><p>SSMBA hyperparameters for each dataset and domain are provided in table 8. Hyperparameters are chosen based on in-domain validation performance. A detailed analysis of hyperparameter tuning is provided in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Statistical Testing</head><p>For the statistical tests on sentiment analysis and NLI tasks, we use a Wilcoxon ranked-sum test. Specifically, we compare averages of model performances on pairs of training and test domains. For example, in a dataset with 3 domains, D1, D2, and D3, we have 3 in-domain train-test pairs (D1-D1, D2-D2, D3-D3), and 6 out-of-domain traintest pairs (D1-D2, D1-D3, D2-D1, D2-D3, D3-D1, D3-D2). We calculate the average performance for each model on each pair, then compare the matched in-domain and out-of-domain pairs. Since the number of samples we can compare depends on the total number of domains in the dataset, a larger number of datasets gives us a better sense of our statistical significance.</p><p>For the statistical tests on machine translation tasks, we use a paired bootstrap resampling approach <ref type="bibr" target="#b19">(Koehn, 2004)</ref>. Since the test works only on a single system's output, we run the test on every pairing of seeds and test domains for the two comparison models. We report the significance level only if all tests result in a small enough probability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SSMBA generates synthetic examples by corrupting then reconstructing the original training inputs. To form the augmented dataset, corresponding outputs are preserved from the original data or generated from a supervised model f trained on the original data.</figDesc><graphic coords="3,74.37,62.81,213.53,135.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Require: perturbation function q reconstruction function r 2: Input: Dataset D = {(x 1 , y 1 ) . . . (x n , y n )} number of augmented examples m 3: function SSMBA(D, m) 4: train a model f on D 5: for (x i , y i ) ∈ D do 6: for j ∈ 1 . . . m do 7: sample perturbed x ij ∼ q(x |x i ) 8: sample reconstructed xij ∼ r(x|x ij ) 9: generate ŷij ← f (x ij ) or preserve the original y i D aug = {(x ij , ŷij )} i=1...n,j=1...m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: OOD accuracy of models trained on successively subsampled datasets. The full training set contains 25k examples. Error bars show standard deviation in OOD accuracy across models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Boost in OOD accuracy (%) of models trained with SSMBA augmentation using different sampling methods. Error bars show standard deviation in OOD accuracy across models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: OOD accuracy (%) of models trained with different amounts of SSMBA augmentation. 0 augmentation corresponds to a baseline model. Error bars show standard deviation in OOD accuracy across models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>contains movie review excerpts. Following Hendrycks et al. 2020 we pair this dataset with the IMDb dataset (Maas et al., Dataset summary statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Domain n n n</cell><cell>l l l</cell><cell cols="2">Train Test</cell></row><row><cell cols="2">AR-Clothing *</cell><cell>4</cell><cell>35</cell><cell>25k  †</cell><cell>2k</cell></row><row><cell>AR-Full</cell><cell>*</cell><cell cols="2">10 67</cell><cell>25k  †</cell><cell>2k</cell></row><row><cell>Yelp</cell><cell>*</cell><cell>4</cell><cell cols="2">138 25k  †</cell><cell>2k</cell></row><row><cell>Movies</cell><cell>SST2 IMDb</cell><cell>--</cell><cell cols="2">11 296 46k 66k</cell><cell>1k 2k</cell></row><row><cell>MNLI</cell><cell>*</cell><cell cols="2">10 36</cell><cell>80k</cell><cell>1k</cell></row><row><cell></cell><cell>R1</cell><cell>-</cell><cell>92</cell><cell>17k</cell><cell>1k</cell></row><row><cell>ANLI</cell><cell>R2</cell><cell>-</cell><cell>90</cell><cell>46k</cell><cell>1k</cell></row><row><cell></cell><cell>R3</cell><cell>-</cell><cell>82</cell><cell>100k</cell><cell>1k</cell></row><row><cell>IWSLT</cell><cell>-</cell><cell>1</cell><cell>24</cell><cell>160k</cell><cell>7k</cell></row><row><cell>OPUS</cell><cell cols="2">Medical 5</cell><cell>15</cell><cell>1.1m</cell><cell>2k</cell></row><row><cell>de-rm</cell><cell>Law Blogs</cell><cell>--</cell><cell>22 25</cell><cell>100k -</cell><cell>2k 2k</cell></row></table><note><p>n: number of domains. l: average tokenized input length. A * in the domain column indicates that the statistics are identical across domains within that dataset. Training sets marked with a † are sampled randomly from a larger dataset. Refer to Appendix A for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average in-domain (ID) and out-of-domain (OOD) accuracy (%) for models trained on sentiment analysis datasets. Average performance across datasets is weighted by number of domains contained in each dataset. Accuracies marked with a * and † are statistically significantly higher than unaugmented models and the next best model respectively, both with p &lt; 0.01.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">AR-Full</cell><cell cols="2">AR-Clothing</cell><cell cols="2">Movies</cell><cell></cell><cell>Yelp</cell><cell>Average</cell></row><row><cell cols="3">Model Augmentation</cell><cell>ID</cell><cell>OOD</cell><cell>ID</cell><cell>OOD</cell><cell>ID</cell><cell>OOD</cell><cell>ID</cell><cell>OOD</cell><cell>ID</cell><cell>OOD</cell></row><row><cell></cell><cell>None</cell><cell></cell><cell>69.46</cell><cell cols="8">66.32 69.25 67.80 90.74 71.94 62.51 61.28 70.16 66.17</cell></row><row><cell></cell><cell>EDA</cell><cell></cell><cell>67.32</cell><cell cols="4">64.47 66.87 65.21 88.43</cell><cell>68.3</cell><cell cols="3">58.39 57.19 67.56 63.55</cell></row><row><cell>RNN</cell><cell>CBERT</cell><cell></cell><cell>69.94</cell><cell cols="8">66.77 69.56 68.10 91.01 72.11 63.17 61.75 70.17 66.57</cell></row><row><cell></cell><cell>UDA</cell><cell></cell><cell>69.92</cell><cell cols="8">66.97 69.98 68.24 90.05 69.73 63.40 62.13 70.64 66.53</cell></row><row><cell></cell><cell>SSMBA</cell><cell></cell><cell cols="9">70.38  *  † 67.41  *  † 70.19 68.60  *  † 89.61 73.20 63.85 62.83  *  † 70.96 67.31</cell></row><row><cell></cell><cell>None</cell><cell></cell><cell>70.67</cell><cell cols="8">67.64 70.14 68.52 92.92 72.11 65.13 64.46 71.68 67.63</cell></row><row><cell></cell><cell>EDA</cell><cell></cell><cell>68.52</cell><cell cols="8">66.03 67.76 66.17 91.22 74.20 60.99 59.88 69.13 65.65</cell></row><row><cell>CNN</cell><cell>CBERT</cell><cell></cell><cell>70.62</cell><cell cols="8">67.70 70.13 68.23 92.92 71.56 65.09 64.19 71.65 67.49</cell></row><row><cell></cell><cell>UDA</cell><cell></cell><cell>70.80</cell><cell cols="8">68.06 70.29 68.70 92.63 72.55 65.22 64.32 71.77 67.89</cell></row><row><cell></cell><cell>SSMBA</cell><cell></cell><cell cols="9">71.10  *  68.18  *  70.74 69.04  *  92.93 74.67 65.59 64.81  *  † 72.11 68.33</cell></row><row><cell></cell><cell></cell><cell cols="2">MNLI</cell><cell cols="2">ANLI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Augmentation</cell><cell>ID</cell><cell>OOD</cell><cell>ID</cell><cell>OOD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell cols="5">84.29 80.61 42.54 43.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EDA</cell><cell cols="5">83.44 80.34 45.59 42.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CBERT</cell><cell cols="5">84.24 80.34 46.68 43.53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UDA</cell><cell cols="5">84.24 80.99 45.85 42.89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSMBA</cell><cell cols="3">85.71 82.44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* † 48.46 * † 43.80</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Boost in OOD accuracy (%) of models trained with SSMBA augmented data generated with different reconstruction functions.</figDesc><table><row><cell>We subsample 25% of the</cell></row><row><cell>original dataset to form a new training set, then</cell></row><row><cell>repeat this process successively to form exponen-</cell></row><row><cell>tially smaller and smaller datasets. The smallest</cell></row><row><cell>dataset contains only 24 examples. For each dataset</cell></row><row><cell>fraction, we train 10 models and average perfor-</cell></row><row><cell>mance, tuning a set of SSMBA hyperparameters on</cell></row><row><cell>the same ID validation data. Figure 4 shows that</cell></row><row><cell>SSMBA offers OOD performance gains across al-</cell></row><row><cell>most all dataset sizes, even in low resource settings</cell></row><row><cell>with less than 100 training examples.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Resources used in preparing this research were provided, in part, by the <rs type="funder">Province of Ontario</rs>, the <rs type="institution">Government of Canada through CIFAR</rs>, and companies sponsoring the Vector Institute <ref type="url" target="www.vectorinstitute.ai/#partners">www. vectorinstitute.ai/#partners</ref>. This work was partly supported by <rs type="funder">Samsung Advanced Institute of Technology</rs> (<rs type="programName">Next Generation Deep Learning</rs>: from pattern recognition to AI) and <rs type="projectName">Samsung Research (Improving Deep Learning using Latent Structure</rs>). We thank <rs type="person">Julian McAuley</rs>, <rs type="person">Vishaal Prasad</rs>, <rs type="person">Taylor Killian</rs>, <rs type="person">Victoria Cheng</rs>, and <rs type="person">Aparna Balagopalan</rs> for helpful comments and discussion.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_puQJNfb">
					<orgName type="project" subtype="full">Samsung Research (Improving Deep Learning using Latent Structure</orgName>
					<orgName type="program" subtype="full">Next Generation Deep Learning</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do not have enough data? deep learning to the rescue</title>
		<author>
			<persName><forename type="first">Ateret</forename><surname>Anaby-Tavor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Goldbraich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Tepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Zwerdling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In Proceedings of the 2020 AAAI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalized denoising autoencoders as generative models</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt 2014</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11 th International Workshop on Spoken Language Translation</title>
		<meeting>the 11 th International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Branden</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Pietsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanay</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin</forename><forename type="middle">Man</forename><surname>Yeung</surname></persName>
		</author>
		<title level="m">Open sourcing german bert</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<title level="m">Semi-Supervised Learning (Adaptive Computation and Machine Learning)</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vicinal risk minimization</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2090</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pretrained transformers improve out-of-distribution robustness</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data augmentation for visual question answering</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Yousefhussien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3529</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="198" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Geometric robustness of deep networks: Analysis and improvement</title>
		<author>
			<persName><forename type="first">Can</forename><surname>Kanbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Moosavi-Dezfooli Seyed-Mohsen</surname></persName>
		</author>
		<author>
			<persName><surname>Frossard</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00467</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4441" to="4449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>arxiv:1412.6980Comment</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2015</date>
		</imprint>
	</monogr>
	<note>Published as a conference paper at the</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2072</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02245</idno>
		<title level="m">Data Augmentation using Pretrained Transformer Models. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">Marjan Ghazvininejad,. 2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Domain robustness in neural machine translation</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annette</forename><forename type="middle">Rios</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.03109</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fined-grained aspects</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<title level="m">Adversarial NLI: A New Benchmark for Natural Language Understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1723" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</meeting>
		<imprint>
			<publisher>USA. Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Data augmentation with manifold exploring geometric transformations for increased performance and robustness</title>
		<author>
			<persName><forename type="first">Magdalini</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit Guha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rüdiger</forename><surname>Göbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wachinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Effectiveness of Data Augmentation in Image Classification using Deep Learning</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m">Dataset Shift in Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS EM C 2 Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The trilingual ALLEGRA corpus: Presentation and possible use for lexicon induction</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Cartoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2890" to="2896" />
		</imprint>
	</monogr>
	<note>Istanbul, Turkey. European Languages Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for WMT 16</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2323</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-49430-8_13</idno>
		<title level="m">Transformation Invariance in Pattern Recognition -Tangent Distance and Tangent Propagation</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="239" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
		<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Neural Information Processing Systems</title>
		<meeting>the 2017 Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">That&apos;s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets</title>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1306</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2557" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SwitchOut: an efficient data augmentation algorithm for neural machine translation</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="856" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">EDA: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6383" to="6389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Conditional bert contextual augmentation</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Data noising as smoothing in neural network language models</title>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiming</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafksy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 International Conference on Learning Representations</title>
		<meeting>the 2017 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Yiben</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Ping</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Downey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11546</idno>
		<title level="m">Generative Data Augmentation for Commonsense Reasoning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
		<ptr target="https://www.yelp.com/dataset" />
	</analytic>
	<monogr>
		<title level="j">Yelp. Yelp open dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fast and accurate reading comprehension by combining self-attention and convolution</title>
		<author>
			<persName><forename type="first">Adams</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
