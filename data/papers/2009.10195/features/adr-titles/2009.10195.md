- Decision to use Self-Supervised Manifold Based Data Augmentation (SSMBA) for data augmentation
- Choice of corruption and reconstruction functions for SSMBA
- Selection of masked language models (MLMs) as the reconstruction model
- Decision to apply SSMBA across multiple NLP tasks (sentiment analysis, NLI, machine translation)
- Choice of datasets for empirical evaluation of SSMBA
- Decision to not rely on task-specific knowledge or dataset-specific fine-tuning
- Selection of model types for different tasks (LSTMs, CNNs, RoBERTa, Transformers)
- Decision on the percentage of tokens to corrupt during augmentation
- Choice of hyperparameters for SSMBA based on in-domain validation performance
- Decision to generate multiple augmented examples per input example
- Choice of evaluation metrics for assessing OOD robustness
- Decision to use a teacher model for generating labels for augmented examples
- Selection of training and validation splits for datasets
- Decision to initialize word embeddings randomly for consistency across models
- Choice of sampling method for generating augmented examples
- Decision to use beam search for target side translations in machine translation tasks