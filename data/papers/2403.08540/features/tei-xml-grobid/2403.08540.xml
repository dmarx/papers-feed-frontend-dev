<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language models scale reliably with over-training and on downstream tasks</title>
				<funder ref="#_zVwmXfB #_XggEybP">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_ZGjmZzg">
					<orgName type="full">Onassis Foundation -Scholarship ID</orgName>
				</funder>
				<funder>
					<orgName type="full">Cisco</orgName>
				</funder>
				<funder>
					<orgName type="full">WEST-AI</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanly P. Finch Centennial Professorship in Engineering</orgName>
				</funder>
				<funder ref="#_Y3qF92Q">
					<orgName type="full">Western Digital, Amazon, WNCG IAP</orgName>
				</funder>
				<funder ref="#_SjFktgc">
					<orgName type="full">Federal Ministry of Education and Research of Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-14">14 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samir</forename><forename type="middle">Yitzhak</forename><surname>Gadre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georgios</forename><surname>Smyrnis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rulin</forename><surname>Shao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jean</forename><surname>Mercat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Xin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marianna</forename><surname>Nezhurina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><surname>Vasiljevic</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Pang</surname></persName>
						</author>
						<author>
							<persName><surname>Koh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yair</forename><surname>Carmon</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Reinhard</forename><surname>Heckel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Munich</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Contextual</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Language models scale reliably with over-training and on downstream tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-14">14 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">101E8FFC81A0D292E2867E35FE3695E2</idno>
					<idno type="arXiv">arXiv:2403.08540v2[cs.CL]</idno>
					<note type="submission">10 17 10 19 10 21 Compute (6ND, D = MN 10 17 10 19 10 21 Compute (6ND, D = MN) [FLOPs] 1 2 3 4 5 Training set: RedPajama 10 17 10 19 10 21 Compute (6ND, D = MN</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., "Chinchilla optimal" regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32× over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)-each from experiments that take 300× less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20× less compute. Our experiments are available at <ref type="url" target="https://github.com/mlfoundations/scaling">https://github.com/mlfoundations/scaling</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Training large language models is expensive. Furthermore, training high-quality models requires a complex recipe of algorithmic techniques and training data. To reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate their efficacy to larger model and data regimes via scaling laws. With reliable extrapolation, it is possible to quickly iterate at small scale and still pick the method that will perform best for the final large training run. Indeed, this workflow has become commonplace for training state-of-the-art language models like Chinchilla 70B <ref type="bibr" target="#b44">[45]</ref>, PaLM 540B <ref type="bibr" target="#b18">[19]</ref>, GPT-4 <ref type="bibr" target="#b75">[76]</ref>, and many others.  We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training. We are able to extrapolate, in both N and M , the validation performance of models requiring more than 300× the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20× the compute. For this figure, we train all models on RedPajama <ref type="bibr" target="#b111">[112]</ref>.</p><p>Despite their importance for model development, published scaling laws differ from the goals of training state-of-the-art models in important ways. For instance, scaling studies usually focus on the compute-optimal training regime ("Chinchilla optimality" <ref type="bibr" target="#b44">[45]</ref>), where model and dataset size are set to yield minimum loss for a given compute budget. However, this setting ignores inference costs. As larger models are more expensive at inference, it is now common practice to over-train smaller models <ref type="bibr" target="#b112">[113]</ref>. Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets. However, practitioners usually turn to benchmark performance, not loss, to compare models.</p><p>In this paper, we conduct an extensive set of experiments to address both scaling in the over-trained regime and benchmark performance prediction.</p><p>Motivated by the practice of training beyond compute-optimality, we first investigate whether scaling follows reliable trends in the over-trained regime. We notice, as implied by Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref>, for a set of models of different sizes trained with a constant ratio of tokens to parameters, models' reducible loss L ′ <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> follows a power law (L ′ = λ • C -η ) in the amount of training compute C. We find that as one increases the ratio of tokens to parameters, corresponding to more over-training, the scaling exponent η remains about the same, while the scalar λ changes. We explain our observations by reparameterizing existing scaling laws in relation to the amount of over-training.</p><p>To establish empirically that scaling extrapolates in the over-trained regime, we further experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Scaling laws for loss. Typically, scaling laws predict model loss L as a function of the compute C in FLOPs used for training. If one increases the number of parameters N in a model or the number of tokens D that a model is trained on, compute requirements naturally increase. Hence, we assume C is a function of N, D. Following Kaplan et al. <ref type="bibr" target="#b50">[51]</ref>, we use the approximation C = 6N D, which Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> independently verify. We consider,</p><formula xml:id="formula_0">L(C) = E + L ′ (C),<label>(1)</label></formula><p>where E is an irreducible loss and L ′ is the reducible loss. E captures the Bayes error or minimum possible loss achievable on the validation domain. The L ′ (C) term captures what can possibly be learned about the validation domain by training on a source domain. L ′ (C) should approach zero with increased training data and model capacity. L ′ (C) is often assumed to follow a power law: L ′ (C) = λ • C -η (i.a., Hestness et al. <ref type="bibr" target="#b42">[43]</ref>, OpenAI <ref type="bibr" target="#b75">[76]</ref>). It is also often helpful to consider a power law in a log-log plot, where it appears as a line with slope -η and y-intercept log (λ). </p><formula xml:id="formula_1">L ′ (C) = λ • C -η</formula><p>, the exponent η remains relatively constant resulting in lines with approximately fixed slope (Figure <ref type="figure" target="#fig_25">17</ref>). The scalar λ that determines the y-intercept, however, shifts with different token multipliers. This suggests λ is a function of the token multiplier, while η is not.</p><p>Token multipliers. We define a token multiplier M = D/N as the ratio of training tokens to model parameters for notational convenience. M allows us to consider fixed relationships between D and N even as a model gets bigger (i.e., as N becomes larger).</p><p>Compute-optimal training. Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> establish compute-optimal training, where, for any compute budget H, the allocation of parameters and tokens is given by, arg min</p><formula xml:id="formula_2">N,D L(N, D) s.t. C(N, D) = H.<label>(2)</label></formula><p>To solve for the optimal N * , D * , one can sweep N, D for each compute budget, retaining the best configurations. Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> find that as the compute budget increases, N * and D * scale roughly evenly. Assuming equal scaling, there is a fixed compute-optimal token multiplier M * = D * /N * per training distribution.</p><p>Over-training. We define over-training as the practice of allocating compute sub-optimally, so smaller models train on a disproportionately large number of tokens (i.e., M &gt; M * ). While loss should be higher than in the compute-optimal allocation for a given training budget, the resulting models have fewer parameters and thus incur less inference cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scaling laws for over-training</head><p>To propose a scaling law for over-trained models, we first turn to empirical observation. We train four model configurations with parameter counts between 0.011B and 0.411B for token multipliers M between 20 and 640, where M = 20 points lie roughly on the compute-optimal frontier, and larger M corresponds to more over-training. We defer experimental details to Section 3 to focus on our observations first. In Figure <ref type="figure" target="#fig_2">2</ref>, we show loss against compute in a log-log plot for the models trained on three datasets and evaluated on the C4 eval set. We notice parallel lines when fitting power laws to We plot models trained on three datasets and notice an exponential decay of average top-1 error as C4 eval loss, on the x-axis, decreases. We consider on the y-axes average error on 17 evaluations where performance is at least 10 points above random chance for at least one 0.154B scale model. These observations suggest that average top-1 error should be predictable with reliable loss estimates.</p><p>the reducible loss, which suggests a near-constant scaling exponent even with increased over-training. This indicates that scaling behavior should be describable in the amount of over-training.</p><p>In search of an analytic expression for the observations in Figure <ref type="figure" target="#fig_2">2</ref>, we consider existing scaling literature. A common functional form for the risk of a model, as proposed in prior work <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b92">93]</ref> is,</p><formula xml:id="formula_3">L(N, D) = E + AN -α + BD -β .<label>(3)</label></formula><p>Recall from Section 2.1, N is the number of parameters and D the number of training tokens.</p><p>The constants E, A, α, B, β are fit from data. By fitting this parametric form, Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> find that scaling exponents α and β are roughly equal, suggesting that one should scale N and D equally as compute increases. Hence, we assume α = β. With this assumption, we reparameterize Equation (3) in terms of compute C = 6N D and a token multiplier M = D/N . We get,</p><formula xml:id="formula_4">L(C, M ) = E + aM η + bM -η C -η ,<label>(4)</label></formula><p>where η = α/2, a = A(1/6) -η , b = B(1/6) -η gives the relation to Equation <ref type="bibr" target="#b2">(3)</ref>. For a complete derivation, see Appendix B.</p><p>Equation ( <ref type="formula" target="#formula_4">4</ref>) has the following interpretation: (i) The scaling exponent η is not dependent on M . Thus, we always expect lines with the same slope in the log-log plot-as in Figure <ref type="figure" target="#fig_2">2</ref>. (ii) The term aM η + bM -η determines the offsets between curves with different token multipliers. Hence, we expect non-overlapping, parallel lines in the log-log plot for the range of M we consider-also consistent with Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>Recall that we make the assumption α = β, which implies equal scaling of parameters and tokens as more compute is available. However, as explained in Appendix B, even if α ̸ = β, we get a parameterization that implies the power-law exponent remains constant with over-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scaling laws for downstream error</head><p>Scaling is typically studied in the context of loss <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b71">72]</ref>, which Schaeffer et al. <ref type="bibr" target="#b99">[100]</ref> note is smoother than metrics like accuracy. However, practitioners often use downstream benchmark accuracy as a proxy for model quality and not loss on perplexity evaluation sets. To better connect scaling laws and over-training to task prediction, we revisit the suite of models plotted in Figure <ref type="figure" target="#fig_2">2</ref>. In Figure <ref type="figure" target="#fig_3">3</ref>, we plot average downstream top-1 errors over evaluations sourced from LLM-Foundry <ref type="bibr" target="#b68">[69]</ref> against the C4 eval loss. We defer details of the setup to Section 3 to focus here on a key observation: average error appears to follow exponential decay as loss decreases.</p><p>Based on the exponential decay we observe in Figure <ref type="figure" target="#fig_3">3</ref>, we propose the following relationship between downstream average top-1 error Err and loss L,</p><formula xml:id="formula_5">Err(L) = ϵ -k • exp (-γL),<label>(5)</label></formula><p>where ϵ, k, γ are fit from data. Equation ( <ref type="formula" target="#formula_5">5</ref>) also has an interpretation in terms of model perplexity PP(L) = exp (L),</p><formula xml:id="formula_6">Err(PP) = ϵ -k • PP -γ .<label>(6)</label></formula><p>Namely, Err follows a power law in PP that is bounded from above by ϵ signifying arbitrarily high error and from below by ϵ -k • exp(-γE), where E is the Bayes error from Equation (4).</p><p>Equation ( <ref type="formula" target="#formula_5">5</ref>) in conjunction with Equation (4) suggests a three-step method to predict Err as a function of compute and the amount of over-training. For choices of training and validation distributions, (i) fit a scaling law to Equation (4) using triplets of compute C, token multiplier M , and measured loss L on a validation set to yield (C, M ) → L. (ii) Fit a scaling law to Equation (5) using pairs of loss L and downstream error Err for models to get L → Err. (iii) Chain predictions to get (C, M ) → Err.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Constructing a scaling testbed</head><p>In this section, we discuss our experimental setup to test the predictions suggested by Equations ( <ref type="formula" target="#formula_4">4</ref>) and <ref type="bibr" target="#b4">(5)</ref>. We first present our general language modeling setup (Section 3.1). Next, we discuss our strategy for determining model configurations for our scaling investigation (Section 3.2) and fitting scaling laws (Section 3.3). We then present metrics to validate how well scaling laws predict loss and downstream performance (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training setup</head><p>We train transformers <ref type="bibr" target="#b115">[116]</ref> for next token prediction, based on architectures like GPT-2 <ref type="bibr" target="#b84">[85]</ref> and LLaMA <ref type="bibr" target="#b112">[113]</ref>. We employ GPT-NeoX <ref type="bibr" target="#b14">[15]</ref> as a standardized tokenizer for all data. See Appendix C for architecture, optimization, and hyperparameter details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model configurations</head><p>To get final configurations for the 0.011B to 0.411B parameter models plotted in Figures <ref type="figure" target="#fig_2">2</ref> and <ref type="figure" target="#fig_3">3</ref>, we first conduct a wide grid search over a total of 435 models, trained from scratch, from 0.01B to 0.5B</p><p>10 17 10 19 10 21 Compute (6ND) [FLOPs] 2 3 4 5 6 Loss: OpenLM eval Search 10 17 10 19 10 21 Compute (6ND) [FLOPs] Filter 10 17 10 19 10 21 Compute (6ND) [FLOPs] Fit Grid search models Selected models Target 1.4B model Target 6.9B model Interpolation Extrapolation 10 22 1.9 1.8 1.7 Figure 4: Search, filter, fit: A recipe for selecting configurations for scaling. (left) To generate the final configurations presented in Table 3, we run a 435 model grid search over model width, hidden dimension, number of attention heads, batch size, and warmup steps. All models are trained near compute-optimally. (center) We plot the efficient frontier of models, which appear to follow a trend, excluding models from 5.2 × 10 16 to 5.2 × 10 17 , which fall below the trend. (right)</p><p>We fit a power law with irreducible error to the remaining configurations, picking four configurations that closely track the full model suite ("Selected models"). These models extrapolate the performance of 1.4B, 6.9B target models. Shaded regions represent bootstrap 95% confidence intervals. parameters (Figure <ref type="figure">4</ref> (left)). We train on the original OpenLM data mix <ref type="bibr" target="#b38">[39]</ref>, which largely consists of RedPajama <ref type="bibr" target="#b111">[112]</ref> and The Pile <ref type="bibr" target="#b30">[31]</ref>. While we eventually plan to over-train models, at this step we search for base configurations near compute-optimality. We train on 20 tokens per parameter (M = 20), which, in early experiments, gives models near the compute-optimal frontier. This is similar to findings in Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref>'s Table <ref type="table" target="#tab_6">3</ref>, which suggests that M = 20 is near-optimal for the Chinchilla experimental setup.</p><p>To find maximally performant small-scale models on validation data, we tune model width, number of layers, number of attention heads, warmup steps, and batch size. Our validation set, OpenLM eval, contains tokens from recent arXiv papers, the OpenLM codebase itself, and news articles. We find in early experiments that qk-LayerNorm makes models less sensitive to learning rate, which is a phenomenon Wortsman et al. <ref type="bibr" target="#b122">[123]</ref> report in their Figure <ref type="figure" target="#fig_1">1</ref>. Hence, we fix the learning rate (3e-3) for our sweeps. We also perform smaller grid searches over 1.4B and 6.9B parameter model configurations at M = 20, retaining the best configurations.</p><p>At this point, we have many models, several of which give poor performance; following prior work <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51]</ref>, we want to keep only models that give best performance. Hence, in Figure <ref type="figure">4</ref> (center), we filter out models that do not lie on the Pareto frontier. While there appears to be a general trend, configurations between 5.2 × 10 16 and 5.2 × 10 17 FLOPs lie below the frontier established by other models. We hypothesize these models over-perform as they are trained for more optimization steps than their neighbors based on our power-of-two batch sizes. We provide support for this hypothesis in Appendix F, but opt to remove these models from our investigation.</p><p>To ensure tractable compute requirements for our scaling experiments, we require a subset of models that follows the trend of the entire Pareto frontier. In Figure <ref type="figure">4</ref> (right), we fit trends to the Pareto models and to a subset of four models. We notice that the trends closely predict both the performance Table <ref type="table">1</ref>: Default number of parameters N and token multiplier M to fit our scaling laws. We invest ∼100 A100 hours to fit Equation ( <ref type="formula" target="#formula_4">4</ref>) and ∼1,000 A100 hours to fit Equation <ref type="bibr" target="#b4">(5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N M</head><p>Used to fit Equation ( <ref type="formula" target="#formula_4">4</ref>) Used to fit Equation ( <ref type="formula" target="#formula_5">5</ref>)</p><formula xml:id="formula_7">0.011B 20 ✓ ✓ 0.079B 20 ✓ ✓ 0.154B 20 ✓ ✓ 0.411B 20 ✓ ✓ 0.011B 320 ✓ ✓ 1.4B 20 ✗ ✓ Total compute C [FLOPs] 2.4e19 2.7e20</formula><p>of the 1.4B and 6.9B models, suggesting that our small-scale configurations reliably extrapolate in the compute-optimal setting.</p><p>Moving forward, we do not tune hyperparameters for other token multipliers (i.e., M ̸ = 20), on other training or evaluation distributions, or on validation sets for downstream tasks. For more details including specific hyperparameters, see Appendix D.</p><p>To create our scaling testbed, we start with the four small-scale, base configurations from our grid search: N ∈ {0.011B, 0.079B, 0.154B, 0.411B}. To ensure our conclusions are not particular to a single training distribution, we train models on each of C4 <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b87">88]</ref>, RedPajama <ref type="bibr" target="#b111">[112]</ref>, and RefinedWeb <ref type="bibr" target="#b81">[82]</ref>, which have 138B, 1.15T, and 600B tokens, respectively, for different token multipliers M ∈ {5, 10, 20, 40, 80, 160, 320, 640}. We omit runs that require more tokens than are present in a dataset (i.e., N = 0.411B, M = 640 for C4). We additionally train N = 1.4B models at M = 20 and at the largest token multiplier possible without repeating tokens (i.e., 80 for C4, 640 for RedPajama, and 320 for RefinedWeb). We train N = 6.9B, M = 20 models on each dataset given the relevance of 7B parameter models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b112">113]</ref>. In total this results in a testbed of 104 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fitting scaling laws</head><p>We fit Equation (4) to approximate E, a, b, η using curve-fitting in SciPy <ref type="bibr" target="#b116">[117]</ref> (i.e., Levenberg-Marquardt to minimize non-linear least squares). We repeat this process to fit Equation (5) to approximate ϵ, k, γ. We invest ∼100 A100 hours to train the models required to fit a scaling law for loss and ∼1,000 A100 hours for a corresponding law for downstream error. Unless otherwise specified, we fit to the N, M pairs in Table <ref type="table">1</ref>, which are a subset of our full testbed. Our configurations allow us to test for extrapolation to the N = 1.4B, M = 640 (900B token) and the N = 6.9B, M = 20 (138B token) regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation setup</head><p>Evaluation datasets. Unless otherwise stated, our default validation loss dataset is C4 eval. For downstream tasks, we adopt a subset from 46 tasks from LLM-foundry <ref type="bibr" target="#b68">[69]</ref>, which includes standard tasks with both zero-shot and few-shot evaluations. Specifically, we consider a 17-task subset where, for each evaluation, at least one 0.154B scale model-trained with as many as 99B tokens-gets 10 percentage points above chance accuracy: ARC-Easy <ref type="bibr" target="#b22">[23]</ref>, BIG-bench: CS algorithms <ref type="bibr" target="#b10">[11]</ref>, BIG-bench: Dyck languages <ref type="bibr" target="#b10">[11]</ref>, BIG-bench: Novel Concepts <ref type="bibr" target="#b10">[11]</ref>, BIG-bench: Operators <ref type="bibr" target="#b10">[11]</ref>, BIG- 1.1% 0.0% 0.2% 0.7% 0.9% 0.0% 0.6% 2.6% 0.3% 0.2% 0.4% 0.1% 0.7% 0.8% 1.5% 0.5% 1.1% 1.1% 3.3% 2.8% 0.6% 0.5% 0.2% 0.0% 2.8% 0.2% 2.0% 0.8% 1.5%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3%</head><p>Train: C4 Eval: C4 eval 0.3% 0.0% 0.3% 1.7% 1.1% 0.0% 1.0% 2.2% 0.3% 0.2% 0.7% 1.4% 2.1% 2.3% 0.8% 0.5% 0.6% 0.0% 0.4% 0.4% 0.3% 0.4% 0.2% 0.1% 0.3% 0.3%</p><p>1.4% 1.1% 0.1% 0.7% 0.7% Train: RedPajama Eval: C4 eval 10 20 40 80 160 320 640 M</p><p>0.9% 0.0% 0.9% 1.9% 1.0% 0.0% 1.1% 2.4% 0.1% 0.0% 0.5% 1.2% 2.0% 0.9% 0.9% 0.2% 0.6% 2.8% 2.2% 0.8% 0.9% 0.2% 0.1% 0.5% 0.8% 0.9% 0.9% 0.3% 0.6% 0.0%</p><p>1.6% Train: RefinedWeb Eval: C4 eval 0.0% 2.0% 4.0% 6.0% 8.0% 10.0% Relative error bench: QA WikiData <ref type="bibr" target="#b10">[11]</ref>, BoolQ <ref type="bibr" target="#b20">[21]</ref>, Commonsense QA <ref type="bibr" target="#b106">[107]</ref>, COPA <ref type="bibr" target="#b91">[92]</ref>, CoQA <ref type="bibr" target="#b90">[91]</ref>, HellaSwag (zero-shot) <ref type="bibr" target="#b125">[126]</ref>, HellaSwag (10-shot) <ref type="bibr" target="#b125">[126]</ref>, LAMBADA <ref type="bibr" target="#b76">[77]</ref>, PIQA <ref type="bibr" target="#b13">[14]</ref>, PubMed QA Labeled <ref type="bibr" target="#b49">[50]</ref>, SQuAD <ref type="bibr" target="#b89">[90]</ref>, and WinoGrand <ref type="bibr" target="#b54">[55]</ref>. For more details on evaluation datasets see Appendix E. We focus on this subset to ensure we are measuring signal, not noise. Including downstream tasks like MMLU <ref type="bibr" target="#b39">[40]</ref>, where performance is close to random chance, however, does not invalidate our results as we show in our evaluation set ablations (Appendix F).</p><p>Metrics. We consider three main metrics: Validation loss, which is the cross entropy between a model's output and the one-hot ground truth token, averaged over all tokens in a sequence and over all sequences in a dataset. Average top-1 error, which is a uniform average over the 17 downstream evaluations, as mentioned in the above paragraph.</p><p>To measure how good a prediction ζ(C, M ) is, we measure Relative prediction error : |ζ(C, M ) -ζ GT |/ζ GT , where ζ is the predicted loss L or the average top-1 error Err. ζ GT is the ground truth measurement to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results: Reliable extrapolation</head><p>In this Section, we quantify the extent to which the scaling laws developed in Section 2 extrapolate larger model performance using the scaling testbed from Section 3. By default, we fit Equations ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>) to the configurations in Table <ref type="table">1</ref>, use C4 eval for loss, and the 17-task split from Section 3.4 for average top-1 error.</p><p>Over-trained performance is predictable. We highlight our main over-training results in Figure <ref type="figure" target="#fig_1">1</ref> (left). Namely, we are able to extrapolate both in the number of parameters N and the token multiplier M to closely predict the C4 eval performance of a 1.4B parameter model trained on 900B RedPajama tokens (N = 1.4B, M = 640). Our prediction, which takes 300× less compute to construct than the final 1.4B run, is accurate to within 0.7% relative error. Additionally, for the N = 6.9B, M = 20 run, near compute-optimal, the relative error is also 0.7%.  <ref type="formula" target="#formula_4">4</ref>) is useful in practice for predicting over-trained scaling behavior. (iii) Fitting to Equation ( <ref type="formula" target="#formula_4">4</ref>) gives good prediction accuracy near compute-optimal. More specifically, predictions are accurate both for the 1.4B over-trained model and the 6.7B compute-optimal model using a single scaling fit.</p><p>While Figure <ref type="figure" target="#fig_1">1</ref> explores a specific case of making predictions in the over-trained regime, we aim to understand the error profile of our predictions across training datasets, token multipliers, and number of parameters. Hence, Figure <ref type="figure" target="#fig_6">5</ref> shows the relative error between ground truth loss and predicted loss on C4 eval for models in our testbed. We notice uniformly low prediction error suggesting that predictions are accurate in many settings.</p><p>Average top-1 error is predictable. Figure <ref type="figure" target="#fig_1">1</ref> (right) presents our main result in estimating scaling laws for downstream error. Concretely, we use the models indicated in Table <ref type="table">1</ref> to fit Equations ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>), chaining the scaling fits to predict the average top-1 error as a function of training compute C and the token multiplier M . Our fits allow us to predict, using 20× less compute, the downstream performance of a 6.9B model trained on 138B RedPajama tokens to within 0.05% relative error and a 1.4B model trained on RedPajama 900B tokens to within 3.6% relative error.</p><p>Table <ref type="table" target="#tab_4">2</ref> additionally shows the relative error of our downstream performance predictions for models trained on C4, RedPajama, and RefinedWeb, indicating that our scaling law functional forms are applicable on many training datasets. We note that while average accuracy is predictable, individual downstream task predictions are significantly more noisy. We report relative error for more model predictions in Figures <ref type="figure" target="#fig_16">11</ref> and <ref type="figure" target="#fig_2">12</ref>. We also find that if we remove the 1.4B model for the Equation ( <ref type="formula" target="#formula_5">5</ref>) fit, relative error jumps, for instance, from 0.05% to 10.64% on the 17-task split for the 6.9B, 138B token RedPajama prediction. This highlights the importance of investing more compute when constructing scaling laws for downstream task prediction compared to loss prediction.</p><p>Under-training, out-of-distribution scaling, compute-reliability trade-offs. In addition to our main results presented above, we include additional results in Appendix F, which we summarize here. First, we notice that when token multipliers become too small (i.e., M = 5) scaling becomes unreliable and lies off the trend. Additionally, multipliers other than 20, such as 10, 40, and 80, garner points that are roughly on the compute optimal frontier (Figure <ref type="figure" target="#fig_12">9</ref>). This observation suggests that the compute-optimal multiplier may lie in a range rather than take a single value. To probe the limits of reliable scaling, we attempt to break our scaling laws in out-of-distribution settings. We find that models trained on C4-English filtered-and evaluated on next token prediction on code domains have a high relative error in many cases. Perhaps surprisingly, evaluating the same models on German next token prediction gives reliable loss scaling (Figure <ref type="figure" target="#fig_1">10</ref>). We additionally examine the compute necessary to create accurate scaling laws, finding that scaling laws can be constructed more cheaply for loss prediction than for downstream error prediction (Figures <ref type="figure" target="#fig_22">15</ref> and <ref type="figure" target="#fig_23">16</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>We review the most closely related work in this section. For additional related work, see Appendix G.</p><p>Scaling laws. Early works on scaling artificial neural networks observe predictable power-law scaling in the training set size and number of model parameters <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b92">93]</ref>. Alabdulmohsin et al. <ref type="bibr" target="#b1">[2]</ref> stress the importance of looking at the extrapolation regime of a scaling law. Yang et al. <ref type="bibr" target="#b123">[124]</ref> prescribe architectural and hyperparameter changes when scaling model width to realize performant models; Yang et al. <ref type="bibr" target="#b124">[125]</ref> make analogous recommendations when scaling model depth. Bi et al. <ref type="bibr" target="#b12">[13]</ref> propose hyperparameter aware scaling laws. Unlike the aforementioned work, our investigation focuses on over-training and predicting downstream accuracy.</p><p>Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> investigate how the number of model parameters N and training tokens D should be chosen to minimize loss L given a compute budget C. Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> find that when scaling up C, both N and D should be scaled equally up to a multiplicative constant (i.e., N ∝ C ∼0.5 and D ∝ C ∼0.5 ) to realize compute-optimality. Appendix C of the Chinchilla paper additionally suggests that these findings hold across three datasets. However, Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> do not verify their scaling laws for training beyond compute-optimality, or for downstream error prediction-both of which are central to our work.</p><p>Sardana &amp; Frankle <ref type="bibr" target="#b97">[98]</ref> propose modifications to the Chinchilla formulation to incorporate inference costs into the definition of compute-optimality and solve for various fixed inference budgets. Their key finding, which is critical for our work, is that when taking into account a large enough inference budget, it is optimal to train smaller models for longer than the original Chinchilla recommendations.</p><p>Our work presupposes that over-training can be beneficial. Instead of solving for inference-optimal schemes, we support empirically a predictive theory of scaling in the over-trained regime. Additionally, we provide experiments across many validation and training sets.</p><p>For predicting downstream scaling beyond loss, Isik et al. <ref type="bibr" target="#b46">[47]</ref> relate the number of pre-training tokens to downstream cross-entropy and machine translation BLEU score <ref type="bibr" target="#b77">[78]</ref> after fine-tuning. In contrast, we take a holistic approach to evaluation by looking at top-1 error over many natural language tasks. Schaeffer et al. <ref type="bibr" target="#b99">[100]</ref> argue that emergent abilities <ref type="bibr" target="#b119">[120]</ref> are a product of non-linear metrics and propose smoother alternatives. As a warmup for why non-linear metrics may be hard to predict, Schaeffer et al. <ref type="bibr" target="#b99">[100]</ref> consider predicting an ℓ length sequence exactly:</p><formula xml:id="formula_8">Err(N, ℓ) ≈ 1 -PP(N ) -ℓ ,</formula><p>where N is the number of parameters in a model and PP is its perplexity. This is a special case of our Equations ( <ref type="formula" target="#formula_5">5</ref>) and <ref type="bibr" target="#b5">(6)</ref>, where the number of training tokens does not appear, ϵ = 1, k = 1, and γ = ℓ. In contrast, we treat ϵ, k, γ as free parameters for a scaling law fit, finding that average error over downstream tasks can make for a predictable metric.</p><p>Over-training in popular models. There has been a rise in over-trained models <ref type="bibr" target="#b112">[113,</ref><ref type="bibr" target="#b113">114]</ref> and accompanying massive datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b111">112]</ref>. For example, Chinchilla 70B <ref type="bibr" target="#b44">[45]</ref> is trained with a token multiplier of 20, while LLaMA-2 7B <ref type="bibr" target="#b113">[114]</ref> uses a token multiplier of 290. In our investigation, we look at token multipliers from 5 to 640 to ensure coverage of popular models and relevance for future models that may be trained on even more tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations, future work, and conclusion</head><p>Limitations and future work. We identify limitations, which provide motivation for future work.</p><p>• Hyperparameters. While our configurations are surprisingly amenable to reliable scaling across many training and testing distributions without further tuning, there is a need to develop scaling laws that do not require extensive hyperparameter sweeps.</p><p>• Scaling up. Validating the trends in this paper for even larger runs is a valuable direction. Additionally, repeating our setup for models that achieve non-trivial performance on harder evaluations like MMLU is left to future work.</p><p>• Scaling down. Actualizing predictable scaling with even cheaper runs is important to make this area of research more accessible, especially for downstream error prediction.</p><p>• Failure cases. While we present a preliminary analysis of when scaling is unreliable, future work should investigate conditions under which scaling breaks down.</p><p>• Post-training. It is common to employ fine-tuning interventions after pre-training, which we do not consider. Quantifying to what degree over-training the base model provides benefits after post-training is an open area of research.</p><p>• Individual downstream task prediction. While we find that averaging over many task error metrics can make for a predictable metric, per-task predictions are left to future work.</p><p>• In-the-wild performance. Downstream task performance is a proxy for the in-the-wild user experience. Analyzing scaling trends in the context of this experience is timely.</p><p>• Dataset curation. Our work only deals with existing training datasets. Exploring dataset curation for improved model scaling is another promising direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>We show that the loss of over-trained models, trained past compute-optimality, is predictable. Furthermore, we propose and validate a scaling law relating loss to average downstream task performance. We hope our work will inspire others to further examine the relationship between model training and downstream generalization. Our testbed will be made publicly available, and we hope it will make scaling research more accessible to researchers and practitioners alike. </p><formula xml:id="formula_9">L(C, M ) = E + A C 6M -α 2 + B CM 6 -α 2 , = E + A 1 6 -α 2 M α 2 + B 1 6 -α 2 M -α 2 C -α 2 .</formula><p>This is equal to Equation ( <ref type="formula" target="#formula_4">4</ref>), making the substitutions η = α/2, a = A(1/6) -η , b = B(1/6) -η , as noted in the main body.</p><p>Relation to compute-optimal training. Recall that we made the assumption α = β, which implies equal scaling of parameters and tokens to realize compute-optimal models. While this assumption is empirically justified <ref type="bibr" target="#b44">[45]</ref>, even if α ̸ = β, we get a parameterization that implies the power law exponent in Equation ( <ref type="formula" target="#formula_4">4</ref>) remains constant with over-training, while the power law scalar changes.</p><p>To find a compute-optimal training setting, Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> propose to minimize the right-hand side of Equation We now deviate from compute-optimal training by modifying the model size and tokens by multiplication with a constant √ m, according to</p><formula xml:id="formula_10">N m = 1 √ m N * , D m = √ mD * .<label>(7)</label></formula><p>This modification keeps the compute constant (i.e., 6N m D m = 6N * D * ). The risk, then, becomes</p><formula xml:id="formula_11">L(f Nm,Dm ) = E + m α 2 Aγ -α β+α + m -β 2 Bγ β β+α C -αβ α+β . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>We again expect the same power law exponent and changing power law scalar. Note that m in Equation ( <ref type="formula" target="#formula_11">8</ref>) is similar to M in Equation ( <ref type="formula" target="#formula_4">4</ref>). Specifically, m is a multiple of the Chinchilla-optimal token multiplier M * = D * /N * , which is no longer fixed as a compute budget changes for α ̸ = β.   <ref type="formula" target="#formula_4">4</ref>). Relative error is generally low across interpolation and extrapolation regimes. Relative error is largest for the RedPajama N = 1.4B, M = 640 prediction at 15.4%. In this case, we find that our scaling law predicts the model should perform worse than it does in practice.</p><p>Scaling is largely predictable in-distribution (ID). Prior work focuses on understanding scaling using ID loss, often using training loss directly <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51]</ref>. Hence, we also consider Paloma <ref type="bibr" target="#b64">[65]</ref> loss evaluation sets, which are designed to probe performance in specific domains. We use Paloma's C4 <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b87">88]</ref>, RedPajama <ref type="bibr" target="#b111">[112]</ref>, and Falcon-RefinedWeb <ref type="bibr" target="#b81">[82]</ref> splits to probe for ID loss. As seen in Figure <ref type="figure" target="#fig_9">7</ref>, relative error is mostly low. Relative error is largest for the N = 1.4B, M = 640 RedPajama run at 15.4%. Examining this case specifically, we find that the model performs better than the scaling law prediction. We hypothesize that as a model sees more tokens there is an increased likelihood of near-duplicate sequences ID, resulting in performance that is better than predicted.</p><p>Relative error is stable across many choices of downstream evaluation suites. To understand how sensitive our investigation is to our choices of downstream evaluation sets, we consider several other options as seen in Figure <ref type="figure" target="#fig_11">8</ref>. We find that our prediction errors are fairly (i) low and (ii) consistent for many choices of downstream evaluation sets including the whole suite of 46 evaluations. Scaling can break down when under-training. We find that when a token multiple is too small (i.e., under-training regime), scaling appears unreliable. In Figure <ref type="figure" target="#fig_12">9</ref> we see for M = 5 the scaling trend is different. We hypothesize that tuning hyperparameters (e.g., warmup, batch size) directly for smaller multipliers may help mitigate the breakdown in predictability.</p><p>Scaling can be unpredictable out-of-distribution (OOD). Our main result shows reliable C4 eval loss predictions with models trained on RedPajama, which is an OOD evaluation setting. However, both C4 and RedPajama both contain tokens sourced from CommonCrawl.</p><p>To further probe OOD performance, we measure the relative error of scaling laws fit to models trained on C4 and evaluated on Paloma's 100 programming languages <ref type="bibr" target="#b64">[65]</ref>, Paloma's Penn Tree Bank (PTB) split <ref type="bibr" target="#b65">[66]</ref>, and a German version of C4 <ref type="bibr" target="#b26">[27]</ref>. Recall that the C4 training set we use has been filtered for English text. Hence we expect (i) the proportion of code is minimal, (ii) the "&lt;unk&gt;" substrings in PTB raw text do not appear frequently, and (iii) German is not prevalent. We notice that extrapolation relative error tends to be high for large M, N on programming languages and PTB (Figure <ref type="figure" target="#fig_1">10</ref> (left, center)). In contrast, for German C4, relative error is still low across  Recall that we consider a 17 task evaluation suite created by including only test sets where any 0.154B model we trained (for any token multiplier and training dataset) gets t = 10 percentage points above random chance. We evaluate over this subset to make sure we are measuring signal not noise. Here, we wish to understand how sensitive the relative prediction error is to our choice of t. (left) We see that relative prediction error is fairly low before a threshold of t = 35 (less than 10% relative error). When too many tasks are excluded (i.e., t ≥ 40) relative error spikes. Averaging over all 46 datasets (t = -5 as some evals are worse than random chance) also makes for a predictable metric (less than 3% relative error). (right) A parallel view, showing how many tasks are removed as t increases. 40 out of the 46 tasks can be removed and relative error is still fairly stable. the extrapolation range, with a maximum relative error of 7.6% at the N =1.4B, M = 80 scale (Figure <ref type="figure" target="#fig_1">10</ref> (right)). We hypothesize that further modifications to scaling laws are necessary to predict when scaling should be reliable as a function of the training and evaluation distributions.</p><p>Small-scale experiments can predict average downstream top-1 error. To verify that chaining Equations ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>) is effective in practice, we collect C4 eval loss and downstream error pairs for the configurations in Table <ref type="table">1</ref>. In Figure <ref type="figure" target="#fig_16">11</ref>, we look at relative error for our scaling predictions in the context of Average top-1 error over 46 evals and in Figure <ref type="figure" target="#fig_2">12</ref> over the high-signal 17 eval subset. We again notice reliable scaling in interpolation and extrapolation regimes, suggesting the validity of our procedure to predict downstream average top-1 error.</p><p>Loss evaluation ablations for downstream trends. Figure <ref type="figure" target="#fig_18">13</ref> presents the correlation between downstream error and loss evaluated on different validation sets (C4, RedPajama, and RefinedWeb). Regardless of the validation set (x-axis), models follow the exponential decay relationship given in Equation ( <ref type="formula" target="#formula_5">5</ref>), suggesting the choice of validation loss is not critical for the appearance of this phenomenon.</p><p>Investing more compute in a scaling law makes it more predictive. Thus far we have looked at standard configurations from Table <ref type="table">1</ref> to construct our scaling laws, mainly to demonstrate extrapolation to larger N, M . However, for practitioners, the main constraint is often training compute. Hence, we wish to understand the trade-offs between the amount of compute invested in creating a scaling law and the relative error of the resulting law in the over-trained regime. In Figure <ref type="figure" target="#fig_20">14</ref> (left), we see that as one increases the amount of compute, it is possible to get better fits with lower relative error. In Figure <ref type="figure" target="#fig_20">14</ref> (right), we see a similar trend as one increases the number of data points used to fit a scaling law. Blue stars indicate the configurations from Table <ref type="table">1</ref>, which provide accurate predictions relative to the general trends-hinting at their usefulness for our investigation. In Figures <ref type="figure" target="#fig_22">15</ref> and <ref type="figure" target="#fig_23">16</ref> we repeat the compute analysis comparing trade-offs for loss prediction and error prediction for our RedPajama 1.4B parameter, 900B token and 6.9B parameter, 138B token runs respectively. We find that less compute is generally necessary to construct a loss scaling law that achieves the same relative error as that of an error prediction scaling law.</p><p>On compute-optimal token multipliers. We consider 20 tokens per parameter as close to compute-optimal for our experiments. Here we investigate, using different approaches, what the compute-optimal token multipliers are for each dataset-assuming one should scale number of parameter and training tokens equally as Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref> suggest.</p><p>Turning to Figure <ref type="figure" target="#fig_12">9</ref>, we notice that there are many multipliers, between 10 and 80 that yield models close to the frontier. Hence, empirically, it appears choices within this range should be suitable for the optimal token multiplier.</p><p>We can also compute an optimal token multiplier using the coefficients in Table <ref type="table">6</ref>. Based on Hoffmann et al. <ref type="bibr" target="#b44">[45]</ref>'s Equation ( <ref type="formula" target="#formula_4">4</ref>) and the assumption that α = β, we write,</p><formula xml:id="formula_13">N * (C) = G C 6 1 2 , D * (C) = G -1 C 6 1 2 , G = a b 1 4η .<label>(9)</label></formula><p>To compute M * = D * /N * , we then have,</p><formula xml:id="formula_14">M * = b a 1 2η . (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>Using the values from Table <ref type="table">6</ref> and Equation <ref type="bibr" target="#b9">(10)</ref>, we find M * C4 = 3.36, M * RedPajama = 7.42, M * RefinedWeb = 5.85, where the subscript gives the dataset name. These values conflict with the 6.9B N 0.3% 0.2% 0.6% 0.9% 0.2% 0.3% 1.3% 1.2% 0.4% 1.0% 0.1% 0.3% 0.3% 0.0% 0.2% 0.7% 0.5% 1.2% 1.7% 1.0% 0.4% 0.6% 0.2% 0.0% 1.6% 0.0% 0.4% 0.1% 0.4% 1.1% 1.3% 0.2% 0.6% 0.4% 0.1% 0.0% 0.3% 0.8% 1.0% 0.6% 1.3% 0.2% 0.2% 0.0% 0.6% 0.9% 0.9% 0.3% 1.3% 0.8% 1.3% 1.5% 1.0% 1.0% 1.0% 0.3% 3.4%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1%</head><p>Train: RedPajama, Downstream: 46-task split 1.2% 0.1% 0.1% 0.7% 0.3% 0.8% 0.6% 0.5% 1.4% 0.4% 0.9% 0.8% 1.0% 1.2% 0.8% 0.1% 0.6% 0.3% 1.1% 0.7% 0.9% 0.5% 1.1% 1.1% 1.7% 1.6% 0.6% 0.9% 0.3% 4.3%  <ref type="formula" target="#formula_5">5</ref>). Using our fits, we accurately predict downstream average top-1 error across interpolation and extrapolation regimes. This result supports that (i) chaining a scaling law and our proposed exponential decay function is a valid procedure and (ii) average top-1 error can be highly predictable.</p><p>Scaling beyond language modeling. There is a large body of work on scaling neural networks beyond language modeling, for example in computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b126">127]</ref>, multimodal learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>, and image reconstruction <ref type="bibr" target="#b51">[52]</ref>.</p><p>Over-training in existing models. To contextualize the extent to which we over-train, we provide token multipliers for popular models in Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Broader impact</head><p>Language models have known risks in terms of harmful language, toxicity, and human automation-to name a few <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b120">121]</ref>. We include the following for our public release "WARNING: These are base models and not aligned with post-training. They are provided as is and intended as research artifacts only." However, even as research artifacts, we recognize that models can still be misused by malicious actors or can be harmful to benevolent actors. When deciding to release our models and experiments, we considered (i) the benefit to the scientific community and (ii) the benchmark performance relative to other models that have already been released. For (i) we feel that our testbed is of use to others in the community who want to do scaling research, but do not necessarily have the means to train these model artifacts themselves. Hence, we predict (and hope) releasing all models and experiments will be helpful to others wanting to participate in scaling research. For (ii), we note that there are publicly available models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b113">114]</ref>, which outperform models from our testbed and that are more likely to be widely adopted. Finally, we recognize that advancing scaling science also has potential for harm. Specifically, while we are concerned with loss and downstream task performance for popular evaluation settings, it is possible that nefarious actors may use scaling laws to help design more harmful models.   <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b131">132]</ref> reading comprehension multiple choice 3 510 0.25 AGIEval LSAT RC <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b131">132]</ref> reading comprehension multiple choice 3 268 0.25 AGIEval SAT English <ref type="bibr" target="#b131">[132]</ref> reading comprehension multiple choice 3 206 0.25 ARC-Challenge <ref type="bibr" target="#b22">[23]</ref> world knowledge multiple choice 10 2376 0.25 ARC-Easy <ref type="bibr" target="#b22">[23]</ref> world knowledge multiple choice 10 2376 0.25 BBQ <ref type="bibr" target="#b78">[79]</ref> safety multiple choice 3 58492 0.50 BIG-bench: CS algorithms <ref type="bibr" target="#b10">[11]</ref> symbolic problem solving language modeling 10 1320 0.00 BIG-bench: Conceptual combinations <ref type="bibr" target="#b10">[11]</ref> language understanding multiple choice 10 103 0.25 BIG-bench: Conlang translation <ref type="bibr" target="#b10">[11]</ref> language understanding language modeling 0 164 0.00 BIG-bench: Dyck languages <ref type="bibr" target="#b10">[11]</ref> symbolic problem solving language modeling 10 1000 0.00 BIG-bench: Elementary math QA <ref type="bibr" target="#b10">[11]</ref> symbolic problem solving multiple choice 10 38160 0.25 BIG-bench: Language identification <ref type="bibr" target="#b10">[11]</ref> language understanding multiple choice 10 10000 0.25 BIG-bench: Logical deduction <ref type="bibr" target="#b10">[11]</ref> symbolic problem solving multiple choice 10 1500 0.25 BIG-bench: Misconceptions <ref type="bibr" target="#b10">[11]</ref> world knowledge multiple choice 10 219 0.50 BIG-bench: Novel Concepts <ref type="bibr" target="#b10">[11]</ref> commonsense reasoning multiple choice 10 32 0.25 BIG-bench: Operators <ref type="bibr" target="#b10">[11]</ref> symbolic problem solving language modeling 10 210 0.00 BIG-bench: QA WikiData <ref type="bibr" target="#b10">[11]</ref> world knowledge language modeling 10 20321 0.00 BIG-bench: Repeat copy logic <ref type="bibr" target="#b10">[11]</ref> symbolic problem solving language modeling 10 32 0.00 BIG-bench: Strange stories <ref type="bibr" target="#b10">[11]</ref> commonsense reasoning multiple choice 10 174 0.50 BIG-bench: Strategy QA <ref type="bibr" target="#b10">[11]</ref> commonsense reasoning multiple choice 10 2289 0.50 BIG-bench: Understanding fables <ref type="bibr" target="#b10">[11]</ref> reading comprehension multiple choice 10 189 0.25 BoolQ <ref type="bibr" target="#b20">[21]</ref> reading comprehension multiple choice 10 3270 0.50 COPA <ref type="bibr" target="#b91">[92]</ref> commonsense reasoning multiple choice 0 100 0.50 CoQA <ref type="bibr" target="#b90">[91]</ref> reading comprehension language modeling 0 7983 0.00 Commonsense QA <ref type="bibr" target="#b106">[107]</ref> commonsense reasoning multiple choice 10 1221 0.25 Enterprise PII classification <ref type="bibr" target="#b80">[81]</ref> safety multiple choice 10 3395 0.50 HellaSwag (10-shot) <ref type="bibr" target="#b125">[126]</ref> language understanding multiple choice 10 10042 0.25 HellaSwag (zero-shot) <ref type="bibr" target="#b125">[126]</ref> language understanding multiple choice 0 10042 0.25 Jeopardy <ref type="bibr" target="#b68">[69]</ref> world knowledge language modeling 10 2117 0.00 LAMBADA <ref type="bibr" target="#b76">[77]</ref> language understanding language modeling 0 5153 0.00 LogiQA <ref type="bibr" target="#b57">[58]</ref> symbolic problem solving multiple choice 10 651 0.25 MMLU (5-shot) <ref type="bibr" target="#b39">[40]</ref> world knowledge multiple choice 5 14042 0.25 MMLU (zero-shot) <ref type="bibr" target="#b39">[40]</ref> world knowledge multiple choice 0 14042 0.25 MathQA <ref type="bibr" target="#b4">[5]</ref> symbolic problem solving multiple choice 10 2983 0.25 OpenBook QA <ref type="bibr" target="#b67">[68]</ref> commonsense reasoning multiple choice 0 500 0.25 PIQA <ref type="bibr" target="#b13">[14]</ref> commonsense reasoning multiple choice 10 1838 0.50 PubMed QA Labeled <ref type="bibr" target="#b49">[50]</ref> reading comprehension language modeling 10 1000 0.00 SIQA <ref type="bibr" target="#b96">[97]</ref> commonsense reasoning multiple choice 10 1954 0.50 SQuAD <ref type="bibr" target="#b89">[90]</ref> reading comprehension language modeling 10 10570 0.00 Simple Arithmetic: NoSpaces <ref type="bibr" target="#b68">[69]</ref> symbolic problem solving language modeling 10 1000 0.00 Simple Arithmetic: WithSpaces <ref type="bibr" target="#b68">[69]</ref> symbolic problem solving language modeling 10 1000 0.00 WinoGender MC: Female <ref type="bibr" target="#b93">[94]</ref> safety multiple choice 10 60 0.50 WinoGender MC: Male <ref type="bibr" target="#b93">[94]</ref> safety multiple choice 10 60 0.50 WinoGrande <ref type="bibr" target="#b94">[95]</ref> language understanding schema 0 1267 0.50 WinoGrand <ref type="bibr" target="#b54">[55]</ref> language understanding schema 0 273 0.50</p><p>Table 6: Scaling law fit parameters. Here we present our scaling coefficients fit to Equations ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>) using configurations from Table <ref type="table">1</ref>.</p><p>Training dataset Fit for Equation (4): L(C, M ) = Fit for Equation (5):</p><formula xml:id="formula_16">Err(L) = E + (a • M η + b • M -η )C η ϵ -k • exp (-γL)<label>C4</label></formula><p>[27, 88] 1.51 + 141 • M 0.121 + 190 • M -0.121 C -0.121 0.850 -2.08 • exp (-0.756 • L) RedPajama [112] 1.84 + 212 • M 0.136 + 367 • M -0.136 C -0.136 0.857 -2.21 • exp (-0.715 • L) RefinedWeb [82] 1.73 + 157 • M 0.127 + 246 • M -0.127 C -0.127 0.865 -2.21 • exp (-0.707 • L)   We observe that regardless of evaluation loss distribution (x-axis), models tend to follow Equation <ref type="bibr" target="#b4">(5)</ref>. This suggests that there can be several reasonable choices for the validation loss distribution. Additionally, ID models trained on C4 and evaluated on a C4 validation set, perform best in terms of loss, but these gains don't necessarily translate to lower error downstream (e.g., (left column)). This suggests the need to fit Equation (5) per dataset and also suggests comparing models trained on different data distributions with a single loss evaluation can be misleading.   <ref type="formula" target="#formula_4">4</ref>) with as many as 29 models trained on RedPajama. Specifically, a grid formed by N ∈ {0.011B, 0.079B, 0.154B, 0.411B}, M ∈ {5, 10, 20, 40, 80, 160, 320} gives 28 models and a N = 1.4B, M = 20 run gives the last model. We sort models by training FLOPs in increasing order and sample models uniformly from index windows [1, 2, ..., n] for n ∈ <ref type="bibr">[5, 6, .., 29]</ref> to fit Equation (4). The blue star represents the default configuration presented in Table <ref type="table">1</ref>. The prediction target is a N = 1.4B, M = 640 (D = 900B) model. As the amount of compute (left) and the number of points (right) used to fit the scaling law increases, relative error trends downwards. Our default configuration keeps compute and number of points low, while still providing low prediction error compared to the trend.  The compute necessary to accurately predict loss is less than that needed to accurately predict (right) average downstream error. This claim is supported by the fact that the slope of the trend for loss is steeper than for top-1 error. These findings corroborate Figure <ref type="figure" target="#fig_23">16</ref>. The compute necessary to accurately predict loss is less than that needed to accurately predict (right) average downstream error. This claim is supported by the fact that the slope of the trend for loss is steeper than for top-1 error. These findings corroborate Figure <ref type="figure" target="#fig_22">15</ref>.  In Figure <ref type="figure" target="#fig_2">2</ref>, we notice roughly parallel lines (i.e., roughly constant scaling exponent η) in the log-log plot of loss vs. compute, even as the token multiplier M changes. Here we plot η vs. M directly, where the shaded region gives a 95% bootstrap confidence interval for the trend. This view supports that η is relatively constant. Table <ref type="table">8</ref>: Token multipliers of existing models. In our work, we run experiments with token multipliers between 5 and 640 for {GPT-2 <ref type="bibr" target="#b84">[85]</ref>, LLaMA <ref type="bibr" target="#b112">[113]</ref>}-style decoder-only architectures.</p><p>Model family Parameters N Training tokens D Token multiplier M T5 [89] 11B 34B 3.1 GPT-3 [16] 175B 300B 1.7 Gopher [86] 280B 300B 1.1 Chinchilla [45] 70B 1.4T 20.0 LLaMA [113] 7B 1T 140.0 LLaMA [113] 70B 1.4T 20.0 LLaMA-2 [114] 7B 2T 290.0 LLaMA-2 [114] 70B 2T 30.0 XGen [74] 7B 1.5T 210.0 MPT [110] 7B 1T 140.0</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Reliable scaling with over-training and on downstream error prediction. (left) We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training.We are able to extrapolate, in both N and M , the validation performance of models requiring more than 300× the training compute used to construct the scaling law. (right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss. We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction. We predict the average error over 17 downstream tasks for models trained with over 20× the compute. For this figure, we train all models on RedPajama<ref type="bibr" target="#b111">[112]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scaling in the over-trained regime follows consistent power law exponents. We notice parallel lines in the log-log plots of reducible loss vs. training compute for a range of token multipliers M , which give the ratio of training tokens to model parameters. Larger M corresponds to more over-training. For a power law giving reducible loss as a function of compute: L ′ (C) = λ • C -η , the exponent η remains relatively constant resulting in lines with approximately fixed slope (Figure17). The scalar λ that determines the y-intercept, however, shifts with different token multipliers. This suggests λ is a function of the token multiplier, while η is not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Average top-1 error scales as a function of loss. We plot models trained on three datasets and notice an exponential decay of average top-1 error as C4 eval loss, on the x-axis, decreases. We consider on the y-axes average error on 17 evaluations where performance is at least 10 points above random chance for at least one 0.154B scale model. These observations suggest that average top-1 error should be predictable with reliable loss estimates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Relative error on C4 eval for different training distributions. Boxes highlighted in yellow correspond to pairs-number of parameters N , token multiplier M -used to fit Equation (4). Larger values of M correspond to more over-training. The prediction error is low in both interpolation and extrapolation ranges. Below N = 1.4B, empty squares correspond to runs that were not possible due to the limited dataset size for single epoch training. At N = 1.4B we run at M = 20 and at the largest possible multiplier. At N = 6.9B, we run at M = 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 2 . 2 2 . 3 6 3a scaling testbed 6 3 . 1 3 . 2 3 . 3 3 . 4 8 4 36 B</head><label>2223631323334836</label><figDesc>Developing scaling laws for over-training and downstream tasks 3 2.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Scaling laws for over-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Scaling laws for downstream error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Constructing Training setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Model configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Fitting scaling laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Evaluation setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Scaling-law derivationsWe first show that reparameterizing Equation (3) in terms of the compute C and token multiplier M for α = β yields Equation (4). Combining C = 6N D and M = D/N yields N = C/(6M ) and D = CM/6. Inserting these into Equation (3) yields,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 3 ) 1 α+β</head><label>31</label><figDesc>subject to the compute constraint C = 6N D. This yields, N * = γ γ = αA βB , for notational convenience. The associated risk is, L(N * , D * ) = E + Aγ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: In-distribution (ID) settings. Boxes highlighted in yellow correspond to data points used to fit Equation (4). Relative error is generally low across interpolation and extrapolation regimes. Relative error is largest for the RedPajama N = 1.4B, M = 640 prediction at 15.4%. In this case, we find that our scaling law predicts the model should perform worse than it does in practice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>t (i.e., include evals where any model gets t percentage points above random chance at 0.154B scales)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Downstream evaluation set ablation for 6.9B parameter, 138B token runs.Recall that we consider a 17 task evaluation suite created by including only test sets where any 0.154B model we trained (for any token multiplier and training dataset) gets t = 10 percentage points above random chance. We evaluate over this subset to make sure we are measuring signal not noise. Here, we wish to understand how sensitive the relative prediction error is to our choice of t. (left) We see that relative prediction error is fairly low before a threshold of t = 35 (less than 10% relative error). When too many tasks are excluded (i.e., t ≥ 40) relative error spikes. Averaging over all 46 datasets (t = -5 as some evals are worse than random chance) also makes for a predictable metric (less than 3% relative error). (right) A parallel view, showing how many tasks are removed as t increases. 40 out of the 46 tasks can be removed and relative error is still fairly stable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Scaling with small token multipliers. For smaller multipliers (e.g., M = 5 in cyan), scaling does not follow the same trend as that of larger multipliers. Additionally, many token multipliers (e.g., M ∈ {10, 20, 40, 80}) garner points close to the compute-optimal frontier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Relative error on average top-1 predictions (46 task split). Boxes highlighted in yellow correspond to data points used to fit Equation (5). Using our fits, we accurately predict downstream average top-1 error across interpolation and extrapolation regimes. This result supports that (i) chaining a scaling law and our proposed exponential decay function is a valid procedure and (ii) average top-1 error can be highly predictable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Correlation between average top-1 error and evaluation loss. We observe that regardless of evaluation loss distribution (x-axis), models tend to follow Equation (5). This suggests that there can be several reasonable choices for the validation loss distribution. Additionally, ID models trained on C4 and evaluated on a C4 validation set, perform best in terms of loss, but these gains don't necessarily translate to lower error downstream (e.g., (left column)). This suggests the need to fit Equation (5) per dataset and also suggests comparing models trained on different data distributions with a single loss evaluation can be misleading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Trade-offs between scaling law for loss fitting considerations and reliability. Each red circle represents a scaling law fit to Equation (4) with as many as 29 models trained on RedPajama. Specifically, a grid formed by N ∈ {0.011B, 0.079B, 0.154B, 0.411B}, M ∈ {5, 10, 20, 40, 80, 160, 320} gives 28 models and a N = 1.4B, M = 20 run gives the last model. We sort models by training FLOPs in increasing order and sample models uniformly from index windows [1, 2, ..., n] for n ∈ [5, 6, .., 29] to fit Equation (4). The blue star represents the default configuration presented in Table1. The prediction target is a N = 1.4B, M = 640 (D = 900B) model. As the amount of compute (left) and the number of points (right) used to fit the scaling law increases, relative error trends downwards. Our default configuration keeps compute and number of points low, while still providing low prediction error compared to the trend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><figDesc>] used for the scaling fit Relative error: 17-task split Trend Default setting from Table2Individual estimates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Compute vs. relative error for the 1.4B, 900B token RedPajama run. (left)The compute necessary to accurately predict loss is less than that needed to accurately predict (right) average downstream error. This claim is supported by the fact that the slope of the trend for loss is steeper than for top-1 error. These findings corroborate Figure16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Compute vs. relative error for the 6.9B, 138B token RedPajama run. (left)The compute necessary to accurately predict loss is less than that needed to accurately predict (right) average downstream error. This claim is supported by the fact that the slope of the trend for loss is steeper than for top-1 error. These findings corroborate Figure15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 17 :</head><label>17</label><figDesc>Figure17: Scaling exponent vs. token multiplier. In Figure2, we notice roughly parallel lines (i.e., roughly constant scaling exponent η) in the log-log plot of loss vs. compute, even as the token multiplier M changes. Here we plot η vs. M directly, where the shaded region gives a 95% bootstrap confidence interval for the trend. This view supports that η is relatively constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Downstream relative prediction error at 6.9B parameters and 138B tokens. While predicting accuracy on individual zero-shot downstream evaluations can be challenging ("Individual"), predicting averages across downstream datasets is accurate ("Avg.").Scaling can be predictable even when one increases both the model size and the amount of over-training compared to the training runs used to fit a scaling law. (ii) The form presented in Equation (</figDesc><table><row><cell>Individual top-1 error</cell><cell>Avg. top-1 error</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Main models and hyperparameters used in our investigation. Models have number of parameters N , with number of layers n layers , number of attention heads n heads , model width d model , and width per attention head d head . Batch sizes are global and in units of sequences. Each sequence has 2,048 tokens. A100 GPU hours are at M = 20, which are near compute-optimal runs. For the 1.4B scale, a batch size of 256 performs slightly better than 512.</figDesc><table><row><cell></cell><cell>0.011B</cell><cell>1.1% 0.0% 0.1% 0.7% 0.9% 0.0% 0.6% Train: C4 Eval: C4 (Paloma split)</cell><cell>4.6% 0.0% 0.3% 2.8% 1.5% 0.0% 0.8% Train: RedPajama Eval: RedPajama (Paloma split)</cell><cell>1.1% 0.0% 0.9% 1.6% 0.8% 0.0% 1.1% Train: RefinedWeb Eval: RefinedWeb (Paloma split)</cell><cell cols="2">10.0%</cell></row><row><cell></cell><cell>0.079B</cell><cell>2.6% 0.3% 0.2% 0.8% 0.2% 0.3% 1.4%</cell><cell>0.5% 0.0% 1.1% 2.2% 3.0% 3.5% 3.3%</cell><cell>2.3% 0.1% 0.4% 1.0% 2.0% 3.2% 2.3%</cell><cell>8.0%</cell><cell></cell></row><row><cell>N</cell><cell>0.154B 0.411B 1.4B 6.9B</cell><cell>10 2.0% 0.5% 1.3% 1.0% 3.9% 3.5% 1.3% 20 40 80 160 320 640 M 0.2% 0.2% 0.2% 3.5% 0.5% 2.7% 0.4% 2.3% 3.6%</cell><cell>10 0.2% 0.0% 0.5% 1.3% 1.4% 1.0% 0.5% 20 40 80 160 320 640 M 0.1% 0.0% 0.2% 0.3% 0.7%10.0%10.3% 3.0% 15.4% 10.3%</cell><cell>10 1.1% 0.2% 0.1% 1.5% 0.8% 2.0% 2.4% 20 40 80 160 320 M 640 0.7% 0.1% 1.6% 2.3% 2.0% 2.3% 4.3% 1.4% 5.6% 6.0%</cell><cell>0.0% 2.0% 4.0% 6.0%</cell><cell>Relative error</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Topologies for our grid searches. We consider 130 architectures for our grid search. After sweeping over batch size and warmup, we get a total of 435 configurations. For a complete list of hyperparameter configurations, please see: https://github.com/mlfoundations/scaling</figDesc><table><row><cell>n layers</cell><cell>n heads</cell><cell>d model</cell><cell>Number of</cell><cell>n layers</cell><cell>n heads</cell><cell>d model</cell><cell>Number of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>parameters [B]</cell><cell></cell><cell></cell><cell></cell><cell>parameters [B]</cell></row><row><cell>4</cell><cell>4</cell><cell>96</cell><cell>0.010</cell><cell>12</cell><cell>4</cell><cell>512</cell><cell>0.093</cell></row><row><cell>4</cell><cell>12</cell><cell>96</cell><cell>0.010</cell><cell>16</cell><cell>12</cell><cell>488</cell><cell>0.100</cell></row><row><cell>12</cell><cell>12</cell><cell>96</cell><cell>0.011</cell><cell>8</cell><cell>16</cell><cell>640</cell><cell>0.105</cell></row><row><cell>12</cell><cell>4</cell><cell>96</cell><cell>0.011</cell><cell>8</cell><cell>4</cell><cell>640</cell><cell>0.105</cell></row><row><cell>8</cell><cell>4</cell><cell>96</cell><cell>0.011</cell><cell>8</cell><cell>8</cell><cell>640</cell><cell>0.105</cell></row><row><cell>16</cell><cell>4</cell><cell>96</cell><cell>0.011</cell><cell>12</cell><cell>8</cell><cell>576</cell><cell>0.106</cell></row><row><cell>16</cell><cell>12</cell><cell>96</cell><cell>0.011</cell><cell>16</cell><cell>16</cell><cell>512</cell><cell>0.106</cell></row><row><cell>8</cell><cell>12</cell><cell>96</cell><cell>0.011</cell><cell>4</cell><cell>4</cell><cell>768</cell><cell>0.106</cell></row><row><cell>24</cell><cell>4</cell><cell>96</cell><cell>0.012</cell><cell>12</cell><cell>12</cell><cell>576</cell><cell>0.106</cell></row><row><cell>24</cell><cell>12</cell><cell>96</cell><cell>0.012</cell><cell>16</cell><cell>8</cell><cell>512</cell><cell>0.106</cell></row><row><cell>4</cell><cell>4</cell><cell>192</cell><cell>0.021</cell><cell>4</cell><cell>8</cell><cell>768</cell><cell>0.106</cell></row><row><cell>4</cell><cell>8</cell><cell>192</cell><cell>0.021</cell><cell>12</cell><cell>4</cell><cell>576</cell><cell>0.106</cell></row><row><cell>4</cell><cell>12</cell><cell>192</cell><cell>0.021</cell><cell>4</cell><cell>16</cell><cell>768</cell><cell>0.106</cell></row><row><cell>8</cell><cell>8</cell><cell>192</cell><cell>0.023</cell><cell>16</cell><cell>4</cell><cell>512</cell><cell>0.106</cell></row><row><cell>8</cell><cell>4</cell><cell>192</cell><cell>0.023</cell><cell>4</cell><cell>12</cell><cell>768</cell><cell>0.106</cell></row><row><cell>8</cell><cell>12</cell><cell>192</cell><cell>0.023</cell><cell>16</cell><cell>12</cell><cell>576</cell><cell>0.122</cell></row><row><cell>12</cell><cell>4</cell><cell>192</cell><cell>0.025</cell><cell>16</cell><cell>4</cell><cell>576</cell><cell>0.122</cell></row><row><cell>12</cell><cell>8</cell><cell>192</cell><cell>0.025</cell><cell>16</cell><cell>8</cell><cell>576</cell><cell>0.122</cell></row><row><cell>12</cell><cell>12</cell><cell>192</cell><cell>0.025</cell><cell>12</cell><cell>4</cell><cell>640</cell><cell>0.126</cell></row><row><cell>16</cell><cell>4</cell><cell>192</cell><cell>0.026</cell><cell>24</cell><cell>12</cell><cell>488</cell><cell>0.126</cell></row><row><cell>16</cell><cell>8</cell><cell>192</cell><cell>0.026</cell><cell>12</cell><cell>16</cell><cell>640</cell><cell>0.126</cell></row><row><cell>16</cell><cell>12</cell><cell>192</cell><cell>0.026</cell><cell>12</cell><cell>8</cell><cell>640</cell><cell>0.126</cell></row><row><cell>24</cell><cell>8</cell><cell>192</cell><cell>0.030</cell><cell>24</cell><cell>8</cell><cell>512</cell><cell>0.133</cell></row><row><cell>24</cell><cell>4</cell><cell>192</cell><cell>0.030</cell><cell>24</cell><cell>4</cell><cell>512</cell><cell>0.133</cell></row><row><cell>24</cell><cell>12</cell><cell>192</cell><cell>0.030</cell><cell>24</cell><cell>16</cell><cell>512</cell><cell>0.133</cell></row><row><cell>4</cell><cell>12</cell><cell>288</cell><cell>0.033</cell><cell>8</cell><cell>8</cell><cell>768</cell><cell>0.134</cell></row><row><cell>4</cell><cell>4</cell><cell>288</cell><cell>0.033</cell><cell>8</cell><cell>16</cell><cell>768</cell><cell>0.134</cell></row><row><cell>8</cell><cell>12</cell><cell>288</cell><cell>0.037</cell><cell>8</cell><cell>4</cell><cell>768</cell><cell>0.134</cell></row><row><cell>8</cell><cell>4</cell><cell>288</cell><cell>0.037</cell><cell>8</cell><cell>12</cell><cell>768</cell><cell>0.134</cell></row><row><cell>4</cell><cell>4</cell><cell>320</cell><cell>0.038</cell><cell>16</cell><cell>16</cell><cell>640</cell><cell>0.146</cell></row><row><cell>4</cell><cell>8</cell><cell>320</cell><cell>0.038</cell><cell>16</cell><cell>8</cell><cell>640</cell><cell>0.146</cell></row><row><cell>12</cell><cell>12</cell><cell>288</cell><cell>0.041</cell><cell>16</cell><cell>4</cell><cell>640</cell><cell>0.146</cell></row><row><cell>12</cell><cell>4</cell><cell>288</cell><cell>0.041</cell><cell>24</cell><cell>8</cell><cell>576</cell><cell>0.154</cell></row><row><cell>8</cell><cell>8</cell><cell>320</cell><cell>0.043</cell><cell>24</cell><cell>4</cell><cell>576</cell><cell>0.154</cell></row><row><cell>8</cell><cell>4</cell><cell>320</cell><cell>0.043</cell><cell>24</cell><cell>12</cell><cell>576</cell><cell>0.154</cell></row><row><cell>16</cell><cell>4</cell><cell>288</cell><cell>0.045</cell><cell>4</cell><cell>8</cell><cell>1024</cell><cell>0.155</cell></row><row><cell>16</cell><cell>12</cell><cell>288</cell><cell>0.045</cell><cell>4</cell><cell>16</cell><cell>1024</cell><cell>0.155</cell></row><row><cell>12</cell><cell>4</cell><cell>320</cell><cell>0.049</cell><cell>4</cell><cell>4</cell><cell>1024</cell><cell>0.155</cell></row><row><cell>12</cell><cell>8</cell><cell>320</cell><cell>0.049</cell><cell>12</cell><cell>8</cell><cell>768</cell><cell>0.162</cell></row><row><cell>24</cell><cell>4</cell><cell>288</cell><cell>0.053</cell><cell>12</cell><cell>4</cell><cell>768</cell><cell>0.162</cell></row><row><cell>24</cell><cell>12</cell><cell>288</cell><cell>0.053</cell><cell>12</cell><cell>12</cell><cell>768</cell><cell>0.162</cell></row><row><cell>16</cell><cell>8</cell><cell>320</cell><cell>0.055</cell><cell>12</cell><cell>16</cell><cell>768</cell><cell>0.162</cell></row><row><cell>16</cell><cell>4</cell><cell>320</cell><cell>0.055</cell><cell>24</cell><cell>16</cell><cell>640</cell><cell>0.186</cell></row><row><cell>4</cell><cell>12</cell><cell>488</cell><cell>0.062</cell><cell>24</cell><cell>8</cell><cell>640</cell><cell>0.186</cell></row><row><cell>4</cell><cell>4</cell><cell>512</cell><cell>0.065</cell><cell>24</cell><cell>4</cell><cell>640</cell><cell>0.186</cell></row><row><cell>4</cell><cell>16</cell><cell>512</cell><cell>0.065</cell><cell>16</cell><cell>16</cell><cell>768</cell><cell>0.191</cell></row><row><cell>4</cell><cell>8</cell><cell>512</cell><cell>0.065</cell><cell>16</cell><cell>4</cell><cell>768</cell><cell>0.191</cell></row><row><cell>24</cell><cell>8</cell><cell>320</cell><cell>0.066</cell><cell>16</cell><cell>8</cell><cell>768</cell><cell>0.191</cell></row><row><cell>24</cell><cell>4</cell><cell>320</cell><cell>0.066</cell><cell>16</cell><cell>12</cell><cell>768</cell><cell>0.191</cell></row><row><cell>4</cell><cell>4</cell><cell>576</cell><cell>0.074</cell><cell>8</cell><cell>8</cell><cell>1024</cell><cell>0.206</cell></row><row><cell>4</cell><cell>8</cell><cell>576</cell><cell>0.074</cell><cell>8</cell><cell>4</cell><cell>1024</cell><cell>0.206</cell></row><row><cell>4</cell><cell>12</cell><cell>576</cell><cell>0.074</cell><cell>8</cell><cell>16</cell><cell>1024</cell><cell>0.206</cell></row><row><cell>8</cell><cell>12</cell><cell>488</cell><cell>0.075</cell><cell>24</cell><cell>8</cell><cell>768</cell><cell>0.247</cell></row><row><cell>8</cell><cell>4</cell><cell>512</cell><cell>0.079</cell><cell>24</cell><cell>12</cell><cell>768</cell><cell>0.247</cell></row><row><cell>8</cell><cell>8</cell><cell>512</cell><cell>0.079</cell><cell>24</cell><cell>4</cell><cell>768</cell><cell>0.247</cell></row><row><cell>8</cell><cell>16</cell><cell>512</cell><cell>0.079</cell><cell>24</cell><cell>16</cell><cell>768</cell><cell>0.247</cell></row><row><cell>4</cell><cell>4</cell><cell>640</cell><cell>0.085</cell><cell>12</cell><cell>8</cell><cell>1024</cell><cell>0.257</cell></row><row><cell>4</cell><cell>16</cell><cell>640</cell><cell>0.085</cell><cell>12</cell><cell>4</cell><cell>1024</cell><cell>0.257</cell></row><row><cell>4</cell><cell>8</cell><cell>640</cell><cell>0.085</cell><cell>12</cell><cell>16</cell><cell>1024</cell><cell>0.257</cell></row><row><cell>12</cell><cell>12</cell><cell>488</cell><cell>0.087</cell><cell>16</cell><cell>8</cell><cell>1024</cell><cell>0.309</cell></row><row><cell>8</cell><cell>4</cell><cell>576</cell><cell>0.090</cell><cell>16</cell><cell>4</cell><cell>1024</cell><cell>0.309</cell></row><row><cell>8</cell><cell>12</cell><cell>576</cell><cell>0.090</cell><cell>16</cell><cell>16</cell><cell>1024</cell><cell>0.309</cell></row><row><cell>8</cell><cell>8</cell><cell>576</cell><cell>0.090</cell><cell>24</cell><cell>16</cell><cell>1024</cell><cell>0.412</cell></row><row><cell>12</cell><cell>16</cell><cell>512</cell><cell>0.093</cell><cell>24</cell><cell>8</cell><cell>1024</cell><cell>0.412</cell></row><row><cell>12</cell><cell>8</cell><cell>512</cell><cell>0.093</cell><cell>24</cell><cell>4</cell><cell>1024</cell><cell>0.412</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>46 downstream tasks. All downstream tasks considered in this work, evaluated via LLM-foundry<ref type="bibr" target="#b68">[69]</ref>. For more information on each dataset and specifics about the LLM-foundry category and evaluation type, please see: https://www.mosaicml.com/llm-evaluation.</figDesc><table><row><cell>Downstream task</cell><cell>LLM-foundry category</cell><cell>Evaluation type</cell><cell cols="3">Shots Samples Baseline</cell></row><row><cell>AGIEval LSAT AR [118, 131, 132]</cell><cell cols="2">symbolic problem solving multiple choice</cell><cell>3</cell><cell>230</cell><cell>0.25</cell></row><row><cell>AGIEval LSAT LR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Downstream relative prediction error at 6.9B, 138B tokens, with and without the 1.4B data point. Recall in Table1, we introduce a N = 1.4B, M = 20 run to get better downstream error predictions. Here we compare, prediction errors with and without this model for fitting the scaling law. Note that without the model (i.e., rows with "w/o 1.4B") average top-1 predictions, over the 17 tasks. are less accurate.</figDesc><table><row><cell>Scaling law fit</cell><cell>Train set</cell><cell cols="5">ARC-E LAMBADA OpenBook QA HellaSwag 17 eval</cell></row><row><cell></cell><cell></cell><cell>[23]</cell><cell>[77]</cell><cell>[68]</cell><cell>[126]</cell><cell></cell></row><row><cell>Table 1</cell><cell>C4 [27, 88]</cell><cell>28.96%</cell><cell>15.01%</cell><cell>16.80%</cell><cell>79.58%</cell><cell>0.14%</cell></row><row><cell cols="2">Table 1 w/o 1.4B C4 [27, 88]</cell><cell>0.92%</cell><cell>2.04%</cell><cell>96.16%</cell><cell>61.79%</cell><cell>0.42%</cell></row><row><cell>Table 1</cell><cell>RedPajama [112]</cell><cell>5.21%</cell><cell>14.39%</cell><cell>8.44%</cell><cell>25.73%</cell><cell>0.05%</cell></row><row><cell cols="2">Table 1 w/o 1.4B RedPajama [112]</cell><cell>8.13%</cell><cell>11.07%</cell><cell>7.56%</cell><cell>30.98%</cell><cell>10.64%</cell></row><row><cell>Table 1</cell><cell>RefinedWeb [82]</cell><cell>26.06%</cell><cell>16.55%</cell><cell>1.92%</cell><cell>81.96%</cell><cell>2.94%</cell></row><row><cell cols="2">Table 1 w/o 1.4B RefinedWeb [82]</cell><cell>15.39%</cell><cell>6.26%</cell><cell>6.79%</cell><cell>6.52%</cell><cell>15.79%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>SYG is supported by an <rs type="funder">NSF</rs> <rs type="grantName">Graduate Research Fellowship</rs>, GS by the <rs type="funder">Onassis Foundation -Scholarship ID</rs>: <rs type="grantNumber">F ZS 056-1/2022-2023</rs>, and MN by the <rs type="funder">Federal Ministry of Education and Research of Germany</rs> under grant no. <rs type="grantNumber">01IS22094B</rs> <rs type="funder">WEST-AI</rs>. We thank <rs type="person">Stability AI</rs> and <rs type="institution">Toyota Research Institute (TRI)</rs> for access to compute resources. This research has been supported by <rs type="funder">NSF</rs> Grants <rs type="grantNumber">AF 1901292</rs>, <rs type="grantNumber">CNS 2148141</rs>, Tripods CCF 1934932, IFML CCF 2019844, and research gifts by <rs type="funder">Western Digital, Amazon, WNCG IAP</rs>, <rs type="institution">UT Austin Machine Learning Lab (MLL)</rs>, <rs type="funder">Cisco</rs>, and the <rs type="funder">Stanly P. Finch Centennial Professorship in Engineering</rs>. We also thank <rs type="person">Kushal Arora</rs>, <rs type="person">Alper Canberk</rs>, <rs type="person">Mia Chiquier</rs>, <rs type="person">Sachit Menon</rs>, <rs type="person">Chuer Pan</rs>, <rs type="person">Purva Tendulkar</rs>, and <rs type="person">Mandi Zhao</rs> for valuable feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zVwmXfB">
					<orgName type="grant-name">Graduate Research Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_ZGjmZzg">
					<idno type="grant-number">F ZS 056-1/2022-2023</idno>
				</org>
				<org type="funding" xml:id="_SjFktgc">
					<idno type="grant-number">01IS22094B</idno>
				</org>
				<org type="funding" xml:id="_XggEybP">
					<idno type="grant-number">AF 1901292</idno>
				</org>
				<org type="funding" xml:id="_Y3qF92Q">
					<idno type="grant-number">CNS 2148141</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Contributions</head><p>Names ordered alphabetically.</p><p>Model training and experiment babysitting. Achal Dave (notably, the 1.4B parameter, 900B token run), Samir Yitzhak Gadre</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataloading. Georgios Smyrnis</head><p>Training tokens. Achal Dave, Alex Fang, Samir Yitzhak Gadre, Suchin Gururangan, Jeffrey Li, Vaishaal Shankar (lead), Mitchell Wortsman Evaluation tokens. Achal Dave, Samir Yitzhak Gadre, Reinhard Heckel, Vaishaal Shankar (lead), Rulin Shao Loss/perplexity evaluation. Samir Yitzhak Gadre Downstream evaluation. Vaishaal Shankar Project-specific planning, infrastructure, plots, and analysis. Samir Yitzhak Gadre OpenLM <ref type="bibr" target="#b38">[39]</ref> open-source infrastructure. Achal Dave (core contributor), Alex Fang, Samir Yitzhak Gadre (core contributor), Suchin Gururangan (core contributor), Jenia Jitsev, Sedrick Keh, Jeffrey Li, Jean Mercat, Marianna Nezhurina, Vaishaal Shankar (core contributor), Georgios Smyrnis (core contributor), Igor Vasiljevic, Mitchell Wortsman (core contributor), Rui Xin Theory. Yair Carmon (original idea that "parallel lines" should show up in scaling plots), Samir Yitzhak Gadre (various derivations, empirical verification, related validation loss to average top-1 error as in Equation ( <ref type="formula">5</ref>)), Reinhard Heckel (derived a scaling form based on Chinchilla Approach 3 <ref type="bibr" target="#b44">[45]</ref>, which appears in Equation ( <ref type="formula">4</ref>)), Niklas Muennighoff (derived a scaling form based on Chinchilla Approach 3, similar to Equation (4)), Mitchell Wortsman (provided intuition about irreducible loss and why it is critical). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Writing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional training details</head><p>Architecture. As stated in the main paper, we train transformers <ref type="bibr" target="#b115">[116]</ref>, based on auto-regressive, decoder-only, pre-normalization architectures like GPT-2 <ref type="bibr" target="#b84">[85]</ref> and LLaMA <ref type="bibr" target="#b112">[113]</ref>. We adopt OpenLM <ref type="bibr" target="#b38">[39]</ref> for modeling, which utilizes PyTorch <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b79">80]</ref>, xformers <ref type="bibr" target="#b53">[54]</ref>, triton <ref type="bibr" target="#b74">[75]</ref>, FlashAttention <ref type="bibr" target="#b23">[24]</ref>, FSDP <ref type="bibr" target="#b129">[130]</ref>, and bfloat16 automatic mixed precision. Like LLaMA, we omit bias terms, but replace RMSNorm <ref type="bibr" target="#b127">[128]</ref> with LayerNorm <ref type="bibr" target="#b7">[8]</ref>, which has readily available fused implementations. Following Wortsman et al. <ref type="bibr" target="#b122">[123]</ref>, we apply qk-LayerNorm <ref type="bibr" target="#b24">[25]</ref>, which adds robustness to otherwise poor hyperparameter choices (e.g., learning rate). We use SwiGLU <ref type="bibr" target="#b101">[102]</ref> activations and depth-scaled initialization <ref type="bibr" target="#b128">[129]</ref>. We use a sequence length of 2,048, rotary positional embeddings <ref type="bibr" target="#b105">[106]</ref>, and the GPT-NeoX-20B tokenizer <ref type="bibr" target="#b14">[15]</ref>, which yields a vocabulary size of 50k. We do not use weight tying <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b83">84]</ref>. We sample without replacement during training and employ sequence packing without attention masking. We separate documents in our training corpora with end-of-text tokens.</p><p>Objectives and optimization. We train with a standard causal language modeling objective (i.e., next token prediction) with an additive z-loss <ref type="bibr" target="#b18">[19]</ref> (coefficient 1e-4), which mitigates output logit norm growth <ref type="bibr" target="#b66">[67]</ref> instabilities. We use the AdamW optimizer <ref type="bibr" target="#b61">[62]</ref> (PyTorch defaults except beta2 = 0.95), with independent weight decay <ref type="bibr" target="#b122">[123]</ref> (coefficient 1e-4). For the learning rate schedule, we use linear warmup and cosine decay. We cool down to a low learning rate (3e-5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional grid search details</head><p>Final model configurations. We present our final hyperparameters in Table <ref type="table">3</ref>.</p><p>Grid search configuration selection. Recall in Section 3.3, we run a grid search over many configurations. We present the architectures we sweep over in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Evaluation dataset details</head><p>All 46 downstream evaluations are based on MosaicML's LLM-foundry evaluation suite <ref type="bibr" target="#b68">[69]</ref>. We specifically consider the datasets given in Table <ref type="table">5</ref>. Recall that we use a subset of 17 of these  FLOPs over-perform relative to their neighbors. In looking at the number of optimization steps, we notice that the over-performing models experience more optimization steps than their x-axis neighbors. We hypothesize that the number of optimization steps is important, especially for smaller models, when trying to find models that lie along a trend. (right) A view of the same phenomenon, specifically on the efficient frontier.</p><p>evaluations that give signal (are above random chance) for the compute range we consider. See Appendix F, where we ablate over the 17 subset design choice by including more and less evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional results</head><p>Scaling law fits. We present specific coefficients for our fits in Table <ref type="table">6</ref>.</p><p>Small-scale experiments can predict model rank order. We expect to be able to rank hypothetical models based on their predicted performance, which is useful when deciding what large-scale runs to train. To verify, we rank 9 testbed models with N ≥ 1.4B by ground-truth top-1 error and by estimated top-1 error. We find high rank correlation of 0.88 for the 17-task split.</p><p>Over-performing grid search models experience more optimization steps. As mentioned in Section 3.3 and Figure <ref type="figure">4</ref>, we notice that models between 0.011B to 0.079B (i.e., 5.2 × 10 16 to 5.2 × 10 17 FLOPs trained near compute-optimal) over-perform compared to the trend established by other models in our initial grid searches. This results in a bump in the scaling plot. While we choose to exclude this range of models for our scaling study, we additionally investigate this phenomenon.</p><p>In Figure <ref type="figure">6</ref> we color grid search configurations by the number of optimization steps (i.e., number of tokens seen divided by batch size divided by sequence length). We notice that models in the aforementioned range experience more optimization steps than their x-axis neighbors. For context, Figure <ref type="figure">1</ref> (left) in Kaplan et al. <ref type="bibr" target="#b50">[51]</ref> also shows a bump; however, there the performance is worse than the general trend instead of better as in our work. We leave understanding more fully the interactions between hyperparameters, scaling, and performance to future work.    <ref type="formula">4</ref>). Recall that the C4 training set is English-filtered. Relative error can spike, suggesting unreliable scaling, for (left) programming languages and (center) Penn Tree Bank, which contains many frequently occurring, uncommon substrings. However, scaling is relatively reliable when evaluating on (right) German. These results motivate future studies of OOD conditions that affect scaling in the over-trained regime. observation in Figure <ref type="figure">9</ref>, which suggests M = 5 is already too small to give points on the Pareto frontier. We hypothesize this mismatch arises because we fit our scaling laws using models with M ≥ 20. Additionally, we hyperparamter-tune at M = 20. As previously discussed, it is likely possible to find better hyperparameter configurations at M = 5 with further hyperparameter tuning at this token multiplier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional related work</head><p>Language modeling. Language models can be grouped into encoder-only <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b95">96]</ref>, encoder-decoder <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b88">89]</ref>, and decoder-only architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b121">122]</ref>. Most current implementations are based on the transformer <ref type="bibr" target="#b115">[116]</ref>. However, there has been a recent resurgence in scaling language models based on non-transformer architectures <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b82">83]</ref>. Further, there has been substantial work on adapting pre-trained language models to better follow instructions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b132">133]</ref>. However, following prior work <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b71">72]</ref> and given their overall prevalence, we limit ourselves to GPT-style, decoder-only transformers that have solely been pre-trained.</p><p>Scaling laws. Kaplan et al. <ref type="bibr" target="#b50">[51]</ref> investigate scaling trends in GPT language models. Bahri et al. <ref type="bibr" target="#b8">[9]</ref> investigate different scaling regimes theoretically, and Sharma &amp; Kaplan <ref type="bibr" target="#b100">[101]</ref> relate scaling coefficients to data manifold dimensions. Tay et al. <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b108">109]</ref> elucidate the connection between model architecture and scaling trends, while Hernandez et al. <ref type="bibr" target="#b41">[42]</ref>, Tay et al. <ref type="bibr" target="#b107">[108]</ref> develop scaling laws for transfer learning. Ivgi et al. <ref type="bibr" target="#b47">[48]</ref> also consider transfer learning scaling laws and highlight the importance of hyperparameter selection in the low-compute regime. Bansal et al. <ref type="bibr" target="#b9">[10]</ref>, Ghorbani et al. <ref type="bibr" target="#b31">[32]</ref>, Gordon et al. <ref type="bibr" target="#b32">[33]</ref> develop scaling laws for neural machine translation. Caballero et al. <ref type="bibr" target="#b16">[17]</ref> propose a scaling law functional form, which they demonstrate is predictive in several domains.    <ref type="formula">5</ref>). Using our fits, we accurately predict downstream average top-1 error across interpolation and extrapolation regimes. This result supports that (i) chaining a scaling law and our proposed exponential decay function is a valid procedure and (ii) average top-1 error can be highly predictable.  Here we plot models from our testbed for each scatter plot. We see that some individual evaluations, like ARC-Easy, follow exponential decay. Others, like BIG-bench: CS algorithms, show step function behavior. Still others, like MathQA, hover around random chance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring the limits of large scale pre-training</title>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.02095" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Revisiting neural scaling laws in language and vision</title>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2209.06640" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey on data selection for language models</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bairu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haewon</forename><surname>Jeong</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.16827" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Santacoder: don&apos;t reach for the stars! arXiv preprint</title>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Munoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2301.03988" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MathQA: Towards interpretable math word problem solving with operation-based formalisms</title>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1245" />
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Voznesensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geeta</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Chourdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherlock</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Reso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saroufim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eikan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://pytorch.org/blog/pytorch-2-paper-tutorial" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient large scale language modeling with mixtures of experts</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giridharan</forename><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halil</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.804" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Explaining neural scaling laws</title>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Sharma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.06701" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data scaling laws in nmt: The effect of noise and architecture</title>
		<author>
			<persName><forename type="first">Yamini</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behrooz</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/bansal22b.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<ptr target="https://openreview.net/forum?id=uyTL5Bvosj" />
	</analytic>
	<monogr>
		<title level="m">Transactions on Machine Learning Research (TMLR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note>BIG bench authors</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3442188.3445922" />
	</analytic>
	<monogr>
		<title level="m">Proceedings ACM conference on fairness, accountability, and transparency (FAccT)</title>
		<meeting>ACM conference on fairness, accountability, and transparency (FAccT)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Deepseek-Ai Xiao</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanhuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiushi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazuo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaige</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangbo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Hui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panpan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenfeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishi</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangli</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Mei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuecheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing-Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Mei You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xin Yuan Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2401.02954" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.11641" />
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gpt-neox-20b: An open-source autoregressive language model</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pieler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Weinbach</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.bigscience-1.9" />
	</analytic>
	<monogr>
		<title level="m">BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.14165" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Broken neural scaling laws</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=sckjveqlCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reproducible scaling laws for contrastive language-image learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2212.07143" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benton</forename><forename type="middle">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2204.02311" />
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.11416" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1300" />
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.05457" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.14135" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to 22 billion parameters</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">Peter</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/dehghani23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Documenting large webtext corpora: A case study on the colossal clean crawled corpus</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Agnew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.98" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Emma</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Cui</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.06905" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><surname>Kto</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.01306" />
		<title level="m">Model alignment as prospect theoretic optimization</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Datacomp: In search of the next generation of multimodal datasets</title>
		<author>
			<persName><forename type="first">Yitzhak</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Hayase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thao</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruba</forename><surname>Wortsman Ryan Marten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahim</forename><surname>Orgad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyani</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Saukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.14108" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2101.00027" />
		<title level="m">The Pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scaling laws for neural machine translation</title>
		<author>
			<persName><forename type="first">Behrooz</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.07740" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data and parameter scaling laws for neural machine translation</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mitchell A Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><surname>Kaplan</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.478" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.00838" />
		<title level="m">Accelerating the science of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Mamba: Linear-time sequence modeling with selective state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.00752" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=yWd42CWN3c" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.00396" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Textbooks are all you need</title>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adil</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shital</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harkirat</forename><surname>Singh Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">OpenLM: a minimal but performative language modeling (lm) repository</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitzhak</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achal</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://github.com/mlfoundations/open_lm" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.03300" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.14701" />
		<title level="m">Scaling laws for autoregressive generative modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scaling laws for transfer</title>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.01293" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep learning scaling is predictable, empirically</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Frederick Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Mostofa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.00409" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond human-level accuracy: Computational challenges in deep learning</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.01736" />
	</analytic>
	<monogr>
		<title level="m">Principles and Practice of Parallel Programming (PPoPP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.15556" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khashayar</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.01462" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Scaling laws for downstream task performance of large language models. arXiv</title>
		<author>
			<persName><forename type="first">Berivan</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Hazimeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Paparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.04177" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scaling laws under the microscope: Predicting transformer performance from small scale experiments</title>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.findings-emnlp.544" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">El</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><surname>Sayed</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.06825" />
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D19-1259" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2001.08361" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Analyzing the sample complexity of selfsupervised image reconstruction methods</title>
		<author>
			<persName><forename type="first">Tobit</forename><surname>Klug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dogukan</forename><surname>Atik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Heckel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.19079" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.11942" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Caggiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Naren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Tintore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/xformers" />
		<title level="m">xformers: A modular and hackable transformer modelling library</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
		<ptr target="https://aaai.org/papers/59-4492-the-winograd-schema-challenge" />
	</analytic>
	<monogr>
		<title level="m">International conference on the principles of knowledge representation and reasoning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Starcoder: may the source be with you! arXiv preprint</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.06161" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.08124" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.03545" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The data provenance initiative: A large scale audit of dataset licensing &amp; attribution in ai</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naana</forename><surname>Obeng-Marnu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Sileo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Brannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Khazam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jad</forename><surname>Kabbara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Perisetla</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.16787" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.05101" />
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Pykhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Abulkhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Indraneil</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Ding</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Risdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nii</forename><surname>Osae Osae Dade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Krauß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yekun</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhtasham</forename><surname>Oblokulov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armel</forename><surname>Zebaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Scholak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Muñoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.19173" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fingpt: Large generative models for a small language</title>
		<author>
			<persName><forename type="first">Ville</forename><surname>Risto Luukkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jouni</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Eskelinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna-Mari</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Kupari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><surname>Piktus</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.emnlp-main.164" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groenveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanneneh</forename><surname>Hajishirz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><forename type="middle">Dodge</forename><surname>Paloma</surname></persName>
		</author>
		<ptr target="https://paloma.allen.ai" />
		<title level="m">A benchmark for evaluating language model fit</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/J93-2004" />
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Effects of parameter norm growth during transformer training: Inductive bias from gradient descent</title>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.133" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1809.02789" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Llm evaluation scores</title>
		<author>
			<persName><surname>Mosaicml</surname></persName>
		</author>
		<ptr target="https://www.mosaicml.com/llm-evaluation" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Crosslingual generalization through multitask finetuning</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.acl-long.891" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armel</forename><surname>Zebaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swayam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><surname>Octopack</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.07124" />
		<title level="m">Instruction tuning code large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scaling data-constrained language models</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.16264" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.09906" />
		<title level="m">Generative representational instruction tuning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Laban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidiya</forename><surname>Murakhovs'ka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kumar Choubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghana</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2309.03450" />
		<title level="m">Shafiq Rayhan Joty, and Caiming Xiong. Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Triton</surname></persName>
		</author>
		<ptr target="https://github.com/openai/triton" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title/>
		<ptr target="https://arxiv.org/abs/2303.08774" />
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernandez</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1144" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P02-1040" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">BBQ: A hand-built bias benchmark for question answering</title>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelica</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phu</forename><surname>Mon Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.findings-acl.165" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.01703" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">EnterprisePII dataset</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Patronus</surname></persName>
		</author>
		<ptr target="https://tinyurl.com/" />
		<imprint>
			<date type="published" when="2023">2023. 2r5x9bst</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.01116" />
		<title level="m">The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">RWKV: Reinventing RNNs for the transformer era</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Arcadinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Grella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kranthi</forename><surname>Gv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuzheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemyslaw</forename><surname>Kazienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kocon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartłomiej</forename><surname>Koptyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayden</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaju</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ipsit</forename><surname>Mantri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferdinand</forename><surname>Mom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Wind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Woźniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui-Jie</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.findings-emnlp.936" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<ptr target="https://aclanthology.org/E17-2025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the European Chapter</title>
		<meeting>the Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">G</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><forename type="middle">W</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.11446" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.18290" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.10683" />
	</analytic>
	<monogr>
		<title level="m">The Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">CoQA: A conversational question answering challenge</title>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/Q19-1016" />
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Cosmin Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName><surname>Gordon</surname></persName>
		</author>
		<ptr target="https://people.ict.usc.edu/~gordon/copa.html" />
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI) Spring Symposium</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A constructive prediction of the generalization error across scales</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">S</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.12673" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N18-2002" />
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1907.10641" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.01108" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Social IQa: Commonsense reasoning about social interactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D19-1454" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Beyond chinchilla-optimal: Accounting for inference in language model scaling laws</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Sardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2401.00448" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Efficient Natural Language and Speech Processing</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">What language model to train if you have one million gpu hours?</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><surname>Phang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.findings-emnlp" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Are emergent abilities of large language models a mirage?</title>
		<author>
			<persName><forename type="first">Rylan</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brando</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.15004" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A neural scaling law from the dimension of the data manifold</title>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.10802" />
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.05202" />
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Aya dataset: An open-access collection for multilingual instruction tuning</title>
		<author>
			<persName><forename type="first">Shivalika</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freddie</forename><surname>Vargus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dsouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abinaya</forename><surname>Börje F Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Yin</forename><surname>Mahendiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herumb</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Shandilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deividas</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Mataciunas</surname></persName>
		</author>
		<author>
			<persName><surname>Omahony</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.06619</idno>
		<ptr target="https://arxiv.org/abs/2402.06619" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.00159" />
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Beyond neural scaling laws: beating power law scaling via data pruning</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Sorscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=UmvSlP-PyV" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtadha</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.09864" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1421" />
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Scale efficiently: Insights from pre-training and fine-tuning transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=f2OYVDyfIB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Scaling laws vs model architectures: How does inductive bias influence scaling?</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.findings-emnlp.825" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Introducing mpt-7b: A new standard for open-source, commercially usable llms</title>
		<author>
			<persName><forename type="first">Nlp</forename><surname>Mosaicml</surname></persName>
		</author>
		<author>
			<persName><surname>Team</surname></persName>
		</author>
		<ptr target="www.mosaicml.com/blog/mpt-7b" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranesh</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laichee</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renelito Delos</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Lamda</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.08239" />
		<title level="m">Language models for dialog applications</title>
		<editor>
			<persName><forename type="first">Alejandra</forename><surname>Molina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Erin</forename><surname>Hoffman-John</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Josh</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravi</forename><surname>Rajakumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alena</forename><surname>Butryna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Viktoriya</forename><surname>Kuzmina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joe</forename><surname>Fenton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aaron</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rachel</forename><surname>Bernstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Blaise</forename><surname>Aguera-Arcas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marian</forename><surname>Croak</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Redpajama: an open dataset for training large language models</title>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
	</analytic>
	<monogr>
		<title level="m">Together Computer</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2302.13971" />
		<title level="m">LLaMA: Open and Efficient Foundation Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2307.09288" />
	</analytic>
	<monogr>
		<title level="m">Open Foundation and Fine-Tuned Chat Models</title>
		<editor>
			<persName><forename type="first">Eric</forename><surname>Michael</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Smith</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ranjan</forename><surname>Subramanian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Binh</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adina</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jian</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Puxin</forename><surname>Xiang Kuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iliyan</forename><surname>Yan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuchen</forename><surname>Zarov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Angela</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Melanie</forename><surname>Fan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sharan</forename><surname>Kambadur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelien</forename><surname>Narang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Robert</forename><surname>Rodriguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Stojnic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Edunov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Scialom</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Aya model: An instruction finetuned open-access multilingual language model</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viraat</forename><surname>Aryabumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Yin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gbemileke</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Onilude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivalika</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui-Lee</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><surname>Kayid</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.07827" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.03762" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<author>
			<persName><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stéfan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Jarrod Millman</surname></persName>
		</author>
		<author>
			<persName><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">İlhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antônio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
		<ptr target="https://rdcu.be/b08Wh" />
	</analytic>
	<monogr>
		<title level="m">Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">From lsat: The progress and challenges of complex reasoning</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2108.00648" />
	</analytic>
	<monogr>
		<title level="m">Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=gEZrGCozdqR" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Emergent abilities of large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=yzkSU5zdwD" />
	</analytic>
	<monogr>
		<title level="m">Transactions on Machine Learning Research (TMLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.04359" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<author>
			<persName><forename type="first">Bigscience</forename><surname>Workshop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><surname>Yvon</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2211.05100" />
		<title level="m">A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Novak</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2309.14322" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Tensor programs V: Tuning large neural networks via zero-shot hyperparameter transfer</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.03466" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Feature learning in infinite depth neural networks</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soufiane</forename><surname>Hayou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=17pVDnpwwl" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P19-1472" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.04560" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.07467" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Improving deep transformer with depth-scaled initialization and merged attention</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D19-1083" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Pytorch fsdp: Experiences on scaling fully sharded data parallel</title>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Chien Chin Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Less</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Shojanazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Balioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geeta</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.14778/3611540.3611569</idno>
		<ptr target="https://dl.acm.org/doi/10.14778/3611540.3611569" />
	</analytic>
	<monogr>
		<title level="m">Very Large Data Bases Conference (VLDB)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Jec-qa: A legal-domain question answering dataset</title>
		<author>
			<persName><forename type="first">Haoxi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.12011" />
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Agieval: A human-centric benchmark for evaluating foundation models</title>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiduo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Saied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.06364" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Astraios: Parameter-efficient instruction tuning code large language models</title>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armel</forename><surname>Zebaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitchakarn</forename><surname>Suppattarachai ; Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Muennighoff</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2401.00788" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Leandro von Werra, Harm de Vries</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
