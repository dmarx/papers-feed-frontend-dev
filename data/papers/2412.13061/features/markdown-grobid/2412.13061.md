# VidTok VIDTOK A VERSATILE AND OPEN-SOURCE VIDEO TOKENIZER

## Abstract

## 

Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce Vid-Tok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.

† Project lead.

## INTRODUCTION

Visual generation and understanding have emerged as prominent research areas, driven by the capacity of visual data to offer immersive experiences [(Ho et al., 2022b;](#)[Singer et al., 2023;](#b25)[Ho et al., 2022a;](#)[Yu et al., 2023;](#b38)[Kondratyuk et al., 2024;](#b14)[Yang et al., 2024c;](#)[Bai et al., 2024;](#b0)[Zhu et al., 2024)](#b45), convey rich semantic information [(Li et al., 2023;](#b16)[Zhang et al., 2024b;](#)[Liu et al., 2024)](#b17), and function as an interface for models to VidTok interact with the physical world [(Yang et al., 2024b;](#b35)[a;](#)[Zhang et al., 2024a;](#)[Chen et al., 2024b)](#). However, the high degree of redundancy inherent in pixel-level representations [(Sullivan et al., 2012)](#b26) has led to a shift in modern methodologies. These approaches often employ visual tokenization techniques [(Rombach et al., 2022;](#b24)[OpenAI, 2024;](#b20)[Kondratyuk et al., 2024;](#b14)[Wu et al., 2024;](#b32)[Chameleon, 2024)](#b2), transforming raw visual data into compact latent tokens, which serve as a more efficient basis for tasks involving generation and understanding.

The adoption of visual tokenization has catalyzed extensive research on image tokenizers [(Rombach et al., 2022;](#b24)[Zheng et al., 2022;](#b44)[Patil et al., 2024)](#b22), resulting in the development of several open-source tokenizers that serve as widely used tools to advance and streamline image-related research (CompVis; HuggingFace; TencentARC). However, comparable resources and tools remain largely absent in the domain of video. While it is possible to treat each frame of a video as an independent image and compress it using an image tokenizer, this approach overlooks temporal redundancies and consistency, resulting in latent tokens that are temporally redundant and potentially inconsistent across frames.

... ...

## Encoder Regularizer Decoder Input Output

Latent Space Recent efforts have sought to address this gap by introducing video tokenizers that incorporate temporal modeling. However, these approaches often fail to account for diverse use cases and exhibit limitations in performance. For instance, [Yang et al. (2024c)](#) exclusively offers tokenizers with continuous tokens, while [Kondratyuk et al. (2024)](#b14) demonstrates the effectiveness of discrete tokens but remains unavailable as an open-source tool. In this work, we introduce VidTok, a versatile and state-ofthe-art video tokenizer designed to support both continuous and discrete tokenizations effectively. Our approach follows common architecture as illustrated in Fig. [2](#fig_1), and incorporates several key advancements over existing solutions:

• Model architecture. We handle spatial and temporal sampling separately, reducing computational complexity without sacrificing reconstruction quality. Specifically, we employ 2D convolutions in spatial up/downsampling modules and adopt an AlphaBlender operator in temporal up/downsampling modules, while the remaining parts still utilize 3D convolutions to perform information fusion.

• Advanced quantization techniques. To address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ) [(Van Den Oord et al., 2017)](#b28), we propose the use of Finite Scalar Quantization (FSQ) in discrete video tokenization. By optimizing the implicit codebook directly, this approach substantially improves discrete tokenizers.

• Improved training strategies. To improve training efficiency, we employ a two-stage training strategy: initially pre-training the full model on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. Furthermore, we observe that utilizing training data with reduced frame rates effectively improves the model's ability to represent motion dynamics.

Building upon the aforementioned advancements, we train VidTok on a large-scale video dataset and evaluate its performance on widely used benchmarks such as MCL-JCV [(Wang et al., 2016)](#b29) and a web video evaluation set. Experimental results reveal that VidTok outperforms previous models in both discrete and continuous tokenization, achieving superior results across all evaluated metrics, including PSNR, SSIM, LPIPS, and FVD.

## RELATED WORKS

## DISCRETE VIDEO TOKENIZATION

Discrete tokenization maps input images to a latent space and quantizes the latent representations using a codebook of vectors by identifying the nearest codebook vector. Compared to continuous tokens, discrete tokens offer the advantage of mitigating error accumulation during the autoregressive generation process. Building on the foundation of discrete image tokenization [(Van Den Oord et al., 2017)](#b28), discrete video tokenization extends this approach to video data [(Yan et al., 2021;](#b33)[Yu et al., 2024;](#b6)[Wang et al., 2024;](#b30)[NVIDIA)](#b19). It incorporates temporal modeling to effectively manage the temporal redundancies inherent in video sequences.

VideoGPT [(Yan et al., 2021)](#b33) leverages VQ-VAE [(Van Den Oord et al., 2017)](#b28) to learn downsampled discrete latent representations of raw video data through the use of 3D convolutions and axial self-attention. Subsequently, a GPT-like architecture is employed to autoregressively model these discrete latents, utilizing spatio-temporal position encodings. This approach produces video samples that are competitive with state-of-the-art GAN-based models for video generation. [MAGVIT-v2 (Yu et al., 2024)](#) observes that the generation performance initially improves but then deteriorates for larger vocabulary in VQ-VAE, and decreasing the code embedding dimension when increasing the vocabulary size facilitates learning over the distribution of a large vocabulary [(Yu et al., 2022)](#b37). Building on this insight, MAGVIT-v2 reduces the embedding dimension of the VQ-VAE codebook to zero and introduces Lookup-Free Quantization (LFQ), which eliminates the embedding lookup process. This approach improves both reconstruction and generation quality in language models as vocabulary size increases. As a concurrent work, Cosmos-Tokenizer (NVIDIA) utilizes Finite Scalar Quantization (FSQ) [(Mentzer et al., 2024)](#b18) to achieve discrete tokenization, where each dimension is quantized to a small, fixed set of values.

In this work, we integrate several key advancements, including FSQ, to develop a state-of-the-art discrete video tokenizer. The proposed tokenizer is designed to facilitate a wide range of applications in video analysis, generation, and modeling, fostering further innovation in the field.

## CONTINUOUS VIDEO TOKENIZATION

Compared to discrete tokenization, continuous tokenization [(Zhao et al., 2024;](#b43)[hpcaitech;](#)[Chen et al., 2024a;](#)[Yang et al., 2024c](#); NVIDIA) generally offers higher reconstruction fidelity [(Rombach et al., 2022)](#b24). It is typically employed in conjunction with continuous space modeling techniques, such as diffusion models [(Ho et al., 2020)](#b7), to enhance the quality and smoothness of generated outputs. For example, Latent Video Diffusion Models (LVDMs) [(Blattmann et al., 2023;](#b1)[Guo et al., 2024;](#b6)[Yang et al., 2024c;](#)[OpenAI, 2024)](#b20) efficiently and effectively generate video content by compressing visual data into continuous latent representation first and then operating on it with denoising techniques. A notable example of this approach is OpenAI's Sora (OpenAI, 2024), which serves as a representative work in this domain.

CV-VAE [(Zhao et al., 2024)](#b43) introduces a continuous video tokenizer designed to achieve spatio-temporal compression of videos, with a latent space that aligns with the latent space of existing image VAEs [(Rombach et al., 2022)](#b24) through its proposed latent space regularization method. Open-Sora (hpcaitech) and Open-Sora-Plan (PKU-YuanGroup; [Chen et al., 2024a)](#) are two open-source projects aimed at reproducing OpenAI's Sora. Both projects offer continuous video tokenizers that effectively perform spatial and temporal compression.

CogVideoX [(Yang et al., 2024c)](#) introduces a continuous tokenizer that preserves a greater amount of information by maintaining a larger number of latent channels, resulting in enhanced reconstruction fidelity. More recently, Cosmos-Tokenizer (NVIDIA) also provides continuous video tokenizers with various compression ratios.

The proposed VidTok builds upon the publicly available models mentioned above by incorporating several key advancements, with the goal of establishing a foundational tokenizer for video-related research.

## VIDTOK

In this section, we first introduce the general structure of the video tokenizer with detailed notations. From Sec. 3.2 to Sec. 3.4, we introduce the improved model architecture, the advanced quantization technique, and the improved training strategy respectively.

## OVERVIEW OF VIDEO TOKENIZER

To enhance efficiency, existing approaches for video generation and understanding often utilize video tokenizers (e.g., 3D VAEs (Kingma & Welling, 2014)) to convert raw visual data into compact latent tokens. As illustrated in Fig. [2](#fig_1), these methods typically involve an encoder that compresses video data into compact latent tokens across both spatial and temporal dimensions, followed by a decoder that reconstructs the tokens back into pixel space. Depending on the scenario, latent tokens can be either continuous [(Zhao et al., 2024;](#b43)[Yang et al., 2024c;](#)[OpenAI, 2024)](#b20) or discrete [(Yu et al., 2024;](#b6)[Wang et al., 2024;](#b30)[NVIDIA)](#b19), and the model architecture may be designed to operate in a causal [(Yu et al., 2024)](#b6) or non-causal [(Blattmann et al., 2023)](#b1) manner. To enhance the model's capacity for generating novel data samples and to mitigate overfitting to the training dataset, it is essential to apply appropriate regularization within the latent space (Kingma & Welling, 2014; Van Den Oord et al., 2017). VidTok 3D InputBlock 2D+1D DownBlock 2D+1D DownBlock 2D+1D DownBlock AlphaBlender Temporal DownBlock AlphaBlender Temporal DownBlock 3D MidBlock 3D MidBlock 3D OutConv 3D InConv 2D+1D UpBlock 2D+1D UpBlock 2D+1D UpBlock 2D+1D UpBlock 3D OutputBlock AlphaBlender Temporal UpBlock AlphaBlender Temporal UpBlock Regularizer 2D+1D DownBlock AlphaBlender Temporal DownBlock AlphaBlender Temporal UpBlock Interpolate [scale=(2,](#tab_2)[1,](#tab_1)[1)   AvgPool3d  stride=(2,](#tab_3)[1,](#tab_1)[1)   Conv3d  stride=(2,](#tab_3)[1,](#tab_1)[1](#tab_1) Let E and D denote the encoder and the decoder of the video tokenizer respectively, R denote the regularizer applied in the latent space, r t and r s denote the temporal and spatial compression ratios respectively. A video containing N frames is denoted as

$X = {x 1 , x 2 , ..., x N } ∈ R N ×3×H×W , where N = r t * n, H = r s * h and W = r s * w.$The workflow can be formulated as:

$Z = R(E(X)), X = D(Z)(1)$where Z ∈ R n×c×h×w denotes the compressed latent representation and X ∈ R N ×3×H×W denotes the reconstructed video.

In causal scenarios, the first frame is typically treated as an independent image for compression, enabling the visual tokenizer to function as both an image and video tokenizer [(Yu et al., 2024)](#b6). At this point, a video X ∈ R (N +1)×3×H×W , which contains N + 1 frames, is compressed into Z ∈ R (n+1)×c×h×w . Specifically, the first frame is compressed solely in the spatial dimension, while the subsequent frames undergo compression in both temporal and spatial dimensions.

## MODEL ARCHITECTURE

In the existing literature, it is widely acknowledged that fully 3D architectures offer superior reconstruction quality, albeit at a high computational cost [(Chen et al., 2024a)](#). However, in this work, we demonstrate that substituting a portion of these 3D convolutions with a combination of 2D and 1D convolutions-effectively decoupling spatial and temporal sampling-can achieve comparable reconstruction quality while significantly reducing computational demands.

The detailed network architecture is illustrated in Fig. [3](#fig_2). As shown, 2D convolutions are employed for spatial upsampling and downsampling modules, while an AlphaBlender operator is utilized in the temporal upsampling and downsampling modules. The remaining components, including the input/output layers and bottleneck layers, leverage 3D convolutions to facilitate information fusion. The specific structures of the temporal upsampling and downsampling modules are depicted on the right side of Fig. [3](#fig_2). Additionally, layer normalization [(Lei Ba et al., 2016)](#b15) is incorporated throughout the architecture to enhance stability and performance. Experimental results, as summarized in Tab. 2, validate the effectiveness of the proposed architectural design.

AlphaBlender operator. Given a parameter α within the range [0, 1], the AlphaBlender operator performs the following operation to input x 1 and input x 2 :

$x = α * x 1 + (1 -α) * x 2 (2)$where x is the result after blending, and α can be either learnable or a given hyperparameter (PKU-YuanGroup).

In this work, we adopt a pre-defined α = Sigmoid(0.2). In causal cases, all 3D and 1D convolutions are configured to operate causally, ensuring that each frame has access only to historical information from preceding frames. For a given video X ∈ R (N +1)×3×H×W , the first frame is duplicated r t -1 times and inserted before the original first frame. After completing the full workflow, the process yields (n + 1) * r t frames. By discarding the first r t -1 frames, the reconstructed video X ∈ R (n * rt+1)×3×H×W is obtained.

$z 0 z 2 z 1 -1 1 -2 codebook FSQ VQ$
## FINITE SCALAR QUANTIZATION

Variational AutoEncoders (VAEs) (Kingma & Welling, 2014) are a class of generative models that map each data point, such as an image, from a complex dataset into a continuous distribution within a latent space, rather than assigning it to a single deterministic point. Conversely, the decoder performs the inverse operation, mapping representations from the latent space back to the original input space. However, due to the increasing demand for discrete latent variables, Vector Quantised-Variational AutoEncoder (VQ-VAE) [(Van Den Oord et al., 2017)](#b28) were introduced. Unlike standard VAEs, VQ-VAEs map inputs to a finite set of vectors (i.e., codebook), through a process known as vector quantization. This approach represents each input by the closest vector in the codebook, which is learned during training. By combining the generative capabilities of VAEs with the advantages of discrete representations, VQ-VAEs provide a robust framework for various machine learning applications, including data compression, representation learning, and generative modeling.

In this work, we employ Finite Scalar Quantization (FSQ) [(Mentzer et al., 2024)](#b18) to generate discrete tokens. The central principle of FSQ is that each scalar entry in the latent representation is independently quantized to the nearest pre-defined scalar value through rounding. In contrast to Vector Quantization (VQ), FSQ eliminates the need for codebook learning, thereby improving training stability [(Mentzer et al., 2024;](#b18)[Yu et al., 2024)](#b6). The approach can be described as follows: Given a vector z = (z 1 , z 2 , ..., z d ) with d channels, each channel z i is mapped to a value in a finite set of L pre-defined values, resulting in a quantized representation ẑ, which is one of L d possible vectors. An example is shown in Fig. [4](#fig_3), where d=3 and L=5, representing an implicit codebook with size L d = 125. Notably, when L is set to 2, each z i can take one of two possible values, yielding binary latents. This mechanism corresponds to the Lookup-Free Quantization (LFQ) method proposed in MAGVIT-v2 [(Yu et al., 2024)](#b6).

The experiments in Sec. 4.3.2 show that FSQ has significant advantages in codebook utilization, reconstruction quality and training stability, functioning as an advanced quantization technique that effectively improves discrete tokenizers.

## IMPROVED TRAINING STRATEGIES

Training video tokenizers is often computationally intensive, requiring substantial resources (e.g., 3, 072 GPU hours for 256 × 256 resolution videos). This necessitates the development of efficient strategies to reduce computational costs while maintaining model performance. In this work, we implement a two-stage training approach to address this challenge: the full model is initially pre-trained on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. Specifically, the model is first trained from scratch using videos at 128 × 128 resolution. In the second stage, the decoder is fine-tuned using videos at 256 × 256 resolution.

The experimental results presented in Tab. 4 demonstrate that the proposed two-stage training strategy achieves performance comparable to training the model from scratch on 256 × 256 resolution videos, while substantially VidTok reducing computational costs-cutting training time by half, from 3, 072 GPU hours to 1, 536 GPU hours. Furthermore, since the encoder remains unchanged, the fine-tuned model retains compatibility with the latent space of the pre-fine-tuned model. This ensures that the model can adapt efficiently to novel domains without impacting the integrity of models trained on the same latent space.

Moreover, as the video tokenizer is designed to model the motion dynamics of input videos, it is essential to efficiently represent these dynamics within the model. In this study, we empirically observe that training with data at reduced frame rates significantly enhances the model's capability to capture and represent motion dynamics. This finding is substantiated through the experimental results presented in Tab. 4 and Fig. [6](#fig_5), which illustrate the improved reconstruction quality achieved with lower frame rate training data.

## EXPERIMENTS

This section verifies the proposed VidTok through comparative experiments with existing state-of-the-art video tokenizers [(Yu et al., 2024;](#b6)[Wang et al., 2024;](#b30)[NVIDIA;](#b19)[Zhao et al., 2024;](#b43)[hpcaitech;](#)[PKU-YuanGroup;](#b23)[Yang et al., 2024c)](#) and comprehensive ablation studies. Fig. [1](#fig_0) provides several radar charts for a quick comparison.

## EXPERIMENTAL SETTING

Dataset and metrics. For training, we utilize a self-collected video dataset, divided into two subsets based on video quality: (1) Training Set 1, comprising approximately 10 million low-resolution videos (e.g., 480p); and (2) Training Set 2, consisting of approximately 6 million high-resolution videos (e.g., 1080p). All videos in the dataset are natural videos characterized by diverse lighting conditions, motion patterns, and scenarios. For evaluation, we follow the protocol of MAGVIT-v2 [(Yu et al., 2024)](#b6) and use two benchmark datasets: the MCL-JCV dataset [(Wang et al., 2016)](#b29) and the validation set of a web video dataset. Evaluation videos are resized to 256 × 256 with a frame rate of 30 FPS.

The video reconstruction performance of the models is assessed using four widely-used metrics: Peak Signalto-Noise Ratio (PSNR) [(Hore & Ziou, 2010)](#b10), Structural Similarity Index Measure (SSIM) [(Wang et al., 2004)](#b31), Learned Perceptual Image Patch Similarity (LPIPS) [(Zhang et al., 2018)](#b40) and Fréchet Video Distance (FVD) [(Unterthiner et al., 2018)](#b27).

Implementation details. We implement video tokenizers with various settings, including both causal and non-causal cases, continuous and discrete latents, and different video compression ratios. All models are trained with four loss terms: a reconstruction term, a perceptual term, an adversarial term and a regularization term. The first three terms follow the practice in Latent Diffusion Models [(Rombach et al., 2022)](#b24). For the regularization term, we use KL loss [(Kingma & Welling, 2014)](#b13) in continuous tokenizers, and entropy penalty and commitment losses in discrete tokenizers [(Yu et al., 2024)](#b6).

In the first training stage, Training Set 1 is resized to a resolution of 128 × 128 and used for initial model training. We train VidTok for 50, 000 steps with batch size 16. In the second stage, Training Set 2 is resized to 256 × 256 and employed for fine-tuning. We fine-tune the decoder for another 30, 000 steps with batch size 8. The frame rate of the training data is maintained at 3 frames per second (FPS) during both stages. We use Adam optimizer [(Kingma & Ba, 2015)](#b12) with a constant learning rate of 1 × 10 -5 . The training is conducted on 8 NVIDIA 40G A100 GPUs with PyTorch [(Paszke et al., 2019)](#b21).

Baselines. We compare our method with the following state-of-the-art solutions: (1) MAGVIT-v2 [(Yu et al., 2024)](#b6): a discrete video tokenizer which maps videos to a discrete latent space using the LFQ representation;

(2) OmniTokenizer [(Wang et al., 2024)](#b30): a discrete video tokenizer using VQ as the discrete representation;

(3) CV-VAE [(Zhao et al., 2024)](#b43): a continuous video tokenizer with a latent space that aligns with the latent space of existing image VAEs; (4) : an open-source project aimed at reproducing OpenAI's Sora which offers a continuous video tokenizer; (5) Open-Sora-Plan-v1.2 (PKU-YuanGroup): another open-source project aimed at reproducing OpenAI's Sora; (6) CogVideoX [(Yang et al., 2024c)](#): a continuous tokenizer that preserves a greater amount of information by maintaining a larger number of latent channels; (7) Cosmos-Tokenizer (NVIDIA): a suite of continuous and discrete video tokenizers with various compression ratios. We conduct thorough experiments in Sec. 4.2, with aligned settings for all methods to guarantee fairness in comparison. 

## COMPARISON WITH BASELINES

To evaluate the advancements achieved by VidTok, we compare its performance against state-of-the-art models across various scenarios, encompassing both discrete and continuous tokenization approaches. The comprehensive comparison results are presented in Tab. 1. All performance metrics reported in the table, except for those of MAGVIT-v2 [(Yu et al., 2024)](#b6), are obtained through our own experiments conducted under an identical evaluation protocol to ensure consistency and fairness. For MAGVIT-v2, as the model is not publicly accessible, we reference the results reported in their original publication. It is important to note that these results were obtained on a resolution of 17 × 360 × 640, differing from the 17 × 256 × 256 resolution used for the other models in our comparison.

Compared to existing discrete tokenziers [(Yu et al., 2024;](#b6)[Wang et al., 2024;](#b30)[NVIDIA)](#b19), VidTok demonstrates significantly superior reconstruction performance, even when utilizing a smaller codebook size (e.g., 32, 768). This highlights the effectiveness of our approach in discrete tokenization. In the context of continuous tokenization, VidTok achieves comprehensive improvements across all evaluation metrics, regardless of whether the latent representation comprises 4 or 16 channels. Notably, these advancements are achieved even with a smaller model size, surpassing the performance of state-of-the-art methods [(Zhao et al., 2024;](#b43)[hpcaitech;](#)[PKU-YuanGroup;](#b23)[Yang et al., 2024c;](#)[NVIDIA)](#b19). These results underscore the effectiveness of VidTok in both discrete and continuous tokenization tasks.

We present the corresponding visual reconstruction results in Fig. [5](#fig_4) for qualitative comparison. From these visual results, our method exhibits a distinct advantage in detail reconstruction fidelity and subjective viewing experience.

## ABLATION EXPERIMENTS

We conduct comprehensive ablation experiments to validate the superiority of the proposed model architecture, the advanced quantization technique and the improved training strategies. All ablation experiments are conducted with a video compression ratio of 4 × 8 × 8 and an input size of 17 × 256 × 256, evaluated on MCL-JCV [(Wang et al., 2016)](#b29).

## ABLATION ON THE MODEL ARCHITECTURE

To evaluate the effectiveness of our proposed model architecture, we compare it with three alternative variants in terms of computational complexity and reconstruction quality. (1) Variant 1 employs a fully 3D architecture, integrating spatial and temporal sampling using 3D convolutions. (2) Variant 2 separates spatial and temporal  sampling, but does not incorporate the AlphaBlender operator for temporal sampling. (3) Variant 3 replaces all 3D convolutions with 2D convolutions.

The experimental results, summarized in Tab. 2, provide insights into the trade-offs between model performance and computational efficiency. The results indicate that employing a fully 3D architecture (Variant 1) results in high computational complexity and model size. By modifying the architecture to replace 3D convolutions in the spatio-temporal sampling modules with a combination of 2D and 1D convolutions (Variant 2), we achieve a significant reduction in computational load without notable degradation in reconstruction quality. Building upon Variant 2, the introduction of the AlphaBlender operator for temporal sampling yields substantial improvements across most metrics, albeit with a slight increase in computational cost. Furthermore, replacing all 3D convolutions with 2D convolutions (Variant 3) leads to a marked decline in reconstruction performance, underscoring the importance of retaining 3D convolutions for effective spatio-temporal representation. Overall, the findings in Tab. 2 highlight the efficacy of the proposed architecture, which strikes a balance between computational efficiency and reconstruction performance.

## ABLATION ON THE DISCRETE TECHNIQUES

In Tab. 3, we present a comparison of various quantization methods, including VQ [(Van Den Oord et al., 2017)](#b28), LFQ [(Yu et al., 2024)](#b6), and FSQ [(Mentzer et al., 2024)](#b18). Additionally, we analyze the impact of the regularization loss term on the performance of discrete tokenizers.

The results highlight several key observations. Traditional VQ suffers from common challenges, such as training instability and codebook collapse, which lead to extremely low codebook utilization and suboptimal reconstruction quality. In contrast, LFQ and FSQ achieve nearly 100% codebook utilization by directly optimizing an implicit codebook, resulting in significantly enhanced tokenizer performance. Furthermore, FSQ outperforms LFQ's binary quantization by achieving better reconstruction fidelity, suggesting reduced information loss during the quantization process.

The effects of regularization loss vary across quantization methods. For conventional VQ, the absence of regularization loss leads to model collapse and convergence failure. In the case of LFQ, while the model remains capable of convergence without regularization, it experiences a marked decline in codebook utilization and reconstruction performance. FSQ, on the other hand, demonstrates superior training stability, with its performance remaining largely unaffected even in the absence of the regularization loss term.  In summary, FSQ emerges as a highly effective quantization technique, offering significant advantages in codebook utilization, reconstruction quality, and training stability. These attributes position FSQ as an advanced method for enhancing the performance of discrete tokenizers.

## ABLATION ON THE TRAINING STRATEGIES

As detailed in Sec. 3.4, we employ a two-stage training strategy: pre-training the full model on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. To evaluate the efficiency and effectiveness of this approach, we conduct an ablation study, with results summarized in Tab. 4.

In the first row, training the full model on high-resolution videos directly from scratch requires 3,072 GPU hours.

In contrast, the results in the fourth row demonstrate that the proposed two-stage training strategy-starting with low-resolution data and then fine-tuning on high-resolution data-reduces training time by half (from 3,072 to 1,536 GPU hours) while achieving comparable reconstruction quality. A comparison between the third and fourth rows reveals that fine-tuning only the decoder during the second stage produces similar performance to fine-tuning the entire model, with a lower computational cost. This approach also ensures that the low-resolution and high-resolution models share a unified latent space due to the fixed encoder, enabling latent models trained in this shared space to be reused across resolutions and domains.

Additionally, the last row examines the impact of varying the sampling rate during training. Qualitative results, presented in Fig. [6](#fig_5), indicate that using training data with reduced frame rates enhances the model's ability to represent motion dynamics effectively.

## MODEL SUMMARY

We provide a comprehensive summary of model performance in Tab. 5, covering both continuous and discrete tokenization, various video compression ratios, and causal versus non-causal scenarios. From Tab. 5, it is evident that as the video compression ratio increases, reconstruction performance deteriorates. Non-causal models generally outperform causal ones, as they are able to capture more extensive temporal information, which aids in the high-fidelity reconstruction of fine details. In the continuous case, increasing the number of channels in the latent representation allows for the retention of more information, leading to better reconstruction performance. Similarly, in the discrete case, a larger codebook size usually means smaller quantization errors, preserving more accurate information and thus achieving better reconstruction fidelity.

A comparison between the continuous and discrete cases reveals that FSQ with a codebook size of 262, 144 achieves reconstruction performance comparable to 'KL -4chn'.

Additionally, we compare the performance across different settings: 1) KL -16chn, with a video compression ratio of 4 × 8 × 8; 2) KL -8chn, with a video compression ratio is 2 × 8 × 8; 3) KL -4chn, with a video compression ratio is 4 × 4 × 4. Our analysis indicates that when the latent space contains the same amount of data, allocating it to the channel dimension tends to result in relatively better reconstruction performance.

All models and source code associated with this work are publicly available at [https://github.com/ microsoft/VidTok](https://github.com/microsoft/VidTok). We aspire for this contribution to serve as a foundation for and inspire further advancements in this research domain.

## CONCLUSION

In this paper, we present VidTok, a versatile and open-source video tokenizer that achieves state-of-the-art performance in both continuous and discrete tokenization. By converting raw visual data into compact latent tokens, VidTok provides an efficient foundation for tasks related to visual generation and understanding. Through the incorporation of advancements in model architecture, discrete representation, and training strategies, VidTok surpasses existing methods, demonstrating notable improvements across several performance metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation protocols. Additionally, we conduct extensive ablation experiments to thoroughly investigate the performance characteristics of the video tokenizer. We hope this work will inspire future research in this area.

![Figure 1: Illustration of the quantitative comparison of discrete and continuous tokenization performance across our VidTok model and state-of-the-art methods, evaluated using four metrics: PSNR, SSIM, LPIPS, and FVD. All performance metrics are obtained through experiments conducted under a consistent evaluation protocol to ensure fairness and comparability. Larger chart areas correspond to better performance across all metrics.]()

![Figure 2: An overview of video tokenizers.]()

![Figure 3: The improved model architecture. In the context of a causal setting, consider an input with dimensions T × H × W = 17 × 256 × 256. Assuming a temporal compression factor of 4 and a spatial compression factor of 8, the intermediate latent representation is reduced to dimensions T × H × W = 5 × 32 × 32.]()

![Figure 4: Left: Vector Quantization (VQ) employed in Vector Quantised-Variational AutoEncoder (VQ-VAE) (Van Den Oord et al., 2017). Right: Finite Scalar Quantization (FSQ) (Mentzer et al., 2024) utilized in our model.]()

![Figure 5: Qualitative comparison with the state-of-the-art video tokenizers.]()

![Figure 6: The influence of different sample rates on model performance during training. The second row presents the test results obtained using training data with a sample rate of 8 FPS, while the third row shows the test results using training data with a sample rate of 3 FPS. The results demonstrate that employing training data with reduced frame rates enhances the model's capacity to effectively capture motion dynamics.]()

![Quantitative comparison with the state-of-the-art video tokenizers. All evaluated models are causal and have a video compression ratio of 4 × 8 × 8. The input resolution for most models is 17 × 256 × 256, except for MAGVIT-v2 * , which is evaluated on 17 × 360 × 640 as reported in the original study. The sample rate of testing data is 30 FPS. We highlight the best and the second-best numbers in bold and underline respectively.]()

![Ablation study on the model architecture. Variant 1: fully 3D architecture. Variant 2: w/o AlphaBlender. Variant 3: w/o 3D architecture. We use 'KL -4chn' as regularizer for all settings.]()

![Analysis of the impact of discrete techniques on model performance. R.L. denotes Regularization Loss, while U.R. represents Utilization Rate.]()

![Ablation study on the proposed training strategy. To ensure a fair comparison, both stages use training data from Training Set 1. Across all configurations, the regularizer 'KL -4chn' is employed. The training computational cost, measured in GPU hours, is evaluated using NVIDIA A100 GPUs. Sample Rate First Stage Second Stage Fix Enc. PSNR↑ SSIM↑ LPIPS↓ FVD↓ GPU Hours]()

![Model summary. We offer a suite of models with diverse configurations, encompassing both continuous and discrete tokenization, various video compression ratios (VCR), and options for causal and non-causal scenarios. These configurations are designed to address the distinct requirements of various downstream tasks.]()

