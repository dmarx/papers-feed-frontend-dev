<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VidTok VIDTOK A VERSATILE AND OPEN-SOURCE VIDEO TOKENIZER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-17">17 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anni</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junliang</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinle</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">VidTok VIDTOK A VERSATILE AND OPEN-SOURCE VIDEO TOKENIZER</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-17">17 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">367E73C9147C86B2D4EC268770BA7F44</idno>
					<idno type="arXiv">arXiv:2412.13061v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce Vid-Tok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.</p><p>† Project lead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual generation and understanding have emerged as prominent research areas, driven by the capacity of visual data to offer immersive experiences <ref type="bibr">(Ho et al., 2022b;</ref><ref type="bibr" target="#b25">Singer et al., 2023;</ref><ref type="bibr">Ho et al., 2022a;</ref><ref type="bibr" target="#b38">Yu et al., 2023;</ref><ref type="bibr" target="#b14">Kondratyuk et al., 2024;</ref><ref type="bibr">Yang et al., 2024c;</ref><ref type="bibr" target="#b0">Bai et al., 2024;</ref><ref type="bibr" target="#b45">Zhu et al., 2024)</ref>, convey rich semantic information <ref type="bibr" target="#b16">(Li et al., 2023;</ref><ref type="bibr">Zhang et al., 2024b;</ref><ref type="bibr" target="#b17">Liu et al., 2024)</ref>, and function as an interface for models to VidTok interact with the physical world <ref type="bibr" target="#b35">(Yang et al., 2024b;</ref><ref type="bibr">a;</ref><ref type="bibr">Zhang et al., 2024a;</ref><ref type="bibr">Chen et al., 2024b)</ref>. However, the high degree of redundancy inherent in pixel-level representations <ref type="bibr" target="#b26">(Sullivan et al., 2012)</ref> has led to a shift in modern methodologies. These approaches often employ visual tokenization techniques <ref type="bibr" target="#b24">(Rombach et al., 2022;</ref><ref type="bibr" target="#b20">OpenAI, 2024;</ref><ref type="bibr" target="#b14">Kondratyuk et al., 2024;</ref><ref type="bibr" target="#b32">Wu et al., 2024;</ref><ref type="bibr" target="#b2">Chameleon, 2024)</ref>, transforming raw visual data into compact latent tokens, which serve as a more efficient basis for tasks involving generation and understanding.</p><p>The adoption of visual tokenization has catalyzed extensive research on image tokenizers <ref type="bibr" target="#b24">(Rombach et al., 2022;</ref><ref type="bibr" target="#b44">Zheng et al., 2022;</ref><ref type="bibr" target="#b22">Patil et al., 2024)</ref>, resulting in the development of several open-source tokenizers that serve as widely used tools to advance and streamline image-related research (CompVis; HuggingFace; TencentARC). However, comparable resources and tools remain largely absent in the domain of video. While it is possible to treat each frame of a video as an independent image and compress it using an image tokenizer, this approach overlooks temporal redundancies and consistency, resulting in latent tokens that are temporally redundant and potentially inconsistent across frames.</p><p>... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Regularizer Decoder Input Output</head><p>Latent Space Recent efforts have sought to address this gap by introducing video tokenizers that incorporate temporal modeling. However, these approaches often fail to account for diverse use cases and exhibit limitations in performance. For instance, <ref type="bibr">Yang et al. (2024c)</ref> exclusively offers tokenizers with continuous tokens, while <ref type="bibr" target="#b14">Kondratyuk et al. (2024)</ref> demonstrates the effectiveness of discrete tokens but remains unavailable as an open-source tool. In this work, we introduce VidTok, a versatile and state-ofthe-art video tokenizer designed to support both continuous and discrete tokenizations effectively. Our approach follows common architecture as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, and incorporates several key advancements over existing solutions:</p><p>• Model architecture. We handle spatial and temporal sampling separately, reducing computational complexity without sacrificing reconstruction quality. Specifically, we employ 2D convolutions in spatial up/downsampling modules and adopt an AlphaBlender operator in temporal up/downsampling modules, while the remaining parts still utilize 3D convolutions to perform information fusion.</p><p>• Advanced quantization techniques. To address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ) <ref type="bibr" target="#b28">(Van Den Oord et al., 2017)</ref>, we propose the use of Finite Scalar Quantization (FSQ) in discrete video tokenization. By optimizing the implicit codebook directly, this approach substantially improves discrete tokenizers.</p><p>• Improved training strategies. To improve training efficiency, we employ a two-stage training strategy: initially pre-training the full model on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. Furthermore, we observe that utilizing training data with reduced frame rates effectively improves the model's ability to represent motion dynamics.</p><p>Building upon the aforementioned advancements, we train VidTok on a large-scale video dataset and evaluate its performance on widely used benchmarks such as MCL-JCV <ref type="bibr" target="#b29">(Wang et al., 2016)</ref> and a web video evaluation set. Experimental results reveal that VidTok outperforms previous models in both discrete and continuous tokenization, achieving superior results across all evaluated metrics, including PSNR, SSIM, LPIPS, and FVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DISCRETE VIDEO TOKENIZATION</head><p>Discrete tokenization maps input images to a latent space and quantizes the latent representations using a codebook of vectors by identifying the nearest codebook vector. Compared to continuous tokens, discrete tokens offer the advantage of mitigating error accumulation during the autoregressive generation process. Building on the foundation of discrete image tokenization <ref type="bibr" target="#b28">(Van Den Oord et al., 2017)</ref>, discrete video tokenization extends this approach to video data <ref type="bibr" target="#b33">(Yan et al., 2021;</ref><ref type="bibr" target="#b6">Yu et al., 2024;</ref><ref type="bibr" target="#b30">Wang et al., 2024;</ref><ref type="bibr" target="#b19">NVIDIA)</ref>. It incorporates temporal modeling to effectively manage the temporal redundancies inherent in video sequences.</p><p>VideoGPT <ref type="bibr" target="#b33">(Yan et al., 2021)</ref> leverages VQ-VAE <ref type="bibr" target="#b28">(Van Den Oord et al., 2017)</ref> to learn downsampled discrete latent representations of raw video data through the use of 3D convolutions and axial self-attention. Subsequently, a GPT-like architecture is employed to autoregressively model these discrete latents, utilizing spatio-temporal position encodings. This approach produces video samples that are competitive with state-of-the-art GAN-based models for video generation. <ref type="bibr">MAGVIT-v2 (Yu et al., 2024)</ref> observes that the generation performance initially improves but then deteriorates for larger vocabulary in VQ-VAE, and decreasing the code embedding dimension when increasing the vocabulary size facilitates learning over the distribution of a large vocabulary <ref type="bibr" target="#b37">(Yu et al., 2022)</ref>. Building on this insight, MAGVIT-v2 reduces the embedding dimension of the VQ-VAE codebook to zero and introduces Lookup-Free Quantization (LFQ), which eliminates the embedding lookup process. This approach improves both reconstruction and generation quality in language models as vocabulary size increases. As a concurrent work, Cosmos-Tokenizer (NVIDIA) utilizes Finite Scalar Quantization (FSQ) <ref type="bibr" target="#b18">(Mentzer et al., 2024)</ref> to achieve discrete tokenization, where each dimension is quantized to a small, fixed set of values.</p><p>In this work, we integrate several key advancements, including FSQ, to develop a state-of-the-art discrete video tokenizer. The proposed tokenizer is designed to facilitate a wide range of applications in video analysis, generation, and modeling, fostering further innovation in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONTINUOUS VIDEO TOKENIZATION</head><p>Compared to discrete tokenization, continuous tokenization <ref type="bibr" target="#b43">(Zhao et al., 2024;</ref><ref type="bibr">hpcaitech;</ref><ref type="bibr">Chen et al., 2024a;</ref><ref type="bibr">Yang et al., 2024c</ref>; NVIDIA) generally offers higher reconstruction fidelity <ref type="bibr" target="#b24">(Rombach et al., 2022)</ref>. It is typically employed in conjunction with continuous space modeling techniques, such as diffusion models <ref type="bibr" target="#b7">(Ho et al., 2020)</ref>, to enhance the quality and smoothness of generated outputs. For example, Latent Video Diffusion Models (LVDMs) <ref type="bibr" target="#b1">(Blattmann et al., 2023;</ref><ref type="bibr" target="#b6">Guo et al., 2024;</ref><ref type="bibr">Yang et al., 2024c;</ref><ref type="bibr" target="#b20">OpenAI, 2024)</ref> efficiently and effectively generate video content by compressing visual data into continuous latent representation first and then operating on it with denoising techniques. A notable example of this approach is OpenAI's Sora (OpenAI, 2024), which serves as a representative work in this domain.</p><p>CV-VAE <ref type="bibr" target="#b43">(Zhao et al., 2024)</ref> introduces a continuous video tokenizer designed to achieve spatio-temporal compression of videos, with a latent space that aligns with the latent space of existing image VAEs <ref type="bibr" target="#b24">(Rombach et al., 2022)</ref> through its proposed latent space regularization method. Open-Sora (hpcaitech) and Open-Sora-Plan (PKU-YuanGroup; <ref type="bibr">Chen et al., 2024a)</ref> are two open-source projects aimed at reproducing OpenAI's Sora. Both projects offer continuous video tokenizers that effectively perform spatial and temporal compression.</p><p>CogVideoX <ref type="bibr">(Yang et al., 2024c)</ref> introduces a continuous tokenizer that preserves a greater amount of information by maintaining a larger number of latent channels, resulting in enhanced reconstruction fidelity. More recently, Cosmos-Tokenizer (NVIDIA) also provides continuous video tokenizers with various compression ratios.</p><p>The proposed VidTok builds upon the publicly available models mentioned above by incorporating several key advancements, with the goal of establishing a foundational tokenizer for video-related research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDTOK</head><p>In this section, we first introduce the general structure of the video tokenizer with detailed notations. From Sec. 3.2 to Sec. 3.4, we introduce the improved model architecture, the advanced quantization technique, and the improved training strategy respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OVERVIEW OF VIDEO TOKENIZER</head><p>To enhance efficiency, existing approaches for video generation and understanding often utilize video tokenizers (e.g., 3D VAEs (Kingma &amp; Welling, 2014)) to convert raw visual data into compact latent tokens. As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, these methods typically involve an encoder that compresses video data into compact latent tokens across both spatial and temporal dimensions, followed by a decoder that reconstructs the tokens back into pixel space. Depending on the scenario, latent tokens can be either continuous <ref type="bibr" target="#b43">(Zhao et al., 2024;</ref><ref type="bibr">Yang et al., 2024c;</ref><ref type="bibr" target="#b20">OpenAI, 2024)</ref> or discrete <ref type="bibr" target="#b6">(Yu et al., 2024;</ref><ref type="bibr" target="#b30">Wang et al., 2024;</ref><ref type="bibr" target="#b19">NVIDIA)</ref>, and the model architecture may be designed to operate in a causal <ref type="bibr" target="#b6">(Yu et al., 2024)</ref> or non-causal <ref type="bibr" target="#b1">(Blattmann et al., 2023)</ref> manner. To enhance the model's capacity for generating novel data samples and to mitigate overfitting to the training dataset, it is essential to apply appropriate regularization within the latent space (Kingma &amp; Welling, 2014; Van Den Oord et al., 2017). VidTok 3D InputBlock 2D+1D DownBlock 2D+1D DownBlock 2D+1D DownBlock AlphaBlender Temporal DownBlock AlphaBlender Temporal DownBlock 3D MidBlock 3D MidBlock 3D OutConv 3D InConv 2D+1D UpBlock 2D+1D UpBlock 2D+1D UpBlock 2D+1D UpBlock 3D OutputBlock AlphaBlender Temporal UpBlock AlphaBlender Temporal UpBlock Regularizer 2D+1D DownBlock AlphaBlender Temporal DownBlock AlphaBlender Temporal UpBlock Interpolate <ref type="table" target="#tab_2">scale=(2,</ref><ref type="table" target="#tab_1">1,</ref><ref type="table" target="#tab_3">1)   AvgPool3d  stride=(2,</ref><ref type="table" target="#tab_1">1,</ref><ref type="table" target="#tab_3">1)   Conv3d  stride=(2,</ref><ref type="table" target="#tab_1">1,</ref><ref type="table" target="#tab_1">1</ref> Let E and D denote the encoder and the decoder of the video tokenizer respectively, R denote the regularizer applied in the latent space, r t and r s denote the temporal and spatial compression ratios respectively. A video containing N frames is denoted as</p><formula xml:id="formula_0">X = {x 1 , x 2 , ..., x N } ∈ R N ×3×H×W , where N = r t * n, H = r s * h and W = r s * w.</formula><p>The workflow can be formulated as:</p><formula xml:id="formula_1">Z = R(E(X)), X = D(Z)<label>(1)</label></formula><p>where Z ∈ R n×c×h×w denotes the compressed latent representation and X ∈ R N ×3×H×W denotes the reconstructed video.</p><p>In causal scenarios, the first frame is typically treated as an independent image for compression, enabling the visual tokenizer to function as both an image and video tokenizer <ref type="bibr" target="#b6">(Yu et al., 2024)</ref>. At this point, a video X ∈ R (N +1)×3×H×W , which contains N + 1 frames, is compressed into Z ∈ R (n+1)×c×h×w . Specifically, the first frame is compressed solely in the spatial dimension, while the subsequent frames undergo compression in both temporal and spatial dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODEL ARCHITECTURE</head><p>In the existing literature, it is widely acknowledged that fully 3D architectures offer superior reconstruction quality, albeit at a high computational cost <ref type="bibr">(Chen et al., 2024a)</ref>. However, in this work, we demonstrate that substituting a portion of these 3D convolutions with a combination of 2D and 1D convolutions-effectively decoupling spatial and temporal sampling-can achieve comparable reconstruction quality while significantly reducing computational demands.</p><p>The detailed network architecture is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. As shown, 2D convolutions are employed for spatial upsampling and downsampling modules, while an AlphaBlender operator is utilized in the temporal upsampling and downsampling modules. The remaining components, including the input/output layers and bottleneck layers, leverage 3D convolutions to facilitate information fusion. The specific structures of the temporal upsampling and downsampling modules are depicted on the right side of Fig. <ref type="figure" target="#fig_2">3</ref>. Additionally, layer normalization <ref type="bibr" target="#b15">(Lei Ba et al., 2016)</ref> is incorporated throughout the architecture to enhance stability and performance. Experimental results, as summarized in Tab. 2, validate the effectiveness of the proposed architectural design.</p><p>AlphaBlender operator. Given a parameter α within the range [0, 1], the AlphaBlender operator performs the following operation to input x 1 and input x 2 :</p><formula xml:id="formula_2">x = α * x 1 + (1 -α) * x 2 (2)</formula><p>where x is the result after blending, and α can be either learnable or a given hyperparameter (PKU-YuanGroup).</p><p>In this work, we adopt a pre-defined α = Sigmoid(0.2). In causal cases, all 3D and 1D convolutions are configured to operate causally, ensuring that each frame has access only to historical information from preceding frames. For a given video X ∈ R (N +1)×3×H×W , the first frame is duplicated r t -1 times and inserted before the original first frame. After completing the full workflow, the process yields (n + 1) * r t frames. By discarding the first r t -1 frames, the reconstructed video X ∈ R (n * rt+1)×3×H×W is obtained.</p><formula xml:id="formula_3">z 0 z 2 z 1 -1 1 -2 codebook FSQ VQ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FINITE SCALAR QUANTIZATION</head><p>Variational AutoEncoders (VAEs) (Kingma &amp; Welling, 2014) are a class of generative models that map each data point, such as an image, from a complex dataset into a continuous distribution within a latent space, rather than assigning it to a single deterministic point. Conversely, the decoder performs the inverse operation, mapping representations from the latent space back to the original input space. However, due to the increasing demand for discrete latent variables, Vector Quantised-Variational AutoEncoder (VQ-VAE) <ref type="bibr" target="#b28">(Van Den Oord et al., 2017)</ref> were introduced. Unlike standard VAEs, VQ-VAEs map inputs to a finite set of vectors (i.e., codebook), through a process known as vector quantization. This approach represents each input by the closest vector in the codebook, which is learned during training. By combining the generative capabilities of VAEs with the advantages of discrete representations, VQ-VAEs provide a robust framework for various machine learning applications, including data compression, representation learning, and generative modeling.</p><p>In this work, we employ Finite Scalar Quantization (FSQ) <ref type="bibr" target="#b18">(Mentzer et al., 2024)</ref> to generate discrete tokens. The central principle of FSQ is that each scalar entry in the latent representation is independently quantized to the nearest pre-defined scalar value through rounding. In contrast to Vector Quantization (VQ), FSQ eliminates the need for codebook learning, thereby improving training stability <ref type="bibr" target="#b18">(Mentzer et al., 2024;</ref><ref type="bibr" target="#b6">Yu et al., 2024)</ref>. The approach can be described as follows: Given a vector z = (z 1 , z 2 , ..., z d ) with d channels, each channel z i is mapped to a value in a finite set of L pre-defined values, resulting in a quantized representation ẑ, which is one of L d possible vectors. An example is shown in Fig. <ref type="figure" target="#fig_3">4</ref>, where d=3 and L=5, representing an implicit codebook with size L d = 125. Notably, when L is set to 2, each z i can take one of two possible values, yielding binary latents. This mechanism corresponds to the Lookup-Free Quantization (LFQ) method proposed in MAGVIT-v2 <ref type="bibr" target="#b6">(Yu et al., 2024)</ref>.</p><p>The experiments in Sec. 4.3.2 show that FSQ has significant advantages in codebook utilization, reconstruction quality and training stability, functioning as an advanced quantization technique that effectively improves discrete tokenizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IMPROVED TRAINING STRATEGIES</head><p>Training video tokenizers is often computationally intensive, requiring substantial resources (e.g., 3, 072 GPU hours for 256 × 256 resolution videos). This necessitates the development of efficient strategies to reduce computational costs while maintaining model performance. In this work, we implement a two-stage training approach to address this challenge: the full model is initially pre-trained on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. Specifically, the model is first trained from scratch using videos at 128 × 128 resolution. In the second stage, the decoder is fine-tuned using videos at 256 × 256 resolution.</p><p>The experimental results presented in Tab. 4 demonstrate that the proposed two-stage training strategy achieves performance comparable to training the model from scratch on 256 × 256 resolution videos, while substantially VidTok reducing computational costs-cutting training time by half, from 3, 072 GPU hours to 1, 536 GPU hours. Furthermore, since the encoder remains unchanged, the fine-tuned model retains compatibility with the latent space of the pre-fine-tuned model. This ensures that the model can adapt efficiently to novel domains without impacting the integrity of models trained on the same latent space.</p><p>Moreover, as the video tokenizer is designed to model the motion dynamics of input videos, it is essential to efficiently represent these dynamics within the model. In this study, we empirically observe that training with data at reduced frame rates significantly enhances the model's capability to capture and represent motion dynamics. This finding is substantiated through the experimental results presented in Tab. 4 and Fig. <ref type="figure" target="#fig_5">6</ref>, which illustrate the improved reconstruction quality achieved with lower frame rate training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>This section verifies the proposed VidTok through comparative experiments with existing state-of-the-art video tokenizers <ref type="bibr" target="#b6">(Yu et al., 2024;</ref><ref type="bibr" target="#b30">Wang et al., 2024;</ref><ref type="bibr" target="#b19">NVIDIA;</ref><ref type="bibr" target="#b43">Zhao et al., 2024;</ref><ref type="bibr">hpcaitech;</ref><ref type="bibr" target="#b23">PKU-YuanGroup;</ref><ref type="bibr">Yang et al., 2024c)</ref> and comprehensive ablation studies. Fig. <ref type="figure" target="#fig_0">1</ref> provides several radar charts for a quick comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETTING</head><p>Dataset and metrics. For training, we utilize a self-collected video dataset, divided into two subsets based on video quality: (1) Training Set 1, comprising approximately 10 million low-resolution videos (e.g., 480p); and (2) Training Set 2, consisting of approximately 6 million high-resolution videos (e.g., 1080p). All videos in the dataset are natural videos characterized by diverse lighting conditions, motion patterns, and scenarios. For evaluation, we follow the protocol of MAGVIT-v2 <ref type="bibr" target="#b6">(Yu et al., 2024)</ref> and use two benchmark datasets: the MCL-JCV dataset <ref type="bibr" target="#b29">(Wang et al., 2016)</ref> and the validation set of a web video dataset. Evaluation videos are resized to 256 × 256 with a frame rate of 30 FPS.</p><p>The video reconstruction performance of the models is assessed using four widely-used metrics: Peak Signalto-Noise Ratio (PSNR) <ref type="bibr" target="#b10">(Hore &amp; Ziou, 2010)</ref>, Structural Similarity Index Measure (SSIM) <ref type="bibr" target="#b31">(Wang et al., 2004)</ref>, Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b40">(Zhang et al., 2018)</ref> and Fréchet Video Distance (FVD) <ref type="bibr" target="#b27">(Unterthiner et al., 2018)</ref>.</p><p>Implementation details. We implement video tokenizers with various settings, including both causal and non-causal cases, continuous and discrete latents, and different video compression ratios. All models are trained with four loss terms: a reconstruction term, a perceptual term, an adversarial term and a regularization term. The first three terms follow the practice in Latent Diffusion Models <ref type="bibr" target="#b24">(Rombach et al., 2022)</ref>. For the regularization term, we use KL loss <ref type="bibr" target="#b13">(Kingma &amp; Welling, 2014)</ref> in continuous tokenizers, and entropy penalty and commitment losses in discrete tokenizers <ref type="bibr" target="#b6">(Yu et al., 2024)</ref>.</p><p>In the first training stage, Training Set 1 is resized to a resolution of 128 × 128 and used for initial model training. We train VidTok for 50, 000 steps with batch size 16. In the second stage, Training Set 2 is resized to 256 × 256 and employed for fine-tuning. We fine-tune the decoder for another 30, 000 steps with batch size 8. The frame rate of the training data is maintained at 3 frames per second (FPS) during both stages. We use Adam optimizer <ref type="bibr" target="#b12">(Kingma &amp; Ba, 2015)</ref> with a constant learning rate of 1 × 10 -5 . The training is conducted on 8 NVIDIA 40G A100 GPUs with PyTorch <ref type="bibr" target="#b21">(Paszke et al., 2019)</ref>.</p><p>Baselines. We compare our method with the following state-of-the-art solutions: (1) MAGVIT-v2 <ref type="bibr" target="#b6">(Yu et al., 2024)</ref>: a discrete video tokenizer which maps videos to a discrete latent space using the LFQ representation;</p><p>(2) OmniTokenizer <ref type="bibr" target="#b30">(Wang et al., 2024)</ref>: a discrete video tokenizer using VQ as the discrete representation;</p><p>(3) CV-VAE <ref type="bibr" target="#b43">(Zhao et al., 2024)</ref>: a continuous video tokenizer with a latent space that aligns with the latent space of existing image VAEs; (4) : an open-source project aimed at reproducing OpenAI's Sora which offers a continuous video tokenizer; (5) Open-Sora-Plan-v1.2 (PKU-YuanGroup): another open-source project aimed at reproducing OpenAI's Sora; (6) CogVideoX <ref type="bibr">(Yang et al., 2024c)</ref>: a continuous tokenizer that preserves a greater amount of information by maintaining a larger number of latent channels; (7) Cosmos-Tokenizer (NVIDIA): a suite of continuous and discrete video tokenizers with various compression ratios. We conduct thorough experiments in Sec. 4.2, with aligned settings for all methods to guarantee fairness in comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARISON WITH BASELINES</head><p>To evaluate the advancements achieved by VidTok, we compare its performance against state-of-the-art models across various scenarios, encompassing both discrete and continuous tokenization approaches. The comprehensive comparison results are presented in Tab. 1. All performance metrics reported in the table, except for those of MAGVIT-v2 <ref type="bibr" target="#b6">(Yu et al., 2024)</ref>, are obtained through our own experiments conducted under an identical evaluation protocol to ensure consistency and fairness. For MAGVIT-v2, as the model is not publicly accessible, we reference the results reported in their original publication. It is important to note that these results were obtained on a resolution of 17 × 360 × 640, differing from the 17 × 256 × 256 resolution used for the other models in our comparison.</p><p>Compared to existing discrete tokenziers <ref type="bibr" target="#b6">(Yu et al., 2024;</ref><ref type="bibr" target="#b30">Wang et al., 2024;</ref><ref type="bibr" target="#b19">NVIDIA)</ref>, VidTok demonstrates significantly superior reconstruction performance, even when utilizing a smaller codebook size (e.g., 32, 768). This highlights the effectiveness of our approach in discrete tokenization. In the context of continuous tokenization, VidTok achieves comprehensive improvements across all evaluation metrics, regardless of whether the latent representation comprises 4 or 16 channels. Notably, these advancements are achieved even with a smaller model size, surpassing the performance of state-of-the-art methods <ref type="bibr" target="#b43">(Zhao et al., 2024;</ref><ref type="bibr">hpcaitech;</ref><ref type="bibr" target="#b23">PKU-YuanGroup;</ref><ref type="bibr">Yang et al., 2024c;</ref><ref type="bibr" target="#b19">NVIDIA)</ref>. These results underscore the effectiveness of VidTok in both discrete and continuous tokenization tasks.</p><p>We present the corresponding visual reconstruction results in Fig. <ref type="figure" target="#fig_4">5</ref> for qualitative comparison. From these visual results, our method exhibits a distinct advantage in detail reconstruction fidelity and subjective viewing experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION EXPERIMENTS</head><p>We conduct comprehensive ablation experiments to validate the superiority of the proposed model architecture, the advanced quantization technique and the improved training strategies. All ablation experiments are conducted with a video compression ratio of 4 × 8 × 8 and an input size of 17 × 256 × 256, evaluated on MCL-JCV <ref type="bibr" target="#b29">(Wang et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">ABLATION ON THE MODEL ARCHITECTURE</head><p>To evaluate the effectiveness of our proposed model architecture, we compare it with three alternative variants in terms of computational complexity and reconstruction quality. (1) Variant 1 employs a fully 3D architecture, integrating spatial and temporal sampling using 3D convolutions. (2) Variant 2 separates spatial and temporal  sampling, but does not incorporate the AlphaBlender operator for temporal sampling. (3) Variant 3 replaces all 3D convolutions with 2D convolutions.</p><p>The experimental results, summarized in Tab. 2, provide insights into the trade-offs between model performance and computational efficiency. The results indicate that employing a fully 3D architecture (Variant 1) results in high computational complexity and model size. By modifying the architecture to replace 3D convolutions in the spatio-temporal sampling modules with a combination of 2D and 1D convolutions (Variant 2), we achieve a significant reduction in computational load without notable degradation in reconstruction quality. Building upon Variant 2, the introduction of the AlphaBlender operator for temporal sampling yields substantial improvements across most metrics, albeit with a slight increase in computational cost. Furthermore, replacing all 3D convolutions with 2D convolutions (Variant 3) leads to a marked decline in reconstruction performance, underscoring the importance of retaining 3D convolutions for effective spatio-temporal representation. Overall, the findings in Tab. 2 highlight the efficacy of the proposed architecture, which strikes a balance between computational efficiency and reconstruction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">ABLATION ON THE DISCRETE TECHNIQUES</head><p>In Tab. 3, we present a comparison of various quantization methods, including VQ <ref type="bibr" target="#b28">(Van Den Oord et al., 2017)</ref>, LFQ <ref type="bibr" target="#b6">(Yu et al., 2024)</ref>, and FSQ <ref type="bibr" target="#b18">(Mentzer et al., 2024)</ref>. Additionally, we analyze the impact of the regularization loss term on the performance of discrete tokenizers.</p><p>The results highlight several key observations. Traditional VQ suffers from common challenges, such as training instability and codebook collapse, which lead to extremely low codebook utilization and suboptimal reconstruction quality. In contrast, LFQ and FSQ achieve nearly 100% codebook utilization by directly optimizing an implicit codebook, resulting in significantly enhanced tokenizer performance. Furthermore, FSQ outperforms LFQ's binary quantization by achieving better reconstruction fidelity, suggesting reduced information loss during the quantization process.</p><p>The effects of regularization loss vary across quantization methods. For conventional VQ, the absence of regularization loss leads to model collapse and convergence failure. In the case of LFQ, while the model remains capable of convergence without regularization, it experiences a marked decline in codebook utilization and reconstruction performance. FSQ, on the other hand, demonstrates superior training stability, with its performance remaining largely unaffected even in the absence of the regularization loss term.  In summary, FSQ emerges as a highly effective quantization technique, offering significant advantages in codebook utilization, reconstruction quality, and training stability. These attributes position FSQ as an advanced method for enhancing the performance of discrete tokenizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">ABLATION ON THE TRAINING STRATEGIES</head><p>As detailed in Sec. 3.4, we employ a two-stage training strategy: pre-training the full model on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. To evaluate the efficiency and effectiveness of this approach, we conduct an ablation study, with results summarized in Tab. 4.</p><p>In the first row, training the full model on high-resolution videos directly from scratch requires 3,072 GPU hours.</p><p>In contrast, the results in the fourth row demonstrate that the proposed two-stage training strategy-starting with low-resolution data and then fine-tuning on high-resolution data-reduces training time by half (from 3,072 to 1,536 GPU hours) while achieving comparable reconstruction quality. A comparison between the third and fourth rows reveals that fine-tuning only the decoder during the second stage produces similar performance to fine-tuning the entire model, with a lower computational cost. This approach also ensures that the low-resolution and high-resolution models share a unified latent space due to the fixed encoder, enabling latent models trained in this shared space to be reused across resolutions and domains.</p><p>Additionally, the last row examines the impact of varying the sampling rate during training. Qualitative results, presented in Fig. <ref type="figure" target="#fig_5">6</ref>, indicate that using training data with reduced frame rates enhances the model's ability to represent motion dynamics effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">MODEL SUMMARY</head><p>We provide a comprehensive summary of model performance in Tab. 5, covering both continuous and discrete tokenization, various video compression ratios, and causal versus non-causal scenarios. From Tab. 5, it is evident that as the video compression ratio increases, reconstruction performance deteriorates. Non-causal models generally outperform causal ones, as they are able to capture more extensive temporal information, which aids in the high-fidelity reconstruction of fine details. In the continuous case, increasing the number of channels in the latent representation allows for the retention of more information, leading to better reconstruction performance. Similarly, in the discrete case, a larger codebook size usually means smaller quantization errors, preserving more accurate information and thus achieving better reconstruction fidelity.</p><p>A comparison between the continuous and discrete cases reveals that FSQ with a codebook size of 262, 144 achieves reconstruction performance comparable to 'KL -4chn'.</p><p>Additionally, we compare the performance across different settings: 1) KL -16chn, with a video compression ratio of 4 × 8 × 8; 2) KL -8chn, with a video compression ratio is 2 × 8 × 8; 3) KL -4chn, with a video compression ratio is 4 × 4 × 4. Our analysis indicates that when the latent space contains the same amount of data, allocating it to the channel dimension tends to result in relatively better reconstruction performance.</p><p>All models and source code associated with this work are publicly available at <ref type="url" target="https://github.com/microsoft/VidTok">https://github.com/ microsoft/VidTok</ref>. We aspire for this contribution to serve as a foundation for and inspire further advancements in this research domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we present VidTok, a versatile and open-source video tokenizer that achieves state-of-the-art performance in both continuous and discrete tokenization. By converting raw visual data into compact latent tokens, VidTok provides an efficient foundation for tasks related to visual generation and understanding. Through the incorporation of advancements in model architecture, discrete representation, and training strategies, VidTok surpasses existing methods, demonstrating notable improvements across several performance metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation protocols. Additionally, we conduct extensive ablation experiments to thoroughly investigate the performance characteristics of the video tokenizer. We hope this work will inspire future research in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the quantitative comparison of discrete and continuous tokenization performance across our VidTok model and state-of-the-art methods, evaluated using four metrics: PSNR, SSIM, LPIPS, and FVD. All performance metrics are obtained through experiments conducted under a consistent evaluation protocol to ensure fairness and comparability. Larger chart areas correspond to better performance across all metrics.</figDesc><graphic coords="1,89.55,426.92,430.43,137.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of video tokenizers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The improved model architecture. In the context of a causal setting, consider an input with dimensions T × H × W = 17 × 256 × 256. Assuming a temporal compression factor of 4 and a spatial compression factor of 8, the intermediate latent representation is reduced to dimensions T × H × W = 5 × 32 × 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Vector Quantization (VQ) employed in Vector Quantised-Variational AutoEncoder (VQ-VAE) (Van Den Oord et al., 2017). Right: Finite Scalar Quantization (FSQ) (Mentzer et al., 2024) utilized in our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative comparison with the state-of-the-art video tokenizers.</figDesc><graphic coords="8,96.14,81.86,417.26,607.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The influence of different sample rates on model performance during training. The second row presents the test results obtained using training data with a sample rate of 8 FPS, while the third row shows the test results using training data with a sample rate of 3 FPS. The results demonstrate that employing training data with reduced frame rates enhances the model's capacity to effectively capture motion dynamics.</figDesc><graphic coords="10,87.35,212.96,434.81,144.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison with the state-of-the-art video tokenizers. All evaluated models are causal and have a video compression ratio of 4 × 8 × 8. The input resolution for most models is 17 × 256 × 256, except for MAGVIT-v2 * , which is evaluated on 17 × 360 × 640 as reported in the original study. The sample rate of testing data is 30 FPS. We highlight the best and the second-best numbers in bold and underline respectively.</figDesc><table><row><cell>Method</cell><cell>Regularizer</cell><cell>Param.</cell><cell></cell><cell cols="2">MCL-JCV</cell><cell></cell><cell></cell><cell cols="2">WebVid-Val</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">PSNR↑ SSIM↑ LPIPS↓ FVD↓ PSNR↑ SSIM↑ LPIPS↓ FVD↓</cell></row><row><cell>MAGVIT-v2  *</cell><cell>LFQ -262, 144</cell><cell>-</cell><cell>26.18</cell><cell>-</cell><cell>0.104</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OmniTokenizer</cell><cell>VQ -8, 192</cell><cell>51M</cell><cell cols="8">26.93 0.841 0.165 232.7 26.26 0.883 0.112 48.46</cell></row><row><cell>Cosmos-DV</cell><cell cols="10">FSQ -64, 000 101M 28.07 0.743 0.212 227.7 29.39 0.741 0.170 57.97</cell></row><row><cell>Ours-FSQ</cell><cell cols="10">FSQ -32, 768 157M 29.16 0.854 0.117 196.9 31.04 0.883 0.089 45.34</cell></row><row><cell>Ours-FSQ</cell><cell cols="10">FSQ -262, 144 157M 29.82 0.867 0.106 160.1 31.76 0.896 0.080 38.17</cell></row><row><cell>CV-VAE</cell><cell>KL -4chn</cell><cell cols="9">182M 28.56 0.823 0.163 334.2 30.79 0.863 0.116 70.39</cell></row><row><cell>Open-Sora-v1.2</cell><cell>KL -4chn</cell><cell cols="9">393M 29.44 0.766 0.164 350.7 31.02 0.764 0.137 112.34</cell></row><row><cell cols="2">Open-Sora-Plan-v1.2 KL -4chn</cell><cell cols="9">239M 29.07 0.839 0.131 201.7 30.85 0.869 0.101 44.76</cell></row><row><cell>Ours-KL</cell><cell>KL -4chn</cell><cell cols="9">157M 29.64 0.852 0.114 194.2 31.53 0.878 0.087 36.88</cell></row><row><cell>CogVideoX</cell><cell>KL -16chn</cell><cell cols="9">206M 33.76 0.930 0.076 93.2 36.22 0.952 0.049 15.30</cell></row><row><cell>Cosmos-CV</cell><cell>AE -16chn</cell><cell cols="9">101M 31.27 0.817 0.149 153.7 33.04 0.818 0.107 23.85</cell></row><row><cell>Ours-KL</cell><cell>KL -16chn</cell><cell cols="8">157M 35.04 0.942 0.047 78.9 37.53 0.961 0.032</cell><cell>9.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the model architecture. Variant 1: fully 3D architecture. Variant 2: w/o AlphaBlender. Variant 3: w/o 3D architecture. We use 'KL -4chn' as regularizer for all settings.</figDesc><table><row><cell>Method</cell><cell>Param.</cell><cell>FLOPs</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>LPIPS↓</cell><cell>FVD↓</cell></row><row><cell>Variant 1</cell><cell>245M</cell><cell>16.98 T</cell><cell>29.39</cell><cell>0.847</cell><cell>0.117</cell><cell>176.9</cell></row><row><cell>Variant 2</cell><cell>142M</cell><cell>7.17 T</cell><cell>29.36</cell><cell>0.846</cell><cell>0.119</cell><cell>185.7</cell></row><row><cell>Variant 3</cell><cell>126M</cell><cell>10.18 T</cell><cell>29.26</cell><cell>0.846</cell><cell>0.120</cell><cell>200.6</cell></row><row><cell>Ours</cell><cell>157M</cell><cell>10.35 T</cell><cell>29.64</cell><cell>0.852</cell><cell>0.114</cell><cell>194.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Analysis of the impact of discrete techniques on model performance. R.L. denotes Regularization Loss, while U.R. represents Utilization Rate.</figDesc><table><row><cell>Regularizer</cell><cell>w/ R.L.</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>LPIPS↓</cell><cell>FVD↓</cell><cell>U.R.↑</cell></row><row><cell>VQ -262,144</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VQ -262,144</cell><cell>✓</cell><cell>23.22</cell><cell>0.657</cell><cell>0.336</cell><cell>960.5</cell><cell>0.2%</cell></row><row><cell>LFQ -262,144</cell><cell></cell><cell>23.91</cell><cell>0.688</cell><cell>0.251</cell><cell>619.8</cell><cell>4.2%</cell></row><row><cell>LFQ -262,144</cell><cell>✓</cell><cell>28.04</cell><cell>0.833</cell><cell>0.133</cell><cell>208.1</cell><cell>99.9%</cell></row><row><cell>FSQ -262,144</cell><cell></cell><cell>29.75</cell><cell>0.866</cell><cell>0.109</cell><cell>167.5</cell><cell>99.8%</cell></row><row><cell>FSQ -262,144</cell><cell>✓</cell><cell>29.82</cell><cell>0.867</cell><cell>0.106</cell><cell>160.1</cell><cell>99.8%</cell></row><row><cell>FSQ -32,768</cell><cell>✓</cell><cell>29.16</cell><cell>0.854</cell><cell>0.117</cell><cell>196.9</cell><cell>100.0%</cell></row><row><cell>FSQ -4,096</cell><cell>✓</cell><cell>28.36</cell><cell>0.832</cell><cell>0.133</cell><cell>218.1</cell><cell>100.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the proposed training strategy. To ensure a fair comparison, both stages use training data from Training Set 1. Across all configurations, the regularizer 'KL -4chn' is employed. The training computational cost, measured in GPU hours, is evaluated using NVIDIA A100 GPUs. Sample Rate First Stage Second Stage Fix Enc. PSNR↑ SSIM↑ LPIPS↓ FVD↓ GPU Hours</figDesc><table><row><cell>3 FPS</cell><cell>256 × 256</cell><cell>-</cell><cell>-</cell><cell>29.19</cell><cell>0.843</cell><cell>0.127 174.9</cell><cell>3,072</cell></row><row><cell>3 FPS</cell><cell>128 × 128</cell><cell>-</cell><cell>-</cell><cell>29.02</cell><cell>0.838</cell><cell>0.130 221.7</cell><cell>960</cell></row><row><cell>3 FPS</cell><cell cols="2">128 × 128 256 × 256</cell><cell></cell><cell>29.15</cell><cell>0.842</cell><cell>0.127 203.2</cell><cell>1,728</cell></row><row><cell>3 FPS</cell><cell cols="2">128 × 128 256 × 256</cell><cell>✓</cell><cell>29.21</cell><cell>0.843</cell><cell>0.125 189.8</cell><cell>1,536</cell></row><row><cell>8 FPS</cell><cell cols="2">128 × 128 256 × 256</cell><cell>✓</cell><cell>29.02</cell><cell>0.839</cell><cell>0.126 219.2</cell><cell>1,536</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Model summary. We offer a suite of models with diverse configurations, encompassing both continuous and discrete tokenization, various video compression ratios (VCR), and options for causal and non-causal scenarios. These configurations are designed to address the distinct requirements of various downstream tasks.</figDesc><table><row><cell>Regularizer</cell><cell>Causal</cell><cell>Input Size</cell><cell>VCR</cell><cell cols="4">Latent Size Param. PSNR↑ SSIM↑ LPIPS↓ FVD↓</cell></row><row><cell>KL -4chn</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 8 × 8 5 × 32 × 32 157M</cell><cell>29.64</cell><cell>0.852</cell><cell>0.114 194.2</cell></row><row><cell>KL -4chn</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 16 × 16 5 × 16 × 16 199M</cell><cell>25.05</cell><cell>0.711</cell><cell>0.228 549.1</cell></row><row><cell>KL -4chn</cell><cell></cell><cell cols="3">16 × 256 × 256 4 × 8 × 8 4 × 32 × 32 158M</cell><cell>30.60</cell><cell>0.876</cell><cell>0.098 157.9</cell></row><row><cell>KL -4chn</cell><cell></cell><cell cols="3">16 × 256 × 256 4 × 16 × 16 4 × 16 × 16 199M</cell><cell>26.06</cell><cell>0.751</cell><cell>0.190 423.2</cell></row><row><cell>KL -8chn</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 8 × 8 5 × 32 × 32 157M</cell><cell>31.83</cell><cell>0.897</cell><cell>0.083 109.3</cell></row><row><cell>KL -16chn</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 8 × 8 5 × 32 × 32 157M</cell><cell>35.04</cell><cell>0.942</cell><cell>0.047</cell><cell>78.9</cell></row><row><cell>KL -8chn</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 2 × 8 × 8 9 × 32 × 32 149M</cell><cell>33.86</cell><cell>0.928</cell><cell>0.057</cell><cell>80.7</cell></row><row><cell>KL -4chn</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 4 × 4 5 × 64 × 64 155M</cell><cell>34.78</cell><cell>0.941</cell><cell>0.051</cell><cell>87.2</cell></row><row><cell>FSQ -4,096</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 8 × 8 5 × 32 × 32 157M</cell><cell>28.36</cell><cell>0.832</cell><cell>0.133 218.1</cell></row><row><cell>FSQ -32,768</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 8 × 8 5 × 32 × 32 157M</cell><cell>29.16</cell><cell>0.854</cell><cell>0.117 196.9</cell></row><row><cell>FSQ -262,144</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 8 × 8 5 × 32 × 32 157M</cell><cell>29.82</cell><cell>0.867</cell><cell>0.106 160.1</cell></row><row><cell>FSQ -262,144</cell><cell>✓</cell><cell cols="3">17 × 256 × 256 4 × 16 × 16 5 × 16 × 16 199M</cell><cell>25.38</cell><cell>0.738</cell><cell>0.206 430.1</cell></row><row><cell>FSQ -262,144</cell><cell></cell><cell cols="3">× 256 × 256 4 × 8 × 8 4 × 32 × 32 157M</cell><cell>30.78</cell><cell>0.889</cell><cell>0.091 132.1</cell></row><row><cell>FSQ -262,144</cell><cell></cell><cell cols="3">× 256 × 256 4 × 16 × 16 4 × 16 × 16 199M</cell><cell>26.37</cell><cell>0.772</cell><cell>0.171 357.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We extend our gratitude to all individuals who contributed their insights and efforts to this project, including <rs type="person">Chong Luo</rs>, <rs type="person">Ruoyu Feng</rs>, and <rs type="person">Zhipeng Huang</rs> for their valuable discussions, and <rs type="person">Qi Dai</rs> for his guidance on the open-source process.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uniedit: A unified tuning-free framework for video motion and appearance editing</title>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoji</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.13185</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Chameleon: Mixed-modal early-fusion foundation models</title>
		<author>
			<persName><surname>Chameleon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.09818</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Od-vae: An omni-dimensional video compressor for improving latent video diffusion model</title>
		<author>
			<persName><forename type="first">Liuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.01199</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Igor: Image-goal representations are the atomic control units for foundation models in embodied ai</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Cathera</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.00785</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<ptr target="https://github.com/CompVis/latent-diffusion" />
	</analytic>
	<monogr>
		<title level="j">CompVis. latent-diffusion</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Imagen video: High definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8633" to="8646" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image quality metrics: Psnr vs. ssim</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djemel</forename><surname>Ziou</surname></persName>
		</author>
		<ptr target="https://github.com/hpcaitech/Open-Sora" />
	</analytic>
	<monogr>
		<title level="m">2010 20th international conference on pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
	<note>hpcaitech. Open-sora</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<ptr target="https://github.com/huggingface/open-muse" />
	</analytic>
	<monogr>
		<title level="j">HuggingFace. open-muse</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">April 14-16, 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Videopoet: A large language model for zero-shot video generation</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Videochat: Chat-centric video understanding</title>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06355</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finite scalar quantization: Vq-vae made simple</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Nvidia</forename><surname>Vidtok</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/Cosmos-Tokenizer" />
		<title level="m">Cosmos-tokenizer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Sora</surname></persName>
		</author>
		<ptr target="https://openai.com/index/sora/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><surname>Patrick Von Platen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01808</idno>
		<title level="m">An open muse reproduction</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><surname>Pku-Yuangroup</surname></persName>
		</author>
		<ptr target="https://github.com/PKU-YuanGroup/Open-Sora-Plan" />
		<title level="m">Open-sora-plan</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the high efficiency video coding (hevc) standard</title>
		<author>
			<persName><forename type="first">Jens-Rainer</forename><surname>Gary J Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woo-Jin</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Wiegand</surname></persName>
		</author>
		<ptr target="https://github.com/TencentARC/Open-MAGVIT2" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<title level="s">TencentARC. Open-magvit</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1649" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<title level="m">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mcl-jcv: a jnd-based h. 264/avc video quality assessment dataset</title>
		<author>
			<persName><forename type="first">Haiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Yuchieh Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longguang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international conference on image processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1509" to="1513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Junke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyue</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Omnitokenizer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09399</idno>
		<title level="m">A joint image-video tokenizer for visual generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Janus: Decoupling visual encoding for unified multimodal understanding and generation</title>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Ruan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13848</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Videogpt: Video generation using vq-vae and transformers</title>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10157</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning interactive real-world simulators</title>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Kamyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Position: Video as the new language for real-world decision making</title>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">C</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Parker-Holder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Machine Learning</title>
		<meeting>the 41st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2024-07">Jul 2024b</date>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vector-quantized image modeling with improved VQGAN</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Vidtok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Magvit: Masked generative video transformer</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10459" to="10469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language model beats diffusion-tokenizer is key to visual generation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><surname>Bharadwaj Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Versari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.07356</idno>
		<title level="m">Video in-context learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.02713</idno>
		<title level="m">Video instruction tuning with synthetic data</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Cv-vae: A compatible video vae for latent generative video models</title>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoshu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20279</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Movq: Modulating quantized vectors for high-fidelity image generation</title>
		<author>
			<persName><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung-Long</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23412" to="23425" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Compositional 3d-aware video generation with llm director</title>
		<author>
			<persName><forename type="first">Hanxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
