- Decision to adopt a parameter-efficient framework for transformer models
- Choice to replace feed-forward networks with mixture-of-experts (MoE) layers
- Implementation of parameter sharing across transformer blocks
- Decision to use individual normalization layers instead of shared ones
- Selection of WideNet as the model name for the proposed framework
- Choice of benchmarks for evaluating WideNet's performance
- Decision to focus on both computer vision and natural language processing tasks
- Choice of routing mechanism for expert selection in MoE
- Implementation of balanced loading strategies for expert utilization
- Decision to use a differentiable load balance loss for routing optimization
- Choice of hyperparameters for buffer capacity and expert selection
- Decision to evaluate the model's performance against existing architectures like ViT and ALBERT
- Choice to conduct comprehensive experiments to validate the proposed framework
- Decision to extend the framework to other transformer models like BERT
- Choice to emphasize the trade-off between model width and depth in design decisions