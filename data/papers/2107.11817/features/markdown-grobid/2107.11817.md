# Go Wider Instead of Deeper

## Abstract

## 

More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference. In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-ofexperts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by 1.5% with 0.72× trainable parameters. Using 0.46× and 0.13× parameters, our WideNet can still surpass ViT and ViT-MoE by 0.8% and 2.1%, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by 1.8% on average and surpass BERT using factorized embedding parameterization by 0.8% with fewer parameters. 1

## Introduction

Transformer-based models have achieved promising results on various tasks (e.g., Q&A [(Qu et al. 2019;](#b13)[Yang et al. 2020)](#b29), relation extraction [(Xue et al. 2020b,a;](#)[Zhou et al. 2020)](#b35)). To further improve the effectiveness and efficiency of the transformer, there are two trains of thought to deploy trainable parameters. The first thought is to scale transformer along width to more trainable parameters (e.g., Switch Transformer [(Fedus, Zoph, and Shazeer 2021)](#b6), ViT-MoE [(Riquelme et al. 2021)](#b18)). These sparse models can scale to extremely large models with comparable FLOPs by sparse conditional computation. Another thought is to decrease the trainable parameters for a lite model. To this end, some works propose to reuse the trainable parameters across transformer blocks (e.g., Universal Transformer [(Dehghani et al. 2018)](#b2) and ALBERT [(Lan et al. 2019)](#b9)). Model compression [(Xu et al. 2020;](#b25)[Sun et al. 2019](#b22)) can also make transformer more parameter efficient.

The two existing methods both have their own limitations. For huge models, one typical and effective method to scale trainable parameters is replacing part of the feed-forward network (FFN) layer in transformer blocks with MoE layers. In each MoE layer, to refine one single token representation, only a few experts are activated, so the MoE based transformer holds comparable FLOPs with the vanilla transformer. However, during training and inference, we are required to use advanced parallelisms (e.g., tensor [(Shoeybi et al. 2019)](#b20), sequence [(Li et al. 2021)](#b11), pipeline [(Huang et al. 2018](#b7)) and expert parallelism [(Lepikhin et al. 2020](#b10))) to hold these models on TPU or GPU. Also, the performance cannot improve linearly during scaling. Another limitation is that the sparseness of MoE based models cannot scale well on relatively small datasets. We will discuss the reason for this phenomenon in the following sections. For small models, although they can reduce trainable parameters significantly by going shallower, the performance of these shallower models is still under the original transformers. These smaller models are constructed by compressing the original model along with depth so all transformer blocks share the same knowledge. Such structure induces the unavoidable loss of model capacity.

In this paper, we present a parameter deployment framework that deploys trainable parameters more effectively: going wider instead of deeper. We then implement it on the transformer and named it as WideNet. Specially, we first employs parameter sharing along with depth to go shallower. Due to avoidable model capacity loss, we go wider by using the same MoE layer in all transformer blocks. The multihead attention layer is also shared across the blocks. To help the transformer blocks learn different semantics and maximize the modeling capacity from MoE layer, we do not share the normalization layers. Different trainable parameters of the normalization layer enable transformer blocks to be fed by diversified representations. Since the modeling capacity of each transformer block has been enhanced by the MoE layer, it can model diversified semantics effec-

LayerNorm Multi-Head Attention LayerNorm MoE i Transformer Block Recurrence FFN 0 FFN 1 FFN 2 FFN 3 Router h ha th att i moe i l i l+1 ha L i ha 1 i i Figure 1: The overall architecture of the proposed WideNet. Compared with vanilla transformer, we replace FFN layer by MoE layer and share the trainable parameters except the normalization layers.

tively with the same trainable parameters. Therefore, with one attention layer and one single stronger MoE layer learning complex representations, and independent normalization layers for diversified semantic representations, going wider instead of deeper is a more parameter-efficient and effective framework.

Compared with simply scaling along the width, going wider instead of deeper is a more parameter-efficient framework, which makes the models small enough to be adapted to downstream tasks without advanced parallelisms. Second, each expert in WideNet can be trained by more token representations so that it has better generalization performance.

Compared with the models simply compressed along with the depth, all transformer blocks in WideNet share one same MoE layer instead of one FFN layer. Such structure maximizes the modeling ability of every transformer block. More experts can model more complex token representations with a stronger capacity. Another difference is the independent normalization layers. These layers come with few additional trainable parameters, but they can transform input representations to other semantic domains. In this case, with a strong enough single MoE layer, WideNet can still model semantics from different levels well. Moreover, in every transformer block, each expert only receives a part of token representations that usually correspond to different input tokens.

Our contributions are summarized as three folds:

• To improve the parameter efficiency, we propose sharing the MoE layer across transformer blocks. The shared experts can receive diversified token representations in different transformer blocks, which enables each expert to be fully trained.

• We propose to keep individual normalization layer across transformer blocks. The individual normalization layers can transform input hidden vectors to semantic information by adding few trainable parameters. Then, diversified input can be fed into the same attention layer or stronger MoE layer to model different semantics. • By combing the two thoughts above, we propose going wider instead of deeper, a more parameter-efficient and effective framework. We then implement this framework as WideNet and evaluate it on both computer vision and natural language processing tasks. Due to the more efficient parameter deployment, WideNet outperforms baselines with less trainable parameters. We expect our WideNet can serve as a next-generation transformer backbone.

## Mixture-of-Experts

In this paper, we focus on a novel trainable parameter deployment framework and implement this framework on the transformer as WideNet. The overall structure is shown in Fig. [1](#). We use Vision Transformer as the backbone in this example, which means we normalize the representations before the attention layer or FFN layer. We also extend WideNet to other transformer models (e.g., BERT [(Devlin et al. 2019)](#b4)) in this paper. In WideNet, we replace the FFN layer with the MoE layer. Parameter sharing across transformer blocks is employed for a more parameter-efficient deployment. Within each MoE layer, we have one router to select K experts to learn more complex representations. Please note the trainable parameters in layer normalization are not shared for more diversified semantic representations.

## Conditional Computation with MoE

Our core idea is to deploy more trainable parameters along the width and fewer trainable parameters along with the depth. To this end, we employ MoE to scale transformer along with width. As a typical conditional computation model [(Bengio 2013)](#b0), MoE only activates a few experts, i.e., subsets of a network. For each input, we feed only a part of hidden representations required to be processed into the selected experts. Following [Shazeer et al. (2017)](#b19), given E trainable experts and input representation x ∈ R D , the output of MoE model can be formulated as:

$MoE(x) = E i=1 g(x) i e(x) i (1)$where e(•) i is a non-linear transformation R D → R D of i th expert, and g(•) i is i th element of the output of trainable router g(•), a non-linear mapping R D → R E . Usually, both e(•) and g(•) are parameterized by neural networks.

According to the formulation above, when g(•) is a sparse vector, only part of experts would be activated and updated by back-propagation during training. In this paper, for both vanilla MoE and our WideNet, each expert is an FFN layer.

## Routing

To ensure a sparse routing g(•), we use TopK() to select the top ranked experts. Then, following [Riquelme et al. (2021)](#b18), g(•) can be written as:

$g(x) = TopK(softmax(f (x) + ))(2)$where f (•) is routing linear transformation R D → R E , and ∼ N (0, 1 E 2 ) is a Gaussian noise for exploration of expert routing. We use softmax after f (•) for better performance and more sparse experts [(Riquelme et al. 2021;](#b18)[Fedus, Zoph, and Shazeer 2021)](#b6). When K E, most elements of g(x) would be zero so that sparse conditional computation is achieved.

## Balanced Loading

In MoE based transformer, we dispatch each token to K experts. During training, if the MoE model has no regularization, most tokens may be dispatched to a small portion of experts. Such an unbalanced assignment would decrease the throughput of the MoE model. In addition, more importantly, most additional trainable parameters would not be fully trained so that the sparse conditional model cannot surpass the corresponding dense model during scaling. Therefore, for balanced loading, we have two things to avoid:

(1) too many tokens dispatched to one single expert, and (2) too few tokens received by one single expert. To solve the first issue, buffer capacity B is required. That is, for each expert, we only preserve B token at most regardless of how many tokens are dispatched to this expert. If more than B = CKN L tokens are assigned, the left tokens would be dropped. C is the capacity ratio, a pre-defined hyperparameter to control the ratio of tokens preserved for each expert. Usually, C ∈ [1, 2], and we set C as 1.2 when no special explanation is used. K is the number of selected experts for each token. N is the batch size on each device[foot_1](#foot_1) . L is the sequence length. For computer vision tasks, L denotes the number of patch tokens in each image.

Buffer capacity B helps us drop redundant tokens for each expert to maximize throughput but it cannot ensure all experts to receive enough token to train. In other words, until now, the routing is still unbalanced. Therefore, we follow [Fedus, Zoph, and Shazeer (2021)](#b6) to use a differentiable load balance loss instead of separate load-balancing and importance-weighting losses for a balanced loading in the router. For each routing operation, given E experts and N batches with N L tokens, the following auxiliary loss is added to the total model loss during training:

$l balance = E • E i=1 m i • P i (3)$where m is vector. i th element is the fraction of tokens dispatched to expert i:

$m i = 1 L L j=1 h(x j ) i (4)$where h(•) is a index vector selected by TopK in Eq. 2. h(x j ) i is i th element of h(x j ). It is noticeable that, different from g(x) i in Eq. 2, m i and h(x j ) i are non-differentiable. However, a differentiable loss function is required to optimize MoE in an end-to-end fashion. Therefore, we define P i in Eq. 3 as:

$P i = softmax(f (x) + ) i(5)$We can observe P i is i th element of routing linear transformation after softmax activation function, and P i is differentiable.

The goal of load balancing loss is to achieve a balanced assignment. When we minimize l balance , we can see both m and P would close to a uniform distribution.

## Go wider instead of deeper Sharing MoE across transformer blocks

As shown in Fig. [1](#), WideNet adopts parameter sharing across transformer blocks to improve parameter efficiency, and MoE layer is used to improve model capacity. In addition, as we use the MoE layer to obtain a stronger modeling ability, to overcome the overfitting from sparse conditional computation, we are required to feed enough tokens to each expert. To this end, WideNet uses the same router and experts in different transformer blocks. Formally, given hidden representations H 1 = {h 1 1 , h 1 2 , . . . , h 1 L } as input of the first transformer block, we can define the parameter sharing as H i+1 = MoE(H i ), which is different from the existing MoE based models H i+1 = MoE i (H i ). Please note that, although we share trainable parameters in the MoE layer including the router, token representations corresponding to the same token are different in every transformer block. That is, h j i and h j+1 i may be dispatched to different experts. Therefore, each expert would be trained by more varied tokens for better generalization performance.

## Individual Layer Normalization

Although existing works [(Lan et al. 2019)](#b9) show that the activations in different transformer blocks are similar, the cosine distance is still much larger than zero. Therefore, different from existing works [(Dehghani et al. 2018;](#b2)[Lan et al. 2019](#b9)) sharing all weights across transformer blocks, to encourage more diversified input representations of different blocks, we only share multi-head attention layer and FFN (or MoE) layer, which means trainable parameters of layer normalization are different across blocks.

In summary, i th transformer block in our framework can be written as:

$x = LayerNormal att i (x) x = MHA(x ) + x x = LayerNormal moe i (x) x = MoE(x ) + x (6)$The normalization layer LayerNormal(•) is:

$LayerNormal(x) = x -E[x] Var[x] + * γ + β (7)$where γ ∈ R D and β ∈ R D are two trainable vectors. Layer normalization only requires these two small vectors so individual normalization would just add few trainable parameters into our framework. We can find the difference between shared layer normalization and the individual ones is the mean and magnitude of output. For shared layer normalization, the input of MHA and MoE layer are more similar in different transformer blocks. Since we have shared trainable matrixes, we encourage more diversified input to represent various semantics in different transformer blocks.

## Optimization

Although we reuse the trainable parameters of the router in every transformer block, the assignment would be different due to different input representations. Therefore, given T times routing operation with the same trainable parameters, we have the following loss for optimization:

$loss = l main + λ T t=1 l T balance (8$$)$where λ is a hyper-parameter to ensure a balanced assignment, and we set it as a relatively large number, i.e., 0.01 in this work. Similar to existing MoE based models, we found the performance is non-sensitive to λ. l main is the main target of our transformer. For example, on supervised image classification, l main is cross-entropy loss.

## Experiments Computer Vision

Experimental Settings We use ILSVRC-2012 Ima-geNet [(Deng et al. 2009](#b3)) and Cifar10 (Krizhevsky, Hinton Instead of achieving SoTA performance, the goal of this paper is to show that our parameter deployment framework can improve the transformer backbone with less trainable parameters. Therefore, we employ LAMB instead of AdamW for more general and typical experiments. For MoE based models (i.e., ViT-MoE and WideNet), we set the weight of load balance loss λ as 0.01. Without special instructions, we use 4 experts in total and Top 2 experts selected in each transformer block. The capacity ratio C is set as 1.2 for a trade-off between accuracy and speed. We pretrain our models on 256 TPUv3 cores. According to recent work [(Zhai et al. 2021)](#b33), different types of the prediction head have no significant difference on ImageNet's few-shot performance. We also verify this conclusion on training Im-ageNet from scratch. In this work, for ViT, we use the typical token head, which means we insert [CLS] token at the start of patch tokens and use it to classify the image. For MoE based models, to fully use the token representations after the final MoE layer, we employ a global average pooling head instead of the token head.

During finetuning, we still follow [(Dosovitskiy et al. 2020](#b5)) and use SGD optimizer with momentum. Compared with pretraining on ImageNet-1K, label smoothing and warm-up are removed. 

## Natural Language Processing

The main contribution of this work is to design a more parameter-efficient and plug-in framework for various AI applications. Therefore, we further evaluate our work on natural language processing (NLP) after computer vision (CV). The training of experiments on NLP can still be splitted into 2 stages, pretraining and finetuning.

Experimental Settings Following BERT [(Devlin et al. 2019)](#b4) and ALBERT [(Lan et al. 2019)](#b9), in this section, we pretrain all models by English Wikipedia [(Devlin et al. 2019)](#b4) and BOOKCORPUS [(Zhu et al. 2015)](#b36). Since the goal of this work is to design a parameter-efficient framework, all models including BERT use factorized embedding parameterization. That is, the WordPiece embedding size E is 128. The hyperparameter settings of experiments on NLP can be found in Appendix, which is the same as ALBERT for a fair comparison. Similar to the experiments on vision tasks, we pretrain our models by LAMB on 256 TPUv3 cores. The learning rate is 0.00176, which is the same as ALBERT claimed [(You et al. 2019a)](#).

During finetuning, we evaluate our model on the General Language Understanding Evaluation (GLUE) benchmark [(Wang et al. 2018)](#b24), two versions of the Stanford Ques-tion Answering (SQuAD) dataset [(Rajpurkar et al. 2016;](#b16)[Rajpurkar, Jia, and Liang 2018)](#b15). For GLUE experiments, we report median over 5 runs on development set because of relatively large variance. Downstream Evaluation Different from the experiments on CV, we report the evaluation results on downstream tasks directly in this section. As shown in Table [2](#tab_3), when we use more experts, our WideNet outperforms ALBERT by a large margin. For instance, WideNet with 4 experts surpasses AL-BERT by 1.2% in average. When we increase the number of experts E to 16 to achieve slightly less trainiable parameters than BERT with factorized embedding parameterization, our WideNet also outperforms it on all four downstream tasks, which shows the parameter-efficiency and effectiveness of going wider instead of deeper.

## MoE Analysis

To investigate the reason why MoE cannot scale well on smaller datasets like ImageNet-1K, we conduct two sets of experiments on ViT-MoE and WideNet, respectively. Given following hyper-parameters: (1) Number of training images N I ; (2) Number of patch tokens per image N p ; (3) Number of experts in each transformer block E; (4) Capacity ratio C; (5) Number of experts selected in each transformer block K, as we usually use a large λ, we can assume few tokens would be dropped when we are using C slightly larger than 1.0. Then, we can approximate T ≈ N I NpK E

. Existing works [(Riquelme et al. 2021;](#b18)[Yang et al. 2021](#b28)) have shown that decreasing N I , N p , K and C can induce a performance drop. In the first set of experiments of this section, we scale the number of experts in every transformer block E to control the tokens fed into each expert on ImageNet-1K.

Results are shown in Fig. [2](#fig_0). We observe that more experts (trainable parameters) lead to overfitting although more experts mean stronger modeling capacity. Training accuracy is lower than testing accuracy because of data augmentation we introduced in the Experimental Settings Section.

To further verify that each expert requires varied tokens to train, we conduct the second set of experiments on WideNet. We define the transformer blocks using the same routing assignment that belongs to one group. To change the input diversity of each expert, each group includes more than one transformer block. That is, the hidden representations corresponding to the same token would be fed into the same expert within the same group. We set G groups in total and each group includes D G transformer blocks, where D is the number of transformer blocks.  As shown in Fig. [3](#fig_1), when we use fewer groups, which means we have fewer routing operations, there is an obvious performance drop. We can suggest less diversified tokens are fed to each expert because fewer groups mean less routing and assignments. Therefore, more diversified tokens are required to train MoE based models on smaller datasets. More importantly, such results show the effectiveness and necessity of our design, routing at every transformer block.

## Layer Norm Analysis

We are to analyze the reason why individual layer normalization can improve performance in this section. Compared with the vanilla transformer structure, we share trainable matrixes in MHA and FFN (or MoE) layer across transformer blocks. The modeling capacity is compressed due to the same trainable parameters across blocks. Although WideNet uses the MoE layer to replace the FFN layer to improve capacity, different blocks are still using the same trainable parameters. Therefore, in WideNet, we encourage more diversified input to represent various semantics in different transformer blocks. Compared with vanilla ViT, we expect a larger variance of trainable vectors γ and β across blocks. In this section, we are interested in layer normalization before MoE or FFN.

Therefore, for i th element of trainable vector γ or β in j th block, we compute the distance between this element and all other elements of all vectors from other blocks. Taken γ as example, we can formulate the value y we are interested in as:

$y = 1 M N 2 N j=1 M m=1 N n=1 I (j =n) |γ ij -γ mn | (9)$where N is the number of transformer blocks, M is the dimension of vector γ or β.

In Fig. [4](#) and Fig. [5](#), we can observe that both γ and β in WideNet have larger y than those in ViT, which means MoE receives more diversified input than ViT. Such result proves our assumption that individual normalization layer can help to model various semantics model with shared large trainable matrixes like MoE. 

## Ablation Study: Contributions of key modifications

We first conduct the ablation study to investigate the contributions of our three key modifications (i.e., Independent Layer Normalization, scaling width with MoE layer, and compressing depth with parameter sharing). The results are reported in Table [3](#tab_4).

We first replace the individual layer normalizations with the shared ones. We can observe there is a performance drop with almost the same trainable parameters. Such observation shows the effectiveness of our design. In addition, we recover the MoE layer to the FFN layer. Without the MoE layer, the training would be extremely difficult with much less trainable parameters. For example, WideNet-B without MoE layer encounters gradient explosion, and there is a significant performance drop. Finally, without parameter sharing across transformer blocks, we can also observe a slight performance drop and significant parameter increment. For WideNet-H without parameter sharing, it encounters out-ofmemory when training on 256 TPUv3 cores.  As shown in Table [4](#tab_5), compared with ViT-L, WideNet-L is more computation expensive. We can observe a training time increment. However, when WideNet-L uses fewer transformer blocks (i.e., 12 blocks) than ViT-L, WideNet-L outperforms ViT-L by 0.7% with slightly less training time and 13.1% parameters, and, similarly, there is a larger performance improvement than ViT-L with parameter sharing. We also scale ViT-L using parameter sharing to a wider FFN layer. Then, for each token, ViT-L would have comparable computation with WideNet-L setting K as 2. We can see scaling to more trainable parameters and FLOPs cannot improve the performance of ViT-L, which also shows the effectiveness and necessity of our framework. Although ViT-L has a comparable computation cost with WideNet for each token, WideNet still spends more training time per epoch. According to our experiments, there are two reasons, i.e., routing operation and C > 1.0. We leave optimize this as our future work.

## Conclusion

In this paper, we propose to go wider instead of deeper for more efficient and effective parameter deployment. We implement this plug and play framework as WideNet. Especially, WideNet first compresses trainable parameters along with depth by parameter-sharing across transformer blocks. To maximize the modeling ability of each transformer block, we replace the FFN layer with the MoE layer. Then, individual layer normalization provides a more parameter-efficient way to transform semantic representations across blocks. We show that WideNet achieves the best performance by less trainable parameters on both computer vision and natural language processing backbones. In particular, on ImageNet-1K, our best model achieves 80.1 Top-1 accuracy with only 63M parameters, which outperforms ViT and ViT-MoE by a large margin. On four natural language processing datasets, WideNet outperforms ALBERT by a large margin and surpass BERT with less trainable parameters. Also, the investigation shows the reason why MoE cannot scale well on smaller datasets. That is, each expert requires enough tokens to train. Moreover, we verified that individual normalization can transform hidden representations to other domains for more diversified semantics. In summary, we show that there is a great potential of this framework to train more parameter-efficient models.

![Figure 2: Results of scaling the number of experts.]()

![Figure 3: Results of scaling the number of groups.]()

![Figure 4: Divergence of γ with LayerNorm layers.]()

![Results on ImageNet-1K pretraining.]()

![Results of funetuning on GLUE benchmarks Cifar10 in Appendix. Observe that WideNet-H achieves the best performance and significantly outperforms ViT and ViT-MoE models on ImageNet-1K. Compared with the strongest baseline, our WideNet-H outperforms ViT-B by 1.5% with less trainable parameters. Even if we use the smallest model, WideNet-B, it still achieves comparable performance with ViT-L and ViT-MoE-B with over 4× less trainable parameters. When we scale up to WideNet-L, it has surpassed all baselines with half trainable parameters of ViT-B and 0.13× parameters of ViT-L. Another observation is, unlike training MoE based models on huge datasets (e.g., JFT-300M (Sun et al. 2017) and C4 (Raffel et al. 2019)), MoE cannot benefit ViT on ImageNet-1K, which is 200 times smaller than original ViT-MoE used in pretraining 3 .]()

![Results of ablation study on ImageNet-1K to to investigate the contributions of our three key modifications (i.e., Independent Layer Normalization, scaling width with MoE layer and compressing depth with parameter sharing).]()

![Results of ablation study on ImageNet-1K to evaluate our WideNet with comparable speed or computation cost. #Blocks is the number of transformer blocks. FNN dim means the dimension of FFN layer. Para Sharing is whether we shared parameters across transformer blocks. Time denotes to TPUv3 core days.As we set the number of selected experts K as 2 and capacity ratio C as 1.2 in WideNet, there is extra computation cost than vanilla ViT. Therefore, we conduct a second set of ablation studies to evaluate our WideNet with comparable speed or computation cost with the baselines.]()

We will release our code upon acceptance.

For easier using on downstream tasks, we implement our method with only data parallelism.

This dataset is not publicly available.

