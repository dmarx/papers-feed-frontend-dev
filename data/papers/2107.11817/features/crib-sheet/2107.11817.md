- **Key Concept**: Go wider instead of deeper to improve parameter efficiency in transformer models.
  
- **Framework**: WideNet, a parameter-efficient framework that employs parameter sharing and mixture-of-experts (MoE) layers.

- **MoE Layer**: Replaces the feed-forward network (FFN) in transformer blocks, allowing for conditional computation where only a few experts are activated per input.

- **Parameter Sharing**: 
  - Shared MoE layer across transformer blocks to enhance modeling capacity.
  - Individual normalization layers for each block to allow diverse semantic representations.

- **Performance Metrics**:
  - WideNet outperforms Vision Transformer (ViT) by 1.5% with 0.72Ã— trainable parameters on ImageNet-1K.
  - Surpasses ALBERT by 1.8% and BERT with factorized embedding by 0.8% on NLP tasks with fewer parameters.

- **Routing Mechanism**: 
  - Uses TopK() to select the top-ranked experts for each input.
  - Routing function: \( g(x) = TopK(softmax(f(x) + \epsilon)) \), where \( f(x) \) is a linear transformation and \( \epsilon \) is Gaussian noise.

- **Balanced Loading**: 
  - Introduces a buffer capacity \( B \) to limit the number of tokens assigned to each expert.
  - Load balancing loss: \( l_{balance} = E \cdot \sum_{i=1}^{E} m_i \cdot P_i \), where \( m_i \) is the fraction of tokens dispatched to expert \( i \).

- **Layer Normalization**: 
  - Different normalization parameters for each transformer block to enhance representation diversity.
  - Normalization formula: 
    \[
    LayerNorm(x) = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} \cdot \gamma + \beta
    \]
    where \( \gamma \) and \( \beta \) are trainable parameters.

- **Architecture Overview**: 
  - Each transformer block consists of:
    - LayerNorm
    - Multi-Head Attention (MHA)
    - MoE layer
  - Overall structure: 
    \[
    x = LayerNorm_{att}(x) + MHA(x) \\
    x = LayerNorm_{moe}(x) + MoE(x)
    \]

- **Conclusion**: WideNet provides a more parameter-efficient and effective framework for transformer models, enabling better performance with fewer trainable parameters.