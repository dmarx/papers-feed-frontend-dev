<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Processing for the OpenGPT-X Model Family</title>
				<funder ref="#_MbZdwcd">
					<orgName type="full">German Federal Ministry for Economic Affairs and Climate Action (BMWK)</orgName>
				</funder>
				<funder>
					<orgName type="full">Federal Ministry of Education and Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-11">11 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolo</forename><forename type="middle">'</forename><surname>Brandizzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hammam</forename><surname>Abdelwahab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anirban</forename><surname>Bhowmick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lennard</forename><surname>Helmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benny</forename><forename type="middle">Jörg</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pavel</forename><surname>Denisov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qasid</forename><surname>Saleem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Fromm</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Rutmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farzad</forename><surname>Naderi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamad</forename><forename type="middle">Saif</forename><surname>Agy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Schwirjow</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Küch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luzian</forename><surname>Hahn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Malte</forename><surname>Ostendorff</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DFKI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Ortiz Suarez</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DFKI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georg</forename><surname>Rehm</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DFKI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dennis</forename><surname>Wegener</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Flores-Herr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joachim</forename><surname>Köhler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Data Processing for the OpenGPT-X Model Family</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-11">11 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">ADC67472A9233E11348C9A5048F0DCEE</idno>
					<idno type="arXiv">arXiv:2410.08800v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an indepth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been an exponential increase in the development and deployment of large language models (LLMs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>. The most prominent and accurate models are often the products of major technology companies and are available either through APIs (e.g. OpenAI's GPT models) or with non-open licenses (e.g. Meta's LLaMA family <ref type="foot" target="#foot_0">1</ref> ). Moreover, these models are often US-centric, predominantly fluent in American English, and imbued with the values inherent to that cultural context. Conversely, open and unrestricted models that focus on a variety of languages and cultures often emerge from academic settings. However, these models generally lack the accuracy and power of their more robust commercial counterparts. This disparity is primarily due to the differences in Trove <ref type="foot" target="#foot_3">10</ref> , which was released alongside the dataset. The pipeline builds upon the methodology used in RefinedWeb, beginning with text extraction from WARC files using the trafilatura tool. Language filtering is performed with fastText, applying a cutoff score of 0.65 to ensure the inclusion of high-quality English documents. After this, the dataset undergoes both document-wise and linewise heuristic filtering to further enhance quality by removing noise and non-linguistic content. The final deduplication step employs the MinHash algorithm, ensuring a clean and diverse dataset that is well-suited for large-scale language model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multilingual datasets</head><p>Creating high-quality multilingual datasets presents distinct challenges compared to monolingual English datasets. Language identification becomes significantly more complex, as models must accurately differentiate between multiple of languages, often with limited training data for lowerresource languages. Additionally, ensuring consistent quality across languages is difficult due to varying amounts of available web data, and some languages are overrepresented while others suffer from a lack of resources. High quality multilingual datasets are essential for training models that perform well across diverse linguistic and cultural contexts, enabling the training of LLMs that are not inherently biased toward dominant languages like English.</p><p>CCNet <ref type="bibr" target="#b47">[47]</ref> is among the first datasets built on CommonCrawl data, containing 3.2TB (1.5 billion documents) in 130 languages. The CCNet pipeline begins by extracting paragraphs from WET files and performing SHA-1-based deduplication. The extracted text is normalized by converting it to lowercase, replacing all numbers with zeros, and removing accents and punctuation. Language detection is based on fastText, with documents with a language score below 0.5 being discarded. The final step in the pipeline involves filtering documents based on perplexity scores calculated with language-specific 5-gram Kneser-Ney language models <ref type="bibr" target="#b19">[20]</ref>. The corresponding models are trained on tokenized Wikipedia data, using the sentence piece tokenizer <ref type="bibr" target="#b28">[29]</ref> to ensure consistent text segmentation.</p><p>Multilingual C4 (mC4) <ref type="bibr" target="#b48">[48]</ref> extends the C4 dataset to 101 languages and contains approximately 27TB of text. In contrast to C4, which focuses on English, mC4 replaces the language classification tool with cld3 <ref type="foot" target="#foot_4">11</ref> to handle the identification of multiple languages. A significant modification in mC4 is the introduction of a filtering step that requires documents to contain at least three lines of text, each with 200 or more characters. This broader filter replaces C4's original language-specific rule, which filtered based on English punctuation marks, ensuring a more consistent approach across languages and improving the quality of multilingual data.</p><p>OSCAR 22.01 <ref type="bibr" target="#b1">[2]</ref> contains about 8TB of text in 153 languages. It is produced with the Ungoliant pipeline <ref type="bibr" target="#b0">[1]</ref> from the CommonCrawl WET (Web Extracted Text) files. The filtration starts with language detection, which utilizes the fastText model to identify language of each line in a document. If the confidence score is less then 0.8, then the line is assigned with to the unknown language class. The proportion of each language in a document is calculated as a percentage of bytes in the document assigned with that language. A document is classified as multilingual if it contains at least 5 lines, not more than 5 languages, the proportion of each language is at least 1/(m + 1) (where m is the number of languages in the document) and the proportion of the unknown language is not larger than 1/(m + 1). Otherwise, the document is classified as monolingual and is assigned with the language having the highest weighted confidence score. The weighted confidence of a language is calculated as a sum of products of byte size and language confidence of each line classified to that language divided by the total number of bytes in the document. A monolingual document is passed to further processing steps if the weighted confidence of its language is at least 0.6. Language identification is followed by the filtering of documents based on the line lengths, proportions of characters of certain Unicode classes, and the URL based UT1 blocklist. <ref type="foot" target="#foot_5">12</ref> Deduplication is not applied to non-English data, and only line-wise deduplication is applied to English data.</p><p>BigScience ROOTS <ref type="bibr" target="#b31">[32]</ref> is a dataset containing about 1.6TB of text in 46 natural languages. This dataset comprises data from the crowdsourced list of 252 monolingual and multilingual text collections, accompanied by documents extracted from the CommonCrawl WARC files according to the list of domains suggested by the community members. Extraction of text from HTML files is performed by custom code inspired by the CommonCrawl extractor. Deduplication is performed on the level of data sources. Additional data is obtained from the OSCAR version 21.09 dataset <ref type="bibr" target="#b37">[38]</ref>. The OSCAR derived documents are passed through heuristic rules based on word frequencies and are deduplicated in two steps using SimHash <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> and substring deduplication <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>The MADLAD-400 <ref type="bibr" target="#b29">[30]</ref> dataset contains about 30TB of text in 419 languages. The data is obtained from the CommonCrawl dumps and is initially deduplicated on the line level, followed by basic prefiltering similar to the C4 rules. The semi-supervised language identification model <ref type="bibr" target="#b8">[9]</ref>, which is trained with two tasks (supervised language detection task and unsupervised corrupted input recovery task <ref type="bibr" target="#b41">[42]</ref>), is used to classify the documents after that. The language identification step is followed by another set of quality filtering heuristics. A few language-specific processing and filtering rules are applied based on the manual inspection of 20 documents per language.</p><p>RedPajama-v2<ref type="foot" target="#foot_6">foot_6</ref> contains about 20 billion documents and 30 trillion tokens in 5 European languages. It is produced with the extended CCNet pipeline and utilizes both heuristic based and classifier based content filtering. The deduplication is performed on the document level using a Bloom filter.</p><p>The HPLT <ref type="bibr" target="#b11">[12]</ref> dataset contains 50.1TB of text in 75 languages. The data is sourced from the Internet Archive and CommonCrawl dumps in WARC format. First, text is extracted using the warc2text tool<ref type="foot" target="#foot_7">foot_7</ref> and classified by language using the CLD2 library <ref type="foot" target="#foot_8">15</ref> . Next, text is cleaned from encoding errors, and another two-stage paragraph-level LID is performed. Its first stage utilizes the fastText model, and the second stage applies dictionary based spell checking for several languages related to the fastText result and selects the language, for which fewer mistakes are detected <ref type="bibr" target="#b4">[5]</ref>. Finally, MinHash based deduplication is performed on the document level.</p><p>The collection of data we processed comprises data from both web and curated sources, including parts of The Pile and RedPajama-v2, and the content and metadata are normalized in a uniform way by the data processing pipeline described in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data selection</head><p>The performance of large language models in downstream tasks benefits from large volumes of diverse, high-quality training data. Key properties for effective pretraining include the diversity of knowledge, domains, and tasks, which enhance a model's generalization abilities <ref type="bibr" target="#b13">[14]</ref>, as well as the appropriate size to fully saturate the model's learning capacity <ref type="bibr" target="#b20">[21]</ref>. To mitigate cultural bias from monolingual English datasets, we emphasize the selection of non-English data. Specifically, we aim to balance our data to encompass all European languages. This section outlines the data selection requirements that ensure these desirable properties and provides an overview of the chosen data sources.</p><p>Given the aforementioned properties, our approach to data selection involves ensuring a large volume <ref type="foot" target="#foot_9">16</ref> of diverse and high-quality processed data for LLMs training.</p><p>Data Quantity and Diversity. A significant portion of our data is derived from crawled web pages, complemented with curated datasets to ensure a broad spectrum of languages, genres, text types, and domains. These curated datasets are carefully selected to enhance performance in various downstream tasks such as question answering, machine translation, and summarization.</p><p>Data Quality. The quality of large language models is highly dependent on the textual training data, often measured in terms of the number of tokens or words. High-quality data should be multilingual (covering as many relevant languages as possible), diverse (sourced from multiple domains and document types), free of toxic or offensive content (to prevent harmful outputs), and unbiased (to ensure fair model behavior). In this context, data quality directly reflects the properties of the text itself. For a detailed discussion of our filtering processes, see Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Selected data</head><p>Following these requirements, our data sources can be categorized into two groups. The first group consists of curated data from multiple existing datasets, providing a relatively small but diverse set of high-quality knowledge sources. The second group comprises general web data sourced from the CommonCrawl project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Curated data</head><p>Curated datasets are essential for providing high-quality, diverse training data for our language models. These datasets have typically undergone a quality review process, either prior to their release or through ongoing public review, and are publicly available (see Appendix 8.3 for a list of our curated datasets). This includes data such as Wikipedia, collections of scientific articles, books, and source code.</p><p>To select appropriate datasets, we employed the following considerations:</p><p>1. Legal and Licensing</p><p>• Is the license unproblematic? (e.g., allows research and commercial use, is permissive, and does not include restrictive copyleft clauses).</p><p>• Is the data GDPR-compliant?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Linguistic and Relevance</head><p>• Does the dataset contain documents in a language relevant to our research?</p><p>• Does the dataset contain a sufficient quantity of tokens/words/documents?</p><p>• Do the documents cover relevant topics? (e.g. for downstream applications)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Quality and Integrity</head><p>• Is the content of documents error free? (e.g., no artifacts from Optical Character Recognition, misspellings, or antiquated language)</p><p>• Are documents from a geographical region of interest?</p><p>• Are the documents from a relevant time period? (e.g. no historic documents)</p><p>• Are the documents human-generated<ref type="foot" target="#foot_10">foot_10</ref> ?</p><p>• Is the dataset artificial? (e.g. no automatically generated content or synthetic data)</p><p>• Is there no significant overlap with other datasets already in use?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Resource Availability</head><p>• Are there sufficient resources available to preprocess the data? (e.g., adequate disk space and computational resources)</p><p>• Has the dataset not been fully or partially superseded by a newer version? (e.g. no outdated version)</p><p>If one or more of these questions were answered negatively, the dataset was excluded from further processing. These criteria ensure that the curated datasets selected for our project are of the highest quality, relevance, and suited for our specific objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">CommonCrawl data</head><p>Unlike curated data, web data is available in a large amount. For this reason, we prioritize processing web data only based on recency and language. Specifically, we use data from CommonCrawl (CC), an organization that maintains a free, open-source repository of web data accessible to anyone since they provide the largest amount of data. They perform monthly crawls of internet webpages, compiling them into data dumps labeled in the format YYYY-WW (e.g., 2024-22, see Section 5.2 for a list of selected dumps). CommonCrawl dumps can be downloaded from URLs <ref type="foot" target="#foot_11">18</ref> in two main formats: WARC and WET. WARC (Web ARChive format) files contain raw data from the crawl, including the full page HTML and request metadata. WET (WARC Encapsulated Text) files provide a text-only version of the websites, excluding the web metadata. In our pipeline, we use the WET files exclusively as they provide the extracted textual content of web pages in a simplified format. This eliminates the need for extensive parsing of raw HTML data found in WARC files, allowing us to focus directly on text analysis. For instance, the CC dump "2024-22" includes data crawled from 2.5 billion web pages, totaling 18.35 TiB of compressed HTML data. This particular dump consists of 90,000 WET files, which are grouped into segments for processing. This largescale web data significantly expands the scope of our collection, providing a robust foundation for pretraining large language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data pipelines</head><p>This section describes the data pipelines we used for processing both curated and web data. Given the distinct nature and processing requirements of these two types of data, the pipelines are described in separate subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pipeline for curated data</head><p>Curated datasets are published in a variety of formats and metadata. In contrast to web crawled data, these datasets consist of thematically related and high-quality texts, due to the previous curation process (e.g. reviewing for scientific articles). Therefore, a quality-based filter is less necessary and the preprocessing is based the pipeline in Figure <ref type="figure" target="#fig_0">1</ref>. Format Conversion. The first step in processing curated data involves converting the raw data into JSONL format. Depending on the raw data's format, we employ different processing tools. For PDF documents, Grobid<ref type="foot" target="#foot_12">foot_12</ref> converts PDFs to XML/TEI as an intermediate format which we then convert to JSONL to identify and remove layout elements from the content. For Wikimedia datasets (e.g., Wikipedia, Wikibooks), wikiConverter parses the Wikimedia XML dumps and converts them to JSONL. The data is first converted using mwparserfromhell<ref type="foot" target="#foot_13">foot_13</ref> , then list pages and disambiguation pages are removed, and finally the remaining data is processed as with other curated datasets. For any other format (e.g. plain text, csv, sql dumps, etc), we created custom conversion tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Normalization</head><p>Normalizing the content is important to ensure that the rest of the pipeline consistently handles data, facilitating versioning, processing, and sharing. We implement two types of normalization: content normalization and metadata normalization.</p><p>Content Normalization. Content normalization ensures that the text data is uniformly formatted and encoded. (i) All text data is encoded in UTF-8 to maintain a standard character encoding format. (ii) We apply NFKC normalization <ref type="foot" target="#foot_14">21</ref> to the text content, converting characters to their canonical decomposition followed by compatibility composition to ensure that visually similar characters are represented consistently. (iii) Excessive white space is trimmed, and inconsistent spacing is corrected. (iv) Finally, we remove common conversion artefacts such as HTML tags or special characters to ensure the text is free from noise.</p><p>Metadata Normalization. Metadata normalization is important to maintain a consistent set of essential metadata attributes that provide a snapshot of each document. This metadata forms the basis for subsequent analysis and filtering steps. The format for a single document (a line in a JSONL file) is as follows:</p><p>{ "meta": { "docid": &lt;corpus/language/fileno/docno&gt;, "url": &lt;url:String&gt;, "title": &lt;title:String&gt;, "download_date": &lt;ISO-date:String&gt;", "language": &lt;ISO-language-code:String&gt;, "language_score": &lt;language-detection-score:Float&gt;}, "text": &lt;text:String&gt; }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Where</head><p>• meta contains all document metadata.</p><p>• docid is a unique ID comprised of the corpus name, the number of the file a document originates from, and the running document number within this file.</p><p>• url is a string representing the URL of a document (where appropriate).</p><p>• title is the title of a document (where appropriate).</p><p>• download_date is the download date in YYYY-MM-DD format.</p><p>• language is a 2-or 3-letter ISO language code <ref type="bibr" target="#b22">[23]</ref> for the language of the content.</p><p>• language_score is a score estimated from language detection <ref type="bibr" target="#b26">[27]</ref> (1.0 where the language is not ambiguous); "xx" is used for non-language content such as source code.</p><p>• text is the normalized document content.</p><p>For an example document, see Appendix 8.2. Note that the example for the document format already includes information from language detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Language Detection</head><p>Following content normalization, language identification is performed using fasttext <ref type="bibr" target="#b26">[27]</ref>. The detected language code is included both in the metadata field and as part of the filename for the pipeline output to enable grouping of data by language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Filtering</head><p>To ensure appropriate data quality, we aim to remove low-quality documents as early as possible in the processing pipeline. We apply a set of filters to remove some outliers:</p><p>• Documents with a language score lower than 0.5 are filtered out.</p><p>• Documents that contain fewer than 200 characters are removed.</p><p>These thresholds (200 characters, 0.5 language score) were chosen based on settings used in previous projects (e.g., HPLT <ref type="foot" target="#foot_15">22</ref> ) and are designed to ensure that only relevant and high-quality data proceeds to the subsequent stages of processing.</p><p>The final output of the curated data pipeline is a set of JSONL files, organized by language and corpus, ready for further analysis and model training. Processing web data from large-scale sources like CommonCrawl presents significant challenges due to its unstructured and noisy nature. To transform this raw web data into a suitable format for training LLMs, we utilized a specialized pipeline (see Figure <ref type="figure" target="#fig_1">2</ref>). This section provides a detailed breakdown of each processing stage, including normalization, filtering, and deduplication, using the Ungoliant pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dataset Acquisition and Preparation</head><p>The process begins by downloading CommonCrawl dumps in WET file format using a custom download script. These dumps contain the textual content and metadata of web pages, compressed in .txt.gzip format. After retrieving the raw data, we organized it into separate folders based on the dump year for easier processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ungoliant Pipeline</head><p>The core of the web data processing relies on the Ungoliant pipeline <ref type="bibr" target="#b0">[1]</ref>, a modular system optimized for handling CommonCrawl corpora and producing an OSCAR-like dataset <ref type="bibr" target="#b1">[2]</ref>. The Ungoliant pipeline is conceptually split into several key components:</p><p>• Normalization: Similar to the normalization applied to curated data (see Section 4.1), this step ensures consistency in text encoding, removing noise, normalizing text formatting, and encoding all content into UTF-8.</p><p>• Computation of quality warnings: Ungoliant generates quality warnings for each document which are then used for subsequent filtering stages.</p><p>• Computation of harmful-perplexity: Harmful content is identified using perplexity scores based on a pretrained KenLM model <ref type="bibr" target="#b23">[24]</ref>. This model evaluates documents to determine whether they contain harmful content (e.g., adult material).</p><p>• Language detection: Sentence-based language identification is performed using embedded <ref type="foot" target="#foot_16">23</ref>pretrained fastText models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>• Prefiltering: Documents are filtered based on criteria similar to those used for curated data (see Section 4.1.3), such as removing documents with a low number of characters or low language detection scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Filtering</head><p>After the documents pass through the Ungoliant pipeline, we apply additional filtering steps based on quality warnings and harmful-perplexity scores to further refine the dataset. Quality Warning filtering. Ungoliant flags documents with several quality warnings: tiny, noisy, header, footer, and short_sentences. These warnings are accompanied by predefined thresholds, and documents that exceed these thresholds are filtered out. The thresholds are defined as follows:</p><p>• tiny: When the document contains fewer than 5 sentences (lines).</p><p>• noisy: When the ratio of non-letters to the total number of characters exceeds 50%.</p><p>• header: Defined through a three-step process:</p><p>-Iterate through the first 20% of a document <ref type="foot" target="#foot_17">24</ref> .</p><p>-Identify short sentences (lines) with fewer than 100 characters.</p><p>-Annotate the document as header if more than 50% of the sentences in this section are short.</p><p>• footer: Similar to the header but applied in reverse order to the above one.</p><p>• short_sentences: When the document has a high number (≥ 50%) of lines with less than 100 characters.</p><p>Harmful-Perplexity Filtering. Filtering adult content is a critical part of ensuring that the models are trained on safe and appropriate data. We use the perplexity scores provided by the KenLM-based model trained on adult content from the UT1 Blacklist <ref type="bibr" target="#b23">[24]</ref> as part of the Ungoliant pipeline. Lower perplexity scores indicate a higher likelihood that the document contains harmful or adult content. We set a perplexity threshold of 5, meaning any document with a perplexity score below this threshold is filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Document deduplication</head><p>Building on the quality filtering performed in the previous stage, the next step after running the Ungoliant pipeline is document deduplication. This process refines the dataset by removing both exact and near duplicate documents, thus reducing the overall size of the data and improving its quality for LLM training. After extensive testing <ref type="bibr" target="#b33">[34]</ref>, we found that the MinHash/LSH algorithm offers the best balance of precision, recall, and computational efficiency for deduplicating large volumes of web data. The deduplication process was executed on a per-dump and per-language basis ("local" deduplication), which is both convenient and backed by newer research <ref type="bibr" target="#b38">[39]</ref> that confirms that local deduplication is more favorable than global (across dump and/or language) approaches for preserving data diversity while minimizing redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Technical Infrastructure</head><p>To effectively manage the extensive data processing requirements, we leveraged significant computational resources. The processing of large-scale datasets, including both curated datasets and CommonCrawl data, required the use of high-performance computing infrastructure.</p><p>Initially, a NVIDIA DGX-2 machine was employed for downloading and processing the curated datasets, as well as for deduplicating the first CommonCrawl dumps. As the volume of data increased, the processing of the remaining CommonCrawl datasets was distributed across two large compute clusters.</p><p>The technical specifications of these compute clusters are as follows:</p><p>• Compute Cluster 1: Provided by TU Dresden, consisting of compute nodes with 2 x Intel Xeon Platinum 8470 (52 cores) @ 2.00 GHz CPU and 512 GB RAM.</p><p>• Compute Cluster 2: Provided by FZ Jülich, consisting of compute nodes with 2 x Intel Xeon Platinum 8168 (2x24 cores) @ 2.7 GHz CPU and 192 GB RAM.</p><p>These resources enabled us to handle the extensive data processing requirements necessary for pretraining our large language models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data Analysis</head><p>This section provides a detailed analysis of the datasets used in our project, focusing on both curated and web data sources. The goal is to offer a comprehensive overview of the data characteristics.</p><p>We begin by presenting statistics for the curated datasets (Section 5.1), including details on corpus names, languages, text types, and other relevant metadata. We then analyze the web data (Section 5.2), examining the size, composition, and the effects of filtering and deduplication. This includes a breakdown of the number of documents, word counts, and language distributions.</p><p>It is important to note how LLM training often uses sampling techniques to optimize data distribution, considering factors like language balance and the fertility score <ref type="bibr" target="#b3">[4]</ref>. In this paper, we focus solely on the raw data collection, filtering, and integration process, without adjusting distributions for training purposes. For further information on a model trained on part of the data we process, refer to <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Curated Data</head><p>As previously mentioned, curated data undergoes a distinct processing pipeline. This is due to the assumption that curated datasets generally maintain a higher level of quality, thereby requiring less intensive filtering and deduplication compared to web data. A comprehensive list of the curated datasets used in our study is provided in Table <ref type="table">4</ref>, which details 75 datasets, including information on language, format, license, domain, and the number of documents/words, along with the filtering percentage. In the subsequent sections, we will conduct a detailed analysis of the most significant columns.</p><p>37.9% 23.9% 26.2% EN 37.9% EU24 26.2% XX 23.9% DE 5.2% IT 2.4% (a) Complete word distribution 46.2% 5.9% 9.2% 10.8% 12.3% DE 46.2% NO 12.3% IT 10.8% SVK 9.2% ES 5.9% SL 4.2% DA 3.9% (b) Word distribution excluding EN, EU24, and XX. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Licenses</head><p>As outlined in the data selection process (see Section 3.1.1), selecting datasets according to legal and licensing constraints is a crucial first step. Figure <ref type="figure" target="#fig_2">3</ref> illustrates the distribution of licenses across the curated datasets used in this project. The majority (52%) of the selected datasets' licenses are permissive and not restricted for commercial use, featuring licenses like CC0 (24%), CC BY (18.7%), MIT (5.3%), and Apache (4%) <ref type="foot" target="#foot_18">25</ref> . These licenses enable broad usage of the data, including for commercial purposes, and promote ease of dissemination and integration in various projects.</p><p>Conversely, 44% of the datasets are categorized under restrictive licenses, which limit their use, especially for commercial applications. These include licenses such as CC BY-NC-SA (6.7%), CC BY-NC (4%), and the "Various" category (14.7%), which represents datasets compiled from multiple sources and classified under the most restrictive license to mitigate legal risks.</p><p>An important observation is that many datasets did not explicitly mention a license on their official website. For these datasets, we conducted further investigation into the sources, often uncovering related licensing information. However, for some datasets (marked with a dagger in Table <ref type="table">4</ref>), we were unable to directly determine the correct licenses despite extensive efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Languages</head><p>Our curated dataset encompasses 24 unique languages. For clarity, datasets that include a substantial portion of the official 24 EU languages are grouped under the category "EU24". Additionally, the analysis also features statistics for other European languages, such as Norwegian (NO). The label "XX" is used to indicate datasets focused on source code.</p><p>As shown in Figure <ref type="figure" target="#fig_3">4a</ref>, English (EN) accounts for 37.9% of the total word count, followed by the EU24 languages at 26.2%, and source code (XX) at 23.9%. Together, these three categories make up 88% of the curated data. The inclusion of source code is particularly important in this context, as it provides curated, high-quality examples for training models on coding tasks.</p><p>Since many datasets feature multiple languages, specific per-language statistics are not always provided, but rather an overall distribution is emphasized. After removing the dominant categories (EN, EU24, and XX), Figure <ref type="figure" target="#fig_3">4b</ref> reveals that German (DE) constitutes 46.2% of the remaining data. This significant proportion reflects the regional focus of the project, as it is based in Germany, and emphasizes the role of localized datasets in shaping the overall corpus. Other notable languages include Norwegian, Italian, and Spanish, though their contributions remain relatively smaller compared to German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Word Distribution and Dataset Representation in Multilingual Corpora</head><p>One would expect the word distribution to follow the number of datasets available; however, correlation analysis reveals this is only partially true. The Spearman correlation is moderate (0.562, p = 0.0028), while the Pearson correlation indicates a stronger linear relationship (0.855, P &lt; 0.001). By using linear regression, we observe notable deviations in certain languages. For instance, German (DE) exhibits a significant negative residual, with the actual word count being 5.1 billion fewer than expected, despite having a high number of diverse datasets. This suggests that while German datasets are numerous, they tend to be smaller in size or less word-dense compared to other languages. On the other hand, English (EN) has a positive residual, showing 3.88 billion more words than predicted, which can be attributed to the oversampling of English datasets in the corpus. In contrast, French (FR) underperforms relative to its dataset count, with a negative residual of 1.54 billion, indicating that the French data is underrepresented in terms of word volume.</p><p>This analysis underscores the need for balanced sampling across languages. While oversampling English may provide more content for model training, undersampling key languages like French could result in biases that limit the multilingual capabilities of the models. Hence, we recommend strategic oversampling of underrepresented languages and careful moderation of overrepresented languages to ensure linguistic diversity and fairness in the curated dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Domains</head><p>The curated dataset spans multiple domains, each contributing a different share to the total word count. Table <ref type="table" target="#tab_1">1</ref> provides a breakdown of the number of datasets, the average word count per dataset, total word count, and the percentage contribution of each domain.</p><p>Source code emerges as the largest domain by word count, accounting for 40.46% of the total curated data, with over 73 billion words. This is followed by law and administration, which contributes 20.40%, and the web domain at 13.69%. Collectively, these three domains represent 74.55% of the total word count. This strong presence of technical, legal, and digital content suggests that the curated dataset is well-suited for training models focused on tasks related to programming, legal reasoning, and web-based applications.</p><p>On the other hand, smaller domains such as culture (0.08%), recreation (0.08%), and knowledge base (0.23%) contribute much less to the overall dataset. These domains are likely underrepresented either due to the limited availability of datasets or their inherently smaller size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Sizes</head><p>The distribution of word counts within the curated dataset is highly skewed, with a few large datasets contributing the majority of the total word volume as shown in Figure <ref type="figure" target="#fig_4">5</ref>. Initially, the largest contributors were StarCoder, EurLex, and MaCoCu, together accounting for 50% of the total word count. However, since StarCoder focuses on source code rather than natural language, it was excluded from further analysis, and the word count distribution was recalculated.</p><p>In the revised analysis, four datasets (peS2o, MaCoCu, Legal MC4, and EurLex ) now make up 50% of the total word count. Expanding this to 70%, the dataset contributions broaden to include three additional Pile subsets (PMC extracts, Openwebtext2, and Free Law Opinions V2 ), as well as Wikimedia Wikipedia. Notably, 17 datasets account for 90% of the total word count, underscoring the significant concentration of data within a limited number of large datasets.</p><p>Additionally, certain datasets display an unusually high word count relative to their document counts. A regression analysis was performed to better understand these discrepancies, calculating residuals to detect significant deviations from the expected document-to-word ratio. This analysis revealed the following outliers:</p><p>• Spanish legal corpora (ES): This legal corpus consists of only 15 documents but contains over 1.38 billion words. The residual analysis <ref type="bibr">(5.4)</ref> shows that the word count is approximately 221 times higher than expected based on the number of documents. This suggests that individual documents within this corpus are unusually large.</p><p>• Projekt Gutenberg (EU24): A well-known collection of books, Projekt Gutenberg contributes over 3.37 billion words with only 60,912 documents. This results in a positive residual of 1.8, indicating that the dataset's word count per document is significantly higher than the regression model predicted. This observation aligns with expectations for book collections, as books typically contain more words per document compared to other formats such as articles or reports.</p><p>• Pile: PMC extracts (EN): A medical dataset with 2.8 million documents and over 12.1 billion words, PMC extracts exhibits a positive residual of 1.07. This indicates that the word count is significantly higher than predicted by the model. This is expected, as the dataset primarily contains full-length medical research articles, which are generally more detailed and contentrich compared to other document types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Filtering</head><p>Across the curated datasets, the average filtering percentage is 5.33% (±5.98%). Based on these values, filtering can be split in three categories:</p><p>• Low Filtering (&lt; 1%): Datasets that underwent minimal filtering, typically because they contained well-structured, clean data to begin with.</p><p>• Medium Filtering (1% to 11.31%): The majority of datasets fall into this category, with moderate filtering applied to remove noise or short documents without heavily impacting the total word count.</p><p>• High Filtering (&gt;11.31%): Datasets with a high filtering percentage, indicating substantial noise or irrelevant content.</p><p>In reviewing the impact of filtering, we found that documents with low language scores typically fall into one of four categories: i) mixed-language documents, ii) documents with conversion errors containing numerous special characters, iii) documents involving closely related languages (e.g., Croatian and Serbo-Croatian), and iv) documents in low-resourced languages where the content was misidentified as a different language.</p><p>To further understand the impact of filtering across datasets, we explored potential correlations between the percentage of filtered data and several key dataset characteristics: the number of words, number of documents, format, language, and domain. Our analysis showed only modest correlations in most cases, indicating that filtering tends to act somewhat independently of these factors (refer to Appendix 8.3.1 for a more detailed analysis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.6">Summary</head><p>The curated dataset presents a comprehensive and diverse collection of 75 datasets spanning 25 languages, various domains, and multiple formats, offering a robust foundation for a wide range of research tasks. Despite some imbalances in distribution, particularly with large datasets dominating the total word count, the variety within the dataset makes it a valuable asset for training LLMs. The high presence of technical and legal data, alongside source code, reflects the strengths of this dataset in supporting tasks in these fields.</p><p>While filtering in general is necessary to ensure data quality, its impact on the curated datasets is relatively moderate, with only a few datasets requiring significant cleaning. The weak correlations found between filtering percentages and dataset characteristics suggest that filtering is driven more by the specificities of the dataset than by its size, format, or domain. In preparing data for training LLMs, we found that a strategic approach to selecting subsets is key. Leveraging the wide domain and language diversity can help ensure more balanced outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Web Data</head><p>As previously mentioned (Section 3.1.2), the web data used in this project was sourced from Com-monCrawl. For this study, we utilized 60 distinct dumps from CommonCrawl, spanning a broad timeframe. The earliest dump was week <ref type="bibr">42 of 2014 (2014-42)</ref>, and the most recent dump was week 5 of 2023 . This extensive range allowed us to capture a wide variety of web content across nearly a decade, ensuring that the dataset reflects both historical and contemporary web information.</p><p>On average, each CommonCrawl dump contains approximately 2.7 billion documents. Across all 60 dumps, we accumulated a total of 173 billion documents, representing around 703 terabytes of raw, unprocessed data. This vast volume of data provided a strong foundation for training largescale language models but also introduced significant challenges in terms of data processing and filtering, as discussed in previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Year Week</head><p>2014 42 <ref type="bibr">2015 14, 48 2016 22, 44 2017 13, 47, 51 2018 5, 9, 13, 17, 22, 26, 30, 34, 39, 43, 47, 51 2019 4, 9, 13, 18, 22, 26, 30, 35, 39, 47, 51 2020 5, 10, 16, 24, 29, 34, 40, 45,</ref>  Table 2  presents the list of years and the corresponding weeks for each dump.</p><p>As can be seen from the list, the distribution of dumps across weeks is uneven. Earlier years, particularly 2014 through 2016, have fewer weeks represented compared to more recent years. However, the dumps from these earlier years typically contain a higher dump density, i.e. average document size (see Figure <ref type="figure" target="#fig_5">6</ref>). After 2017, CommonCrawl transitioned to a strategy of producing more frequent dumps, each with relatively smaller average document sizes.</p><p>This shift in strategy has important implications for data processing. Special attention must be paid to the earlier years <ref type="bibr">(2014)</ref><ref type="bibr">(2015)</ref><ref type="bibr">(2016)</ref>, where deduplication can be more computationally expensive due to the larger document sizes. In these cases, employing stricter filtering mechanisms may be necessary to ensure efficient processing and to avoid excessive resource consumption during deduplication.</p><p>Compute power Tracking the compute power required during data processing is essential for planning and securing the necessary resources. A well-structured plan not only ensures that adequate resources are available but also provides an opportunity to analyze computational efficiency and sustainability.</p><p>Below, we estimated total CPU hours consumed at each stage of the pipeline:</p><p>• Conversion: On average, converting a dump takes 115,2 CPU hours. For all the data combined this stage took 6,912 CPU hours.</p><p>• Filtering: On average, filtering one dump takes 763 CPU hours. For all dumps combined, this stage required 45,810 CPU hours.</p><p>• Deduplication: Deduplication is notably resource-intensive, taking an average of 3,680 CPU hours per dump. For the entire dataset, this stage consumed 221,230 CPU hours. Deduplication is by far the most time-consuming step, consuming 80.8% of the total compute power, which is why it is kept as the final stage in our pipeline (see Section 4.2.4). This is followed by filtering, which takes 16.7%, and conversion, accounting for 2.5%. Deduplication Effect Our deduplication methodology is based on a MinHash + Locality Sensitive Hashing approach that is explained in detail in <ref type="bibr" target="#b33">[34]</ref>, where the choice of algorithm, along with precision and recall metrics, are analyzed. In this section, we shift focus to examining how deduplication disproportionately affects different languages. Figure <ref type="figure" target="#fig_6">7</ref> illustrates the percentage of data removed after deduplication for each language, ranging from a minimum of 29.03% for Hungarian to a maximum of 68.95% for English. While it may seem intuitive to correlate the deduplication percentage with data availability, a Pearson correlation analysis reveals only a modest positive correlation of 0.54 (p = 0.006).</p><p>This suggests that some languages may be subject to an "unfair" amount of deduplication relative to their size. Although deduplication operates by identifying and removing similar content, this implies that certain languages contain a higher proportion of similar data, which may result in a final model that struggles to capture linguistic nuances.</p><p>To evaluate this disproportionality, we introduce the Deduplication Disparity Index (DDI), which measures the impact of deduplication on each language relative to its available web data. For each language l, the DDI is calculated as the Z-score of the deduplication-to-data ratio R l , which measures the percentage of data deduplicated in relation to the total web data available for that language:</p><formula xml:id="formula_0">DDI l = R l -µ R σ R</formula><p>, where R l = d l W l Here, d l is the deduplication percentage for language l, W l is the total number of web data words for language l, µ R is the average deduplication-to-data ratio across all languages, and σ R is the standard deviation of the deduplication-to-data ratios. This Z-score transformation centers the DDI around zero, highlighting languages where deduplication has a disproportionately high or low impact.</p><p>Languages with a high positive Z-score, such as Maltese (DDI mt = 4.519) and Croatian (DDI mt = 1.0735), are significantly impacted by deduplication, while others have scores closer to zero (DDI = -0.25 ± 0.012), indicating a more balanced deduplication-to-data ratio. This metric provides a systematic way to assess fairness in the data processing pipeline and highlights languages that might require further attention during preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Summary</head><p>The web data for this project was sourced from 60 distinct CommonCrawl dumps spanning 2014 2023, resulting in a total of 173 billion documents (703 terabytes of raw data). This broad timeframe captures both historical and contemporary web content.</p><p>Deduplication was the most resource-intensive step, consuming 80.8% of total CPU hours allocated for data processing. We introduced the Deduplication Disparity Index (DDI) to identify languages disproportionately affected by deduplication, highlighting the need for tailored processing strategies to ensure fairness and data quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Insights and Observations</head><p>During the process of preparing the data for LLM training, several challenges emerged that shaped the development of the data pipeline. These challenges are important for understanding both the complexity of large-scale multilingual dataset processing and the technical, organizational, and legal obstacles one can encounter. By presenting these observations, we provide key findings and recommendations to guide future projects and to navigate similar challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Quality, Availability and Management</head><p>Data quality and availability are key criteria in the construction of datasets for language models. Ensuring high-quality data is essential for model performance, while the availability of data across different languages, genres, and domains presents unique challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Assessment</head><p>The definition of high-quality data remains a complex and evolving topic in the field of LLMs, which has gained more traction as these models scale and diversify across languages and domains. Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">46]</ref>.</p><p>Traditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">49]</ref>. One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>However, this approach comes with several drawbacks. First, despite the promising results, a significant limitation is the lack of transparency in the LLM's mapping between defined criteria and system output in defining high-quality content. There is often no clear link between the model's concept of quality and concrete linguistic characteristics, such as sentence structure, readability, or word length. Additionally, running LLM inference on large datasets substantially increases computational demands, making this approach accessible only to those with significant resources. Finally, using LLMs to filter or label data introduce licensing challenges, particularly for commercial applications. We therefore recommend prioritizing more interpretable quality signals in data filtering.</p><p>These signals are directly mappable to tangible document characteristics, making them transparent and easy to justify. While LLM-based filtering techniques show promise, we believe there is a strong need for further research into how these models define high-quality data. At the very least, a hybrid approach combining both traditional and AI-based filtering could balance between efficiency, interpretability, and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of Multilingual Data</head><p>As seen in Section 5, one of the primary challenges is the significant imbalance in data availability across languages. This disparity makes it difficult to create training data distributions that ensure downstream models perform well in a wide variety of languages. High-resource languages like English dominate the dataset, while low-resource languages, such as Irish or Maltese, have far fewer available documents, thus requiring special consideration such as avoiding filtering and/or deduplicating to not further reduce the number of tokens available for training in these languages.</p><p>Versioning and Standardization Data versioning plays a crucial role in maintaining a clear record of changes and ensuring reproducibility throughout the pipeline. To facilitate reproducibility, we opted for a unified format (JSONL) across all datasets which makes it easier to track changes and compare versions, as any modifications can be identified and versioning information is represented consistently across changes. Furthermore, each step of our pipeline saves intermediate results, allowing for a granular approach to version control.</p><p>In addition to this, the intermediate results were moved to remote servers, separate from the training environment, to safeguard against data corruption or loss. While this precaution provides an extra layer of security, it does come at the cost of increased storage usage, which needs to be carefully managed. Nonetheless, data versioning supports tracking changes to the pipeline by storing the corresponding pipeline version in the metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Licensing and Compliance</head><p>A significant issue is the lack of clear licensing documentation. Researchers sometimes neglect to accurately represent the licenses under which their data falls, leaving legal ambiguities. While license identification remains a complex challenge <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b17">18]</ref>, we strongly advocate for more precise legal reporting in dataset publications. Initiatives like the Dataset Cards by Hugging Face<ref type="foot" target="#foot_19">foot_19</ref> are a positive step toward improving licensing transparency. They allow dataset creators to include clear license information by default, promoting better legal clarity. However, it is important to note that adding a license in these dataset cards is not mandatory, which means some datasets may still be published without explicit legal documentation.</p><p>Even when data is available, special care must be taken to comply with the licenses under which it is provided. License compliance is crucial from the outset, as the licensing terms of any single dataset can impose restrictions on the downstream use of merged datasets or models trained on that data. Selecting even one dataset with a more restrictive license can constrain the licensing flexibility of all derived artifacts, including processed datasets and trained models. Ensuring that data sources have compatible and permissive licenses is therefore a critical early step in the project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Technical and Organizational Challenges</head><p>The successful creation of large-scale multilingual datasets involves navigating a range of technical and organizational challenges. Addressing these issues is essential for maintaining efficiency, ensuring data integrity, and fostering effective collaboration among diverse teams. The following sections explore the technical challenges of processing data at scale and the organizational aspects necessary for project success.</p><p>Processing at Scale Processing large-scale datasets presents significant technical challenges, both in terms of the associated costs and the complexity of managing software, hardware, and distributed systems. Large-scale computations often require access to multiple computing resources, which introduces additional complexities, such as varying system requirements, the need to transfer large volumes of data, and the maintenance of software across different platforms. Despite the availability of resources, managing distributed processing at scale remains a significant barrier, and exploring distributed training approaches may offer potential solutions for mitigating some of these difficulties in the future.</p><p>At the start of any project of this magnitude, it is crucial to consider the size of the datasets, the availability of storage and compute resources in the clusters, and the allocation of both CPU and GPU resources. One key aspect that is often overlooked is ensuring that the processed data is in the same location as the final model training environment, as transferring large datasets between different clusters can significantly impact efficiency and cost. However, budget or resources constraints may limit the ability to carry out both data processing and model training on the same cluster. In such cases, it becomes essential to carefully plan and optimize across these dimensions, considering factors such as storage, computational availability, and network transfer costs.</p><p>Deduplication Strategy In our project, a global deduplication approach (where deduplication is applied across multiple CommonCrawl dumps) was found to be impractical for two primary reasons. First, for high-resource languages like English, the combined dataset before deduplication is too large to be processed within our computational environments. Second, as noted in <ref type="bibr" target="#b38">[39]</ref>, global deduplication does not provide measurable benefits for downstream model performance.</p><p>Instead, a local deduplication strategy, applied per-dump and per-language, was adopted. This approach reduced the dataset size by approximately 30%, significantly improving processing efficiency in subsequent stages and enhancing the overall quality of the training data by minimizing redundancy while preserving diversity.</p><p>Organizational aspects For future large-scale data projects, it is essential to recognize the importance of managing diverse expertise, which spans scientific, technical, legal, and linguistic domains. Coordinating these competencies is crucial for the successful identification and evaluation of data sources, as well as for handling the vast volume of data. However, simply collecting data is not enough to ensure smooth operations or the sustainability of data pipelines and platforms.</p><p>We strongly recommend establishing robust data governance and lineage strategies at the outset of the project. Defining clear roles, responsibilities, and processes early on can significantly improve communication and collaboration across teams, aligning efforts as the project evolves. Implementing governance frameworks, such as those outlined in <ref type="bibr" target="#b24">[25]</ref>, can provide valuable structure, helping to maintain efficiency and ensure long-term success. Early planning in these areas will prevent organizational bottlenecks and provide a foundation for sustainable data management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Adapting to Rapid Innovation</head><p>The rapid pace of innovation in data processing methodologies for language models introduces a continual challenge. New tools and software frequently emerge, offering solutions to existing bottlenecks such as enhanced processing efficiency, improved storage capabilities, and advanced filtering techniques. However, integrating these innovations into established systems often necessitates workflow adaptations, retraining personnel, and potentially reconfiguring entire pipelines or rerunning pipelines on all datasets. This process can disrupt system stability while requiring considerable investment of time and resources.</p><p>Finding a balance between adopting innovations and maintaining operational consistency is essential. While the implementation of new software can result in performance improvements, it is critical to evaluate which components of the system should remain stable to ensure uninterrupted functionality. Furthermore, frequent updates (such as newly released curated datasets or Common-Crawl dumps) add to the complexity, as these updates necessitate continuous modifications to the data pipeline without undermining the overall coherence of the system.</p><p>A systematic approach to addressing this challenge involves organizing the pipeline into modular components from the outset. By clearly separating and labeling each segment of the pipeline, it becomes possible to identify which elements are adaptable to future innovations and which should remain static due to their critical nature. For instance, components such as deduplication processes may benefit from frequent optimization, whereas core elements like metadata normalization or key filtering mechanisms might need to remain constant to maintain reliability. Furthermore, depending on the stage of the project, it may be prudent to commit to a particular solution, even if suboptimal, rather than continuously reconfiguring the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Outlook</head><p>In this paper, we presented the entire process of data preparation for the OpenGPT-X model family, covering every step from the initial selection of data to delivering the finalized datasets for model training. We outlined the requirements that guided our data selection, which conceptually split our sources into two categories: curated data and web data. This distinction shaped our approach to the implementation of our data pipelines: one for curated data, which required minimal filtering, and another for web data, which focused heavily on filtering and deduplication. In addition to providing a thorough description of our pipeline and data preparation methods, we included an indepth analysis of the resulting datasets. This analysis ensures transparency, aligning with European data recommendations and best practices in scientific research.</p><p>Finally, to contribute to the broader research field and promote openness, we highlighted the challenges faced during this project and the lessons learned. By sharing these insights, we aim to provide valuable guidance for future projects that undertake large-scale data preparation for multilingual language models. We utilized a subset of the preprocessed datasets detailed in this paper to pretrain large language models for the OpenGPTX project.</p><p>As part of our future research, we plan to investigate the use of synthetic data generation for the creation of LLM pretraining data and for advanced data quality filtering, e.g. based on LLMs as a judge. Furthermore, we are looking into generating training data that is compliant with current or upcoming laws and regulations (EU AI Act, GDPR), e.g. by removing or masking personally identifiable information. We intend to extend our collection of datasets, also targeting both datasets for underrepresented domains as well as languages. With additional processing requirements due to advanced processing techniques, and a growing number of available datasets, the demand for compute resources to generate high-quality training data will also increase.</p><p>We also examined the impact of format on filtering, but found no significant correlation (r = 0.03, p = 0.79). However, when considering the language of the dataset, we found a more notable correlation (r = 0.37, p = 0.001), hinting that certain languages may be more prone to needing extensive filtering, possibly due to differences in data quality or availability across languages.</p><p>Finally, domain showed no significant correlation with the percentage of data filtered (r = -0.004, p = 0.97).</p><p>These results suggest that filtering operates somewhat independently from these broader dataset characteristics. Language differences, in particular, seem to warrant more focused attention in future analyses, as they exhibit a stronger connection to filtering needs compared to other factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.2">List of Curated Datasets</head><p>The complete list of curated dataset is presented in Table <ref type="table">4</ref> (next page).</p><p>Table <ref type="table">4</ref>: This table provides an overview of various multilingual datasets utilized in OpenGPT-X project. Each entry begins with the Corpus ID of the dataset and a link to its project page. The Language/s column specifies the languages included within each dataset (see Section 5.1). The Format of the datasets is also noted, indicating the file type, such as TXT, JSON, or XML. The License column outlines the legal terms governing the use of each dataset, for uncommon licenses a link is provided. The Domain column reflects the specific field or subject area that the dataset pertains to, such as Law, Math, or Medical. The # Docs column presents the total number of documents contained in each dataset, while the # Words column conveys the total word count in thousand. Finally, the % Filtered (Filt.) column indicates the percentage of documents that have been filtered out during preprocessing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Data pipeline for curated datasets: The pipeline involves format conversion, content normalization, followed by language detection, content normalization and filtering. The output is high-quality training data in JSONL format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of the Web Data Processing Pipeline: The pipeline processes WET files with Ungoliant by normalizing, computing quality warnings and harmful-perplexity, detecting language, and prefiltering. Afterwards, data is filtered based on quality and harmful-perplexity before deduplication, producing high-quality training data in JSONL format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of curated dataset licenses. Most datasets have permissive licenses (CC0, CC BY, MIT, Apache), while restrictive licenses are primarily represented by datasets sourced from multiple origins (Various).</figDesc><graphic coords="13,177.48,95.04,257.05,207.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Word distribution across different languages in the curated datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Log-scaled comparison of word counts and document counts across datasets, sorted by word size. Some datasets exhibit a notably high word count despite having relatively few documents, indicating longer average document sizes.</figDesc><graphic coords="17,156.06,95.04,299.87,178.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Analysis of CommonCrawl dumps used in the dataset, showing the distribution of dump weeks (blue), billion documents (red) and average dump density (yellow) per year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Percentage of data removed after deduplication for each language, showcasing the varying impact of deduplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Total words, average words per dataset (Avg Words/DS), and documents for each domain, sorted by total words. significantly fewer words (712.8 million) compared to monolingual (4.38 billion) ones. Indeed, while contributing to language diversity, they often lack the volume of content found in larger monolingual datasets.</figDesc><table><row><cell>On average, multilingual datasets contain</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>List of dumps by year and week.Dumps In this section, we provide a detailed look at the CommonCrawl dumps used in our project. The dumps span multiple years, with varying distributions of weeks per year.</figDesc><table><row><cell>50</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Meta restricts commercial use based on monthly active users and restricts the use of LLaMa model to improve other LLMs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://opengpt-x.de/en/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>General Data Protection Regulation, available at: https://gdpr-info.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_3"><p>https://github.com/huggingface/datatrove</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_4"><p>https://github.com/google/cld3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_5"><p>https://dsi.ut-capitole.fr/blacklists/index_en.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_6"><p>https://github.com/togethercomputer/RedPajama-Data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_7"><p>https://github.com/bitextor/warc2text</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_8"><p>https://github.com/CLD2Owners/cld2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_9"><p>The amount of training data is typically measured by the number of tokens or words. However, the choice of a tokenizer affects the number of generated tokens, making absolute numbers for different models not directly comparable. Therefore, we use the wc command line utility to estimate the number of words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_10"><p>While it is not always easy to detect AI-generated content, extra caution should be applied for datasets created after 2021 to avoid including synthetic data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_11"><p>Following the schema https://data.CommonCrawl.org/crawl-data/CC-MAIN-&lt;YEAR-MONTH&gt;/index.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_12"><p>https://github.com/kermitt2/grobid</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_13"><p>https://github.com/earwig/mwparserfromhell</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_14"><p>Normalization Form KC as described in https://unicode.org/reports/tr15/#Norm_Forms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_15"><p>https://hplt-project.org/datasets/v1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_16"><p>Ungoliant supports the detection of 176 languages.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_17"><p>The percentage is calculated on a line-base, i.e. when the character '\n' is found.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_18"><p>To simplify aggregation, the license versions are omitted here and in Figure3. However, in Table4, the specific version numbers for all the datasets are clearly indicated.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_19"><p>https://huggingface.co/docs/hub/en/datasets-cards</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>This work was funded by the <rs type="funder">German Federal Ministry for Economic Affairs and Climate Action (BMWK)</rs> through the project <rs type="projectName">OpenGPT-X</rs> (project no. <rs type="grantNumber">68GX21007D</rs>). The authors gratefully acknowledge the <rs type="institution">Gauss Centre for Supercomputing e.V.</rs> (<ref type="url" target="http://www.gauss-centre.eu">http://www.gauss-centre.eu</ref>) for providing compute resources on the <rs type="institution" subtype="infrastructure">GCS Supercomputer JUWELS</rs> at <rs type="institution">Jülich Supercomputing Centre (JSC)</rs>.</p><p>The authors gratefully acknowledge the compute resources made available to them on the highperformance computer at the <rs type="institution">NHR Center of TU Dresden</rs>. This center is jointly supported by the <rs type="funder">Federal Ministry of Education and Research</rs> and the state governments participating in the NHR (<ref type="url" target="http://www.nhr-verein.de/unsere-partner">http://www.nhr-verein.de/unsere-partner</ref>).</p><p>Many thanks to the OpenGPT-X project partner IONOS for providing a NVIDIA DGX-2 machine to facilitate development of our data pipelines as well as processing many curated datasets.</p><p>We acknowledge the use of AI tools (i.e. ChatGPT-4/Claude) in the early stages of this paper for improving readability and brainstorming useful insights for the analysis section.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_MbZdwcd">
					<idno type="grant-number">68GX21007D</idno>
					<orgName type="project" subtype="full">OpenGPT-X</orgName>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">GCS Supercomputer JUWELS</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b3">4</ref> <p><ref type="url" target="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</ref> 5 <ref type="url" target="https://digital-strategy.ec.europa.eu/en/policies/legislation-open-data">https://digital-strategy.ec.europa.eu/en/policies/legislation-open-data</ref> 6 <ref type="url" target="https://commoncrawl.org/">https://commoncrawl.org/</ref> 2</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Dataset</head><p>Monolingual Dataset Text Extraction Language Identification Filtering Deduplication C4 <ref type="bibr" target="#b41">[42]</ref> CC (WARC) langdetect (≥ 0.99) Non-language, outliers, LID 3-sentence spans The Pile <ref type="bibr" target="#b13">[14]</ref> jusText pycld2, fastText LID MinHashLSH RefinedWeb <ref type="bibr" target="#b39">[40]</ref> trafilatura fastText LID, DW/ LW heuristics MinHashLSH, token matching, URL deduplication Dolma <ref type="bibr" target="#b44">[44]</ref> CC (WARC) fastText (≥ 0.5) LID , DW/ L, Toxic content URL matching, raw doc match, Bloom filter FineWeb <ref type="bibr" target="#b38">[39]</ref> trafilatura fastText DW/ LW heuristics MinHash</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Dataset Text Extraction Language Identification Filtering Deduplication</head><p>CCNet <ref type="bibr" target="#b47">[47]</ref> WET fastText (≥ 0.5) Perplexity filtering, LID PW SHA-1 mC4 <ref type="bibr" target="#b48">[48]</ref> CC (WARC) cld3 C4, 3 lines of 200+ characters 3-sentence spans OSCAR 22.01 <ref type="bibr" target="#b1">[2]</ref> WET fastText (≥ 0.8) LW LID, WD Unicode rules, UT1 blocklist Line-wise deduplication (English only) BigScience ROOTS <ref type="bibr" target="#b31">[32]</ref> Custom extractor Manual (by data source)</p><p>Word frequency heuristics SimHash, substring deduplication Glot500 <ref type="bibr" target="#b21">[22]</ref> Custom crawling Manual (by data source) Language-script matching, BigScience ROOTS rules Sentences RedPajama-v2 <ref type="bibr" target="#b10">[11]</ref> WET fastText Classifier, heuristic filtering DW Bloom filter MADLAD-400 <ref type="bibr" target="#b29">[30]</ref> Unknown Semi-supervised LID Prefiltering similar to C4 LW deduplication HPLT <ref type="bibr" target="#b11">[12]</ref> warc2text CLD2, fastText Two-stage LID, dictionary spell check DW MinHash </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Metadata Content Example</head><p>{ "meta": { "url": "<ref type="url" target="https://en.wikipedia.org/wiki/Organic%20Chemistry/Cover">https://en.wikipedia.org/wiki/Organic%20Chemistry/Cover</ref>", "title": "Organic Chemistry/Cover", "docid": "wikimedia/wikibooks/enwikibooks-20240401", "language": "en", "language_score": 1.0, "download_data": "2024-04-01", } "text": "Welcome to the world's foremost open contentOrganic Chemistry Textbookon the web!\n\n The Study of Organic Chemistry[...]", }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Curated Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">Filtering</head><p>The correlation between the percentage filtered and the total number of words in a dataset was relatively low at r = 0.26 (p = 0.022), suggesting only a weak relationship. Similarly, a slightly stronger correlation was observed between the percentage filtered and the number of documents, at r = 0.33 (p = 0.003), indicating that datasets with more documents tend to require somewhat more filtering, but the relationship is still far from definitive. 27 Very short documents (headlines) 28 Noise from PDF conversion. 29 Noisy data and short document. 30 Short documents. 31 The ncc corpus includes the following licenses : NLOD 2.0, CC0 1.0, CC BY-NC 2.0, CC BY-SA 3.0 32 Short documents. 33 Noisy documents. 34 This Pile part is extracted from ArXiv where the author can decide between a variety of licenses. 35 This Pile part is extracted from PubMed where the author can decide between a variety of licenses. 36 This Pile part is extracted from PubMed where the author can decide between a variety of licenses. 37 Noisy documenta and conversion errors. 38 Low language score due to similar languages. 39 Short documents. 40 The Stack is a collection of source code from repositories with various licenses. Any use of all or part of the code gathered in The Stack must abide by the terms of the original licenses, including attribution clauses when relevant. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Abadji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><forename type="middle">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="DOI">10.14618/ids-pub-10468</idno>
		<ptr target="https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick</title>
		<editor>
			<persName><forename type="first">Harald</forename><surname>Lüngen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><surname>Kupietz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Piotr</forename><surname>Bański</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adrien</forename><surname>Barbaresi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename><surname>Clematide</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ines</forename><surname>Pisetta</surname></persName>
		</editor>
		<meeting>the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick<address><addrLine>Mannheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-07-12">12 July 2021. 2021</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Leibniz-Institut für Deutsche Sprache</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards a Cleaner Document-Oriented Multilingual Crawled Corpus</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Abadji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4344" to="4355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The elephant in the room: Analyzing the presence of big tech in natural language processing research</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Philip Wahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Ruas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelie</forename><surname>Neveol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanny</forename><surname>Ducel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Fort</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.734</idno>
		<ptr target="https://aclanthology.org/2023.acl-long.734,doi:10.18653/v1/2023.acl-long.734" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13141" to="13160" />
		</imprint>
	</monogr>
	<note>Long Papers) Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tokenizer choice for LLM training: Negligible or crucial?</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaudia</forename><surname>Thellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Rutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Lübbering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Klug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niclas</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Schulze Buschhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charvi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Arno</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Jurkschat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hammam</forename><surname>Abdelwahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Ostendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Weinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafet</forename><surname>Sifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kesselheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Flores-Herr</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2024.FINDINGS-NAACL.247</idno>
		<idno>doi:10.18653/V1/2024.FINDINGS-NAACL.247</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.findings-naacl.247" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2024</title>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Helena</forename><surname>Gómez-Adorno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</editor>
		<meeting><address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">June 16-21, 2024. 2024</date>
			<biblScope unit="page" from="3907" to="3924" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FastSpell: The LangId Magic Spell</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Bañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gema</forename><surname>Ramírez-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaume</forename><surname>Zaragoza-Bernabeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Ortíz-Rojas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</title>
		<meeting>the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7133" to="7140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trafilatura: A web scraping library and command-line tool for text discovery and extraction</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Barbaresi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><surname>Bloom</surname></persName>
		</author>
		<idno type="DOI">10.1145/362686.362692</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Broder</surname></persName>
		</author>
		<idno type="DOI">10.1109/SEQUEN.1997.666900</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Compression and Complexity of Sequences 1997, SEQUENCES &apos;97</title>
		<meeting>the Compression and Complexity of Sequences 1997, SEQUENCES &apos;97<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language id in the wild: Unexpected challenges on the path to a thousand-language web text corpus</title>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Breiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Van Esch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6588" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName><surname>Moses S Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thiry-fourth annual ACM symposium on Theory of computing</title>
		<meeting>the thiry-fourth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Redpajama: an open dataset for training large language models</title>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
	</analytic>
	<monogr>
		<title level="m">Together Computer</title>
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A New Massive Multilingual Dataset for High-Performance Language Technologies</title>
		<author>
			<persName><forename type="first">Ona</forename><surname>De Gibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Nail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Arefyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Bañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelmer</forename><surname>Van Der Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaume</forename><surname>Zaragoza-Bernabeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Aulamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gema</forename><surname>Ramírez-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kutuzov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</title>
		<meeting>the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1116" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Gptscore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04166</idno>
		<title level="m">Evaluate as you desire</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Pile: An 800GB dataset of diverse text for language modeling</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<ptr target="https://arxiv.org/abs/2101.00027" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Openwebtext corpus</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06893</idno>
		<title level="m">Learning word vectors for 157 languages</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11644</idno>
		<title level="m">Textbooks are all you need</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">C4corpus: Multilingual web-size corpus with free license</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omnia</forename><surname>Zayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Nicoletta Calzolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hélène</forename><surname>Mazo</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2016/summaries/388.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation LREC 2016<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">May 23-28, 2016. 2016</date>
		</imprint>
	</monogr>
	<note>Asunción Moreno, Jan Odijk, and Stelios Piperidis European Language Resources Association (ELRA)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A comprehensive overview of large language models (llms) for cyber defences: Opportunities and directions</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Hassanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nour</forename><surname>Moustafa</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2405.14487</idno>
		<idno type="arXiv">arXiv:2405.14487</idno>
		<idno>doi:10.48550/ARXIV. 2405.14487</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.14487" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth workshop on statistical machine translation</title>
		<meeting>the sixth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Neural Information Processing Systems</title>
		<meeting>the 36th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="30016" to="30030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glot500: Scaling multilingual corpora and language models to 500 languages</title>
		<author>
			<persName><forename type="first">Ayyoob</forename><surname>Imanigooghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hossein Kargaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Severini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jalili</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nora</forename><surname>Sabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunlan</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1082" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Code for individual languages and language groups. Standard, International Organization for Standardization</title>
		<author>
			<persName><surname>Iso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Perplexed by quality: A perplexity-based method for adult and harmful content detection in multilingual heterogeneous web data</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangling</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Zevallos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Suarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10440</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data governance in the age of large-scale data-driven language technology</title>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maraim</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Danchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samson</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeerak</forename><surname>Talat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somaieh</forename><surname>Nikpoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3531146.3534637</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;22</title>
		<meeting>the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2206" to="2222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hérve</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext.zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Large language models are state-of-the-art evaluators of translation quality</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.14520</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10959</idno>
		<title level="m">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Madlad-400: A multilingual and document-level large audited dataset</title>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derrick</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romi</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data augmentation using pre-trained transformer models</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.lifelongnlp-1.3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</title>
		<editor>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hadrien</forename><surname>Glaude</surname></persName>
		</editor>
		<meeting>the 2nd Workshop on Life-long Learning for Spoken Language Systems<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-12">December 2020</date>
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset</title>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Hugo Laurençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">González</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Ponferrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Frohberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Šaško</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giada</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pistilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somaieh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maraim</forename><surname>Nikpoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Van Strien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">Chien</forename><surname>Almubarak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itziar</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Dios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamik</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Violette</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Lepercq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><forename type="middle">Alexandra</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><surname>Jernite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31809" to="31826" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deduplicating Training Data Makes Language Models Better</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8424" to="8445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evaluation of Document Deduplication Algorithms for Large Text Corpora</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lennard</forename><surname>Helmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benny</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoha</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, Optimization, and Data Science</title>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Elanton Fernandes, and Hammam Abdelwahab to appear</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Suffix arrays: A new method for on-line string searches</title>
		<author>
			<persName><forename type="first">Udi</forename><surname>Manber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gene</forename><surname>Myers</surname></persName>
		</author>
		<idno type="DOI">10.1137/0222058</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="935" to="948" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting near-duplicates for web crawling</title>
		<author>
			<persName><forename type="first">Gurmeet</forename><surname>Singh Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><surname>Opengpt-X</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.13866365</idno>
		<title level="m">Progress report: Towards european llms</title>
		<imprint>
			<date type="published" when="2024-10">October 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A monolingual approach to contextualized word embeddings for mid-resource languages</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Javier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.156</idno>
		<ptr target="https://aclanthology.org/2020.acl-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="1703" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.17557</idno>
		<idno type="arXiv">arXiv:2406.17557</idno>
		<title level="m">The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf" />
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Policy-aware content reuse on the web</title>
		<author>
			<persName><forename type="first">Oshani</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lalana</forename><surname>Kagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Berners-Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2009, 8th International Semantic Web Conference, ISWC 2009</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tom</forename><surname>Heath</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lee</forename><surname>Feigenbaum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Enrico</forename><surname>Motta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Krishnaprasad</forename><surname>Thirunarayan</surname></persName>
		</editor>
		<meeting><address><addrLine>Chantilly, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">October 25-29, 2009</date>
			<biblScope unit="volume">5823</biblScope>
			<biblScope unit="page" from="553" to="568" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04930-9_35</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Is chatgpt a good nlg evaluator? a preliminary study</title>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengkui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04048</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Finetuned multimodal language models are high-quality image-text data filters</title>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Mrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sateesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.02677</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><surname>Ccnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">Extracting high quality monolingual datasets from web crawl data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GPT3Mix: Leveraging large-scale language models for text augmentation</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongju</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woomyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.192</idno>
		<ptr target="https://aclanthology.org/2021.findings-emnlp.192,doi:10.18653/v1/2021.findings-emnlp" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<editor>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Scott</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">-Tau</forename><surname>Yih</surname></persName>
		</editor>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="2225" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
