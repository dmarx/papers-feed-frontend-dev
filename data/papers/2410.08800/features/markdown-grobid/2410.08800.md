# Data Processing for the OpenGPT-X Model Family

## Abstract

## 

This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an indepth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.

## Introduction

In recent years, there has been an exponential increase in the development and deployment of large language models (LLMs) [[3,](#b2)[19]](#b18). The most prominent and accurate models are often the products of major technology companies and are available either through APIs (e.g. OpenAI's GPT models) or with non-open licenses (e.g. Meta's LLaMA family [1](#foot_0) ). Moreover, these models are often US-centric, predominantly fluent in American English, and imbued with the values inherent to that cultural context. Conversely, open and unrestricted models that focus on a variety of languages and cultures often emerge from academic settings. However, these models generally lack the accuracy and power of their more robust commercial counterparts. This disparity is primarily due to the differences in Trove [10](#foot_3) , which was released alongside the dataset. The pipeline builds upon the methodology used in RefinedWeb, beginning with text extraction from WARC files using the trafilatura tool. Language filtering is performed with fastText, applying a cutoff score of 0.65 to ensure the inclusion of high-quality English documents. After this, the dataset undergoes both document-wise and linewise heuristic filtering to further enhance quality by removing noise and non-linguistic content. The final deduplication step employs the MinHash algorithm, ensuring a clean and diverse dataset that is well-suited for large-scale language model training.

## Multilingual datasets

Creating high-quality multilingual datasets presents distinct challenges compared to monolingual English datasets. Language identification becomes significantly more complex, as models must accurately differentiate between multiple of languages, often with limited training data for lowerresource languages. Additionally, ensuring consistent quality across languages is difficult due to varying amounts of available web data, and some languages are overrepresented while others suffer from a lack of resources. High quality multilingual datasets are essential for training models that perform well across diverse linguistic and cultural contexts, enabling the training of LLMs that are not inherently biased toward dominant languages like English.

CCNet [[47]](#b47) is among the first datasets built on CommonCrawl data, containing 3.2TB (1.5 billion documents) in 130 languages. The CCNet pipeline begins by extracting paragraphs from WET files and performing SHA-1-based deduplication. The extracted text is normalized by converting it to lowercase, replacing all numbers with zeros, and removing accents and punctuation. Language detection is based on fastText, with documents with a language score below 0.5 being discarded. The final step in the pipeline involves filtering documents based on perplexity scores calculated with language-specific 5-gram Kneser-Ney language models [[20]](#b19). The corresponding models are trained on tokenized Wikipedia data, using the sentence piece tokenizer [[29]](#b28) to ensure consistent text segmentation.

Multilingual C4 (mC4) [[48]](#b48) extends the C4 dataset to 101 languages and contains approximately 27TB of text. In contrast to C4, which focuses on English, mC4 replaces the language classification tool with cld3 [11](#foot_4) to handle the identification of multiple languages. A significant modification in mC4 is the introduction of a filtering step that requires documents to contain at least three lines of text, each with 200 or more characters. This broader filter replaces C4's original language-specific rule, which filtered based on English punctuation marks, ensuring a more consistent approach across languages and improving the quality of multilingual data.

OSCAR 22.01 [[2]](#b1) contains about 8TB of text in 153 languages. It is produced with the Ungoliant pipeline [[1]](#b0) from the CommonCrawl WET (Web Extracted Text) files. The filtration starts with language detection, which utilizes the fastText model to identify language of each line in a document. If the confidence score is less then 0.8, then the line is assigned with to the unknown language class. The proportion of each language in a document is calculated as a percentage of bytes in the document assigned with that language. A document is classified as multilingual if it contains at least 5 lines, not more than 5 languages, the proportion of each language is at least 1/(m + 1) (where m is the number of languages in the document) and the proportion of the unknown language is not larger than 1/(m + 1). Otherwise, the document is classified as monolingual and is assigned with the language having the highest weighted confidence score. The weighted confidence of a language is calculated as a sum of products of byte size and language confidence of each line classified to that language divided by the total number of bytes in the document. A monolingual document is passed to further processing steps if the weighted confidence of its language is at least 0.6. Language identification is followed by the filtering of documents based on the line lengths, proportions of characters of certain Unicode classes, and the URL based UT1 blocklist. [12](#foot_5) Deduplication is not applied to non-English data, and only line-wise deduplication is applied to English data.

BigScience ROOTS [[32]](#b31) is a dataset containing about 1.6TB of text in 46 natural languages. This dataset comprises data from the crowdsourced list of 252 monolingual and multilingual text collections, accompanied by documents extracted from the CommonCrawl WARC files according to the list of domains suggested by the community members. Extraction of text from HTML files is performed by custom code inspired by the CommonCrawl extractor. Deduplication is performed on the level of data sources. Additional data is obtained from the OSCAR version 21.09 dataset [[38]](#b37). The OSCAR derived documents are passed through heuristic rules based on word frequencies and are deduplicated in two steps using SimHash [[10,](#b9)[36]](#b35) and substring deduplication [[33,](#b32)[35]](#b34).

The MADLAD-400 [[30]](#b29) dataset contains about 30TB of text in 419 languages. The data is obtained from the CommonCrawl dumps and is initially deduplicated on the line level, followed by basic prefiltering similar to the C4 rules. The semi-supervised language identification model [[9]](#b8), which is trained with two tasks (supervised language detection task and unsupervised corrupted input recovery task [[42]](#b41)), is used to classify the documents after that. The language identification step is followed by another set of quality filtering heuristics. A few language-specific processing and filtering rules are applied based on the manual inspection of 20 documents per language.

RedPajama-v2[foot_6](#foot_6) contains about 20 billion documents and 30 trillion tokens in 5 European languages. It is produced with the extended CCNet pipeline and utilizes both heuristic based and classifier based content filtering. The deduplication is performed on the document level using a Bloom filter.

The HPLT [[12]](#b11) dataset contains 50.1TB of text in 75 languages. The data is sourced from the Internet Archive and CommonCrawl dumps in WARC format. First, text is extracted using the warc2text tool[foot_7](#foot_7) and classified by language using the CLD2 library [15](#foot_8) . Next, text is cleaned from encoding errors, and another two-stage paragraph-level LID is performed. Its first stage utilizes the fastText model, and the second stage applies dictionary based spell checking for several languages related to the fastText result and selects the language, for which fewer mistakes are detected [[5]](#b4). Finally, MinHash based deduplication is performed on the document level.

The collection of data we processed comprises data from both web and curated sources, including parts of The Pile and RedPajama-v2, and the content and metadata are normalized in a uniform way by the data processing pipeline described in this work.

## Data selection

The performance of large language models in downstream tasks benefits from large volumes of diverse, high-quality training data. Key properties for effective pretraining include the diversity of knowledge, domains, and tasks, which enhance a model's generalization abilities [[14]](#b13), as well as the appropriate size to fully saturate the model's learning capacity [[21]](#b20). To mitigate cultural bias from monolingual English datasets, we emphasize the selection of non-English data. Specifically, we aim to balance our data to encompass all European languages. This section outlines the data selection requirements that ensure these desirable properties and provides an overview of the chosen data sources.

Given the aforementioned properties, our approach to data selection involves ensuring a large volume [16](#foot_9) of diverse and high-quality processed data for LLMs training.

Data Quantity and Diversity. A significant portion of our data is derived from crawled web pages, complemented with curated datasets to ensure a broad spectrum of languages, genres, text types, and domains. These curated datasets are carefully selected to enhance performance in various downstream tasks such as question answering, machine translation, and summarization.

Data Quality. The quality of large language models is highly dependent on the textual training data, often measured in terms of the number of tokens or words. High-quality data should be multilingual (covering as many relevant languages as possible), diverse (sourced from multiple domains and document types), free of toxic or offensive content (to prevent harmful outputs), and unbiased (to ensure fair model behavior). In this context, data quality directly reflects the properties of the text itself. For a detailed discussion of our filtering processes, see Section 4.

## Selected data

Following these requirements, our data sources can be categorized into two groups. The first group consists of curated data from multiple existing datasets, providing a relatively small but diverse set of high-quality knowledge sources. The second group comprises general web data sourced from the CommonCrawl project.

## Curated data

Curated datasets are essential for providing high-quality, diverse training data for our language models. These datasets have typically undergone a quality review process, either prior to their release or through ongoing public review, and are publicly available (see Appendix 8.3 for a list of our curated datasets). This includes data such as Wikipedia, collections of scientific articles, books, and source code.

To select appropriate datasets, we employed the following considerations:

1. Legal and Licensing

• Is the license unproblematic? (e.g., allows research and commercial use, is permissive, and does not include restrictive copyleft clauses).

• Is the data GDPR-compliant?

## Linguistic and Relevance

• Does the dataset contain documents in a language relevant to our research?

• Does the dataset contain a sufficient quantity of tokens/words/documents?

• Do the documents cover relevant topics? (e.g. for downstream applications)

## Quality and Integrity

• Is the content of documents error free? (e.g., no artifacts from Optical Character Recognition, misspellings, or antiquated language)

• Are documents from a geographical region of interest?

• Are the documents from a relevant time period? (e.g. no historic documents)

• Are the documents human-generated[foot_10](#foot_10) ?

• Is the dataset artificial? (e.g. no automatically generated content or synthetic data)

• Is there no significant overlap with other datasets already in use?

## Resource Availability

• Are there sufficient resources available to preprocess the data? (e.g., adequate disk space and computational resources)

• Has the dataset not been fully or partially superseded by a newer version? (e.g. no outdated version)

If one or more of these questions were answered negatively, the dataset was excluded from further processing. These criteria ensure that the curated datasets selected for our project are of the highest quality, relevance, and suited for our specific objectives.

## CommonCrawl data

Unlike curated data, web data is available in a large amount. For this reason, we prioritize processing web data only based on recency and language. Specifically, we use data from CommonCrawl (CC), an organization that maintains a free, open-source repository of web data accessible to anyone since they provide the largest amount of data. They perform monthly crawls of internet webpages, compiling them into data dumps labeled in the format YYYY-WW (e.g., 2024-22, see Section 5.2 for a list of selected dumps). CommonCrawl dumps can be downloaded from URLs [18](#foot_11) in two main formats: WARC and WET. WARC (Web ARChive format) files contain raw data from the crawl, including the full page HTML and request metadata. WET (WARC Encapsulated Text) files provide a text-only version of the websites, excluding the web metadata. In our pipeline, we use the WET files exclusively as they provide the extracted textual content of web pages in a simplified format. This eliminates the need for extensive parsing of raw HTML data found in WARC files, allowing us to focus directly on text analysis. For instance, the CC dump "2024-22" includes data crawled from 2.5 billion web pages, totaling 18.35 TiB of compressed HTML data. This particular dump consists of 90,000 WET files, which are grouped into segments for processing. This largescale web data significantly expands the scope of our collection, providing a robust foundation for pretraining large language models.

## Data pipelines

This section describes the data pipelines we used for processing both curated and web data. Given the distinct nature and processing requirements of these two types of data, the pipelines are described in separate subsections.

## Pipeline for curated data

Curated datasets are published in a variety of formats and metadata. In contrast to web crawled data, these datasets consist of thematically related and high-quality texts, due to the previous curation process (e.g. reviewing for scientific articles). Therefore, a quality-based filter is less necessary and the preprocessing is based the pipeline in Figure [1](#fig_0). Format Conversion. The first step in processing curated data involves converting the raw data into JSONL format. Depending on the raw data's format, we employ different processing tools. For PDF documents, Grobid[foot_12](#foot_12) converts PDFs to XML/TEI as an intermediate format which we then convert to JSONL to identify and remove layout elements from the content. For Wikimedia datasets (e.g., Wikipedia, Wikibooks), wikiConverter parses the Wikimedia XML dumps and converts them to JSONL. The data is first converted using mwparserfromhell[foot_13](#foot_13) , then list pages and disambiguation pages are removed, and finally the remaining data is processed as with other curated datasets. For any other format (e.g. plain text, csv, sql dumps, etc), we created custom conversion tools.

## Normalization

Normalizing the content is important to ensure that the rest of the pipeline consistently handles data, facilitating versioning, processing, and sharing. We implement two types of normalization: content normalization and metadata normalization.

Content Normalization. Content normalization ensures that the text data is uniformly formatted and encoded. (i) All text data is encoded in UTF-8 to maintain a standard character encoding format. (ii) We apply NFKC normalization [21](#foot_14) to the text content, converting characters to their canonical decomposition followed by compatibility composition to ensure that visually similar characters are represented consistently. (iii) Excessive white space is trimmed, and inconsistent spacing is corrected. (iv) Finally, we remove common conversion artefacts such as HTML tags or special characters to ensure the text is free from noise.

Metadata Normalization. Metadata normalization is important to maintain a consistent set of essential metadata attributes that provide a snapshot of each document. This metadata forms the basis for subsequent analysis and filtering steps. The format for a single document (a line in a JSONL file) is as follows:

{ "meta": { "docid": <corpus/language/fileno/docno>, "url": <url:String>, "title": <title:String>, "download_date": <ISO-date:String>", "language": <ISO-language-code:String>, "language_score": <language-detection-score:Float>}, "text": <text:String> }

## Where

• meta contains all document metadata.

• docid is a unique ID comprised of the corpus name, the number of the file a document originates from, and the running document number within this file.

• url is a string representing the URL of a document (where appropriate).

• title is the title of a document (where appropriate).

• download_date is the download date in YYYY-MM-DD format.

• language is a 2-or 3-letter ISO language code [[23]](#b22) for the language of the content.

• language_score is a score estimated from language detection [[27]](#b26) (1.0 where the language is not ambiguous); "xx" is used for non-language content such as source code.

• text is the normalized document content.

For an example document, see Appendix 8.2. Note that the example for the document format already includes information from language detection.

## Language Detection

Following content normalization, language identification is performed using fasttext [[27]](#b26). The detected language code is included both in the metadata field and as part of the filename for the pipeline output to enable grouping of data by language.

## Filtering

To ensure appropriate data quality, we aim to remove low-quality documents as early as possible in the processing pipeline. We apply a set of filters to remove some outliers:

• Documents with a language score lower than 0.5 are filtered out.

• Documents that contain fewer than 200 characters are removed.

These thresholds (200 characters, 0.5 language score) were chosen based on settings used in previous projects (e.g., HPLT [22](#foot_15) ) and are designed to ensure that only relevant and high-quality data proceeds to the subsequent stages of processing.

The final output of the curated data pipeline is a set of JSONL files, organized by language and corpus, ready for further analysis and model training. Processing web data from large-scale sources like CommonCrawl presents significant challenges due to its unstructured and noisy nature. To transform this raw web data into a suitable format for training LLMs, we utilized a specialized pipeline (see Figure [2](#fig_1)). This section provides a detailed breakdown of each processing stage, including normalization, filtering, and deduplication, using the Ungoliant pipeline.

## Dataset Acquisition and Preparation

The process begins by downloading CommonCrawl dumps in WET file format using a custom download script. These dumps contain the textual content and metadata of web pages, compressed in .txt.gzip format. After retrieving the raw data, we organized it into separate folders based on the dump year for easier processing.

## Ungoliant Pipeline

The core of the web data processing relies on the Ungoliant pipeline [[1]](#b0), a modular system optimized for handling CommonCrawl corpora and producing an OSCAR-like dataset [[2]](#b1). The Ungoliant pipeline is conceptually split into several key components:

• Normalization: Similar to the normalization applied to curated data (see Section 4.1), this step ensures consistency in text encoding, removing noise, normalizing text formatting, and encoding all content into UTF-8.

• Computation of quality warnings: Ungoliant generates quality warnings for each document which are then used for subsequent filtering stages.

• Computation of harmful-perplexity: Harmful content is identified using perplexity scores based on a pretrained KenLM model [[24]](#b23). This model evaluates documents to determine whether they contain harmful content (e.g., adult material).

• Language detection: Sentence-based language identification is performed using embedded [23](#foot_16)pretrained fastText models [[27,](#b26)[26]](#b25).

• Prefiltering: Documents are filtered based on criteria similar to those used for curated data (see Section 4.1.3), such as removing documents with a low number of characters or low language detection scores.

## Filtering

After the documents pass through the Ungoliant pipeline, we apply additional filtering steps based on quality warnings and harmful-perplexity scores to further refine the dataset. Quality Warning filtering. Ungoliant flags documents with several quality warnings: tiny, noisy, header, footer, and short_sentences. These warnings are accompanied by predefined thresholds, and documents that exceed these thresholds are filtered out. The thresholds are defined as follows:

• tiny: When the document contains fewer than 5 sentences (lines).

• noisy: When the ratio of non-letters to the total number of characters exceeds 50%.

• header: Defined through a three-step process:

-Iterate through the first 20% of a document [24](#foot_17) .

-Identify short sentences (lines) with fewer than 100 characters.

-Annotate the document as header if more than 50% of the sentences in this section are short.

• footer: Similar to the header but applied in reverse order to the above one.

• short_sentences: When the document has a high number (≥ 50%) of lines with less than 100 characters.

Harmful-Perplexity Filtering. Filtering adult content is a critical part of ensuring that the models are trained on safe and appropriate data. We use the perplexity scores provided by the KenLM-based model trained on adult content from the UT1 Blacklist [[24]](#b23) as part of the Ungoliant pipeline. Lower perplexity scores indicate a higher likelihood that the document contains harmful or adult content. We set a perplexity threshold of 5, meaning any document with a perplexity score below this threshold is filtered out.

## Document deduplication

Building on the quality filtering performed in the previous stage, the next step after running the Ungoliant pipeline is document deduplication. This process refines the dataset by removing both exact and near duplicate documents, thus reducing the overall size of the data and improving its quality for LLM training. After extensive testing [[34]](#b33), we found that the MinHash/LSH algorithm offers the best balance of precision, recall, and computational efficiency for deduplicating large volumes of web data. The deduplication process was executed on a per-dump and per-language basis ("local" deduplication), which is both convenient and backed by newer research [[39]](#b38) that confirms that local deduplication is more favorable than global (across dump and/or language) approaches for preserving data diversity while minimizing redundancy.

## Technical Infrastructure

To effectively manage the extensive data processing requirements, we leveraged significant computational resources. The processing of large-scale datasets, including both curated datasets and CommonCrawl data, required the use of high-performance computing infrastructure.

Initially, a NVIDIA DGX-2 machine was employed for downloading and processing the curated datasets, as well as for deduplicating the first CommonCrawl dumps. As the volume of data increased, the processing of the remaining CommonCrawl datasets was distributed across two large compute clusters.

The technical specifications of these compute clusters are as follows:

• Compute Cluster 1: Provided by TU Dresden, consisting of compute nodes with 2 x Intel Xeon Platinum 8470 (52 cores) @ 2.00 GHz CPU and 512 GB RAM.

• Compute Cluster 2: Provided by FZ Jülich, consisting of compute nodes with 2 x Intel Xeon Platinum 8168 (2x24 cores) @ 2.7 GHz CPU and 192 GB RAM.

These resources enabled us to handle the extensive data processing requirements necessary for pretraining our large language models. 

## Data Analysis

This section provides a detailed analysis of the datasets used in our project, focusing on both curated and web data sources. The goal is to offer a comprehensive overview of the data characteristics.

We begin by presenting statistics for the curated datasets (Section 5.1), including details on corpus names, languages, text types, and other relevant metadata. We then analyze the web data (Section 5.2), examining the size, composition, and the effects of filtering and deduplication. This includes a breakdown of the number of documents, word counts, and language distributions.

It is important to note how LLM training often uses sampling techniques to optimize data distribution, considering factors like language balance and the fertility score [[4]](#b3). In this paper, we focus solely on the raw data collection, filtering, and integration process, without adjusting distributions for training purposes. For further information on a model trained on part of the data we process, refer to [[37]](#b36).

## Curated Data

As previously mentioned, curated data undergoes a distinct processing pipeline. This is due to the assumption that curated datasets generally maintain a higher level of quality, thereby requiring less intensive filtering and deduplication compared to web data. A comprehensive list of the curated datasets used in our study is provided in Table [4](#), which details 75 datasets, including information on language, format, license, domain, and the number of documents/words, along with the filtering percentage. In the subsequent sections, we will conduct a detailed analysis of the most significant columns.

37.9% 23.9% 26.2% EN 37.9% EU24 26.2% XX 23.9% DE 5.2% IT 2.4% (a) Complete word distribution 46.2% 5.9% 9.2% 10.8% 12.3% DE 46.2% NO 12.3% IT 10.8% SVK 9.2% ES 5.9% SL 4.2% DA 3.9% (b) Word distribution excluding EN, EU24, and XX. 

## Licenses

As outlined in the data selection process (see Section 3.1.1), selecting datasets according to legal and licensing constraints is a crucial first step. Figure [3](#fig_2) illustrates the distribution of licenses across the curated datasets used in this project. The majority (52%) of the selected datasets' licenses are permissive and not restricted for commercial use, featuring licenses like CC0 (24%), CC BY (18.7%), MIT (5.3%), and Apache (4%) [25](#foot_18) . These licenses enable broad usage of the data, including for commercial purposes, and promote ease of dissemination and integration in various projects.

Conversely, 44% of the datasets are categorized under restrictive licenses, which limit their use, especially for commercial applications. These include licenses such as CC BY-NC-SA (6.7%), CC BY-NC (4%), and the "Various" category (14.7%), which represents datasets compiled from multiple sources and classified under the most restrictive license to mitigate legal risks.

An important observation is that many datasets did not explicitly mention a license on their official website. For these datasets, we conducted further investigation into the sources, often uncovering related licensing information. However, for some datasets (marked with a dagger in Table [4](#)), we were unable to directly determine the correct licenses despite extensive efforts.

## Languages

Our curated dataset encompasses 24 unique languages. For clarity, datasets that include a substantial portion of the official 24 EU languages are grouped under the category "EU24". Additionally, the analysis also features statistics for other European languages, such as Norwegian (NO). The label "XX" is used to indicate datasets focused on source code.

As shown in Figure [4a](#fig_3), English (EN) accounts for 37.9% of the total word count, followed by the EU24 languages at 26.2%, and source code (XX) at 23.9%. Together, these three categories make up 88% of the curated data. The inclusion of source code is particularly important in this context, as it provides curated, high-quality examples for training models on coding tasks.

Since many datasets feature multiple languages, specific per-language statistics are not always provided, but rather an overall distribution is emphasized. After removing the dominant categories (EN, EU24, and XX), Figure [4b](#fig_3) reveals that German (DE) constitutes 46.2% of the remaining data. This significant proportion reflects the regional focus of the project, as it is based in Germany, and emphasizes the role of localized datasets in shaping the overall corpus. Other notable languages include Norwegian, Italian, and Spanish, though their contributions remain relatively smaller compared to German.

## Analysis of Word Distribution and Dataset Representation in Multilingual Corpora

One would expect the word distribution to follow the number of datasets available; however, correlation analysis reveals this is only partially true. The Spearman correlation is moderate (0.562, p = 0.0028), while the Pearson correlation indicates a stronger linear relationship (0.855, P < 0.001). By using linear regression, we observe notable deviations in certain languages. For instance, German (DE) exhibits a significant negative residual, with the actual word count being 5.1 billion fewer than expected, despite having a high number of diverse datasets. This suggests that while German datasets are numerous, they tend to be smaller in size or less word-dense compared to other languages. On the other hand, English (EN) has a positive residual, showing 3.88 billion more words than predicted, which can be attributed to the oversampling of English datasets in the corpus. In contrast, French (FR) underperforms relative to its dataset count, with a negative residual of 1.54 billion, indicating that the French data is underrepresented in terms of word volume.

This analysis underscores the need for balanced sampling across languages. While oversampling English may provide more content for model training, undersampling key languages like French could result in biases that limit the multilingual capabilities of the models. Hence, we recommend strategic oversampling of underrepresented languages and careful moderation of overrepresented languages to ensure linguistic diversity and fairness in the curated dataset.

## Domains

The curated dataset spans multiple domains, each contributing a different share to the total word count. Table [1](#tab_1) provides a breakdown of the number of datasets, the average word count per dataset, total word count, and the percentage contribution of each domain.

Source code emerges as the largest domain by word count, accounting for 40.46% of the total curated data, with over 73 billion words. This is followed by law and administration, which contributes 20.40%, and the web domain at 13.69%. Collectively, these three domains represent 74.55% of the total word count. This strong presence of technical, legal, and digital content suggests that the curated dataset is well-suited for training models focused on tasks related to programming, legal reasoning, and web-based applications.

On the other hand, smaller domains such as culture (0.08%), recreation (0.08%), and knowledge base (0.23%) contribute much less to the overall dataset. These domains are likely underrepresented either due to the limited availability of datasets or their inherently smaller size.

## Sizes

The distribution of word counts within the curated dataset is highly skewed, with a few large datasets contributing the majority of the total word volume as shown in Figure [5](#fig_4). Initially, the largest contributors were StarCoder, EurLex, and MaCoCu, together accounting for 50% of the total word count. However, since StarCoder focuses on source code rather than natural language, it was excluded from further analysis, and the word count distribution was recalculated.

In the revised analysis, four datasets (peS2o, MaCoCu, Legal MC4, and EurLex ) now make up 50% of the total word count. Expanding this to 70%, the dataset contributions broaden to include three additional Pile subsets (PMC extracts, Openwebtext2, and Free Law Opinions V2 ), as well as Wikimedia Wikipedia. Notably, 17 datasets account for 90% of the total word count, underscoring the significant concentration of data within a limited number of large datasets.

Additionally, certain datasets display an unusually high word count relative to their document counts. A regression analysis was performed to better understand these discrepancies, calculating residuals to detect significant deviations from the expected document-to-word ratio. This analysis revealed the following outliers:

• Spanish legal corpora (ES): This legal corpus consists of only 15 documents but contains over 1.38 billion words. The residual analysis [(5.4)](#) shows that the word count is approximately 221 times higher than expected based on the number of documents. This suggests that individual documents within this corpus are unusually large.

• Projekt Gutenberg (EU24): A well-known collection of books, Projekt Gutenberg contributes over 3.37 billion words with only 60,912 documents. This results in a positive residual of 1.8, indicating that the dataset's word count per document is significantly higher than the regression model predicted. This observation aligns with expectations for book collections, as books typically contain more words per document compared to other formats such as articles or reports.

• Pile: PMC extracts (EN): A medical dataset with 2.8 million documents and over 12.1 billion words, PMC extracts exhibits a positive residual of 1.07. This indicates that the word count is significantly higher than predicted by the model. This is expected, as the dataset primarily contains full-length medical research articles, which are generally more detailed and contentrich compared to other document types.

## Filtering

Across the curated datasets, the average filtering percentage is 5.33% (±5.98%). Based on these values, filtering can be split in three categories:

• Low Filtering (< 1%): Datasets that underwent minimal filtering, typically because they contained well-structured, clean data to begin with.

• Medium Filtering (1% to 11.31%): The majority of datasets fall into this category, with moderate filtering applied to remove noise or short documents without heavily impacting the total word count.

• High Filtering (>11.31%): Datasets with a high filtering percentage, indicating substantial noise or irrelevant content.

In reviewing the impact of filtering, we found that documents with low language scores typically fall into one of four categories: i) mixed-language documents, ii) documents with conversion errors containing numerous special characters, iii) documents involving closely related languages (e.g., Croatian and Serbo-Croatian), and iv) documents in low-resourced languages where the content was misidentified as a different language.

To further understand the impact of filtering across datasets, we explored potential correlations between the percentage of filtered data and several key dataset characteristics: the number of words, number of documents, format, language, and domain. Our analysis showed only modest correlations in most cases, indicating that filtering tends to act somewhat independently of these factors (refer to Appendix 8.3.1 for a more detailed analysis).

## Summary

The curated dataset presents a comprehensive and diverse collection of 75 datasets spanning 25 languages, various domains, and multiple formats, offering a robust foundation for a wide range of research tasks. Despite some imbalances in distribution, particularly with large datasets dominating the total word count, the variety within the dataset makes it a valuable asset for training LLMs. The high presence of technical and legal data, alongside source code, reflects the strengths of this dataset in supporting tasks in these fields.

While filtering in general is necessary to ensure data quality, its impact on the curated datasets is relatively moderate, with only a few datasets requiring significant cleaning. The weak correlations found between filtering percentages and dataset characteristics suggest that filtering is driven more by the specificities of the dataset than by its size, format, or domain. In preparing data for training LLMs, we found that a strategic approach to selecting subsets is key. Leveraging the wide domain and language diversity can help ensure more balanced outcomes.

## Web Data

As previously mentioned (Section 3.1.2), the web data used in this project was sourced from Com-monCrawl. For this study, we utilized 60 distinct dumps from CommonCrawl, spanning a broad timeframe. The earliest dump was week [42 of 2014 (2014-42)](#), and the most recent dump was week 5 of 2023 . This extensive range allowed us to capture a wide variety of web content across nearly a decade, ensuring that the dataset reflects both historical and contemporary web information.

On average, each CommonCrawl dump contains approximately 2.7 billion documents. Across all 60 dumps, we accumulated a total of 173 billion documents, representing around 703 terabytes of raw, unprocessed data. This vast volume of data provided a strong foundation for training largescale language models but also introduced significant challenges in terms of data processing and filtering, as discussed in previous sections.

## Year Week

2014 42 [2015 14, 48 2016 22, 44 2017 13, 47, 51 2018 5, 9, 13, 17, 22, 26, 30, 34, 39, 43, 47, 51 2019 4, 9, 13, 18, 22, 26, 30, 35, 39, 47, 51 2020 5, 10, 16, 24, 29, 34, 40, 45,](#)  Table 2  presents the list of years and the corresponding weeks for each dump.

As can be seen from the list, the distribution of dumps across weeks is uneven. Earlier years, particularly 2014 through 2016, have fewer weeks represented compared to more recent years. However, the dumps from these earlier years typically contain a higher dump density, i.e. average document size (see Figure [6](#fig_5)). After 2017, CommonCrawl transitioned to a strategy of producing more frequent dumps, each with relatively smaller average document sizes.

This shift in strategy has important implications for data processing. Special attention must be paid to the earlier years [(2014)](#)[(2015)](#)[(2016)](#), where deduplication can be more computationally expensive due to the larger document sizes. In these cases, employing stricter filtering mechanisms may be necessary to ensure efficient processing and to avoid excessive resource consumption during deduplication.

Compute power Tracking the compute power required during data processing is essential for planning and securing the necessary resources. A well-structured plan not only ensures that adequate resources are available but also provides an opportunity to analyze computational efficiency and sustainability.

Below, we estimated total CPU hours consumed at each stage of the pipeline:

• Conversion: On average, converting a dump takes 115,2 CPU hours. For all the data combined this stage took 6,912 CPU hours.

• Filtering: On average, filtering one dump takes 763 CPU hours. For all dumps combined, this stage required 45,810 CPU hours.

• Deduplication: Deduplication is notably resource-intensive, taking an average of 3,680 CPU hours per dump. For the entire dataset, this stage consumed 221,230 CPU hours. Deduplication is by far the most time-consuming step, consuming 80.8% of the total compute power, which is why it is kept as the final stage in our pipeline (see Section 4.2.4). This is followed by filtering, which takes 16.7%, and conversion, accounting for 2.5%. Deduplication Effect Our deduplication methodology is based on a MinHash + Locality Sensitive Hashing approach that is explained in detail in [[34]](#b33), where the choice of algorithm, along with precision and recall metrics, are analyzed. In this section, we shift focus to examining how deduplication disproportionately affects different languages. Figure [7](#fig_6) illustrates the percentage of data removed after deduplication for each language, ranging from a minimum of 29.03% for Hungarian to a maximum of 68.95% for English. While it may seem intuitive to correlate the deduplication percentage with data availability, a Pearson correlation analysis reveals only a modest positive correlation of 0.54 (p = 0.006).

This suggests that some languages may be subject to an "unfair" amount of deduplication relative to their size. Although deduplication operates by identifying and removing similar content, this implies that certain languages contain a higher proportion of similar data, which may result in a final model that struggles to capture linguistic nuances.

To evaluate this disproportionality, we introduce the Deduplication Disparity Index (DDI), which measures the impact of deduplication on each language relative to its available web data. For each language l, the DDI is calculated as the Z-score of the deduplication-to-data ratio R l , which measures the percentage of data deduplicated in relation to the total web data available for that language:

$DDI l = R l -µ R σ R$, where R l = d l W l Here, d l is the deduplication percentage for language l, W l is the total number of web data words for language l, µ R is the average deduplication-to-data ratio across all languages, and σ R is the standard deviation of the deduplication-to-data ratios. This Z-score transformation centers the DDI around zero, highlighting languages where deduplication has a disproportionately high or low impact.

Languages with a high positive Z-score, such as Maltese (DDI mt = 4.519) and Croatian (DDI mt = 1.0735), are significantly impacted by deduplication, while others have scores closer to zero (DDI = -0.25 ± 0.012), indicating a more balanced deduplication-to-data ratio. This metric provides a systematic way to assess fairness in the data processing pipeline and highlights languages that might require further attention during preprocessing.

## Summary

The web data for this project was sourced from 60 distinct CommonCrawl dumps spanning 2014 2023, resulting in a total of 173 billion documents (703 terabytes of raw data). This broad timeframe captures both historical and contemporary web content.

Deduplication was the most resource-intensive step, consuming 80.8% of total CPU hours allocated for data processing. We introduced the Deduplication Disparity Index (DDI) to identify languages disproportionately affected by deduplication, highlighting the need for tailored processing strategies to ensure fairness and data quality.

## Insights and Observations

During the process of preparing the data for LLM training, several challenges emerged that shaped the development of the data pipeline. These challenges are important for understanding both the complexity of large-scale multilingual dataset processing and the technical, organizational, and legal obstacles one can encounter. By presenting these observations, we provide key findings and recommendations to guide future projects and to navigate similar challenges.

## Data Quality, Availability and Management

Data quality and availability are key criteria in the construction of datasets for language models. Ensuring high-quality data is essential for model performance, while the availability of data across different languages, genres, and domains presents unique challenges.

## Quality Assessment

The definition of high-quality data remains a complex and evolving topic in the field of LLMs, which has gained more traction as these models scale and diversify across languages and domains. Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent [[39,](#b38)[46]](#b46).

Traditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks [[17,](#b16)[31,](#b30)[49]](#b49). One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs [[45,](#b45)[28,](#b27)[13]](#b12).

However, this approach comes with several drawbacks. First, despite the promising results, a significant limitation is the lack of transparency in the LLM's mapping between defined criteria and system output in defining high-quality content. There is often no clear link between the model's concept of quality and concrete linguistic characteristics, such as sentence structure, readability, or word length. Additionally, running LLM inference on large datasets substantially increases computational demands, making this approach accessible only to those with significant resources. Finally, using LLMs to filter or label data introduce licensing challenges, particularly for commercial applications. We therefore recommend prioritizing more interpretable quality signals in data filtering.

These signals are directly mappable to tangible document characteristics, making them transparent and easy to justify. While LLM-based filtering techniques show promise, we believe there is a strong need for further research into how these models define high-quality data. At the very least, a hybrid approach combining both traditional and AI-based filtering could balance between efficiency, interpretability, and scalability.

## Availability of Multilingual Data

As seen in Section 5, one of the primary challenges is the significant imbalance in data availability across languages. This disparity makes it difficult to create training data distributions that ensure downstream models perform well in a wide variety of languages. High-resource languages like English dominate the dataset, while low-resource languages, such as Irish or Maltese, have far fewer available documents, thus requiring special consideration such as avoiding filtering and/or deduplicating to not further reduce the number of tokens available for training in these languages.

Versioning and Standardization Data versioning plays a crucial role in maintaining a clear record of changes and ensuring reproducibility throughout the pipeline. To facilitate reproducibility, we opted for a unified format (JSONL) across all datasets which makes it easier to track changes and compare versions, as any modifications can be identified and versioning information is represented consistently across changes. Furthermore, each step of our pipeline saves intermediate results, allowing for a granular approach to version control.

In addition to this, the intermediate results were moved to remote servers, separate from the training environment, to safeguard against data corruption or loss. While this precaution provides an extra layer of security, it does come at the cost of increased storage usage, which needs to be carefully managed. Nonetheless, data versioning supports tracking changes to the pipeline by storing the corresponding pipeline version in the metadata.

## Licensing and Compliance

A significant issue is the lack of clear licensing documentation. Researchers sometimes neglect to accurately represent the licenses under which their data falls, leaving legal ambiguities. While license identification remains a complex challenge [[43,](#b42)[18]](#b17), we strongly advocate for more precise legal reporting in dataset publications. Initiatives like the Dataset Cards by Hugging Face[foot_19](#foot_19) are a positive step toward improving licensing transparency. They allow dataset creators to include clear license information by default, promoting better legal clarity. However, it is important to note that adding a license in these dataset cards is not mandatory, which means some datasets may still be published without explicit legal documentation.

Even when data is available, special care must be taken to comply with the licenses under which it is provided. License compliance is crucial from the outset, as the licensing terms of any single dataset can impose restrictions on the downstream use of merged datasets or models trained on that data. Selecting even one dataset with a more restrictive license can constrain the licensing flexibility of all derived artifacts, including processed datasets and trained models. Ensuring that data sources have compatible and permissive licenses is therefore a critical early step in the project.

## Technical and Organizational Challenges

The successful creation of large-scale multilingual datasets involves navigating a range of technical and organizational challenges. Addressing these issues is essential for maintaining efficiency, ensuring data integrity, and fostering effective collaboration among diverse teams. The following sections explore the technical challenges of processing data at scale and the organizational aspects necessary for project success.

Processing at Scale Processing large-scale datasets presents significant technical challenges, both in terms of the associated costs and the complexity of managing software, hardware, and distributed systems. Large-scale computations often require access to multiple computing resources, which introduces additional complexities, such as varying system requirements, the need to transfer large volumes of data, and the maintenance of software across different platforms. Despite the availability of resources, managing distributed processing at scale remains a significant barrier, and exploring distributed training approaches may offer potential solutions for mitigating some of these difficulties in the future.

At the start of any project of this magnitude, it is crucial to consider the size of the datasets, the availability of storage and compute resources in the clusters, and the allocation of both CPU and GPU resources. One key aspect that is often overlooked is ensuring that the processed data is in the same location as the final model training environment, as transferring large datasets between different clusters can significantly impact efficiency and cost. However, budget or resources constraints may limit the ability to carry out both data processing and model training on the same cluster. In such cases, it becomes essential to carefully plan and optimize across these dimensions, considering factors such as storage, computational availability, and network transfer costs.

Deduplication Strategy In our project, a global deduplication approach (where deduplication is applied across multiple CommonCrawl dumps) was found to be impractical for two primary reasons. First, for high-resource languages like English, the combined dataset before deduplication is too large to be processed within our computational environments. Second, as noted in [[39]](#b38), global deduplication does not provide measurable benefits for downstream model performance.

Instead, a local deduplication strategy, applied per-dump and per-language, was adopted. This approach reduced the dataset size by approximately 30%, significantly improving processing efficiency in subsequent stages and enhancing the overall quality of the training data by minimizing redundancy while preserving diversity.

Organizational aspects For future large-scale data projects, it is essential to recognize the importance of managing diverse expertise, which spans scientific, technical, legal, and linguistic domains. Coordinating these competencies is crucial for the successful identification and evaluation of data sources, as well as for handling the vast volume of data. However, simply collecting data is not enough to ensure smooth operations or the sustainability of data pipelines and platforms.

We strongly recommend establishing robust data governance and lineage strategies at the outset of the project. Defining clear roles, responsibilities, and processes early on can significantly improve communication and collaboration across teams, aligning efforts as the project evolves. Implementing governance frameworks, such as those outlined in [[25]](#b24), can provide valuable structure, helping to maintain efficiency and ensure long-term success. Early planning in these areas will prevent organizational bottlenecks and provide a foundation for sustainable data management.

## Adapting to Rapid Innovation

The rapid pace of innovation in data processing methodologies for language models introduces a continual challenge. New tools and software frequently emerge, offering solutions to existing bottlenecks such as enhanced processing efficiency, improved storage capabilities, and advanced filtering techniques. However, integrating these innovations into established systems often necessitates workflow adaptations, retraining personnel, and potentially reconfiguring entire pipelines or rerunning pipelines on all datasets. This process can disrupt system stability while requiring considerable investment of time and resources.

Finding a balance between adopting innovations and maintaining operational consistency is essential. While the implementation of new software can result in performance improvements, it is critical to evaluate which components of the system should remain stable to ensure uninterrupted functionality. Furthermore, frequent updates (such as newly released curated datasets or Common-Crawl dumps) add to the complexity, as these updates necessitate continuous modifications to the data pipeline without undermining the overall coherence of the system.

A systematic approach to addressing this challenge involves organizing the pipeline into modular components from the outset. By clearly separating and labeling each segment of the pipeline, it becomes possible to identify which elements are adaptable to future innovations and which should remain static due to their critical nature. For instance, components such as deduplication processes may benefit from frequent optimization, whereas core elements like metadata normalization or key filtering mechanisms might need to remain constant to maintain reliability. Furthermore, depending on the stage of the project, it may be prudent to commit to a particular solution, even if suboptimal, rather than continuously reconfiguring the system.

## Conclusion and Outlook

In this paper, we presented the entire process of data preparation for the OpenGPT-X model family, covering every step from the initial selection of data to delivering the finalized datasets for model training. We outlined the requirements that guided our data selection, which conceptually split our sources into two categories: curated data and web data. This distinction shaped our approach to the implementation of our data pipelines: one for curated data, which required minimal filtering, and another for web data, which focused heavily on filtering and deduplication. In addition to providing a thorough description of our pipeline and data preparation methods, we included an indepth analysis of the resulting datasets. This analysis ensures transparency, aligning with European data recommendations and best practices in scientific research.

Finally, to contribute to the broader research field and promote openness, we highlighted the challenges faced during this project and the lessons learned. By sharing these insights, we aim to provide valuable guidance for future projects that undertake large-scale data preparation for multilingual language models. We utilized a subset of the preprocessed datasets detailed in this paper to pretrain large language models for the OpenGPTX project.

As part of our future research, we plan to investigate the use of synthetic data generation for the creation of LLM pretraining data and for advanced data quality filtering, e.g. based on LLMs as a judge. Furthermore, we are looking into generating training data that is compliant with current or upcoming laws and regulations (EU AI Act, GDPR), e.g. by removing or masking personally identifiable information. We intend to extend our collection of datasets, also targeting both datasets for underrepresented domains as well as languages. With additional processing requirements due to advanced processing techniques, and a growing number of available datasets, the demand for compute resources to generate high-quality training data will also increase.

We also examined the impact of format on filtering, but found no significant correlation (r = 0.03, p = 0.79). However, when considering the language of the dataset, we found a more notable correlation (r = 0.37, p = 0.001), hinting that certain languages may be more prone to needing extensive filtering, possibly due to differences in data quality or availability across languages.

Finally, domain showed no significant correlation with the percentage of data filtered (r = -0.004, p = 0.97).

These results suggest that filtering operates somewhat independently from these broader dataset characteristics. Language differences, in particular, seem to warrant more focused attention in future analyses, as they exhibit a stronger connection to filtering needs compared to other factors.

## List of Curated Datasets

The complete list of curated dataset is presented in Table [4](#) (next page).

Table [4](#): This table provides an overview of various multilingual datasets utilized in OpenGPT-X project. Each entry begins with the Corpus ID of the dataset and a link to its project page. The Language/s column specifies the languages included within each dataset (see Section 5.1). The Format of the datasets is also noted, indicating the file type, such as TXT, JSON, or XML. The License column outlines the legal terms governing the use of each dataset, for uncommon licenses a link is provided. The Domain column reflects the specific field or subject area that the dataset pertains to, such as Law, Math, or Medical. The # Docs column presents the total number of documents contained in each dataset, while the # Words column conveys the total word count in thousand. Finally, the % Filtered (Filt.) column indicates the percentage of documents that have been filtered out during preprocessing.

![Figure 1: Data pipeline for curated datasets: The pipeline involves format conversion, content normalization, followed by language detection, content normalization and filtering. The output is high-quality training data in JSONL format.]()

![Figure2: Overview of the Web Data Processing Pipeline: The pipeline processes WET files with Ungoliant by normalizing, computing quality warnings and harmful-perplexity, detecting language, and prefiltering. Afterwards, data is filtered based on quality and harmful-perplexity before deduplication, producing high-quality training data in JSONL format.]()

![Figure 3: Distribution of curated dataset licenses. Most datasets have permissive licenses (CC0, CC BY, MIT, Apache), while restrictive licenses are primarily represented by datasets sourced from multiple origins (Various).]()

![Figure 4: Word distribution across different languages in the curated datasets.]()

![Figure 5: Log-scaled comparison of word counts and document counts across datasets, sorted by word size. Some datasets exhibit a notably high word count despite having relatively few documents, indicating longer average document sizes.]()

![Figure 6: Analysis of CommonCrawl dumps used in the dataset, showing the distribution of dump weeks (blue), billion documents (red) and average dump density (yellow) per year.]()

![Figure 7: Percentage of data removed after deduplication for each language, showcasing the varying impact of deduplication.]()

![Total words, average words per dataset (Avg Words/DS), and documents for each domain, sorted by total words. significantly fewer words (712.8 million) compared to monolingual (4.38 billion) ones. Indeed, while contributing to language diversity, they often lack the volume of content found in larger monolingual datasets.]()

![List of dumps by year and week.Dumps In this section, we provide a detailed look at the CommonCrawl dumps used in our project. The dumps span multiple years, with varying distributions of weeks per year.]()

Meta restricts commercial use based on monthly active users and restricts the use of LLaMa model to improve other LLMs.

https://opengpt-x.de/en/

General Data Protection Regulation, available at: https://gdpr-info.eu/

https://github.com/huggingface/datatrove

https://github.com/google/cld3

https://dsi.ut-capitole.fr/blacklists/index_en.php

https://github.com/togethercomputer/RedPajama-Data

https://github.com/bitextor/warc2text

https://github.com/CLD2Owners/cld2

The amount of training data is typically measured by the number of tokens or words. However, the choice of a tokenizer affects the number of generated tokens, making absolute numbers for different models not directly comparable. Therefore, we use the wc command line utility to estimate the number of words.

While it is not always easy to detect AI-generated content, extra caution should be applied for datasets created after 2021 to avoid including synthetic data.

Following the schema https://data.CommonCrawl.org/crawl-data/CC-MAIN-<YEAR-MONTH>/index.html.

https://github.com/kermitt2/grobid

https://github.com/earwig/mwparserfromhell

Normalization Form KC as described in https://unicode.org/reports/tr15/#Norm_Forms

https://hplt-project.org/datasets/v1

Ungoliant supports the detection of 176 languages.

The percentage is calculated on a line-base, i.e. when the character '\n' is found.

To simplify aggregation, the license versions are omitted here and in Figure3. However, in Table4, the specific version numbers for all the datasets are clearly indicated.

https://huggingface.co/docs/hub/en/datasets-cards

