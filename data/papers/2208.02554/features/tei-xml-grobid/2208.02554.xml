<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vocabulary Transfer for Biomedical Texts: Add Tokens if You Can Not Add Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Priyanka</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Higher School of Economics St</orgName>
								<address>
									<settlement>Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mosin</forename><surname>Vladislav</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Higher School of Economics St</orgName>
								<address>
									<settlement>Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Leya</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Higher School of Economics St</orgName>
								<address>
									<settlement>Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ivan</forename><forename type="middle">P</forename><surname>Yamshchikov</surname></persName>
							<email>ivan.yamshchikov@thws.de</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CAIRO</orgName>
								<orgName type="institution" key="instit2">THWS Würzburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vocabulary Transfer for Biomedical Texts: Add Tokens if You Can Not Add Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF9EAAE9A4BA9FB4CE23151B5EDBE2C8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Working within specific NLP subdomains presents significant challenges, primarily due to a persistent deficit of data. Stringent privacy concerns and limited data accessibility often drive this shortage. Additionally, the medical domain demands high accuracy, where even marginal improvements in model performance can have profound impacts. In this study, we investigate the potential of vocabulary transfer to enhance model performance in biomedical NLP tasks. Specifically, we focus on vocabulary extension, a technique that involves expanding the target vocabulary to incorporate domain-specific biomedical terms. Our findings demonstrate that vocabulary extension, leads to measurable improvements in both downstream model performance and inference time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Transformer architecture, introduced by <ref type="bibr" target="#b12">Vaswani et al. (2017)</ref>, has revolutionized natural language processing across various domains. In the biomedical field, several Transformer-based models have been specifically tailored for biomedical corpora, including BioBERT <ref type="bibr" target="#b7">(Lee et al., 2020)</ref>, PubMedBERT <ref type="bibr" target="#b3">(Gu et al., 2021)</ref>, ClinicalBERT <ref type="bibr" target="#b5">(Huang et al., 2019)</ref>, and MedGPT <ref type="bibr" target="#b14">(Zuo et al., 2019)</ref>, among others. The complexity of tokenization in biomedical texts arises from multiple factors. Biomedical language often diverges significantly from general English in both syntax and lexicon, frequently incorporating complex compound terms, non-standard abbreviations, and specialized terminologies that reflect the field's dynamic and rapidly evolving nature. Biomedical literature often includes acronyms, abbreviations, digits, internal capitalization, special characters, and structured information like medical codes and timestamps.</p><p>This paper explores applicability of vocabulary transfer introduced in Mosin et al. <ref type="bibr">(2023)</ref> to biomedical domain. Traditionally, language models employ the same tokenization method during both initial training and subsequent fine-tuning, typically encompassing thousands of tokens. These tokens can vary from subword units to full words. However, <ref type="bibr" target="#b8">Mosin et al. (2023)</ref> propose that developing a new, task-specific tokenization strategy during the fine-tuning stage may significantly improve model performance.</p><p>Vocabulary transfer becomes particularly particularly advantageous when the fine-tuning dataset differs substantially from the one used in initial training. Research in biomedical tokenization has highlighted several problematic cases that exemplify the challenges inherent in this domain, for detailed examples see Noa P. Cruz Díaz <ref type="bibr">(2015)</ref>.</p><p>Though in this paper we experiment with biomedical data we believe that similar vocabulary extension approach would be beneficial for other NLP domains where the data is scarce. <ref type="bibr" target="#b2">(Gee et al., 2022)</ref> has demonstrated the vocabulary transfer could be beneficial for model compression in business applications. <ref type="bibr" target="#b13">(Yamshchikov et al., 2022)</ref> has shown the benefits of vocabulary transfer when using the model trained on the modern Greek texts on the historical texts in ancient Greek, while <ref type="bibr" target="#b11">(Remy et al., 2024;</ref><ref type="bibr" target="#b0">Alexandrov et al., 2024)</ref> show its benefits when working with low-resource languages. This paper demonstrates that vocabulary transfer is applicable to biomedical texts and can bring certain benefits. We also demonstrate that increasing vocabulary size, i.e. vocabulary extension, during vocabulary transfer significantly improves downstream performance. We believe this result is not limited to biomedical data but would hold on any other specific domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Vocabulary transfer</head><p>In their study, <ref type="bibr" target="#b8">Mosin et al. (2023)</ref> introduce the concept of vocabulary transfer. Let V denote the original vocabulary obtained during the pretraining arXiv:2208.02554v3 [cs.CL] 16 Nov 2024 phase, comprising M tokens denoted as {t k , v k }, where t k is a text segment forming a token, and v k is the corresponding embedding for that token. Then V represents the new vocabulary utilized during fine-tuning, consisting of N tokens denoted as { t k , v k }, where t k is a text segment forming a new token, and v k is its corresponding embedding. This customized tokenization strategy in the fine-tuning phase facilitates improved model performance on specific tasks or datasets.</p><p>To transfer pretrained knowledge from existing tokens to new, corpus-specific tokens, a heuristic token-matching procedure can be employed. In this paper, we evaluate two token initialization heuristics. First, if a token in the new vocabulary directly matches a token in the original vocabulary, its corresponding embedding is assigned to the new token. We refer to this approach as matched vocabulary transfer. Additionally, some new tokens may be decomposable into partitions of multiple tokens from the original vocabulary. For each such token in the new vocabulary, we generate all possible partitions comprising tokens from the original vocabulary and select the partition with the minimal number of tokens. If multiple partitions have the same number of tokens, we choose the one containing the longest token. The embedding for the new token is then initialized by averaging the embeddings of the tokens in the selected partition. We refer to this approach as averaged transfer. These methods correspond to those described by <ref type="bibr" target="#b8">Mosin et al. (2023)</ref>, where matched aligns with the "Match Old Tokens" strategy and averaged corresponds to "VIPI".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>We conduct experiments on text classification as a downstream task within the biological domain <ref type="bibr" target="#b9">(Mujtaba et al., 2019;</ref><ref type="bibr" target="#b1">Gao et al., 2021;</ref><ref type="bibr" target="#b6">Hughes et al., 2017)</ref> using two datasets: OHSUMED <ref type="bibr" target="#b4">(Hersh et al., 1994)</ref>, a medical dataset for the classification of cardiovascular diseases, and the Kaggle Medical Texts Dataset 1 , which classifies various patient conditions including digestive system diseases, cardiovascular diseases, neoplasms, nervous system diseases, and general pathological conditions. The downstream dataset was split into 80%, 10% dev, and 10% test. Our results demonstrate that increasing the number of tokens enhances classifier accuracy when using masked language modeling (MLM) and vocabulary transfer before downstream 1 <ref type="url" target="https://www.kaggle.com/chaitanyakck/medical-text">https://www.kaggle.com/chaitanyakck/medical-text</ref> classification tasks.</p><p>Table <ref type="table">1</ref> summarizes the parameters of the datasets that we experiment with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Numer of Number of Records Labels OHSUMED 13 929 23 Kaggle 28 880 5</p><p>Table <ref type="table">1</ref>: Parameters of the datasets used for experiments with the number of records and the number of labels.  Table 2: Number of Text Data in each Label of Kaggle dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted a series of experiments using the base version of the BERT model on various medical datasets. Initially, we explored the adoption of a new, dataset-specific vocabulary through an intermediary masked language modeling (MLM) step, involving pretraining on the downstream dataset with updated tokenization. Subsequently, we performed multiple experiments with different parameters and compared the results to a baseline approach, which involved simple fine-tuning on the downstream dataset without applying vocabulary transfer or altering the initial tokenization. This baseline serves as a reference point in our experimental analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fine-tuning Transferred Vocabulary and Change in Classifier Accuracy</head><p>In these experiments, we investigated the limitations of simple token matching in vocabulary transfer and assessed the impact of masked language modeling (MLM) step on the overall performance after vocabulary transfer. We also experiment with the size of the final vocabulary. First, merely assigning new embeddings to tokens is insufficient for enhancing model performance. Table <ref type="table">3</ref> presents the relative change in accuracy of downstream classifiers for a medical dataset with five classes without intermediate MLM step and with it. It stands to reason that MLM is important since it allows the model to adapt to new data-set specific tokenization.</p><p>Table <ref type="table">3</ref> also llustrates the impact of vocabulary extension. Indeed, reducing the vocabulary size from 16,000 to 8,000 tokens results in a minor accuracy decrease of 0.26% for VIPI alone, while the MLM+VIPI approach experiences a more pronounced decline of 4.03%. In contrast, increasing the vocabulary size to 32,000 tokens leads to a 1.24% decrease in accuracy for VIPI alone, but a 2.16% improvement with the MLM+VIPI method. This trend continues at 64,000 tokens, where VIPI alone decreases by 3.45%, whereas the MLM+VIPI approach results in a 2.51% improvement.</p><p>These findings suggest that expanding vocabu-lary size enhances model performance. This indicates that MLM effectively prepares the model to leverage larger vocabularies in domain-specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Vocabulary Size and Inference time</head><p>In the medical domain, inference time might be critically important as it directly influences the speed and efficiency of healthcare analysis. Since patient data is highly sensitive and medical emergencies might occur in various conditions, local inference on diagnostic device rather than server-based inference might be vastly beneficial. This makes inference speed and efficiency paramount. Rapid inference facilitates real-time decision-making, swift processing of medical data, and timely responses in emergencies. Moreover, efficient inference optimizes resource utilization, accelerates patient care workflows, and enhances the overall experience for healthcare professionals. Faster inference times contribute to a more responsive, accessible, and effective healthcare system, ultimately improving patient care and outcomes, particularly in timesensitive medical analyses. Vocabulary size has a direct impact on inference time. In our experiment, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, we found that using a classifier with a larger vocabulary leads to increased inference time if the intermediary MLM step was not performed. However, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, incorporating a masked language model (MLM) training after vocabulary transfer results in decreased inference time as the vocabulary size increases.While larger vocabulary sizes generally increases inference time incorporating an MLM step allows the efficient handling of domain-specific vocabularies.This optimization reduces the computational burden during tokenization. Naturally, MLM enhances compression enabling the moder to process domain-specific data more efficiently throughout the all stages such as tokenization, embedding and final prediction generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Several key aspects of vocabulary transfer for medical texts are evident from our analysis. First, the intermediary MLM step proves beneficial. Our further experiments show that even when applied to pre-existing tokenization, MLM on downstream data before training the classifier provides certain benefits. Likely, tokens rare in standard English Vocabulary size Change of accuracy Change of accuracy transfer only transfer and MLM 16000 → 8000 -0.26% -4.03% 16000 → 32000 -1.24% +2.16% 16000 → 64000 -3.45% +2.51%</p><p>Table <ref type="table">3</ref>: Relative change in downstream classifier accuracy and the impact of corpus-specific tokenization on a Kaggle medical dataset compared to standard fine-tuning with 16,000 tokens.  Our findings also shows that the MLM step not only improves the model's classification performance but also leads to a reduction in inference time. By this we could say that MLM step compresses the token representation and optimizes the model for faster interference, even when the vocabulary size is significantly increased. By optimizing inference time, we can develop more efficient and responsive models. While more advanced and robust procedures may exist, our findings suggest that even a straightforward approach to vocabulary transfer in medical NLP can be significantly enhanced by expanding the vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper demonstrates the potential benefits of vocabulary transfer in medical natural language processing. We analyze the impact of various stages of vocabulary transfer on classification performance using medical datasets. Our findings indicate that increasing the vocabulary size leads to improved model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Our experiments are limited to text classification tasks using the base version of the BERT model tested on several specific datasets. We do anticipate that the proposed approach could be beneficial for other models and subdomains, since vocabulary transfer seem to have demonstrably similar effects in various domains, see <ref type="bibr" target="#b13">(Yamshchikov et al., 2022;</ref><ref type="bibr" target="#b0">Alexandrov et al., 2024;</ref><ref type="bibr" target="#b2">Gee et al., 2022)</ref>. However, when working with models of bigger scale then BERT the effects of vocabulary transfer might be more or less pronounced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>This paper complies with the ACL Ethics Policy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance on Ohsumed data, vocabulary size is 32 000</figDesc><graphic coords="2,315.28,235.24,200.00,200.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance on Kaggle Medical dataset, vocabulary size is 32 000</figDesc><graphic coords="3,80.00,70.86,200.00,200.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Change of classifier accuracy on Kaggle Medical dataset, inference time with respect to vocabulary size + VIPI only.</figDesc><graphic coords="4,73.98,193.61,212.04,140.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relative change in accuracy of downstream classifiers on Kaggle Medical dataset, inference time with respect to vocabulary size after MLM and VIPI</figDesc><graphic coords="4,73.98,397.02,212.04,140.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>summarizes the total number of frequencies in each label in Kaggle dataset.</figDesc><table><row><cell>Label</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="6">Frequency 3163 1494 1925 3051 4805</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Mr. Pavel Chizhov</rs> and <rs type="person">Mr. Alexey Tikhonov</rs> for their advice, productive ideas and support.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Anton</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Niklas Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08699</idno>
		<title level="m">Mitigating catastrophic forgetting in language transfer via model merging</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Limitations of transformers on clinical text classification</title>
		<author>
			<persName><forename type="first">Shang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alawad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Gounley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Schaefferkoetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Cheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">B</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoinette</forename><surname>Stroup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3596" to="3607" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast vocabulary transfer for language model compression</title>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Zugarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Rigutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ohsumed: An interactive retrieval evaluation and new large test collection for research</title>
		<author>
			<persName><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName><surname>Hickam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;94</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<title level="m">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Medical text classification using convolutional neural networks. In Informatics for Health: Connected Citizen-Led Wellness and Population Health</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Kotoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IOS Press</publisher>
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fine-tuning transformers: Vocabulary transfer</title>
		<author>
			<persName><forename type="first">Igor</forename><surname>Vladislav Mosin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borislav</forename><surname>Samenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kozlovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><forename type="middle">P</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName><surname>Yamshchikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">317</biblScope>
			<biblScope unit="page">103860</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Clinical text classification research trends: Systematic literature review and open issues</title>
		<author>
			<persName><forename type="first">Ghulam</forename><surname>Mujtaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyana</forename><surname>Shuib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norisma</forename><surname>Idris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam Hoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Gopal Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Khowaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khairunisa</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Friday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nweke</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="494" to="520" />
		</imprint>
	</monogr>
	<note>Expert systems with applications</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analysis of biomedical tokenization: Problems and strategies</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Maña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">López</forename><surname>Noa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Cruz</forename><surname>Díaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis</title>
		<meeting>the Sixth International Workshop on Health Text Mining and Information Analysis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Trans-tokenization and cross-lingual vocabulary transfers: Language adaptation of llms for low-resource nlp</title>
		<author>
			<persName><forename type="first">François</forename><surname>Remy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Delobelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayastan</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfiya</forename><surname>Khabibullina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miryam</forename><surname>De Lhoneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.04303</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bert in plutarch&apos;s shadows</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Yamshchikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yorgos</forename><surname>Pantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Jost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6071" to="6080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rucir at trec 2019: Conversational assistance track</title>
		<author>
			<persName><forename type="first">Xiaochen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename></persName>
		</author>
		<idno type="DOI">10.1145/1122445.1122456</idno>
	</analytic>
	<monogr>
		<title level="m">28th Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>TREC 2019 -Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
