{
  "arxivId": "2303.09489",
  "title": "Effectively Modeling Time Series with Simple Discrete State Spaces",
  "authors": "Michael Zhang, Khaled K. Saab, Michael Poli, Tri Dao, Karan Goel, Christopher R\u00e9",
  "abstract": "Time series modeling is a well-established problem, which often requires that\nmethods (1) expressively represent complicated dependencies, (2) forecast long\nhorizons, and (3) efficiently train over long sequences. State-space models\n(SSMs) are classical models for time series, and prior works combine SSMs with\ndeep learning layers for efficient sequence modeling. However, we find\nfundamental limitations with these prior approaches, proving their SSM\nrepresentations cannot express autoregressive time series processes. We thus\nintroduce SpaceTime, a new state-space time series architecture that improves\nall three criteria. For expressivity, we propose a new SSM parameterization\nbased on the companion matrix -- a canonical representation for discrete-time\nprocesses -- which enables SpaceTime's SSM layers to learn desirable\nautoregressive processes. For long horizon forecasting, we introduce a\n\"closed-loop\" variation of the companion SSM, which enables SpaceTime to\npredict many future time-steps by generating its own layer-wise inputs. For\nefficient training and inference, we introduce an algorithm that reduces the\nmemory and compute of a forward pass with the companion matrix. With sequence\nlength $\\ell$ and state-space size $d$, we go from $\\tilde{O}(d \\ell)$\nna\\\"ively to $\\tilde{O}(d + \\ell)$. In experiments, our contributions lead to\nstate-of-the-art results on extensive and diverse benchmarks, with best or\nsecond-best AUROC on 6 / 7 ECG and speech time series classification, and best\nMSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1)\nfits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more\naccurately on longer horizons than prior state-of-the-art, and (3) speeds up\ntraining on real-world ETTh1 data by 73% and 80% relative wall-clock time over\nTransformers and LSTMs.",
  "url": "https://arxiv.org/abs/2303.09489",
  "issue_number": 394,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/394",
  "created_at": "2025-01-04T15:02:18.932613",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-28T07:09:58.237Z",
  "main_tex_file": null,
  "published_date": "2023-03-16T17:08:21Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI"
  ]
}