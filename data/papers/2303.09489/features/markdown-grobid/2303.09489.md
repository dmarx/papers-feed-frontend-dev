# Effectively Modeling Time Series with Simple Discrete State Spaces

## Abstract

## 

Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -a canonical representation for discrete-time processes-which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a "closed-loop" variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layerwise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length and state-space size d, we go from Õ(d ) naïvely to Õ(d + ). In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR(p) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73% and 80% relative wall-clock time over Transformers and LSTMs.

* Equal Contribution. Order determined by forecasting competition.

## Introduction

Time series modeling is a well-established problem, with tasks such as forecasting and classification motivated by many domains such as healthcare, finance, and engineering [[63]](#b62). However, effective time series modeling presents several challenges:

• First, methods should expressively capture complex, long-range, and autoregressive dependencies. Time series data often reflects higher order dependencies, seasonality, and trends, governing how past samples determine future terms [[10]](#b9). This motivates many classical approaches that model these properties [[8,](#b7)[75]](#b74), alongside expressive deep learning mechanisms such as attention [[70]](#b69) and fully connected layers that model interactions between every sample in an input sequence [[78]](#b77).

• Second, methods should be able to forecast a wide range of long horizons over various data domains. Reflecting real world demands, popular forecasting benchmarks evaluate methods on 34 different tasks [[23]](#b22) and 24-960 time-step horizons [[80]](#b79). Furthermore, as testament to accurately learning time series processes, forecasting methods should ideally also be able to predict future time-steps on horizons they were not explicitly trained on.

• Finally, methods should be efficient with training and inference. Many time series applications require processing very long sequences, e.g., classifying audio data with sampling rates up to 16,000 Hz [[73]](#b72). To handle such settings-where we still need large enough models that can expressively model this data-training and inference should ideally scale subquadratically with sequence length and model size in time and space complexity.

Unfortunately, existing time series methods struggle to achieve all three criteria. Classical methods (c.f., ARIMA [[8]](#b7), exponential smoothing (ETS) [[75]](#b74)) often require manual data preprocessing and model selection to identify expressive-enough models. Deep learning methods commonly train to predict specific horizon lengths, i.e., as direct multi-step forecasting [[13]](#b12), and we find this hurts their ability to forecast longer horizons (Sec. 4.2.2). They also face limitations achieving high expressivity and efficiency. Fully connected networks (FCNs) such as NLinear [[78]](#b77) scale quadratically in O( h) space complexity (with input length and forecast length h). Recent Transformer-based models reduce this complexity to O( + h), but do not always outperform the aforementioned fully connected networks on forecasting benchmarks [[47,](#b46)[80]](#b79).

We thus propose SpaceTime, a deep state-space architecture for effective time series modeling. To achieve this, we focus on improving each criteria via three core contributions:

1. For expressivity, our key idea and building block is a linear layer that models time series processes as state-space models (SSMs) via the companion matrix (Fig. [1](#)). We start with SSMs due to their connections to both classical time series analysis [[32,](#b31)[41]](#b40) and recent deep learning advances [[27]](#b26).

Classically, many time series models such as ARIMA and exponential smoothing (ETS) can be expressed as SSMs [[8,](#b7)[75]](#b74). Meanwhile, recent state-of-the-art deep sequence models [[27]](#b26) have used SSMs to outperform Transformers and LSTMs on challenging long-range benchmarks [[68]](#b67). Their primary innovations show how to formulate SSMs as neural network parameters that are practical to train. However, we find limitations with these deep SSMs for time series data. While we build on their advances, we prove that these prior SSM representations [[27,](#b26)[28,](#b27)[31]](#b30) cannot capture autoregressive processes fundamental for time series. We thus specifically propose the companion matrix representation for its expressive and memory-efficient properties. We prove that the companion matrix SSM recovers fundamental autoregressive (AR) and smoothing processes modeled in classical techniques such as ARIMA and ETS, while only requiring O(d) memory to represent an O(d 2 ) matrix. Thus, SpaceTime inherits the benefits of prior SSM-based sequence models, while introducing improved expressivity that recovers fundamental time series processes simply through its layer weights.

2. For forecasting long horizons, we introduce a new "closed-loop" view of SSMs. Prior deep SSM architectures either apply the SSM as an "open-loop" [[27]](#b26), where fixed-length inputs necessarily generate same-length outputs, or use closed-loop autoregression where final layer outputs are fed through the entire network as next-time-step inputs [[24]](#b23). We describe issues with both approaches in Sec. 3.2, and instead achieve autogressive forecasting in a deep network with only a single Figure [1](#): We learn time series processes as state-space models (SSMs) (top left). We represent SSMs with the companion matrix, which is a highly expressive representation for discrete time series (top middle), and compute such SSMs efficiently as convolutions or recurrences via a shift + low-rank decomposition (top right). We use these SSMs to build SpaceTime, a new time series architecture broadly effective across tasks and domains (bottom).

SSM layer. We do so by explicitly training the SSM layer to predict its next time-step inputs, alongside its usual outputs. This allows the SSM to recurrently generate its own future inputs that lead to desired outputs-i.e., those that match an observed time series-so we can forecast over many future time-steps without explicit data inputs.

3. For efficiency, we introduce an algorithm for efficient training and inference with the companion matrix SSM. We exploit the companion matrix's structure as a "shift plus low-rank" matrix, which allows us to reduce the time and space complexity for computing SSM hidden states and outputs from Õ(d ) to Õ(d + ) in SSM state size d and input sequence length .

In experiments, we find SpaceTime consistently obtains state-of-the-art or near-state-of-the-art results, achieving best or second-best AUROC on 6 out of 7 ECG and audio speech time series classification tasks, and best mean-squared error (MSE) on 14 out of 16 Informer benchmark forecasting tasks [[80]](#b79). SpaceTime also sets a new best average ranking across 34 tasks on the Monash benchmark [[23]](#b22). We connect these gains with improvements on our three effective time series modeling criteria. For expressivity, on synthetic ARIMA processes SpaceTime learns AR processes that prior deep SSMs cannot. For long horizon forecasting, SpaceTime consistently outperforms prior state-of-the-art on the longest horizons by large margins. SpaceTime also generalizes better to new horizons not used for training. For efficiency, on speed benchmarks SpaceTime obtains 73% and 80% relative wall-clock speedups over parameter-matched Transformers and LSTMs respectively, when training on real-world ETTh1 data.

## Preliminaries

Problem setting. We evaluate effective time series modeling with classification and forecasting tasks. For both tasks, we are given input sequences of "look-back" or "lag" time series samples u t-:t-1 = (u t-, . . . , u t-1 ) ∈ R ×m for sample feature size m. For classification, we aim to classify the sequence as the true class y out of possible classes Y. For forecasting, we aim to correctly predict H future time-steps over a "horizon" y t,t+h-1 = (u t , . . . , u t+h-1 ) ∈ R h×m .

State-space models for time series. We build on the discrete-time state-space model (SSM), which maps observed inputs u k to hidden states x k , before projecting back to observed outputs y k

$x k+1 = Ax k + Bu k(1)$$y k = Cx k + Du k(2)$where

$A ∈ R d×d , B ∈ R d×m , C ∈ R m ×d$, and D ∈ R m ×m . For now, we stick to single-input single-output conventions where m, m = 1, and let D = 0. To model time series in the single SSM setting, we treat u and y as copies of the same process, such that

$y k+1 = u k+1 = C(Ax k + Bu k )(3)$We can thus learn a time series SSM by treating A, B, C as black-box parameters in a neural net layer, i.e., by updating A, B, C via gradient descent s.t. with input u k and state x k at time-step k, following (3) predicts ŷk+1 that matches the next time-step sample y k+1 = u k+1 . This SSM framework and modeling setup is similar to prior works [[27,](#b26)[28]](#b27), which adopt a similar interpretation of inputs and outputs being derived from the "same" process, e.g., for language modeling. Here we study and improve this framework for time series modeling. As extensions, in Sec. 3.1.1 we show how (1) and ( [2](#formula_1)) express univariate time series with the right A representation. In Sec. 3.1.2 we discuss the multi-layer setting, where layer-specific u and y now differ, and we only model first layer inputs and last layer outputs as copies of the same time series process.

## Method: SpaceTime

We now present SpaceTime, a deep architecture that uses structured state-spaces for more effective time-series modeling. SpaceTime is a standard multi-layer encoder-decoder sequence model, built as a stack of repeated layers that each parametrize multiple SSMs. We designate the last layer as the "decoder", and prior layers as "encoder" layers. Each encoder layer processes an input time series sample as a sequence-to-sequence map. The decoder layer then takes the encoded sequence representation as input and outputs a prediction (for classification) or sequence (for forecasting).

Below we expand on our contributions that allow SpaceTime to improve expressivity, longhorizon forecasting, and efficiency of time series modeling. In Sec. 3.1, we present our key building block, a layer that parametrizes the companion matrix SSM (companion SSM) for expressive autoregressive modeling. In Sec. 3.2, we introduce a specific instantiation of the companion SSM to flexibly forecast long horizons. In Sec. 3.3, we provide an efficient inference algorithm that allows SpaceTime to train and predict over long sequences in sub-quadratic time and space complexity.

## The Multi-SSM SpaceTime layer

We discuss our first core contribution and key building block of our model, the SpaceTime layer, which captures the companion SSM 's expressive properties, and prove that the SSM represents multiple fundamental processes. To scale up this expressiveness in a neural architecture, we then go 

## Expressive State-Space Models with the Companion Matrix

For expressive time series modeling, our SSM parametrization represents the state matrix A as a companion matrix. Our key motivation is that A should allow us to capture autoregressive relationships between a sample u k and various past samples u k-1 , u k-2 , . . . , u k-n . Such dependencies are a basic yet essential premise for time series modeling; they underlie many fundamental time series processes, e.g., those captured by standard ARIMA models. For example, consider the simplest version of this, where u k is a linear combination of p prior samples (with coefficients φ 1 , . . . , φ p )

$u k = φ 1 u k-1 + φ 2 u k-2 + . . . φ p u k-p(4)$i.e., a noiseless, unbiased AR(p) process in standard ARIMA time series analysis [[8]](#b7).

To allow (3) to express (4), we need the hidden state x k to carry information about past samples. However, while setting the state-space matrices as trainable neural net weights may suggest we can learn arbitrary task-desirable A and B via supervised learning, prior work showed this could not be done without restricting A to specific classes of matrices [[28,](#b27)[31]](#b30).

Fortunately, we find that a class of relatively simple A matrices suffices. We propose to set A ∈ R d×d as the d × d companion matrix, a square matrix of the form:

$(Companion Matrix) A =        0 0 . . . 0 a 0 1 0 . . . 0 a 1 0 1 . . . 0 a 2 . . . . . . . . . . . . 0 0 . . . 1 a d-1        i.e., A i,j =      1 for i -1 = j a i for j = d -1 0 otherwise(5)$Then simply letting state dimension d = p, assuming initial hidden state x 0 = 0, and setting

$a := a 0 a 1 . . . a d-1 T = 0, B = 1 0 . . . 0 T , C = φ 1 . . . φ p$allows the discrete SSM in (1, 2) to recover the AR(p) process in [(4)](#b3). We next extend this result in Proposition 1, proving in App. B that setting A as the companion matrix allows the SSM to recover a wide range of fundamental time series and dynamical system processes beyond the AR(p) process.

Proposition 1. A companion state matrix SSM can represent ARIMA [[8]](#b7), exponential smoothing [[37,](#b36)[75]](#b74), and controllable linear time-invariant systems [[11]](#b10).

As a result, by training neural network layers that parameterize the companion SSM, we provably enable these layers to learn the ground-truth parameters for multiple time series processes. In addition, as we only update a ∈ R d (5), we can efficiently scale the hidden-state size to capture more expressive processes with only O(d) parameters. Finally, by learning multiple such SSMs in a single layer, and stacking multiple such layers, we can further scale up expressivity in a deep architecture.

Prior SSMs are insufficient. We further support the companion SSM by proving that existing related SSM representations used in [[1,](#b0)[27,](#b26)[31,](#b30)[65]](#b64) cannot capture the simple yet fundamental AR(p) process. Such works, including S4 and S4D, build on the Linear State-Space Layer (LSSL) [[28]](#b27), and cannot represent AR processes due to their continuous-time or diagonal parametrizations of A. We defer the proof to App. B.1. In Sec. 4.2.1, we empirically support this analysis, showing that these prior SSMs fit synthetic AR processes less accurately than the companion SSM. This suggests the companion matrix resolves a fundamental limitation in related work for time series.

## Layer Architecture and Multi-SSM Computation

Architecture. To capture and scale up the companion SSM's expressive and autoregressive modeling capabilities, we model multiple companion SSMs in each SpaceTime layer's weights. SpaceTime layers are similar to prior work such as LSSLs, with A, B, C as trainable weights, and D added back as a skip connection. To model multiple SSMs, we add a dimension to each matrix. For s SSMs per SpaceTime layer, we specify weights A ∈ R s×d×d , B ∈ R d×s , and C ∈ R s×d . Each slice in the s dimension represents an individual SSM. We thus compute s outputs and hidden states in parallel by following (1) and ( [2](#formula_1)) via simple matrix multiplications on standard GPUs.

To model dependencies across individual SSM outputs, we optionally follow each SpaceTime layer with a one-layer nonlinear feedforward network (FFN). The FFN thus mixes the m outputs across a SpaceTime layer's SSMs, allowing subsequent layers to model dependencies across SSMs.

Computation. To compute the companion SSM, we could use the recurrence in (1). However, this sequential operation is slow on modern GPUs, which parallelize matrix multiplications. Luckily, as described in [[27]](#b26) we can also compute the SSM as a 1-D convolution. This enables parallelizable inference and training. To see how, note that given a sequence with at least k inputs and hidden state x 0 = 0, the hidden state and output at time-step k by induction are:

$x k = k-1 j=0 A k-1-j Bu j and y k = k-1 j=0 CA k-1-j Bu j(6)$We can thus compute hidden state x k and output y k as 1-D convolutions with "filters" as

$F x = (B, AB, A 2 B, . . . , A -1 B) (Hidden State Filter)(7)$F y = (CB, CAB, CA 2 B, . . . , CA -1 B) (Output Filter) (8)

$x k = (F x * u)[k] and y k = (F y * u)[k](9)$So when we have inputs available for each output (i.e., equal-sized input and output sequences) we can obtain outputs by first computing output filters F y (8), and then computing outputs efficiently with the Fast Fourier Transform (FFT). We thus compute each encoder SSM as a convolution.

For now we note two caveats. Having inputs for each output is not always true, e.g., with long horizon forecasting. Efficient inference also importantly requires that F y can be computed efficiently, but this is not necessarily trivial for time series: we may have long input sequences with large k.

Fortunately we later provide solutions for both. In Sec. 3.2, we show how to predict output samples many time-steps ahead of our last input sample via a "closed-loop" forecasting SSM. In Sec. 3.3 we show how to compute both hidden state and output filters efficiently over long sequences via an efficient inference algorithm that handles the repeated powering of A k .

## Built-in Data Preprocessing with Companion SSMs

We now show how beyond autoregressive modeling, the companion SSM also enables SpaceTime layers to do standard data preprocessing techniques used to handle nonstationarities. Consider differencing and smoothing, two classical techniques to handle nonstationarity and noise:

$u k = u k -u k-1 (1st-order differencing) u k = 1 n n-1 i=0$u k-i (n-order moving average smoothing)

We explicitly build these preprocessing operations into a SpaceTime layer by simply initializing companion SSM weights. Furthermore, by specifying weights for multiple SSMs, we simultaneously perform preprocessing with various orders in one forward pass. We do so by setting a = 0 and B = [1, 0, . . . , 0] T , such that SSM outputs via the convolution view [(6)](#b5) are simple sliding windows / 1-D convolutions with filter determined by C. We can then recover arbitrary n-order differencing or average smoothing via C weight initializations, e.g., (see App. D.7.1 for more examples), C = 1 -2 1 0 0 . . . 0 1/n . . . 1/n 0 0 . . . 0 (2nd-order differencing) (n-order moving average smoothing) (10)

## Long Horizon Forecasting with Closed-loop SSMs

We now discuss our second core contribution, which enables long horizon forcasting. Using a slight variation of the companion SSM, we allow the same constant size SpaceTime model to forecast over many horizons. This forecasting SSM recovers the flexible and stateful inference of RNNs, while retaining the faster parallelizable training of computing SSMs as convolutions.

Challenges and limitations. For forecasting, a model must process an input lag sequence of length and output a forecast sequence of length h, where h = necessarily. Many state-of-the-art neural nets thus train by specifically predicting h-long targets given -long inputs. However, in Sec. 4.2.2 we find this hurts transfer to new horizons in other models, as they only train to predict specific horizons. Alternatively, we could output horizons autoregressively through the network similar to stacked RNNs as in SaShiMi [[24]](#b23) or DeepAR [[61]](#b60). However, we find this can still be relatively inefficient, as it requires passing states to each layer of a deep network.

Closed-loop SSM solution. Our approach is similar to autoregression, but only applied at a single SpaceTime layer. We treat the inputs and outputs as distinct processes in a multi-layer network, and add another matrix K to each decoder SSM to model future input time-steps explicitly. Letting ū = (ū 0 , . . . , ū -1 ) be the input sequence to a decoder SSM and u = (u 0 , . . . , u -1 ) be the original input sequence, we jointly train A, B, C, K such that x k+1 = Ax k + B ūk , and

$ŷk+1 = Cx k+1 (where ŷk+1 = y k+1 = u k+1 ) (11) ûk+1 = Kx k+1 (where ûk+1 = ūk+1 )(12)$We thus train the decoder SpaceTime layer to explicitly model its own next time-step inputs with A, B, K, and model its next time-step outputs (i.e., future time series samples) with A, B, C. For forecasting, we first process the lag terms via ( [11](#)) and ( [12](#formula_11)) as convolutions

$x k = k-1 j=0 A k-1-j Bu j and ûk = K k-1 j=0 A k-1-j B ūj(13)$for k ∈ [0, -1]. To forecast h future time-steps, with last hidden state x we first predict future input û via [(12)](#b11). Plugging this back into the SSM and iterating for h -1 future time-steps leads to

$x +i = (A + BK) i x for i = 1, . . . , h -1(14)$⇒ (y , . . . , y +h-1

$) = C(A + BK) i x i∈[h-1](15)$We can thus use Eq. 15 to get future outputs without sequential recurrence, using the same FFT operation as for Eq. 8, 9. This flexibly recovers O( + h) time complexity for forecasting h future time-steps, assuming that powers (A + BK) h are taken care of. Next, we derive an efficient matrix powering algorithm to take care of this powering and enable fast training and inference in practice.

## Efficient Inference with the Companion SSM

We finally discuss our third contribution, where we derive an algorithm for efficient training and inference with the companion SSM. To motivate this section, we note that prior efficient algorithms to compute powers of the state matrix A were only proposed to handle specific classes of A, and do not apply to the companion matrix [[24,](#b23)[27,](#b26)[29]](#b28). Recall from Sec. 3.1.2 that for a sequence of length , we want to construct the output filter F y = (CB, . . . , CA -1 B), where A is a d×d companion matrix and B, C are d×1 and 1×d matrices. Naïvely, we could use sparse matrix multiplications to compute powers CA j B for j = 0, . . . , -1 sequentially. As A has O(d) nonzeros, this would take O( d) time. We instead derive an algorithm that constructs this filter in O( log + d log d) time. The main idea is that rather than computing the filter directly, we can compute its spectrum (its discrete Fourier transform) more easily, i.e.,

$F y [m] := F(F y ) = -1 j=0 CA j ω mj B = C(I -A )(I -Aω m ) -1 B, m = 0, 1, . . . , -1.$where ω = exp(-2πi/ ) is the -th root of unity. This reduces to computing the quadratic form of the resolvent (I -Aω m ) -1 on the roots of unity (the powers of ω). Since A is a companion matrix, we can write A as a shift matrix plus a rank-1 matrix, A = S + ae T d . Thus Woodbury's formula reduces this computation to the resolvent of a shift matrix (I -Sω m ) -1 , with a rank-1 correction. This resolvent can be shown analytically to be a lower-triangular matrix consisting of roots of unity, and its quadratic form can be computed by the Fourier transform of a linear convolution of size d. Thus one can construct F y k by linear convolution and the FFT, resulting in O( log + d log d) time. We validate in Sec. 4.2.3 that Algorithm 1 leads to a wall-clock time speedup of 2× compared to computing the output filter naïvely by powering A. In App. B.2, we prove the time complexity O( log + d log d) and correctness of Algorithm 1. We also provide an extension to the closed-loop SSM, which can also be computed in subquadratic time as A + BK is a shift plus rank-2 matrix.

## Algorithm 1 Efficient Output Filter F y Computation

Require: A is a companion matrix parameterized by the last column a ) , . . . , q ( d/ ) ] and return the length-Fourier transform of the sum F (q (1) 

$∈ R d , B ∈ R d , C = C(I -A ) ∈ R d , sequence length . 1: Define quad(u, v) ∈ R for vectors u, v ∈ R d : compute q = u * v (linear convolution), zero-pad to length d/ , split into d/ chunks of size of the form [q (1$$+ • • • + q ( d/ )$). 2: Compute the roots of unity z = [ω 0 , . . . , ω -1 ] where ω = exp(-2πi/ ). 

## Experiments

We test SpaceTime on a broad range of time series forecasting and classification tasks. In Sec. 4.1, we evaluate whether SpaceTime's contributions lead to state-of-the-art results on standard benchmarks. To help explain SpaceTime's performance and validate our contributions, in Sec. 4.2 we then evaluate whether these gains coincide with empirical improvements in expressiveness (Sec. 4.2.1), forecasting flexibility (Sec. 4.2.2), and training efficiency (Sec. 4.2.3).

## Main Results: Time Series Forecasting and Classification

For forecasting, we evaluate SpaceTime on 40 forecasting tasks from the popular Informer [[80]](#b79) and Monash [[23]](#b22) benchmarks, testing on horizons 8 to 960 time-steps long. For classification, we evaluate SpaceTime on seven medical ECG or speech audio classification tasks, which test on sequences up to 16,000 time-steps long. For all results, we report mean evaluation metrics over three seeds. denotes the method was computationally infeasible on allocated GPUs, e.g., due to memory constraints (same resources for all methods; see App. C for details). App. C also contains additional dataset, implementation, and hyperparameter details.

Informer (forecasting). We report univariate time series forecasting results in Table [1](#tab_1), comparing against recent state-of-the-art methods [[78,](#b77)[81]](#b80), related state-space models [[27]](#b26), and other competitive deep architectures. We include extended results on additional horizons and multivariate forecasting in App. D.2. We find SpaceTime obtains lowest MSE and MAE on 14 and 11 forecasting settings respectively, 3× more than prior state-of-the-art. SpaceTime also outperforms S4 on 15 / 16 settings, supporting the companion SSM representation.

Monash (forecasting). We also evaluate on 32 datasets in the Monash forecasting benchmark [[23]](#b22), spanning domains including finance, weather, and traffic. For space, we report results in Table [20](#tab_2) (App. D.3). We compare against 13 classical and deep learning baselines. SpaceTime achieves best RMSE on 7 tasks and sets new state-of-the-art average performance across all 32 datasets. SpaceTime's relative improvements also notably grow on long horizon tasks (Fig. [6](#)).

ECG (multi-label classification). Beyond forecasting, we show that SpaceTime can also perform state-of-the-art time series classification. To classify sequences, we use the same sequence model architecture in Sec. 3.1. Like prior work [[27]](#b26), we simply use the last-layer FFN to project from number of SSMs to number of classes, and mean pooling over length before a softmax to output class logits. In Table [2](#tab_2), we find that SpaceTime obtains best or second-best AUROC on five out of six tasks, outperforming both general sequence models and specialized architectures.

Speech Audio (single-label classification). We further test SpaceTime on long-range audio classification on the Speech Commands dataset [[73]](#b72). The task is classifying raw audio sequences  of length 16,000 into 10 word classes. We use the same pooling operation for classification as in ECG. SpaceTime outperforms domain-specific architectures, e.g., WaveGan-D [[19]](#b18) and efficient Transformers, e.g., Performer [[14]](#b13) (Table [3](#tab_3)).

## Improvement on Criteria for Effective Time Series Modeling

For further insight into SpaceTime's performance, we now validate that our contributions improve expressivity (4.2.1), forecasting ability (4.2.2), and efficiency (4.2.3) over existing approaches.

## Expressivity

To first study SpaceTime's expressivity, we test how well SpaceTime can fit controlled autoregressive processes. To validate our theory on SpaceTime's expressivity gains in Sec. 3.1, we compare against recent related SSM architectures such as S4 [[27]](#b26) and S4D [[29]](#b28).

For evaluation, we generate noiseless synthetic AR(p) sequences. We test if models learn the true process by inspecting whether the trained model weights recover transfer functions specified by   SpaceTime captures AR(p) processes more precisely than similar deep SSM models such as S4 [[27]](#b26) and S4D [[29]](#b28), forecasting future samples and learning ground-truth transfer functions more accurately.  the AR coefficients [[53]](#b52). We use simple 1-layer 1-SSM models, with state-space size equal to AR p, and predict one time-step given p lagged inputs (the smallest sufficient setting).

In Fig. [3](#fig_5) we compare the trained forecasts and transfer functions (as frequency response plots) of SpaceTime, S4, and S4D models on a relatively smooth AR(4) process and sharp AR(6) process. Our results support the relative expressivity of SpaceTime's companion matrix SSM. While all models accurately forecast the AR(4) time series, only SpaceTime recovers the ground-truth transfer functions for both, and notably forecasts the AR(6) process more accurately (Fig. [3c,](#fig_5)[d](#)).

## Long Horizon Forecasting

To next study SpaceTime's improved long horizon forecasting capabilities, we consider two additional long horizon tasks. First, we test on much longer horizons than prior settings (c.f., Table [1](#tab_1)). Second, we test a new forecasting ability: how well methods trained to forecast one horizon transfer to longer horizons at test-time. For both, we use the popular Informer ETTh datasets. We compare SpaceTime with NLinear-the prior state-of-the-art on longer-horizon ETTh datasets-an FCN that learns a dense linear mapping between every lag input and horizon output [[78]](#b77).

We find SpaceTime outperforms NLinear on both long horizon tasks. On training to predict long horizons, SpaceTime consistently obtains lower MSE than NLinear on all settings (Table [4](#tab_4)). On transferring to new horizons, SpaceTime models trained to forecast 192 time-step horizons transfer more accurately and consistently to forecasting longer horizons up to 576 time-steps (Fig. [4](#fig_6)). This suggests SpaceTime more convincingly learns the time series process; rather than only fitting to the specified horizon, the same model can generalize to new horizons.  [5](#tab_5)). Our efficient algorithm (Sec. 3.3) is also important; it speeds up training by 2×, and makes SpaceTime's training time competitive with efficient models such as S4. On (2), we find SpaceTime also scales near-linearly with input sequence length, achieving 91% faster training time versus similarly recurrent LSTMs (Fig. [5](#)).

## Conclusion

We introduce SpaceTime, a state-space time series model. We achieve high expressivity by modeling SSMs with the companion matrix, long-horizon forecasting with a closed-loop SSM variant, and efficiency with a new algorithm to compute the companion SSM. We validate SpaceTime's proposed components on extensive time series forecasting and classification tasks.

## Ethics Statement

A main objective of our work is to improve the ability to classify and forecast time series, which has real-world applications in many fields. These applications may have high stakes, such as classifying abnormalities in medical time series. In these situations, incorrect predictions may lead to harmful patient outcomes. It is thus critical to understand that while we aim to improve time series modeling towards these applications, we do not solve these problems. Further analysis and development into where models fail in time series modeling is necessary, including potentials intersections with research directions such as robustness and model biases when aiming to deploy machine learning models in real world applications.

## Reproducibility

We include code for the main results in

Table 1 at [https://github.com/HazyResearch/spacetime](https://github.com/HazyResearch/spacetime). We provide training hyperparameters and dataset details for each benchmark in Appendix C, discussing the Informer forecasting benchmark in Appendix C.1, the Monash forecasting benchmark in Appendix C.2, and the ECG and speech audio classification benchmarks in Appendix C.3. We provide proofs for all propositions and algorithm complexities in Appendix B. Appendix: Effectively Modeling Time Series with Simple Discrete State Spaces

Table of Contents A Related Work 19 A.1 Classical Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.2 Deep Learning Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B Proofs and Theoretical Discussion 21 B.1 Expressivity Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.2 Efficiency Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.3 Companion Matrix Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 C Experiment Details 28 C.1 Informer Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 C.2 Monash Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 C.3 Time Series Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D Extended experimental results 29 D.1 Expressivity on digital filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 D.2 Informer Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 D.3 Monash Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.4 ECG Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.5 Efficiency Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.6 SpaceTime Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 D.7 SpaceTime Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

## A Related Work

## A.1 Classical Approaches

Classical approaches in time series modeling include the Box-Jenkins method [[7]](#b6), exponential smoothing [[38,](#b37)[75]](#b74), autoregressive integrated moving average (ARIMA) [[8]](#b7), and state-space models [[32]](#b31). In such approaches, the model is usually manually selected based analyzing time series features (e.g., seasonality and order of non-stationarity), where the selected model is then fitted for each individual time series. While classical approaches may be more interpretable than recent deep learning techniques, the domain expertise and manual labor needed to succesfully apply them renders them infeasible to the common setting of modeling thousands, or millions, of time series.

## A.2 Deep Learning Approaches

Recurrent models. Common deep learning architectures for modeling sequence data are the family of recurrent neural networks, which include GRUs [[15]](#b14), LSTMs [[36]](#b35), and DeepAR [[61]](#b60). However, due to the recurrent nature of RNNs, they are slow to train and may suffer from vanishing/exploding gradients, making them difficult to train [[55]](#b54).

Deep State Space models. Recent work has investigated combining the expressive strengths of SSMs with the scalable strengths of deep neural networks [[27,](#b26)[58]](#b57). [[58]](#b57) propose to train a global RNN that transforms input covariates to sequence-spcific SSM parameters; however, one downside of this approach is that they inherit the drawbacks of RNNs. More recent approaches, such as LSSL [[28]](#b27), S4 [[27]](#b26), S4D [[29]](#b28), and S5 [[65]](#b64), directly parameterize the layers of a neural network with multiple linear SSMs, and overcome common recurrent training drawbacks by leveraging the convolutional view of SSMs. While deep SSM models have been shown great promise in time series modeling, we show in our work -which builds off deep SSMs -that current deep SSM approaches are not able to capture autoregressive processes due to their continuous nature.

Neural differential equations as nonlinear state spaces. [[12]](#b11) parametrizes the vector field of continuous-time autonomous systems. These models, termed Neural Differential Equations (NDEs) have seen extensive application to time series and sequences, first by [[60]](#b59) and then by [[43,](#b42)[50,](#b49)[51]](#b50) with the notable extension to Neural Controlled Differential Equations (Neural CDEs). Neural CDEs can be considered the continuous-time, nonlinear version of state space models and RNNs [[42]](#b41). Rather than introducing nonlinearity between linear state space layers, Neural CDEs model nonlinear systems driven by a control input.

The NDE framework has been further applied by [[56]](#b55) to model graph time series via Neural Graph Differential Equations. In [[57]](#b56), a continuous-depth ResNet generalization based on ODEs is proposed, and in [[44]](#b43) numerical techniques to enable learning of stiff dynamical systems with Neural ODEs are investigated. The idea of parameterizing the vector field of a differential equation with a neural network, popularized by NDEs, can be traced back to earlier works [[21,](#b20)[74,](#b73)[79]](#b78).

Transformers. While RNNs and its variants have shown some success at time series modeling, a major limitation is their applicability to long input sequences. Since RNNs are recurrent by nature, they require long traversal paths to access past inputs, which leads to vanishing/exploding gradients and as a result struggle with capturing long-range dependencies.

To counteract the long-range dependency problem with RNNs, a recent line of work considers Transformers for time series modeling. The motivation is that due to the attention mechanism, a Transformer can directly model dependencies between any two points in the input sequence, independently of how far apart the points are. However, the high expressivity of the attention mechanism comes at the cost of the time and space complexity being quadratic in sequence length, making Transformers infeasible for very long sequences. As a result, many works consider specialized Transformer architectures with sparse attention mechanisms to bring down the quadratic complexity. For example, [[6]](#b5) propose LogSparse self-attention, where a cell attends to a subset of past cells (as opposed to all cells), where closer cells are attended to more frequently, proportional to the log of their distance, which brings down complexity from O( 2 ) to O( (log ) 2 ). [[80]](#b79) propose ProbSparse selfattention, which achieves O( log ) time and memory complexity, where they propose a generative style decoder to speed inference. [[47]](#b46) propose a pyramidal attention mechanism which shows linear time and space complexity with sequence length. Autoformer [[77]](#b76) suggests more specialization is needed in time series with a decomposition forecasting architecture, which extracts long-term stationary trend from the seasonal series and utilizes an auto-correlation mechanism, which discovers the period-based dependencies. [[82]](#b81) believes previous attempts of Transformer-based architectures do not capture global statistical properties, and to do so requires an attention mechanism in the frequency domain. Confromer [[30]](#b29) stacks convolutional and self-attention modules into a shared layer to combine the strengths of local interactions from convolutional modules and global interactions from self-attention modules. Perceiver AR [[34]](#b33) builds on the Perceiver architecture, which reduces the computational complexity of transformers by performing self-attention in a latent space, and extends Perceiver's applicability to causal autoregressive generation.

While these works have shown exciting progress on time series forecasting, their proposed architectures are specialized to handle specific time series settings (e.g., long input sequences, or seasonal sequences), and are commonly trained to output a fixed target horizon length [[80]](#b79), i.e., as direct multi-step forecasting (DMS) [[13]](#b12). Thus, while effective at specific forecasting tasks, their setups are not obviously applicable to a broad range of time series settings (such as forecasting arbitrary horizon lengths, or generalizing to classification or regression tasks).

Moreover, [[78]](#b77) showed that simpler alternatives to Transformers, such as data normalization plus a single linear layer (NLinear), can outperform these specialized Transformer architectures when similarly trained to predict the entire fixed forecasting horizons. Their results suggest that neither the attention mechanism nor the proposed modifications of these time series Transformers may be best suited for time series modeling. Instead, the success of these prior works may just be from learning to forecast the entire horizon with fully connected dependencies between prior time-step inputs and future time-step outputs, where a fully connected linear layer is sufficient.

Other deep learning methods. Other works also investigate pure deep learning architectures with no explicit temporal components, and show these models can also perform well on time series forecasting. [[54]](#b53) propose N-BEATS, a deep architecture based on backward and forward residual links. Even simpler, [[78]](#b77) investigate single linear layer models for time series forecasting. Both works show that simple architectures are capable of achieving high performance for time series forecasting. In particular, with just data normalization, the NLinear model in [[78]](#b77) obtained state-of-the-art performance on the popular Informer benchmark [[80]](#b79). Given an input sequence of past lag terms and a target output sequence of future horizon terms, for every horizon output their model simply learns the fully connected dependencies between that output and every input lag sample. However, FCNs such as NLinear also carry inefficient downsides. Unlike Transformers and SSM-based models, the number of parameters for FCNs scales directly with input and output sequence length, i.e., O( h) for inputs and h outputs. Meanwhile, SpaceTime shows that the SSM can improve the modeling quality of deep architectures, while maintaining constant parameter count regardless of input or output length. Especially when forecasting long horizons, we achieve higher forecasting accuracy with smaller models.

## B Proofs and Theoretical Discussion

## B.1 Expressivity Results

Proposition 1. An SSM with a companion state matrix can represent i.

ARIMA [[8]](#b7) ii.

## Exponential smoothing

iii. Controllable LTI systems [[11]](#b10) Proof of Proposition 1. We show each case separately. We either provide a set of algebraic manipulations to obtain the desired model from a companion SSM, or alternatively invoke standard results from signal processing and system theory. i.

We start with a standard ARMA(p, q) model

$y k = u k + q i=1 θ i u k-i + p i=1 φ i y k-i p i$We consider two cases:

Case (1): Outputs y are a shifted (lag-1) version of the inputs u

$y k+1 = y k + q i=1 θ i y k-i + p i=1 φ i y k-i+1 p i = (1 + φ 1 y k ) + q i=1 (θ i + φ i+1 )y k-i + p i=q+1 θ i y k-i(16)$where, without loss of generality, we have assumed that p > q for notational convenience. The autoregressive system ( [16](#formula_19)) is equivalent to

$A B C D =          0 0 . . . 0 0 1 1 0 . . . 0 0 0 . . . . . . . . . . . . . . . . . . 0 0 . . . 0 0 0 0 0 . . . 1 0 0 (1 + φ 1 ) (θ 1 + φ 2 ) . . . θ d-1 θ d 0          .$in state-space form, with x ∈ R d and d = max(p, q). Note that the state-space formulation is not unique.

Case (2): Outputs y are "shaped noise". The ARMA(p,q) formulation (classically) defines inputs u as white noise samples[foot_0](#foot_0) , ∀k : p(u k ) is a normal distribution with mean zero and some variance. In this case, we can decompose the output as follows:

$y ar k = p i=1 φ i y k-i p i y ma k = u k + q i=1 θ i u k-i$such that y k = y ar k + y ma k . The resulting state-space models are:

$A ar B ar C ar D ar =          0 0 . . . 0 0 1 1 0 . . . 0 0 0 . . . . . . . . . . . . . . . . . . 0 0 . . . 0 0 0 0 0 . . . 1 0 0 φ 1 φ 2 . . . φ p-1 φ p 0          . and A ma B ma C ma D ma =          0 0 . . . 0 0 1 1 0 . . . 0 0 0 . . . . . . . . . . . . . . . . . . 0 0 . . . 0 0 0 0 0 . . . 1 0 0 θ 1 θ 2 . . . θ q-1 θ q 1          .$Note that A ar ∈ R p×p , A ma ∈ R q×q . More generally, our method can represent any ARMA process as the sum of two SpaceTime heads: one taking as input the time series itself, and one the driving signal u.

ARIMA ARIMA processes are ARMA(p, q) applied to differenced time series. For example, first-order differencing y k = u k -u k-1 . Differencing corresponds to high-pass filtering of the signal y, and can be thus be realized via a convolution [[66]](#b65).

Any digital filter that can be expressed as a difference equation admits a state-space representation in companion form [[53]](#b52), and hence can be learned by SpaceTime.

ii.

Simple exponential smoothing (SES) [[9]](#b8)

$y k = αy k-1 + α(1 -α)y k-2 + • • • + α(1 -α) p-1 y k-p(17)$is an AR process with a parametrization involving a single scalar 0 < α < 1 and can thus be represented in companion form as shown above.

iii.

Let (A, B, C) be any controllable linear system. Controllability corresponds to invertibility of the Krylov matrix [11, Thm 6.1, p145]

$K(A, B) = [B, AB, . . . , A d-1 B], K(A, B) ∈ R d×d .$From rank(K) = d, it follows that there exists a a ∈ R d

$a 0 B + a 1 AB + • • • + a d-1 A d-1 B + A d B = 0. Thus AK = [AB, A 2 B, . . . , A d B] = [AB, A 2 B, . . . , A d-1 B column left shift of K , -(a 0 B + a 1 Ab + • • • + a d-1 A d-1 B) linear combination, columns of K ] = K(S f -ae d-1 )$where G = (S fae d-1 ) is a companion matrix.

$AK = KG ⇐⇒ G = K -1 AK.$Therefore G is similar to A. We can then construct a companion form state space (G, B, C, D) from A using the relation above.

## Proposition 2. No class of continuous-time LSSL SSMs can represent the noiseless AR(p) process.

Proof of Proposition 2. Recall from Sec. 3.1.1 that a noiseless AR(p) process is defined by

$y t = p i=1 φ i y t-i = φ 1 y t-1 + . . . + φ p y t-p(18)$with coefficients φ 1 , . . . , φ p . This is represented by the SSM

$x t+1 = Sx t + Bu t (19$$)$$y t = Cx t + Du t (20$$)$when S ∈ R p×p is the shift matrix, B ∈ R p×1 is the first basis vector e 1 , C ∈ R 1×p is a vector of coefficients φ 1 , . . . , φ p , and D = 0, i.e.,

$S =        0 0 . . . 0 0 1 0 . . . 0 0 0 1 . . . 0 0 . . . . . . . . . . . . 0 0 . . . 1 0        , B = 1 0 . . . 0 T , C = φ 1 . . . φ p(21)$We prove by contradiction that a continuous-time LSSL SSM cannot represent such a process. Consider the following solutions to a continuous-time system and a system [(18)](#b17), both in autonomous form

$x cont t+1 = e A x t x disc t+1 = Sx t . It follows x cont t+1 = x disc t+1 ⇐⇒ e A = S ⇐⇒ A = log (S).$we have reached a contradiction by [17, Theorem 1], as S is singular by definition and thus its matrix logarithm does not exist.

## B.2 Efficiency Results

We first prove that Algorithm 1 yields the correct output filter F y . We then analyze its time complexity, showing that it takes time O( log + d log d) for sequence length and state dimension d.

Theorem 1. Algorithm 1 returns the filter F y = (CB, . . . , CA -1 B).

Proof. We follow the outline of the proof in Section 3.3. Instead of computing F y directly, we compute its spectrum (its discrete Fourier transform):

$F y [m] := F(F y ) = -1 j=0 CA j ω mj B = C(I-A )(I-Aω m ) -1 B = C(I-Aω m ) -1 B, m = 0, 1, . . . , -1.$where ω = exp(-2πi/ ) is the -th root of unity. This reduces to computing the quadratic form of the resolvent (I -Aω m ) -1 on the roots of unity (the powers of ω). Since A is a companion matrix, we can write A as a shift matrix plus a rank-1 matrix, A = S + ae T d , where e d is the d-th basis vector [0, . . . , 0, 1] and the shift matrix S is:

$S =        0 0 . . . 0 0 1 0 . . . 0 0 0 1 . . . 0 0 . . . . . . . . . . . . . . . 0 0 . . . 1 0       $. Thus Woodbury's matrix identity (i.e., Sherman-Morrison formula) yields:

$(I -Aω m ) -1 = (I -ω m S -ω m ae d ) -1 = (I -ω m S) -1 + (I -ω m S) -1 ω m ae d (I -ω m S) -1 1 -ω m e d (I -ω m S) -1 a .$This is the resolvent of the shift matrix (I -ω m S) -1 , with a rank-1 correction. Hence

$F y = C(I -ω m S) -1 B + C(I -ω m S) -1 ae d (I -ω m S) -1 B ω -m -e d (I -ω m S) -1 a .(22)$We now need to derive how to compute the quadratic form of a resolvent of the shift matrix efficiently. Fortunately the resolvent of the shift matrix has a very special structure that closely relates to the Fourier transform. We show analytically that:

$(I -ω m S) -1 =        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        .$It is easy to verify by multiplying this matrix with I -ω m S to see if we obtain the identity matrix.

Recall that multiplying with S on the left just shifts all the columns down by one index. Therefore:

$       1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        (I -ω m S) =        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        -ω m S        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        =        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        -ω m        0 0 . . . 0 0 1 0 . . . 0 0 ω m 1 . . . 0 0 . . . . . . . . . . . . . . . ω (d-2)m ω (d-3)m . . . 1 0        =        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        -        0 0 . . . 0 0 ω m 0 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω 0        =I.$Thus the resolvent of the shift matrix indeed has the form of a lower-triangular matrix containing the roots of unity. Now that we have the analytic formula of the resolvent, we can derive its quadratic form, given some vectors u, v ∈ R d . Substituting in, we have

$u T (I -ω m S) -1 v = u 1 v 1 + u 2 v 1 ω m + u 2 v 2 + u 3 v 1 ω 2m + u 3 v 2 ω m + u 3 v 1 + . . . .$
## Grouping terms by powers of ω, we see that we want to compute u

$1 v 1 + u 2 v 2 + • • • + u d v d , then u 2 v 1 + u 3 v 2 + • • • + u d v d-1$, and so on. The term corresponding to ω km is exactly the k-th element of the linear convolution u * v. Define q = u * v, then u T (I -ω m S) -1 v is just the Fourier transform of u * v. To deal with the case where d > , we note that the powers of roots of unity will repeat, so we just need to extend the output of u * v to be multiples of , then split them into chunk of size , then sum them up and take the length-Fourier transform. This is exactly the procedure quad(u, v) defined in Algorithm 1.

Once we have derived the quadratic form of the resolvent (I -ω m S) -1 , simply plugging it into the Woodbury's matrix identity (Equation ( [22](#formula_37))) yields Algorithm 1.

We analyze the algorithm's complexity. Proof. We see that computing the quadratic form of the resolvent (I -ω m S) -1 involves a linear convolution of size d and a Fourier transform of size . The linear convolution can be done by performing an FFT of size 2d on both inputs, multiply them pointwise, then take the inverse FFT of size 2d. This has time complexity O(d log d). The Fourier transform of size has time complexity O( log ). The whole algorithm needs to compute four such quadratic forms, hence it takes time O( log + d log d).

Remark. We see that the algorithm easily extends to the case where the matrix A is a companion matrix plus low-rank matrix (of some rank k). We can write A as the sum of the shift matrix and a rank-(k + 1) matrix (since A itself is the sum of a shift matrix and a rank-1 matrix). Using the same strategy, we can use the Woodbury's matrix identity for the rank-(k + 1) case. The running time will then scale as O(k( log + d log d)).

## B.3 Companion Matrix Stability

Normalizing companion parameters for bounded gradients Proposition 3 (Bounded SpaceTime Gradients). Given s, the norm of the gradient of a SpaceTime layer is bounded for all k < s if

$d-1 i=0 |a i | = 1$Proof. Without loss of generality, we assume x 0 = 0. Since the solution at time s is

$y s = C s-1 i-1 A s-i-1 Bu i$we compute the gradient w.r.t u k as

$dy s du k = CA s-k-1 B.(23)$The largest eigenvalue of A

$max{eig(A)} ≤ max 1, d-1 i=0 |a i | Corollary of Gershgorin [35, Theorem 1] = 1 using i |a i | = 1$is 1, which implies convergence of the operator CA s-k-1 B. Thus, the gradients are bounded.

We use the proposition above to ensure gradient boundedness in SpaceTime layers by normalizing a every forward pass. performance of cardiologists and emergency residents in triaging ECGs, which would permit accurate interpretations in settings where specialists may not be present [[33,](#b32)[59]](#b58).

We use the publicly available PTB-XL dataset [[25,](#b24)[71,](#b70)[72]](#b71), which contains 21,837 12-lead ECG recordings of 10 seconds each obtained from 18,885 patients. Each ECG recording is annotated by up to two cardiologists with one or more of the 71 ECG statements (labels). These ECG statements conform to the SCP-ECG standard [[62]](#b61). Each statement belongs to one or more of the following three categories -diagnostic, form, and rhythm statements. The diagnostic statements are further organised in a hierarchy containing 5 superclasses and 24 subclasses.

This provides six sets of annotations for the ECG statements based on the different categories and granularities: all (all ECG statements), diagnostic (only diagnostic statements including both subclass and superclass statements), diagnostic subclass (only diagnostic subclass statements), diagnostic superclass (only diagnostic superclass statements), form (only form statements), and rhythm (only rhythm statements). These six sets of annotations form different prediction tasks which are referred to as all, diag, sub-diag, super-diag, form, and rhythm respectively. The diagnostic superclass task is multi-class classification, and the other tasks are multi-label classification.

ECG classification training details. To tune SpaceTime and S4, we performed a grid search over the learning rate {0.01, 0.001}, model dropout {0.1, 0.2}, number of SSMs per layer {128, 256}, and number of layers {4, 6}, and chose the parameters that resulted in highest validation AUROC. The SSM state dimension was fixed to 64, with gated linear units as the non-linearity between stacked layers. We additionally apply layer normalization. We use a cosine learning rate scheduler, with a warmup period of 5 epochs. We train all models for 100 epochs.

Speech Commands training details. To train SpaceTime, we use the same hyperparameters used by S4: a learning rate of 0.01 with a plateau scheduler with patience 20, dropout of 0.1, 128 SSMs per layer, 6 layers, batch normalization, trained for 200 epochs.

Hardware details. For both ECG and Speech Commands, all experiments were run on a single NVIDIA Tesla A100 Ampere 40 GB GPU.

## D Extended experimental results

## D.1 Expressivity on digital filters

We experimentally verify whether SpaceTime can approximate the input-output map of digital filter admitting a state-space representation, with improved generalization over baseline models given test inputs of unseen frequencies.

We generate a dataset of 1028 sinusoidal signals of length 200

$x(t) = sin (2πωt)$where ω ∈ [2, 40] [50, 100] in the training set and ω ∈ [(40,](#b39)[50)](#b49) in the test set. The outputs are obtained by filtering x, i.e., y = F(x) where F is in the family of digital filters.

We introduce common various sequence-to-sequence layers or models as baselines: the original S4 diagonal plus low-rank [[27]](#b26), a single-layer LSTM, a single 1d convolution (Conv1d), a dense linear layer (NLinear), a single self-attention layer. All models are trained for 800 epochs with batch size 256, learning rate 10 -3 and Adam. We repeat this experiment for digital filters of different orders [[53]](#b52). The results are shown in Figure [8](#). SpaceTime learns to match the frequency response of the target filter, producing the correct output for inputs at test frequencies.  

## D.6 SpaceTime Ablations

To better understand how the proposed SpaceTime SSMs lead to the improved empirical performance, we include ablations on the individual closed-loop forecasting SSM (Section 3.2) and preprocessing SSMs (Section 3.1.3).

## D.6.1 Closed-loop Forecasting SSM

To study how the closed-loop SSM improves long horizon forecasting accuracy, we remove the closed-loop SSM component in our default SpaceTime forecasting architecture (c.f., Appendix D.7, and compare the default SpaceTime with one without any closed-loop SSMs on Informer forecasting tasks. For models without closed-loop SSMs, we replace the last layer with the standard "open-loop" SSM framework in Section 3.1.2), and keep all other layers the same. Finally, for baseline comparison against another SSM without the closed-loop component, we compare against S4.

In Table [10](#tab_10), we report standardized MSE on Informer ETT datasets. Adding the closed-loop SSM consistently improves forecasting accuracy, on average lowering relative MSE by 33.2%. Meanwhile, even without the closed-loop SSM, SpaceTime outperforms S4, again suggesting that the companion matrix parameterization is beneficial for autoregressive time series forecasting. To study how the preprocessing SSM improves long horizon forecasting accuracy, we next compare how SpaceTime performs with and without the weight-initializing preprocessing SSMs introduced in Section 3.1.3. We compare the default SpaceTime architecture (Table [12](#tab_12) with (1) replacing the preprocessing SSMs with randomly initialized default companion SSMs, and (2) removing the preprocessing SSMs altogether. For the former, we preserve the number of layers, but now train the first-layer SSM weights. For the latter, there is one-less layer, but the same number of trainable parameters (as we fix and freeze the weights for each preprocessing SSM).

In Table [11](#tab_11), we report standardized MSE on Informer ETT datasets. We find fixing the first layer SSMs of a SpaceTime network to preprocessing SSMs consistently improves forecasting performance, achieving 4.55% lower MSE on average than the ablation with just trainable companion matrices. Including the preprocessing layer also improves MSE by 9.26% on average compared to removing the layer altogether. These results suggest that preprocessing SSMs are beneficial for time series forecasting, e.g., by performing classic time series modeling techniques on input data. Unlike other approaches, SpaceTime is able to flexibly and naturally incorporate these operations into its network layers via simple weight initializations of the same general companion SSM structure. 

## D.7 SpaceTime Architectures

We provide the specific SpaceTime architecture configurations used for forecasting and classification tasks. Each configuration follows the general architecture presented in Section 3.1 and Figure [2](#fig_0), and consists of repeated Multi-SSM SpaceTime layers. We first provide additional details on specific instantiations of the companion SSMs we use in our models, e.g., how we instantiate preprocessing SSMs to recover specific techniques (Section 3.1.3). We then include the layer-specific details of the number and type of SSM used in each network.

## D.7.1 Specific SSM parameterizations

In Section 3.1.1, we described the general form of the companion SSM used in this work. By default, for any individual SSM we learn the a column in A and the vectors B, C as trainable parameters in a neural net module. We refer to these SSMs specifically as companion SSMs.

In addition, as discussed in Sections 3.1.1 and 3.1.3, we can also fix a, B, or C to specific values to recover useful operations when computing the SSM outputs. We describe specific instantiations of the companion SSM used in our models below (with dimensionality referring to one SSM).

Shift SSM. We fix the a vector in the companion state matrix A ∈ R d×d to the 0 vector ∈ R d , such that A is the shift matrix (see Eq. 21 for an example). This is a generalization of a 1-D "sliding window" convolution with fixed kernel size equal to SSM state dimension d. To see how, note that if B is also fixed to the first basis vector e 1 ∈ R d×1 , then this exactly recovers a 1-D convolution with kernel determined by C.

Differencing SSM. As a specific version of the preprocessing SSM discussed in Section 3.1.3, we fix a = 0, B = e 1 , and set C to recover various order differencing when computing the SSM, i.e., C = 1 0 0 0 0 . . . 0 (0-order differencing, i.e., an identity function)

C = 1 -1 0 0 0 . . . 0 (1st-order differencing) (25)

$C = 1 -2 1 0 0 . . . 0 (2nd-order differencing)(26)$C = 1 -3 3 -1 0 . . . 0 (3rd-order differencing)

In this work, we only use the above 0, 1st, 2nd, or 3rd-order differencing instantiations. With multiple differencing SSMs in a multi-SSM SpaceTime layer, we initialize differencing SSMs by running through the orders repeatedly in sequence. For example, given five differencing SSMs, the first four SSMs perform 0, 1st, 2nd, and 3rd-order differencing respectively, while the fifth performs 0-order differencing again.

Moving Average Residual (MA residual) SSM. As another version of the preprocessing SSM, we can fix a = 0, B = e 1 , and set C such that the SSM outputs sample residuals from a moving average applied over the input sequence. For an n-order moving average, we compute outputs with C specified as

$C = 1 -1/n, -1/n, . . . -1/n, 0 . . . 0$(n-order moving average residual)

For each MA residual SSM, we randomly initialize the order by uniform-randomly sampling an integer in the range [[4, d]](#), where d is again the state-space dimension size (recall C ∈ R 1×d ). We pick 4 as a heuristic which was not finetuned; we leave additional optimization here for further work.

## D.7.2 Task-specific SpaceTime Architectures

Here we provide layer-level details on the SpaceTime networks used in this work. For each task, we describe number of layers, number of SSMs per layer, state-space dimension (fixed for all SSMs in a network), and which SSMs are used in each layer.

Expanding on this last detail, as previously discussed in Section 3.1.2, in each SpaceTime layer we can specify multiple SSMs in each layer, computing their outputs in parallel to produce a multidimensional output that is fed as the input to the next SpaceTime layer. The "types" of SSMs do not all have to be the same per layer, and we list the type (companion, shift, differencing, MA residual) and closed-loop designation (standard, closed-loop) of the SSMs in each layer below.

For an additional visual overview of a SpaceTime network, please refer back to Figure [2](#fig_0).

Forecasting: Informer and Monash. We describe the architecture in Table [12](#tab_12). We treat the first SpaceTime layer as "preprocessing" layer, which performs differencing and moving average residual operations on the input sequence. We treat the last SpaceTime layer as a "forecasting" layer, which autoregressively outputs future horizon predictions given the second-to-last layer's outputs as an input sequence.

Classification: ECG. We describe the architectures for each ECG classification task in Tables 13-18. For all models, we use state-space dimension d = 64. As described in the experiments, for classification we compute logits with a mean pooling over the output sequence, where pooling is computed over the sequence length.

Classification: Speech Audio. We describe the architecture for the Speech Audio task in Table [19](#tab_19). We use state-space dimension d = 1024. As described in the experiments, for classification we compute logits with a mean pooling over the output sequence, where pooling is computed over the sequence length. Table 20: Monash forecasting. Test RMSE of SpaceTimefor each dataset (best result selected via validation RMSE, average of 3 runs). 

![Figure 2: SpaceTime architecture and components. (Left): Each SpaceTime layer carries weights that model multiple companion SSMs, followed optionally by a nonlinear FFN. The SSMs are learned in parallel (1) and computed as a single matrix multiplication (2). (Right): We stack these layers into a SpaceTime network, where earlier layers compute SSMs as convolutions for fast sequence-to-sequence modeling and data preprocessing, while a decoder layer computes SSMs as recurrences for dynamic forecasting.]()

![No class of continuous-time LSSL SSMs can represent the noiseless AR(p) process.]()

![Compute F y = quad( C, B) + quad( C, a) * quad(e d , B)/(z -quad(e d , a)) ∈ R , where e d = [0, . . . , 0, 1] is the d-th basis vector.Return the inverse Fourier transform F y = F -1 ( F y ).]()

![AR(4) Transfer Func.]()

![AR(6) Transfer Func.]()

![Figure3: AR(p) expressiveness benchmarks. SpaceTime captures AR(p) processes more precisely than similar deep SSM models such as S4[27] and S4D[29], forecasting future samples and learning ground-truth transfer functions more accurately.]()

![Figure 4: Forecasting transfer. Mean MSE (±1 standard deviation). SpaceTime transfers more accurately and consistently to horizons not used for training versus NLinear [78].]()

![Algorithm 1 has time complexity O( log + d log d) for sequence length and state dimension d.]()

![Figure 7: Training wall-clock time versus horizon length for SpaceTime, S4, LSTM, and Transformer.]()

![]()

![Univariate forecasting results on Informer Electricity Transformer Temperature (ETT) datasets[80]. Best results in bold. SpaceTime results reported as means over three seeds. We include additional datasets, horizons, and method comparisons in App. D.2]()

![ECG]()

![Speech Audio classification[73]]()

![Longer horizon forecasting on Informer ETTh data. Standardized MSE reported. SpaceTime obtains lower MSE when forecasting longer horizons.]()

![Train wall-clock time. Seconds per epoch when training on ETTh1 data.]()

![Comparing sequence models on the task of approximating the input-output map defined by digital filters of different orders. Test RMSE on held-out inputs at unseen frequencies.]()

![Univariate forecasting results on Informer datasets. Best results in bold. SpaceTime obtains best MSE on 19 out of 25 and best MAE on 20 out of 25 dataset and horizon tasks.Relative test RMSE rankings ( * /13 models) across different slices of the 33 datasets in the Monash repository[23]. SpaceTime sets best overall ranking across all tasks and is significantly more accurate on tasks involving long forecast horizon and larger number of training samples.]()

![Closed-loop SSM Ablation We ablate the closed-loop SSM component in SpaceTime, comparing against the prior S4 SSM on four Informer time series forecasting tasks. Removing the closed-loop SSM consistently hurts forecasting accuracy for SpaceTime.]()

![Preprocessing SSM Ablation We ablate the preprocessing SSM layer in SpaceTime, comparing against either replacing the SSMs with companion SSMs (Companion) or removing the layer (Removed). Including preprocessing SSMs consistently improves forecasting accuracy.]()

![SpaceTime forecasting architecture. For all SSMs, we keep state-space dimension d = 128. Repeated Identity denotes repeating the input to match the number of SSMs in the next layer, i.e., 128 SSMs in this case. For each forecasting task, d denotes time series samples' number of features, denotes the lag size (number of past samples given as input), and h denotes the horizon size (number of future samples to be predicted).]()

![SpaceTime architecture for ECG SuperDiagnostic classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 5.]()

![SpaceTime architecture for ECG SubDiagnostic classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 23.]()

![SpaceTime architecture for ECG Diagnostic classification. For all SSMs, we keep statespace dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 44.]()

![SpaceTime architecture for ECG Form classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 19.]()

![SpaceTime architecture for ECG Rhythm classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 12.]()

![SpaceTime architecture for ECG All classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 71.]()

![SpaceTime architecture for Speech Audio classification. For all SSMs, we keep state-space dimension d = 1024. Input samples have d = 1 features and are length = 16000 time-steps long. The number of classes c = 10.]()

Other formulations with forecast residuals are also common.

A task can belong to multiple splits, resulting in overlapping splits. For example, a task can involve both long context as well as long forecasting horizon.

