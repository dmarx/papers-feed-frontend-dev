<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effectively Modeling Time Series with Simple Discrete State Spaces</title>
				<funder ref="#_7M7q4ve">
					<orgName type="full">Interactive Human-AI Teaming)</orgName>
				</funder>
				<funder>
					<orgName type="full">Accenture</orgName>
				</funder>
				<funder ref="#_BDEjdX7">
					<orgName type="full">Xilinx</orgName>
				</funder>
				<funder>
					<orgName type="full">VMWare</orgName>
				</funder>
				<funder ref="#_2g3rNje">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Salesforce</orgName>
				</funder>
				<funder>
					<orgName type="full">Total</orgName>
				</funder>
				<funder>
					<orgName type="full">Analog Devices</orgName>
				</funder>
				<funder ref="#_NBaFtaF">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford Data Science Initiative</orgName>
					<orgName type="abbreviated">SDSI</orgName>
				</funder>
				<funder>
					<orgName type="full">Facebook</orgName>
				</funder>
				<funder ref="#_yj99eb7">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Ericsson</orgName>
				</funder>
				<funder ref="#_6TCVEvc">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-03-17">March 17, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
							<email>mzhang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
							<email>ksaab@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
							<email>poli@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<email>tridao@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
							<email>kgoel@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Effectively Modeling Time Series with Simple Discrete State Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-17">March 17, 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">DB88DB166410DEB2E679CDE53ED46261</idno>
					<idno type="arXiv">arXiv:2303.09489v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -a canonical representation for discrete-time processes-which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a "closed-loop" variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layerwise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length and state-space size d, we go from Õ(d ) naïvely to Õ(d + ). In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR(p) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73% and 80% relative wall-clock time over Transformers and LSTMs.</p><p>* Equal Contribution. Order determined by forecasting competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time series modeling is a well-established problem, with tasks such as forecasting and classification motivated by many domains such as healthcare, finance, and engineering <ref type="bibr" target="#b62">[63]</ref>. However, effective time series modeling presents several challenges:</p><p>• First, methods should expressively capture complex, long-range, and autoregressive dependencies. Time series data often reflects higher order dependencies, seasonality, and trends, governing how past samples determine future terms <ref type="bibr" target="#b9">[10]</ref>. This motivates many classical approaches that model these properties <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b74">75]</ref>, alongside expressive deep learning mechanisms such as attention <ref type="bibr" target="#b69">[70]</ref> and fully connected layers that model interactions between every sample in an input sequence <ref type="bibr" target="#b77">[78]</ref>.</p><p>• Second, methods should be able to forecast a wide range of long horizons over various data domains. Reflecting real world demands, popular forecasting benchmarks evaluate methods on 34 different tasks <ref type="bibr" target="#b22">[23]</ref> and 24-960 time-step horizons <ref type="bibr" target="#b79">[80]</ref>. Furthermore, as testament to accurately learning time series processes, forecasting methods should ideally also be able to predict future time-steps on horizons they were not explicitly trained on.</p><p>• Finally, methods should be efficient with training and inference. Many time series applications require processing very long sequences, e.g., classifying audio data with sampling rates up to 16,000 Hz <ref type="bibr" target="#b72">[73]</ref>. To handle such settings-where we still need large enough models that can expressively model this data-training and inference should ideally scale subquadratically with sequence length and model size in time and space complexity.</p><p>Unfortunately, existing time series methods struggle to achieve all three criteria. Classical methods (c.f., ARIMA <ref type="bibr" target="#b7">[8]</ref>, exponential smoothing (ETS) <ref type="bibr" target="#b74">[75]</ref>) often require manual data preprocessing and model selection to identify expressive-enough models. Deep learning methods commonly train to predict specific horizon lengths, i.e., as direct multi-step forecasting <ref type="bibr" target="#b12">[13]</ref>, and we find this hurts their ability to forecast longer horizons (Sec. 4.2.2). They also face limitations achieving high expressivity and efficiency. Fully connected networks (FCNs) such as NLinear <ref type="bibr" target="#b77">[78]</ref> scale quadratically in O( h) space complexity (with input length and forecast length h). Recent Transformer-based models reduce this complexity to O( + h), but do not always outperform the aforementioned fully connected networks on forecasting benchmarks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b79">80]</ref>.</p><p>We thus propose SpaceTime, a deep state-space architecture for effective time series modeling. To achieve this, we focus on improving each criteria via three core contributions:</p><p>1. For expressivity, our key idea and building block is a linear layer that models time series processes as state-space models (SSMs) via the companion matrix (Fig. <ref type="figure">1</ref>). We start with SSMs due to their connections to both classical time series analysis <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41]</ref> and recent deep learning advances <ref type="bibr" target="#b26">[27]</ref>.</p><p>Classically, many time series models such as ARIMA and exponential smoothing (ETS) can be expressed as SSMs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b74">75]</ref>. Meanwhile, recent state-of-the-art deep sequence models <ref type="bibr" target="#b26">[27]</ref> have used SSMs to outperform Transformers and LSTMs on challenging long-range benchmarks <ref type="bibr" target="#b67">[68]</ref>. Their primary innovations show how to formulate SSMs as neural network parameters that are practical to train. However, we find limitations with these deep SSMs for time series data. While we build on their advances, we prove that these prior SSM representations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref> cannot capture autoregressive processes fundamental for time series. We thus specifically propose the companion matrix representation for its expressive and memory-efficient properties. We prove that the companion matrix SSM recovers fundamental autoregressive (AR) and smoothing processes modeled in classical techniques such as ARIMA and ETS, while only requiring O(d) memory to represent an O(d 2 ) matrix. Thus, SpaceTime inherits the benefits of prior SSM-based sequence models, while introducing improved expressivity that recovers fundamental time series processes simply through its layer weights.</p><p>2. For forecasting long horizons, we introduce a new "closed-loop" view of SSMs. Prior deep SSM architectures either apply the SSM as an "open-loop" <ref type="bibr" target="#b26">[27]</ref>, where fixed-length inputs necessarily generate same-length outputs, or use closed-loop autoregression where final layer outputs are fed through the entire network as next-time-step inputs <ref type="bibr" target="#b23">[24]</ref>. We describe issues with both approaches in Sec. 3.2, and instead achieve autogressive forecasting in a deep network with only a single Figure <ref type="figure">1</ref>: We learn time series processes as state-space models (SSMs) (top left). We represent SSMs with the companion matrix, which is a highly expressive representation for discrete time series (top middle), and compute such SSMs efficiently as convolutions or recurrences via a shift + low-rank decomposition (top right). We use these SSMs to build SpaceTime, a new time series architecture broadly effective across tasks and domains (bottom).</p><p>SSM layer. We do so by explicitly training the SSM layer to predict its next time-step inputs, alongside its usual outputs. This allows the SSM to recurrently generate its own future inputs that lead to desired outputs-i.e., those that match an observed time series-so we can forecast over many future time-steps without explicit data inputs.</p><p>3. For efficiency, we introduce an algorithm for efficient training and inference with the companion matrix SSM. We exploit the companion matrix's structure as a "shift plus low-rank" matrix, which allows us to reduce the time and space complexity for computing SSM hidden states and outputs from Õ(d ) to Õ(d + ) in SSM state size d and input sequence length .</p><p>In experiments, we find SpaceTime consistently obtains state-of-the-art or near-state-of-the-art results, achieving best or second-best AUROC on 6 out of 7 ECG and audio speech time series classification tasks, and best mean-squared error (MSE) on 14 out of 16 Informer benchmark forecasting tasks <ref type="bibr" target="#b79">[80]</ref>. SpaceTime also sets a new best average ranking across 34 tasks on the Monash benchmark <ref type="bibr" target="#b22">[23]</ref>. We connect these gains with improvements on our three effective time series modeling criteria. For expressivity, on synthetic ARIMA processes SpaceTime learns AR processes that prior deep SSMs cannot. For long horizon forecasting, SpaceTime consistently outperforms prior state-of-the-art on the longest horizons by large margins. SpaceTime also generalizes better to new horizons not used for training. For efficiency, on speed benchmarks SpaceTime obtains 73% and 80% relative wall-clock speedups over parameter-matched Transformers and LSTMs respectively, when training on real-world ETTh1 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Problem setting. We evaluate effective time series modeling with classification and forecasting tasks. For both tasks, we are given input sequences of "look-back" or "lag" time series samples u t-:t-1 = (u t-, . . . , u t-1 ) ∈ R ×m for sample feature size m. For classification, we aim to classify the sequence as the true class y out of possible classes Y. For forecasting, we aim to correctly predict H future time-steps over a "horizon" y t,t+h-1 = (u t , . . . , u t+h-1 ) ∈ R h×m .</p><p>State-space models for time series. We build on the discrete-time state-space model (SSM), which maps observed inputs u k to hidden states x k , before projecting back to observed outputs y k</p><formula xml:id="formula_0">x k+1 = Ax k + Bu k<label>(1)</label></formula><formula xml:id="formula_1">y k = Cx k + Du k<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">A ∈ R d×d , B ∈ R d×m , C ∈ R m ×d</formula><p>, and D ∈ R m ×m . For now, we stick to single-input single-output conventions where m, m = 1, and let D = 0. To model time series in the single SSM setting, we treat u and y as copies of the same process, such that</p><formula xml:id="formula_3">y k+1 = u k+1 = C(Ax k + Bu k )<label>(3)</label></formula><p>We can thus learn a time series SSM by treating A, B, C as black-box parameters in a neural net layer, i.e., by updating A, B, C via gradient descent s.t. with input u k and state x k at time-step k, following (3) predicts ŷk+1 that matches the next time-step sample y k+1 = u k+1 . This SSM framework and modeling setup is similar to prior works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, which adopt a similar interpretation of inputs and outputs being derived from the "same" process, e.g., for language modeling. Here we study and improve this framework for time series modeling. As extensions, in Sec. 3.1.1 we show how (1) and ( <ref type="formula" target="#formula_1">2</ref>) express univariate time series with the right A representation. In Sec. 3.1.2 we discuss the multi-layer setting, where layer-specific u and y now differ, and we only model first layer inputs and last layer outputs as copies of the same time series process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method: SpaceTime</head><p>We now present SpaceTime, a deep architecture that uses structured state-spaces for more effective time-series modeling. SpaceTime is a standard multi-layer encoder-decoder sequence model, built as a stack of repeated layers that each parametrize multiple SSMs. We designate the last layer as the "decoder", and prior layers as "encoder" layers. Each encoder layer processes an input time series sample as a sequence-to-sequence map. The decoder layer then takes the encoded sequence representation as input and outputs a prediction (for classification) or sequence (for forecasting).</p><p>Below we expand on our contributions that allow SpaceTime to improve expressivity, longhorizon forecasting, and efficiency of time series modeling. In Sec. 3.1, we present our key building block, a layer that parametrizes the companion matrix SSM (companion SSM) for expressive autoregressive modeling. In Sec. 3.2, we introduce a specific instantiation of the companion SSM to flexibly forecast long horizons. In Sec. 3.3, we provide an efficient inference algorithm that allows SpaceTime to train and predict over long sequences in sub-quadratic time and space complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Multi-SSM SpaceTime layer</head><p>We discuss our first core contribution and key building block of our model, the SpaceTime layer, which captures the companion SSM 's expressive properties, and prove that the SSM represents multiple fundamental processes. To scale up this expressiveness in a neural architecture, we then go </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Expressive State-Space Models with the Companion Matrix</head><p>For expressive time series modeling, our SSM parametrization represents the state matrix A as a companion matrix. Our key motivation is that A should allow us to capture autoregressive relationships between a sample u k and various past samples u k-1 , u k-2 , . . . , u k-n . Such dependencies are a basic yet essential premise for time series modeling; they underlie many fundamental time series processes, e.g., those captured by standard ARIMA models. For example, consider the simplest version of this, where u k is a linear combination of p prior samples (with coefficients φ 1 , . . . , φ p )</p><formula xml:id="formula_4">u k = φ 1 u k-1 + φ 2 u k-2 + . . . φ p u k-p<label>(4)</label></formula><p>i.e., a noiseless, unbiased AR(p) process in standard ARIMA time series analysis <ref type="bibr" target="#b7">[8]</ref>.</p><p>To allow (3) to express (4), we need the hidden state x k to carry information about past samples. However, while setting the state-space matrices as trainable neural net weights may suggest we can learn arbitrary task-desirable A and B via supervised learning, prior work showed this could not be done without restricting A to specific classes of matrices <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Fortunately, we find that a class of relatively simple A matrices suffices. We propose to set A ∈ R d×d as the d × d companion matrix, a square matrix of the form:</p><formula xml:id="formula_5">(Companion Matrix) A =        0 0 . . . 0 a 0 1 0 . . . 0 a 1 0 1 . . . 0 a 2 . . . . . . . . . . . . 0 0 . . . 1 a d-1        i.e., A i,j =      1 for i -1 = j a i for j = d -1 0 otherwise<label>(5)</label></formula><p>Then simply letting state dimension d = p, assuming initial hidden state x 0 = 0, and setting</p><formula xml:id="formula_6">a := a 0 a 1 . . . a d-1 T = 0, B = 1 0 . . . 0 T , C = φ 1 . . . φ p</formula><p>allows the discrete SSM in (1, 2) to recover the AR(p) process in <ref type="bibr" target="#b3">(4)</ref>. We next extend this result in Proposition 1, proving in App. B that setting A as the companion matrix allows the SSM to recover a wide range of fundamental time series and dynamical system processes beyond the AR(p) process.</p><p>Proposition 1. A companion state matrix SSM can represent ARIMA <ref type="bibr" target="#b7">[8]</ref>, exponential smoothing <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b74">75]</ref>, and controllable linear time-invariant systems <ref type="bibr" target="#b10">[11]</ref>.</p><p>As a result, by training neural network layers that parameterize the companion SSM, we provably enable these layers to learn the ground-truth parameters for multiple time series processes. In addition, as we only update a ∈ R d (5), we can efficiently scale the hidden-state size to capture more expressive processes with only O(d) parameters. Finally, by learning multiple such SSMs in a single layer, and stacking multiple such layers, we can further scale up expressivity in a deep architecture.</p><p>Prior SSMs are insufficient. We further support the companion SSM by proving that existing related SSM representations used in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b64">65]</ref> cannot capture the simple yet fundamental AR(p) process. Such works, including S4 and S4D, build on the Linear State-Space Layer (LSSL) <ref type="bibr" target="#b27">[28]</ref>, and cannot represent AR processes due to their continuous-time or diagonal parametrizations of A. We defer the proof to App. B.1. In Sec. 4.2.1, we empirically support this analysis, showing that these prior SSMs fit synthetic AR processes less accurately than the companion SSM. This suggests the companion matrix resolves a fundamental limitation in related work for time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Layer Architecture and Multi-SSM Computation</head><p>Architecture. To capture and scale up the companion SSM's expressive and autoregressive modeling capabilities, we model multiple companion SSMs in each SpaceTime layer's weights. SpaceTime layers are similar to prior work such as LSSLs, with A, B, C as trainable weights, and D added back as a skip connection. To model multiple SSMs, we add a dimension to each matrix. For s SSMs per SpaceTime layer, we specify weights A ∈ R s×d×d , B ∈ R d×s , and C ∈ R s×d . Each slice in the s dimension represents an individual SSM. We thus compute s outputs and hidden states in parallel by following (1) and ( <ref type="formula" target="#formula_1">2</ref>) via simple matrix multiplications on standard GPUs.</p><p>To model dependencies across individual SSM outputs, we optionally follow each SpaceTime layer with a one-layer nonlinear feedforward network (FFN). The FFN thus mixes the m outputs across a SpaceTime layer's SSMs, allowing subsequent layers to model dependencies across SSMs.</p><p>Computation. To compute the companion SSM, we could use the recurrence in (1). However, this sequential operation is slow on modern GPUs, which parallelize matrix multiplications. Luckily, as described in <ref type="bibr" target="#b26">[27]</ref> we can also compute the SSM as a 1-D convolution. This enables parallelizable inference and training. To see how, note that given a sequence with at least k inputs and hidden state x 0 = 0, the hidden state and output at time-step k by induction are:</p><formula xml:id="formula_7">x k = k-1 j=0 A k-1-j Bu j and y k = k-1 j=0 CA k-1-j Bu j<label>(6)</label></formula><p>We can thus compute hidden state x k and output y k as 1-D convolutions with "filters" as</p><formula xml:id="formula_8">F x = (B, AB, A 2 B, . . . , A -1 B) (Hidden State Filter)<label>(7)</label></formula><p>F y = (CB, CAB, CA 2 B, . . . , CA -1 B) (Output Filter) (8)</p><formula xml:id="formula_9">x k = (F x * u)[k] and y k = (F y * u)[k]<label>(9)</label></formula><p>So when we have inputs available for each output (i.e., equal-sized input and output sequences) we can obtain outputs by first computing output filters F y (8), and then computing outputs efficiently with the Fast Fourier Transform (FFT). We thus compute each encoder SSM as a convolution.</p><p>For now we note two caveats. Having inputs for each output is not always true, e.g., with long horizon forecasting. Efficient inference also importantly requires that F y can be computed efficiently, but this is not necessarily trivial for time series: we may have long input sequences with large k.</p><p>Fortunately we later provide solutions for both. In Sec. 3.2, we show how to predict output samples many time-steps ahead of our last input sample via a "closed-loop" forecasting SSM. In Sec. 3.3 we show how to compute both hidden state and output filters efficiently over long sequences via an efficient inference algorithm that handles the repeated powering of A k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Built-in Data Preprocessing with Companion SSMs</head><p>We now show how beyond autoregressive modeling, the companion SSM also enables SpaceTime layers to do standard data preprocessing techniques used to handle nonstationarities. Consider differencing and smoothing, two classical techniques to handle nonstationarity and noise:</p><formula xml:id="formula_10">u k = u k -u k-1 (1st-order differencing) u k = 1 n n-1 i=0</formula><p>u k-i (n-order moving average smoothing)</p><p>We explicitly build these preprocessing operations into a SpaceTime layer by simply initializing companion SSM weights. Furthermore, by specifying weights for multiple SSMs, we simultaneously perform preprocessing with various orders in one forward pass. We do so by setting a = 0 and B = [1, 0, . . . , 0] T , such that SSM outputs via the convolution view <ref type="bibr" target="#b5">(6)</ref> are simple sliding windows / 1-D convolutions with filter determined by C. We can then recover arbitrary n-order differencing or average smoothing via C weight initializations, e.g., (see App. D.7.1 for more examples), C = 1 -2 1 0 0 . . . 0 1/n . . . 1/n 0 0 . . . 0 (2nd-order differencing) (n-order moving average smoothing) (10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Long Horizon Forecasting with Closed-loop SSMs</head><p>We now discuss our second core contribution, which enables long horizon forcasting. Using a slight variation of the companion SSM, we allow the same constant size SpaceTime model to forecast over many horizons. This forecasting SSM recovers the flexible and stateful inference of RNNs, while retaining the faster parallelizable training of computing SSMs as convolutions.</p><p>Challenges and limitations. For forecasting, a model must process an input lag sequence of length and output a forecast sequence of length h, where h = necessarily. Many state-of-the-art neural nets thus train by specifically predicting h-long targets given -long inputs. However, in Sec. 4.2.2 we find this hurts transfer to new horizons in other models, as they only train to predict specific horizons. Alternatively, we could output horizons autoregressively through the network similar to stacked RNNs as in SaShiMi <ref type="bibr" target="#b23">[24]</ref> or DeepAR <ref type="bibr" target="#b60">[61]</ref>. However, we find this can still be relatively inefficient, as it requires passing states to each layer of a deep network.</p><p>Closed-loop SSM solution. Our approach is similar to autoregression, but only applied at a single SpaceTime layer. We treat the inputs and outputs as distinct processes in a multi-layer network, and add another matrix K to each decoder SSM to model future input time-steps explicitly. Letting ū = (ū 0 , . . . , ū -1 ) be the input sequence to a decoder SSM and u = (u 0 , . . . , u -1 ) be the original input sequence, we jointly train A, B, C, K such that x k+1 = Ax k + B ūk , and</p><formula xml:id="formula_11">ŷk+1 = Cx k+1 (where ŷk+1 = y k+1 = u k+1 ) (11) ûk+1 = Kx k+1 (where ûk+1 = ūk+1 )<label>(12)</label></formula><p>We thus train the decoder SpaceTime layer to explicitly model its own next time-step inputs with A, B, K, and model its next time-step outputs (i.e., future time series samples) with A, B, C. For forecasting, we first process the lag terms via ( <ref type="formula">11</ref>) and ( <ref type="formula" target="#formula_11">12</ref>) as convolutions</p><formula xml:id="formula_12">x k = k-1 j=0 A k-1-j Bu j and ûk = K k-1 j=0 A k-1-j B ūj<label>(13)</label></formula><p>for k ∈ [0, -1]. To forecast h future time-steps, with last hidden state x we first predict future input û via <ref type="bibr" target="#b11">(12)</ref>. Plugging this back into the SSM and iterating for h -1 future time-steps leads to</p><formula xml:id="formula_13">x +i = (A + BK) i x for i = 1, . . . , h -1<label>(14)</label></formula><p>⇒ (y , . . . , y +h-1</p><formula xml:id="formula_14">) = C(A + BK) i x i∈[h-1]<label>(15)</label></formula><p>We can thus use Eq. 15 to get future outputs without sequential recurrence, using the same FFT operation as for Eq. 8, 9. This flexibly recovers O( + h) time complexity for forecasting h future time-steps, assuming that powers (A + BK) h are taken care of. Next, we derive an efficient matrix powering algorithm to take care of this powering and enable fast training and inference in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Inference with the Companion SSM</head><p>We finally discuss our third contribution, where we derive an algorithm for efficient training and inference with the companion SSM. To motivate this section, we note that prior efficient algorithms to compute powers of the state matrix A were only proposed to handle specific classes of A, and do not apply to the companion matrix <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. Recall from Sec. 3.1.2 that for a sequence of length , we want to construct the output filter F y = (CB, . . . , CA -1 B), where A is a d×d companion matrix and B, C are d×1 and 1×d matrices. Naïvely, we could use sparse matrix multiplications to compute powers CA j B for j = 0, . . . , -1 sequentially. As A has O(d) nonzeros, this would take O( d) time. We instead derive an algorithm that constructs this filter in O( log + d log d) time. The main idea is that rather than computing the filter directly, we can compute its spectrum (its discrete Fourier transform) more easily, i.e.,</p><formula xml:id="formula_15">F y [m] := F(F y ) = -1 j=0 CA j ω mj B = C(I -A )(I -Aω m ) -1 B, m = 0, 1, . . . , -1.</formula><p>where ω = exp(-2πi/ ) is the -th root of unity. This reduces to computing the quadratic form of the resolvent (I -Aω m ) -1 on the roots of unity (the powers of ω). Since A is a companion matrix, we can write A as a shift matrix plus a rank-1 matrix, A = S + ae T d . Thus Woodbury's formula reduces this computation to the resolvent of a shift matrix (I -Sω m ) -1 , with a rank-1 correction. This resolvent can be shown analytically to be a lower-triangular matrix consisting of roots of unity, and its quadratic form can be computed by the Fourier transform of a linear convolution of size d. Thus one can construct F y k by linear convolution and the FFT, resulting in O( log + d log d) time. We validate in Sec. 4.2.3 that Algorithm 1 leads to a wall-clock time speedup of 2× compared to computing the output filter naïvely by powering A. In App. B.2, we prove the time complexity O( log + d log d) and correctness of Algorithm 1. We also provide an extension to the closed-loop SSM, which can also be computed in subquadratic time as A + BK is a shift plus rank-2 matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Efficient Output Filter F y Computation</head><p>Require: A is a companion matrix parameterized by the last column a ) , . . . , q ( d/ ) ] and return the length-Fourier transform of the sum F (q (1) </p><formula xml:id="formula_16">∈ R d , B ∈ R d , C = C(I -A ) ∈ R d , sequence length . 1: Define quad(u, v) ∈ R for vectors u, v ∈ R d : compute q = u * v (linear convolution), zero-pad to length d/ , split into d/ chunks of size of the form [q (1</formula><formula xml:id="formula_17">+ • • • + q ( d/ )</formula><p>). 2: Compute the roots of unity z = [ω 0 , . . . , ω -1 ] where ω = exp(-2πi/ ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We test SpaceTime on a broad range of time series forecasting and classification tasks. In Sec. 4.1, we evaluate whether SpaceTime's contributions lead to state-of-the-art results on standard benchmarks. To help explain SpaceTime's performance and validate our contributions, in Sec. 4.2 we then evaluate whether these gains coincide with empirical improvements in expressiveness (Sec. 4.2.1), forecasting flexibility (Sec. 4.2.2), and training efficiency (Sec. 4.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results: Time Series Forecasting and Classification</head><p>For forecasting, we evaluate SpaceTime on 40 forecasting tasks from the popular Informer <ref type="bibr" target="#b79">[80]</ref> and Monash <ref type="bibr" target="#b22">[23]</ref> benchmarks, testing on horizons 8 to 960 time-steps long. For classification, we evaluate SpaceTime on seven medical ECG or speech audio classification tasks, which test on sequences up to 16,000 time-steps long. For all results, we report mean evaluation metrics over three seeds. denotes the method was computationally infeasible on allocated GPUs, e.g., due to memory constraints (same resources for all methods; see App. C for details). App. C also contains additional dataset, implementation, and hyperparameter details.</p><p>Informer (forecasting). We report univariate time series forecasting results in Table <ref type="table" target="#tab_1">1</ref>, comparing against recent state-of-the-art methods <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b80">81]</ref>, related state-space models <ref type="bibr" target="#b26">[27]</ref>, and other competitive deep architectures. We include extended results on additional horizons and multivariate forecasting in App. D.2. We find SpaceTime obtains lowest MSE and MAE on 14 and 11 forecasting settings respectively, 3× more than prior state-of-the-art. SpaceTime also outperforms S4 on 15 / 16 settings, supporting the companion SSM representation.</p><p>Monash (forecasting). We also evaluate on 32 datasets in the Monash forecasting benchmark <ref type="bibr" target="#b22">[23]</ref>, spanning domains including finance, weather, and traffic. For space, we report results in Table <ref type="table" target="#tab_2">20</ref> (App. D.3). We compare against 13 classical and deep learning baselines. SpaceTime achieves best RMSE on 7 tasks and sets new state-of-the-art average performance across all 32 datasets. SpaceTime's relative improvements also notably grow on long horizon tasks (Fig. <ref type="figure">6</ref>).</p><p>ECG (multi-label classification). Beyond forecasting, we show that SpaceTime can also perform state-of-the-art time series classification. To classify sequences, we use the same sequence model architecture in Sec. 3.1. Like prior work <ref type="bibr" target="#b26">[27]</ref>, we simply use the last-layer FFN to project from number of SSMs to number of classes, and mean pooling over length before a softmax to output class logits. In Table <ref type="table" target="#tab_2">2</ref>, we find that SpaceTime obtains best or second-best AUROC on five out of six tasks, outperforming both general sequence models and specialized architectures.</p><p>Speech Audio (single-label classification). We further test SpaceTime on long-range audio classification on the Speech Commands dataset <ref type="bibr" target="#b72">[73]</ref>. The task is classifying raw audio sequences  of length 16,000 into 10 word classes. We use the same pooling operation for classification as in ECG. SpaceTime outperforms domain-specific architectures, e.g., WaveGan-D <ref type="bibr" target="#b18">[19]</ref> and efficient Transformers, e.g., Performer <ref type="bibr" target="#b13">[14]</ref> (Table <ref type="table" target="#tab_3">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Improvement on Criteria for Effective Time Series Modeling</head><p>For further insight into SpaceTime's performance, we now validate that our contributions improve expressivity (4.2.1), forecasting ability (4.2.2), and efficiency (4.2.3) over existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Expressivity</head><p>To first study SpaceTime's expressivity, we test how well SpaceTime can fit controlled autoregressive processes. To validate our theory on SpaceTime's expressivity gains in Sec. 3.1, we compare against recent related SSM architectures such as S4 <ref type="bibr" target="#b26">[27]</ref> and S4D <ref type="bibr" target="#b28">[29]</ref>.</p><p>For evaluation, we generate noiseless synthetic AR(p) sequences. We test if models learn the true process by inspecting whether the trained model weights recover transfer functions specified by   SpaceTime captures AR(p) processes more precisely than similar deep SSM models such as S4 <ref type="bibr" target="#b26">[27]</ref> and S4D <ref type="bibr" target="#b28">[29]</ref>, forecasting future samples and learning ground-truth transfer functions more accurately.  the AR coefficients <ref type="bibr" target="#b52">[53]</ref>. We use simple 1-layer 1-SSM models, with state-space size equal to AR p, and predict one time-step given p lagged inputs (the smallest sufficient setting).</p><p>In Fig. <ref type="figure" target="#fig_5">3</ref> we compare the trained forecasts and transfer functions (as frequency response plots) of SpaceTime, S4, and S4D models on a relatively smooth AR(4) process and sharp AR(6) process. Our results support the relative expressivity of SpaceTime's companion matrix SSM. While all models accurately forecast the AR(4) time series, only SpaceTime recovers the ground-truth transfer functions for both, and notably forecasts the AR(6) process more accurately (Fig. <ref type="figure" target="#fig_5">3c,</ref> <ref type="figure">d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Long Horizon Forecasting</head><p>To next study SpaceTime's improved long horizon forecasting capabilities, we consider two additional long horizon tasks. First, we test on much longer horizons than prior settings (c.f., Table <ref type="table" target="#tab_1">1</ref>). Second, we test a new forecasting ability: how well methods trained to forecast one horizon transfer to longer horizons at test-time. For both, we use the popular Informer ETTh datasets. We compare SpaceTime with NLinear-the prior state-of-the-art on longer-horizon ETTh datasets-an FCN that learns a dense linear mapping between every lag input and horizon output <ref type="bibr" target="#b77">[78]</ref>.</p><p>We find SpaceTime outperforms NLinear on both long horizon tasks. On training to predict long horizons, SpaceTime consistently obtains lower MSE than NLinear on all settings (Table <ref type="table" target="#tab_4">4</ref>). On transferring to new horizons, SpaceTime models trained to forecast 192 time-step horizons transfer more accurately and consistently to forecasting longer horizons up to 576 time-steps (Fig. <ref type="figure" target="#fig_6">4</ref>). This suggests SpaceTime more convincingly learns the time series process; rather than only fitting to the specified horizon, the same model can generalize to new horizons.  <ref type="table" target="#tab_5">5</ref>). Our efficient algorithm (Sec. 3.3) is also important; it speeds up training by 2×, and makes SpaceTime's training time competitive with efficient models such as S4. On (2), we find SpaceTime also scales near-linearly with input sequence length, achieving 91% faster training time versus similarly recurrent LSTMs (Fig. <ref type="figure">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce SpaceTime, a state-space time series model. We achieve high expressivity by modeling SSMs with the companion matrix, long-horizon forecasting with a closed-loop SSM variant, and efficiency with a new algorithm to compute the companion SSM. We validate SpaceTime's proposed components on extensive time series forecasting and classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ethics Statement</head><p>A main objective of our work is to improve the ability to classify and forecast time series, which has real-world applications in many fields. These applications may have high stakes, such as classifying abnormalities in medical time series. In these situations, incorrect predictions may lead to harmful patient outcomes. It is thus critical to understand that while we aim to improve time series modeling towards these applications, we do not solve these problems. Further analysis and development into where models fail in time series modeling is necessary, including potentials intersections with research directions such as robustness and model biases when aiming to deploy machine learning models in real world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Reproducibility</head><p>We include code for the main results in</p><p>Table 1 at <ref type="url" target="https://github.com/HazyResearch/spacetime">https://github.com/HazyResearch/spacetime</ref>. We provide training hyperparameters and dataset details for each benchmark in Appendix C, discussing the Informer forecasting benchmark in Appendix C.1, the Monash forecasting benchmark in Appendix C.2, and the ECG and speech audio classification benchmarks in Appendix C.3. We provide proofs for all propositions and algorithm complexities in Appendix B. Appendix: Effectively Modeling Time Series with Simple Discrete State Spaces</p><p>Table of Contents A Related Work 19 A.1 Classical Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.2 Deep Learning Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B Proofs and Theoretical Discussion 21 B.1 Expressivity Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.2 Efficiency Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.3 Companion Matrix Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 C Experiment Details 28 C.1 Informer Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 C.2 Monash Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 C.3 Time Series Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D Extended experimental results 29 D.1 Expressivity on digital filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 D.2 Informer Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 D.3 Monash Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.4 ECG Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.5 Efficiency Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.6 SpaceTime Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 D.7 SpaceTime Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Classical Approaches</head><p>Classical approaches in time series modeling include the Box-Jenkins method <ref type="bibr" target="#b6">[7]</ref>, exponential smoothing <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b74">75]</ref>, autoregressive integrated moving average (ARIMA) <ref type="bibr" target="#b7">[8]</ref>, and state-space models <ref type="bibr" target="#b31">[32]</ref>. In such approaches, the model is usually manually selected based analyzing time series features (e.g., seasonality and order of non-stationarity), where the selected model is then fitted for each individual time series. While classical approaches may be more interpretable than recent deep learning techniques, the domain expertise and manual labor needed to succesfully apply them renders them infeasible to the common setting of modeling thousands, or millions, of time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Deep Learning Approaches</head><p>Recurrent models. Common deep learning architectures for modeling sequence data are the family of recurrent neural networks, which include GRUs <ref type="bibr" target="#b14">[15]</ref>, LSTMs <ref type="bibr" target="#b35">[36]</ref>, and DeepAR <ref type="bibr" target="#b60">[61]</ref>. However, due to the recurrent nature of RNNs, they are slow to train and may suffer from vanishing/exploding gradients, making them difficult to train <ref type="bibr" target="#b54">[55]</ref>.</p><p>Deep State Space models. Recent work has investigated combining the expressive strengths of SSMs with the scalable strengths of deep neural networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b57">58]</ref>. <ref type="bibr" target="#b57">[58]</ref> propose to train a global RNN that transforms input covariates to sequence-spcific SSM parameters; however, one downside of this approach is that they inherit the drawbacks of RNNs. More recent approaches, such as LSSL <ref type="bibr" target="#b27">[28]</ref>, S4 <ref type="bibr" target="#b26">[27]</ref>, S4D <ref type="bibr" target="#b28">[29]</ref>, and S5 <ref type="bibr" target="#b64">[65]</ref>, directly parameterize the layers of a neural network with multiple linear SSMs, and overcome common recurrent training drawbacks by leveraging the convolutional view of SSMs. While deep SSM models have been shown great promise in time series modeling, we show in our work -which builds off deep SSMs -that current deep SSM approaches are not able to capture autoregressive processes due to their continuous nature.</p><p>Neural differential equations as nonlinear state spaces. <ref type="bibr" target="#b11">[12]</ref> parametrizes the vector field of continuous-time autonomous systems. These models, termed Neural Differential Equations (NDEs) have seen extensive application to time series and sequences, first by <ref type="bibr" target="#b59">[60]</ref> and then by <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> with the notable extension to Neural Controlled Differential Equations (Neural CDEs). Neural CDEs can be considered the continuous-time, nonlinear version of state space models and RNNs <ref type="bibr" target="#b41">[42]</ref>. Rather than introducing nonlinearity between linear state space layers, Neural CDEs model nonlinear systems driven by a control input.</p><p>The NDE framework has been further applied by <ref type="bibr" target="#b55">[56]</ref> to model graph time series via Neural Graph Differential Equations. In <ref type="bibr" target="#b56">[57]</ref>, a continuous-depth ResNet generalization based on ODEs is proposed, and in <ref type="bibr" target="#b43">[44]</ref> numerical techniques to enable learning of stiff dynamical systems with Neural ODEs are investigated. The idea of parameterizing the vector field of a differential equation with a neural network, popularized by NDEs, can be traced back to earlier works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>Transformers. While RNNs and its variants have shown some success at time series modeling, a major limitation is their applicability to long input sequences. Since RNNs are recurrent by nature, they require long traversal paths to access past inputs, which leads to vanishing/exploding gradients and as a result struggle with capturing long-range dependencies.</p><p>To counteract the long-range dependency problem with RNNs, a recent line of work considers Transformers for time series modeling. The motivation is that due to the attention mechanism, a Transformer can directly model dependencies between any two points in the input sequence, independently of how far apart the points are. However, the high expressivity of the attention mechanism comes at the cost of the time and space complexity being quadratic in sequence length, making Transformers infeasible for very long sequences. As a result, many works consider specialized Transformer architectures with sparse attention mechanisms to bring down the quadratic complexity. For example, <ref type="bibr" target="#b5">[6]</ref> propose LogSparse self-attention, where a cell attends to a subset of past cells (as opposed to all cells), where closer cells are attended to more frequently, proportional to the log of their distance, which brings down complexity from O( 2 ) to O( (log ) 2 ). <ref type="bibr" target="#b79">[80]</ref> propose ProbSparse selfattention, which achieves O( log ) time and memory complexity, where they propose a generative style decoder to speed inference. <ref type="bibr" target="#b46">[47]</ref> propose a pyramidal attention mechanism which shows linear time and space complexity with sequence length. Autoformer <ref type="bibr" target="#b76">[77]</ref> suggests more specialization is needed in time series with a decomposition forecasting architecture, which extracts long-term stationary trend from the seasonal series and utilizes an auto-correlation mechanism, which discovers the period-based dependencies. <ref type="bibr" target="#b81">[82]</ref> believes previous attempts of Transformer-based architectures do not capture global statistical properties, and to do so requires an attention mechanism in the frequency domain. Confromer <ref type="bibr" target="#b29">[30]</ref> stacks convolutional and self-attention modules into a shared layer to combine the strengths of local interactions from convolutional modules and global interactions from self-attention modules. Perceiver AR <ref type="bibr" target="#b33">[34]</ref> builds on the Perceiver architecture, which reduces the computational complexity of transformers by performing self-attention in a latent space, and extends Perceiver's applicability to causal autoregressive generation.</p><p>While these works have shown exciting progress on time series forecasting, their proposed architectures are specialized to handle specific time series settings (e.g., long input sequences, or seasonal sequences), and are commonly trained to output a fixed target horizon length <ref type="bibr" target="#b79">[80]</ref>, i.e., as direct multi-step forecasting (DMS) <ref type="bibr" target="#b12">[13]</ref>. Thus, while effective at specific forecasting tasks, their setups are not obviously applicable to a broad range of time series settings (such as forecasting arbitrary horizon lengths, or generalizing to classification or regression tasks).</p><p>Moreover, <ref type="bibr" target="#b77">[78]</ref> showed that simpler alternatives to Transformers, such as data normalization plus a single linear layer (NLinear), can outperform these specialized Transformer architectures when similarly trained to predict the entire fixed forecasting horizons. Their results suggest that neither the attention mechanism nor the proposed modifications of these time series Transformers may be best suited for time series modeling. Instead, the success of these prior works may just be from learning to forecast the entire horizon with fully connected dependencies between prior time-step inputs and future time-step outputs, where a fully connected linear layer is sufficient.</p><p>Other deep learning methods. Other works also investigate pure deep learning architectures with no explicit temporal components, and show these models can also perform well on time series forecasting. <ref type="bibr" target="#b53">[54]</ref> propose N-BEATS, a deep architecture based on backward and forward residual links. Even simpler, <ref type="bibr" target="#b77">[78]</ref> investigate single linear layer models for time series forecasting. Both works show that simple architectures are capable of achieving high performance for time series forecasting. In particular, with just data normalization, the NLinear model in <ref type="bibr" target="#b77">[78]</ref> obtained state-of-the-art performance on the popular Informer benchmark <ref type="bibr" target="#b79">[80]</ref>. Given an input sequence of past lag terms and a target output sequence of future horizon terms, for every horizon output their model simply learns the fully connected dependencies between that output and every input lag sample. However, FCNs such as NLinear also carry inefficient downsides. Unlike Transformers and SSM-based models, the number of parameters for FCNs scales directly with input and output sequence length, i.e., O( h) for inputs and h outputs. Meanwhile, SpaceTime shows that the SSM can improve the modeling quality of deep architectures, while maintaining constant parameter count regardless of input or output length. Especially when forecasting long horizons, we achieve higher forecasting accuracy with smaller models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs and Theoretical Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Expressivity Results</head><p>Proposition 1. An SSM with a companion state matrix can represent i.</p><p>ARIMA <ref type="bibr" target="#b7">[8]</ref> ii.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exponential smoothing</head><p>iii. Controllable LTI systems <ref type="bibr" target="#b10">[11]</ref> Proof of Proposition 1. We show each case separately. We either provide a set of algebraic manipulations to obtain the desired model from a companion SSM, or alternatively invoke standard results from signal processing and system theory. i.</p><p>We start with a standard ARMA(p, q) model</p><formula xml:id="formula_18">y k = u k + q i=1 θ i u k-i + p i=1 φ i y k-i p i</formula><p>We consider two cases:</p><p>Case (1): Outputs y are a shifted (lag-1) version of the inputs u</p><formula xml:id="formula_19">y k+1 = y k + q i=1 θ i y k-i + p i=1 φ i y k-i+1 p i = (1 + φ 1 y k ) + q i=1 (θ i + φ i+1 )y k-i + p i=q+1 θ i y k-i<label>(16)</label></formula><p>where, without loss of generality, we have assumed that p &gt; q for notational convenience. The autoregressive system ( <ref type="formula" target="#formula_19">16</ref>) is equivalent to</p><formula xml:id="formula_20">A B C D =          0 0 . . . 0 0 1 1 0 . . . 0 0 0 . . . . . . . . . . . . . . . . . . 0 0 . . . 0 0 0 0 0 . . . 1 0 0 (1 + φ 1 ) (θ 1 + φ 2 ) . . . θ d-1 θ d 0          .</formula><p>in state-space form, with x ∈ R d and d = max(p, q). Note that the state-space formulation is not unique.</p><p>Case (2): Outputs y are "shaped noise". The ARMA(p,q) formulation (classically) defines inputs u as white noise samples<ref type="foot" target="#foot_0">foot_0</ref> , ∀k : p(u k ) is a normal distribution with mean zero and some variance. In this case, we can decompose the output as follows:</p><formula xml:id="formula_21">y ar k = p i=1 φ i y k-i p i y ma k = u k + q i=1 θ i u k-i</formula><p>such that y k = y ar k + y ma k . The resulting state-space models are:</p><formula xml:id="formula_22">A ar B ar C ar D ar =          0 0 . . . 0 0 1 1 0 . . . 0 0 0 . . . . . . . . . . . . . . . . . . 0 0 . . . 0 0 0 0 0 . . . 1 0 0 φ 1 φ 2 . . . φ p-1 φ p 0          . and A ma B ma C ma D ma =          0 0 . . . 0 0 1 1 0 . . . 0 0 0 . . . . . . . . . . . . . . . . . . 0 0 . . . 0 0 0 0 0 . . . 1 0 0 θ 1 θ 2 . . . θ q-1 θ q 1          .</formula><p>Note that A ar ∈ R p×p , A ma ∈ R q×q . More generally, our method can represent any ARMA process as the sum of two SpaceTime heads: one taking as input the time series itself, and one the driving signal u.</p><p>ARIMA ARIMA processes are ARMA(p, q) applied to differenced time series. For example, first-order differencing y k = u k -u k-1 . Differencing corresponds to high-pass filtering of the signal y, and can be thus be realized via a convolution <ref type="bibr" target="#b65">[66]</ref>.</p><p>Any digital filter that can be expressed as a difference equation admits a state-space representation in companion form <ref type="bibr" target="#b52">[53]</ref>, and hence can be learned by SpaceTime.</p><p>ii.</p><p>Simple exponential smoothing (SES) <ref type="bibr" target="#b8">[9]</ref> </p><formula xml:id="formula_23">y k = αy k-1 + α(1 -α)y k-2 + • • • + α(1 -α) p-1 y k-p<label>(17)</label></formula><p>is an AR process with a parametrization involving a single scalar 0 &lt; α &lt; 1 and can thus be represented in companion form as shown above.</p><p>iii.</p><p>Let (A, B, C) be any controllable linear system. Controllability corresponds to invertibility of the Krylov matrix [11, Thm 6.1, p145]</p><formula xml:id="formula_24">K(A, B) = [B, AB, . . . , A d-1 B], K(A, B) ∈ R d×d .</formula><p>From rank(K) = d, it follows that there exists a a ∈ R d</p><formula xml:id="formula_25">a 0 B + a 1 AB + • • • + a d-1 A d-1 B + A d B = 0. Thus AK = [AB, A 2 B, . . . , A d B] = [AB, A 2 B, . . . , A d-1 B column left shift of K , -(a 0 B + a 1 Ab + • • • + a d-1 A d-1 B) linear combination, columns of K ] = K(S f -ae d-1 )</formula><p>where G = (S fae d-1 ) is a companion matrix.</p><formula xml:id="formula_26">AK = KG ⇐⇒ G = K -1 AK.</formula><p>Therefore G is similar to A. We can then construct a companion form state space (G, B, C, D) from A using the relation above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2. No class of continuous-time LSSL SSMs can represent the noiseless AR(p) process.</head><p>Proof of Proposition 2. Recall from Sec. 3.1.1 that a noiseless AR(p) process is defined by</p><formula xml:id="formula_27">y t = p i=1 φ i y t-i = φ 1 y t-1 + . . . + φ p y t-p<label>(18)</label></formula><p>with coefficients φ 1 , . . . , φ p . This is represented by the SSM</p><formula xml:id="formula_28">x t+1 = Sx t + Bu t (<label>19</label></formula><formula xml:id="formula_29">)</formula><formula xml:id="formula_30">y t = Cx t + Du t (<label>20</label></formula><formula xml:id="formula_31">)</formula><p>when S ∈ R p×p is the shift matrix, B ∈ R p×1 is the first basis vector e 1 , C ∈ R 1×p is a vector of coefficients φ 1 , . . . , φ p , and D = 0, i.e.,</p><formula xml:id="formula_32">S =        0 0 . . . 0 0 1 0 . . . 0 0 0 1 . . . 0 0 . . . . . . . . . . . . 0 0 . . . 1 0        , B = 1 0 . . . 0 T , C = φ 1 . . . φ p<label>(21)</label></formula><p>We prove by contradiction that a continuous-time LSSL SSM cannot represent such a process. Consider the following solutions to a continuous-time system and a system <ref type="bibr" target="#b17">(18)</ref>, both in autonomous form</p><formula xml:id="formula_33">x cont t+1 = e A x t x disc t+1 = Sx t . It follows x cont t+1 = x disc t+1 ⇐⇒ e A = S ⇐⇒ A = log (S).</formula><p>we have reached a contradiction by [17, Theorem 1], as S is singular by definition and thus its matrix logarithm does not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Efficiency Results</head><p>We first prove that Algorithm 1 yields the correct output filter F y . We then analyze its time complexity, showing that it takes time O( log + d log d) for sequence length and state dimension d.</p><p>Theorem 1. Algorithm 1 returns the filter F y = (CB, . . . , CA -1 B).</p><p>Proof. We follow the outline of the proof in Section 3.3. Instead of computing F y directly, we compute its spectrum (its discrete Fourier transform):</p><formula xml:id="formula_34">F y [m] := F(F y ) = -1 j=0 CA j ω mj B = C(I-A )(I-Aω m ) -1 B = C(I-Aω m ) -1 B, m = 0, 1, . . . , -1.</formula><p>where ω = exp(-2πi/ ) is the -th root of unity. This reduces to computing the quadratic form of the resolvent (I -Aω m ) -1 on the roots of unity (the powers of ω). Since A is a companion matrix, we can write A as a shift matrix plus a rank-1 matrix, A = S + ae T d , where e d is the d-th basis vector [0, . . . , 0, 1] and the shift matrix S is:</p><formula xml:id="formula_35">S =        0 0 . . . 0 0 1 0 . . . 0 0 0 1 . . . 0 0 . . . . . . . . . . . . . . . 0 0 . . . 1 0       </formula><p>. Thus Woodbury's matrix identity (i.e., Sherman-Morrison formula) yields:</p><formula xml:id="formula_36">(I -Aω m ) -1 = (I -ω m S -ω m ae d ) -1 = (I -ω m S) -1 + (I -ω m S) -1 ω m ae d (I -ω m S) -1 1 -ω m e d (I -ω m S) -1 a .</formula><p>This is the resolvent of the shift matrix (I -ω m S) -1 , with a rank-1 correction. Hence</p><formula xml:id="formula_37">F y = C(I -ω m S) -1 B + C(I -ω m S) -1 ae d (I -ω m S) -1 B ω -m -e d (I -ω m S) -1 a .<label>(22)</label></formula><p>We now need to derive how to compute the quadratic form of a resolvent of the shift matrix efficiently. Fortunately the resolvent of the shift matrix has a very special structure that closely relates to the Fourier transform. We show analytically that:</p><formula xml:id="formula_38">(I -ω m S) -1 =        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        .</formula><p>It is easy to verify by multiplying this matrix with I -ω m S to see if we obtain the identity matrix.</p><p>Recall that multiplying with S on the left just shifts all the columns down by one index. Therefore:</p><formula xml:id="formula_39">       1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        (I -ω m S) =        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        -ω m S        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        =        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        -ω m        0 0 . . . 0 0 1 0 . . . 0 0 ω m 1 . . . 0 0 . . . . . . . . . . . . . . . ω (d-2)m ω (d-3)m . . . 1 0        =        1 0 . . . 0 0 ω m 1 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω m 1        -        0 0 . . . 0 0 ω m 0 . . . 0 0 ω 2m ω m . . . 0 0 . . . . . . . . . . . . . . . ω (d-1)m ω (d-2)m . . . ω 0        =I.</formula><p>Thus the resolvent of the shift matrix indeed has the form of a lower-triangular matrix containing the roots of unity. Now that we have the analytic formula of the resolvent, we can derive its quadratic form, given some vectors u, v ∈ R d . Substituting in, we have</p><formula xml:id="formula_40">u T (I -ω m S) -1 v = u 1 v 1 + u 2 v 1 ω m + u 2 v 2 + u 3 v 1 ω 2m + u 3 v 2 ω m + u 3 v 1 + . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grouping terms by powers of ω, we see that we want to compute u</head><formula xml:id="formula_41">1 v 1 + u 2 v 2 + • • • + u d v d , then u 2 v 1 + u 3 v 2 + • • • + u d v d-1</formula><p>, and so on. The term corresponding to ω km is exactly the k-th element of the linear convolution u * v. Define q = u * v, then u T (I -ω m S) -1 v is just the Fourier transform of u * v. To deal with the case where d &gt; , we note that the powers of roots of unity will repeat, so we just need to extend the output of u * v to be multiples of , then split them into chunk of size , then sum them up and take the length-Fourier transform. This is exactly the procedure quad(u, v) defined in Algorithm 1.</p><p>Once we have derived the quadratic form of the resolvent (I -ω m S) -1 , simply plugging it into the Woodbury's matrix identity (Equation ( <ref type="formula" target="#formula_37">22</ref>)) yields Algorithm 1.</p><p>We analyze the algorithm's complexity. Proof. We see that computing the quadratic form of the resolvent (I -ω m S) -1 involves a linear convolution of size d and a Fourier transform of size . The linear convolution can be done by performing an FFT of size 2d on both inputs, multiply them pointwise, then take the inverse FFT of size 2d. This has time complexity O(d log d). The Fourier transform of size has time complexity O( log ). The whole algorithm needs to compute four such quadratic forms, hence it takes time O( log + d log d).</p><p>Remark. We see that the algorithm easily extends to the case where the matrix A is a companion matrix plus low-rank matrix (of some rank k). We can write A as the sum of the shift matrix and a rank-(k + 1) matrix (since A itself is the sum of a shift matrix and a rank-1 matrix). Using the same strategy, we can use the Woodbury's matrix identity for the rank-(k + 1) case. The running time will then scale as O(k( log + d log d)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Companion Matrix Stability</head><p>Normalizing companion parameters for bounded gradients Proposition 3 (Bounded SpaceTime Gradients). Given s, the norm of the gradient of a SpaceTime layer is bounded for all k &lt; s if</p><formula xml:id="formula_42">d-1 i=0 |a i | = 1</formula><p>Proof. Without loss of generality, we assume x 0 = 0. Since the solution at time s is</p><formula xml:id="formula_43">y s = C s-1 i-1 A s-i-1 Bu i</formula><p>we compute the gradient w.r.t u k as</p><formula xml:id="formula_44">dy s du k = CA s-k-1 B.<label>(23)</label></formula><p>The largest eigenvalue of A</p><formula xml:id="formula_45">max{eig(A)} ≤ max 1, d-1 i=0 |a i | Corollary of Gershgorin [35, Theorem 1] = 1 using i |a i | = 1</formula><p>is 1, which implies convergence of the operator CA s-k-1 B. Thus, the gradients are bounded.</p><p>We use the proposition above to ensure gradient boundedness in SpaceTime layers by normalizing a every forward pass. performance of cardiologists and emergency residents in triaging ECGs, which would permit accurate interpretations in settings where specialists may not be present <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>We use the publicly available PTB-XL dataset <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref>, which contains 21,837 12-lead ECG recordings of 10 seconds each obtained from 18,885 patients. Each ECG recording is annotated by up to two cardiologists with one or more of the 71 ECG statements (labels). These ECG statements conform to the SCP-ECG standard <ref type="bibr" target="#b61">[62]</ref>. Each statement belongs to one or more of the following three categories -diagnostic, form, and rhythm statements. The diagnostic statements are further organised in a hierarchy containing 5 superclasses and 24 subclasses.</p><p>This provides six sets of annotations for the ECG statements based on the different categories and granularities: all (all ECG statements), diagnostic (only diagnostic statements including both subclass and superclass statements), diagnostic subclass (only diagnostic subclass statements), diagnostic superclass (only diagnostic superclass statements), form (only form statements), and rhythm (only rhythm statements). These six sets of annotations form different prediction tasks which are referred to as all, diag, sub-diag, super-diag, form, and rhythm respectively. The diagnostic superclass task is multi-class classification, and the other tasks are multi-label classification.</p><p>ECG classification training details. To tune SpaceTime and S4, we performed a grid search over the learning rate {0.01, 0.001}, model dropout {0.1, 0.2}, number of SSMs per layer {128, 256}, and number of layers {4, 6}, and chose the parameters that resulted in highest validation AUROC. The SSM state dimension was fixed to 64, with gated linear units as the non-linearity between stacked layers. We additionally apply layer normalization. We use a cosine learning rate scheduler, with a warmup period of 5 epochs. We train all models for 100 epochs.</p><p>Speech Commands training details. To train SpaceTime, we use the same hyperparameters used by S4: a learning rate of 0.01 with a plateau scheduler with patience 20, dropout of 0.1, 128 SSMs per layer, 6 layers, batch normalization, trained for 200 epochs.</p><p>Hardware details. For both ECG and Speech Commands, all experiments were run on a single NVIDIA Tesla A100 Ampere 40 GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extended experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Expressivity on digital filters</head><p>We experimentally verify whether SpaceTime can approximate the input-output map of digital filter admitting a state-space representation, with improved generalization over baseline models given test inputs of unseen frequencies.</p><p>We generate a dataset of 1028 sinusoidal signals of length 200</p><formula xml:id="formula_46">x(t) = sin (2πωt)</formula><p>where ω ∈ [2, 40] [50, 100] in the training set and ω ∈ <ref type="bibr" target="#b39">(40,</ref><ref type="bibr" target="#b49">50)</ref> in the test set. The outputs are obtained by filtering x, i.e., y = F(x) where F is in the family of digital filters.</p><p>We introduce common various sequence-to-sequence layers or models as baselines: the original S4 diagonal plus low-rank <ref type="bibr" target="#b26">[27]</ref>, a single-layer LSTM, a single 1d convolution (Conv1d), a dense linear layer (NLinear), a single self-attention layer. All models are trained for 800 epochs with batch size 256, learning rate 10 -3 and Adam. We repeat this experiment for digital filters of different orders <ref type="bibr" target="#b52">[53]</ref>. The results are shown in Figure <ref type="figure">8</ref>. SpaceTime learns to match the frequency response of the target filter, producing the correct output for inputs at test frequencies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 SpaceTime Ablations</head><p>To better understand how the proposed SpaceTime SSMs lead to the improved empirical performance, we include ablations on the individual closed-loop forecasting SSM (Section 3.2) and preprocessing SSMs (Section 3.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6.1 Closed-loop Forecasting SSM</head><p>To study how the closed-loop SSM improves long horizon forecasting accuracy, we remove the closed-loop SSM component in our default SpaceTime forecasting architecture (c.f., Appendix D.7, and compare the default SpaceTime with one without any closed-loop SSMs on Informer forecasting tasks. For models without closed-loop SSMs, we replace the last layer with the standard "open-loop" SSM framework in Section 3.1.2), and keep all other layers the same. Finally, for baseline comparison against another SSM without the closed-loop component, we compare against S4.</p><p>In Table <ref type="table" target="#tab_10">10</ref>, we report standardized MSE on Informer ETT datasets. Adding the closed-loop SSM consistently improves forecasting accuracy, on average lowering relative MSE by 33.2%. Meanwhile, even without the closed-loop SSM, SpaceTime outperforms S4, again suggesting that the companion matrix parameterization is beneficial for autoregressive time series forecasting. To study how the preprocessing SSM improves long horizon forecasting accuracy, we next compare how SpaceTime performs with and without the weight-initializing preprocessing SSMs introduced in Section 3.1.3. We compare the default SpaceTime architecture (Table <ref type="table" target="#tab_12">12</ref> with (1) replacing the preprocessing SSMs with randomly initialized default companion SSMs, and (2) removing the preprocessing SSMs altogether. For the former, we preserve the number of layers, but now train the first-layer SSM weights. For the latter, there is one-less layer, but the same number of trainable parameters (as we fix and freeze the weights for each preprocessing SSM).</p><p>In Table <ref type="table" target="#tab_11">11</ref>, we report standardized MSE on Informer ETT datasets. We find fixing the first layer SSMs of a SpaceTime network to preprocessing SSMs consistently improves forecasting performance, achieving 4.55% lower MSE on average than the ablation with just trainable companion matrices. Including the preprocessing layer also improves MSE by 9.26% on average compared to removing the layer altogether. These results suggest that preprocessing SSMs are beneficial for time series forecasting, e.g., by performing classic time series modeling techniques on input data. Unlike other approaches, SpaceTime is able to flexibly and naturally incorporate these operations into its network layers via simple weight initializations of the same general companion SSM structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 SpaceTime Architectures</head><p>We provide the specific SpaceTime architecture configurations used for forecasting and classification tasks. Each configuration follows the general architecture presented in Section 3.1 and Figure <ref type="figure" target="#fig_0">2</ref>, and consists of repeated Multi-SSM SpaceTime layers. We first provide additional details on specific instantiations of the companion SSMs we use in our models, e.g., how we instantiate preprocessing SSMs to recover specific techniques (Section 3.1.3). We then include the layer-specific details of the number and type of SSM used in each network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7.1 Specific SSM parameterizations</head><p>In Section 3.1.1, we described the general form of the companion SSM used in this work. By default, for any individual SSM we learn the a column in A and the vectors B, C as trainable parameters in a neural net module. We refer to these SSMs specifically as companion SSMs.</p><p>In addition, as discussed in Sections 3.1.1 and 3.1.3, we can also fix a, B, or C to specific values to recover useful operations when computing the SSM outputs. We describe specific instantiations of the companion SSM used in our models below (with dimensionality referring to one SSM).</p><p>Shift SSM. We fix the a vector in the companion state matrix A ∈ R d×d to the 0 vector ∈ R d , such that A is the shift matrix (see Eq. 21 for an example). This is a generalization of a 1-D "sliding window" convolution with fixed kernel size equal to SSM state dimension d. To see how, note that if B is also fixed to the first basis vector e 1 ∈ R d×1 , then this exactly recovers a 1-D convolution with kernel determined by C.</p><p>Differencing SSM. As a specific version of the preprocessing SSM discussed in Section 3.1.3, we fix a = 0, B = e 1 , and set C to recover various order differencing when computing the SSM, i.e., C = 1 0 0 0 0 . . . 0 (0-order differencing, i.e., an identity function)</p><p>C = 1 -1 0 0 0 . . . 0 (1st-order differencing) (25)</p><formula xml:id="formula_48">C = 1 -2 1 0 0 . . . 0 (2nd-order differencing)<label>(26)</label></formula><p>C = 1 -3 3 -1 0 . . . 0 (3rd-order differencing)</p><p>In this work, we only use the above 0, 1st, 2nd, or 3rd-order differencing instantiations. With multiple differencing SSMs in a multi-SSM SpaceTime layer, we initialize differencing SSMs by running through the orders repeatedly in sequence. For example, given five differencing SSMs, the first four SSMs perform 0, 1st, 2nd, and 3rd-order differencing respectively, while the fifth performs 0-order differencing again.</p><p>Moving Average Residual (MA residual) SSM. As another version of the preprocessing SSM, we can fix a = 0, B = e 1 , and set C such that the SSM outputs sample residuals from a moving average applied over the input sequence. For an n-order moving average, we compute outputs with C specified as</p><formula xml:id="formula_50">C = 1 -1/n, -1/n, . . . -1/n, 0 . . . 0</formula><p>(n-order moving average residual)</p><p>For each MA residual SSM, we randomly initialize the order by uniform-randomly sampling an integer in the range <ref type="bibr">[4, d]</ref>, where d is again the state-space dimension size (recall C ∈ R 1×d ). We pick 4 as a heuristic which was not finetuned; we leave additional optimization here for further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7.2 Task-specific SpaceTime Architectures</head><p>Here we provide layer-level details on the SpaceTime networks used in this work. For each task, we describe number of layers, number of SSMs per layer, state-space dimension (fixed for all SSMs in a network), and which SSMs are used in each layer.</p><p>Expanding on this last detail, as previously discussed in Section 3.1.2, in each SpaceTime layer we can specify multiple SSMs in each layer, computing their outputs in parallel to produce a multidimensional output that is fed as the input to the next SpaceTime layer. The "types" of SSMs do not all have to be the same per layer, and we list the type (companion, shift, differencing, MA residual) and closed-loop designation (standard, closed-loop) of the SSMs in each layer below.</p><p>For an additional visual overview of a SpaceTime network, please refer back to Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>Forecasting: Informer and Monash. We describe the architecture in Table <ref type="table" target="#tab_12">12</ref>. We treat the first SpaceTime layer as "preprocessing" layer, which performs differencing and moving average residual operations on the input sequence. We treat the last SpaceTime layer as a "forecasting" layer, which autoregressively outputs future horizon predictions given the second-to-last layer's outputs as an input sequence.</p><p>Classification: ECG. We describe the architectures for each ECG classification task in Tables 13-18. For all models, we use state-space dimension d = 64. As described in the experiments, for classification we compute logits with a mean pooling over the output sequence, where pooling is computed over the sequence length.</p><p>Classification: Speech Audio. We describe the architecture for the Speech Audio task in Table <ref type="table" target="#tab_19">19</ref>. We use state-space dimension d = 1024. As described in the experiments, for classification we compute logits with a mean pooling over the output sequence, where pooling is computed over the sequence length. Table 20: Monash forecasting. Test RMSE of SpaceTimefor each dataset (best result selected via validation RMSE, average of 3 runs). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SpaceTime architecture and components. (Left): Each SpaceTime layer carries weights that model multiple companion SSMs, followed optionally by a nonlinear FFN. The SSMs are learned in parallel (1) and computed as a single matrix multiplication (2). (Right): We stack these layers into a SpaceTime network, where earlier layers compute SSMs as convolutions for fast sequence-to-sequence modeling and data preprocessing, while a decoder layer computes SSMs as recurrences for dynamic forecasting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proposition 2 .</head><label>2</label><figDesc>No class of continuous-time LSSL SSMs can represent the noiseless AR(p) process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 : 4 :</head><label>34</label><figDesc>Compute F y = quad( C, B) + quad( C, a) * quad(e d , B)/(z -quad(e d , a)) ∈ R , where e d = [0, . . . , 0, 1] is the d-th basis vector.Return the inverse Fourier transform F y = F -1 ( F y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>AR(4) Transfer Func.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>AR<ref type="bibr" target="#b5">(6)</ref> Transfer Func.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: AR(p) expressiveness benchmarks. SpaceTime captures AR(p) processes more precisely than similar deep SSM models such as S4<ref type="bibr" target="#b26">[27]</ref> and S4D<ref type="bibr" target="#b28">[29]</ref>, forecasting future samples and learning ground-truth transfer functions more accurately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Forecasting transfer. Mean MSE (±1 standard deviation). SpaceTime transfers more accurately and consistently to horizons not used for training versus NLinear [78].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 2 .</head><label>2</label><figDesc>Algorithm 1 has time complexity O( log + d log d) for sequence length and state dimension d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Training wall-clock time versus horizon length for SpaceTime, S4, LSTM, and Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="3,58.66,211.61,537.23,99.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Univariate forecasting results on Informer Electricity Transformer Temperature (ETT) datasets<ref type="bibr" target="#b79">[80]</ref>. Best results in bold. SpaceTime results reported as means over three seeds. We include additional datasets, horizons, and method comparisons in App. D.2</figDesc><table><row><cell cols="2">Methods</cell><cell cols="2">SpaceTime</cell><cell cols="2">NLinear</cell><cell>FILM</cell><cell></cell><cell>S4</cell><cell></cell><cell cols="2">FedFormer</cell><cell cols="2">Autoformer</cell><cell cols="2">Informer</cell><cell cols="2">ARIMA</cell></row><row><cell></cell><cell>Metric</cell><cell cols="16">MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE</cell></row><row><cell>ETTh1</cell><cell>96 192 336 720</cell><cell cols="16">0.054 0.181 0.053 0.177 0.055 0.178 0.316 0.490 0.079 0.215 0.071 0.206 0.193 0.377 0.058 0.184 0.066 0.207 0.069 0.204 0.072 0.207 0.345 0.516 0.104 0.245 0.114 0.262 0.217 0.395 0.073 0.209 0.069 0.212 0.081 0.226 0.083 0.229 0.825 0.846 0.119 0.270 0.107 0.258 0.202 0.381 0.086 0.231 0.076 0.222 0.080 0.226 0.090 0.240 0.190 0.355 0.142 0.299 0.126 0.283 0.183 0.355 0.103 0.253</cell></row><row><cell>ETTh2</cell><cell>96 192 336 720</cell><cell cols="16">0.119 0.268 0.129 0.278 0.127 0.272 0.381 0.501 0.128 0.271 0.153 0.306 0.213 0.373 0.273 0.407 0.151 0.306 0.169 0.324 0.182 0.335 0.332 0.458 0.185 0.330 0.204 0.351 0.227 0.387 0.315 0.446 0.169 0.332 0.194 0.355 0.204 0.367 0.655 0.670 0.231 0.378 0.246 0.389 0.242 0.401 0.367 0.488 0.188 0.352 0.225 0.381 0.241 0.396 0.630 0.662 0.278 0.420 0.268 0.409 0.291 0.439 0.413 0.519</cell></row><row><cell>ETTm1</cell><cell>96 192 336 720</cell><cell cols="16">0.026 0.121 0.026 0.122 0.029 0.127 0.651 0.733 0.033 0.140 0.056 0.183 0.109 0.277 0.033 0.136 0.039 0.152 0.039 0.149 0.041 0.153 0.190 0.372 0.058 0.186 0.081 0.216 0.151 0.310 0.049 0.169 0.051 0.173 0.052 0.172 0.053 0.175 0.428 0.581 0.084 0.231 0.076 0.218 0.427 0.591 0.065 0.196 0.074 0.213 0.073 0.207 0.071 0.205 0.254 0.433 0.102 0.250 0.110 0.267 0.438 0.586 0.089 0.231</cell></row><row><cell>ETTm2</cell><cell>96 192 336 720</cell><cell cols="16">0.060 0.179 0.063 0.182 0.065 0.189 0.153 0.318 0.067 0.198 0.065 0.189 0.088 0.225 0.211 0.340 0.090 0.222 0.090 0.223 0.094 0.233 0.183 0.350 0.102 0.245 0.118 0.256 0.132 0.283 0.237 0.371 0.113 0.255 0.117 0.259 0.124 0.274 0.204 0.367 0.130 0.279 0.154 0.305 0.180 0.336 0.264 0.396 0.166 0.318 0.170 0.318 0.173 0.323 0.482 0.567 0.178 0.325 0.182 0.335 0.300 0.435 0.310 0.441</cell></row><row><cell></cell><cell>Count</cell><cell>14</cell><cell>11</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>ECG</figDesc><table><row><cell>Task AUROC</cell><cell>All</cell><cell cols="5">Diag Sub-diag Super-diag Form Rhythm</cell></row><row><cell>SpaceTime</cell><cell cols="2">0.936 0.941</cell><cell>0.933</cell><cell>0.929</cell><cell>0.883</cell><cell>0.967</cell></row><row><cell>S4</cell><cell cols="2">0.938 0.939</cell><cell>0.929</cell><cell>0.931</cell><cell>0.895</cell><cell>0.977</cell></row><row><cell>Inception-1D</cell><cell cols="2">0.925 0.931</cell><cell>0.930</cell><cell>0.921</cell><cell>0.899</cell><cell>0.953</cell></row><row><cell>xRN-101</cell><cell cols="2">0.925 0.937</cell><cell>0.929</cell><cell>0.928</cell><cell>0.896</cell><cell>0.957</cell></row><row><cell>LSTM</cell><cell cols="2">0.907 0.927</cell><cell>0.928</cell><cell>0.927</cell><cell>0.851</cell><cell>0.953</cell></row><row><cell>Transformer</cell><cell cols="2">0.857 0.876</cell><cell>0.882</cell><cell>0.887</cell><cell>0.771</cell><cell>0.831</cell></row><row><cell cols="3">Wavelet + NN 0.849 0.855</cell><cell>0.859</cell><cell>0.874</cell><cell>0.757</cell><cell>0.890</cell></row></table><note><p>statement classification on PTB-XL (100 Hz version). Baseline AUROC from<ref type="bibr" target="#b66">[67]</ref> </p><p>(error bars in App. D.4).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Speech Audio classification<ref type="bibr" target="#b72">[73]</ref> </figDesc><table><row><cell>Method</cell><cell>Acc. (%)</cell></row><row><cell>SpaceTime</cell><cell>97.29</cell></row><row><cell>S4</cell><cell>98.32</cell></row><row><cell>LSSL</cell><cell></cell></row><row><cell>WaveGan-D</cell><cell>96.25</cell></row><row><cell>Transformer</cell><cell></cell></row><row><cell>Performer</cell><cell>30.77</cell></row><row><cell>CKConv</cell><cell>71.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Longer horizon forecasting on Informer ETTh data. Standardized MSE reported. SpaceTime obtains lower MSE when forecasting longer horizons.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.5 2.0</cell><cell>ETTh1 | Train Horizon 192 NLinear SpaceTime</cell><cell>0.5</cell><cell>ETTh2 | Train Horizon 192 NLinear SpaceTime</cell></row><row><cell cols="2">Dataset Horizon</cell><cell>720</cell><cell>960 1080 1440 1800 1920</cell><cell>MSE</cell><cell>0.0 0.5 1.0</cell><cell cols="2">100 200 300 400 500 600 Test Horizon 0.1 0.2 0.3 0.4</cell><cell>Test Horizon 100 200 300 400 500 600</cell></row><row><cell>ETTh1</cell><cell cols="3">NLinear SpaceTime 0.075 0.074 0.072 0.080 0.081 0.088 0.080 0.089 0.085 0.094 0.102 0.104</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ETTh2</cell><cell cols="3">NLinear SpaceTime 0.188 0.225 0.265 0.299 0.438 0.459 0.224 0.273 0.290 0.329 0.450 0.493</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Train wall-clock time. Seconds per epoch when training on ETTh1 data.</figDesc><table><row><cell>Method SpaceTime → No Algorithm 1 S4 Transformer</cell><cell># params 148k 148k 151k 155k</cell><cell>seconds/epoch 66 49 240 132</cell><cell>Epoch wall-clock time</cell><cell>0 50 100 150 200 250 300 350</cell><cell>Input sequence length 250 500 750 1000 1250 1500 1750 2000 SpaceTime S4 LSTM Transformer</cell></row><row><cell>LSTM</cell><cell>145k</cell><cell>336</cell><cell cols="3">Figure 5: Wall-clock time scaling. Empir-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">ically, SpaceTime scales near-linearly with</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">input sequence length.</cell></row></table><note><p>4.2.3 Efficiency</p><p>To finally study if our companion matrix algorithm enables efficient training on long sequences, we conduct two speed benchmarks. We (1) compare the wall-clock time per training epoch of SpaceTime to standard sequence models, e.g., LSTMs and Transformers, with similar pararemeter counts, and (2) empirically test our theory in Sec. 3.3, which suggests SpaceTime trains near-linearly with sequence length and state dimension. For (1), we use ETTh1 data with lag and horizon 720 time-steps long. For (2), we use synthetic data, scaling sequences from 100-2000 time-steps long.</p><p>On (1) we find SpaceTime reduces clock time on ETTh1 by 73% and 80% compared to Transformers and LSTMs (Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparing sequence models on the task of approximating the input-output map defined by digital filters of different orders. Test RMSE on held-out inputs at unseen frequencies.</figDesc><table><row><cell>Filter</cell><cell cols="2">Order SpaceTime</cell><cell>S4</cell><cell cols="2">Conv1D LSTM NLinear Transformer</cell></row><row><cell>Butterworth</cell><cell>2</cell><cell>0.0055</cell><cell cols="2">0.0118 0.0112 0.0115 1.8420</cell><cell>0.5535</cell></row><row><cell></cell><cell>3</cell><cell>0.0057</cell><cell cols="2">0.3499 0.0449 0.0231 1.7085</cell><cell>0.6639</cell></row><row><cell></cell><cell>10</cell><cell>0.0039</cell><cell cols="2">0.8077 0.4747 0.2753 1.5162</cell><cell>0.7191</cell></row><row><cell>Chebyshev 1</cell><cell>2</cell><cell>0.0187</cell><cell cols="2">0.0480 0.0558 0.0285 1.9313</cell><cell>0.2452</cell></row><row><cell></cell><cell>3</cell><cell>0.0055</cell><cell cols="2">0.0467 0.0615 0.0178 1.8077</cell><cell>0.4028</cell></row><row><cell></cell><cell>10</cell><cell>0.0620</cell><cell cols="2">0.6670 0.1961 0.1463 1.5069</cell><cell>0.7925</cell></row><row><cell>Chebyshev 2</cell><cell>2</cell><cell>0.0112</cell><cell cols="2">0.0121 0.0067 0.0019 0.4101</cell><cell>0.0030</cell></row><row><cell></cell><cell>3</cell><cell>0.0201</cell><cell cols="2">0.0110 0.0771 0.0102 0.4261</cell><cell>0.0088</cell></row><row><cell></cell><cell>10</cell><cell>0.0063</cell><cell cols="2">0.6209 0.3361 0.1911 1.5584</cell><cell>0.7936</cell></row><row><cell>Elliptic</cell><cell>2</cell><cell>0.0001</cell><cell cols="2">0.0300 0.0565 0.0236 1.9150</cell><cell>0.2445</cell></row><row><cell></cell><cell>3</cell><cell>0.0671</cell><cell cols="2">0.0868 0.0551 0.0171 1.8782</cell><cell>0.4198</cell></row><row><cell></cell><cell>10</cell><cell>0.0622</cell><cell cols="2">0.0909 0.1352 0.1344 1.4901</cell><cell>0.7368</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Univariate forecasting results on Informer datasets. Best results in bold. SpaceTime obtains best MSE on 19 out of 25 and best MAE on 20 out of 25 dataset and horizon tasks.Relative test RMSE rankings ( * /13 models) across different slices of the 33 datasets in the Monash repository<ref type="bibr" target="#b22">[23]</ref>. SpaceTime sets best overall ranking across all tasks and is significantly more accurate on tasks involving long forecast horizon and larger number of training samples.</figDesc><table><row><cell></cell><cell cols="4">Monash Model Rankings</cell></row><row><cell cols="2">Horizon</cell><cell></cell><cell>Long 4.25 3.08</cell><cell></cell></row><row><cell>Short</cell><cell>5.14 5.10</cell><cell>5.76</cell><cell cols="2">5.67 6.50 6.77 5.67 5.15 4.58</cell><cell>Short Context</cell></row><row><cell cols="2">Large 2.30</cell><cell cols="2">5.37 4.25 5.63 5.42</cell><cell>4.01 3.03</cell><cell>Long</cell></row><row><cell cols="2">Dataset</cell><cell></cell><cell>Small</cell><cell></cell></row><row><cell cols="3">SpaceTime</cell><cell>DeepAR</cell><cell cols="2">TBATS</cell></row><row><cell>0 Figure 6: 0 100 200 300 400 500 Epoch wall-clock time</cell><cell cols="5">200 Horizon sequence length 400 600 800 SpaceTime S4 LSTM Transformer</cell><cell>1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Closed-loop SSM Ablation We ablate the closed-loop SSM component in SpaceTime, comparing against the prior S4 SSM on four Informer time series forecasting tasks. Removing the closed-loop SSM consistently hurts forecasting accuracy for SpaceTime.</figDesc><table><row><cell></cell><cell cols="2">ETTh1 (720) ETTh2 (720) ETTm1 (720) ETTm2 (720)</cell></row><row><cell>Method / Ablation</cell><cell>MSE MAE MSE MAE MSE MAE MSE</cell><cell>MAE</cell></row><row><cell>SpaceTime</cell><cell>0.076 0.222 0.188 0.352 0.074 0.213 0.166</cell><cell>0.318</cell></row><row><cell cols="2">SpaceTime No Closed-loop 0.114 0.271 0.278 0.431 0.156 0.310 0.213</cell><cell>0.365</cell></row><row><cell>S4 (No Closed-loop)</cell><cell>0.190 0.355 0.630 0.662 0.254 0.433 0.482</cell><cell>0.567</cell></row><row><cell>D.6.2 Preprocessing SSM</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Preprocessing SSM Ablation We ablate the preprocessing SSM layer in SpaceTime, comparing against either replacing the SSMs with companion SSMs (Companion) or removing the layer (Removed). Including preprocessing SSMs consistently improves forecasting accuracy.</figDesc><table><row><cell>ETTh1 (720) ETTh2 (720) ETTm1 (720) ETTm2 (720)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>SpaceTime forecasting architecture. For all SSMs, we keep state-space dimension d = 128. Repeated Identity denotes repeating the input to match the number of SSMs in the next layer, i.e., 128 SSMs in this case. For each forecasting task, d denotes time series samples' number of features, denotes the lag size (number of past samples given as input), and h denotes the horizon size (number of future samples to be predicted).</figDesc><table><row><cell>Layer</cell><cell>Details</cell><cell></cell><cell cols="2">Input Size Output Size</cell></row><row><cell>Decoder</cell><cell>Linear</cell><cell></cell><cell>128 ×</cell><cell>d × h</cell></row><row><cell>SSM Layer 3</cell><cell>Companion (closed-loop)</cell><cell>× 128</cell><cell>128 ×</cell><cell>128 ×</cell></row><row><cell>SSM Layer 2</cell><cell>Companion (standard)</cell><cell>× 128</cell><cell>128 ×</cell><cell>128 ×</cell></row><row><cell></cell><cell>Differencing (standard)</cell><cell>× 64</cell><cell></cell><cell></cell></row><row><cell>SSM Layer 1</cell><cell></cell><cell></cell><cell>128 ×</cell><cell>128 ×</cell></row><row><cell></cell><cell>MA Residual (standard)</cell><cell>× 64</cell><cell></cell><cell></cell></row><row><cell>Encoder</cell><cell cols="2">Repeated Identity</cell><cell>d ×</cell><cell>128 ×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>SpaceTime architecture for ECG SuperDiagnostic classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 5.</figDesc><table><row><cell>Layer</cell><cell>Details</cell><cell></cell><cell cols="2">Input Size Output Size</cell></row><row><cell>Classifier</cell><cell cols="2">Mean Pooling</cell><cell>c ×</cell><cell>c × 1</cell></row><row><cell>Decoder</cell><cell>Linear</cell><cell></cell><cell>256 ×</cell><cell>c ×</cell></row><row><cell>SSM Layer 5</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 4</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 3</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 2</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 1</cell><cell>Differencing (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>Encoder</cell><cell>Linear</cell><cell></cell><cell>d ×</cell><cell>256 ×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>SpaceTime architecture for ECG SubDiagnostic classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 23.</figDesc><table><row><cell>Layer</cell><cell>Details</cell><cell></cell><cell cols="2">Input Size Output Size</cell></row><row><cell>Classifier</cell><cell cols="2">Mean Pooling</cell><cell>c ×</cell><cell>c × 1</cell></row><row><cell>Decoder</cell><cell>Linear</cell><cell></cell><cell>256 ×</cell><cell>c ×</cell></row><row><cell>SSM Layer 5</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 4</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 3</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 2</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 1</cell><cell>Differencing (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>Encoder</cell><cell>Linear</cell><cell></cell><cell>d ×</cell><cell>256 ×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>SpaceTime architecture for ECG Diagnostic classification. For all SSMs, we keep statespace dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 44.</figDesc><table><row><cell>Layer</cell><cell>Details</cell><cell></cell><cell cols="2">Input Size Size</cell></row><row><cell>Classifier</cell><cell cols="2">Mean Pooling</cell><cell>c ×</cell><cell>c × 1</cell></row><row><cell>Decoder</cell><cell>Linear</cell><cell></cell><cell>256 ×</cell><cell>c ×</cell></row><row><cell>SSM Layer 5</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 4</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 3</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 2</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 1</cell><cell>Differencing (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>Encoder</cell><cell>Linear</cell><cell></cell><cell>d ×</cell><cell>256 ×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>SpaceTime architecture for ECG Form classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 19.</figDesc><table><row><cell>Layer</cell><cell>Details</cell><cell></cell><cell cols="2">Input Size Output Size</cell></row><row><cell>Classifier</cell><cell cols="2">Mean Pooling</cell><cell>c ×</cell><cell>c × 1</cell></row><row><cell>Decoder</cell><cell>Linear</cell><cell></cell><cell>256 ×</cell><cell>c ×</cell></row><row><cell>SSM Layer 5</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 4</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 3</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 2</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell></cell><cell>Differencing (standard)</cell><cell>× 192</cell><cell></cell><cell></cell></row><row><cell>SSM Layer 1</cell><cell></cell><cell></cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell></cell><cell>MA Residual (standard)</cell><cell>× 64</cell><cell></cell><cell></cell></row><row><cell>Encoder</cell><cell>Linear</cell><cell></cell><cell>d ×</cell><cell>256 ×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 :</head><label>17</label><figDesc>SpaceTime architecture for ECG Rhythm classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 12.</figDesc><table><row><cell>Layer</cell><cell cols="2">Details</cell><cell></cell><cell cols="2">Input Size Output Size</cell></row><row><cell>Classifier</cell><cell cols="3">Mean Pooling</cell><cell>c ×</cell><cell>c × 1</cell></row><row><cell>Decoder</cell><cell cols="2">Linear</cell><cell></cell><cell>256 ×</cell><cell>c ×</cell></row><row><cell></cell><cell cols="2">Companion (standard)</cell><cell>× 128</cell><cell></cell></row><row><cell>SSM Layer 5</cell><cell></cell><cell></cell><cell></cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell></cell><cell>Shift (standard)</cell><cell cols="2">× 128</cell><cell></cell></row><row><cell></cell><cell cols="2">Companion (standard)</cell><cell>× 128</cell><cell></cell></row><row><cell>SSM Layer 4</cell><cell></cell><cell></cell><cell></cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell></cell><cell>Shift (standard)</cell><cell cols="2">× 128</cell><cell></cell></row><row><cell></cell><cell cols="2">Companion (standard)</cell><cell>× 128</cell><cell></cell></row><row><cell>SSM Layer 3</cell><cell></cell><cell></cell><cell></cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell></cell><cell>Shift (standard)</cell><cell cols="2">× 128</cell><cell></cell></row><row><cell></cell><cell cols="2">Companion (standard)</cell><cell>× 128</cell><cell></cell></row><row><cell>SSM Layer 2</cell><cell></cell><cell></cell><cell></cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell></cell><cell>Shift (standard)</cell><cell cols="2">× 128</cell><cell></cell></row><row><cell>SSM Layer 1</cell><cell cols="2">Differencing (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>Encoder</cell><cell cols="2">Linear</cell><cell></cell><cell>d ×</cell><cell>256 ×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 18 :</head><label>18</label><figDesc>SpaceTime architecture for ECG All classification. For all SSMs, we keep state-space dimension d = 64. Input samples have d = 12 features and are length = 1000 time-steps long. The number of classes c = 71.</figDesc><table><row><cell>Layer</cell><cell>Details</cell><cell></cell><cell cols="2">Input Size Output Size</cell></row><row><cell>Classifier</cell><cell cols="2">Mean Pooling</cell><cell>c ×</cell><cell>c × 1</cell></row><row><cell>Decoder</cell><cell>Linear</cell><cell></cell><cell>256 ×</cell><cell>c ×</cell></row><row><cell>SSM Layer 5</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 4</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 3</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 2</cell><cell>Shift (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell></cell><cell>Differencing (standard)</cell><cell>× 192</cell><cell></cell><cell></cell></row><row><cell>SSM Layer 1</cell><cell></cell><cell></cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell></cell><cell>MA Residual (standard)</cell><cell>× 64</cell><cell></cell><cell></cell></row><row><cell>Encoder</cell><cell>Linear</cell><cell></cell><cell>d ×</cell><cell>256 ×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 19 :</head><label>19</label><figDesc>SpaceTime architecture for Speech Audio classification. For all SSMs, we keep state-space dimension d = 1024. Input samples have d = 1 features and are length = 16000 time-steps long. The number of classes c = 10.</figDesc><table><row><cell>Layer</cell><cell>Details</cell><cell></cell><cell cols="2">Input Size Output Size</cell></row><row><cell>Classifier</cell><cell cols="2">Mean Pooling</cell><cell>c ×</cell><cell>c × 1</cell></row><row><cell>Decoder</cell><cell>Linear</cell><cell></cell><cell>256 ×</cell><cell>c ×</cell></row><row><cell>SSM Layer 6</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 5</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 4</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 3</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 2</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>SSM Layer 1</cell><cell>Companion (standard)</cell><cell>× 256</cell><cell>256 ×</cell><cell>256 ×</cell></row><row><cell>Encoder</cell><cell>Linear</cell><cell></cell><cell>d ×</cell><cell>256 ×</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Other formulations with forecast residuals are also common.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A task can belong to multiple splits, resulting in overlapping splits. For example, a task can involve both long context as well as long forecasting horizon.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>We thank <rs type="person">Albert Gu</rs>, <rs type="person">Yining Chen</rs>, <rs type="person">Dan Fu</rs>, <rs type="person">Ke Alexander Wang</rs>, and <rs type="person">Rose Wang</rs> for helpful discussions and feedback. We also gratefully acknowledge the support of <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); <rs type="institution">US DEVCOM ARL</rs> under No. <rs type="grantNumber">W911NF-21-2-0251</rs> (<rs type="funder">Interactive Human-AI Teaming)</rs>; <rs type="funder">ONR</rs> under No. <rs type="grantNumber">N000141712266</rs> (<rs type="affiliation">Unifying Weak Supervision</rs>); <rs type="grantNumber">ONR N00014-20-1-2480</rs>: <rs type="projectName">Understanding and Applying Non-Euclidean Geometry in Machine Learning</rs>; <rs type="grantNumber">N000142012275</rs> (NEPTUNE); NXP, <rs type="funder">Xilinx</rs>, <rs type="affiliation">LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF</rs>, <rs type="funder">Accenture</rs>, <rs type="funder">Ericsson</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Analog Devices</rs>, <rs type="person">Google Cloud</rs>, <rs type="funder">Salesforce</rs>, <rs type="funder">Total</rs>, the <rs type="institution">HAI-GCP Cloud Credits for Research program</rs>, the <rs type="funder">Stanford Data Science Initiative (SDSI)</rs>, and members of the Stanford DAWN project: <rs type="funder">Facebook</rs>, <rs type="person">Google</rs>, and <rs type="funder">VMWare</rs>. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of <rs type="funder">NIH</rs>, <rs type="institution">ONR</rs>, or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2g3rNje">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_yj99eb7">
					<idno type="grant-number">CCF1763315</idno>
				</org>
				<org type="funding" xml:id="_7M7q4ve">
					<idno type="grant-number">W911NF-21-2-0251</idno>
				</org>
				<org type="funding" xml:id="_NBaFtaF">
					<idno type="grant-number">N000141712266</idno>
				</org>
				<org type="funded-project" xml:id="_6TCVEvc">
					<idno type="grant-number">ONR N00014-20-1-2480</idno>
					<orgName type="project" subtype="full">Understanding and Applying Non-Euclidean Geometry in Machine Learning</orgName>
				</org>
				<org type="funding" xml:id="_BDEjdX7">
					<idno type="grant-number">N000142012275</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Informer Forecasting</head><p>Dataset details. In Table <ref type="table">1</ref>, we evaluate all methods with datasets and horizon tasks from the Informer benchmark <ref type="bibr" target="#b79">[80]</ref>. We use the datasets and horizons evaluated on in recent works <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b81">82]</ref>, which evaluate on electricity transformer temperature time series (ETTh1, ETTh2, ETTm1, ETTm2) with forecasting horizons {96, 192, 336, 720}. We extend this comparison in Appendix D.2 to all datasets and forecasting horizons in <ref type="bibr" target="#b79">[80]</ref>, which also consider weather and electricity (ECL) time series data.</p><p>Training details. We train SpaceTime on all datasets for 50 epochs using AdamW optimizer <ref type="bibr" target="#b47">[48]</ref>, cosine scheduling, and early stopping based on best validation standardized MSE. We performed a grid search over number of SSMs {64, 128} and weight decay {0, 0.0001}. Like prior forecasting works, we treat the input lag sequence as a hyperparameter, and train to predict each forecasting horizon with either 336 or 720 time-step-long input sequences for all datasets and horizons. For all datasets, we use a 3-layer SpaceTime network with 128 SSMs per layer. We train with learning rate 0.01, weight decay 0.0001, batch size 32, and dropout 0.25.</p><p>Hardware details. All experiments were run on a single NVIDIA Tesla P100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Monash Forecasting</head><p>The Monash Time Series Forecasting Repository <ref type="bibr" target="#b22">[23]</ref> provides an extensive benchmark suite for time series forecasting models, with over 30 datasets (including various configurations) spanning finance, traffic, weather and medical domains. We compare SpaceTime against 13 baselines provided by the Monash benchmark: simple exponential smoothing (SES) <ref type="bibr" target="#b21">[22]</ref>, Theta <ref type="bibr" target="#b2">[3]</ref>, TBATS <ref type="bibr" target="#b17">[18]</ref>, ETS <ref type="bibr" target="#b74">[75]</ref>, DHR-ARIMA <ref type="bibr" target="#b38">[39]</ref>, Pooled Regression (PR) <ref type="bibr" target="#b68">[69]</ref>, CatBoost <ref type="bibr" target="#b19">[20]</ref>, FFNN, DeepAR <ref type="bibr" target="#b60">[61]</ref>, N-BEATS <ref type="bibr" target="#b53">[54]</ref>, WaveNet <ref type="bibr" target="#b51">[52]</ref>, vanilla Transformer <ref type="bibr" target="#b69">[70]</ref>. A complete list of the datasets considered and baselines, including test results (average RMSE across 3 seeded runs) is available in Table <ref type="table">20</ref>.</p><p>Training details. We optimize SpaceTime on all datasets using Adam optimizer for 40 epochs with a linear learning rate warmup phase of 20 epochs and cosine decay. We initialize learning rate at 0.001, reach 0.004 after warmup, and decay to 0.0001. We do not use weight decay or dropout.</p><p>We perform a grid search over number of layers {3, 4, 5, 6}, number of SSMs per layer {8, 16, 32, 64, 128} and number of channels (width of the model) {1, 4, 8, 16}. Hyperparameter tuning is performed for each dataset. We pick the model based on best validation RMSE performance.</p><p>Hardware details. All experiments were run on a single NVIDIA GeForce RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Time Series Classification</head><p>ECG classification (motivation and dataset description). Electrocardiograms (ECG) are commonly used as one of the first examination tools for assessing and diagnosing cardiovascular diseases, which are a major cause of mortality around the world <ref type="bibr" target="#b1">[2]</ref>. However, ECG interpretation remains a challenging task for cardiologists and general practitioners <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>. Incorrect interpretation of ECG can result in misdiagnosis and delayed treatment, which can be potentially life-threatening in critical situations such as emergency rooms, where an accurate interpretation is needed quickly.</p><p>To mitigate these challenges, deep learning approaches are increasingly being applied to interpret ECGs. These approaches have been used for predicting the ECG rhythm class <ref type="bibr" target="#b32">[33]</ref>, detecting atrial fibrillation <ref type="bibr" target="#b4">[5]</ref>, rare cardiac diseases like cardiac amyloidosis <ref type="bibr" target="#b25">[26]</ref>, and a variety of other abnormalities <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b63">64]</ref>. Deep learning approaches have shown preliminary promise in matching the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Informer Forecasting</head><p>Univariate long horizon forecasts with Informer splits. Beyond the ETT datasets and horizons evaluated on in Table <ref type="table">7</ref>, we also compare SpaceTime to alternative time series methods on the complete datasets and horizons used in the original Informer paper <ref type="bibr" target="#b79">[80]</ref>. We compare against recent architectures which similarly evaluate on these settings, including ETSFormer <ref type="bibr" target="#b75">[76]</ref>, SCINet <ref type="bibr" target="#b45">[46]</ref>, and Yformer <ref type="bibr" target="#b48">[49]</ref>, and other comparison methods found in the Informer paper, such as Reformer <ref type="bibr" target="#b44">[45]</ref> and ARIMA. SpaceTime obtains best results on 20 out of 25 settings, the most of any method. Multivariate signals. We additionally compare the performance of SpaceTime to state-ofthe-art comparison methods on ETT multivariate settings. We focus on horizon length 720, the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Monash Forecasting</head><p>We report the results across all datasets in Table <ref type="table">20</ref>. We also investigate the performance of models by aggregating datasets based on common characteristics. Concretely, we generate sets of tasks 2 based on the following properties:</p><p>• Large dataset: the dataset contains more than 2000 effective training samples.</p><p>• Long context: the models are provided a context of length greater than 20 as input.</p><p>• Long horizon: and the models are asked to forecast longer than 20 steps in the future.</p><p>Figure <ref type="figure">6</ref> shows the average x/13 model ranking in terms of test RMSE across splits. We contextualize SpaceTime results with best classical and deep learning methods (TBATS and DeepAR). SpaceTime relative performance is noticeably higher when context and forecasting horizons are longer, and when a larger number of samples is provided during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 ECG Classification</head><p>In addition to our results table in the main paper, we also provide the mean and standard deviations of the two models we ran in house (SpaceTime and S4) in Table <ref type="table">9</ref>.</p><p>Table <ref type="table">9</ref>: ECG statement classification on PTB-XL (100 Hz version). We report the mean and standard deviation over three random seeds for the three methods we ran in house. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Efficiency Results</head><p>We additionally empirically validate that SpaceTime trains in near-linear time with horizon sequence length. We also use synthetic data, scaling horizon from 1 -1000.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Diffusion-based time series imputation and forecasting with structured state space models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M L</forename><surname>Alcaraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.09399</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trend analysis of cardiovascular disease mortality, incidence, and mortality-to-incidence ratio: results from global burden of disease study 2017</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zayeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Public Health</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The theta model: a decomposition approach to forecasting</title>
		<author>
			<persName><forename type="first">V</forename><surname>Assimakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Screening for cardiac contractile dysfunction using an artificial intelligence-enabled electrocardiogram</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">I</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lopez-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Mckie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Ladewig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Satam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Pellikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enriquez-Sarano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Munger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="74" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An artificial intelligence-enabled ecg algorithm for the identification of patients with atrial fibrillation during sinus rhythm: a retrospective analysis of outcome prediction</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">I</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lopez-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Asirvatham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Gersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rabinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="2019">10201. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Some recent advances in forecasting and control</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time series analysis: forecasting and control</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Brown</surname></persName>
		</author>
		<title level="m">Statistical forecasting for inventory control</title>
		<imprint>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Time-series forecasting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chatfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Linear system theory and design</title>
		<imprint>
			<publisher>Saunders college publishing</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Direct multi-step estimation and forecasting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chevillon</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-6419.2007.00518.x</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-6419.2007.00518.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Surveys</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="785" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accuracy of physicians&apos; electrocardiogram interpretations: a systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Pusic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA internal medicine</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1461" to="1471" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the existence and uniqueness of the real logarithm of a matrix</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Culver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Forecasting time series with complex seasonal patterns using exponential smoothing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>De Livera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">496</biblScope>
			<biblScope unit="page" from="1513" to="1527" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Puckette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04208</idno>
		<title level="m">Adversarial audio synthesis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dorogush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ershov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11363</idno>
		<title level="m">Catboost: gradient boosting with categorical features support</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Funahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exponential smoothing: The state of the art</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Gardner</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Godahewa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Montero-Manso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06643</idno>
		<title level="m">Monash time series forecasting archive</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">It&apos;s raw! audio generation with state-space models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09729</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A N</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.1161/01.CIR.101.23.e215</idno>
	</analytic>
	<monogr>
		<title level="j">PhysioBank, PhysioToolkit, and PhysioNet. Circulation</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="215" to="e220" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Artificial intelligence-enabled fully automated detection of cardiac amyloidosis using electrocardiograms and echocardiograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beussink-Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ikura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Katsumata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Gaggin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Itabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Macrae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00396</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="572" to="585" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11893</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Diagonal state spaces are as effective as structured state spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14343</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">State-space models. Handbook of econometrics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3039" to="3080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haghpanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Tison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bourn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Turakhia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="69" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07765</idno>
		<title level="m">General-purpose, long-context autoregressive modeling with perceiver ar</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bounding the roots of polynomials</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Macey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The College Mathematics Journal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="292" to="295" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Forecasting seasonals and trends by exponentially weighted moving averages</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Holt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="10" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Forecasting with exponential smoothing: the state space approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Snyder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Forecasting: principles and practice</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Athanasopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OTexts</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Competency in electrocardiogram interpretation among graduating medical students</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Jablonover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stagnaro-Green</surname></persName>
		</author>
		<idno type="DOI">10.1080/10401334.2014.918882</idno>
		<idno type="PMID">25010240</idno>
		<ptr target="https://doi.org/10.1080/10401334.2014.918882" />
	</analytic>
	<monogr>
		<title level="j">Teaching and Learning in Medicine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="284" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02435</idno>
		<title level="m">On neural differential equations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6696" to="6707" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rackauckas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stiff neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">93122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Time series is a special sequence: Forecasting with sample convolution and interaction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09305</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dustdar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=0EXmFzUn5I" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Yformer: U-net inspired transformer architecture for far horizon time series forecasting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Madhusudhanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burchert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duong-Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08255</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Differentiable multiple shooting layers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16532" to="16544" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural rough differential equations for long time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7829" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Discrete-time signal processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">N-beats: Neural basis expansion analysis for interpretable time series forecasting</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10437</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07532</idno>
		<title level="m">Graph neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Erichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02389</idno>
		<title level="m">Continuous-in-depth neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep state space models for time series forecasting</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7796" to="7805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Automatic diagnosis of the 12-lead ecg using a deep neural network</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Paixão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Canazart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Macfarlane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meira</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepar: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Health informatics -standard communication protocol -part 91064: Computerassisted electrocardiography</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Secretary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Time series analysis and its applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Shumway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Stoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Stoffer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Artificial intelligence-enhanced electrocardiography in cardiovascular disease management</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Siontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">I</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Cardiology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="465" to="478" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Simplified state space layers for sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.04933</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Wavelets and filter banks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep learning for ecg analysis: Benchmarks and insights from ptb-xl</title>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaeffter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2020.3022989</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1519" to="1528" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On the identification of sales forecasting models in the presence of promotions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Trapero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kourentzes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the operational Research Society</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="307" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-D</forename><surname>Bousseljot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kreiseler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Lunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaeffter</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-020-0495-6</idno>
		<ptr target="https://doi.org/10.1038/s41597-020-0495-6" />
		<title level="m">PTB-XL, a large publicly available electrocardiography dataset. Scientific Data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">154</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-D</forename><surname>Bousseljot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaeffter</surname></persName>
		</author>
		<title level="m">PTB-XL, a large publicly available electrocardiography dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematics and Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Forecasting sales by exponentially weighted moving averages</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Winters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="324" to="342" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Etsformer: Exponential smoothing transformers for time-series forecasting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01381</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22419" to="22430" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.13504</idno>
		<title level="m">Are transformers effective for time series forecasting? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A comprehensive review of stability analysis of continuous-time recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1229" to="1262" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11106" to="11115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08897</idno>
		<title level="m">Frequency improved legendre memory model for long-term time series forecasting</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12740</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
