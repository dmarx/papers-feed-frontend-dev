<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Structured Representations with Hyperbolic Embeddings</title>
				<funder ref="#_ygq7wcY">
					<orgName type="full">MEXT KAKENHI</orgName>
				</funder>
				<funder ref="#_BMDxH3w">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-02">2 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Sinha</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Now at Netflix Inc. 38th Conference on Neural Information Processing Systems (NeurIPS 2024</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siqi</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
							<email>makoto.yamada@oist.jp</email>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
							<email>hanzhao@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Okinawa Institute of Science and Technology</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Structured Representations with Hyperbolic Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-02">2 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">A4FAF7AA11C202631578E7B5792A52B3</idno>
					<idno type="arXiv">arXiv:2412.01023v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work <ref type="bibr" target="#b103">[104]</ref> proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context <ref type="bibr" target="#b7">[8]</ref>. In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations. HypStructure is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss. It can be combined with any standard task loss to learn hierarchy-informed features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance, especially under low-dimensional scenarios. For a better understanding of structured representation, we perform an eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is available at <ref type="url" target="https://github.com/uiuctml/HypStructure">https://github.com/uiuctml/HypStructure</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Real-world datasets, such as ImageNet <ref type="bibr" target="#b71">[72]</ref> and CIFAR <ref type="bibr" target="#b44">[45]</ref>, often exhibit a natural hierarchy or an inherent label structure that describes a structured relationship between different classes in the data. In the absence of an existing hierarchy, it is often possible to cheaply construct or infer this hierarchy from the label space directly <ref type="bibr" target="#b63">[64]</ref>. However, the majority of existing representation learning methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> treat the labels as permutation invariant, ignoring this semantically-rich hierarchical label information. Recently, Zeng et al. <ref type="bibr" target="#b103">[104]</ref> offer a promising approach to embed the tree-hierarchy explicitly in representation learning using a tree-metric-based regularizer, leading to improvements in generalization performance. The approach uses a computation of shortest paths between two classes in the tree hierarchy to enforce the same structure in the feature space, by means of a Cophenetic Correlation Coefficient (CPCC) <ref type="bibr" target="#b78">[79]</ref> based regularizer. However, their approach uses the ℓ 2 distance in the Euclidean space, distorting the parent-child representations in the hierarchy <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b48">49]</ref> owing to the bounded dimensionality of the Euclidean space <ref type="bibr" target="#b7">[8]</ref>.</p><p>Hyperbolic geometry has recently gained growing interest in the field of representation learning <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>. Hyperbolic spaces can be viewed as the continuous analog of a tree, allowing for embedding tree-like data in finite dimensions with minimal distortion <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b23">24]</ref>. Unlike Euclidean spaces with zero curvature and spherical spaces with positive curvature, the hyperbolic spaces have negative curvature enabling the length to grow exponentially with its radius. Owing to these advantages, hyperbolic geometry has been used for various applications such as natural language processing <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b15">16]</ref>, image classification <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b17">18]</ref>, object detection <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b20">21]</ref>, action retrieval <ref type="bibr" target="#b54">[55]</ref>, and hierarchical clustering <ref type="bibr" target="#b99">[100]</ref>. However, the aim of using hyperbolic geometry in these approaches is often to implicitly leverage the hierarchical nature of the data.</p><p>In this work, given a label hierarchy, we argue that accurately and explicitly embedding the hierarchical information into the representation space has several benefits, and for this purpose, we propose HypStructure, a hyperbolic label-structure based regularization approach that extends the proposed methodology in Zeng et al. <ref type="bibr" target="#b103">[104]</ref> for semantically structured learning in the hyperbolic space. HypStructure can be easily combined with any standard task loss for optimization, and enables the learning of discriminative and hierarchy-informed features. In summary, our contributions are as follows:</p><p>• We propose HypStructure and demonstrate its effectiveness in the supervised hierarchical classification tasks on three real-world vision benchmark datasets, and show that our proposed approach is effective in both training from scratch, or fine-tuning if there are resource constraints.</p><p>• We qualitatively and quantitatively assess the nature of the learned representations and demonstrate that along with the performance gains, using HypStructure as a regularizer leads to more interpretable as well as tree-like representations as a side benefit. The low-dimensional representative capacity of hyperbolic geometry is well-known <ref type="bibr" target="#b5">[6]</ref>, and interestingly, we observe that training with HypStructure allows for learning extremely low-dimensional representations with distortion values lower than even their corresponding high-dimensional Euclidean counterparts.</p><p>• We argue that representations learned with an underlying hierarchical structure are beneficial not only for the in-distribution (ID) classification tasks but also for Out-of-distribution (OOD) detection tasks. We empirically demonstrate that learning ID representations with HypStructure leads to improved OOD detection on 9 real-world OOD datasets without sacrificing ID accuracy <ref type="bibr" target="#b105">[106]</ref>.</p><p>• Inspired by the improvements in OOD detection, we provide a formal analysis of the eigenspectrum of the in-distribution hierarchy-informed features learned with CPCC-style structured regularization methods, thus leading to a better understanding of the behavior of structured representations in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we first provide a background of structured representation learning and then discuss the limited representation capacity of the Euclidean space for hierarchical information, which serves as the primary motivation for our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Structured representation learning <ref type="bibr" target="#b103">[104]</ref> breaks the permutation invariance of flat representation learning by incorporating a hierarchical regularization term with a standard classification loss. The regularization term is specifically designed to enforce class-conditioned grouping or partitioning in the feature space, based on a given hierarchy.</p><p>More specifically, given a weighted tree T = (V, E, e) with vertices V , edges E and edge weights e, let us compute a tree metric d T for any pair of nodes v, v ′ ∈ V , as the weighted length of the shortest path in T between v and v ′ . For a real world dataset D = {(x i , y i )} N i=1 , we can specify a label tree T where a node v i ∈ V , v i corresponds to a subset of classes, and D i ⊆ D denote the subset of data points with class label v i . We denote dataset distance between D i and D j as ρ(v i , v j ) = d (D i , D j ), where d(•, •) is any distance metric in the feature space, varied by design.</p><p>With a collection of tree metric d T and dataset distances ρ, we can use the Cophenetic Correlation Coefficient (CPCC) <ref type="bibr" target="#b78">[79]</ref>, inherently a Pearson's correlation coefficient, to evaluate the correspondence between the nodes of the tree, and the features in the representation space. Let d T , ρ denote the mean of the collection of distances, then CPCC is defined as</p><formula xml:id="formula_0">CPCC(d T , ρ) := i&lt;j (d T (v i , v j ) -d T )(ρ(v i , v j ) -ρ) ( i&lt;j (d T (v i , v j ) -d T ) 2 ) 1/2 ( i&lt;j (ρ(v i , v j ) -ρ) 2 ) 1/2 . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>For the supervised classification task, we consider the training set D in tr = {(x i , y i )} N i=1 and we aim to learn the network parameter θ for a feature encoder f θ : X → Z, where Z ⊆ R d denotes the representation/feature space. For structured representation learning, the feature encoder is usually followed by a classifier g w , and the parameters θ, w are learnt by minimizing L along with a standard flat (non-hierarchical) classification loss, for instance, Cross-Entropy (CE) or Supervised Contrastive (SupCon) <ref type="bibr" target="#b38">[39]</ref> loss, with the structured regularization term as:</p><formula xml:id="formula_2">L(D) = (x,y)∈D ℓ Flat (x, y, θ, w) -α • CPCC(d T , ρ).</formula><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>Using a composite objective as defined in Equation ( <ref type="formula" target="#formula_3">2</ref>), we can enforce the distance relationship between a pair of representations in the feature space, to behave similarly to the tree metric between the same vertices. For instance, consider a simple label tree with a root node, a coarse level, and a fine level, where subsets of fine classes share the same coarse parent. For this hierarchy, we would expect the fine classes of the same parents (e.g., apple and banana are fruits) to have closer representations in the feature space, whereas fine classes with different coarse parents (e.g., an apple is a fruit and a tulip is a flower) should be further apart. The learned structure-informed representations reflect these hierarchical relationships and lead to interpretable features with better generalization <ref type="bibr" target="#b103">[104]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ℓ 2 -CPCC</head><p>Equation (1) offers the flexibility of designing a metric to measure the similarity between two data subsets, and Zeng et al. <ref type="bibr" target="#b103">[104]</ref> define the Euclidean dataset distance as</p><formula xml:id="formula_4">ρ ℓ2 (D i , D j ) := ∥ 1 |Di| x∈Di f (x) -1 |Dj | x ′ ∈Dj f (x ′ )∥ 2 .</formula><p>The distance between datasets is thus the ℓ 2 distance between two Euclidean centroids of their class-conditioned representations, which is unsuitable for modeling tree-like data <ref type="bibr" target="#b7">[8]</ref>. Additionally, this regularization approach in Zeng et al. <ref type="bibr" target="#b103">[104]</ref> is applied only to the leaf nodes of T for efficiency.</p><p>However, this leaf-only formulation of the CPCC offers an approximation of the structured information, since the distance between non-leaf nodes is not restricted explicitly by the regularization. This approximation, therefore, leads to a loss of information contained in the original hierarchy T .</p><p>Actually, it is impossible to embed d T into ℓ 2 exactly. Or more formally, there exists no bijection φ such that d T (φ(z i ), φ(z j )) = ∥z i -z j ∥ 2 irrespective of how large the feature dimension d is. We provide two such examples for a toy label tree in Figure <ref type="figure" target="#fig_0">1</ref>, below.   Since we cannot embed an arbitrary tree T into ℓ 2 without distortion, it would also affect the optimization of the ℓ 2 -CPCC in a classification problem, where the tree weights encode knowledge of class similarity. To verify our claims, we consider the optimization of 512-dimensional ℓ 2 -CPCC structured representations for CIFAR10 <ref type="bibr" target="#b44">[45]</ref>. The CIFAR10 dataset consists of a small label hierarchy as shown in Figure <ref type="figure" target="#fig_2">2</ref> (left).</p><p>The optimal CPCC is achieved when each tree metric value corresponds to a single ρ ℓ2 . However, in Figure <ref type="figure" target="#fig_2">2</ref> (right), even with an optimization of the ℓ 2 -CPCC loss for the entire tree, we observe a sub-optimal train CPCC less than 1, where the distance between two coarse nodes, transportation and animal, is far away from the desired solution. Furthermore, optimization of the CPCC loss for only the leaf nodes, leads to an even larger distortion of the tree metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Hyperbolic spaces are more suitable for embedding hierarchical relationships with low distortion <ref type="bibr" target="#b74">[75]</ref> and low dimensions. Hence, motivated by the aforementioned challenges, we propose a Hyperbolic Structured regularizer for hierarchy-informed representation learning. We begin with the basics of hyperbolic geometry, followed by the detailed methodology of our proposed approach. Hyperbolic spaces are non-Euclidean spaces with negative curvature where given a fixed point and a line, there exist infinitely many parallel lines that can pass through this point. There are several commonly used isometric hyperbolic models <ref type="bibr" target="#b3">[4]</ref>. For this work, we mainly use the Poincaré Ball model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hyperbolic Geometry</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperboloid</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.1 (Manifold)</head><p>. A manifold M is a set of points z that are locally Euclidean. Every point z of the manifold M is attached to a tangent space T z M, which is a vector space over the reals of the same dimensionality as M that contain all the possible directions that can tangentially pass through z. </p><formula xml:id="formula_5">d Bc (z 1 , z 2 ) = 2 √ c tanh -1 √ c -(1 + 2c ⟨-z 1 , z 2 ⟩ + c∥z 2 ∥ 2 )z 1 + (1 -c∥z 1 ∥ 2 )z 2 1 -2c ⟨z 1 , z 2 ⟩ + c 2 ∥z 1 ∥ 2 ∥z 2 ∥ 2 .<label>(3)</label></formula><p>For c → 0, we can recover the properties of the Euclidean geometry since</p><formula xml:id="formula_6">lim c→0 d Bc (z 1 , z 2 ) = 2∥z 1 -z 2 ∥. Since T z B d c is isomorphic to R d ,</formula><p>we can connect vectors in Euclidean space and hyperbolic space with the bijection between T z B d c and B d c <ref type="bibr" target="#b87">[88]</ref>. For z = 0, the exponential map exp c 0 :</p><formula xml:id="formula_7">T z B d c → B d c and logarithm map log c 0 : B d c → T z B d c have the closed form of exp c 0 (v) = tanh √ c∥v∥ v √ c∥v∥ , log c 0 (u) = 1 √ c tanh -1 √ c∥u∥ u ∥u∥ .<label>(4)</label></formula><p>Alternatively, to guarantee the correctness of Poincaré distance computation, we can also process any Euclidean vector with a clipping module <ref type="bibr" target="#b65">[66]</ref> clip</p><formula xml:id="formula_8">c (v) = v, if ∥v∥ &lt; 1/ √ c 1 √ c -ϵ v ∥v∥ , otherwise ,<label>(5)</label></formula><p>so the clipped vector is within the Poincare disk. We set ϵ as a small positive number in practice.</p><formula xml:id="formula_9">Definition 3.3 (Klein Model). Klein model (K d c , g K ) consists of an 1/ √ c-radius open ball K d c = {z ∈ R d : c∥z∥ 2 &lt;</formula><p>1} and a metric tensor g K different from g B . Similar to the mean computation in Euclidean space, let γ i = 1/ 1 -c∥z i ∥ 2 , the Einstein midpoint of a group of Klein vectors z 1 , . . . z n ∈ K d c has a simple expression of a weighted average</p><formula xml:id="formula_10">HypAve K (z 1 , . . . z n ) = n i=1 γ i z i n i=1 γ i .<label>(6)</label></formula><p>We illustrate the relationship between the different hyperbolic models in Figure <ref type="figure" target="#fig_3">3</ref>. The hyperboloid space models d-dimensional hyperbolic geometry on a d + 1-dimensional space. When d = 2, the Klein model is the tangent plane of the hyperboloid model at (0, 0, 1), and the Poincaré disk shares the same support as the Klein disk, although shifted downwards and centered at the origin. Given a triangle on the hyperboloid model, its projection on the Klein model preserves the straight sides, but the projection of a line on the Poincaré model is a part of a circular arc or the diameter of the disk. Let z B , z K be coordinates of z under Poincaré and Klein model respectively, the prototype operations on B d c require transformations between B d c and K d c as</p><formula xml:id="formula_11">z B = z K 1 + 1 -c∥z K ∥ 2 , z K = 2z B 1 + c∥z B ∥ 2 .<label>(7)</label></formula><p>For example, in Figure <ref type="figure" target="#fig_3">3</ref>, O ′ is the HypAve K of A ′ , B ′ , C ′ , and can be mapped back to the Poincaré disk to get the Poincaré prototype (HypAve B ) O of points A, B, C by a change of coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HypStructure: Hyperbolic Structured Regularization</head><p>At a high level, HypStructure uses a combination of two losses: a Hyperbolic Cophenetic Correlation Coefficient Loss (HypCPCC)), and a Hyperbolic centering loss (HypCenter) for embedding the hierarchy in the representation space. Below we describe the two components of HypStructure.</p><p>The pseudocode of HypStructure is shown in Algorithm 1 in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HypCPCC (Hyperbolic Cophenetic Correlation Coefficient):</head><p>We extend the ℓ 2 -CPCC methodology in Zeng et al. <ref type="bibr" target="#b103">[104]</ref> to the hyperbolic space in HypCPCC. Three major steps of HypCPCC are (i) map Euclidean vectors to Poincaré space (ii) compute class prototypes (iii) use Poincaré distance for CPCC. Specifically, we first project each z i ∈ R d to B d c , and compute the Poincaré centroid for each vertex of T using hyperbolic averaging as shown in Equation <ref type="bibr" target="#b5">(6)</ref> and Equation <ref type="bibr" target="#b6">(7)</ref>. Alternatively, we can also compute Euclidean centroids z = 1 |Di| z∈Di z for each vertex, and project each z ∈ R d to B d c either by exp c 0 or clip c . After the computation of hyperbolic centroids, we use the pairwise distances between all vertex pairs in T in the Poincaré ball, to compute the HypCPCC loss using Equation (1) by setting ρ = d Bc .</p><p>HypCenter (Hyperbolic Centering): Inspired by Sarkar's low-distortion construction <ref type="bibr" target="#b74">[75]</ref> that places the root node of a tree at the origin, we propose a centering loss for this positioning, that enforces the representation of the root node to be close to the center of the Poincaré disk, and the representations of its children to be closer to the border of Poincaré disk. We enforce this constraint by minimizing the norm of the hyperbolic representation of the root node as ℓ center = ∥HypAve B (exp c 0 (z 1 ), . . . , exp c 0 (z N ))∥. Alternatively, for centroids computed in the Euclidean space and mapped to the Poincaré disk, we minimize ℓ center = 1/N N i=1 f θ (x i ) directly due to the monotonicity of tanh(•) in the exponential map.</p><p>Finally, for α, β &gt; 0, we can learn the hierarchy-informed representations by minimizing</p><formula xml:id="formula_12">L(D) = (x,y)∈D ℓ Flat (x, y, θ) -α • HypCPCC(d T , d Bc ) + β • ℓ center (x, θ),<label>(8)</label></formula><p>where ℓ Flat is a standard classification loss, such as the CE loss or the SupCon loss.</p><p>Time Complexity: In a batch computation setting with a batch size b and the number of classes (leaf nodes) as k, the computational complexity for a HypStructure computation to embed the full tree will still be O(d • min{b 2 , k 2 }), which is the same as the complexity of a Euclidean leaf-only CPCC.</p><p>The additional knowledge gained from internal nodes allows us to reason about the relationship between higher-level concepts, and the hyperbolic representations help in achieving a low distortion of hierarchical information for better performance in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments on several large-scale image benchmark datasets to evaluate the performance of HypStructure as compared to the Flat and ℓ 2 -CPCC baselines for hierarchy embedding, classification, and OOD detection tasks.</p><p>Datasets and Setup Following the common benchmarks in the literature, we consider three realworld vision datasets, namely CIFAR10, CIFAR100 <ref type="bibr" target="#b44">[45]</ref> and ImageNet100 <ref type="bibr" target="#b58">[59]</ref> for training, which vary in scale, number of classes, and number of images per class. We construct the ImageNet100 dataset by sampling 100 classes from the ImageNet-1k <ref type="bibr" target="#b71">[72]</ref> dataset following <ref type="bibr" target="#b58">[59]</ref>. For CIFAR100, a three-level hierarchy is available with the dataset release <ref type="bibr" target="#b44">[45]</ref>. Since no hierarchy is available for the CIFAR10 and ImageNet100 datasets, we construct a hierarchy for CIFAR10 manually in Figure <ref type="figure" target="#fig_2">2</ref>. For ImageNet100, we create a subtree from the WordNet <ref type="bibr" target="#b18">[19]</ref> given 100 classes as leaves. More details regarding the datasets, network, training and setup are provided in the Appendix B.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quality of Hierarchical Information</head><p>Table 1: Evaluation of hierarchical information distortion and classification accuracy using SupCon [39] as ℓ Flat . All metrics are reported as mean (standard deviation) over 3 seeds. Dataset (Backbone) Method Distortion of Hierarchy Classification Accuracy δrel (↓) CPCC (↑) Fine (↑) Coarse (↑) CIFAR10 (ResNet-18) Flat 0.232 (0.001) 0.573 (0.002) 94.64 (0.12) 99.16 (0.04) ℓ2-CPCC 0.174 (0.002) 0.966 (0.001) 94.47 (0.13) 98.91 (0.02) HypStructure 0.094 (0.003) 0.992 (0.001) 94.79 (0.14) 99.18 (0.04) CIFAR100 (ResNet-34) Flat 0.209 (0.002) 0.534 (0.119) 74.96 (0.14) 84.15 (0.19) ℓ2-CPCC 0.213 (0.006) 0.779 (0.002) 76.07 (0.19) 85.28 (0.32) HypStructure 0.127 (0.016) 0.766 (0.007) 76.68 (0.22) 86.01 (0.13) ImageNet100 (ResNet-34) Flat 0.168 (0.003) 0.429 (0.002) 90.01 (0.07) 90.77 (0.11) ℓ2-CPCC 0.213 (0.009) 0.834 (0.002) 89.57 (0.38) 90.34 (0.28) HypStructure 0.134 (0.001) 0.841 (0.001) 90.12 (0.01) 90.84 (0.02) 16 32 64 12 8 25 6 51 2 Embedding Dimension 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 Gromov's rel Flat (512) 2-CPCC (512) HypStructure (Ours) First, to assess the tree-likeness of the learnt representations, we measure the Gromov's hyperbolicity δ rel <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> of the features in Table <ref type="table">1</ref>. Lower δ rel indicates higher tree-likeness and a perfect tree metric space has δ rel = 0 (more details in Appendix B.5). To also evaluate the correspondence of the feature distances with ground truth tree metrics, we compute CPCC on test sets. We observe that HypStructure reduces distortion of hierarchical information over Flat by upto 59.4% and over ℓ 2 -CPCC by upto 45.4%, while also consistently improving the test CPCC for most datasets.</p><p>We also perform a qualitative analysis of the learnt representations from HypStructure on the CIFAR10 dataset, and visualize them in a Poincaré disk using UMAP <ref type="bibr" target="#b56">[57]</ref> in Figure <ref type="figure">5a</ref>. We can observe clearly that the samples for fine classes arrange themselves in the Poincaré disk based on the hierarchy tree as seen in Figure <ref type="figure" target="#fig_2">2</ref>, being closer to the classes which share a coarse class parent.</p><p>To examine the impact of feature dimension on the representative capacity of the hyperbolic space, we vary the feature dimension for HypStructure and compute the δ rel for each learnt feature.</p><p>Comparing the distortion of features with the Flat and ℓ 2 -CPCC settings in Figure <ref type="figure" target="#fig_5">4</ref>, we observe that δ rel decreases consistently with increasing dimensions, implying that high dimension features using HypStructure are more tree-like, and better than Flat and ℓ 2 -CPCCs' 512-dimension baselines.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification</head><p>Following Zeng et al. <ref type="bibr" target="#b103">[104]</ref>, we treat leaf nodes in the hierarchy as fine classes and their parent nodes as coarse classes. To evaluate the quality of the learnt representations, we perform a classification task on the fine and coarse classes using a kNN-classifier following <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b109">110]</ref> and report the performance on the three datasets in Table <ref type="table">1</ref>. We observe that HypStructure leads to upto 2.2% improvements over Flat and upto 0.8% improvements over ℓ 2 -CPCC on both fine and coarse accuracy.</p><p>We also visualize the learnt test features from Flat vs HypStructure on the CIFAR100 dataset using Euclidean t-SNE <ref type="bibr" target="#b88">[89]</ref> and show the visualizations in Figure <ref type="figure">5b</ref> and Figure <ref type="figure">5c</ref> respectively. We observe that HypStructure leads to sharper and more discriminative representations in Euclidean space. Additionally, we see that the fine classes belonging to a coarse class (the same shades of colors) which are semantically closer in the label hierarchy, are grouped closer and more compactly in the feature space as well, as compared to Flat. We also perform evaluations using the linear evaluation protocol <ref type="bibr" target="#b38">[39]</ref> and observe an identical trend in the accuracy, we report these results in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OOD Detection</head><p>In addition to leveraging the hierarchy explicitly for the purpose of learning tree-like ID representations, we argue that a structured separation of features in the hyperbolic space as enforced by HypStructure is helpful for the OOD detection task as well. To verify our claim, we perform an exhaustive evaluation on 9 real-world OOD datasets and demonstrate that HypStructure leads to improvements in the OOD detection AUROC. We share more details below.</p><p>Method OOD Dataset AUROC (↑) Overall AUROC SVHN Textures Places365 LSUN iSUN Avg.(↑) B.C.(↑) ProxyAnchor [41] 82.43 84.99 79.84 91.68 84.96 84.78 51.42 CE + SimCLR [94] 94.45 82.01 71.48 89.00 83.82 84.15 31.42 CSI [85] 92.65 86.47 76.27 83.78 84.98 84.83 40.00 CIDER [61] 95.16 90.42 73.43 96.33 82.98 87.67 60.00 SSD+ (SupCon) [76] 94.19 86.18 79.90 85.18 84.08 85.90 54.28 KNN+ (SupCon) [83] 92.78 88.35 77.58 89.30 82.69 86.14 40.00 ℓ 2 -CPCC [104] 93.08 90.45 77.21 82.77 82.79 85.26 40.00 HypStructure 95.97 88.43 78.12 97.01 84.51 88.81 82.85  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Problem Setting</head><p>Out-of-distribution (OOD) data refers to samples that do not belong to the in-distribution (ID) and whose label set is disjoint from Y in and therefore should not be predicted by the model. Therefore , Textures <ref type="bibr" target="#b8">[9]</ref> and iNaturalist <ref type="bibr" target="#b89">[90]</ref> when ImageNet100 is used as the ID dataset. This subset of datasets is prepared by <ref type="bibr" target="#b58">[59]</ref> and is created with overlapping classes from ImageNet-1k removed from these datasets to ensure there is no overlap in the distributions.</p><p>OOD detection scores While several scores have been proposed for the task of OOD detection, we evaluate our proposed method using the Mahalanobis score <ref type="bibr" target="#b75">[76]</ref>, computed by estimating the mean and covariance of the in-distribution training features. The Mahalanobis score is defined as</p><formula xml:id="formula_13">s(x) = (f (x) -µ) ⊤ Σ -1 (f (x) -µ),<label>(9)</label></formula><p>where µ, Σ are the mean and covariance of in-distribution training features. Sehwag et al. <ref type="bibr" target="#b75">[76]</ref> present the Mahalanobis score (eq. ( <ref type="formula" target="#formula_13">9</ref>)) in a generalized version for multiple feature clusters. However, since they empirically observe that the single-cluster version achieves the highest performance <ref type="bibr" target="#b75">[76]</ref>, we will focus on this version.</p><p>After computing the OOD detection scores, we measure the area under the receiver operating characteristic curve (AUROC) as the primary evaluation metric following Lee et al. <ref type="bibr" target="#b46">[47]</ref>, Sehwag et al. <ref type="bibr" target="#b75">[76]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Main Results and Discussion</head><p>We report the AUROC averaged over all the OOD datasets (5 datasets for CIFAR10 and CIFAR100, 4 datasets for ImageNet100) in Figure <ref type="figure" target="#fig_9">7a</ref> and Table <ref type="table" target="#tab_2">2</ref> In addition to the Flat (SupCon) and ℓ 2 -CPCC baselines, we also compare our method with other state-of-the-art methods (see Appendix C.3.1 for more details about existing OOD detection methods). We observe that HypStructure leads to a consistent improvement in the OOD detection score, with upto 2% in average AUROC. We also report the dataset-wise OOD detection results for the CIFAR100 ID dataset in Table <ref type="table" target="#tab_10">7a</ref> along with Average AUROC. To remove the bias in the Average AUROC metric towards any single dataset, we also evaluate the Borda Count (B.C.) <ref type="bibr" target="#b57">[58]</ref> and report the same, along with a detailed comparison with more OOD detection methods in Table <ref type="table" target="#tab_10">7a</ref>, and Tables <ref type="table" target="#tab_9">6</ref> and <ref type="table" target="#tab_10">7</ref> in the Appendix C.3.</p><p>We observe that HypStructure ranks in the highest performing methods consistently, thereby demonstrating a higher Borda Count as well. We additionally visualize the CIFAR100 (ID) vs SVHN (OOD) features learnt from HypStructure, using a hyperbolic UMAP visualization in Figure <ref type="figure" target="#fig_9">7b</ref>. We observe that training with HypStructure leads to an improvement in the separation of ID vs OOD features in the Poincaré disk.</p><p>Additional Experiments, Ablations and Visualizations: More experiments using hyperbolic contrastive losses and hyperbolic networks, ablation studies on each component of HypStructure and additional visualizations can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Eigenspectrum Analysis of Structured Representations</head><p>As seen in Figure <ref type="figure" target="#fig_9">7a</ref>, we observe a significant improvement in the OOD detection performance using HypStructure with the Mahalanobis score eq. ( <ref type="formula" target="#formula_13">9</ref>). After training a composite loss with CPCC till convergence, let us denote the matrix of the normalized in-distribution trained feature as Z ∈ R n×d . Naturally, we inspect the eigenvalue properties of Σ (i.e, Z), and observe that K = ZZ ⊤ ∈ R n×n exhibits a hierarchical block structure (Figure <ref type="figure" target="#fig_10">8a</ref>) where the diagonal blocks have a significantly higher correlation than other off-diagonal values, leading us to the following theorem. Theorem 5.1 (Eigenspectrum of Structured Representation with Balanced Label Tree). Let T be a balanced tree with height H, such that each level has C h nodes, h ∈ [0, H]. Let us denote each entry of K as r h where h is the height of the lowest common ancestor of the row and the column sample. If r h ≥ 0, ∀h, then: (i) For h = 0, we have</p><formula xml:id="formula_14">C 0 -C 1 eigenvalues λ 0 = 1 -r 1 . (ii) For 0 &lt; h ≤ H -1, we have C h -C h+1 eigenvalues λ h = λ h-1 + (r h -r h+1 ) C0 C h . (iii) The last eigenvalue is λ H = λ H-1 + C 0 r H .</formula><p>We defer the eigenspectrum analysis for an arbitrary label tree to Appendix A. Theorem 5.1 implies a phase transition pattern in the eigenspectrum. There always exists a significant gap in the eigenvalues representing each level of nodes in the hierarchy, and the eigenvalues corresponding to the coarsest level are the highest in magnitude. CIFAR100 has a balanced three-level label hierarchy where each coarse label has five fine labels as its children. In Figure <ref type="figure" target="#fig_10">8b</ref>, we visualize the eigenspectrum of CIFAR100 for HypStructure, ℓ 2 -CPCC and the Flat objective. We observe a significant drop in the eigenvalues for features learnt from two hierarchical regularization approaches, ℓ 2 -CPCC and HypStructure, at approximately the 20 th largest eigenvector (which corresponds to the number of coarse classes), whereas these phase transitions do not appear for standard flat features. We also observe that the magnitude of coarse eigenvalues are approximately at the same scale. In summary, Theorem 5.1 helps us to formally characterize the difference between flat and structured representations. CPCC style (eq. ( <ref type="formula" target="#formula_0">1</ref>)) regularization methods can also be treated as dimensionality reduction techniques, where the structured features can be explained mostly by the coarser level features. For the OOD detection setting, this property differentiates the ID and OOD samples at the coarse level itself using a lower number of dimensions, and makes the OOD detection task easier. We visualize the OOD detection AUROC on SVHN (OOD) corresponding to the CIFAR100 (ID) features with the top-k principal component for different methods, in Figure <ref type="figure" target="#fig_10">8c</ref>. We observe that for features learnt using HypStructure, accurately embedding the hierarchical information leads to the top 20 eigenvectors (corresponding to the coarse classes) being the most informative for OOD detection. Recall that CIDER <ref type="bibr" target="#b60">[61]</ref> is a state-of-the-art method proposed specifically for improving OOD detection by increasing inter-class dispersion and intra-class compactness. We note that CPCC style (eq. ( <ref type="formula" target="#formula_0">1</ref>)) methods can be seen as a generalization of CIDER on higher-level concepts, where the same rules are applied for coarse labels as well, along with the fine classes. When the ID and OOD distributions are far enough, using coarse level feature might be sufficient for OOD detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Learning with Label Hierarchy. Several recent works have explored how to leverage hierarchical information between classes for various purposes such as relational consistency <ref type="bibr" target="#b13">[14]</ref>, designing specific hierarchical classification architectures <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b67">68]</ref>, hierarchical conditioning of the logits <ref type="bibr" target="#b12">[13]</ref>, learning order preserving embeddings <ref type="bibr" target="#b14">[15]</ref>, and improving classification accuracy <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b33">34]</ref>. The proposed structural regularization framework in <ref type="bibr" target="#b103">[104]</ref> offers an interesting approach to embed a tree metric to learn structured representations through an explicit objective term, although they rely on the ℓ 2 distance, which is less than ideal for learning hierarchies. Hyperbolic Geometry. <ref type="bibr" target="#b65">[66]</ref> first proposed using the hyperbolic space to learn hierarchical representations of symbolic data such as text and graphs by embedding them into a Poincaré ball. Since then, the use of hyperbolic geometry has been explored in several different applications. Khrulkov et al. <ref type="bibr" target="#b39">[40]</ref> proposed a hyperbolic image embedding for few-shot learning and person re-identification. <ref type="bibr" target="#b19">[20]</ref> proposed hyperbolic neural network layers, enabling the development of hybrid architectures such as hyperbolic convolutional neural networks <ref type="bibr" target="#b77">[78]</ref>, graph convolutional networks <ref type="bibr" target="#b11">[12]</ref>, hyperbolic variational autoencoders <ref type="bibr" target="#b55">[56]</ref> and hyperbolic attention networks <ref type="bibr" target="#b23">[24]</ref>. Additionally, these hybrid architectures have also been explored for different tasks such as deep metric learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b99">100]</ref>, object detection <ref type="bibr" target="#b45">[46]</ref> and natural language processing <ref type="bibr" target="#b15">[16]</ref>. There have also been several investigations into the properties of hyperbolic spaces and models such as low distortion <ref type="bibr" target="#b74">[75]</ref>, small generalization error <ref type="bibr" target="#b83">[84]</ref> and representation capacity <ref type="bibr" target="#b61">[62]</ref>. However, none of these works have leveraged hyperbolic geometry for explicitly embedding a hierarchy in the representation space via structured regularization, and usually attempt to leverage the underlying hierarchy implicitly using hyperbolic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Future Work</head><p>In this work, we introduce HypStructure, a simple-yet-effective structural regularization framework for incorporating the label hierarchy into the representation space using hyperbolic geometry. In particular, we demonstrate that accurately embedding the hierarchical relationships leads to empirical improvements in both classification as well as the OOD detection tasks, while also learning hierarchyinformed features that are more interpretable and exhibit less distortion with respect to the label hierarchy. We are also the first to formally characterize properties of hierarchy-informed features via an eigenvalue analysis, and also relate it to the OOD detection task, to the best of our knowledge. We acknowledge that our proposed method depends on the availability or construction of an external hierarchy for computing the HypCPCC objective. If the hierarchy is unavailable or contains noise, this could present challenges. Therefore, it is important to evaluate how injecting noisy hierarchies into CPCC-based methods impacts downstream tasks. While the current work uses the Poincaré ball model, exploring the representation trade-offs and empirical performances using other models of hyperbolic geometry in HypStructure, such as the Lorentz model <ref type="bibr" target="#b66">[67]</ref> is an interesting future direction. Further theoretical investigation into establishing the error bounds of CPCC style structured regularization objectives is of interest as well.</p><p>the scalar factor of the tree distance matrix. Since each entry of K is the dot product of two samples, the relative order in K is the opposite. ■ Corollary A.1. If we use the Poincaré distance (eq. ( <ref type="formula" target="#formula_5">3</ref>) in CPCC and let the curvature constant c = 1, the statement of cosine distance in Lemma A.1 still holds.</p><p>Proof. Since the Poincaré distance (eq. ( <ref type="formula" target="#formula_5">3</ref>)) is only defined for vectors with magnitude less than 1, let us consider the case where before the clipping operation, both u and v are outside the unit ball. After applying clip 1 , ∥u∥ = ∥v∥ = 1 -ϵ, where ϵ is a small constant (10 -5 ). Then</p><formula xml:id="formula_15">∥u∥ 2 = (1 -ϵ) 2 = 1 -2ϵ + ϵ 2 . Define 2ϵ -ϵ 2 as ξ, making ∥u∥ 2 := 1 -ξ where ξ is also a small constant such that O(ξ 2 ) is negligible. Poincaré(u, v) = 2 ln ∥u -v∥ + ∥u∥ 2 ∥v∥ 2 -2u • v + 1 (1 -∥u∥ 2 )(1 -∥v∥ 2 ) = 2 ln ∥u -v∥ + 2 -2u • v -2ξ + ξ 2 ξ ≈ 2 ln ∥u -v∥ + √ 2 -2u • v -2ξ ξ = 2 ln ∥u -v∥ + ∥u∥ 2 + ∥v∥ 2 -2u • v ξ = 2 ln 2 ∥u -v∥ ξ</formula><p>We can see that the Poincaré distance monotonically increases with Euclidean distances ∥u -v∥. This property ensures the relative order of any two entries for Euclidean CPCC and Poincare CPCC matrices in K to be the same. Then, we can argue about the structure of K, either Euclidean or Poincare, to have the hierarchical diagonalized structure as in Figure <ref type="figure" target="#fig_10">8a</ref>. So any statement applied for a Poincaré version of CPCC will also hold for the Euclidean CPCC counterpart. ■</p><p>For each level of the tree, due to the optimization of CPCC loss, the corresponding off diagonal entries of K, which represent the intra-level-class similarities, are much smaller than inter-level-class values. We thus have a symmetric similarity matrix that takes on the following structure, where the red regions are greater than orange regions, which are further greater than the blue regions. </p><formula xml:id="formula_16">K =              1 r 1 11</formula><formula xml:id="formula_17">            </formula><p>Each non-diagonal entry is called r h ij where i, j are the index of the diagonal block, or the finest label id of one sample, and h is the height of the lowest common ancestor of the two samples in the row and the column. Since every two leaves sharing the lowest common ancestor of the same height have the same tree distance, each entry of K with the same superscript will be the same so we can drop the i, j subscript in the notation. The size of each block is defined by the number of samples within one label. Then, the shown submatrix of K corresponds to the following tree in Figure <ref type="figure" target="#fig_12">9</ref>. Next, we present several useful lemmas and theorems. Theorem A. <ref type="bibr">1 ([3]</ref>). Let R be a p × p full-rank correlation matrix that has a k-group block structure, with groups of size p i (i = 1 : k, k i=1 p i = p). Let r ii be the correlation for any pair of different variables within group i. Let r ij be the common correlation between any variable in group i and j. Denote the mean of the i-th diagonal block of R by</p><formula xml:id="formula_18">R i = (1/p i )(1 + (p i -1)r ii ). Then: 1. R has p i -1 eigenvalues 1 -r ii (i = 1 : k).</formula><p>2. The rest of the eigenvalues are those from k × k symmetric matrix A whose diagonal elements are a ii = p i R i and whose off-diagonal elements are</p><formula xml:id="formula_19">a ij = √ p i • p j r ij .</formula><p>Lemma A. Proof. Using the definition of eigenvalues, we want to compute the determinant of matrix</p><formula xml:id="formula_20">       <label>1</label></formula><p>-λ p . . . . . . p p . . . . . . . . . 1 -λ . . . . . . . . . p p . . . . . . p 1 -λ</p><formula xml:id="formula_21">       </formula><p>Adding the second till the last row to the first row, we have</p><formula xml:id="formula_22">        1 -λ + (d -1)p 1 -λ + (d -1)p . . . . . . 1 -λ + (d<label>-</label></formula><p>1)p p . . . . . . . . . 1 -λ . . . . . . . . . p p . . . . . . p 1 -λ         Dividing the first row by 1 -λ + (d -1)p, we now have         1 1 . . . . . . 1 p . . . . . . . . . 1 -λ . . . . . . . . . p p . . . . . . p 1 -λ</p><formula xml:id="formula_23">       </formula><p>Subtracting the second till the last row by p times the first row results in an upper triangular matrix</p><formula xml:id="formula_24">       1 1 . . . . . . 1 0 1 -λ -p 0 . . . 1 -λ -p . . . . . . . . . 0 0 . . . . . . 0 1 -λ -p        Thus, det(M -λI) = (1 -λ + (d -1)p)(1 -λ -p) d-1 . ■ Notice that</formula><p>Lemma A.2 is a special case of Theorem A.1 where the label tree is a two level basic tree with the root node and d leaves in the second label all being the direct children of the root node. Now we can leverage Theorem A.1 and Lemma A.2 to investigate the eigenspectrum of K by proving the following theorem: Theorem A.2 (Eigenspectrum of CPCC-based Structured Representation). If T is a tree whose root node has height H where each level has C h nodes, h ∈ [0, H]. K = ZZ ⊤ is a block structured correlation matrix as a result of CPCC optimization, where each off-diagonal entry ∈ [-1, 1] can be written as r h and h is the height of the lowest common ancestor of the i-th row and the 00j-th column sample. Let ∆ = r 1 -r h , p i , i ∈ [C h ] be the number of children for nodes at height h, and p max be the maximum. For any h ≥ 1, if r h ≥ M ≥ 0, r h+1 ≤ m, then (i) We have C 0 -C h eigenvalues, that come from the eigenvalues of a C 0 × C 0 matrix sharing the same C h of p i × p i diagonal blocks from K subtracting r h , and off diagonal values are all zero.</p><p>(ii) The rest of C h eigenvalues come from a C h × C h matrix, whose diagonal entries are the mean of each p i × p i diagonal block from K, and the off diagonal entries are √ p i p j r ij where r ij is the correlation between node i and node j of height h. 1) , C 0 -C h eigenvalues are all smaller than C h eigenvalues.</p><formula xml:id="formula_25">(iii) If m ≤ M -2∆(pmax-1) pmax(C h -</formula><p>Proof. Part (i) and (ii) can be extended from the proof of Theorem A.1. Let G be the n × C h matrix where G ij = 1 if the i-th sample is in group j, otherwise G ij = 0. For any n -C h eigenvectors in the orthogonal complement of the column space of G, the eigenvector of K is also the eigenvector of</p><formula xml:id="formula_26">    K 1 0 • • • 0 0 K 2 • • • 0 . . . . . . . . . . . . 0 0 • • • K k    </formula><p>where due to the hierarchical structure of block matrix, K i has the format of</p><formula xml:id="formula_27">            1 -r h r 1 -r h • • • r j -r h 0 • • • 0 r 1 -r h 1 -r h • • • 0 0 • • • 0 . . . . . . . . . . . . . . . . . . r j -r h 0 • • • 1 -r h 0 • • • 0 0 0 • • • 0 1 -r h • • • 0 . . . . . . . . . . . . . . . . . . 0 0 • • • 0 0 • • • 1 -r h            </formula><p>The rest of C h eigenvectors come from the symmetric</p><formula xml:id="formula_28">C h × C h matrix A = (G ⊤ G) -1/2 (G ⊤ KG)(G ⊤ G) -1/2</formula><p>, by some basica algebra, we know</p><formula xml:id="formula_29">a ij = 1 √ pi • (sum of all r ij entries in p i × p j block) • 1 √ pj .</formula><p>For more details, we refer the reader to Theorem 3.1 in Cadima et al. <ref type="bibr" target="#b2">[3]</ref> where C 1 = k using their notation.</p><p>Since the largest absolute value of K's eigenvalues, is bounded above by the largest row sum of the absolute values of K <ref type="bibr" target="#b34">[35]</ref>, first n -C h eigenvectors are bounded above by</p><formula xml:id="formula_30">U = max i (1 -r h ) + (p i -1)(r 1 -r h ) = (1 -r h ) + (p max -1)∆. (i) we have C 1 -C 2 eigenvalues of the form λ 1 = γ(1 - r 2 C 0 /C 1 γ ) = γ - C 0 C 1 r 2 = 1 + ( C 0 C 1 -1)r 1 - C 0 C 1 r 2 = 1 -r 1 + C 0 C 1 (r 1 -r 2 ) = λ 0 + C 0 C 1 (r 1 -r 2 ) (ii) For 0 &lt; h ≤ H -2, we have C h+1 -C h+2 eigenvalues of the form λ h+1 -λ h = γ C 0 C 1 r h+1 γ - r h+2 γ C 1 C h+1 λ h+1 = λ h + (r h+1 -r h+2 ) C 0 C h+1 (iii) The last eigenvalue is λ H -λ H-1 = C 1 r H /γ • C 0 C 1 γ = C 0 r H</formula><p>Therefore, we proved the theorem by showing the induction step from K H-1 to K H holds. ■</p><p>Note that the true symmetric covariance matrix K ′ might not be having the exact format as K, but it can be seen as a perturbation of K where ∥K ′ -K∥ ≤ ϵ, ϵ is a small constant. By Weyl's inequality <ref type="bibr" target="#b92">[93]</ref>, the approximation error of each eigenvalue is bounded by</p><formula xml:id="formula_31">[λ i -ϵ, λ i + ϵ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Algorithm and Experimental Details</head><p>In this section, we first provide an overview of our algorithm, followed by a discussion on the choice of the flat loss and additional experimental details about the training and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Broader Impact Statement</head><p>Our work proposes HypStructure, a structured hyperbolic regularization approach to embed hierarchical information into the learnt representations. This provides significant advancements in understanding and utilizing hierarchical real-world data, particularly for tasks such as representation learning, classification and OOD detection, and we recognize both positive societal impacts and potential risks of this work. The ability to better model hierarchical data in a structured and interpretable fashion is particularly helpful for domains such as AI for science and healthcare, where the learnt representations will be more reflective of the underlying relationships in the domain space. Additionally, the low-dimensional capabilities of hyperbolic geometry can lead to gains in computational efficiency and reduce the carbon footprint in large scale machine learning. However, real-world hierarchical data often incorporates existing biases which may be amplified by structured representation learning, and hence it is important to incorporate fairness constraints to mitigate this risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Pseudocode for HypStructure</head><p>The training scheme for our HypStructure based structured regularization framework is provided in Algorithm 1. At a high level, in HypStructure, we optimize a combination of the following two losses: (1) a hyperbolic CPCC loss to encourage the representations in the hyperbolic space to correspond with the label hierarchy, (2) a hyperbolic centering loss to position the representation corresponding to the root of the node at the centre of the Poincaré ball and the children nodes around it. Compute the Task loss: ℓ Flat (g w (z i ), y i )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Get hyperbolic representations using exp. map (eq. ( <ref type="formula" target="#formula_7">4</ref>)): zi ← exp c 0 (z i )</p><p>8:</p><p>Calculate class prototypes using hyp. Averaging (eq. ( <ref type="formula" target="#formula_10">6</ref>)): ω i ← HypAve K ( zv 1 , . . . zv j )</p><p>9:</p><p>Compute pairwise hyp. distances (eq. ( <ref type="formula" target="#formula_5">3</ref>)</p><formula xml:id="formula_32">) ∀v i , v j ∈ V : ρ(v i , v j ) ← d Bc (ω i , ω j ) 10:</formula><p>Get hyp. CPCC loss (eq. ( <ref type="formula" target="#formula_0">1</ref>): HypCPCC(d T , ρ)</p><p>11:</p><p>Compute hyp. centering loss using (Equation ( <ref type="formula" target="#formula_10">6</ref>)): ℓ center = ∥HypAve B ( z1 , . . . , zB ∥)</p><formula xml:id="formula_33">12:</formula><p>Get total loss using Equation ( <ref type="formula" target="#formula_12">8</ref>): L(D B )</p><p>13:</p><p>Compute Gradients for learnable parameters at time t: u t (θ, w) ← ∇ θ,w L(D B )</p><p>14:</p><p>Refresh the parameters: (θ, w) t+1 ← (θ, w) t -η B u t (θ, w) Output: (z 1 , . . . z N ); θ, w</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Choice of Flat loss</head><p>We use the Supervised Contrastive <ref type="bibr" target="#b38">[39]</ref> (SupCon) loss as the first choice for a flat loss in our experimentation. Let q y be the one-hot vector with the y-th index as 1. The Cross Entropy (CE) loss, defined between the predictions g • f θ (x) and the labels y, as ℓ CE (g • f (x), y) := -i∈[k] q i log(g(f (x)) i ) has been used quite extensively in large-scale classification problems in the literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b97">98]</ref>. However, several shortcoming of the CE loss, such as lack of robustness <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b106">107]</ref> and poor generalization <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">53]</ref> have been discovered in recent research. Contrastive learning has emerged as a viable alternative to the CE loss, to address these shortcomings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28]</ref>. The underlying principle for these methods is to pull together embeddings for positive pairs and push apart the embeddings for negative samples, in the feature space. In the absence of labels, positive samples are created by data augmentations of images and negative samples are randomly chosen from the minibatch. However, when the labels are available, the class information can be leveraged to extend this methodology as a Supervised Contrastive loss (SupCon) by pulling together embeddings from the same class, and pushing apart the embeddings from different classes. This offers a more stable solution for a variety of tasks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b82">83]</ref>. Definition B.1 (SupCon Loss). Given a training sample x i , feature encoder f θ (•) and a projection head h(•), we denote the normalized feature representations from the projection head as:</p><formula xml:id="formula_34">u i = h (f θ (x i )) ∥h(f θ (x i ))∥ 2 ,<label>(10)</label></formula><p>For the N training samples {(x i , y i )} N i=1 , we denote the training batch of 2N (augmented) pairs as {( xi , ỹi )} 2N i=1 and define the SupCon loss as:</p><formula xml:id="formula_35">ℓ SupCon = 1 2N 2N i=1</formula><p>-log</p><formula xml:id="formula_36">1 2Ny i -1 2N k=1 1(k ̸ = i)1(ỹ k = ỹi )e u T i •u k /τ 2N k=1 1(k ̸ = i)e u T i •u k /τ ,<label>(11)</label></formula><p>where N yi refers to the number of images with label y i in the batch, τ is the temperature parameter, • refers to the inner product, and u i and u k are the normalized feature representations using Equation <ref type="bibr" target="#b9">(10)</ref> for xi and xk respectively.</p><p>While the numerator in the formulation in Equation ( <ref type="formula" target="#formula_36">11</ref>) only considers the samples (and its augmentations) belonging to the same class, the denominator sums over all the negatives as well. Overall, this encourages the network to closely align the feature representations for all the samples belonging to the same class, while pushing apart the representations of samples across different classes.</p><p>We note that our proposed method HypStructure is not limited to the choice of euclidean classification losses as ℓ Flat and we report additional results with hyperbolic classification losses in Sections C.8 and C.9 respectively, demonstrating the wide applicability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.1 Software and Hardware</head><p>We implement our method in PyTorch 2.2.2 and run all experiments on a single NVIDIA GeForce RTX-A6000 GPU. The code for our methodology will be open sourced for a wider audience upon acceptance, in the spirit of reproducible research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 Architecture, Hyperparameters and Training</head><p>We use the ResNet-18 <ref type="bibr" target="#b26">[27]</ref> network as the backbone for CIFAR10, and ResNet-34 as the backbone for CIFAR100 and ImageNet100 datasets. We use a ReLU activated multi layer perceptron with one hidden layer as the projection head h(.) where its hidden layer dimension is the same as input dimension size and the output dimension is 128. We follow the original hyperparameter settings from <ref type="bibr" target="#b38">[39]</ref> for training the CIFAR10 and CIFAR100 models from scratch with a temperature τ = 0.1, feature dimension 512, and training for 500 epochs with an initial learning rate of 0.5 with cosine annealing, optimizing using SGD with momentum 0.9 and weight decay 10 -4 , and a batch size of 512 for all the experiments. For ImageNet100, we finetune the ResNet-34 for 20 epochs following <ref type="bibr" target="#b60">[61]</ref> with an initial learning rate of 0.01 and update the weights of the last residual block and the nonlinear projection head, while freezing the parameters in the first three residual blocks. We use the same α values as the regularization parameters for the CPCC loss in Equation ( <ref type="formula" target="#formula_3">2</ref>) (ℓ 2 -CPCC) and in Equation ( <ref type="formula" target="#formula_12">8</ref>) (our proposed method HypStructure) for a fair comparison and find that the default regularization hyperparameter for the CPCC loss α = 1.0 for both ℓ 2 -CPCC and HypStructure performs well for the experiments on the CIFAR10 and CIFAR100 datasets. We observe that the experiments on the IMAGENET100 dataset benefit from a lower α = 0.5. Additionally, we set the hyperparameter for the centering loss in our methodology as β = 0.01 for all the experiments. We use the default curvature value of c = 1.0 for the mapping and distance computations in the Poincaré ball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.3 Datasets and Hierarchy</head><p>We use the following three datasets for our primary experimentation and training in this work 1. CIFAR10 ( <ref type="bibr" target="#b44">[45]</ref>). It consists of 50,000 training images and 10,000 test images from 10 different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CIFAR100([45]</head><p>). It also consists of 50,000 training images and 10,000 test images, however the images belong to 100 classes. Note that the classes are not identical to the CIFAR10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ImageNet100([72]</head><p>). This dataset is created as a subset of the large-scale ImageNet dataset following Ming et al. <ref type="bibr" target="#b58">[59]</ref>. The original ImageNet dataset consists of 1,000 classes and 1.2 million training images and 50,000 validation images. We construct the ImageNet100 dataset from this original dataset by sampling 100 classes, which results in 128,241 training images and 5000 validation images. We mention the specific classes used for sampling below.</p><p>In addition to the training and validation images, we also require the label hierarchy for each of these datasets for the CPCC computation in ℓ 2 -CPCC and HypStructure approaches. For CIFAR100, we use the three-level hierarchy provided with the dataset release <ref type="foot" target="#foot_0">3</ref> . We show this hierarchy in Table <ref type="table" target="#tab_6">3</ref>, where the top-level is the root of the tree. Since no hierarchy is available for the CIFAR10 and ImageNet100 datasets, we construct a hierarchy for CIFAR10 manually, as seen in Figure <ref type="figure" target="#fig_2">2</ref>. For ImageNet100, we create a subtree from the WordNet<ref type="foot" target="#foot_1">foot_1</ref> hierarchy, given the 100 aforementioned classes as leaves. We consider the classes which are one level above the leaf nodes in the hierarchy as the coarse classes, following Zeng et al. <ref type="bibr" target="#b103">[104]</ref>.</p><p>For the task of OOD detection, we use the following five diverse OOD datasets for CIFAR10 and CIFAR100 as ID datasets, following the literature <ref type="bibr" target="#b82">[83]</ref>: SVHN <ref type="bibr" target="#b64">[65]</ref>, Textures <ref type="bibr" target="#b8">[9]</ref>, Places365 <ref type="bibr" target="#b108">[109]</ref>, LSUN <ref type="bibr" target="#b101">[102]</ref> and iSUN <ref type="bibr" target="#b98">[99]</ref>. When ImageNet100 is used as the ID dataset, we use 4 diverse OOD datasets as the ones in <ref type="bibr" target="#b36">[37]</ref>, namely subsets of iNaturalist <ref type="bibr" target="#b89">[90]</ref>, SUN <ref type="bibr" target="#b95">[96]</ref>, Places <ref type="bibr" target="#b108">[109]</ref> and Textures <ref type="bibr" target="#b8">[9]</ref>. These datasets have been processed so that there is no overlap with the ImageNet classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Delta Hyperbolicity Metrics</head><p>We use Gromov's δ rel to evaluate the tree-likeness of the data in Section 4.1, following Khrulkov et al. <ref type="bibr" target="#b39">[40]</ref>. For an arbitrary metric space X with metric d, for any three points x, y, z ∈ X , we can define the Gromov product as</p><formula xml:id="formula_37">(y, z) x = 1 2 (d(x, y) + d(x, z) -d(y, z))</formula><p>Then, δ-hyperbolicity can be defined as the minimum value of δ such that for any four points x, y, z, w ∈ X , the following condition holds:</p><formula xml:id="formula_38">(x, z) w ≥ min((x, y) w , (y, z) w ) -δ</formula><p>It can be shown that equivalently, there exists a geometric definition of δ-hyperbolicity. A geodesic triangle in X is δ-slim if each of its side is contained in the δ-neighbourhood of the union of the other two sides. We define δ-hyperbolicity as the minimum δ that guarantees any triangle in X is δ-slim.</p><p>From Figure <ref type="figure" target="#fig_15">10</ref>, when the curvature of the surface increases, the geodesic triangle converges to a tree/star graph, and δ gradually reduces to 0.</p><p>Following Khrulkov et al. <ref type="bibr" target="#b39">[40]</ref>, we use the scale-invariant metric δ rel = 2δ diam(X ) for evaluation, so that the δ rel is normalized in [0, 1], and the diam(•) is the set diameter or the maximal pairwise distance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Results using Linear Evaluation</head><p>We also perform an evaluation of the fine classification accuracy following the common linear evaluation protocol Khosla et al. <ref type="bibr" target="#b38">[39]</ref> where a linear classifier is trained on top of the normalized penultimate layer features. We report these accuracies for the models trained on the CIFAR10 and CIFAR100 datasets in Table <ref type="table" target="#tab_7">4</ref> for the leaf-only variants of the models. We observe that the relative trend of accuracies is identical to the ones reported using the kNN evaluation in Table <ref type="table">1</ref> and our proposed method HypStructure outperforms the flat and ℓ 2 -CPCC methods on both the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Component-wise Ablation Study of HypStructure</head><p>To understand the role of each component in our proposed methodology HypStructure, we perform a detailed ablation study with the different components and measure the fine and the coarse accuracies on the CIFAR100 dataset. Specifically, we examine 1. the role of embedding all internal nodes in the label hierarchy (eq. ( <ref type="formula" target="#formula_12">8</ref>) and line 10 in Algorithm 1), as opposed to only using leaf nodes as in Zeng et al. <ref type="bibr" target="#b103">[104]</ref>. We refer to the inclusion of internal nodes as T int .</p><p>2. the role of hyperbolic class centroids computation using hyperbolic averaging (eq. ( <ref type="formula" target="#formula_10">6</ref>) and line 8 in Algorithm 1), as opposed to the Euclidean computation of class prototypes as in Zeng et al. <ref type="bibr" target="#b103">[104]</ref>. We refer to the hyperbolic class centroid computation as ω hyp .</p><p>3. the role of the hyperbolic centering loss in our proposed methodology (eq. ( <ref type="formula" target="#formula_12">8</ref>) and line 11 in Algorithm 1), as opposed to not using a centering loss. We refer to the inclusion of the centering loss as ℓ center .</p><p>We ablate over the aforementioned settings, where a ✓ denotes the inclusion of that setting, and report the results on the CIFAR100 dataset in Table <ref type="table" target="#tab_8">5</ref>. Firstly, we observe that while the centering loss ℓ center improves the coarse accuracy only by a small increment, it leads to a significant improvement in the fine accuracy (rows 1 → 2 and 4 → 5), indicating that the centering of the root in the poincare disk allows for a better relative positioning of the fine classes within the coarse class groups. Secondly, we observe that both the inclusion of internal nodes T int , and the hyperbolic computation of the class centroids ω hyp is critical for accurately embedding the hierarchy, and removing either of these components (i.e. rows 5 → 3 for T int and rows 5 → 2 for ω hyp ), leads to a degradation in both the fine as well as the coarse accuracies. The best overall performance is observed when all three of the components are included (row 5). The goal of prior works in the OOD literature is the supervised setting of learning an accurate classifier for ID data, along with an ID-OOD detection methodology and this task has been explored in the generative model setting <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b96">97]</ref>, and more extensively in the supervised discriminative model setting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b59">60]</ref>. The methods in this setting can be categorized into four sub-categories following <ref type="bibr" target="#b105">[106]</ref>, primarily:</p><p>Post-Hoc Inference These methods design post-processing/scoring mechanisms on base classifiers such as MSP <ref type="bibr" target="#b29">[30]</ref>, ODIN <ref type="bibr" target="#b50">[51]</ref>, ReAct <ref type="bibr" target="#b81">[82]</ref>, SSD+ <ref type="bibr" target="#b75">[76]</ref>, KNN+ <ref type="bibr" target="#b82">[83]</ref> and RankFeat <ref type="bibr" target="#b79">[80]</ref>.</p><p>Training without outlier data These methods involve training-time regularization or different objective functions for improving OOD detection capabilities such as G-ODIN <ref type="bibr" target="#b35">[36]</ref>, CSI <ref type="bibr" target="#b84">[85]</ref>, LogitNorm <ref type="bibr" target="#b91">[92]</ref> and CIDER <ref type="bibr" target="#b60">[61]</ref>.</p><p>Training with outlier data These methods assume access to auxiliary OOD training samples such as OE <ref type="bibr" target="#b30">[31]</ref> and MixOE <ref type="bibr" target="#b104">[105]</ref>.</p><p>Data Augmentation These methods improve the generalization ability of image classifiers such as StyleAugment <ref type="bibr" target="#b21">[22]</ref>, AugMix <ref type="bibr" target="#b31">[32]</ref> and RegMixup <ref type="bibr" target="#b68">[69]</ref>.</p><p>Our proposed work can be considered primarily in the Training without outlier data category, and we note that none of the prior works use any additional structural regularization term in the objective functions. We report the dataset-wise OOD detection results in Tables 7a, 6 and 7 for CIFAR100, CIFAR10 and ImageNet100 respectively. We compare with several other state-of-the-art baseline OOD detection methods for CIFAR10 and CIFAR100, namely ProxyAnchor <ref type="bibr" target="#b40">[41]</ref>, SimCLR <ref type="bibr" target="#b6">[7]</ref> CSI <ref type="bibr" target="#b84">[85]</ref>, and CIDER <ref type="bibr" target="#b60">[61]</ref> respectively. Results for these methods are taken from CIDER <ref type="bibr" target="#b60">[61]</ref> where contrastive learning based OOD detection methods typically outperforms non-contrastive learning ones. For ImageNet100, in the absence of the available class ids used to train the original models in CIDER <ref type="bibr" target="#b60">[61]</ref>, we finetune the ResNet34 models on the created ImageNet100 dataset. For CIDER and SupCon, we use the official implementations and hyperparameters provided by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.2 Dataset-wise OOD Detection Results</head><p>We observe that our proposed method leads to an improvement in the average OOD detection AUROC over all the ID datasets. In practice, we find that the Euclidean-centroid computational variant (first compute the Euclidean centroids and then apply the exponential map) of our proposed method performs slightly better than the hyperbolic-centroid computational variant (first apply the exponential map and then compute the hyperbolic average), for the specific task of OOD detection, while having equivalent performance on the ID classification task. Hence, we report the OOD detection accuracy corresponding to the first version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Visualization of Learned Features</head><p>We provide additional visualizations of the learnt features from our proposed method HypStructure on the CIFAR10, CIFAR100 and ImageNet100 datasets in <ref type="bibr">Figures 11,</ref><ref type="bibr" target="#b11">12</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Effect of Centering Loss and Embedding Internal Node</head><p>Embedding the internal tree nodes in HypStructure T int (as compared to only leaf nodes in prior work CPCC) and placing the root node at the center of the Poincaré disk with ℓ center loss, helps in embedding the hierarchy more accurately. To understand the impact of these components, we first visualize the learnt representations from HypStructure, with and without these components -i.e. embedding internal nodes and a centering loss vs leaf only nodes, via UMAP in Figure <ref type="figure" target="#fig_17">14</ref> (CIFAR100) and Figure <ref type="figure" target="#fig_18">15</ref> (ImageNet100). We also provide a performance comparison (fine accuracy) in Table <ref type="table">8</ref>.</p><formula xml:id="formula_39">Method</formula><p>CIFAR10 CIFAR100 ImageNet100 HypStructure (leaf only) 94.54 76.22 89.85 HypStructure (with internal nodes and centering) 94.79 76.68 90.12</p><p>Table 8: Fine accuracy comparison of HypStructure with vs. without internal nodes and centering on CIFAR10, CIFAR100, and ImageNet100 datasets.</p><p>First, based on Figures Figure <ref type="figure" target="#fig_17">14</ref> and Figure <ref type="figure" target="#fig_18">15</ref>, one can note that in the leaf-only setting without embedding internal nodes and centering loss (figures on the left), the samples belonging to the fine classes which share the same parent (same color) are in close proximity reflecting the hierarchy accurately, however the samples are not spread evenly. With the embedding of internal nodes and a centering loss (right), we note that the representations are spread between the center (root) to the boundary as well as across the Poincaré disk, which is more representative of the original hierarchy. This also leads to performance improvements as can be seen in Table <ref type="table">8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Effect of Label Hierarchy Weights</head><p>Compared to ranking-based hyperbolic losses <ref type="bibr" target="#b65">[66]</ref>, our HypCPCC factors in absolute values of the node-to-node distances. The learned hierarchy with HypCPCC will not only implicitly encode the correct parent-child relations, but can also learn more complex and weighted hierarchical relationships more accurately. To demonstrate this, we modify the CIFAR10 tree hierarchy, and gradually increase the weight for the left transportation branch to 2× and 4× and use new weighted trees for the CPCC tree distance computation. We visualize the learnt representations in Figure <ref type="figure" target="#fig_7">16</ref> and we can observe that in the learned representations from left to right, the distance between the transportation classes (blue) are larger as compared to other classes, as expected.</p><p>(a) (b) (c)</p><p>Figure <ref type="figure" target="#fig_7">16</ref>: HypStructure can learn more nuanced representations with weighted hierarchy trees. Hyperbolic UMAP visualizations on CIFAR10 using HypStructure with differently weighted leftsubtrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Effect of Klein Averaging</head><p>We experiment with the two HypCPCC variants, using Klein Averaging or Euclidean mean for centroid computation, as mentioned in Section 3.2 and report the results in Table <ref type="table">9</ref>. We empirically observe that using the Klein averaging leads to performance improvements across datasets.</p><p>Method CIFAR10 CIFAR100 ImageNet100 Euclidean 94.56 75.64 90.08 Klein 94.79 76.68 90.12</p><p>Table 9: Fine accuracy comparison between Euclidean and Klein centroid computation methods in HypCPCC on CIFAR10, CIFAR100, and ImageNet100 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 Experiments with the Hyperbolic Supervised Contrastive Loss</head><p>We experiment with the Hyperbolic Supervised Contrastive Loss as proposed in <ref type="bibr" target="#b20">[21]</ref> as the choice of the ℓ Flat loss and refer to this loss as ℓ HypSupCon . We follow the original setup as described by the authors for the measurement of the ℓ HypSupCon , where the representations from the encoders are not normalized directly, instead an exponential map is used to project these features from the Euclidean space to the Poincaré ball first. Then, the inner product measurement in the ℓ SupCon is replaced with the negative hyperbolic distances in the Poincaré ball to compute the ℓ HypSupCon loss. We also experiment with our proposed methodology HypStructure along with the ℓ HypSupCon loss and report the classification accuracies and hierarchy embedding metrics for both these settings in Table <ref type="table" target="#tab_15">10</ref>. We further report the OOD detection performance on CIFAR10, CIFAR100 and ImageNet100 as in-distribution datasets for both of these settings in Tables 11, 12 and 13 respectively. We observe that using HypStructure with a hyperbolic loss such as ℓ HypSupCon as the Flat loss leads to improvements in accuracy across classification and OOD detection tasks while also improving the quality of embedding the hierarchy. This demonstrates the wide applicability of our proposed method HypStructure which can be used in conjunction with both euclidean and non-euclidean classification losses.   We experiment with Clipped Hyperbolic Neural Networks (HNNs) <ref type="bibr" target="#b25">[26]</ref> as a hyperbolic backbone and use our proposed methodology HypStructure in conjunction with the hyperbolic Multinomial Logistic Regression (MLR) loss. We report the classification accuracies and hierarchy embedding metrics on the CIFAR10 and CIFAR100 datasets in Table <ref type="table" target="#tab_19">14</ref>, and the OOD detection performances using CIFAR10 and CIFAR100 as in-distribution datasets in Tables <ref type="table" target="#tab_20">15</ref> and <ref type="table" target="#tab_21">16</ref> respectively. We observe that using HypStructure along with a hyperbolic backbone leads to improvements in classification accuracies, reduced distortion in embedding the hierarchy, and improved OOD detection performance overall, demonstrating the wide applicability of HypStructure with hyperbolic networks.    Justification: The abstract and the introduction both clearly state the claims made by the paper, along with a clear description of the contributions, assumptions and limitations.</p><p>Guidelines:</p><p>• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Limitations</head><p>Question: Does the paper discuss the limitations of the work performed by the authors?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: Yes, the limitations of the current work as well as the avenues for future improvements to the current work can be found in Section 7.</p><p>Guidelines:</p><p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper.</p><p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.</p><p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theory Assumptions and Proofs</head><p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Open access to data and code</head><p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p><p>Answer: [Yes]</p><p>Justification: The paper includes the specific instructions to access the datasets used for all the experimentation. These can be found in Section B in the Appendix. The code for this project is released to the public.</p><p>Guidelines:</p><p>• The answer NA means that paper does not include experiments requiring code.</p><p>• Please see the NeurIPS code and data submission guidelines (<ref type="url" target="https://nips.cc/public/guides/CodeSubmissionPolicy">https://nips.cc/ public/guides/CodeSubmissionPolicy</ref>) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (<ref type="url" target="https://nips.cc/public/guides/CodeSubmissionPolicy">https: //nips.cc/public/guides/CodeSubmissionPolicy</ref>) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p><p>• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setting/Details</head><p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p><p>Answer: [Yes] Justification: Yes, the paper uses standard data splits from publicly available benchmark sources and provides details regarding the choice of hyperparameters, optimizer and other decisions necessary for understanding the results. These details can be found in Section B in the Appendix.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment Statistical Significance</head><p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p><p>Answer: [Yes]</p><p>Justification: Yes, the paper reports the error bars and other information about the statistical significance of the results in the Section 4 in the main paper.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean.</p><p>• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, the paper provides details about the compute resources, hardware and software needed to reproduce the experiments in Section B of the Appendix. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <ref type="url" target="https://neurips.cc/public/EthicsGuidelines">https://neurips.cc/public/EthicsGuidelines</ref>? Answer: [Yes]</p><p>Justification: Yes, the research conducted in this paper conforms in every respect, with the NeurIPS code of ethics.</p><p>Guidelines:</p><p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p><p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Broader Impacts</head><p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: We discuss the potential positive and negative societal impacts of our work in Section B.1 of the Appendix. Guidelines:</p><p>• The answer NA means that there is no societal impact of the work performed.</p><p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Safeguards</head><p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper uses publicly available datasets, and does not release any data or code that have a risk of misuse. Guidelines:</p><p>• The answer NA means that the paper poses no such risks.</p><p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have appropriately cited the original owners of data and code which is used in the paper. Guidelines:</p><p>• The answer NA means that the paper does not use existing assets.</p><p>• The authors should cite the original paper that produced the code package or dataset.</p><p>• The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p><p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">New Assets</head><p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p><p>Answer: [NA]</p><p>Justification: The paper does not release any new assets.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not release new assets.</p><p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.">Crowdsourcing and Research with Human Subjects</head><p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p><p>Answer: <ref type="bibr">[NA]</ref> Justification: The paper does not involve crowdsourcing, nor research with human subjects.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.">Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects</head><p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p><p>Answer: <ref type="bibr">[NA]</ref> Justification: The paper does not involve crowdsourcing nor research with human subjects.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (left) An unweighted label tree with two coarse nodes: F , G. F contains two fine classes A, B and G contains three fine classes C, D, E. We cannot embed this in ℓ 2 exactly (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Example 1 .Example 2 . 2 √ 3 3</head><label>1223</label><figDesc>We intend to embed all nodes in T , including purple internal nodes. Notice that G, C, D, E is a star graph centered at G. Since CG = DG = 1, CD = 2, by triangle inequality C, D, G must be on the same line where G is the center of CD. Similarly, G must be at the center of DE. Hence, the location of E must be at C, which contradicts the uniqueness of all nodes in T .As an easier problem, let us only embed leaf nodes into the Euclidean space as shown in Figure1. Since CD = DE = CE = 2, they must be on a plane with an equilateral triangle △ CDE in Euclidean geometry. Then all the green classes have the same distance 4 to each yellow class. Therefore, A, B must be on the line perpendicular to △ CDE and intersecting the plane with O, which is the barycenter of △ CDE . Due to the uniqueness and symmetry of A, B, we must have AO = BO = 1 to satisfy AB = 2. AO = 1, OE = , AE = 4 which contradicts the Pythagorean Theorem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Using ℓ 2 -CPCC for structured representation on CIFAR10. CIFAR10 hierarchy (left) has a three level structure with 13 vertices. For a 512-dimensional embedding, we apply ℓ 2 -CPCC either for the full tree (middle) or the leaf nodes only (right) and plot the ground truth tree metric against pairwise Euclidean centroid distances of the learnt representation. The optimal train CPCC is 1.</figDesc><graphic coords="4,304.16,69.22,201.67,50.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Lines on different models for 2-dimensional hyperbolic space.</figDesc><graphic coords="4,297.28,413.25,238.26,115.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 3 . 2 (</head><label>32</label><figDesc>Poincaré Ball Model). Given c as a constant, the Poincaré ball model (B d c , g B ) is defined by a manifold of an open ball B d c = {z ∈ R d : c∥z∥ 2 &lt; 1} and metric tensor g B that defines an inner product of T z B d c . The model is equipped with the distance [88] as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evaluation of distortion vs feature dimensions for HypStructure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>(a) Hyperbolic UMAP: HypStructure (b) Euclidean t-SNE: Flat (c) Euclidean t-SNE: HypStructure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Left: Hyperbolic UMAP visualization of CIFAR10's HypStructure representation on Poincaré disk. Middle and Right: t-SNE visualization of learnt representations on CIFAR100.</figDesc><graphic coords="7,228.78,72.00,126.72,101.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>(a) OOD detection performance with CIFAR100 as ID dataset. (b) CIFAR100 (ID) vs. SVHN (OOD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Left: OOD detection score across various datasets on the CIFAR100 ID dataset. Right: Hyperbolic UMAP of the CIFAR100 (ID) test vs SVHN (OOD) test features learnt from HypStructure with a clear separation in the Poincaré disk.</figDesc><graphic coords="7,420.84,510.06,83.17,68.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: CIFAR100 as in-distribution dataset. Left (a): Hierarchical block pattern of K. Middle (b): Top 100 eigenvalues of K for different representation. Right (c): OOD detection for CIFAR100 vs. SVHN with the top k-th principal component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>. . . . . . . . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Subtree corresponds to the shown submatrix of K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 .</head><label>2</label><figDesc>Given d by d matrix M where M ii = 1, ∀i ∈ [d], and M ij = p otherwise, i.e., λ 1 = 1 + p(d -1) and λ 2 = • • • = λ d = 1 -p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Algorithm 1 4 : 5 :</head><label>145</label><figDesc>HypStructure: Hyperbolic Structured Representation Learning Input: Batch size B, Label tree T = (V, E, e), Number of epochs K, Task Loss formulation ℓ Flat , Encoder f θ , Classifier Head g w , Learning Rate η, Hyperparameters α, β 1: Initialize model parameters: θ, w 2: for epoch = 1, 2, . . . K do 3: for batch = 1, 2, . . . , B do Get image-label pairs: {(x i , y i )} B i=1 Forward pass to compute the representations: (z 1 . . . z B ) ← (f θ (x 1 ) . . . (f θ (x B )) 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Example of a δ-slim triangle, where each side of △ABC is the geodesic distance of two points in the metric space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :Figure 12 :Figure 13 :</head><label>111213</label><figDesc>Figure 11: Euclidean t-SNE Visualizations on CIFAR10.</figDesc><graphic coords="29,181.22,578.18,118.80,102.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Hyperbolic UMAP Visualizations on CIFAR100 using HypStructure without embedding the internal nodes and a hyperbolic centering loss (left), and with embedding the internal nodes along with a centering loss (right).</figDesc><graphic coords="30,161.42,532.76,138.59,119.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Hyperbolic UMAP Visualizations on ImageNet100 using HypStructure without embedding the internal nodes and a hyperbolic centering loss (left), and with embedding the internal nodes along with a centering loss (right).</figDesc><graphic coords="31,311.98,72.00,138.60,118.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>OOD detection AUROC with CIFAR10 and ImageNet100 as ID.the goal of the OOD detection task is to design a methodology that can solve a binary problem of whether an incoming sample x ∈ X is fromP X i.e. y ∈ Y in (ID) or y / ∈ Y in (OOD).OOD datasets We evaluate on 5 OOD image datasets when CIFAR10 and CIFAR100 are used as the ID datasets, namely SVHN<ref type="bibr" target="#b64">[65]</ref>, Places365<ref type="bibr" target="#b108">[109]</ref>, Textures<ref type="bibr" target="#b8">[9]</ref>, LSUN<ref type="bibr" target="#b101">[102]</ref>, and iSUN<ref type="bibr" target="#b98">[99]</ref>, and 4 large scale OOD test datasets, specifically SUN<ref type="bibr" target="#b101">[102]</ref>, Places365<ref type="bibr" target="#b108">[109]</ref></figDesc><table><row><cell>Method</cell><cell cols="2">AUROC Method</cell><cell>AUROC</cell></row><row><cell cols="2">CIFAR10</cell><cell cols="2">ImageNet100</cell></row><row><cell>SSD+</cell><cell>97.38</cell><cell>SSD+</cell><cell>92.46</cell></row><row><cell>KNN+</cell><cell>97.22</cell><cell>KNN+</cell><cell>92.74</cell></row><row><cell>ℓ 2 -CPCC</cell><cell>76.67</cell><cell>ℓ 2 -CPCC</cell><cell>91.33</cell></row><row><cell>HypStructure</cell><cell>97.75</cell><cell>HypStructure</cell><cell>93.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Class Hierarchy of the CIFAR100 Dataset</figDesc><table><row><cell>Coarse Classes</cell><cell>Fine Classes</cell></row><row><cell>aquatic mammals</cell><cell>beaver, dolphin, otter, seal, whale</cell></row><row><cell>fish</cell><cell>aquarium fish, flatfish, ray, shark, trout</cell></row><row><cell>flowers</cell><cell>orchids, poppies, roses, sunflowers, tulips</cell></row><row><cell>food containers</cell><cell>bottles, bowls, cans, cups, plates</cell></row><row><cell>fruit and vegetables</cell><cell>apples, mushrooms, oranges, pears, sweet peppers</cell></row><row><cell>household electrical devices</cell><cell>clock, computer keyboard, lamp, telephone, television</cell></row><row><cell>household furniture</cell><cell>bed, chair, couch, table, wardrobe</cell></row><row><cell>insects</cell><cell>bee, beetle, butterfly, caterpillar, cockroach</cell></row><row><cell>large carnivores</cell><cell>bear, leopard, lion, tiger, wolf</cell></row><row><cell>large man-made outdoor things</cell><cell>bridge, castle, house, road, skyscraper</cell></row><row><cell>large natural outdoor scenes</cell><cell>cloud, forest, mountain, plain, sea</cell></row><row><cell>large omnivores and herbivores</cell><cell>camel, cattle, chimpanzee, elephant, kangaroo</cell></row><row><cell>medium-sized mammals</cell><cell>fox, porcupine, possum, raccoon, skunk</cell></row><row><cell>non-insect invertebrates</cell><cell>crab, lobster, snail, spider, worm</cell></row><row><cell>people</cell><cell>baby, boy, girl, man, woman</cell></row><row><cell>reptiles</cell><cell>crocodile, dinosaur, lizard, snake, turtle</cell></row><row><cell>small mammals</cell><cell>hamster, mouse, rabbit, shrew, squirrel</cell></row><row><cell>trees</cell><cell>maple, oak, palm, pine, willow</cell></row><row><cell>vehicles 1</cell><cell>bicycle, bus, motorcycle, pickup truck, train</cell></row><row><cell>vehicles 2</cell><cell>lawn-mower, rocket, streetcar, tank, tractor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Linear classification accuracy using SupCon<ref type="bibr" target="#b38">[39]</ref> as ℓ Flat .</figDesc><table><row><cell>Dataset</cell><cell>Method (SupCon)</cell><cell>Fine Accuracy (↑)</cell></row><row><cell></cell><cell>Flat</cell><cell>94.53</cell></row><row><cell>CIFAR10</cell><cell>ℓ 2 -CPCC</cell><cell>95.08</cell></row><row><cell></cell><cell>HypStructure (Ours)</cell><cell>95.18</cell></row><row><cell></cell><cell>Flat</cell><cell>75.11</cell></row><row><cell>CIFAR100</cell><cell>ℓ 2 -CPCC</cell><cell>75.66</cell></row><row><cell></cell><cell>HypStructure (Ours)</cell><cell>77.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the components of HypStructure. We report the Classification accuracies based on the CIFAR100 model trained with ResNet-34.</figDesc><table><row><cell></cell><cell>HypStructure Components</cell><cell></cell><cell cols="2">Classification Acc.↑</cell></row><row><cell cols="4">Internal Nodes (T int ) Hyp. Class Centroids (ω hyp ) Hyp. Centering (ℓ center ) Fine</cell><cell>Coarse</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell>75.03</cell><cell>84.77</cell></row><row><cell>✓</cell><cell></cell><cell>✓</cell><cell>75.61</cell><cell>84.81</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>76.22</cell><cell>85.70</cell></row><row><cell>✓</cell><cell>✓</cell><cell></cell><cell>76.59</cell><cell>86.23</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>76.91</cell><cell>86.22</cell></row><row><cell>C.3 OOD detection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">C.3.1 Related Work and Methods</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results on CIFAR10. OOD detection performance for ResNet-18 trained on CIFAR10.Training with HypStructure achieves strong OOD detection performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">OOD Dataset AUROC (↑)</cell><cell>Avg. (↑)</cell></row><row><cell></cell><cell cols="4">SVHN Textures Places365 LSUN iSUN</cell><cell></cell></row><row><cell>ProxyAnchor</cell><cell>94.55</cell><cell>93.16</cell><cell>92.06</cell><cell>97.02 96.56</cell><cell>94.67</cell></row><row><cell>CE + SimCLR</cell><cell>99.22</cell><cell>96.56</cell><cell>86.70</cell><cell>85.60 86.78</cell><cell>90.97</cell></row><row><cell>CSI</cell><cell>94.69</cell><cell>94.87</cell><cell>93.04</cell><cell>97.93 98.01</cell><cell>95.71</cell></row><row><cell>CIDER</cell><cell>99.72</cell><cell>96.85</cell><cell>94.09</cell><cell>99.01 96.64</cell><cell>97.26</cell></row><row><cell>SSD+</cell><cell>99.51</cell><cell>98.35</cell><cell>95.57</cell><cell>97.83 95.67</cell><cell>97.38</cell></row><row><cell>KNN+</cell><cell>99.61</cell><cell>97.43</cell><cell>94.88</cell><cell>98.01 96.21</cell><cell>97.22</cell></row><row><cell>ℓ 2 -CPCC</cell><cell>93.27</cell><cell>94.76</cell><cell>60.15</cell><cell>75.29 59.87</cell><cell>76.67</cell></row><row><cell cols="2">HypStructure (Ours) 99.75</cell><cell>98.89</cell><cell>94.80</cell><cell>99.67 95.64</cell><cell>97.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Results on ImageNet100. OOD detection performance for ResNet-34 trained on Ima-geNet100. Training with HypStructure achieves strong OOD detection performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">OOD Dataset AUROC (↑)</cell><cell></cell><cell>Avg. (↑)</cell></row><row><cell></cell><cell cols="4">SUN Places365 Textures iNaturalist</cell><cell></cell></row><row><cell>CIDER</cell><cell>91.63</cell><cell>89.29</cell><cell>97.98</cell><cell>96.35</cell><cell>93.81</cell></row><row><cell>SSD+</cell><cell>88.97</cell><cell>85.98</cell><cell>98.49</cell><cell>96.42</cell><cell>92.46</cell></row><row><cell>KNN+</cell><cell>89.48</cell><cell>86.64</cell><cell>98.38</cell><cell>96.46</cell><cell>92.74</cell></row><row><cell>ℓ 2 -CPCC</cell><cell>90.95</cell><cell>86.87</cell><cell>97.41</cell><cell>90.08</cell><cell>91.33</cell></row><row><cell cols="2">HypStructure (Ours) 92.21</cell><cell>90.12</cell><cell>97.33</cell><cell>95.61</cell><cell>93.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Evaluation of hierarchical information distortion and classification accuracy usingHypSupCon<ref type="bibr" target="#b20">[21]</ref> as ℓ Flat . All metrics are reported as mean (standard deviation) over 3 seeds.</figDesc><table><row><cell>Dataset (Backbone)</cell><cell>Method</cell><cell cols="2">Distortion of Hierarchy δ rel (↓) CPCC (↑)</cell><cell>Classification Accuracy Fine (↑) Coarse (↑)</cell></row><row><cell>CIFAR10</cell><cell>Flat</cell><cell cols="3">0.128 (0.007) 0.745 (0.017) 94.58 (0.04) 98.96 (0.01)</cell></row><row><cell>(ResNet-18)</cell><cell cols="2">HypStructure 0.017 (0.001)</cell><cell cols="2">0.989 (0.001) 95.04 (0.02) 99.36 (0.02)</cell></row><row><cell>CIFAR100</cell><cell>Flat</cell><cell cols="3">0.168 (0.002) 0.664 (0.012) 75.81 (0.06) 85.26 (0.07)</cell></row><row><cell>(ResNet-34)</cell><cell cols="4">HypStructure 0.112 (0.005) 0.773 (0.008) 76.22 (0.14) 85.83 (0.06)</cell></row><row><cell>ImageNet100</cell><cell>Flat</cell><cell cols="3">0.157 (0.004) 0.473 (0.004) 89.87 (0.01) 90.41 (0.01)</cell></row><row><cell>(ResNet-34)</cell><cell cols="4">HypStructure 0.126 (0.002) 0.714 (0.003) 90.26 (0.01) 90.95 (0.01)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Results on CIFAR10 when using the HypSupCon<ref type="bibr" target="#b20">[21]</ref> as ℓ Flat using ResNet-18 as the backbone. Training with HypStructure achieves improvements in OOD detection performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">OOD Dataset AUROC (↑)</cell><cell>Avg. (↑)</cell></row><row><cell></cell><cell cols="4">SVHN Textures Places365 LSUN iSUN</cell><cell></cell></row><row><cell>ℓ HypSupCon</cell><cell>89.45</cell><cell>93.39</cell><cell>90.18</cell><cell>98.18 91.31</cell><cell>92.51</cell></row><row><cell cols="2">ℓ HypSupCon + HypStructure (Ours) 91.11</cell><cell>94.45</cell><cell>93.52</cell><cell>99.05 95.24</cell><cell>94.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Results on CIFAR100 when using the HypSupCon<ref type="bibr" target="#b20">[21]</ref> as ℓ Flat using ResNet-34 as the backbone. Training with HypStructure achieves improvements in OOD detection performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">OOD Dataset AUROC (↑)</cell><cell>Avg. (↑)</cell></row><row><cell></cell><cell cols="4">SVHN Textures Places365 LSUN iSUN</cell><cell></cell></row><row><cell>ℓ HypSupCon</cell><cell>80.16</cell><cell>79.61</cell><cell>74.02</cell><cell>70.22 82.35</cell><cell>77.27</cell></row><row><cell cols="2">ℓ HypSupCon + HypStructure (Ours) 82.28</cell><cell>83.51</cell><cell>77.95</cell><cell>86.64 69.86</cell><cell>80.05</cell></row><row><cell cols="3">C.9 Experiments with a Hyperbolic Backbone</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Results on ImageNet100 when using the HypSupCon<ref type="bibr" target="#b20">[21]</ref> as ℓ Flat using ResNet-34 as the backbone. Training with HypStructure achieves improvements in OOD detection performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">OOD Dataset AUROC (↑)</cell><cell></cell><cell>Avg. (↑)</cell></row><row><cell></cell><cell cols="4">SUN Places365 Textures iNaturalist</cell><cell></cell></row><row><cell>ℓ HypSupCon</cell><cell>91.96</cell><cell>90.74</cell><cell>97.42</cell><cell>94.04</cell><cell>93.54</cell></row><row><cell cols="2">ℓ HypSupCon + HypStructure (Ours) 93.87</cell><cell>91.56</cell><cell>97.04</cell><cell>95.16</cell><cell>94.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Evaluation of hierarchical information distortion and classification accuracy using Clipped Hyperbolic Neural Networks<ref type="bibr" target="#b25">[26]</ref> as the backbone. All metrics are reported as mean (standard deviation) over 3 seeds.</figDesc><table><row><cell>Dataset (Backbone)</cell><cell>Method</cell><cell>Distortion of Hierarchy δ rel (↓) CPCC (↑)</cell><cell>Classification Accuracy Fine (↑) Coarse (↑)</cell></row><row><cell>CIFAR10</cell><cell>Flat</cell><cell cols="2">0.084 (0.008) 0.604 (0.004) 94.81 (0.23) 89.71 (2.04)</cell></row><row><cell>(Clipped HNN [26])</cell><cell cols="3">HypStructure 0.013 (0.002) 0.988 (0.001) 94.97 (0.12) 98.35 (0.22)</cell></row><row><cell>CIFAR100</cell><cell>Flat</cell><cell cols="2">0.098 (0.001) 0.528 (0.009) 76.46 (0.26) 49.26 (0.73)</cell></row><row><cell>(Clipped HNN [26])</cell><cell cols="3">HypStructure 0.064 (0.006) 0.624 (0.005) 77.96 (0.14) 55.46 (0.61)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Results on CIFAR10 when using the Clipped Hyperbolic Neural Networks<ref type="bibr" target="#b25">[26]</ref> as the backbone. Training with HypStructure achieves improvements in OOD detection performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">OOD Dataset AUROC (↑)</cell><cell>Avg. (↑)</cell></row><row><cell></cell><cell cols="4">SVHN Textures Places365 LSUN iSUN</cell><cell></cell></row><row><cell>Clipped HNN [26]</cell><cell>92.63</cell><cell>90.74</cell><cell>88.46</cell><cell>95.66 92.41</cell><cell>91.98</cell></row><row><cell cols="2">Clipped HNN [26] + HypStructure (Ours) 95.41</cell><cell>93.91</cell><cell>92.31</cell><cell>96.87 94.92</cell><cell>94.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 16 :</head><label>16</label><figDesc>Results on CIFAR100 when using the Clipped Hyperbolic Neural Networks<ref type="bibr" target="#b25">[26]</ref> as the backbone. Training with HypStructure achieves improvements in OOD detection performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">OOD Dataset AUROC (↑)</cell><cell>Avg. (↑)</cell></row><row><cell></cell><cell cols="4">SVHN Textures Places365 LSUN iSUN</cell><cell></cell></row><row><cell>Clipped HNN [26]</cell><cell>89.94</cell><cell>83.77</cell><cell>77.26</cell><cell>82.87 82.35</cell><cell>83.23</cell></row><row><cell cols="2">Clipped HNN [26] + HypStructure (Ours) 91.56</cell><cell>84.31</cell><cell>78.45</cell><cell>87.53 83.44</cell><cell>85.06</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://www.cs.toronto.edu/ kriz/cifar.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://www.nltk.org/howto/wordnet.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>SZ and HZ are partially supported by an <rs type="funder">NSF</rs> <rs type="grantName">IIS grant</rs> No. <rs type="grantNumber">2416897</rs>. HZ would like to thank the support from a <rs type="grantName">Google Research Scholar Award</rs>. MY was supported by <rs type="funder">MEXT KAKENHI</rs> Grant Number <rs type="grantNumber">24K03004</rs>. We would also like to thank the reviewers for their constructive feedback during the review process. The views and conclusions expressed in this paper are solely those of the authors and do not necessarily reflect the official policies or positions of the supporting companies and government agencies.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BMDxH3w">
					<idno type="grant-number">2416897</idno>
					<orgName type="grant-name">IIS grant</orgName>
				</org>
				<org type="funding" xml:id="_ygq7wcY">
					<idno type="grant-number">24K03004</idno>
					<orgName type="grant-name">Google Research Scholar Award</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following <ref type="bibr" target="#b58">[59]</ref>, we use the below 100 class id's for creating the ImageNet100 subset: n03877845, n03000684, n03110669, n03710721, n02825657, n02113186, n01817953, n04239074, n02002556, n04356056, n03187595, n03355925, n03125729, n02058221, n01580077, n03016953, n02843684, n04371430, n01944390, n03887697, n04037443, n02493793, n01518878, n03840681, n04179913, n01871265, n03866082, n03180011, n01910747, n03388549, n03908714, n01855032, n02134084, n03400231, n04483307, n03721384, n02033041, n01775062, n02808304, n13052670, n01601694, n04136333, n03272562, n03895866, n03995372, n06785654, n02111889, n03447721, n03666591, n04376876, n03929855, n02128757, n02326432, n07614500, n01695060, n02484975, n02105412, n04090263, n03127925, n04550184, n04606251, n02488702, n03404251, n03633091, n02091635, n03457902, n02233338, n02483362, n04461696, n02871525, n01689811, n01498041, n02107312, n01632458, n03394916, n04147183, n04418357, n03218198, n01917289, n02102318, n02088364, n09835506, n02095570, n03982430, n04041544, n04562935, n03933933, n01843065, n02128925, n02480495, n03425413, n03935335, n02971356, n02124075, n07714571, n03133878, n02097130, n02113799, n09399592, n03594945.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix is segmented into the following key parts.</p><p>1. Section A continues the analysis of the eigenspectrum of CPCC-optimized representation matrix and generalizes it for an arbitrary label tree.</p><p>2. Section B discusses additional details about our proposed method HypStructure, its implementation and broader impact of our work. In particular, an overview of the method is first provided, and then we describe hyperparameter settings of our method and the main baselines, followed by extra dataset details and explanation of evaluation metrics.</p><p>3. Section C reports ablation studies, detailed results on OOD detection and provides additional experimental results and visualizations not included in the main paper due to lack of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of Eigenspectrum Analysis</head><p>In this section, we first introduce some notations, discuss the setup for our analysis, followed by preliminary lemmas, and then characterize the eigenspectrum of CPCC-based structured representations in Theorem A.2 for an arbitrary label tree and Theorem 5.1 for a balanced tree presented in the main body.</p><p>Proof Sketch The proof of Theorem A.2 and Theorem 5.1 relies on the important observation of a hierarchical block structure of the covariance matrix of CPCC-regularized features, as shown in Figure <ref type="figure">8a</ref>, which will also be supported by Lemma A.1 and Corollary A.1. Theorem A.1 <ref type="bibr" target="#b2">[3]</ref> and Lemma A.2 characterize the eigenvalues of a block correlation matrix induced from a basic tree where the matrix only has three types of values: diagonal values of 1s, one for within group entry, and another for across group entry. Larger within group entries lead to the larger eigenvalues. Theorem A.1 <ref type="bibr" target="#b2">[3]</ref> and Lemma A.2 are then used as the base case for the induction proof of Theorem 5.1.</p><p>For an arbitrary tree, in Theorem A.2, we use Weyl's Theorem <ref type="bibr" target="#b92">[93]</ref> to bound the gap between within group entries and across group entries that leads to the phase transition of eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup details</head><p>After training with the HypStructure loss till convergence, let us denote the feature matrix as Z ∈ R n×d , where each row of Z is a d-dimensional vector of an in distribution training sample, and the CPCC is maximized to 1. We let C 0 = n, C 1 , C 2 , . . . , C H = 1 be the number of class labels at height h of the tree T . Following the standard pre-processing steps in OOD detection <ref type="bibr" target="#b75">[76]</ref>, we assume that the features are standardized and normalized so that E[Z] = 0 and ∥Z i ∥ 2 = 1, ∀i. Besides, we assume that in T , the distance from root node to each leaf node is the same. Otherwise, following Santurkar et al. <ref type="bibr" target="#b73">[74]</ref>, we can insert dummy parents or children into the tree to make sure vertices at the same level have similar visual granularity. We then apply CPCC to each node in the extended tree, where each leaf node is one sample. We note that although this is slightly different from the implementation where the leaf nodes are fine class nodes, the distance for samples within fine classes are automatically minimized by classification loss like cross-entropy and supervised contrastive loss.</p><p>Given these assumptions, we want to analyze the eigenspectrum of the inverse sample covariance matrix 1 n-1 Z ⊤ Z, which is the same as investigating the eigenvalues of K = ZZ ⊤ where Z is ordered by classes at all levels, i.e., samples having the same fine-grained labels and coarse labels should be placed together. This is because the matrix scaling and permutation will not change the order of singular values.</p><p>Since CPCC (eq. ( <ref type="formula">1</ref>)) is a correlation coefficient, when it is maximized, the n by n pairwise Poincaré distance matrix is perfectly correlated with the ground truth pairwise tree-metric matrix, where each entry is the tree distance between two samples on the tree, no matter we apply CPCC to leaves or all vertices. This implies that in the similarity matrix K, the relative order of entries are the opposite of tree matrix, and it is trivial to show it as follows Lemma A.1. The relative order of entries in K will be the reverse of the order in tree distance.</p><p>. Now considering the CPCC computation, if the CPCC is maximized, the pairwise Euclidean matrix is of On the other hand, for the rest of C h eigenvalues, we analyze matrix A:</p><p>The inequality comes from the effect of the maximization of CPCC that</p><p>The eigenvalues of A 1 , A 2 have the analytical form, where A 1 's eigenvalues have the form of 1 + (p i -1)r h and A 2 's eigenvalues can be derived by Lemma A.2. By Weyl's inequality <ref type="bibr" target="#b92">[93]</ref>, the minimum of these C h eigenvalues is at least</p><p>To guarantee eigenvalues from Part (ii) are larger, we want L ≥ U . We solve this inequality with m, and we will get the desired range of m. ■</p><p>When r h = r 1 in Theorem A.2, we have ∆ = 0. Therefore, for a three level basic tree with only r 1 , r 2 , if m ≤ M/(p max (C 1 -1)), C 0 -C 1 eigenvalues are all smaller than C 1 eigenvalues.</p><p>In general, we have shown that when m, i.e., the across group similarity is sufficiently small, the eigenvalue gap always exists. When the label tree T is balanced, we can further specify the expression of each eigenvalue and the amount of eigenvalue gaps.</p><p>We now formally restate the Theorem 4.1 from the main paper and give its proof.</p><p>Theorem 5.1 (Eigenspectrum of Structured Representation with Balanced Label Tree). Let T be a balanced tree with height H, such that each level has C h nodes, h ∈ [0, H]. Let us denote each entry of K as r h where h is the height of the lowest common ancestor of the row and the column sample. If r h ≥ 0, ∀h, then: (i) For h = 0, we have</p><p>Proof. From Corollary A.1, we know that K ∈ R C0×C0 has a block-wise structure.</p><p>Since all statements are presented recursively, we prove the theorem by structural induction on the height of the tree.</p><p>The base case is Lemma A.2 with a two level hierarchy tree where only (i) and (iii) are applicable, and p = r 1 , C 0 = d, C 1 = 1. By Lemma A.2, K has C 0 -1 eigenvalues as λ 0 = 1 -r 1 , and one eigenvalue as</p><p>0 . Let us now assume that the theorem is true for the balanced tree whose root node is at height H -1. Then if we have a tree with height H. We call the resulting matrix K H .</p><p>By the first bullet point of Theorem A.1 we directly get λ 0 from (i). Then by the second bullet point of Theorem A.1, the rest of the eigenvalues are from the symmetric matrix A H-1 ∈ R C1×C1 whose diagonal elements are γ = 1 + (C 0 /C 1 -1)r 1 and whose off diagonal elements are</p><p>The key is to observe that A H-1 is still a block structured matrix. After A H-1 is scaled by γ, the resulting matrix can be also seen as a result of maximizing CPCC where the off diagonal blocks have smaller values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applying the hypothesis induction, we then know the expression of eigenvalues for A H-1 as</head><p>Answer: [Yes] Justification: Yes, for all the main results of the paper in Section 5, a full set of assumptions and a complete proof is provided in Section A in the Appendix. Guidelines:</p><p>• The answer NA means that the paper does not include theoretical results.</p><p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems.</p><p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Result Reproducibility</head><p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes, the paper discloses all the necessary details about the implemented architectures used, hyperparameters for each setting, algorithm pseudocode and other experimental details to reproduce all the experiments in the paper. These details can be found in Section B in the Appendix. Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways.</p><p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. , with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p><p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tree-like structure in large social and information networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 13th international conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The eigenstructure of block-structured correlation matrices and its implications for principal component analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cadima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Calheiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Preto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Statistics</title>
		<idno type="ISSN">0266-4763</idno>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1360" to="0532" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hyperbolic geometry. Flavors of geometry</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Parry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low-dimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the hyperbolicity of small-world and treelike random graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="434" to="491" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18613" to="18624" />
		</imprint>
	</monogr>
	<note>d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hyperbolic-to-hyperbolic graph convolutional network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="154" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical classification with confidence using generalized logits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Enouen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1874" to="1881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical image classification using entailment cone embeddings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makarova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="836" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Embedding text in hyperbolic spaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12)</title>
		<meeting>the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large margin deep networks for classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hyperbolic vision transformers: Combining improvements in metric learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ermolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mirvakhabova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7409" to="7419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5345" to="5355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00653</idno>
		<title level="m">Hyperbolic contrastive learning for visual representations beyond objects</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bygh9j09KX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hyperbolic groups</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gromov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays in group theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="75" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hyperbolic attention networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJxHsjRqFQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cnn-rnn: a large-scale hierarchical image classification framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia tools and applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10251" to="10271" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clipped hyperbolic classifiers are super-hyperbolic classifiers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyxCxhRcY7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AugMix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>id=Bklr3j0cKX</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ranking info noise contrastive estimation: Boosting contrastive learning via ranked positives</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<meeting><address><addrLine>Cambridge; New York, 2</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>nd ed edition</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10951" to="10960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards scaling out-of-distribution detection for large semantic space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scaled gromov hyperbolic graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jonckheere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lohsoonthorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="180" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hyperbolic image embeddings</title>
		<author>
			<persName><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mirvakhabova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6418" to="6428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Proxy anchor loss for deep metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3238" to="3247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Why normalizing flows fail to detect out-ofdistribution data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Large scale learning of general visual representations for transfer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On hyperbolic embeddings in object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schillingmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition: 44th DAGM German Conference, DAGM GCPR 2022</title>
		<meeting><address><addrLine>Konstanz, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 27-30, 2022. 2022</date>
			<biblScope unit="page" from="462" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-ofdistribution samples and adversarial attacks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weakly supervised image classification with coarse and fine labels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The euclidean space is evil: hyperbolic attribute editing for few-shot image generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22714" to="22724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Building and using a semantivisual image hierarchy</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5540027</idno>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3336" to="3343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hyperbolic visual embedding learning for zero-shot recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9273" to="9281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Energy-based out-of-distribution detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Searching for actions on the hyperbole</title>
		<author>
			<persName><forename type="first">T</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Continuous hierarchical representations with poincaré variational auto-encoders</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Le Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<title level="m">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Classics of social choice</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Urken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hewitt</surname></persName>
		</author>
		<ptr target="https://books.google.com/books" />
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>University of Michigan Press</publisher>
		</imprint>
	</monogr>
	<note>id= 0QPv9cg3g-sC</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Delving into out-of-distribution detection with vision-language representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">POEM: Out-of-distribution detection with posterior sampling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15650" to="15665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How to exploit hyperspherical embeddings for out-ofdistribution detection?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The numerical stability of hyperbolic representation learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="24925" to="24949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Structured label inference for visual understanding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nauata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1257" to="1271" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<publisher>Granada</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning continuous hierarchies in the Lorentz model of hyperbolic geometry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v80/nickel18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A capsule network for hierarchical multi-label image classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Noor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robles-Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kusy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Using mixup as a regularizer can surprisingly improve accuracy &amp; out-of-distribution robustness</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=5j6fWcPccO" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Hierarchical organization in complex networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ravasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26112</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14680" to="14691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Representation tradeoffs for hyperbolic embeddings</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4460" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Breeds: Benchmarks for subpopulation shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=mQPBmvyAuk" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Low Distortion Delaunay Embedding of Trees in Hyperbolic Plane</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-25878-7_34</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-642-25878-7_34" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">7034</biblScope>
			<biblScope unit="page" from="355" to="366" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Ssd: A unified framework for self-supervised outlier detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Input complexity and out-of-distribution detection with likelihood-based generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks++</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mukuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The comparison of dendrograms by objective methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Sokal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Rohlf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Taxon</title>
		<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Rankfeat: Rank-1 feature removal for out-of-distribution detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=-deKNiSOXLG" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">React: Out-of-distribution detection with rectified activations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection with deep nearest neighbors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Generalization error bound for hyperbolic ordinal embedding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nitanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cavazza</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10011" to="10021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A weakly supervised fine label classifier enhanced by coarse supervision</title>
		<author>
			<persName><forename type="first">F</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Hyperbolic trigonometry and its application in the poincaré ball model of hyperbolic geometry</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="135" to="147" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Basic level categorization facilitates visual object recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1511.04103" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Mitigating neural network overconfidence with logit normalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Weyl</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01456804</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<idno type="ISSN">1432-1807</idno>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="441" to="479" />
			<date type="published" when="1912-12">Dec. 1912</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Contrastive training for improved out-ofdistribution detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05566</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Likelihood regret: An out-of-distribution detection score for variational auto-encoder</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06755</idno>
		<title level="m">Turkergaze: Crowdsourcing saliency with webcam based eye tracking</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Unsupervised hyperbolic metric learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12465" to="12474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01409</idno>
		<title level="m">Hyperbolic contrastive learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Learning structured representations by embedding class hierarchy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Des Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Mixture outlier exposure: Towards out-of-distribution detection in fine-grained environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Inkawhich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
			<biblScope unit="page" from="5531" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09301</idno>
		<title level="m">Openood v1. 5: Enhanced benchmark for out-of-distribution detection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Hierarchical learning of multi-task sparse metrics for large-scale image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.01.029</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0031320317300377" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
