<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Foundation Models for Recommendation: A Comprehensive Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-17">17 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts</orgName>
								<address>
									<country>Telecommunications</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yihang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts</orgName>
								<address>
									<country>Telecommunications</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanhao</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts</orgName>
								<address>
									<country>Telecommunications</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts</orgName>
								<address>
									<country>Telecommunications</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiashu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Wilfrid Laurier University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts</orgName>
								<address>
									<country>Telecommunications</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yawen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts</orgName>
								<address>
									<country>Telecommunications</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts</orgName>
								<address>
									<country>Telecommunications</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Foundation Models for Recommendation: A Comprehensive Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-17">17 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">F89151FBF1B35CE32183CD3B7B427545</idno>
					<idno type="arXiv">arXiv:2502.08346v3[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recommender systems are essential components of contemporary digital landscape, enabling personalized services across a diverse range of fields, including e-commerce, social media, and entertainment <ref type="bibr" target="#b14">[Zhang et al., 2023]</ref>. The data in RS generally consist of both structural information (e.g., useritem interactions) and textual information (e.g., user attributes and item descriptions). With the rapid development of graph learning, GNN-based methods have emerged as an important technology in RS, which can further enhance the collaborative signals of collaborative filtering and extend the signals to higher-order structures and external knowledge <ref type="bibr" target="#b11">[Wu et al., 2022]</ref>. However, due to the inherent structural bias, they struggle to handle textual information. This is where the powerful capabilities of large language models, which have made significant impacts in the field of natural language processing (NLP) and come into play in the realm of <ref type="bibr">RS [Yang et al., 2023;</ref><ref type="bibr" target="#b13">Zhai et al., 2024]</ref>. Leveraging the advanced text capabilities of LLM, these methods efficiently capture user and item textual information while integrating world knowledge for improved recommendations. However, their reasoning limitations restrict the collaborative signals they can comprehend. Inspired by the success of LLM in the NLP field, the graph domain has also been undergoing transformation, leading to the emergence of graph foundation models (GFMs) <ref type="bibr">[Liu et al., 2023b]</ref>. By integrating GNN and LLM technologies, GFMbased RS can efficiently utilize data to align user preferences and make more precise recommendations with minimized bias, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>. By appropriately integrating key information from both graph structures and text, GFM-based RS hold significant potential to emerge as a new paradigm in RS.</p><p>The GFM-based RS effectively utilize the technological complementarity of GNN and LLM. GNNs struggle to model textual information, while the reasoning capabilities of LLMs do not support their comprehension of higher-order structural information. These two technologies complement each other's shortcomings in GFM, which emerges as a future opportunity in the field of recommendations. For example, LLMGR <ref type="bibr" target="#b9">[Guo et al., 2024]</ref> injects the embeddings learned by GNN into the token embedding sequence of LLM, and adapts the GFM to the recommendation task through two-stage fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Foundation Models for Recommendation</head><p>Graph-Augmented LLM Token-Level Infusion Syntax-Integrated Injection</p><p>LightLM <ref type="bibr">[Mei and Zhang, 2023]</ref>, LLMGR <ref type="bibr" target="#b9">[Guo et al., 2024]</ref>, TMF <ref type="bibr">[Ma et al., 2024a]</ref>, ELMRec <ref type="bibr">[Wang et al., 2024c]</ref> Syntax-Decoupled Injection XRec <ref type="bibr">[Ma et al., 2024b]</ref>, COMPASS <ref type="bibr">[Qiu et al., 2024]</ref> Context-Level Infusion Explicit Graph-to-Text Mapping KGRec <ref type="bibr" target="#b0">[Abu-Rasheed et al., 2024]</ref>, GLRec <ref type="bibr">[Wu et al., 2024a]</ref>, <ref type="bibr">GAL-Rec [Guan et al., 2024]</ref>, <ref type="bibr">HetGCoT-Rec [Jia et al., 2025]</ref> Implicit Graph Retrieval CLAKG <ref type="bibr" target="#b1">[Chen et al., 2024]</ref>, URLLM <ref type="bibr" target="#b6">[Shen et al., 2024]</ref> LLM-Augmented Graph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topology Augmentation</head><p>Edge Level Expansion LLMRG <ref type="bibr" target="#b8">[Wang et al., 2023]</ref>, LLM-KERec <ref type="bibr">[Zhao et al., 2024a]</ref>, <ref type="bibr">CSRec [Yang et al., 2024a]</ref>, <ref type="bibr">LRD [Yang et al., 2024b]</ref>, FineRec <ref type="bibr" target="#b14">[Zhang et al., 2024]</ref>, <ref type="bibr">SAGCN [Liu et al., 2023a]</ref> Node Level Expansion LLM-PKG <ref type="bibr">[Wang et al., 2024b]</ref>, LLMRG <ref type="bibr" target="#b8">[Wang et al., 2023]</ref>, <ref type="bibr">CoLaKG [Cui et al., 2024]</ref>, CIKG <ref type="bibr" target="#b2">[Hu et al., 2024]</ref>,</p><p>AutoGraph <ref type="bibr" target="#b5">[Shan et al., 2024]</ref>, TopicKG [Jeon et al., 2024] Feature Augmentation GaCLLM [Du et al., 2024], HGNN4Rec [Damianou et al., 2024], SKarREC [Li et al., 2024], P4R [Chen and Suzumura, 2024], LIKR [Sakurai et al., 2024] LLM-Graph Harmonization Embedding Fusion LKPNR [Runfeng <ref type="bibr">et al., 2023]</ref>, DynLLM <ref type="bibr">[Zhao et al., 2024b]</ref> Embedding Alignment LLMRec <ref type="bibr">[Wei et al., 2024]</ref>, RLMRec <ref type="bibr" target="#b4">[Ren et al., 2024]</ref>, DALR <ref type="bibr" target="#b3">[Peng et al., 2024]</ref> Figure <ref type="figure">2</ref>: A taxonomy of GFM-based recommender systems.</p><p>LLMRG <ref type="bibr" target="#b8">[Wang et al., 2023]</ref> constructs inference graphs and divergence graphs based on user interaction history using LLM, which are then encoded by GNN for recommendations. DALR <ref type="bibr" target="#b3">[Peng et al., 2024]</ref> aligns the embeddings encoded by GNN and those encoded by LLM in various ways, using the aligned embeddings for subsequent recommendations.</p><p>In this survey, we comprehensively investigate the relevant work of GFM-based RS, and provide a clear taxonomy based on the synergistic relationship between the graph and LLM in GFM: Graph-augmented LLM, LLM-augmented graph and graph-LLM harmonization. Graph-augmented LLM methods can be viewed as utilizing the structural information of the graph to aid the knowledge obtained from LLM pre-training for recommendations. LLM-augmented graph methods, on the other hand, is led by the structural information of the graph, with the world knowledge of LLM serving as auxiliary information. Graph-LLM harmonization methods involve the equal transformation of these two types of information in the representation space.</p><p>As an evergreen topic in both academia and industry, RS have been the subject of numerous surveys (e.g., <ref type="bibr" target="#b1">[Gao et al., 2023;</ref><ref type="bibr">Wu et al., 2024b;</ref><ref type="bibr">Liu et al., 2023b;</ref><ref type="bibr">Li et al., 2023]</ref>). <ref type="bibr" target="#b1">[Gao et al., 2023;</ref><ref type="bibr">Wu et al., 2024b]</ref> focus on specific methodologies, such as GNN-based RS or the more recent LLM-based RS. <ref type="bibr">[Li et al., 2023]</ref> concentrates on utilizing LLM to enhance graphs for tackling tasks related to graphs. However, the field is rapidly evolving with GFMs emerging as a crucial technique of the RS research. <ref type="bibr">[Liu et al., 2023b]</ref> systematically outlines the existing GFMs from the perspectives of pre-training and adaptation, while overlooking the recommendation which is one of the significant downstream tasks for GFM. This survey provides a timely and comprehensive overview that covers the landscape of GFM-based recommender systems.</p><p>The contributions of this survey can be summarized in the following aspects:1) Pioneering overview: Our survey fills the blank in comprehensive work in the field of GFM-based RS. 2) Clear taxonomy: The comprehensive survey presents a wellstructured taxonomy of GFM-based RS, allowing future work to be easily categorized within the corresponding branches.</p><p>3) Promising outlook: We present the challenges and future research directions in this field, which can serve as a valuable reference for research in this rapidly evolving area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we first introduce the basic concepts of GNNbased RS and LLM-based RS, then we provide the definition of graph foundation models, and finally we introduce the proposed taxonomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GNN/LLM-based Recommender Systems</head><p>As data grows explosively, recommender systems have emerged <ref type="bibr" target="#b1">[Gao et al., 2023]</ref>. There are generally three types of data in RS: user data, item data, and user-item interaction data. These data not only contain strong structural information that requires graph representation, but also are rich in textual descriptions. Given the characteristics of the data in RS, they can essentially be abstracted into text-attribute graphs <ref type="bibr">[Jin et al., 2024]</ref>. As two recently popular approaches, GNN-based RS and LLM-based RS both exhibit certain limitations in capitalizing on the information presented in the graphs. GNN-based RS excel at capturing complex higherorder relationships between nodes and model user preferences based on multi-hop neighbors, thus providing accurate recommendations <ref type="bibr" target="#b10">[Wu et al., 2020;</ref><ref type="bibr" target="#b7">Wang et al., 2021]</ref>. However, the sequential order and semantic meaning of words within node descriptions pose a challenge for representation with graph structures, rendering these methods less effective at handling textual information. Conversely, LLM-based RS, with their robust contextual understanding and world knowledge, excel at processing textual descriptions <ref type="bibr">[Yang et al., 2023;</ref><ref type="bibr" target="#b13">Zhai et al., 2024]</ref>. However, limitations in their sequential modeling designs and reasoning capabilities cause these methods to struggle with managing complex relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Foundation Models</head><p>The field of NLP has witnessed a revolution influenced by the Transformer architecture. Pre-trained language models based on the architecture have demonstrated formidable capabilities <ref type="bibr" target="#b3">[Radford et al., 2019;</ref><ref type="bibr">Kenton and Toutanova, 2019]</ref>. These language foundation models, pre-trained on vast amounts of text, possess impressive generalization abilities that can adapt to a wide array of tasks <ref type="bibr" target="#b0">[Bommasani et al., 2021]</ref>. When the scale of the language foundation model reaches a certain magnitude, it is referred to as the LLM <ref type="bibr">[Zhao et al., 2023]</ref>. Inspired by the success of language foundation models in the NLP field, the field of graph learning also recognized the need to improve model performance and generalization capabilities through pre-training. This gave rise to the concept of graph foundation models <ref type="bibr">[Liu et al., 2023b]</ref>, which are models that are pre-trained on large datasets and incorporate graph structures to solve graph-related tasks. As combinations of LLM and graphs, GFMs acquire emergence and homogenization during pre-training <ref type="bibr">[Liu et al., 2023b]</ref>, enabling them to adapt seamlessly and perform impressively across a variety of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Taxonomy of GFM-based RS</head><p>Gravitating towards the contextual backdrop of RS, we place substantial emphasis on examining the organic integration of graph with LLM in GFM, striving for more precise and user-specific recommendations. In accordance with the interrelationship between graphs and LLMs (as illustrated in Figure <ref type="figure">2</ref>) we group the related works into three principal categories: Graph-augmented LLM, where the structural information from graphs is injected into LLMs to enhance the reasoning and generation capabilities for recommendation; LLM-augmented graph, where the structural information (e.g., topological structure) or textual information (e.g., user profiles and item descriptions) in the graph is enhanced with the aid of LLMs; LLM-graph harmonization, where the semantic embedding in LLMs and the structural embedding in graphs are combined seamlessly to achieve mutual optimization and maximize recommendation performance.</p><p>In the following sections, we provide a comprehensive introduction and discussion of the three main categories in the taxonomy of GFM-based RS.</p><p>3 Graph-Augmented LLM LLMs excel at understanding and generating text but struggle with the complex relational structures inherent in recommender systems. While pre-training corpora and in-context learning provide some information, they lack an explicit mechanism to model the intricate relationships between users and items that are naturally represented as graphs. Recent research bridges this gap by integrating graphs with LLMs, focusing on the core challenge: How to design cross-modal interfaces that effectively bridge graph structures to language models? We categorize current methods into token-level infusion and context-level infusion (as illustrated in Figure <ref type="figure" target="#fig_1">3</ref>), based on where the cross-modal interface is implemented. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Token-Level Infusion</head><p>This strategy integrates structural information directly into the LLM's input at the token level. Nodes or subgraphs are represented as special tokens, allowing the LLM to process structural information alongside text.</p><p>Syntax-Integrated Injection. This approach embeds special tokens as syntactic components within the LLM's input sequence. For example, TMF <ref type="bibr">[Ma et al., 2024a]</ref> introduces</p><formula xml:id="formula_0">[ACTION] tokens like [view] or [purchase]</formula><p>to represent user actions within an interaction sequence. The embeddings for these actions are learned from a multi-behavior graph, enabling the LLM to process complex semantics like "user [views] item". Building on this, ELMRec <ref type="bibr">[Wang et al., 2024c]</ref> generates a GCN-based embedding h i for each item i and adds it as a correction term to the item's original text embedding e i , resulting in a refined embedding e ′ i = e i + h i . This operation blends textual and structural information, improving item representation.</p><p>A natural progression from here is to consider whether the LLM can directly output these special tokens. LLMGR <ref type="bibr" target="#b9">[Guo et al., 2024]</ref> explores this by not only including special tokens in the input but also modifying the LLM's output layer. They introduce a special token for each item, allowing the model to directly generate item tokens as recommendations, effectively creating a tighter link between graph-based recommendations and the LLM's output. Further advancing this line of thought, LightLM [Mei and <ref type="bibr">Zhang, 2023]</ref> proposes a hierarchical indexing scheme, which decomposes user/item IDs into multiple special tokens based on a user-item graph-derived index. Each component token encodes a different attribute or function, moving away from opaque numerical IDs towards more semantically meaningful representations.</p><p>Syntax-Decoupled Injection. This method appends graph embeddings as prefixes or suffixes, separating structural information from the main textual prompt. XRec <ref type="bibr">[Ma et al., 2024b]</ref> prepends GNN-learned embeddings that represent user-item relationships to the prompt. These embeddings are trained to capture high-level semantic concepts, such as preference similarity between users. COMPASS <ref type="bibr">[Qiu et al., 2024]</ref> com-bines user queries with knowledge graph embeddings. A GNN generates embeddings that encapsulate both the user's query and relevant knowledge graph entities, which are then used as a prefix to guide the LLM for recommendation.</p><p>Token-level infusion offers a fine-grained way to integrate structural information, allowing for natural interactions between textual and structural data. However, it often requires modifications to the LLM's architecture or careful prompt engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context-Level Infusion</head><p>This strategy provides structural information as context to the LLM, either through text descriptions or implicit retrieval, avoiding modifications to the LLM's architecture.</p><p>Explicit Graph-to-Text Mapping. This method involves converting localized graph structures into natural language descriptions, essentially translating graph relationships into text.</p><p>The following example illustrates explicit graph-to-text mapping: user A → purchase → item B → payment → credit card ⇒ A purchased B with a credit card. The simplest form of this is exemplified by <ref type="bibr">HetGCoT-Rec [Jia et al., 2025]</ref>. They extract multihop neighbors of a target node from a heterogeneous graph and concatenate their attributes to create a natural language description, which becomes the context for the LLM and informs its recommendations. Similarly, KGRec <ref type="bibr" target="#b0">[Abu-Rasheed et al., 2024]</ref> extracts one-hop and two-hop related nodes from a knowledge graph and inserts them into predefined prompt templates, which guides the LLM in generating explainable recommendations.</p><p>Further refinements involve pre-processing the structural information before mapping it to text. GAL-Rec [Guan et al., 2024] maintains a dynamic queue of negative samples, items the user is known to dislike, based on knowledge graph insights and LLM feedback. Providing both positive and negative samples as context for the LLM allows for more nuanced recommendations. The processing of structural information can also occur after its initial conversion to text. GLRec <ref type="bibr">[Wu et al., 2024a]</ref> constructs natural language descriptions of node paths in a job information graph. Each path represents a sequence of related job attributes or skills. During prompt construction, these paths are assigned different weights based on relevance and dependency strength, offering a more refined, contextually rich input to the LLM.</p><p>Implicit Graph Retrieval. When explicit mapping is difficult, this approach uses GNN embeddings to retrieve relevant information from the graph semantically. For example, CLAKG [Chen et al., 2024] encodes a legal knowledge graph using a GNN, and retrieve relevant legal provisions based on the similarity between a user's case description embedding and the provision embeddings. These provisions are then concatenated into the prompt context. URLLM <ref type="bibr" target="#b6">[Shen et al., 2024]</ref> retrieves a user's historical interactions from an item-attribute graph, focusing on neighbor interactions. These records are added to the prompt, providing the LLM with cross-domain preference information.</p><p>Context-level infusion provides a flexible way to incorporate graph knowledge without altering the LLM's architecture. It leverages the LLM's ability to understand and reason over natural language, making it suitable for scenarios where graph structures can be effectively verbalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Graph-augmented LLM methods enhance recommendations by encoding rich relational information from graphs, typically through token-level or context-level infusion. This couples the benefits of graph-structured data with the power of LLMs: the graph provides valuable relational context, while the LLM leverages its pre-trained knowledge to interpret it. Furthermore, since the LLM is the central component, this approach augments the recommender system's ability to extrapolate and make inferences in scenarios where interaction data is limited.</p><p>However, this heavy reliance on the LLM also introduces inherent biases unsuitable for recommendation, such as a lack of diversity and distributional mismatch with user preferences, potentially limiting its scalability and generalizability. The alternative methods, by shifting the focus to enriching or harmonizing the graph itself, effectively mitigate these issues and offer different trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LLM-Augmented Graph</head><p>Shifting the focus to the graph, the core idea of the LLMaugmented graph methods is to augment the data within graphs using LLM, thereby improving the effectiveness of various GNNs employed for recommendation tasks. Such methods can be categorized into topology augmentation and feature augmentation (as illustrated in Figure <ref type="figure" target="#fig_2">4</ref>), based on the aspects of information enhanced in the text-attribute graph according to LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topology Augmentation</head><p>Topology augmentation refers to the processes where the LLM restructures data, utilizing its world knowledge and contextual understanding capabilities to convert specific textual information into a structured format. Due to the introduction of new structural information, the topological structure of the graph constructed from the data is modified, thereby affecting the subsequent processes to achieve more accurate recommendations. Intuitively, we categorize topology augmentation into two types: edge-level expansion and node-level expansion, based on whether new nodes are introduced.</p><p>Edge-level Expansion. This method refers to the process where LLMs introduce new relationships between nodes in the data, such as complementary and substitutable relationships, which are two primary types of relationships of interest in RS.</p><p>To directly utilize the versatile capabilities of LLMs and the vast world knowledge, it is intuitive to adopt text-centric approaches for adding edges in the graph. These approaches typically rely on prior knowledge to guide LLMs in making relationship judgments and constructions, either at a superficial or deeper level. As a straightforward example, LLM-KERec <ref type="bibr">[Zhao et al., 2024a]</ref> employs LLMs to assess the complementarity between pairs of items, thereby establishing complementary relationships among item pairs and constructing a complementary item graph. From a deeper perspective, in addition to leveraging complementary relationships between items, subjective user-generated reviews can also be utilized. <ref type="bibr">SAGCN [Liu et al., 2023a]</ref> and FineRec <ref type="bibr" target="#b14">[Zhang et al., 2024]</ref> utilize LLMs to extract user opinions on items at varying levels of granularity across multiple item attributes (e.g., price, comfort, etc.), using this information as edges to construct distinct graphs for each attribute.</p><p>Compared to the aforementioned methods, the more refined expansion delves into the relationships within the embedding space, subsequently constructing graphs based on implicit relationships between nodes at a certain level. For example, CSRec <ref type="bibr">[Yang et al., 2024a]</ref> first employs an LLM to generate complementary or substitutable category nodes based on existing classified nodes, and then utilizes other pre-trained language models to map the pairs of newly generated nodes and existing nodes into the node set within the semantic space. The relationships generated by the LLM are also mapped into the node set within the embedding space. In addition to mapping edges based on semantic similarity, connections can also be established based on the similarity. Several works <ref type="bibr">[Yang et al., 2024b;</ref><ref type="bibr">Cui et al., 2024]</ref> employ an LLM to encode the textual information of nodes into embeddings, and then measure potential relationships between these embeddings through carefully designed methods. These potential relationships serve as edges for item nodes in the graph within the embedding space.</p><p>Node-level Expansion. This method further leverages the world knowledge and contextual reasoning capabilities of LLMs, utilizing auxiliary information as new nodes to augment the information of existing nodes. Such method often directly utilizes the condensed information generated by LLM as new nodes to be introduced into the graph, or extracts the requisite information from the LLM's output through particular approaches to serve as new nodes in the graph.</p><p>Several works <ref type="bibr" target="#b2">[Jeon et al., 2024;</ref><ref type="bibr" target="#b2">Hu et al., 2024]</ref> directly employ LLMs to generate auxiliary information nodes (e.g., user interests, item categories) for corresponding users or items based on existing textual information. Introducing such auxiliary information nodes can be viewed as a supplementation of information in the textual space. This approach can, to some extent, assist subsequent GNN in modeling better user or item representations. Furthermore, this type of information supplementation can also be performed in the embedding space. For example, AutoGraph <ref type="bibr" target="#b5">[Shan et al., 2024]</ref> utilizes LLMs to encode the textual information of users and items, followed by quantizing the semantic embeddings of users and items. By quantizing these semantic embeddings, fine-grained auxiliary information embeddings for users and items can be derived, which are then used as new nodes to supplement information for users and items.</p><p>Topology augmentation represents effectively utilizing the knowledge obtained from LLM pre-training to influence the training of the GNN. The key structural information introduced into the graph during this process enables subsequent GNN to learn more comprehensive representations of users and items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Augmentation</head><p>Enhancing the topological structure of the graph using LLMs may introduce biases, as they are not particularly adept at extracting structural information from text. In contrast, directly improving the node features in the graph without altering the topological structure is an task where LLMs truly excel. This method focuses on leveraging the natural language processing capabilities of LLMs to augment data at the textual or embedding level, thereby influencing subsequent recommendations.</p><p>Some works <ref type="bibr">[Li et al., 2024;</ref><ref type="bibr">Chen and Suzumura, 2024</ref>] enhance the textual information of nodes by constructing appropriate prompts for input into LLMs. The enhanced textual information is subsequently encoded into embeddings by language models such as BERT. Unlike the aforementioned methods, GaCLLM <ref type="bibr">[Du et al., 2024]</ref> integrates the stages of LLM and GNN. In this approach, the LLM assumes the role responsible for message passing within the GNN, performing textual message passing and aggregation for each node in the graph, which is subsequently encoded by BERT. As a special case, LIKR <ref type="bibr">[Sakurai et al., 2024]</ref> employs LLMs to analyze user interaction histories to derive user-preferred item attributes. The nodes corresponding to these attributes in the graph serve as rewards for Markov walk-based reinforcement learning within the graph.</p><p>The textual information of users and items, being one of the abundant types of information in RS, significantly impacts the performance of RS when effective utilized. Consequently, LLM is increasingly becoming the preferred technology for feature augmentation of graphs based on textual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>The LLM-augmented graph methods, by incorporating the world knowledge of LLM into graph data, can enhance the capabilities of RS at the data level. This makes these methods more competitive in cold start scenarios with sparse interactions. Furthermore, the LLM in these methods is a plug-andplay component, allowing for flexible choices. Moreover, the data processing of the LLM can be performed offline in advance, and online recommendations based on statistical rules or neural models (e.g., GNN) can be made afterwards, significantly reducing time consumption. This has led to the increasing popularity of these methods in industry. However, these methods also have some drawbacks. First, graph learning methods do not fully exploit the world knowledge learned and utilized by LLM, which is a natural shortcoming of such plugand-play methods. And LLMs have the potential to introduce extraneous noise in both dimensions (topology and feature) of topology augmentation, which may consequently give rise to certain biases. Furthermore, they have poor scalability, as the main body of these methods are shallow GNNs whose model depth is affected by the over-smoothing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LLM-Graph Harmonization</head><p>Graph-augmented LLM methods leverage external graph structures to enhance LLMs but often suffer from inefficiencies in real-time adaptability and increased computational overhead. Conversely, LLM-augmented graph methods attempt to incorporate LLMs into graph-based learning but struggle with scalability and effective knowledge utilization. To address these limitations, this section introduces a novel framework that optimally balances computational efficiency, adaptability, and reasoning capabilities. LLMs excel in capturing rich semantic information from unstructured textual data (e.g., item descriptions and user attributes), while GNNs are adept at modeling the topological structure of graphs (e.g., user-item interactions, social connections). As shown in Figure <ref type="figure" target="#fig_3">5</ref>, harmonizing these two paradigms effectively can significantly enhance recommendation performance. Existing methods can be broadly categorized into two mainstream strategies: embedding fusion and embedding alignment, based on transformations of embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Embedding Fusion</head><p>The embedding fusion approach aims to combine LLMderived textual representations with graph-learned structural embeddings, creating a unified feature space that leverages complementary information. This strategy emphasizes the synergy between textual semantics and graph-based connectivity.</p><p>A notable framework in this domain is DynLLM <ref type="bibr">[Zhao et al., 2024b]</ref>, which incorporates graph structures into LLMs through dynamic memory-enhanced fusion. DynLLM addresses the limitation of static embeddings by using a dualflow interaction mechanism: one flow learns from GNNupdated dynamic embeddings reflecting user-item interactions, while the other adapts LLM-generated embeddings based on real-time textual content. These embeddings are fused in a shared latent space, enhancing both semantic and structural understanding. Such a dynamic fusion approach not only captures the temporal evolution of recommendation data but also integrates high-quality textual semantics, leading to more accurate and adaptive recommendations. Another example is LKPNR <ref type="bibr" target="#b4">[Runfeng et al., 2023]</ref>, which combines LLMs with knowledge graphs to enhance personalized news recommendation. LKPNR leverages the semantic richness of LLMs to generate high-quality news representations and uses knowledge graphs to capture the relational structure of news entities. By integrating these modalities, LKPNR effectively addresses the long-tail problem in news recommendation.</p><p>The embedding fusion paradigm is particularly effective because it exploits the contextual richness of LLMs alongside the relational structures captured by GNNs. By fusing these modalities, models like DynLLM and LKPNR enable direct, dynamic, and efficient utilization of both textual and graph data, significantly improving representation learning in recommendation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Embedding Alignment</head><p>Embedding alignment takes a different route by focusing on reconciling the heterogeneity between LLM-generated textual embeddings and GNN-learned structural representations. This strategy ensures that embeddings from both modalities can operate coherently within a unified representational space, reducing information loss and noise. The refined embeddings can provide more valuable information for recommendations.</p><p>The DALR framework <ref type="bibr" target="#b3">[Peng et al., 2024]</ref> serves as a representative example, where structural embeddings from GNNs (e.g., user-item graph representations) and semantic embeddings from LLMs (e.g., product descriptions, user reviews) are aligned through contrastive learning paradigms. This alignment mitigates the semantic gaps and noise introduced by the inherently different data sources. Similarly, methods such as LLMRec <ref type="bibr">[Wei et al., 2024]</ref> and RLMRec <ref type="bibr" target="#b4">[Ren et al., 2024]</ref> also adopt multimodal alignment techniques, such as contrastive learning and MLP-based alignment, to unify embeddings from diverse modalities. For instance, LLMRec employs a denoised data robustification mechanism to enhance the reliability of augmented recommendation data, while RLMRec leverages contrastive alignment strategies to bridge the semantic space of LLMs with the collaborative relational signals from GNNs, thereby improving the overall quality of the learned representations.</p><p>Embedding alignment excels in its ability to unify heterogeneous modalities, ensuring consistent representation learning that facilitates better downstream recommendation tasks, such as personalized ranking or user preference clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>The two approaches offer distinct advantages in integrating LLMs with graph learning for recommender systems. Embedding fusion directly combines textual semantics and structural relationships, leveraging their complementarity to enhance representation learning and personalization. Dynamic fusion methods, such as DynLLM, further enable real-time adaptation to evolving user preferences. Embedding alignment, in contrast, ensures coherence between textual and structural embeddings by mapping them into a shared space, mitigating inconsistencies. Methods like DALR leverages various contrastive learning approaches to enhance alignment robustness. However, embedding fusion may introduce redundant or conflicting information, and increase computational costs, while embedding alignment which is sensitive to noise depends on high-quality training data. In these methods, the LLM primarily serves as an encoder, and due to the constraints of the scenario, it cannot fully utilize its contextual understanding and language generation capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Challenges and Future Directions</head><p>GFMs have demonstrated great potential in recommender systems by incorporating graph structural information with external world knowledge of LLM. However, several challenges hinder their widespread adoption and effectiveness.</p><p>High Computational Cost and Scalability Issues. Existing GFM-based RS require substantial computational resources, posing challenges for large-scale deployment <ref type="bibr" target="#b13">[Zhai et al., 2024]</ref>. The integration of graph-based reasoning and LLM inference results in high memory consumption and slow inference speed, particularly when processing dense user-item graphs or generating personalized recommendations in realtime <ref type="bibr">[Wang et al., 2024a]</ref>. Unlike traditional recommendation models, which can be efficiently pruned or quantized, GFMs face unique scalability constraints due to their reliance on long-range graph dependencies and LLM-generated representations. Addressing these limitations requires advancements in efficient model compression strategies tailored for graphenhanced LLMs, and adaptive graph sparsification techniques to maintain performance while reducing overhead.</p><p>Robustness Against Noisy and Adversarial Data. Realworld user interactions are inherently noisy, exhibiting shortterm fluctuations, incomplete preferences, and adversarial perturbations <ref type="bibr" target="#b14">[Zhang et al., 2023]</ref>. Traditional recommendation models rely on explicit feedback signals, making them susceptible to biased or manipulated data. In contrast, GFMs integrate graph-based user-item relationships and LLMgenerated contextual representations, which introduces additional sources of noise from both structured and unstructured data. Ensuring robustness requires advancements in self-supervised denoising techniques, adversarial training tailored for multimodal representations, and uncertainty-aware modeling to mitigate the impact of unreliable signals while preserving recommendation accuracy.</p><p>Multi-Modal Information Fusion. Modern recommendation scenarios involve a diverse range of data modalities, including text, structured graphs, images, audio, and video <ref type="bibr" target="#b6">[Tao et al., 2020]</ref>. While existing GFMs primarily focus on textual and structural embeddings, effectively incorporating rich multi-modal signals remains an open challenge. Different modalities exhibit varying levels of granularity, semantic gaps, and computational costs, making seamless integration nontrivial. Future research should explore adaptive fusion frameworks, cross-modal alignment mechanisms, and lightweight multi-modal representation learning to balance efficiency and accuracy in large-scale recommender systems. Lack of End-to-End Optimization. The concept of endto-end recommender system is not unfamiliar. When deep learning was introduced into the field of recommendation, a process encapsulation was essentially performed <ref type="bibr" target="#b1">[Covington et al., 2016]</ref>. However, the early neural model RS have gradually fallen behind the times. The process of such RS can be roughly divided into three stages: matching, ranking, and reranking <ref type="bibr" target="#b1">[Gao et al., 2023]</ref>. In reality, this process is often more refined in industrial applications. Such a meticulous process naturally results in better recommendation performance. However, multi-stage model optimization requires a significant investment of time and manpower. Contrarily, an end-to-end generative RS, different from the one mentioned above, encapsulates multiple stages together for optimization. This significantly reduces complexity and can potentially lead to better performance. Gradually, similar endeavors are being pursued in the industrial field. HSTU <ref type="bibr" target="#b13">[Zhai et al., 2024]</ref> simplifies the internal structure of the LLM and fully implements it through serial modeling. Moreover, <ref type="bibr">[Wang et al., 2024d]</ref> takes into account both structural and textual information. Such an integrated generative recommendation that combines matching and ranking may likely be a hotspot in the future. Knowledge-Preference Gap. While GFMs leverage external knowledge to alleviate data sparsity, a fundamental misalignment persists between globally pre-trained world knowledge and personalized user preferences <ref type="bibr">[Wang et al., 2024a]</ref>. Unlike embedding alignment, which focuses on bridging modality gaps (e.g., between graph structures and textual representations), this discrepancy stems from differences in how LLMs interpret knowledge and how users express preferences. For instance, LLM-based recommendation models may naturally generate factually coherent but overly neutral item descriptions, whereas users often respond more favorably to engaging or sensationalized content (e.g., "Shocking! You won't believe this..."). Addressing this challenge requires advancing preference-aware knowledge adaptation, dynamic refinement techniques, and contrastive learning strategies tailored to user-specific interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>As an indispensable technology in modern society, recommender systems stand as one of the most prominent research areas within the field of artificial intelligence. The emergence of graph foundation models is likely to spark a new wave of research enthusiasm in the field of RS. In this survey, we present the first comprehensive overview of GFM-based RS and propose a logically organized taxonomy. Furthermore, we delve deeply into the challenges and vast potential of this field, aiming to inject new vitality into its research endeavors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of GFM-based RS. Compared with GNNbased or LLM-based RS, GFM-based RS are positioned as integrating both approaches to create more comprehensive recommendations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The illustration of graph-augmented LLM methods: a) Token-Level Infusion, where nodes or subgraphs are represented as special tokens, integrating into LLM's input. b) Context-Level Infusion, where graph information is converted into context by translating graph into text or retrieving relevant text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The of LLM-augmented graph methods: a) Topology Augmentation, where LLMs extract structural information from data to alter and augment the topological structure of the graphs.; b) Feature Augmentation, where LLMs processe the textual information in the data, augment the node text or embedding in the graph without changing the topological structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The illustration of LLM-graph harmonization methods: a) Embedding Fusion, where LLM-derived semantic embeddings and GNN-learned structural embeddings are combined into a unified representation space through fusion mechanisms such as concatenation or attention-based integration; b) Embedding Alignment, where embeddings from both modalities are mapped into a shared space using techniques like contrastive learning or MLP-based transformation to enhance consistency and coherence.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Chen and Suzumura, 2024] Junyi Chen and Toyotaro Suzumura. A prompting-based representation learning method for recommendation with large language models</title>
		<author>
			<persName><surname>Abu-Rasheed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.03008</idno>
		<idno>arXiv:2409.16674</idno>
		<imprint>
			<date type="published" when="2021">2024. 2024. 2021. 2021. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>On the opportunities and risks of foundation models Knowledge graphs as context sources for llm-based explanations of learning recommendations</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leverage knowledge graph and large language model for law article recommendation: A case study of chinese criminal law</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2410.04949</idno>
		<idno>arXiv:2402.16539</idno>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the ACM on Web Conference 2024</title>
		<imprint>
			<date type="published" when="2016">2024. 2024. 2016. 2016. 2024. 2024. 2024. 2024. 2024. 2023. 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="51" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep neural networks for youtube recommendations Comprehending knowledge graphs with large language models for recommender systems Towards graph foundation models for personalization Proc. of RecSys Du et al., 2024 Large language model with graph convolution for recommendation Gao et al., 2023 A survey of graph neural networks for recommender systems: Challenges, methods, and directions Guan et al., 2024 Enhancing collaborative semantics of language modeldriven recommendations via graph-aware learning Guo et al., 2024] Naicheng Guo, Hongwei Cheng, Qianqiao Liang, et al. Integrating large language models with graphical session-based recommendation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Triple modality fusion: Aligning visual, textual, and graph data with large language models for multi-behavior recommendations</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.13544</idno>
		<idno>arXiv:2310.17488</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the user-side knowledge gap in knowledge-aware recommendations with large language models</title>
		<editor>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenton</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lee</forename><surname>Kristina Toutanova</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bert</surname></persName>
		</editor>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Qiyao Ma, Xubin Ren, and Chao Huang</publisher>
			<date type="published" when="2019">2024. 2024. 2024. 2024. 2025. 2024. 2024. 2019. 2019. 2023. 2023. 2024. 2023. 2023. 2024. 2024. 2023</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Jia et al., 2025 Hetgcot-rec: Heterogeneous graph-enhanced chain-ofthought llm reasoning for journal recommendation Large language models on graphs: A comprehensive survey Kenton and Toutanova Pre-training of deep bidirectional transformers for language understanding Proc. of NAACL-HLT A survey of graph meets large language model: Progress and future directions Li et al., 2024 Topic-aware knowledge graph with large language models for interoperability in recommender systems Liu et al., 2023a Understanding before recommendation: Semantic aspectaware review exploitation via large language models ACM Transactions on Information Systems Liu et al., 2023b Towards graph foundation models: A survey and beyond Ma et al., 2024a Learning structure and knowledge aware representation with large language models for concept recommendation Ma et al., 2024b Xrec: Large language models for explainable recommendation Mei and Zhang, 2023] Kai Mei and Yongfeng Zhang Lightlm: a lightweight deep and narrow language model for generative recommendation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unveiling user preferences: A knowledge graph and llmdriven approach for conversational recommendation</title>
		<author>
			<persName><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.14459</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2024. 2024. 2024. 2024. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Denoising alignment with large language model for recommendation Radford et al., 2019 Language models are unsupervised multitask learners OpenAI blog</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning with large language models for recommendation</title>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Web Conference 2024</title>
		<meeting>the ACM on Web Conference 2024<address><addrLine>Ren Togo, Takahiro Ogawa</addrLine></address></meeting>
		<imprint>
			<publisher>Keigo Sakurai</publisher>
			<date type="published" when="2023">2024. 2024. 2023</date>
			<biblScope unit="page" from="3464" to="3475" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Runfeng et al., 2023 Lkpnr: Llm and kg for personalized news recommendation framework Sakurai et al., 2024 et al. Llm is knowledge graph reasoner</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An automatic graph construction framework based on large language models for recommendation</title>
		<author>
			<persName><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.12464</idno>
		<idno>arXiv:2412.18241</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Llm&apos;s intuition-aware knowledge graph reasoning for cold-start sequential recommendation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring user retrieval integration towards large language models for cross-domain sequential recommendation</title>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.03085</idno>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">102277</biblScope>
			<date type="published" when="2020">2024. 2024. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tao et al., 2020 Mgat: Multimodal graph attention network for recommendation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph learning based recommender systems: A review</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>of IJCAI. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing recommender systems with large language model reasoning graphs</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10835</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys, RecSys &apos;24</title>
		<meeting>of RecSys, RecSys &apos;24</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023. 2024</date>
			<biblScope unit="page" from="872" to="877" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Llms for user interest exploration in large-scale recommendation systems</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enabling explainable recommendation in e-commerce with llm-powered product knowledge graph</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.01837</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2024">2024. 2024. 2024</date>
			<biblScope unit="page" from="11696" to="11711" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Enhancing high-order interaction awareness in llm-based recommender model</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Llm-enhanced user-item interactions: Leveraging edge information for optimized recommendations</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09617</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of WSDM</title>
		<meeting>of WSDM</meeting>
		<imprint>
			<date type="published" when="2020">2024. 2024. 2024. 2024. 2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Llmrec: Large language models with graph augmentation for recommendation Wu et al., 2020 A comprehensive survey on graph neural networks learning systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring large language model for graph data understanding in online job recommendations</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07622</idno>
	</analytic>
	<monogr>
		<title level="m">Personalization aware llms for recommendation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022. 2024. 2024. 2023. 2023. 2024</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Wu et al., 2024a Graph neural networks in recommender systems: a survey Wu et al., 2024b A survey on large language models for recommendation World Wide Web Proc. of AAAI Common sense enhanced knowledge-based recommendation with large language model Proc. of DSAA</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequential recommendation with latent relations based on large language model</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations</title>
		<author>
			<persName><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finerec: Exploring fine-grained sequential recommendation</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.02057</idno>
		<idno>arXiv:2303.18223</idno>
	</analytic>
	<monogr>
		<title level="m">Robust recommender system: a survey and future directions</title>
		<imprint>
			<date type="published" when="2023">2023. 2023. 2024. 2024. 2023</date>
			<biblScope unit="page" from="1599" to="1608" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of SIGIR Zhao et al., 2023 A survey of large language models</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Breaking the barrier: utilizing large language models for industrial recommendation systems through an inferential knowledge graph</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.07580</idno>
	</analytic>
	<monogr>
		<title level="m">When large language models meet dynamic graph recommendation</title>
		<imprint>
			<date type="published" when="2024">2024. 2024. 2024</date>
			<biblScope unit="page" from="5086" to="5093" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Zhao et al., 2024b Proc. of CIKM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
