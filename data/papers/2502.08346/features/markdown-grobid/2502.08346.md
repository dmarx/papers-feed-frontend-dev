# Graph Foundation Models for Recommendation: A Comprehensive Survey

## Abstract

## 

Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.

## Introduction

Recommender systems are essential components of contemporary digital landscape, enabling personalized services across a diverse range of fields, including e-commerce, social media, and entertainment [[Zhang et al., 2023]](#b14). The data in RS generally consist of both structural information (e.g., useritem interactions) and textual information (e.g., user attributes and item descriptions). With the rapid development of graph learning, GNN-based methods have emerged as an important technology in RS, which can further enhance the collaborative signals of collaborative filtering and extend the signals to higher-order structures and external knowledge [[Wu et al., 2022]](#b11). However, due to the inherent structural bias, they struggle to handle textual information. This is where the powerful capabilities of large language models, which have made significant impacts in the field of natural language processing (NLP) and come into play in the realm of [RS [Yang et al., 2023;](#)[Zhai et al., 2024]](#b13). Leveraging the advanced text capabilities of LLM, these methods efficiently capture user and item textual information while integrating world knowledge for improved recommendations. However, their reasoning limitations restrict the collaborative signals they can comprehend. Inspired by the success of LLM in the NLP field, the graph domain has also been undergoing transformation, leading to the emergence of graph foundation models (GFMs) [[Liu et al., 2023b]](#). By integrating GNN and LLM technologies, GFMbased RS can efficiently utilize data to align user preferences and make more precise recommendations with minimized bias, as depicted in Figure [1](#fig_0). By appropriately integrating key information from both graph structures and text, GFM-based RS hold significant potential to emerge as a new paradigm in RS.

The GFM-based RS effectively utilize the technological complementarity of GNN and LLM. GNNs struggle to model textual information, while the reasoning capabilities of LLMs do not support their comprehension of higher-order structural information. These two technologies complement each other's shortcomings in GFM, which emerges as a future opportunity in the field of recommendations. For example, LLMGR [[Guo et al., 2024]](#b9) injects the embeddings learned by GNN into the token embedding sequence of LLM, and adapts the GFM to the recommendation task through two-stage fine-tuning.

## Graph Foundation Models for Recommendation

Graph-Augmented LLM Token-Level Infusion Syntax-Integrated Injection

LightLM [[Mei and Zhang, 2023]](#), LLMGR [[Guo et al., 2024]](#b9), TMF [[Ma et al., 2024a]](#), ELMRec [[Wang et al., 2024c]](#) Syntax-Decoupled Injection XRec [[Ma et al., 2024b]](#), COMPASS [[Qiu et al., 2024]](#) Context-Level Infusion Explicit Graph-to-Text Mapping KGRec [[Abu-Rasheed et al., 2024]](#b0), GLRec [[Wu et al., 2024a]](#), [GAL-Rec [Guan et al., 2024]](#), [HetGCoT-Rec [Jia et al., 2025]](#) Implicit Graph Retrieval CLAKG [[Chen et al., 2024]](#b1), URLLM [[Shen et al., 2024]](#b6) LLM-Augmented Graph

## Topology Augmentation

Edge Level Expansion LLMRG [[Wang et al., 2023]](#b8), LLM-KERec [[Zhao et al., 2024a]](#), [CSRec [Yang et al., 2024a]](#), [LRD [Yang et al., 2024b]](#), FineRec [[Zhang et al., 2024]](#b14), [SAGCN [Liu et al., 2023a]](#) Node Level Expansion LLM-PKG [[Wang et al., 2024b]](#), LLMRG [[Wang et al., 2023]](#b8), [CoLaKG [Cui et al., 2024]](#), CIKG [[Hu et al., 2024]](#b2),

AutoGraph [[Shan et al., 2024]](#b5), TopicKG [Jeon et al., 2024] Feature Augmentation GaCLLM [Du et al., 2024], HGNN4Rec [Damianou et al., 2024], SKarREC [Li et al., 2024], P4R [Chen and Suzumura, 2024], LIKR [Sakurai et al., 2024] LLM-Graph Harmonization Embedding Fusion LKPNR [Runfeng [et al., 2023]](#), DynLLM [[Zhao et al., 2024b]](#) Embedding Alignment LLMRec [[Wei et al., 2024]](#), RLMRec [[Ren et al., 2024]](#b4), DALR [[Peng et al., 2024]](#b3) Figure [2](#): A taxonomy of GFM-based recommender systems.

LLMRG [[Wang et al., 2023]](#b8) constructs inference graphs and divergence graphs based on user interaction history using LLM, which are then encoded by GNN for recommendations. DALR [[Peng et al., 2024]](#b3) aligns the embeddings encoded by GNN and those encoded by LLM in various ways, using the aligned embeddings for subsequent recommendations.

In this survey, we comprehensively investigate the relevant work of GFM-based RS, and provide a clear taxonomy based on the synergistic relationship between the graph and LLM in GFM: Graph-augmented LLM, LLM-augmented graph and graph-LLM harmonization. Graph-augmented LLM methods can be viewed as utilizing the structural information of the graph to aid the knowledge obtained from LLM pre-training for recommendations. LLM-augmented graph methods, on the other hand, is led by the structural information of the graph, with the world knowledge of LLM serving as auxiliary information. Graph-LLM harmonization methods involve the equal transformation of these two types of information in the representation space.

As an evergreen topic in both academia and industry, RS have been the subject of numerous surveys (e.g., [[Gao et al., 2023;](#b1)[Wu et al., 2024b;](#)[Liu et al., 2023b;](#)[Li et al., 2023]](#)). [[Gao et al., 2023;](#b1)[Wu et al., 2024b]](#) focus on specific methodologies, such as GNN-based RS or the more recent LLM-based RS. [[Li et al., 2023]](#) concentrates on utilizing LLM to enhance graphs for tackling tasks related to graphs. However, the field is rapidly evolving with GFMs emerging as a crucial technique of the RS research. [[Liu et al., 2023b]](#) systematically outlines the existing GFMs from the perspectives of pre-training and adaptation, while overlooking the recommendation which is one of the significant downstream tasks for GFM. This survey provides a timely and comprehensive overview that covers the landscape of GFM-based recommender systems.

The contributions of this survey can be summarized in the following aspects:1) Pioneering overview: Our survey fills the blank in comprehensive work in the field of GFM-based RS. 2) Clear taxonomy: The comprehensive survey presents a wellstructured taxonomy of GFM-based RS, allowing future work to be easily categorized within the corresponding branches.

3) Promising outlook: We present the challenges and future research directions in this field, which can serve as a valuable reference for research in this rapidly evolving area.

## Preliminaries

In this section, we first introduce the basic concepts of GNNbased RS and LLM-based RS, then we provide the definition of graph foundation models, and finally we introduce the proposed taxonomy.

## GNN/LLM-based Recommender Systems

As data grows explosively, recommender systems have emerged [[Gao et al., 2023]](#b1). There are generally three types of data in RS: user data, item data, and user-item interaction data. These data not only contain strong structural information that requires graph representation, but also are rich in textual descriptions. Given the characteristics of the data in RS, they can essentially be abstracted into text-attribute graphs [[Jin et al., 2024]](#). As two recently popular approaches, GNN-based RS and LLM-based RS both exhibit certain limitations in capitalizing on the information presented in the graphs. GNN-based RS excel at capturing complex higherorder relationships between nodes and model user preferences based on multi-hop neighbors, thus providing accurate recommendations [[Wu et al., 2020;](#b10)[Wang et al., 2021]](#b7). However, the sequential order and semantic meaning of words within node descriptions pose a challenge for representation with graph structures, rendering these methods less effective at handling textual information. Conversely, LLM-based RS, with their robust contextual understanding and world knowledge, excel at processing textual descriptions [[Yang et al., 2023;](#)[Zhai et al., 2024]](#b13). However, limitations in their sequential modeling designs and reasoning capabilities cause these methods to struggle with managing complex relationships.

## Graph Foundation Models

The field of NLP has witnessed a revolution influenced by the Transformer architecture. Pre-trained language models based on the architecture have demonstrated formidable capabilities [[Radford et al., 2019;](#b3)[Kenton and Toutanova, 2019]](#). These language foundation models, pre-trained on vast amounts of text, possess impressive generalization abilities that can adapt to a wide array of tasks [[Bommasani et al., 2021]](#b0). When the scale of the language foundation model reaches a certain magnitude, it is referred to as the LLM [[Zhao et al., 2023]](#). Inspired by the success of language foundation models in the NLP field, the field of graph learning also recognized the need to improve model performance and generalization capabilities through pre-training. This gave rise to the concept of graph foundation models [[Liu et al., 2023b]](#), which are models that are pre-trained on large datasets and incorporate graph structures to solve graph-related tasks. As combinations of LLM and graphs, GFMs acquire emergence and homogenization during pre-training [[Liu et al., 2023b]](#), enabling them to adapt seamlessly and perform impressively across a variety of downstream tasks.

## Taxonomy of GFM-based RS

Gravitating towards the contextual backdrop of RS, we place substantial emphasis on examining the organic integration of graph with LLM in GFM, striving for more precise and user-specific recommendations. In accordance with the interrelationship between graphs and LLMs (as illustrated in Figure [2](#)) we group the related works into three principal categories: Graph-augmented LLM, where the structural information from graphs is injected into LLMs to enhance the reasoning and generation capabilities for recommendation; LLM-augmented graph, where the structural information (e.g., topological structure) or textual information (e.g., user profiles and item descriptions) in the graph is enhanced with the aid of LLMs; LLM-graph harmonization, where the semantic embedding in LLMs and the structural embedding in graphs are combined seamlessly to achieve mutual optimization and maximize recommendation performance.

In the following sections, we provide a comprehensive introduction and discussion of the three main categories in the taxonomy of GFM-based RS.

3 Graph-Augmented LLM LLMs excel at understanding and generating text but struggle with the complex relational structures inherent in recommender systems. While pre-training corpora and in-context learning provide some information, they lack an explicit mechanism to model the intricate relationships between users and items that are naturally represented as graphs. Recent research bridges this gap by integrating graphs with LLMs, focusing on the core challenge: How to design cross-modal interfaces that effectively bridge graph structures to language models? We categorize current methods into token-level infusion and context-level infusion (as illustrated in Figure [3](#fig_1)), based on where the cross-modal interface is implemented. 

## Token-Level Infusion

This strategy integrates structural information directly into the LLM's input at the token level. Nodes or subgraphs are represented as special tokens, allowing the LLM to process structural information alongside text.

Syntax-Integrated Injection. This approach embeds special tokens as syntactic components within the LLM's input sequence. For example, TMF [[Ma et al., 2024a]](#) introduces

$[ACTION] tokens like [view] or [purchase]$to represent user actions within an interaction sequence. The embeddings for these actions are learned from a multi-behavior graph, enabling the LLM to process complex semantics like "user [views] item". Building on this, ELMRec [[Wang et al., 2024c]](#) generates a GCN-based embedding h i for each item i and adds it as a correction term to the item's original text embedding e i , resulting in a refined embedding e ′ i = e i + h i . This operation blends textual and structural information, improving item representation.

A natural progression from here is to consider whether the LLM can directly output these special tokens. LLMGR [[Guo et al., 2024]](#b9) explores this by not only including special tokens in the input but also modifying the LLM's output layer. They introduce a special token for each item, allowing the model to directly generate item tokens as recommendations, effectively creating a tighter link between graph-based recommendations and the LLM's output. Further advancing this line of thought, LightLM [Mei and [Zhang, 2023]](#) proposes a hierarchical indexing scheme, which decomposes user/item IDs into multiple special tokens based on a user-item graph-derived index. Each component token encodes a different attribute or function, moving away from opaque numerical IDs towards more semantically meaningful representations.

Syntax-Decoupled Injection. This method appends graph embeddings as prefixes or suffixes, separating structural information from the main textual prompt. XRec [[Ma et al., 2024b]](#) prepends GNN-learned embeddings that represent user-item relationships to the prompt. These embeddings are trained to capture high-level semantic concepts, such as preference similarity between users. COMPASS [[Qiu et al., 2024]](#) com-bines user queries with knowledge graph embeddings. A GNN generates embeddings that encapsulate both the user's query and relevant knowledge graph entities, which are then used as a prefix to guide the LLM for recommendation.

Token-level infusion offers a fine-grained way to integrate structural information, allowing for natural interactions between textual and structural data. However, it often requires modifications to the LLM's architecture or careful prompt engineering.

## Context-Level Infusion

This strategy provides structural information as context to the LLM, either through text descriptions or implicit retrieval, avoiding modifications to the LLM's architecture.

Explicit Graph-to-Text Mapping. This method involves converting localized graph structures into natural language descriptions, essentially translating graph relationships into text.

The following example illustrates explicit graph-to-text mapping: user A → purchase → item B → payment → credit card ⇒ A purchased B with a credit card. The simplest form of this is exemplified by [HetGCoT-Rec [Jia et al., 2025]](#). They extract multihop neighbors of a target node from a heterogeneous graph and concatenate their attributes to create a natural language description, which becomes the context for the LLM and informs its recommendations. Similarly, KGRec [[Abu-Rasheed et al., 2024]](#b0) extracts one-hop and two-hop related nodes from a knowledge graph and inserts them into predefined prompt templates, which guides the LLM in generating explainable recommendations.

Further refinements involve pre-processing the structural information before mapping it to text. GAL-Rec [Guan et al., 2024] maintains a dynamic queue of negative samples, items the user is known to dislike, based on knowledge graph insights and LLM feedback. Providing both positive and negative samples as context for the LLM allows for more nuanced recommendations. The processing of structural information can also occur after its initial conversion to text. GLRec [[Wu et al., 2024a]](#) constructs natural language descriptions of node paths in a job information graph. Each path represents a sequence of related job attributes or skills. During prompt construction, these paths are assigned different weights based on relevance and dependency strength, offering a more refined, contextually rich input to the LLM.

Implicit Graph Retrieval. When explicit mapping is difficult, this approach uses GNN embeddings to retrieve relevant information from the graph semantically. For example, CLAKG [Chen et al., 2024] encodes a legal knowledge graph using a GNN, and retrieve relevant legal provisions based on the similarity between a user's case description embedding and the provision embeddings. These provisions are then concatenated into the prompt context. URLLM [[Shen et al., 2024]](#b6) retrieves a user's historical interactions from an item-attribute graph, focusing on neighbor interactions. These records are added to the prompt, providing the LLM with cross-domain preference information.

Context-level infusion provides a flexible way to incorporate graph knowledge without altering the LLM's architecture. It leverages the LLM's ability to understand and reason over natural language, making it suitable for scenarios where graph structures can be effectively verbalized.

## Discussion

Graph-augmented LLM methods enhance recommendations by encoding rich relational information from graphs, typically through token-level or context-level infusion. This couples the benefits of graph-structured data with the power of LLMs: the graph provides valuable relational context, while the LLM leverages its pre-trained knowledge to interpret it. Furthermore, since the LLM is the central component, this approach augments the recommender system's ability to extrapolate and make inferences in scenarios where interaction data is limited.

However, this heavy reliance on the LLM also introduces inherent biases unsuitable for recommendation, such as a lack of diversity and distributional mismatch with user preferences, potentially limiting its scalability and generalizability. The alternative methods, by shifting the focus to enriching or harmonizing the graph itself, effectively mitigate these issues and offer different trade-offs.

## LLM-Augmented Graph

Shifting the focus to the graph, the core idea of the LLMaugmented graph methods is to augment the data within graphs using LLM, thereby improving the effectiveness of various GNNs employed for recommendation tasks. Such methods can be categorized into topology augmentation and feature augmentation (as illustrated in Figure [4](#fig_2)), based on the aspects of information enhanced in the text-attribute graph according to LLMs.

## Topology Augmentation

Topology augmentation refers to the processes where the LLM restructures data, utilizing its world knowledge and contextual understanding capabilities to convert specific textual information into a structured format. Due to the introduction of new structural information, the topological structure of the graph constructed from the data is modified, thereby affecting the subsequent processes to achieve more accurate recommendations. Intuitively, we categorize topology augmentation into two types: edge-level expansion and node-level expansion, based on whether new nodes are introduced.

Edge-level Expansion. This method refers to the process where LLMs introduce new relationships between nodes in the data, such as complementary and substitutable relationships, which are two primary types of relationships of interest in RS.

To directly utilize the versatile capabilities of LLMs and the vast world knowledge, it is intuitive to adopt text-centric approaches for adding edges in the graph. These approaches typically rely on prior knowledge to guide LLMs in making relationship judgments and constructions, either at a superficial or deeper level. As a straightforward example, LLM-KERec [[Zhao et al., 2024a]](#) employs LLMs to assess the complementarity between pairs of items, thereby establishing complementary relationships among item pairs and constructing a complementary item graph. From a deeper perspective, in addition to leveraging complementary relationships between items, subjective user-generated reviews can also be utilized. [SAGCN [Liu et al., 2023a]](#) and FineRec [[Zhang et al., 2024]](#b14) utilize LLMs to extract user opinions on items at varying levels of granularity across multiple item attributes (e.g., price, comfort, etc.), using this information as edges to construct distinct graphs for each attribute.

Compared to the aforementioned methods, the more refined expansion delves into the relationships within the embedding space, subsequently constructing graphs based on implicit relationships between nodes at a certain level. For example, CSRec [[Yang et al., 2024a]](#) first employs an LLM to generate complementary or substitutable category nodes based on existing classified nodes, and then utilizes other pre-trained language models to map the pairs of newly generated nodes and existing nodes into the node set within the semantic space. The relationships generated by the LLM are also mapped into the node set within the embedding space. In addition to mapping edges based on semantic similarity, connections can also be established based on the similarity. Several works [[Yang et al., 2024b;](#)[Cui et al., 2024]](#) employ an LLM to encode the textual information of nodes into embeddings, and then measure potential relationships between these embeddings through carefully designed methods. These potential relationships serve as edges for item nodes in the graph within the embedding space.

Node-level Expansion. This method further leverages the world knowledge and contextual reasoning capabilities of LLMs, utilizing auxiliary information as new nodes to augment the information of existing nodes. Such method often directly utilizes the condensed information generated by LLM as new nodes to be introduced into the graph, or extracts the requisite information from the LLM's output through particular approaches to serve as new nodes in the graph.

Several works [[Jeon et al., 2024;](#b2)[Hu et al., 2024]](#b2) directly employ LLMs to generate auxiliary information nodes (e.g., user interests, item categories) for corresponding users or items based on existing textual information. Introducing such auxiliary information nodes can be viewed as a supplementation of information in the textual space. This approach can, to some extent, assist subsequent GNN in modeling better user or item representations. Furthermore, this type of information supplementation can also be performed in the embedding space. For example, AutoGraph [[Shan et al., 2024]](#b5) utilizes LLMs to encode the textual information of users and items, followed by quantizing the semantic embeddings of users and items. By quantizing these semantic embeddings, fine-grained auxiliary information embeddings for users and items can be derived, which are then used as new nodes to supplement information for users and items.

Topology augmentation represents effectively utilizing the knowledge obtained from LLM pre-training to influence the training of the GNN. The key structural information introduced into the graph during this process enables subsequent GNN to learn more comprehensive representations of users and items.

## Feature Augmentation

Enhancing the topological structure of the graph using LLMs may introduce biases, as they are not particularly adept at extracting structural information from text. In contrast, directly improving the node features in the graph without altering the topological structure is an task where LLMs truly excel. This method focuses on leveraging the natural language processing capabilities of LLMs to augment data at the textual or embedding level, thereby influencing subsequent recommendations.

Some works [[Li et al., 2024;](#)[Chen and Suzumura, 2024](#)] enhance the textual information of nodes by constructing appropriate prompts for input into LLMs. The enhanced textual information is subsequently encoded into embeddings by language models such as BERT. Unlike the aforementioned methods, GaCLLM [[Du et al., 2024]](#) integrates the stages of LLM and GNN. In this approach, the LLM assumes the role responsible for message passing within the GNN, performing textual message passing and aggregation for each node in the graph, which is subsequently encoded by BERT. As a special case, LIKR [[Sakurai et al., 2024]](#) employs LLMs to analyze user interaction histories to derive user-preferred item attributes. The nodes corresponding to these attributes in the graph serve as rewards for Markov walk-based reinforcement learning within the graph.

The textual information of users and items, being one of the abundant types of information in RS, significantly impacts the performance of RS when effective utilized. Consequently, LLM is increasingly becoming the preferred technology for feature augmentation of graphs based on textual information.

## Discussion

The LLM-augmented graph methods, by incorporating the world knowledge of LLM into graph data, can enhance the capabilities of RS at the data level. This makes these methods more competitive in cold start scenarios with sparse interactions. Furthermore, the LLM in these methods is a plug-andplay component, allowing for flexible choices. Moreover, the data processing of the LLM can be performed offline in advance, and online recommendations based on statistical rules or neural models (e.g., GNN) can be made afterwards, significantly reducing time consumption. This has led to the increasing popularity of these methods in industry. However, these methods also have some drawbacks. First, graph learning methods do not fully exploit the world knowledge learned and utilized by LLM, which is a natural shortcoming of such plugand-play methods. And LLMs have the potential to introduce extraneous noise in both dimensions (topology and feature) of topology augmentation, which may consequently give rise to certain biases. Furthermore, they have poor scalability, as the main body of these methods are shallow GNNs whose model depth is affected by the over-smoothing problem.

## LLM-Graph Harmonization

Graph-augmented LLM methods leverage external graph structures to enhance LLMs but often suffer from inefficiencies in real-time adaptability and increased computational overhead. Conversely, LLM-augmented graph methods attempt to incorporate LLMs into graph-based learning but struggle with scalability and effective knowledge utilization. To address these limitations, this section introduces a novel framework that optimally balances computational efficiency, adaptability, and reasoning capabilities. LLMs excel in capturing rich semantic information from unstructured textual data (e.g., item descriptions and user attributes), while GNNs are adept at modeling the topological structure of graphs (e.g., user-item interactions, social connections). As shown in Figure [5](#fig_3), harmonizing these two paradigms effectively can significantly enhance recommendation performance. Existing methods can be broadly categorized into two mainstream strategies: embedding fusion and embedding alignment, based on transformations of embeddings.

## Embedding Fusion

The embedding fusion approach aims to combine LLMderived textual representations with graph-learned structural embeddings, creating a unified feature space that leverages complementary information. This strategy emphasizes the synergy between textual semantics and graph-based connectivity.

A notable framework in this domain is DynLLM [[Zhao et al., 2024b]](#), which incorporates graph structures into LLMs through dynamic memory-enhanced fusion. DynLLM addresses the limitation of static embeddings by using a dualflow interaction mechanism: one flow learns from GNNupdated dynamic embeddings reflecting user-item interactions, while the other adapts LLM-generated embeddings based on real-time textual content. These embeddings are fused in a shared latent space, enhancing both semantic and structural understanding. Such a dynamic fusion approach not only captures the temporal evolution of recommendation data but also integrates high-quality textual semantics, leading to more accurate and adaptive recommendations. Another example is LKPNR [[Runfeng et al., 2023]](#b4), which combines LLMs with knowledge graphs to enhance personalized news recommendation. LKPNR leverages the semantic richness of LLMs to generate high-quality news representations and uses knowledge graphs to capture the relational structure of news entities. By integrating these modalities, LKPNR effectively addresses the long-tail problem in news recommendation.

The embedding fusion paradigm is particularly effective because it exploits the contextual richness of LLMs alongside the relational structures captured by GNNs. By fusing these modalities, models like DynLLM and LKPNR enable direct, dynamic, and efficient utilization of both textual and graph data, significantly improving representation learning in recommendation scenarios.

## Embedding Alignment

Embedding alignment takes a different route by focusing on reconciling the heterogeneity between LLM-generated textual embeddings and GNN-learned structural representations. This strategy ensures that embeddings from both modalities can operate coherently within a unified representational space, reducing information loss and noise. The refined embeddings can provide more valuable information for recommendations.

The DALR framework [[Peng et al., 2024]](#b3) serves as a representative example, where structural embeddings from GNNs (e.g., user-item graph representations) and semantic embeddings from LLMs (e.g., product descriptions, user reviews) are aligned through contrastive learning paradigms. This alignment mitigates the semantic gaps and noise introduced by the inherently different data sources. Similarly, methods such as LLMRec [[Wei et al., 2024]](#) and RLMRec [[Ren et al., 2024]](#b4) also adopt multimodal alignment techniques, such as contrastive learning and MLP-based alignment, to unify embeddings from diverse modalities. For instance, LLMRec employs a denoised data robustification mechanism to enhance the reliability of augmented recommendation data, while RLMRec leverages contrastive alignment strategies to bridge the semantic space of LLMs with the collaborative relational signals from GNNs, thereby improving the overall quality of the learned representations.

Embedding alignment excels in its ability to unify heterogeneous modalities, ensuring consistent representation learning that facilitates better downstream recommendation tasks, such as personalized ranking or user preference clustering.

## Discussion

The two approaches offer distinct advantages in integrating LLMs with graph learning for recommender systems. Embedding fusion directly combines textual semantics and structural relationships, leveraging their complementarity to enhance representation learning and personalization. Dynamic fusion methods, such as DynLLM, further enable real-time adaptation to evolving user preferences. Embedding alignment, in contrast, ensures coherence between textual and structural embeddings by mapping them into a shared space, mitigating inconsistencies. Methods like DALR leverages various contrastive learning approaches to enhance alignment robustness. However, embedding fusion may introduce redundant or conflicting information, and increase computational costs, while embedding alignment which is sensitive to noise depends on high-quality training data. In these methods, the LLM primarily serves as an encoder, and due to the constraints of the scenario, it cannot fully utilize its contextual understanding and language generation capabilities.

## Challenges and Future Directions

GFMs have demonstrated great potential in recommender systems by incorporating graph structural information with external world knowledge of LLM. However, several challenges hinder their widespread adoption and effectiveness.

High Computational Cost and Scalability Issues. Existing GFM-based RS require substantial computational resources, posing challenges for large-scale deployment [[Zhai et al., 2024]](#b13). The integration of graph-based reasoning and LLM inference results in high memory consumption and slow inference speed, particularly when processing dense user-item graphs or generating personalized recommendations in realtime [[Wang et al., 2024a]](#). Unlike traditional recommendation models, which can be efficiently pruned or quantized, GFMs face unique scalability constraints due to their reliance on long-range graph dependencies and LLM-generated representations. Addressing these limitations requires advancements in efficient model compression strategies tailored for graphenhanced LLMs, and adaptive graph sparsification techniques to maintain performance while reducing overhead.

Robustness Against Noisy and Adversarial Data. Realworld user interactions are inherently noisy, exhibiting shortterm fluctuations, incomplete preferences, and adversarial perturbations [[Zhang et al., 2023]](#b14). Traditional recommendation models rely on explicit feedback signals, making them susceptible to biased or manipulated data. In contrast, GFMs integrate graph-based user-item relationships and LLMgenerated contextual representations, which introduces additional sources of noise from both structured and unstructured data. Ensuring robustness requires advancements in self-supervised denoising techniques, adversarial training tailored for multimodal representations, and uncertainty-aware modeling to mitigate the impact of unreliable signals while preserving recommendation accuracy.

Multi-Modal Information Fusion. Modern recommendation scenarios involve a diverse range of data modalities, including text, structured graphs, images, audio, and video [[Tao et al., 2020]](#b6). While existing GFMs primarily focus on textual and structural embeddings, effectively incorporating rich multi-modal signals remains an open challenge. Different modalities exhibit varying levels of granularity, semantic gaps, and computational costs, making seamless integration nontrivial. Future research should explore adaptive fusion frameworks, cross-modal alignment mechanisms, and lightweight multi-modal representation learning to balance efficiency and accuracy in large-scale recommender systems. Lack of End-to-End Optimization. The concept of endto-end recommender system is not unfamiliar. When deep learning was introduced into the field of recommendation, a process encapsulation was essentially performed [[Covington et al., 2016]](#b1). However, the early neural model RS have gradually fallen behind the times. The process of such RS can be roughly divided into three stages: matching, ranking, and reranking [[Gao et al., 2023]](#b1). In reality, this process is often more refined in industrial applications. Such a meticulous process naturally results in better recommendation performance. However, multi-stage model optimization requires a significant investment of time and manpower. Contrarily, an end-to-end generative RS, different from the one mentioned above, encapsulates multiple stages together for optimization. This significantly reduces complexity and can potentially lead to better performance. Gradually, similar endeavors are being pursued in the industrial field. HSTU [[Zhai et al., 2024]](#b13) simplifies the internal structure of the LLM and fully implements it through serial modeling. Moreover, [[Wang et al., 2024d]](#) takes into account both structural and textual information. Such an integrated generative recommendation that combines matching and ranking may likely be a hotspot in the future. Knowledge-Preference Gap. While GFMs leverage external knowledge to alleviate data sparsity, a fundamental misalignment persists between globally pre-trained world knowledge and personalized user preferences [[Wang et al., 2024a]](#). Unlike embedding alignment, which focuses on bridging modality gaps (e.g., between graph structures and textual representations), this discrepancy stems from differences in how LLMs interpret knowledge and how users express preferences. For instance, LLM-based recommendation models may naturally generate factually coherent but overly neutral item descriptions, whereas users often respond more favorably to engaging or sensationalized content (e.g., "Shocking! You won't believe this..."). Addressing this challenge requires advancing preference-aware knowledge adaptation, dynamic refinement techniques, and contrastive learning strategies tailored to user-specific interests.

## Conclusion

As an indispensable technology in modern society, recommender systems stand as one of the most prominent research areas within the field of artificial intelligence. The emergence of graph foundation models is likely to spark a new wave of research enthusiasm in the field of RS. In this survey, we present the first comprehensive overview of GFM-based RS and propose a logically organized taxonomy. Furthermore, we delve deeply into the challenges and vast potential of this field, aiming to inject new vitality into its research endeavors.

![Figure 1: An overview of GFM-based RS. Compared with GNNbased or LLM-based RS, GFM-based RS are positioned as integrating both approaches to create more comprehensive recommendations.]()

![Figure 3: The illustration of graph-augmented LLM methods: a) Token-Level Infusion, where nodes or subgraphs are represented as special tokens, integrating into LLM's input. b) Context-Level Infusion, where graph information is converted into context by translating graph into text or retrieving relevant text.]()

![Figure 4: The of LLM-augmented graph methods: a) Topology Augmentation, where LLMs extract structural information from data to alter and augment the topological structure of the graphs.; b) Feature Augmentation, where LLMs processe the textual information in the data, augment the node text or embedding in the graph without changing the topological structure.]()

![Figure 5: The illustration of LLM-graph harmonization methods: a) Embedding Fusion, where LLM-derived semantic embeddings and GNN-learned structural embeddings are combined into a unified representation space through fusion mechanisms such as concatenation or attention-based integration; b) Embedding Alignment, where embeddings from both modalities are mapped into a shared space using techniques like contrastive learning or MLP-based transformation to enhance consistency and coherence.]()

