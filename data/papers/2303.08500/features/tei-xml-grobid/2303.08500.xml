<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Devil&apos;s Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models</title>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
				<funder ref="#_Pr9jQRr">
					<orgName type="full">ARC Centre of Excellence for Automated Decision-Making and Society</orgName>
				</funder>
				<funder ref="#_zNm55Jc #_BPajFHm">
					<orgName type="full">Australian Research Council</orgName>
					<orgName type="abbreviated">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-01-11">11 Jan 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hadi</forename><forename type="middle">M</forename><surname>Dolatabadi</surname></persName>
							<email>h.dolatabadi@unimelb.edu.au</email>
							<idno type="ORCID">0000-0001-9418-1487</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
							<email>sarah.erfani@unimelb.edu.au</email>
							<idno type="ORCID">0000-0003-0885-0643</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Leckie</surname></persName>
							<email>caleckie@unimelb.edu.au</email>
							<idno type="ORCID">0000-0002-4388-0517</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Devil&apos;s Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-11">11 Jan 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">45BCF51114D9D11AB90A610611F71ED0</idno>
					<idno type="arXiv">arXiv:2303.08500v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>availability attacks</term>
					<term>diffusion models</term>
					<term>facial recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Protecting personal data against exploitation of machine learning models is crucial. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data "unexploitable." This paper provides a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can counteract the effectiveness of the dataprotecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-ofthe-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training even under distribution mismatch between the diffusion model and the protected data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Neural networks have achieved great success in various areas of computer vision including object detection <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref>, semantic segmentation <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b35">36]</ref>, and photo-realistic image/video generation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b53">54]</ref>. While the efforts of the community in the development of such models cannot be undermined, this unparalleled success would have been impossible without the abundance of data resources available today <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b34">35]</ref>. In this regard, social media, and the internet in general, provides a platform that can be crawled easily to create massive datasets. This capability can act both as a blessing and a curse: while the collected data can facilitate learning larger, more accurate neural networks, the users lose control over protecting their personal data from being exploited. This issue has raised increasing concerns about misuse of personal data <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Recently, there has been an increasing number of studies on hindering the unauthorized use of personal data for neural network image classifiers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>These methods tend to add an imperceptible amount of noise to the clean images so that while the data has the same appearance as the ground-truth, it cannot provide any meaningful patterns for the neural networks to learn. As a result, such approaches, collectively known as availability attacks <ref type="bibr" target="#b3">[4]</ref>, claim that personal image data can be made unexploitable for the neural networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b70">71]</ref>. While there has been an abundance of research on designing better availability attacks, far too little attention has been paid to counter-attacks that might be employed by adversaries to break such precautionary measures.</p><p>Unfortunately, the assumptions of existing availability attacks are far too weak to make the data unexploitable. For example, consider a user who shares their protected photos over their social media. We can clearly see that once the photos are shared, they cannot be protected against all future countermeasures <ref type="bibr" target="#b46">[47]</ref>. For instance, consider a corporate entity that aims to train face recognition models by crawling over social media without the consent of the users. While this unauthorized entity might not have unprotected versions of a particular person's image from his/her social media, they can have a large pre-trained model representing a facial image distribution. Given this threat model, shown in Figure <ref type="figure">1</ref>, we aim to show that counteracting the protecting perturbations is indeed plausible.</p><p>To this end, we show that pre-trained density estimators are powerful tools that can be used to counteract the effects of the data-protecting perturbations, eventually enabling us to exploit protected data. We utilize the power of diffusion models in representing the image data distributions to show that reverse-engineering unexploitable data is easier than what is thought. In particular, given a training dataset, we first diffuse the images by adding a controlled amount of Gaussian noise following the forward process of a pre-trained diffusion model. Then, we denoise the noisy images using the reverse process of the aforementioned model, resulting in a dataset purified from data-protecting perturbations. Theoretically, using contraction properties of stochastic difference equations we prove that the number of diffusion steps required to cancel the data-protecting perturbations is directly influenced by the</p><note type="other">Large Dataset User Data Protection Diffusion Model</note><p>Protection Defuser Unauthorized Facial Recognition Evil Entity Fig. <ref type="figure">1:</ref> The threat model considered in this paper. Availability attacks cannot guarantee to protect all the data that exists over the web. A data exploiter might use large density estimators to defuse the data-protecting perturbations and exploit the data. magnitude of its norm. Thus, protecting personal data using imperceptible perturbations is not possible. We also empirically show that our approach is surprisingly powerful, being able to deliver the state-of-the-art (SOTA) performance against a wide variety of recent availability attacks. Our findings indicate the fragility of unexploitable data, calling for more research to protect personal data.</p><p>Diffusion models have been extensively used in various areas. Closely related to our work, Yoon et al. <ref type="bibr" target="#b69">[70]</ref> and Nie et al. <ref type="bibr" target="#b41">[42]</ref> have employed diffusion models to increase robustness against adversarial attacks. In contrast to these methods, in this paper, we investigate the capabilities of diffusion models as a threat against personal data protected by availability attacks. In particular, we leverage the SOTA diffusion models as a proxy for the true data distribution and argue why unlearnable examples provide a false sense of data privacy.</p><p>Our contributions can be summarized as follows:</p><p>• We introduce AVATAR as a countermeasure against data availability attacks. To the best of our knowledge, this is the first work that explores the use of diffusion models to circumvent such attacks. • We show the power of AVATAR in breaking availability attacks over five datasets, four architectures, and seven of the most recent availability attacks. AVATAR achieves the SOTA performance against availability attacks, outperforming adversarial training. • Our results indicate that even in the absence of the true data distribution, one can use a similar distribution to counteract availability attacks. • Theoretically, we show that the amount of noise needed to diffuse the data-protecting perturbation is directly related to the magnitude of its norm. This result indicates that achieving both goals of availability attacks (data utility and protection) at the same time is impossible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review the related work to our approach. a) Poisoning and Backdoor Attacks: A considerable number of studies have been published on various types of data poisoning attacks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b19">20]</ref>. These attacks aim to pollute the training data so that they can hinder the performance of the machine learning model at test-time <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>. While these methods are quite successful in achieving this goal, they often tend to perform weakly against neural networks <ref type="bibr" target="#b39">[40]</ref> and appear to be distinguishable from the clean samples, damaging the utility of the underlying data <ref type="bibr" target="#b67">[68]</ref>. Backdoor attacks are a popular family of data poisonings against deep neural networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b11">12]</ref>. Unlike general poisoning attacks, these methods attach triggers to a small fraction of the clean training data so that the model creates an association between the existence of the trigger and a particular class. During inference, the neural network would behave normally on benign samples. However, if the trigger is activated, the model would output the attacker's desired value due to the existence of a backdoor in the model.</p><p>b) Availability Attacks: Motivated to address the lack of personal data privacy, an emerging type of poisoning attacks known as availability attacks have drawn considerable attention. Unlike previous types of poisoning attacks, availability attacks seek to add imperceptible perturbations to the clean training data with two goals in mind. First, the added perturbation should be able to protect the underlying data from being exploited by a neural network during training. Second, the perturbed data should still preserve its normal utility. To understand these constraints, consider a user sharing their photo over their social media. While the user wants to protect their photo from unauthorized use of web-crawlers to train a face recognition model <ref type="bibr" target="#b26">[27]</ref> (first constraint), they still wish their photo to appear normal to their audience (second constraint) <ref type="bibr" target="#b29">[30]</ref>.</p><p>Feng et al. <ref type="bibr" target="#b14">[15]</ref> propose to produce the poisoning perturbations by training an auto-encoder, whose aim is to get the lowest performance from an auxiliary classifier. In a similar spirit, Tian et al. <ref type="bibr" target="#b61">[62]</ref> train a conditional generative adversarial network (GAN) <ref type="bibr" target="#b20">[21]</ref> to generate the availability attacks' perturbation. The training objective is designed to create a spurious correlation between the noisy image and the ground-truth labels. Concurrently, Yu et al. <ref type="bibr" target="#b70">[71]</ref> empirically investigate various types of availability attacks and show that almost all of them leverage these spurious features to create a shortcut within neural networks <ref type="bibr" target="#b18">[19]</ref>. Yu et al. <ref type="bibr" target="#b70">[71]</ref> then propose a fast and scalable approach for perturbation generation by generating randomly-initialized linearly-separable perturbations which can generate availability attacks for an entire dataset in a few seconds. Concurrently, Sandoval-Segura et al. <ref type="bibr" target="#b49">[50]</ref> proposed another approach that generates the random noise independent from the data. In this approach, first the beginning rows and columns of each channel are populated with Gaussian noise. Then, an autoregressive process is used to find the value of the remaining pixel values.</p><p>Another popular approach to generate availability attacks is via direct optimization. Huang et al. <ref type="bibr" target="#b29">[30]</ref> define a bi-level optimization objective to generate error-minimizing noise for data samples and an auxiliary classifier. It is argued that since the perturbed images minimize the auxiliary classifier's loss, they contain no useful information for any other target classifier to learn, and as such, the model would not exploit them during training. In contrast, Fowl et al. <ref type="bibr" target="#b16">[17]</ref> show that using adversarial examples <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b21">22]</ref> as the poisoned data would make it hard for the classifier to learn any meaningful pattern, and thus, they can serve as a powerful family of availability attacks. While optimization-based availability attacks are potent, they are often computationally demanding and several attempts have been made to ease their computational burden <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b75">76]</ref>.</p><p>Compared to various types of availability attacks, preventative measures have received little attention. It has been shown that various data augmentation techniques (such as CutOut <ref type="bibr" target="#b9">[10]</ref>, Mixup <ref type="bibr" target="#b74">[75]</ref>, CutMix <ref type="bibr" target="#b72">[73]</ref>, and Fast Autoaugment <ref type="bibr" target="#b33">[34]</ref>) are not able to prevent availability attacks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b70">71]</ref>. Tao et al. <ref type="bibr" target="#b60">[61]</ref> show that adversarial training <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b12">13]</ref>, originally proposed to enhance robustness against adversarial attacks <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b21">22]</ref>, can be used to train successful classifiers against availability attacks. Later, Fu et al. <ref type="bibr" target="#b17">[18]</ref> extended the error-minimizing noise of Huang et al. <ref type="bibr" target="#b29">[30]</ref> resulting in perturbations that can even prevent adversarial training from learning over the poisoned data. Despite this, adversarial training has remained one of the strongest defense baselines against availability attacks. In this work, we show that one can outperform adversarial training in an attempt to counteract availability attacks. c) Diffusion Models: Denoising diffusion probabilistic modeling (DDPM) <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b27">28]</ref> (also known as score-matching networks <ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref>) are a family of deep generative models that have achieved the SOTA performance in image <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b66">67]</ref>, text-to-image <ref type="bibr" target="#b47">[48]</ref>, video <ref type="bibr" target="#b53">[54]</ref>, and 3D-object <ref type="bibr" target="#b45">[46]</ref> generation. Diffusion models generally comprise of a forward and a backward process <ref type="bibr" target="#b7">[8]</ref>. In the forward process, the model gradually adds noise to the data until it is transformed into Gaussian noise. The backward process is the reverse of the forward process, where the model tries to gradually transform/denoise a Gaussian vector into a data point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>This section formally introduces our proposed method, called AVATAR (dAta aVailAbiliTy Attacks defuseR). First, we define our notation and problem settings. Next, we introduce our proposed approach that materializes our threat model and provide a theoretical analysis of our framework. Finally, we discuss the potential advantages of AVATAR compared to existing methods such as adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Statement</head><p>Let D = {(x (i) , y (i) )} n i=1 be a labeled dataset consisting of n i.i.d. samples x (i) each with a label y (i) . Without loss of generality, in this paper, we consider image data x (i) ∈ R d where d shows the data dimension. Also, we assume that y (i) takes one of the K possible class values {1, 2, . . . , K}. Furthermore, let f θ : R d → R K denote a neural network classifier parameterized by θ that takes an image x and outputs a realvalued vector z = f θ (x) known as the logit. The final decision of the classifier is determined via ŷ = arg max j z j . To train the classifier, one usually aims to minimize the empirical error between the ground-truth labels and the classifier predictions:</p><formula xml:id="formula_0">arg min θ E (x,y)∈D [ℓ(f θ (x), y)],<label>(1)</label></formula><p>where ℓ(•) denotes the cross-entropy loss. Following the convention in availability attacks, we assume that there exists a data curator that manipulates the dataset D into D pr = {(x (i) , y (i) )} n i=1 such that once a neural network is trained over D pr , it performs poorly over the clean data D:</p><formula xml:id="formula_1">arg max Dpr E (x,y)∈D [ℓ(f θ * (x), y)] s.t. θ * = arg min θ E (x,y)∈Dpr [ℓ(f θ (x), y)].<label>(2)</label></formula><p>Since each image x(i) needs to maintain its normal utility, it is assumed that x(i) = x (i) + δ (i) . Here, δ (i) 's are the dataprotecting perturbations such that δ (i) p ≤ ε, where ∥•∥ p denotes the L p norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. dAta aVailAbiliTy Attacks defuseR (AVATAR)</head><p>As discussed, large pre-trained generative models can pose a threat to availability attacks and personal data protection. In this section, we show how diffusion models, which are the SOTA in image generation, can be leveraged to cancel out the effects of availability attacks.</p><p>Recall that availability attacks provide a manipulated version of the original data x that is seemingly unexploitable. At the same time, the protected image x = x + δ should have its normal utility as it is going to be used by the users, e.g., to post over their social media. This condition reflects itself through the constraint that ∥δ∥ p ≤ ε.</p><p>A trivial idea would be to add random noise to the protected perturbation that might counteract the perturbation, but this is detrimental/ineffective in removing the unlearnable effect <ref type="bibr" target="#b29">[30]</ref>.</p><p>As such, we propose to use a diffusion model for denoising as outlined next. <ref type="foot" target="#foot_0">1</ref>Specifically, let us assume that we have a pre-trained DDPM <ref type="bibr" target="#b27">[28]</ref> model that represents the data distribution x 0 ∼ p data (x). The forward process of this model is represented using a Markov chain of length T , such that:</p><formula xml:id="formula_2">xt = 1 -βtxt-1 + βtϵt,<label>(3)</label></formula><p>where ϵ t ∼ N (0, I) is the normal distribution, and t = 1, 2, . . . , T . The constants β t , known as variance schedules, are selected such that x T ∼ N (0, I). If we set α t := t s=1 (1 -β s ), then this Markov process can also be performed via a single step <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_3">xt = √ αtx0 + √ 1 -αtϵ.<label>(4)</label></formula><p>The reverse of this process is also a variational Markov chain which is represented by:</p><formula xml:id="formula_4">xt-1 = 1 √ 1 -βt (xt + βts ϕ (xt, t)) + βtϵt.<label>(5)</label></formula><p>Here, s ϕ (•, t) is a network parameterized by ϕ representing the score of the noisy data distribution at scale t.</p><p>To cancel the effects of the data-protecting perturbations, we propose to first add Gaussian noise to the data. The amount of noise should be adjusted in a way that each image maintains its visual appearance. Otherwise, the semantic information of each image would be lost, and since the reverse process is probabilistic, the original image might not be recovered. In particular, let x be a protected image. We perform the forward process up to a step t * &lt; T such that the semantic information of the image is preserved:</p><formula xml:id="formula_5">xt * = √ αt * x + √ 1 -αt * ϵ.<label>(6)</label></formula><p>Now, we have managed to diminish the effects of the dataprotecting perturbation in x t * . However, this way we would also damage the semantic features of the data which makes it hard to train a neural network model (see the ablation study in Figure <ref type="figure" target="#fig_2">4</ref>). To revert to the normal image space, we use the reverse process of our diffusion model to denoise the data:</p><formula xml:id="formula_6">xt-1 = 1 √ 1 -βt (xt + βts ϕ (xt, t)) + βtϵt.<label>(7)</label></formula><p>Recursively solving Equation ( <ref type="formula" target="#formula_6">7</ref>) from t * to 1, we get a denoised version of the data which we denote by x = x0 . Using this process, , shown in Figure <ref type="figure" target="#fig_0">2</ref>, we unlock the entire dataset D pr , and construct a new one D de = {(x (i) , y (i) )} for neural network training. Algorithm 1 shows our final algorithm for training a neural network using AVATAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Conflicting Assumptions in Availability Attacks</head><p>So far, we discussed how by using diffusion models we can nullify the effects of the data-protecting perturbations. Here, we take a theoretical perspective on our proposed solution and show that in this setting, the two constraints of availability attacks conflict with each other. Specifically, from the perspective of availability attacks our result indicates that for a better data protection against AVATAR, we need larger perturbation norms. However, enlarging the perturbation is in Algorithm 1 dAta aVailAbiliTy Attacks defuseR</p><formula xml:id="formula_7">Input: protected dataset Dpr = {(x (i) , y (i) )} n i=1 , pre-trained diffu- sion model s ϕ (•, t). Output: trained neural network classifier f θ (•).</formula><p>Parameters: noise time-step t * , learning rate α, total epochs E, and batch-size b.</p><p>1: Initialize θ randomly. 2: Set D de = {}. 3: for (x, y) in Dpr do 4:</p><formula xml:id="formula_8">xt * = √ αt * x + √ 1 -αt * ϵ. 5: for t in t * , t * -1, • • • , 0 do 6: xt-1 = 1 √ 1-β t (xt + βts ϕ (xt, t)) + √ βtϵt. 7:</formula><p>end for 8:</p><p>Add (x0, y) to the dataset D de . 9: end for 10: for i = 1, 2, . . . , E do 11:</p><p>Assign D de to batches of size b randomly.</p><p>12:</p><p>for batch in batches do 13:</p><p>θ ← SGD (batch, f θ , α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>end for 15: end for conflict with retaining data utility which is the ultimate aim of availability attacks as discussed in Section III-B. Theorem 1. Let x ∈ R d denote a clean image and x = x + δ its protected version, where δ denotes any arbitrary data protection perturbation. Also, let x0 be the sanitized image using the AVATAR denoising process given in Equations ( <ref type="formula" target="#formula_5">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>. If we set t * such that</p><formula xml:id="formula_9">2 log 2 ∥δ∥ 2 + 4d µ∆ ≤ t * βt * ≤ µ∆ 4d ,</formula><p>then the estimation error between the sanitized x0 and clean image x can be bounded as:</p><formula xml:id="formula_10">E ∥x0 -x∥ 2 ≤ 2(µ + 1)∆,</formula><p>where ∆ = E[∥x0 -x∥ 2 ] and µ &gt; 0 is a constant.</p><p>Proof. See Appendix A for our proof using the contraction property of stochastic difference equations.</p><p>Theorem 1 states that for a protected image with a larger perturbation norm ∥δ∥, a larger amount of noise (determined by t * β t * ) is required. However, the amount of noise cannot be arbitrarily large as the semantic information of the image might be lost in the process (as indicated by the presence of ∆ in the upper-bound).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. AVATAR vs. Adversarial Training</head><p>As Tao et al. <ref type="bibr" target="#b60">[61]</ref> have demonstrated, adversarial training (AT) <ref type="bibr" target="#b38">[39]</ref> could also be used to train successful models over unexploitable data. However, our approach has several key advantages compared to AT:</p><p>1) First, AT modifies the learning algorithm, and as such, it needs to be applied separately for training each neural network. In contrast, AVATAR sanitizes the data only once. As a result, AVATAR is more efficient. 2) Second, as shown by Tsipras et al. <ref type="bibr" target="#b63">[64]</ref>, AT greatly affects the clean accuracy in its learning process, and as such, might not be the ultimate method for defending against availability attacks. 3) Lastly, as Fu et al. <ref type="bibr" target="#b17">[18]</ref> show, one can build unexploitable data against AT that would essentially render AT vulnerable to availability attacks. However, to the best of our knowledge, no adaptive availability attacks have been proposed against diffusion models so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we run various experiments to analyze the performance of AVATAR against availability attacks:</p><p>1) We conduct extensive experiments on seven SOTA availability attacks and show that given the data distribution, AVATAR can counteract them (Section IV-B). 2) We provide detailed comparisons against various preprocessing techniques (Section IV-C), early stopping (Section IV-D), and adversarial training (Section IV-E) to show that AVATAR delivers the best performance. 3) We provide extensive ablation studies into different assumptions made by AVATAR. First, we show that the training data overlap between the diffusion model and the unlearnable example generation has no effect on the performance of AVATAR (Section IV-G). Interestingly, we show that even a similar, different, or even poisoned distribution compared to the true data distribution can counteract availability attacks (Section IV-H). 4) We simulate our scenario given in Figure <ref type="figure">1</ref> for the real-world application of facial recognition to show the plausibility of our approach. Again, here we use a diffusion model trained on a different dataset, but we manage to counteract the unlearnable examples for another dataset (Section IV-I). We also include an extended version of our experimental results in Appendix B.</p><p>TABLE I: The classes of CIFAR-10 and their matching ones in the ImageNet-10 dataset. CIFAR-10 IN-10 Airplane Airliner Automobile Wagon Bird Humming Bird Cat Siamese Cat Deer Ox Dog Golden Retriever Frog Tailed Frog Horse Zebra Ship Container Ship Truck Trailer Truck</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of Experimental Settings</head><p>In this section, we provide the details of our experimental settings.</p><p>a) Datasets: In our experiments, we use four different datasets. CIFAR-10 &amp; 100 <ref type="bibr" target="#b32">[33]</ref> are 32×32 datasets of colored images, where the classes contain different objects, animals, plants, etc. SVHN <ref type="bibr" target="#b40">[41]</ref> is a dataset of house numbers from 0 to 9 in a natural, street view setting. Finally, ImageNet <ref type="bibr" target="#b48">[49]</ref> is a dataset of natural images of size 224 × 224 with 1000 classes. In our experiments, we use two simplified versions of this dataset. First, following the convention of prior research, we select the first 100 classes of this dataset, which we refer to as ImageNet (IN)-100. Second, for our distribution mismatch experiments, we follow Huang et al. <ref type="bibr" target="#b29">[30]</ref> and select 10 classes of ImageNet that are closely aligned with CIFAR-10 and downscale them to 32×32 size. We call this dataset IN-10. The information on the selected classes can be found in Table <ref type="table">I</ref>. Finally, we also use the 32×32 version of the ImageNet dataset for some of our experiments, which we denote by IN-1k-32×32.</p><p>b) Classifiers: In our experiments, we use four types of neural network image classifiers, namely: ResNet-18 (RN-18) <ref type="bibr" target="#b24">[25]</ref>, VGG-16 <ref type="bibr" target="#b52">[53]</ref>, DenseNet-121 (DN-121) <ref type="bibr" target="#b28">[29]</ref>, and WideResNet-34 (WRN-34) <ref type="bibr" target="#b73">[74]</ref>. For training these classifiers over different datasets and also training objectives (vanilla vs. adversarial training (AT)), we follow two different training conventions. The hyper-parameters of each setting are given in Table <ref type="table">II</ref>. Furthermore, Table <ref type="table">III</ref> indicates the setting used for each experiment in the paper. c) Diffusion Models: For the diffusion models used during the denoising process of AVATAR (shown in Figure <ref type="figure" target="#fig_0">2</ref>), we follow the implementation of DiffPure<ref type="foot" target="#foot_1">foot_1</ref> and use score SDE <ref type="bibr" target="#b57">[58]</ref> (for CIFAR-10, CIFAR-100, SVHN, IN-10) and the guided DDPM (for IN-100 and IN-1k-32×32.) <ref type="bibr" target="#b10">[11]</ref>. For CIFAR-10 and IN-100, we download the pre-trained versions available online. <ref type="foot" target="#foot_2">3</ref> for IN-1k-32×32 dataset. Additionally, for CIFAR-100, IN-10, and SVHN we use the PyTorch repository of score SDE <ref type="bibr" target="#b57">[58]</ref>, and train variance-preserving diffusion models with continuous DDPM++ architecture, similar to the one used for CIFAR-10. The FID score of the trained diffusion models is given in Table <ref type="table">IV</ref>.</p><p>d) Availability Attacks: We use seven SOTA availability attacks in our experiments: DeepConfuse (CON) <ref type="bibr" target="#b14">[15]</ref>, Neural Tangent Generalization Attacks (NTGA) <ref type="bibr" target="#b71">[72]</ref>, Errorminimizing Noise (EMN) <ref type="bibr" target="#b29">[30]</ref>, Targeted Adversarial Poisoning (TAP) <ref type="bibr" target="#b16">[17]</ref>, Robust EMN (REMN) <ref type="bibr" target="#b17">[18]</ref>, Shortcut (SHR) <ref type="bibr" target="#b18">[19]</ref>, and Autoregressive attacks (AR) <ref type="bibr" target="#b49">[50]</ref>. The details of each availability attack are given below:</p><p>• For CON <ref type="bibr" target="#b14">[15]</ref>, we use the released protected CIFAR-10 dataset, available online at SHR <ref type="bibr" target="#b70">[71]</ref> repository. <ref type="foot" target="#foot_3">4</ref> Note that since generating this attack for the CIFAR-10 dataset would take 5-7 days, we just used the available data for CIFAR-10 and skipped generating the attack for the other datasets. • For NTGA <ref type="bibr" target="#b71">[72]</ref>, we use their code <ref type="foot" target="#foot_4">5</ref> to generate availability attacks for our datasets. For CIFAR-10, we used the data published online. For CIFAR-100 and SVHN, we used the online repository, and generate NTGA protected data using the CNN surrogate model, time-step of 64, and block-size of 100 to generate perturbations of magnitude ∥δ∥ ∞ ≤ 8/255. Due to limited GPU memory, we used the FNN surrogate model to generate perturbations of magnitude ∥δ∥ ∞ ≤ 0.1 for IN-100. The rest of the hyper-parameters were set similarly to CIFAR-100 and SVHN. • For EMN <ref type="bibr" target="#b29">[30]</ref>, TAP <ref type="bibr" target="#b16">[17]</ref>, and REMN <ref type="bibr" target="#b17">[18]</ref>, we use the online repository of REMN <ref type="foot" target="#foot_5">6</ref> which contains an implementation of EMN and TAP as well. We use the default CIFAR-10 configurations of this repository for CIFAR-10, CIFAR-100, and SVHN. For IN-10, we used the default MiniIN configurations of the REMN code-base. • Moreover, we use the SHR GitHub repository 4 to generate shortcut attacks. For CIFAR-10, CIFAR-100, and SVHN, we use the default settings. For IN-100, we use patchsize of 32 as advised by the authors. • Finally, we use the official data released on the AR GitHub repository for this attack. <ref type="foot" target="#foot_6">7</ref>A few samples for each availability attack are shown in Figure <ref type="figure">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Exploiting Protected Data</head><p>Table <ref type="table">V</ref> shows our results for breaking availability attacks against for four different datasets. As can be seen, AVATAR can significantly improve the performance of neural network training in almost all cases. Moreover, although the training data was produced using diffusion models, the trained neural networks can generalize to unseen test data easily. This trend is more evident in the CIFAR-10 and SVHN datasets where the pre-trained diffusion model can better represent the image data density, as indicated by their low FID scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Data Augmentation Techniques</head><p>AVATAR can be regarded as a type of data pre-processing where the inner mechanics of the learning algorithms are not modified. As such, here we compare our approach with various SOTA data augmentation techniques that can be utilized during model training. To this end, we follow the settings of <ref type="bibr" target="#b29">[30]</ref>, and adopt four widely used data augmentation techniques. In addition, we employ the JPEG and grayscale pre-processing <ref type="bibr" target="#b37">[38]</ref> as well as two blurring techniques in Table <ref type="table">VI</ref>. Finally, we also test the quantization and total variation minimization (TVM) approaches that have shown to be effective against adversarial attacks <ref type="bibr" target="#b23">[24]</ref>. Table <ref type="table">VI</ref> shows the performance of these methods compared to AVATAR. As shown, our approach outperforms various types of pre-processing/data augmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Effect of Early Stopping</head><p>It has been previously shown that early stopping can also be beneficial against availability attacks <ref type="bibr" target="#b29">[30]</ref>. As such, here we run the same set of experiments over availability attacks for the CIFAR-10 dataset, but this time we record the highest accuracy attainable during training. Table <ref type="table">VII</ref> shows our results. As seen, using our approach one achieves stable training, where the variance between the final model accuracy and the highest attainable accuracy is very low. Notably, while these results indicate that existing availability attacks are less powerful than what is thought, early stopping is not sufficient to recover the best model performance. In contrast, AVATAR can significantly cancel the effects of availability attacks.</p><p>TABLE II: Training hyper-parameters used in our experiments. Hyper-parameter Setting #1 Setting #2 Optimizer SGD SGD Scheduler Multi-step Multi-step Initial lr. 0.1 0.1 lr. decay 0.1 (@epoch: 80 &amp; 100) 0.1 (@iter: 16k &amp; 32k) Batch Size 128 128 Training Steps 120 (epochs) 40k (iters) Weight Decay 0.0005 0.0005 PGD Steps (for AT only) -10 PGD Step Size (for AT only) -0.8</p><p>TABLE III: Setting number used for each experiment. <ref type="figure" target="#fig_2">4</ref> ✓ -Figure <ref type="figure">3</ref> -✓</p><formula xml:id="formula_11">Experiment Setting #1 Setting #2 Table V (CIFAR-10) ✓ - Table V (CIFAR-100) ✓ - Table V (SVHN) ✓ - Table V (IN-100) - ✓ Table VI ✓ - Table VII ✓ - Table VIII ✓ - Figure</formula><p>TABLE IV: The FID of the diffusion models used for denoising. * denotes that the FID has been computed using 10k generated samples only. † indicates that the scores have been adapted from relative literature. Dataset FID Dataset FID CIFAR-10 † 2.41 SVHN * 2.59 CIFAR-10 (TAP) * 4.11 CIFAR-100 * 4.85 IN-10 * 17.32 IN † 4.59</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with Adversarial Training</head><p>As mentioned in Section II, adversarial training (AT) <ref type="bibr" target="#b38">[39]</ref> is the most successful defense technique against availability attacks <ref type="bibr" target="#b60">[61]</ref>. For the next set of experiments, we follow the settings of Fu et al. <ref type="bibr" target="#b17">[18]</ref> and compare our approach with AT. To this end, we run two different scenarios. First, we perform AT over the protected data. Then, we run AT over the data that is defused (i.e., counteracted) by AVATAR. In both cases, we vary the perturbation bound ε from 0 to 4, where 0 is the vanilla training. Figure <ref type="figure">3</ref> shows our results. Apart from what we discussed in Section III-D, two additional insights are worth mentioning here:</p><p>(1) In most cases, AVATAR without AT (i.e., ε = 0) performs on-par or better than AT with ε &gt; 0. Thus, AVATAR delivers the SOTA against availability attacks. (2) As seen in Figure <ref type="figure">3</ref>, AT yields the worst performance against REMN <ref type="bibr" target="#b17">[18]</ref>. However, our approach can combat REMN <ref type="bibr" target="#b17">[18]</ref> successfully, and it is the first approach that does so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Setting Diffusion Step t *</head><p>As discussed in Section III-C, setting the diffusion timestep should be performed carefully. Otherwise, either the dataprotecting noise is not eliminated, or the semantic information of the image is lost. Here, we run an ablation study over the diffusion timestep. In particular, for our CIFAR-10 experiments, we run AVATAR with five different timesteps from {0, 100, 200, 300, 400}. Then, we measure the test accuracy of the trained neural networks over the clean test set. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, setting t * too small means that the data-protecting perturbations are not removed. In contrast, setting t * to a large value might remove the semantic information which in turn damages the generalizability of the trained model. For a more thorough discussion on selecting t * , please see Appendix B-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. The Effect of Diffusion Models' Training Data</head><p>It is well-known from the literature that diffusion models are not a mere memorization of their training data <ref type="bibr" target="#b57">[58]</ref> and can further enhance the accuracy of down-stream tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b65">66]</ref>. To empirically eradicate the influence of training data overlap on our results, we perform the following experiment. Apart from our results in Table <ref type="table">V</ref>, we run a second set of experiments where we create disjoint subsets of training data for training diffusion models and those used as unlearnable examples. Then, we train our in-house diffusion model and perform a similar experiment to that of Table V, but this time with this new, non-overlapping set of data. Finally, we measure the performance over the unseen test data. We report the relative error rate with respect to the clean data performance in Figure <ref type="figure" target="#fig_3">5</ref>. As seen, the overlap in diffusion models' training data has no impact on AVATAR's final performance. We further validate this through our real-world experiments in Section IV-I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Distribution Mismatch</head><p>To go even further, we show that AVATAR is even resilient to a distribution mismatch between the diffusion model and the training data. In particular, we train three diffusion models over the protected CIFAR-10 dataset with TAP <ref type="bibr" target="#b16">[17]</ref>, IN-10 which contains 10 classes of ImageNet that are most similar to CIFAR-10 dataset <ref type="bibr" target="#b29">[30]</ref> (see Table <ref type="table">I</ref>  TABLE V: Test accuracy (%) of RN-18 architectures trained over data availability attacks on CIFAR-10, CIFAR-100, and SVHN, and ImageNet-100 datasets without and with our denoising approach. The mean and standard deviation are computed over 5 seeds. For our results over other architectures, please see Table X. Data Method Clean Data Availability Attacks NTGA EMN TAP REMN SHR AR CIFAR-10 Vanilla 94.50 ± 0.09 11.49 ± 0.69 24.85 ± 0.71 7.86 ± 0.90 20.50 ± 1.16 10.82 ± 0.22 12.09 ± 1.12 AVATAR 87.95 ± 0.28 90.95 ± 0.10 90.71 ± 0.19 88.49 ± 0.24 85.69 ± 0.27 91.57 ± 0.18 SVHN Vanilla 96.29 ± 0.12 9.65 ± 0.70 9.13 ± 2.00 65.97 ± 1.99 11.55 ± 0.19 10.59 ± 3.98 6.76 ± 0.07 AVATAR 89.84 ± 0.32 93.84 ± 0.12 93.35 ± 0.10 88.51 ± 0.23 83.82 ± 0.39 94.13 ± 0.17 CIFAR-100 Vanilla 75.01 ± 0.41 1.32 ± 0.31 2.05 ± 0.18 14.10 ± 0.19 10.88 ± 0.33 1.39 ± 0.10 2.15 ± 0.46 AVATAR 63.98 ± 0.55 65.73 ± 0.36 64.99 ± 0.10 64.88 ± 0.08 58.52 ± 0.46 64.54 ± 0.23 IN-100 Vanilla 80.05 ± 0.13 74.74 ± 0.52 1.78 ± 0.17 9.14 ± 0.40 13.28 ± 0.51 43.48 ± 1.56 AVATAR 71.08 ± 0.48 72.84 ± 0.90 76.52 ± 0.46 39.79 ± 0.98 59.85 ± 1.01</p><p>TABLE VI: Test accuracy (%) of RN-18 models trained over data availability attacks on CIFAR-10 dataset using different data augmentation/pre-processing techniques. The results are averaged over 5 runs. The best results are highlighted in bold. Method Clean Data Availability Attacks CON NTGA EMN TAP REMN SHR AR Vanilla 94.50 ± 0.09 15.75 ± 0.82 11.49 ± 0.69 24.85 ± 0.71 7.86 ± 0.90 20.50 ± 1.16 10.82 ± 0.22 12.09 ± 1.12 Cutout 94.39 ± 0.12 13.53 ± 0.34 13.43 ± 1.15 23.79 ± 1.28 9.73 ± 1.06 20.48 ± 1.09 11.78 ± 0.81 11.21 ± 1.01 MixUp 94.87 ± 0.05 28.58 ± 2.88 13.54 ± 0.36 51.48 ± 0.97 30.09 ± 1.93 26.61 ± 1.65 19.69 ± 0.71 12.67 ± 1.02 CutMix 95.16 ± 0.03 19.04 ± 2.74 14.16 ± 1.64 25.30 ± 1.18 7.45 ± 1.21 26.83 ± 1.99 10.89 ± 0.34 11.36 ± 0.50 FAutoAug. 95.11 ± 0.14 51.62 ± 1.28 27.56 ± 2.45 56.31 ± 1.13 20.39 ± 0.81 26.65 ± 0.89 25.88 ± 0.62 13.53 ± 0.79 Median Blur 85.83 ± 0.71 15.14 ± 0.38 28.43 ± 1.41 26.97 ± 0.39 57.16 ± 0.75 23.32 ± 0.69 17.50 ± 0.38 14.97 ± 0.40 Gaus. Blur 94.33 ± 0.08 15.36 ± 0.69 11.86 ± 0.81 24.08 ± 0.40 8.87 ± 0.65 20.89 ± 1.56 11.39 ± 1.91 13.39 ± 1.08 Quantization 94.57 ± 0.14 15.17 ± 0.79 16.29 ± 1.03 25.38 ± 0.68 7.38 ± 1.59 22.33 ± 1.21 11.12 ± 0.24 12.87 ± 0.69 TVM 73.20 ± 1.32 42.82 ± 2.00 47.41 ± 1.37 54.86 ± 2.17 70.66 ± 0.58 19.28 ± 1.12 25.35 ± 2.54 34.09 ± 2.07 Grayscale 92.89 ± 0.20 83.30 ± 0.40 63.21 ± 0.85 92.09 ± 0.22 9.57 ± 0.59 75.84 ± 1.36 57.13 ± 0.87 35.88 ± 0.99 JPEG 84.99 ± 0.28 82.87 ± 0.23 79.26 ± 0.12 84.65 ± 0.16 83.44 ± 0.36 83.66 ± 0.30 69.03 ± 0.62 85.03 ± 0.23 AVATAR (Ours) 90.16 ± 0.21 89.43 ± 0.09 87.95 ± 0.28 90.95 ± 0.10 90.71 ± 0.19 88.49 ± 0.24 85.69 ± 0.27 91.57 ± 0.18</p><p>CIFAR-100. Then, we use these surrogate distributions to sanitize protected CIFAR-10 data and train a neural network over the denoised data. We report our results in Table <ref type="table">VIII</ref>. Surprisingly, our approach can tolerate the distribution mismatch to some extent. As the diffusion model density gets closer to the true training data, the performance gap is gradually closed. Interestingly, even using a diffusion model that is trained over protected data can be beneficial in removing the effects of availability attacks. Note that according to our threat model discussed in Figure <ref type="figure">1</ref>, this case is too extreme, meaning that the data protector needs to add a perturbation to all the data on the web which is almost impossible. Interestingly, our method using the sub-optimal CIFAR-100 distribution is still performing better than grayscale and JPEG compression techniques of Liu et al. <ref type="bibr" target="#b37">[38]</ref>.</p><p>These results motivates us to run AVATAR in a real-world case. In particular, we employ the off-the-shelf diffusion model, DDPM-IP <ref type="bibr" target="#b42">[43]</ref>, that is trained over the 32 × 32 version of the ImageNet dataset in AVATAR. Then, we re-run our experiments of Table V on CIFAR-10, CIFAR-100, and SVHN</p><p>TABLE VII: Test accuracy (%) of RN-18 models trained over data availability attacks on CIFAR-10 dataset. The early stopping rows contain the highest achievable accuracy over the course of training. The results are averaged over 5 runs. Method Data Availability Attacks CON NTGA EMN TAP REMN SHR AR Vanilla 15.75 ± 0.82 11.49 ± 0.69 24.85 ± 0.71 7.86 ± 0.90 20.50 ± 1.16 10.82 ± 0.22 12.09 ± 1.12 + Early Stopping 23.99 ± 6.22 31.71 ± 3.97 27.23 ± 1.83 67.13 ± 2.03 21.90 ± 0.57 22.72 ± 0.83 38.78 ± 8.65 AVATAR (Ours) 89.43 ± 0.09 87.95 ± 0.28 90.95 ± 0.10 90.71 ± 0.19 88.49 ± 0.24 85.69 ± 0.27 91.57 ± 0.18 + Early Stopping 89.55 ± 0.15 88.07 ± 0.22 91.07 ± 0.11 91.00 ± 0.11 88.59 ± 0.26 85.76 ± 0.25 91.63 ± 0.17 using this diffusion model. As this DDPM-IP <ref type="bibr" target="#b42">[43]</ref> uses a cosine schedule <ref type="bibr" target="#b10">[11]</ref>, we need to adjust the value of t * to reflect this change. As we discuss in Appendix B-C, we set t * = 200 to have an equivalent performance to the linear schedule that was used in our earlier experiments.</p><p>Our results are shown in Table <ref type="table">IX</ref>. As seen, AVATAR is resilient to the choice of the diffusion model. Even though there is a distribution mismatch between our test datasets and ImageNet-32×32, our results are on par with the use of the matching data distribution. These results indicate the realworld value of AVATAR which can serve as a strong baseline against availability attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Real-world Example I: Face Recognition</head><p>In Section I, we discussed in detail that the threat model of existing availability attacks is fragile and a malicious adversary might still exploit the personal data. This means that possibly no imperceptible adversary can protect the image data from being maliciously used. To show this, we discussed a realworld example in Section IV following a similar experiment from Huang et al. <ref type="bibr" target="#b29">[30]</ref>. In particular, we create a set of clean and protected identities in the WebFace <ref type="bibr" target="#b68">[69]</ref> dataset by randomly selecting 50 identities from this dataset. As a result, the remaining 10522 identities constitute our clean data. For all of the identities, we randomly split the data so that 80% of that Distribution Data Availability Attacks CON NTGA EMN TAP REMN SHR AR Vanilla 15.75 ± 0.82 11.49 ± 0.69 24.85 ± 0.71 7.86 ± 0.90 20.50 ± 1.16 10.82 ± 0.22 12.09 ± 1.12 CIFAR-10 (TAP) 61.70 ± 2.02 75.62 ± 3.75 64.03 ± 0.98 35.09 ± 2.28 60.16 ± 1.44 74.96 ± 2.82 60.36 ± 2.29 IN-10 80.98 ± 0.06 79.42 ± 0.25 83.78 ± 0.39 82.71 ± 0.24 82.83 ± 0.28 75.91 ± 0.06 84.88 ± 0.19 CIFAR-100 84.85 ± 0.49 83.07 ± 0.33 87.81 ± 0.14 86.55 ± 0.26 85.84 ± 0.19 79.52 ± 0.22 88.59 ± 0.15 CIFAR-10 89.43 ± 0.09 87.95 ± 0.28 90.95 ± 0.10 90.71 ± 0.19 88.49 ± 0.24 85.69 ± 0.27 91.57 ± 0.18</p><p>TABLE IX: Test accuracy (%) of RN-18 models trained over data availability attacks on CIFAR-10, CIFAR-100, SVHN with our denoising approach using the matching distribution and ImageNet-32×32. The mean and standard deviation are computed over 5 seeds. Data Distribution Clean Data Availability Attacks NTGA EMN TAP REMN SHR AR CIFAR-10 CIFAR-10 94.50 ± 0.09 87.95 ± 0.28 90.95 ± 0.10 90.71 ± 0.19 88.49 ± 0.24 85.69 ± 0.27 91.57 ± 0.18 ImageNet-32×32 86.41 ± 0.21 90.17 ± 0.15 89.02 ± 0.15 88.26 ± 0.24 82.97 ± 0.24 90.61 ± 0.18 SVHN SVHN 96.29 ± 0.12 89.84 ± 0.32 93.84 ± 0.12 93.35 ± 0.10 88.51 ± 0.23 83.82 ± 0.39 94.13 ± 0.17 ImageNet-32×32 91.32 ± 0.17 94.82 ± 0.10 95.01 ± 0.21 91.00 ± 0.27 83.12 ± 0.30 94.29 ± 0.22 CIFAR-100 CIFAR-100 75.01 ± 0.41 63.98 ± 0.55 65.73 ± 0.36 64.99 ± 0.10 64.88 ± 0.08 58.52 ± 0.46 64.54 ± 0.23 ImageNet-32×32 65.22 ± 0.55 67.09 ± 0.18 66.52 ± 0.25 66.52 ± 0.15 58.32 ± 0.56 66.44 ± 0.17 The protected users protect their images using data-protecting perturbations. Our approach uses a diffusion model trained over the CelebA <ref type="bibr" target="#b36">[37]</ref> dataset. For all the stealthy data-protecting perturbations our approach manages to recover the performance over protected data. The only exception is SHR, which according to Figure <ref type="figure">8</ref>, leaves a noticeable trace over the image, rendering them not useful anymore.</p><p>data is allocated to a training set and the rest is the test set. We assume that the protected identities would add data-protecting perturbations to their images before sharing them. To this end, we use class-wise EMN <ref type="bibr" target="#b29">[30]</ref>, TAP <ref type="bibr" target="#b16">[17]</ref>, REMN <ref type="bibr" target="#b17">[18]</ref>, and SHR <ref type="bibr" target="#b70">[71]</ref> with a perturbation radius of ∥δ∥ ∞ ≤ 16/255. For perturbation generation using the first three attacks, we follow the settings of Huang et al. <ref type="bibr" target="#b29">[30]</ref>. Specifically, we select 100 random identities from the CelebA <ref type="bibr" target="#b36">[37]</ref> dataset and create an auxiliary dataset consisting of these 100 identities and the 50 protected WebFace <ref type="bibr" target="#b68">[69]</ref> identities. Then, using these 150 identities we generate data protecting perturbations against a neural network with 150 classes. For SHR, however, we generate the data for all the 10572 WebFace identities and select the relevant data for protecting our above-mentioned 50 identities. Once we have the protected data, we train an InceptionResNet <ref type="bibr" target="#b58">[59]</ref> facial recognition over the training set with or without our approach and evaluate the models over the test set. In our case, we assume that the malicious entity has access to a pre-trained diffusion model over CelebA <ref type="bibr" target="#b36">[37]</ref> faces <ref type="foot" target="#foot_7">8</ref> , and can run AVATAR over the protected data that it acquires from crawling the web. Since the WebFace photos are of size 112 × 112 but the diffusion model generates 256 × 256 images, we use bi-linear up-and down-sampling to connect the two. Like the CIFAR-10 experiments, here we also denoise the data with timestep set to 100. Samples of the WebFace dataset along with the protected data are shown in Figure <ref type="figure">8</ref>.</p><p>To evaluate the performance of our method, we test the models over the clean test set and record the recognition accuracy for both the protected and clean identities. <ref type="foot" target="#foot_8">9</ref>As shown in Figure <ref type="figure" target="#fig_4">6</ref>, AVATAR can recover the recognition accuracy over protected identities in all cases except the SHR <ref type="bibr" target="#b70">[71]</ref> perturbations. The reason behind this might be twofold. First, we are using a sub-optimal diffusion model as both the domain and, more importantly, size of the images have a mismatch. Second, looking at Figure <ref type="figure">8</ref>, we see that while the SHR perturbations can protect the data, they trade the stealthiness of the original data due to their large patches. As such, the images would lose their utility. Now, the question is:</p><p>Can we protect the data using stealthy patterns without losing the data utility?</p><p>Interestingly, our theoretical result in Theorem 1 says that this might not be possible. According to Theorem 1, if the data curator wants to makes the denoising process harder, they need to increase the data-protecting perturbation. This increase is naturally at odds with the data utility, since by adding more powerful perturbations we lose the data utility.</p><p>V. CONCLUSION In this paper, we introduced a countermeasure against data protection algorithms that use availability attacks. In particular, we show that by adding a controlled amount of Gaussian noise to the images and subsequently denoising them one can eliminate data-protecting perturbations. To this end, we use the forward and reverse diffusion processes of pre-trained models. We theoretically analyze our approach and show that the amount of Gaussian noise required to defuse the data-protecting perturbations is directly related to their norm. We conduct extensive experiments over various availability attacks. Our experiments demonstrate the superiority of our approach compared to adversarial training, setting a new SOTA defense against availability attacks. AVATAR demonstrates brittleness of availability attacks and calls for more research to protect personal data. Future work involves investigating the applicability of AVATAR to other models such as text-to-image generative models <ref type="bibr" target="#b51">[52]</ref> and its relationship with techniques such as randomized smoothing <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models APPENDIX A PROOFS</head><p>Here we provide our proof for Theorem 1. First, we provide the theoretical results that would be used in our proof. Then, we re-state Theorem 1 and provide its detailed proof. Our proofs heavily borrow from the contraction properties of stochastic difference equations <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Theorem 2 (Discrete stochastic contraction <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b5">6]</ref>). Let</p><formula xml:id="formula_12">x t-1 = h(x t , t) + σ(x t , t)ϵ t ,<label>(8)</label></formula><p>denote a stochastic difference equation where:</p><p>(a) h : R d × N → R d is a contraction mapping, i.e., for every t ∈ N there exists a λ t ∈ [0, 1) such that</p><formula xml:id="formula_13">∥h(x, t) -h(y, t)∥ ≤ λ t ∥x -y∥ ∀ x, y ∈ R d ,<label>(9)</label></formula><formula xml:id="formula_14">(b) σ : R d × N → R is a function such that for every t ∈ N and x ∈ R d Tr σ(x, t)Iσ(x, t) ≤ C t ,<label>(10)</label></formula><p>(c) and ϵ t ∼ N (0, I).</p><p>Then, for two sample trajectories x t-1 and xt-1 that satisfy Equation (8) we have:</p><formula xml:id="formula_15">E ∥x t-1 -xt-1 ∥ 2 ≤ λ 2 t E ∥x t -xt ∥ 2 + 2C t .<label>(11)</label></formula><p>Using Theorem 2 and Equation ( <ref type="formula" target="#formula_4">5</ref>) we can get the following result <ref type="bibr" target="#b5">[6]</ref>.</p><p>Corollary 2.1. The reverse diffusion process of DDPMs are contracting stochastic difference equations.</p><p>Proof. Our proof closely follows that of Chung et al. <ref type="bibr" target="#b5">[6]</ref>. Specifically, we need to show that for the reverse diffusion process given in Equation ( <ref type="formula" target="#formula_4">5</ref>), the conditions of Equations ( <ref type="formula" target="#formula_13">9</ref>) and ( <ref type="formula" target="#formula_14">10</ref>) hold. To show this, note that if we set:</p><formula xml:id="formula_16">h(x t , t) = 1 √ 1 -β t (x t + β t s ϕ (x t , t))</formula><p>and σ(x t , t) = β t then Equations ( <ref type="formula" target="#formula_4">5</ref>) and ( <ref type="formula" target="#formula_12">8</ref>) coincide. Using Lemma A.1. from Chung et al. <ref type="bibr" target="#b5">[6]</ref>, one can show that for</p><formula xml:id="formula_17">λ t = 1 -β t 1 -α t-1 1 -α t<label>(12)</label></formula><p>and</p><formula xml:id="formula_18">C t = dβ t (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>the conditions of Equations ( <ref type="formula" target="#formula_13">9</ref>) and ( <ref type="formula" target="#formula_14">10</ref>) are satisfied. As such, for two reverse sample trajectories x t-1 and xt-1 that satisfy the reverse diffusion process of Equation ( <ref type="formula" target="#formula_4">5</ref>), Equation <ref type="bibr" target="#b10">(11)</ref> holds.</p><p>Next, we present two lemmas that are going to be used in our proof of Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 ([6]</head><p>). For λ t 's given in Equation (12) the following holds:</p><formula xml:id="formula_20">t * s=1 λ 2 s ≤ exp(- t * β t * 2 ).<label>(14)</label></formula><p>Proof. See Lemma C.1. in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Lemma 2. For two random vectors x and y we have:</p><formula xml:id="formula_21">E ∥x + y∥ 2 ≤ 2 E ∥x∥ 2 + 2 E ∥y∥ 2 . (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>Proof. We know that:</p><formula xml:id="formula_23">E ∥x + y∥ 2 = E ∥x∥ 2 + E ∥y∥ 2 + 2 E x ⊤ y ≤ 2 E ∥x∥ 2 + 2 E ∥y∥ 2 ,</formula><p>where the last inequality follows from the fact that E ∥x -y∥ 2 ≥ 0.</p><p>We are now ready to prove our theoretical result.</p><p>Theorem 1 (restated). Let x ∈ R d denote a clean image and x = x + δ its protected version, where δ denotes any arbitrary data protection perturbation. Also, let x0 be the sanitized image using the AVATAR denoising process given in Equations ( <ref type="formula" target="#formula_5">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>. If we set t * such that</p><formula xml:id="formula_24">2 log 2 ∥δ∥ 2 + 4d µ∆ ≤ t * β t * ≤ µ∆ 4d ,</formula><p>then the estimation error between the sanitized x0 and clean image x can be bounded as:</p><formula xml:id="formula_25">E ∥x 0 -x∥ 2 ≤ 2(µ + 1)∆,</formula><p>where ∆ = E[∥x 0 -x∥ 2 ] and µ &gt; 0 is a constant.</p><p>Proof. We are looking to find an upper-bound for the estimation error between the sanitized image and its clean version. Using Lemma 2 we can write:</p><formula xml:id="formula_26">E ∥x 0 -x∥ 2 = E ∥(x 0 -x 0 ) + (x 0 -x)∥ 2 ≤ 2 E ∥x 0 -x 0 ∥ 2 + 2 E ∥x 0 -x∥ 2 ≤ 2 E ∥x 0 -x 0 ∥ 2 + 2 ∆.<label>(16)</label></formula><p>Now, we need to find an upper-bound for the first term. To this end, we are going to use the contraction property of the DDPMs (Corollary 2.1). In particular, given the noisy versions of the clean x and the protected image x = x + δ, in other words:</p><formula xml:id="formula_27">x t * = √ α t * x + √ 1 -α t * ϵ 0 xt * = √ α t * x + √ 1 -α t * ϵ ′ 0 ,<label>(17)</label></formula><p>we know that both x 0 and x0 satisfy the reverse diffusion process, or:</p><formula xml:id="formula_28">x t-1 = 1 √ 1 -</formula><p>β t (x t + β t s ϕ (x t , t)) + β t ϵ t xt-1 = 1 √ 1 -β t (x t + β t s ϕ (x t , t)) + β t ϵ ′ t , ∀ t ∈ {1, 2, . . . , t * },</p><p>where ϵ t , ϵ ′ t ∼ N (0, I). As such, we can treat x 0 and x0 as two sample trajectories of the same stochastic difference equation. Thus, by recursively applying Equation <ref type="bibr" target="#b10">(11)</ref> we would get:</p><formula xml:id="formula_30">E ∥x 0 -x 0 ∥ 2 ≤ E ∥x t * -x t * ∥ 2 t * s=1 λ 2 s + 2 t * s=1 C s s-1 r=1 λ 2 r .<label>(19)</label></formula><p>Now, let us consider each term on the RHS of Equation ( <ref type="formula" target="#formula_30">19</ref>) separately. For the red term, we can write:</p><formula xml:id="formula_31">E ∥x t * -x t * ∥ 2 (1) = E √ α t * (x -x) + √ 1 -α t * (ϵ ′ 0 -ϵ 0 ) 2 (2) = E √ α t * δ + √ 1 -α t * (ϵ ′ 0 -ϵ 0 ) 2 = ∥ √ α t * δ∥ 2 + E √ 1 -α t * (ϵ ′ 0 -ϵ 0 ) 2 + 2 √ α t * √ 1 -α t * δ ⊤ E [ϵ ′ 0 -ϵ 0 ] (3) = α t * ∥δ∥ 2 + (1 -α t * )E ∥(ϵ ′ 0 -ϵ 0 )∥ 2 . (<label>20</label></formula><formula xml:id="formula_32">)</formula><p>where ( <ref type="formula" target="#formula_0">1</ref>) is derived from Equation ( <ref type="formula" target="#formula_27">17</ref>), (2) holds since x = x + δ, and (3) is valid as ϵ 0 , ϵ ′ 0 ∼ N (0, I). Given that: ϵ ′ 0 -ϵ 0 ∼ N (0, 2I), we can simplify Equation <ref type="bibr" target="#b19">(20)</ref> as:</p><formula xml:id="formula_33">E ∥x t * -x t * ∥ 2 = α t * ∥δ∥ 2 + 2(1 -α t * )E [χ] ,</formula><p>where χ follows the chi-squared distribution with d degrees of freedom. Using the fact that 0 &lt; α t * &lt; 1, we can finally write:</p><formula xml:id="formula_34">E ∥x t * -x t * ∥ 2 = α t * ∥δ∥ 2 + 2(1 -α t * )d ≤ ∥δ∥ 2 + 2d.<label>(21)</label></formula><p>Using Lemma 1, for the blue term in Equation ( <ref type="formula" target="#formula_30">19</ref>) we can write:</p><formula xml:id="formula_35">t * s=1 λ 2 s ≤ exp(- t * β t * 2 ).<label>(22)</label></formula><p>Finally, for the green term we have: </p><p>Here, (1) is the result of Equation ( <ref type="formula" target="#formula_18">13</ref>), (2) holds since 0 &lt; λ r &lt; 1 (see Equation ( <ref type="formula" target="#formula_17">12</ref>)), and ( <ref type="formula" target="#formula_2">3</ref>) is derived from</p><formula xml:id="formula_37">0 &lt; β 1 &lt; • • • &lt; β t &lt; 1.</formula><p>Putting Equations ( <ref type="formula" target="#formula_34">21</ref>) to ( <ref type="formula" target="#formula_36">23</ref>) together, we have:</p><formula xml:id="formula_38">E ∥x 0 -x 0 ∥ 2 ≤</formula><p>∥δ∥ 2 + 2d exp(-t * β t * 2 ) + 2dt * β t * . (24) Given that: 2 log 2 ∥δ∥ 2 + 4d µ∆ ≤ t * β t * ≤ µ∆ 4d , we can simplify Equation (24) as: E ∥x 0 -x 0 ∥ 2 ≤ ∥δ∥ 2 + 2d exp(-t * β t * 2 ) + 2dt * β t * ≤ ∥δ∥ 2 + 2d µ∆ 2 ∥δ∥ 2 + 4d + 2d µ∆ 4d ≤ µ∆.</p><p>Replacing Equation <ref type="bibr" target="#b24">(25)</ref> into Equation ( <ref type="formula" target="#formula_26">16</ref>), the proof can be completed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overview of AVATAR. According to a pre-trained diffusion model, we first add a controlled amount of Gaussian noise to the training data. Then, we use the reverse diffusion process to denoise the data which is later going to be used for neural network training.</figDesc><graphic coords="5,127.93,132.24,54.34,54.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>34 Fig. 3 :</head><label>343</label><figDesc>Fig. 3: Test accuracy of CIFAR-10, SVHN, and CIFAR-100 classifiers against availability attacks using adversarial training with different perturbation radii.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Effect of changing the forward process diffusion timestep in AVATAR on the final test accuracy in CIFAR-10 classifiers.</figDesc><graphic coords="10,101.52,360.04,192.77,149.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Relative error rate of RN-18 models trained against availability attacks on CIFAR-10 and SVHN averaged over 5 runs. Overlapping indicates that the diffusion model and availability attacks use the same subset as training data. Non-overlapping means that the diffusion model and availability attacks are trained on disjoint subsets of data. TABLE VIII: Test accuracy (%) of RN-18 models trained over data availability attacks on the CIFAR-10 dataset. For AVATAR, we use different pre-trained distributions over CIFAR-10, poisoned CIFAR-10 (TAP), ImageNet-10 (IN-10) [30], and CIFAR-100. The results are averaged over 5 runs.</figDesc><graphic coords="11,73.72,50.54,205.63,166.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: Test accuracy for protected vs. clean identities in WebFace<ref type="bibr" target="#b68">[69]</ref> facial recognition. The protected users protect their images using data-protecting perturbations. Our approach uses a diffusion model trained over the CelebA<ref type="bibr" target="#b36">[37]</ref> dataset. For all the stealthy data-protecting perturbations our approach manages to recover the performance over protected data. The only exception is SHR, which according to Figure8, leaves a noticeable trace over the image, rendering them not useful anymore.</figDesc><graphic coords="12,172.34,50.54,267.32,155.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>≤</head><figDesc>2dt * β t * .</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that while here we use DDPMs<ref type="bibr" target="#b27">[28]</ref> to demonstrate our method, it can be easily extended to other types of diffusion models as they are all different ways of representing the same process<ref type="bibr" target="#b57">[58]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/NVlabs/DiffPure</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>ForCIFAR-10, we used the checkpoint for the vp/cifar10_ddpmpp_deep_continuous setting on score SDE repository: https://github.com/yang-song/score sde pytorch. Moreover, we used the unconditional 256 × 256 model available on the guided DDPM code-base for IN-100 expriments: https://github.com/openai/guided-diffusion. Finally, we use the pre-trained DDPM-IP<ref type="bibr" target="#b42">[43]</ref> models available on https://github.com/forever208/DDPM-IP</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/dayu11/Availability-Attacks-Create-Shortcuts</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/lionelmessi6410/ntga</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/fshp971/robust-unlearnable-examples</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/psandovalsegura/autoregressive-poisoning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>For this experiment, we use a pre-trained DDPM model over CelebA-HQ: https://github.com/ermongroup/SDEdit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>Running the identity overlap removal of Wang et al.<ref type="bibr" target="#b64">[65]</ref>, we found that only 8 out of 50 protected identities had overlap between CelebA-HQ and WebFace. After removing these identities, we saw no major drop in the final performance of AVATAR.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the <rs type="institution">University of Melbourne</rs>. This Facility was established with the assistance of LIEF Grant <rs type="grantNumber">LE170100200</rs>. <rs type="person">Sarah Erfani</rs> is in part supported by <rs type="funder">Australian Research Council (ARC)</rs> <rs type="grantName">Discovery Early Career Researcher Award (DECRA</rs>) <rs type="grantNumber">DE220100680</rs>. Moreover, this research was partially supported by the <rs type="funder">ARC Centre of Excellence for Automated Decision-Making and Society</rs> (<rs type="grantNumber">CE200100005</rs>), and funded partially by the <rs type="funder">Australian Government</rs> through the <rs type="funder">Australian Research Council</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zNm55Jc">
					<idno type="grant-number">LE170100200</idno>
					<orgName type="grant-name">Discovery Early Career Researcher Award (DECRA</orgName>
				</org>
				<org type="funding" xml:id="_BPajFHm">
					<idno type="grant-number">DE220100680</idno>
				</org>
				<org type="funding" xml:id="_Pr9jQRr">
					<idno type="grant-number">CE200100005</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our findings call for more research into making personal data unexploitable, showing that this goal is far from over. Our implementation is available at this repository: <ref type="url" target="https://github.com/hmdolatabadi/AVATAR">https://github.com/hmdolatabadi/AVATAR</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADDITIONAL EXPERIMENTAL RESULTS</head><p>In this section, we provide additional experiments and insights that were omitted from the main paper due to space limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Denoising Samples</head><p>Figures <ref type="figure">7</ref> and <ref type="figure">8</ref> include samples from the protected IN-100 and WebFace datasets alongside their denoised ones. As seen, AVATAR can successfully recover the benign data except cases where the perturbations are sever enough to remain visible. In these cases, however, the protected data has lost its normal utility due to the visibility of the protecting perturbation.</p><p>Pert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Noisy Denoised (a) NTGA <ref type="bibr" target="#b71">[72]</ref> Pert.</p><p>Input Noisy Denoised (b) EMN <ref type="bibr" target="#b29">[30]</ref> Pert. Input Noisy Denoised (c) TAP <ref type="bibr" target="#b16">[17]</ref> Pert.</p><p>Input Noisy Denoised (d) REMN <ref type="bibr" target="#b17">[18]</ref> Pert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Noisy</head><p>Denoised (e) SHR <ref type="bibr" target="#b70">[71]</ref> Fig. <ref type="figure">7</ref>: Samples from IN-100 dataset. For each attack, we show the perturbation, the protected image, the noisy version of the image, and the denoised one using AVATAR. Fig. <ref type="figure">8</ref>: Samples from the WebFace <ref type="bibr" target="#b68">[69]</ref> dataset. In each case, we generate the data-protecting perturbations with a maximum magnitude of 16/255. For denoising, we use AVATAR based on a diffusion model pre-trained on the CelebA <ref type="bibr" target="#b36">[37]</ref> dataset.</p><p>As seen, the SHR <ref type="bibr" target="#b70">[71]</ref> perturbations leave a noticeable pattern over the protected data, which put their utility (e.g., posting over the social media) under question. Nevertheless, since they have a more global structure, they remain persistent even after denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extended Experimental Results over Different Architectures</head><p>In Table <ref type="table">V</ref>, we presented our results on training RN-18 models over protected data. To show the applicability of our approach across various architectures, we also report our results for three additional architectures, namely DN-121, VGG-16, and WRN-34, in Table <ref type="table">X</ref>. Similar to our RN-18 experiments, AVATAR delivers the best performance against protected data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. On Selecting Diffusion Step t *</head><p>In Figure <ref type="figure">4</ref>, we demonstrated that setting t * = 100 delivers a consistent performance across different architectures. However, chances are that practitioners may want to replace the diffusion model used in AVATAR with one of their own. In such cases, the diffusion model might have different characteristics compared to the ones used in this paper. In this part, we present two methods for setting t * .</p><p>1) Using the α t Curves: A naïve approach in selecting a suitable diffusion timestep t * is using the α t curves between the new diffusion model and a reference model. Specifically, since the value of α t in Equation (3) controls the amount of disruptive noise, we can use the value of α t to guide our hyper-parameter selection. To this end, we can find an equivalent t * such that the value of α t * is set to an acceptable value. This is because if too much disruptive noise is required to be added to the data to counteract the protecting perturbation, it means that the data has already been corrupted so much that it has lost its utility in the first place.</p><p>We demonstrate this approach for selecting the timestep t * for our IN-1k-32×32 experiments in Table <ref type="table">IX</ref>. As discussed in Section IV-H, for this new experiment we want to use a guided diffusion model (DDPM-IP <ref type="bibr" target="#b42">[43]</ref>) which uses a cosine schedule for sampling. As per our prior experience, we know that an acceptable value for t * using a linear scheduler is 100. As such, we can draw the α t curve for both cases, and find an equivalent t * for the cosine scheduler in DDPM-IP. As shown in Figure <ref type="figure">9</ref>, we can see that in this new case we should set t * = 200 to get an equivalent α t as the one which we previously used for the CIFAR-10 experiments.</p><p>2) Using Reconstruction Quality: Another approach to set a viable value for the diffusion timestep t * is through controlling a desirable reconstruction quality. Recall that the goal of availability attacks is to preserve the normal utility of the data. As such, they usually aim to add imperceptible perturbations to the data. This assumption can help us in selecting a good value for t * . In particular, having a small portion of clean data, we can run the denoising process of AVATAR on these benign data and record a reconstruction Peak-to-Signal-Noise-Ratio (PSNR) for different values of t * . In general, as we move towards larger t * , the PSNR drops. We can set an acceptable level of PSNR value, for example 22dB, to select t * . Beyond that, the PSNR drops so significantly that both the clean and protected data become unreasonably noisy, losing their utility.</p><p>To demonstrate this point through our IN-1k-32×32 experiments in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experimental Results over Different Combination of Availability Attacks</head><p>A scenario that might happen in the real-world is that different classes use a different type of protection. To simulate this scenario, we choose five of the best performing availability attacks, namely CON (C), NTGA (N), TAP (T), REMN (R), and SHR (S), based on our results in Table V to protect four classes of the CIFAR-10 dataset. We create different combinations of these five attacks to protect the four classes, resulting in five distinct combinations which we name CNTR, NTRS, RSCN, SCNT, and TRSC. We use AVATAR to defuse the entire dataset, which includes both protected and unprotected classes. To this end, we use our setting from Section IV-H and use DDPM-IP models pre-trained over the IN-1k-32×32 dataset. We then train RN-18 models over protected and defused data. Our results have been reported as confusion matrices in Figure <ref type="figure">11</ref>. As seen, our model is attack-agnostic and can revive the normal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Experimental Results over Different Perturbation Norms</head><p>Another interesting use-case might happen when different classes use a different perturbation norm to protect their data. We designed an experiment on CIFAR-10 to test this case. For these experiments, we first choose four classes of the CIFAR-10 randomly and aim to protect them with availability attacks. We then use four distinct levels of protection, from ε = 4 to ε = 32, to protect these selected classes. Figure <ref type="figure">10</ref> shows a few samples for each of the availability attacks used in this scenario. Like the previous experiment, again we use our settings from Section IV-H to run these experiments. As seen in the confusion matrices of Figure <ref type="figure">12</ref>, AVATAR performance decreases as we increase the perturbation norm. This is in line with our theoretical insights: to protect the data against AVATAR, we need larger perturbations. However, a larger perturbation means losing the regular utility of the data. Fig. <ref type="figure">10</ref>: Samples from protected CIFAR-10 datasets with four different availability attacks. We have selected four classes to protect, where in each case we use a different perturbation norm to protect the data. The protecting perturbations become extremely visible as we increase their norm.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Synthetic data from diffusion models improves imagenet classification</title>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno>CoRR, abs/2304.08466</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new backdoor attack in CNNs by training set corruption without label poisoning</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Barni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kassem</forename><surname>Kallas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedetta</forename><surname>Tondi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="101" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1467" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="317" to="331" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large image datasets: A pyrrhic win for computer vision?</title>
		<author>
			<persName><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><surname>Prabhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1536" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction</title>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongsu</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12403" to="12412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1310" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Diffusion models in vision: A survey</title>
		<author>
			<persName><surname>Florinel-Alin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Hondru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno>CoRR, abs/2209.04747</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>CoRR, abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">COLLIDER: A robust training framework for backdoor data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Dolatabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><surname>Leckie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the 16th Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="681" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ℓ ∞ -robustness and beyond: Unleashing efficient adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Dolatabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><surname>Leckie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 17th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="467" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 9th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to confuse: Generating training time adversarial data with auto-encoder</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11971" to="11981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Preventing unauthorized use of proprietary data: Poisoning for secure dataset release</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping-Yeh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojtek</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>CoRR, abs/2103.02683</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial examples make strong poisons</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping-Yeh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="30339" to="30351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust unlearnable examples: Protecting data privacy against adversarial learning</title>
		<author>
			<persName><forename type="first">Shaopeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Learning Representations</title>
		<meeting>the 10th International Conference on Learning Representations</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses</title>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chulin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions of Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1563" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">BadNets: Identifying vulnerabilities in the machine learning model supply chain</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
		<idno>CoRR, abs/1708.06733</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The secretive company that might end privacy as we know it</title>
		<author>
			<persName><forename type="first">Kashmir</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How photos of your kids are powering surveillance technology</title>
		<author>
			<persName><forename type="first">Kashmir</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Krolik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unlearnable examples: Making personal data unexploitable</title>
		<author>
			<persName><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Monazam</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 9th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8107" to="8116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding blackbox predictions via influence functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6662" to="6672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 13th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">CMX: cross-modal fusion for RGB-X semantic segmentation with transformers</title>
		<author>
			<persName><forename type="first">Huayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<idno>CoRR, abs/2203.04838</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image shortcut squeezing: Countering perturbative availability poisons with compression</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML)</title>
		<meeting>the 40th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards poisoning of deep learning algorithms with back-gradient optimization</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Muñoz-González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambra</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Paudice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasin</forename><surname>Wongrassamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emil</forename><forename type="middle">C</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (AISec@CCS)</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security (AISec@CCS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Diffusion models for adversarial purification</title>
		<author>
			<persName><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16805" to="16827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Input perturbation reduces exposure bias in diffusion models</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="26245" to="26265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Analysis of discrete and hybrid stochastic systems by nonlinear contraction theory</title>
		<author>
			<persName><forename type="first">Quang-Cuong</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Control, Automation, Robotics and Vision (ICARCV)</title>
		<meeting>the 10th International Conference on Control, Automation, Robotics and Vision (ICARCV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A contraction theory approach to stochastic incremental stability</title>
		<author>
			<persName><forename type="first">Quang-Cuong</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Tabareau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Jacques</forename><forename type="middle">E</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="816" to="820" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DreamFusion: Text-to-3D using 2D diffusion</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 11th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data poisoning won&apos;t save you from facial recognition</title>
		<author>
			<persName><forename type="first">Evani</forename><surname>Radiya-Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 10th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10674" to="10685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Autoregressive perturbations for data poisoning</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Sandoval-Segura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasu</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Just how toxic is data poisoning? A unified benchmark for backdoor and data poisoning attacks</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9389" to="9398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Glaze: Protecting artists from style mimicry by text-to-image models</title>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Security Symposium</title>
		<meeting>the USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2187" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 11th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12438" to="12448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scorebased generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procceding of the 9th International Conference on Learning Representations (ICLR)</title>
		<meeting>ceding of the 9th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2nd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Better safe than sorry: Preventing delusive adversaries with adversarial training</title>
		<author>
			<persName><forename type="first">Lue</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songcan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16209" to="16225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Protecting image data privacy with causal confounder</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Confoundergan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Spectral signatures in backdoor attacks</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8011" to="8021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Better diffusion models further improve adversarial training</title>
		<author>
			<persName><forename type="first">Zekai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML)</title>
		<meeting>the 40th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Stable target field for reduced variance score estimation</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyuan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 11th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Generative poisoning attack method against neural networks</title>
		<author>
			<persName><forename type="first">Chaofei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/1703.01340</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1411.7923</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adversarial purification with score-based generative models</title>
		<author>
			<persName><forename type="first">Jongmin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12062" to="12072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Availability attacks create shortcuts</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2367" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Neural tangent generalization attacks</title>
		<author>
			<persName><forename type="first">Chia-Hung</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan-Hung</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12230" to="12240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Data poisoning attacks against outcome interpretations of predictive models</title>
		<author>
			<persName><forename type="first">Hengtong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2165" to="2173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7472" to="7482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">GMNet: graded-feature multilabellearning network for rgb-thermal urban scene semantic segmentation</title>
		<author>
			<persName><forename type="first">Wujie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsheng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7790" to="7802" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
