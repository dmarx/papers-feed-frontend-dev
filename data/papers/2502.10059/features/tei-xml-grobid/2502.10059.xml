<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-14">14 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guangcong</forename><surname>Zheng</surname></persName>
							<email>guangcongzheng@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Shuigenzhan</roleName><forename type="first">Rui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yehao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yining</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-14">14 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">154F4FD9F6BCEE2386EEDAF9C7C5083C</idno>
					<idno type="arXiv">arXiv:2502.10059v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-26T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolation</head><p>Rotate Complex Trajectory Close-up Loop Transition RealCam -I2V Figure 1. We propose RealCam-I2V, a camera controllable image-to-video generation framework for complex real-world camera control and extra applications including camera-controlled loop video generation, generative frame interpolation, and smooth scene transitions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advancements in image-to-video generation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b66">66]</ref> have significantly improved controllability over synthesized videos. However, challenges remain in achieving realistic, controllable camera movement within com- plex real-world scenes. Text-based camera-control methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">50]</ref>, like traditional diffusion-based video generation, are intuitive and straightforward but lack precision in explicit control over camera parameters, such as angle, scale, and movement direction. This limitation has spurred the development of camera-trajectory-guided approaches, which attempt to address these issues by offering finer control over camera movement.</p><p>Current camera-trajectory-guided methods typically rely on relative camera trajectories, as seen in models like Mo-tionCtrl <ref type="bibr" target="#b52">[53]</ref>, CameraCtrl <ref type="bibr" target="#b15">[16]</ref>, CamCo <ref type="bibr" target="#b62">[62]</ref>, and CamI2V <ref type="bibr" target="#b77">[77]</ref>. While these approaches provide more control than text-based models, they are fundamentally limited by their reliance on relative scale trajectories. Training on relative scales results in inconsistencies when applied to real-world scenes, where absolute scale is crucial for realistic depth perception. Additionally, without access to depth information, users find it challenging to draw precise trajectories, making these methods difficult to use effectively.</p><p>To overcome these limitations, we propose RealCam-I2V, a video generation framework that integrates monocular depth estimation as a preprocessing step to construct a robust, absolute-scale 3D scene. Our approach leverages the Depth Anything v2 <ref type="bibr" target="#b64">[64]</ref> model to predict the metric depth of a user-provided reference image, reprojecting its pixels back into camera space to create a stable 3D representation. This 3D scene serves as the foundation for camera control, providing a consistent and absolute scale that is critical for real-world applications.</p><p>In the training stage, we align the reconstructed 3D scene of the reference image with the point cloud of each video sample, reconstructed using COLMAP <ref type="bibr" target="#b39">[40]</ref>, a structurefrom-motion (SfM) method. This alignment allows us to rescale COLMAP-annotated camera parameters to the Depth Anything metric, providing an absolute, stable, and robust scale across training data. By aligning relative-scale camera parameters to absolute scales, we can condition the video generation model on accurately scaled camera trajectories, achieving greater control and scene consistency across diverse real-world images.</p><p>During inference, RealCam-I2V provides an interactive interface where users can intuitively design camera trajectories by drawing within the reconstructed 3D scene of the reference image. This interface renders preview videos of the trajectory in a static scene, offering users real-time feedback and greater control over camera movement. This interactive feature enhances usability, allowing precise trajectory control even for users without specialized knowledge of scene depth. To further improve video quality and control precision, we introduce scene-constrained noise initialization as a mechanism to shape the generation process in its high-noise stages. By using the preview video of the static 3D scene, RealCam-I2V injects scene-visible regions with controlled noise, guiding the video diffusion model's early generation stages. This high-noise feature constrains the initial layout and camera dynamics, providing a strong foundation for the remaining denoising stages. As denoising progresses, the condition-based approach, trained on absolute-scale camera trajectories, preserves global layout and completes the dynamic scene in previously unseen regions. This approach maintains the video diffusion model's capacity for dynamic content generation while ensuring accurate, coherent camera control.</p><p>Our experimental results show that RealCam-I2V achieves significant performance gains in video quality and controllability. When relative scales are aligned to absolute scales, models such as MotionCtrl, CameraCtrl, and CamI2V see substantial improvements in video quality. Furthermore, with the introduction of scene-constrained noise initialization, RealCam-I2V surpasses state-of-theart performance benchmarks, particularly on datasets like RealEstate10K <ref type="bibr" target="#b78">[78]</ref> and out-of-domain images. These results demonstrate the effectiveness of our approach in both controlled and diverse real-world settings. In summary, our contributions are as follows:</p><p>• We identify scale inconsistencies and real-world usability challenges in existing trajectory-based methods and introduce a simple yet effective monocular 3D reconstruction into the preprocessing step of the generation pipeline, serving as a reliable intermediary reference for both training and inference. • With reconstructed 3D scene, we enable absolute-scale training and provide an interactive interface during inference to easily design camera trajectories with preview feedback, along with proposed scene-constrained noise shaping to significantly enhance scene consistency and camera controllability. • Our method overcomes critical real-world application challenges and achieves substantial improvements on the RealEstate10K dataset, establishing a new sota benchmark both in video quality and control precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Diffusion-based Video Generation. The advancement of diffusion models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b75">75]</ref> has led to significant progress in video generation. Due to the scarcity of high-quality video-text datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, researchers have adapted existing text-to-image (T2I) models to facilitate text-to-video (T2V) generation. Notable examples include AnimateDiff <ref type="bibr" target="#b13">[14]</ref>, Align your Latents <ref type="bibr" target="#b2">[3]</ref>, PYoCo <ref type="bibr" target="#b10">[11]</ref>, and Emu Video <ref type="bibr" target="#b11">[12]</ref>. Further advancements, such as LVDM <ref type="bibr" target="#b17">[18]</ref>, VideoCrafter <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, ModelScope <ref type="bibr" target="#b46">[47]</ref>, LAVIE <ref type="bibr" target="#b50">[51]</ref>, and VideoFactory <ref type="bibr" target="#b48">[49]</ref>, have refined these approaches by fine-tuning both spatial and temporal blocks, leveraging T2I models for initialization to improve video quality. Recently, Sora <ref type="bibr" target="#b3">[4]</ref> and CogVideoX <ref type="bibr" target="#b66">[66]</ref> enhance video generation by introducing Transformer-based diffusion backbones <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b69">69]</ref> and utilizing 3D-VAE, unlocking the potential for realistic world simulators. Additionally, SVD <ref type="bibr" target="#b1">[2]</ref>, SEINE <ref type="bibr" target="#b8">[9]</ref>, Pix-elDance <ref type="bibr" target="#b70">[70]</ref> and PIA <ref type="bibr" target="#b74">[74]</ref> have made significant strides in image-to-video generation, achieving notable improve-ments in quality and flexibility. Further, I2VGen-XL <ref type="bibr" target="#b73">[73]</ref>, DynamicCrafter <ref type="bibr" target="#b59">[59]</ref>, and Moonshot <ref type="bibr" target="#b71">[71]</ref> incorporate additional cross-attention layers to strengthen conditional signals during generation. Controllable Generation. Controllable generation has become a central focus in both image <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b76">76]</ref> and video <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b71">71]</ref> generation, enabling users to direct the output through various types of control. A wide range of controllable inputs has been explored, including text descriptions, pose <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b63">63]</ref>, audio <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, identity representations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">57]</ref>, trajectory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b68">68]</ref>.</p><p>Text-based Camera Control. Text-based camera control methods use natural language descriptions to guide camera motion in video generation. AnimateDiff <ref type="bibr" target="#b13">[14]</ref> and SVD <ref type="bibr" target="#b1">[2]</ref> fine-tune LoRAs <ref type="bibr" target="#b19">[20]</ref> for specific camera movements based on text input. Image conductor <ref type="bibr" target="#b27">[28]</ref> proposed to separate different camera and object motions through camera LoRA weight and object LoRA weight to achieve more precise motion control. In contrast, MotionMaster <ref type="bibr" target="#b21">[22]</ref> and Peekaboo <ref type="bibr" target="#b23">[24]</ref> offer training-free approaches for generating coarse-grained camera motions, though with limited precision. VideoComposer <ref type="bibr" target="#b49">[50]</ref> adjusts pixel-level motion vectors to provide finer control, but challenges remain in achieving precise camera control.</p><p>Trajectory-based Camera Control. MotionCtrl <ref type="bibr" target="#b52">[53]</ref>, CameraCtrl <ref type="bibr" target="#b15">[16]</ref>, and Direct-a-Video <ref type="bibr" target="#b65">[65]</ref> use camera pose as input to enhance control, while CVD <ref type="bibr" target="#b26">[27]</ref> extends Cam-eraCtrl for multi-view generation, though still limited by motion complexity. To improve geometric consistency, Pose-guided diffusion <ref type="bibr" target="#b44">[45]</ref>, CamCo <ref type="bibr" target="#b61">[61]</ref>, and CamI2V <ref type="bibr" target="#b77">[77]</ref> apply epipolar constraints for consistent viewpoints. VD3D <ref type="bibr" target="#b0">[1]</ref> introduces a ControlNet <ref type="bibr" target="#b72">[72]</ref>-like conditioning mechanism with spatiotemporal camera embeddings, enabling more precise control. CamTrol <ref type="bibr" target="#b18">[19]</ref> offers a training-free approach that renders static point clouds into multi-view frames for video generation. Cavia <ref type="bibr" target="#b60">[60]</ref> introduces viewintegrated attention mechanisms to improve viewpoint and temporal consistency, while I2VControl-Camera <ref type="bibr" target="#b9">[10]</ref> refines camera movement by employing point trajectories in the camera coordinate system. Despite these advancements, challenges in maintaining camera control and scene-scale consistency remain, which our method seeks to address. It is noted that 4Dim <ref type="bibr" target="#b54">[54]</ref> introduces absolute scale but in 4D novel view synthesis (NVS) of scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Metric Depth Estimation for 3D Reconstrcution</head><p>To obtain a depth map from a given input image, we use a metric depth predictor f depth , which takes the RGB image I as input and outputs the corresponding depth map D(u, v).  The prediction process is formulated as:</p><formula xml:id="formula_0">D(u, v) = f depth (I),</formula><p>where I is the input RGB image and D(u, v) is the predicted depth value for each pixel at coordinates (u, v). This predicted depth map D(u, v) serves as the foundation for projecting the image into 3D space, allowing us to construct a point cloud in the camera coordinate system. The camera intrinsics matrix K is defined as:</p><formula xml:id="formula_1">K =   f x 0 c x 0 f y c y 0 0 1   ,</formula><p>where f x and f y are the focal lengths along the x and y axes, (c x , c y ) is the principal point of the camera. Given a depth map D(u, v), the projected 3D coordinates in the camera coordinate system, p c = (x c , y c , z c ) T , are computed as:</p><formula xml:id="formula_2">p c = D(u, v) • K -1 •   u v 1   .</formula><p>Here u, v, 1 represents the homogeneous coordinates of the pixel, K -1 is the inverse of the intrinsic matrix, which maps pixel coordinates to normalized image coordinates. By applying this transformation to all pixels in the depth map, we obtain a set of 3D points {p c } in the camera coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Absolute-Scale Training</head><p>Camera-controlled Image-to-Video Model. Instead of directly modeling the video x, the latent representation z = E(x) is used for training. The diffusion model ϵ θ learns to estimate the noise ϵ added at each timestep t, conditioned on both a text prompt c txt , a reference image c img , and camera condition c cam , with t ∈ U(0, 1). The training objective simplifies to a reconstruction loss defined as:</p><formula xml:id="formula_3">L = E z,ctxt,cimg,ccam,ϵ,t ∥ϵ -ϵ θ (z t , c txt , c img , c cam , t)∥ 2 2 ,<label>(1)</label></formula><p>where z ∈ R F ×H×W ×C represents the latent code of a video, with F, H, W, C corresponding to frame count, height, width, and channel dimensions.</p><p>The noise-corrupted latent code z t , derived from the ground-truth latent z 0 , is expressed as:</p><formula xml:id="formula_4">z t = α t z 0 + σ t ϵ,<label>(2)</label></formula><p>where σ t = 1 -α 2 t . Here, α t and σ t are hyperparameters governing the diffusion process. Aligning from Relative Scale to Absolute Scale. To convert camera extrinsics from world-to-camera to an absolutescale camera-to-world representation, we defines that the world-to-camera extrinsics matrix F w2c ∈ R 4×4 is inverted to obtain the corresponding camera-to-world matrix:</p><formula xml:id="formula_5">F c2w = F -1 w2c .</formula><p>To express the transformations relative to the first frame, each F c2w is left-multiplied by the camera-to-world matrix of the inverse of first frame F c2w, 1 :</p><formula xml:id="formula_6">c cam = F -1 c2w,1 • F c2w .</formula><p>Here, c cam ∈ R F ×4×4 represents the camera-to-world transformations aligned relative to the first frame. However, the translation component of c cam remains in a relative scene scale. To convert the relative translation to an absolute scale, we align the metric 3D point cloud reconstructed by Depth Anything with the 3D point cloud reconstructed by COLMAP (Structure-from-Motion), as shown in Fig. <ref type="figure" target="#fig_2">4</ref>.</p><p>The alignment process yields a scale factor a and is applied to the translation component of c cam , resulting in an absolute-scale camera-to-world transformation:</p><formula xml:id="formula_7">c abs cam = R a • T 0 1 ,</formula><p>where R is the rotation matrix, T is the relative translation vector. The resulting c abs cam ∈ R F ×4×4 represents the camera-to-world transformations with absolute scene scale, enabling robust and accurate real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scene-Constrained Noise Shaping</head><p>Inspired by SDEdit <ref type="bibr" target="#b31">[32]</ref> and DDIM <ref type="bibr" target="#b40">[41]</ref> inversion, noised features z t can be used for shaping the layout, camera control of the entire image, especially at timestep with highlevel noise. We propose scene-constrained noise shaping, which utilizes preview videos generated along user-defined trajectories in the interactive 3D scene. Each frame of the preview video is treated as a reference frame and provided to the generation process during the high-noise stage. The reference frame's pixels are overlaid onto the modelpredicted z 0 to achieve the shaping effect.</p><p>Next, we detail the process for selecting the pixels to be referenced. As illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>, the primary criterion is that a pixel must be visible under the current camera viewpoint in the preview video. To mitigate issues such as holes where they are pasted onto the clean part (predicted z0) of a noised latent zt, typically at high noise levels 0.9 &lt; t &lt; 1.0 is enough for camera control and maintain dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Mode</head><p>Interpolation Mode caused by inaccurate depth predictions, we apply an additional filtering rule: if a visible pixel's k × k neighborhood contains any invisible pixels, it is considered to lie on an object's edge and potentially affected by depth prediction errors. Such pixels are excluded from selection. Finally, we define the noise shaping process with the following formula:</p><formula xml:id="formula_8">… !! " !! # !! $ ! ! %&amp;" ! ! % … !' " !' " !' " !' " !' " … !! " !! # !! $ ! ! %&amp;" ! ! % … !' " !' " !' " !' " ! ' % … !! " !! # !! $ ! ! %&amp;" ! ! % … !' " !' # !' $ !' $ !' $ Continuation Mode ! ! ∶ B, C, F, H, W ! " ∶ B, C, F, H, W concatenation B, 2×C, F, H, W</formula><formula xml:id="formula_9">z predict = mask • z preview + (1 -mask) • z predict ,</formula><p>where the mask identifies the selected reference pixels, z preview represents the latent features from the preview video, and z predict is the model-predicted latent representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Interploatation, Loop and Continueation</head><p>To support different tasks, including interpolation, looping, and continuation for long video generation, we train video diffusion model with different input concatenation mode, as shown in Fig. <ref type="figure" target="#fig_4">6</ref>. Given a video latents z ∈ R F ×H×W ×C , we define the noised latents of f -th frame at timestep t as z f t . We then select i-th clean frame as the condition frame z i 0 . For interpolation mode, we define z f -1 0 as the end condition frame. For continuation mode, we define all 1 i-th as condition frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Dataset. We train our model on RealEstate10K <ref type="bibr" target="#b78">[78]</ref>, which contains ∼ 70, 000 video clips with well-annotated camera poses. For metric depth alignment of absolute scene scale, we run COLMAP <ref type="bibr" target="#b39">[40]</ref> point triangulator on each video clip with fixed camera intrinsics and extrinsics directly from RealEstate10K, obtaining the sparse point cloud of the reconstructed scene. We then calculate per-point depth scale against the metric depth from depth predictor. We term the median value of per-point depth scales in a frame as the frame-level depth scale. To make stable the training, we discard outliers of video clip whose maximum frame-level depth scale of the whole scene is among the top 2% for too small values or the last 2% for too large values, assuming sorted in ascending order. The same quantile filtering strategy is also applied on the minimum frame-level depth scales of video clips. It remains 58, 000 video clips for training and another 6, 000 for test. During training, we follow Dy-namiCrafter to sample 16 frames from each single video clip while perform resizing, keeping the aspect ratio, and center cropping to fit in our training scheme. We train the model with a random frame stride ranging from 1 to 10 and take random condition frame as data augmentation. We fix the frame stride to 8 and always use the first frame as the condition frame for inference. Implementation Details. We choose DynamiCrafter <ref type="bibr" target="#b59">[59]</ref> as our image-to-video (I2V) base model and seamlessly integrate proposed RealCam-I2V into it as a plugin. For metric depth predictor, we choose Depth Anything V2 <ref type="bibr" target="#b64">[64]</ref> Large Indoor, which is fine-tuned on metric depth estimation. During depth-aligned training, we freeze all parameters of the base model and the depth predictor, while only parameters of proposed method are trainable. More details are listed in dataset section of Appendix. We supervise ϵ-prediction on the model of 256 × 256 resolution and vprediction on the model of 512×320 resolution respectively, following the pre-training scheme of DynamiCrafter. We apply the Adam optimizer with a constant learning rate of 1 × 10 -4 with mixed-precision fp16 and DeepSpeed ZeRO-1. We train proposed method and variants on 8 NVIDIA H100 GPUs with an effective batch size of 64 for 50, 000 steps. More details are listed in implementation section of Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>We follow previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b77">77]</ref> to evaluate camera-controllablity by RotErr, TransErr and CamMC on their estimated camera poses using structure-from-motion (SfM) methods, e.g. COLMAP <ref type="bibr" target="#b39">[40]</ref> and GLOMAP <ref type="bibr" target="#b34">[35]</ref>. We convert the camera pose of each frame in a video clip to be relative to the first frame as canonicalization. We denote the i-th frame relative camera-to-world matrix of ground truth as {R 3×3 i , T 3×1 i }, and that of generated video as { R3×3 i , T 3×1 i }. We randomly select 1, 000 samples from test set for evaluation. We sum up per-frame errors as the scene-level result for camera metrics. Inspired by Zheng et al. <ref type="bibr" target="#b77">[77]</ref>, we repetitively conduct 5 individual trials on each video clips for camera-control metrics to reduce the randomness introduced by SfM tools. Metrics of one video clip are averaged on successful trials at first for later samplewise average to get final results. RotError. We calculate camera rotation errors by the relative angle between generated videos and ground truths in radians for rotation accuracy.</p><formula xml:id="formula_10">RotErr = n i=1 arccos tr( Ri R T i ) -1 2<label>(3)</label></formula><p>TransError. For relative TransErr, we perform scene scale normalization on the camera positions of each video clip. The scene scale of generated video si and grouth truth s i are individually calculated as the L 2 distance from the first camera to the farthest one for each video clip. For absolute TransErr, we normalize both the video clip to the scene scale of ground truth video, i.e. si = s i .</p><formula xml:id="formula_11">TransErr = n i=1 Ti si - T i s i 2<label>(4)</label></formula><p>CamMC. We perform the same scene scale normalization for relative metrics and absolute metrics as TransError, and evaluate the overall camera pose accuracy by directly calculating L 2 similarity on camera-to-world matrices.</p><formula xml:id="formula_12">CamMC = n i=1 Ri Ti si 3×4 -R i T i s i 3×4 2<label>(5)</label></formula><p>FVD. We also assess the visual quality of generative videos by the distribution distance FVD <ref type="bibr" target="#b45">[46]</ref> between generated videos and ground-truths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with SOTA Methods</head><p>We compare our proposed method against models that either lack camera-condition training (DynamiCrafter <ref type="bibr" target="#b59">[59]</ref>) or incorporate camera-condition training, namely Dynami-Crafter+MotionCtrl <ref type="bibr" target="#b52">[53]</ref> (3 × 4 camera extrinsics), Dynam-iCrafter+CameraCtrl <ref type="bibr" target="#b15">[16]</ref>  Table <ref type="table">2</ref>. Evaluation results on Vbench-I2V <ref type="bibr" target="#b22">[23]</ref>, a widely used benchmark suite with dynamic scenes and various types. Due to the efficient frozen parameters finetuning on Dynamicrafter, our method obtains the ability of camera control but decreases little in other metrics despite only training on static RealEstate10K.</p><p>from camera extrinsics and intrinsics as side input), and DynamiCrafter+CamI2V <ref type="bibr" target="#b77">[77]</ref> (current SOTA using Plücker embedding and epipolar attention between all frames), as shown in Tab. 1.</p><p>Our method demonstrates significant improvements in visual quality (FVD) and camera control metrics (TransErr, RotErr, CamMC), particularly on absolute metrics. Specifically, absolute camera metrics improve by +27%, relative camera metrics by +14%, and FVD by +13%. However, these improvements are not fully captured by the RealEstate10K dataset, which has limitations on movement speed and contains mostly static scenes. It's strongly recommended to view the dynamic visualizations in the supplementary materials for a more comprehensive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Effect of absolute-scale training only. As shown in Tab. 3, compared to relative-scale training, absolute-scale training yields notable improvements, especially on absolute metrics. It implies that models trained on absolute-scale data can more accurately capture true-to-scale translations and better understand camera rotations within a realistic spatial framework. The absolute scene scale enhances robustness and compatibility, ensuring that the framework adapts effectively to real-world images and applications. This approach allows for interaction within a unified scale, enabling intuitive user control over camera actions. Effect of absolute-scale training + scene-constrained noise shaping. Adding scene-constrained noise shaping to a model trained with absolute-scale yields substantial gains in video quality and camera controllability. This improvement is evident across both camera metrics and FVD. The synergy of absolute-scale training and sceneconstrained noise shaping ensures robust and precise control in diverse scenarios. As illustrated in Fig. <ref type="figure" target="#fig_5">7</ref>, this combined approach delivers noticeably better dynamics compared to using scene-constrained noise shaping alone. Large camera movements, rotations, and rapid transitions, which previously struggled to maintain consistency and realism, now work seamlessly. This improvement underscores the strength of integrating absolute-scale training with noise  shaping for complex motion scenarios.</p><p>Effect of scene-constrained noise shaping only. As shown in Tab. 3, scene-constrained noise shaping can be used as the sole method for camera control when applied to a base model not trained with any camera conditions. It provides notable improvements in metrics, exemplified by nearly 50% reduction on DynamiCrafter. However, this method underperforms compared to the combined method with absolute-scale training. It also introduces challenges in parameter selection. Applying shaping only in the highnoise phase limits camera contrl in lower noise stages, while extending shaping to mid-noise phase can suppress dynamic elements, resulting in static video output. This limitation affects the fluidity and responsiveness of generated camera movements, making the combination approach preferable for applications requiring natural dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation Zoom in+Transition</head><p>Close-up </p><note type="other">Complex Trace</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Application</head><p>As illustrated in Fig. <ref type="figure" target="#fig_7">9</ref>, we demonstrate the versatility of our method through visualization results across various applications, including videos generated at resolutions of 512×320 and 1024 × 576 with camera control under complex scenarios, such as large movements or rotations. Additionally, our results include camera-controlled loop video generation, generative frame interpolation, and smooth scene transitions, highlighting the robustness of our approach. These visualizations showcase two major breakthroughs: first, our method achieves a real-world application breakthrough by addressing challenges like training-inference scale inconsistency and low usability, ensuring improved robustness and compatibility with real-world images. Second, our framework exhibits superior performance in complex camera motions, handling large and rapid movements, rotations, and dynamics more effectively than existing methods. More ex-tensive results are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitation Analysis and Future Work</head><p>The model was trained on datasets such as RealEstate10K, which consists primarily of real-world, indoor and outdoor videos collected from YouTube. This dataset's content focuses heavily on realistic, static scenes, resulting in a model that excels in these contexts but performs less effectively when applied to scenes with significantly different visual styles, such as anime, oil paintings, or cartoon-like aesthetics. Better data quality, designing algorithm or improving the ability of fundamental model especially in long video generation will be considered into the future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Potential Negative Societal Impacts</head><p>The image-to-video generation technology developed in this work, with its enhanced camera controllability and breakthrough in real-world applications holds the potential for misuse, particularly in the creation of falsified or deceptive video content. The ability to precisely control camera movements and generate realistic sequences from single images could be exploited to produce convincing yet fabricated videos, leading to ethical concerns around misinformation and privacy violations. To mitigate these risks, we advocate for responsible usage and adherence to ethical guidelines when deploying the RealCam-I2V model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we address the scale inconsistencies and real-world usability challenges in existing trajectory-based camera-controlled image-to-video generation methods. We introduce a simple yet effective monocular 3D reconstruction into the preprocessing step of the generation pipeline, serving as a reliable intermediary reference for both training and inference. With reconstructed 3D scene, we enable absolute-scale training and provide an interactive interface during inference to easily design camera trajectories with preview feedback, along with proposed scene-constrained noise shaping to significantly enhance scene consistency and camera controllability. Our method overcomes critical real-world application challenges and achieves substantial improvements on the RealEstate10K dataset, establishing a new sota both in video quality and control precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>We choose DynamiCrafter<ref type="foot" target="#foot_2">foot_2</ref>  <ref type="bibr" target="#b59">[59]</ref> as our image-to-video (I2V) base model. We trained proposed method on 4 publicly accessible variants of DynamiCrafter, namely 256, 512, 512 interp and 1024. We conduct ablation study on resolution 256 × 256, due to the limitation of computing resource. For resolution 256 × 256, we train all models on ϵ-prediction with effective batch size 64 on 8 NVIDIA H100 GPUs for 50, 000 steps, taking about 25 hours. For resolution 512 × 320 and 1024 × 576, we train RealCam-I2V on v-prediction while enable perframe ae and gradient checkpoint to reduce peak GPU memory consumption. We apply the Adam optimizer with a constant learning rate of 1 × 10 -4 with mixed-precision fp16 and DeepSpeed ZeRO-1.</p><p>For MotionCtrl <ref type="bibr" target="#b52">[53]</ref> and CameraCtrl <ref type="bibr" target="#b15">[16]</ref>, we reproduce all results on DynamiCrafter for fair comparison. For CamI2V <ref type="bibr" target="#b77">[77]</ref>, we implement hard mask epipolar attention and set 2 register tokens, aligned with the original paper. In quantitative comparison and ablation study, we set fixed text image CFG to 7.5 and camera CFG to 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Camera Keyframe Interpolation</head><p>In real-world applications, user-provided camera trajectories often consist of a limited number of keyframes (e.g., 4 keyframes). To ensure smooth and continuous motion across the trajectory while adhering to the user's input, we perform linear interpolation in SE(3) space to expand the trajectory to a higher number of frames (e.g., 16 interpolated frames), as shown in Fig. <ref type="figure" target="#fig_8">10</ref>. This step ensures that our model generates consistent and visually coherent videos without compromising the accuracy of user-defined camera movements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparision between text, relative trajectory, and absolute trajectory based camera-controlled image-to-video generation methods on aspects of camera control precision and usability.</figDesc><graphic coords="2,380.44,182.97,100.76,89.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Pipeline. In training, we align camera parameters in RealEstate10K from relative scale to absolute scale. In inference, we use metric depth estimation method to construct a 3d point cloud for users to interactively drawing camera traces.</figDesc><graphic coords="4,209.25,289.72,343.48,103.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. 3D point cloud reconstructed from metric depth estimation (RGB) is robust and unified, whereas the SFM-based reconstruction by methods like COLMAP (Yellow) used in RealEstate10K annotations is in relative scale and may vary across images. Aligning these two 3D scenes enables the transformation from relative to absolute scale (real-world scale).</figDesc><graphic coords="4,60.04,293.47,149.21,108.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Pixels selected for scene-constrained noise shaping, where they are pasted onto the clean part (predicted z0) of a noised latent zt, typically at high noise levels 0.9 &lt; t &lt; 1.0 is enough for camera control and maintain dynamics.</figDesc><graphic coords="5,330.64,157.21,108.59,68.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Concatenation for different tasks, including basic mode, interpolation mode, and continuation mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (a) Without Scene-Constrained Noise Shaping. (b) With Scene-Constrained Noise Shaping. The left shows failures in large movements without scene-constrained noise shaping, while the right illustrates a loss of dynamics when noise shaping is extended to lower noise stages.</figDesc><graphic coords="7,317.25,382.13,236.24,72.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Without kernel size≥3 in noise shaping, unvisible regions will be wrongly pasted to the generated video.</figDesc><graphic coords="8,58.50,281.26,236.24,156.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visualization of various applications. Best viewed as dynamic videos in the supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Camera Trajectory Interpolation.choose Large as the model size, which has 335.3M parameters, and the indoor version. The scene scale of our model is aligned to the metric depth space of Depth Anything V2 Large Indoor, i.e. absolute scene scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with SOTA methods. * denotes the results we reproduced using DynamiCrafter as base I2V model. Our method achieves the state-of-the-art performance on both relative and absolute camera-controllable metrics, while coherently improve visual quality of generated videos, witnessed by a further drop of FVD. Best and second best results are highlighted respectively. We observe nearly +30% improvement on absolute metrics while over +10% improvement on relative metrics and FVD.</figDesc><table><row><cell>(Plücker embedding constructed</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation study. * denotes the results we reproduced using DynamiCrafter as base I2V model. Absolute scene-scale training resolves scale inconsistencies for real-world applications and its improvement on relative metrics indicates a more stable and unified camera control for video generation. Scene-constrained noise shaping can provides substantial improvements in dynamics and large camearabut is less effective than the combined approach, struggling with parameter tuning and dynamic consistency in lower noise stages. Best and second best results are highlighted respectively.</figDesc><table><row><cell>Baseline</cell><cell cols="2">RealCam-I2V Plugin Absolute-Scale Scene-Constrained Noise Shaping</cell><cell>RotErr ↓</cell><cell>TransErr ↓ Rel. Abs.</cell><cell>CamMC ↓ Rel. Abs.</cell><cell>FVD ↓ VideoGPT StyleGAN</cell></row><row><cell>DynamiCrafter  *  [53]</cell><cell></cell><cell>✓</cell><cell>3.3415 1.5163</cell><cell cols="2">9.8024 14.135 11.625 15.726 6.6392 8.4607 7.2108 8.9505</cell><cell>106.02 71.942</cell><cell>92.196 65.014</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.0527</cell><cell cols="2">2.2860 6.8182 2.9312 7.2272</cell><cell>70.292</cell><cell>60.845</cell></row><row><cell>+ MotionCtrl  *  [53]</cell><cell>✓</cell><cell></cell><cell>0.8655</cell><cell cols="2">2.3342 4.2218 2.8083 4.5984</cell><cell>67.130</cell><cell>58.311</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>0.6373</cell><cell cols="2">2.0725 3.2308 2.3771 3.4721</cell><cell>58.885</cell><cell>50.111</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.7373</cell><cell cols="2">1.7619 5.5090 2.1644 5.7648</cell><cell>69.202</cell><cell>58.900</cell></row><row><cell>+ CameraCtrl  *  [16]</cell><cell>✓</cell><cell></cell><cell>0.7042</cell><cell cols="2">1.9477 3.8218 2.3007 4.0829</cell><cell>60.314</cell><cell>51.918</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>0.5436</cell><cell cols="2">1.7954 3.1845 2.0336 3.3620</cell><cell>55.004</cell><cell>46.702</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.4968</cell><cell cols="2">1.4853 3.4069 1.7253 3.5786</cell><cell>63.869</cell><cell>55.276</cell></row><row><cell>+ CamI2V  *  [77]</cell><cell>✓</cell><cell></cell><cell>0.4596</cell><cell cols="2">1.4109 3.0925 1.6282 3.2411</cell><cell>64.451</cell><cell>55.313</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>0.4052</cell><cell cols="2">1.3087 2.4709 1.4869 2.6095</cell><cell>55.229</cell><cell>48.080</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://google.github.io/realestate10k/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https : / / github . com / DepthAnything / Depth -Anything-V2/tree/main/metric_depth</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/Doubiiu/DynamiCrafter</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RealCam-I2V: Real-World Image-to-Video Generation with</head><p>Interactive Complex Camera Control</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We highly recommend that reviewers refer to index.html provided in our supplementary files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The camera trajectory of each video clip from RealEstate10K 1 <ref type="bibr" target="#b78">[78]</ref> is first derived by SLAM methods at lower resolution with the field of view fixed at 90 • . The authors then refine each of camera sequence using a structure-from-motion (SfM) pipeline, performing feature extraction, feature matching and global bundle adjustment successively. Given the unawareness of global scene scale, the resulted camera poses of RealEstate10K are up to an arbitrary scale per clip. For each frame the authors compute the 5-th percentile depth among all point depths from that frame's camera. Computing this depth across all cameras in a video clip gives a set of near plane depths and the whole scene is scaled so that the 10-th percentile of this set of depths is 1.25m.</p><p>While using RealEstate10K's scenes and camera trajectories during inference avoids scale issues within the dataset, challenges arise in more general cases. Specifically, when pairing out-of-domain images with either in-domain or out-of-domain trajectories, the inconsistencies between training and inference scales become evident. These inconsistencies make it impossible to generate realistic and controllable videos.</p><p>The solution lies in reconstructing an absolute-scale scene for any given image. By leveraging metric depth predictor, we can reconstruct the absolute-scale 3D scene for the reference image. This absolute-scale scene bridges the gap between training and inference, enabling robust generalization to real-world applications. With this alignment, the model becomes capable of handling diverse combinations of images and trajectories, ensuring consistent and reliable performance across various scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Predictor</head><p>We choose the metric depth version of Depth Anything V2 2 <ref type="bibr" target="#b64">[64]</ref> as the metric depth predictor. Compared to their basic versions, the authors fine-tune the pre-trained encoder on synthetic datasets for indoor and outdoor metric depth estimation. The indoor model is capable of monocular metric depth estimation within a maximum depth of 20m. We</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Sherwin</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vasilkovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.12781</idno>
		<title level="m">Vd3d: Taming large video diffusion transformers for 3d camera control</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Align your latents: High-resolution video synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22563" to="22575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Stillmoving: Customized video generation without customized video data</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiran</forename><surname>Zada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roni</forename><surname>Paiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08674</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Motionzero: Zero-shot moving object control framework for diffusion-based video generation</title>
		<author>
			<persName><forename type="first">Changgu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianggangxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaoqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.10150</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Videocrafter1: Open diffusion models for high-quality video generation</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoshu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.19512</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Videocrafter2: Overcoming data limitations for highquality video diffusion models</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seine: Shortto-long video diffusion model for generative transition and prediction</title>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Siyu Zhou, and Qian He. I2vcontrol-camera: Precise video camera control with adjustable motion strength</title>
		<author>
			<persName><forename type="first">Wanquan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengqi</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.06525</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Preserve your own correlation: A noise prior for video diffusion models</title>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Songwei Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Balaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22930" to="22941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Emu video: Factorizing text-to-video generation by explicit image conditioning</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akbar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10709</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Atomovideo: High fidelity image-to-video generation</title>
		<author>
			<persName><forename type="first">Litong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01800</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04725</idno>
		<imprint>
			<date type="published" when="2023">2023. 1, 2, 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparsectrl: Adding sparse controls to text-to-video diffusion models</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="330" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cameractrl: Enabling camera control for text-to-video generation</title>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Hao He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.02101</idno>
		<imprint>
			<date type="published" when="2024">2024. 2, 3, 6, 7, 8, 1</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-speech gesture video generation via motion-decoupled diffusion model</title>
		<author>
			<persName><forename type="first">Qiaochu</forename><surname>Xu He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhensong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songcen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2263" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Latent video diffusion models for high-fidelity long video generation</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13221</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Training-free camera control for video generation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10126</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Animate anyone: Consistent and controllable image-to-video synthesis for character animation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8153" to="8163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Motionmaster: Training-free camera motion transfer for video generation</title>
		<author>
			<persName><forename type="first">Teng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yating</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.15789</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Vbench++: Comprehensive and versatile benchmark suite for video generative models</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nattapol</forename><surname>Chanpaisit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.13503</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via masked-diffusion</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Nasery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of multimodal controllable diffusion models</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang-Cong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian-Rui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing-Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="509" to="541" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Videobooth: Diffusion-based video generation with image prompts</title>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6689" to="6700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Collaborative video diffusion: Consistent multi-video generation with camera control</title>
		<author>
			<persName><forename type="first">Zhengfei</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17414</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Image conductor: Precision control for interactive video synthesis</title>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.15339</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generative image dynamics</title>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="24142" to="24153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Latte: Latent diffusion transformer for video generation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gengyun</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03048</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Follow your pose: Pose-guided text-to-video generation using pose-free videos</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4117" to="4125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">Sdedit: Guided image synthesis and editing with stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4296" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Koichi</forename><surname>Namekata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherwin</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gilitschenski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.04989</idno>
		<title level="m">Sgi2v: Self-guided trajectory control in image-to-video generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Global Structure-from-Motion Revisited</title>
		<author>
			<persName><forename type="first">Linfei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">Lutz</forename><surname>Schönberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Controlnext: Powerful and efficient control for image and video generation</title>
		<author>
			<persName><forename type="first">Bohao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuechen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06070</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Moma: Multimodal llm adapter for fast personalized image generation</title>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.05674</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Any-to-any generation via composable diffusion</title>
		<author>
			<persName><forename type="first">Zineng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions</title>
		<author>
			<persName><forename type="first">Linrui</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17485</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Consistent view synthesis with pose-guided diffusion models</title>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhib</forename><surname>Alsisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="16773" to="16783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06571</idno>
		<title level="m">Modelscope text-to-video technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Disentangled control for referring human dance generation in real world. arXiv eprints</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">2307</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation</title>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixi</forename><surname>Tuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiguo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Videocomposer</surname></persName>
		</author>
		<title level="m">Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Lavie: High-quality video generation with cascaded latent diffusion models</title>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqing</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15103</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Customvideo: Customizing text-to-video generation with multiple subjects</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.09962</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Motionctrl: A unified and flexible motion controller for video generation</title>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno>ACM SIGGRAPH 2024</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m">Conference Papers</title>
		<imprint>
			<date type="published" when="2024">2024. 2, 3, 6, 7, 8, 1</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.07860</idno>
		<title level="m">Controlling space and time with diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Motionbooth: Motion-aware customized text-to-video generation</title>
		<author>
			<persName><forename type="first">Jianzong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17758</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spherediffusion: Spherical geometry-aware distortion resilient diffusion model</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuewei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6126" to="6134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Customcrafter: Customized video generation with preserving motion and concept composition abilities</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.13239</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Ifadapter: Instance feature control for grounded text-to-image generation</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.08240</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Dynamicrafter: Animating open-domain images with video diffusion priors</title>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12190</idno>
		<imprint>
			<date type="published" when="2006">2023. 3, 6</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Cavia: Camera-controllable multiview video diffusion with view-integrated attention</title>
		<author>
			<persName><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Gernoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.10774</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Camco: Camera-controllable 3d-consistent image-tovideo generation</title>
		<author>
			<persName><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.02509</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Camco: Camera-controllable 3d-consistent image-tovideo generation</title>
		<author>
			<persName><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.02509</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Magicanimate: Temporally consistent human image animation using diffusion model</title>
		<author>
			<persName><forename type="first">Zhongcong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1481" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09414</idno>
		<title level="m">Depth anything v2</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Direct-a-video: Customized video generation with user-directed camera movement and object motion</title>
		<author>
			<persName><forename type="first">Shiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2024 Conference Papers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ip-Adapter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06721</idno>
		<title level="m">Text compatible image prompt adapter for text-to-image diffusion arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory</title>
		<author>
			<persName><forename type="first">Shengming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.08089</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Efficient video diffusion models via content-frame motion-latent decomposition</title>
		<author>
			<persName><forename type="first">Sihyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.14148</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Make pixels dance: High-dynamic video generation</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8850" to="8860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">David Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01827</idno>
		<title level="m">Caiming Xiong, and Doyen Sahoo. Moonshot: Towards controllable video generation and editing with multimodal conditions</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3836" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.04145</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Your personalized image animator via plug-and-play modules in text-toimage models</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhening</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youqing</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Pia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7747" to="7756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Entropy-driven sampling and training scheme for conditional diffusion generation</title>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiping</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouhong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="754" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Layoutdiffusion: Controllable diffusion model for layout-to-image generation</title>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuewei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22490" to="22499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Cami2v: Camera-controlled image-to-video diffusion model</title>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.15957</idno>
		<imprint>
			<date type="published" when="2024">2024. 2, 3, 6, 7, 8, 1</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
