# RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control

## Abstract

## Interpolation

Rotate Complex Trajectory Close-up Loop Transition RealCam -I2V Figure 1. We propose RealCam-I2V, a camera controllable image-to-video generation framework for complex real-world camera control and extra applications including camera-controlled loop video generation, generative frame interpolation, and smooth scene transitions.

## Introduction

Recent advancements in image-to-video generation [[4,](#b3)[7,](#b6)[8,](#b7)[14,](#b13)[18,](#b17)[66]](#b66) have significantly improved controllability over synthesized videos. However, challenges remain in achieving realistic, controllable camera movement within com- plex real-world scenes. Text-based camera-control methods [[2,](#b1)[14,](#b13)[22,](#b21)[24,](#b23)[28,](#b27)[50]](#b49), like traditional diffusion-based video generation, are intuitive and straightforward but lack precision in explicit control over camera parameters, such as angle, scale, and movement direction. This limitation has spurred the development of camera-trajectory-guided approaches, which attempt to address these issues by offering finer control over camera movement.

Current camera-trajectory-guided methods typically rely on relative camera trajectories, as seen in models like Mo-tionCtrl [[53]](#b52), CameraCtrl [[16]](#b15), CamCo [[62]](#b62), and CamI2V [[77]](#b77). While these approaches provide more control than text-based models, they are fundamentally limited by their reliance on relative scale trajectories. Training on relative scales results in inconsistencies when applied to real-world scenes, where absolute scale is crucial for realistic depth perception. Additionally, without access to depth information, users find it challenging to draw precise trajectories, making these methods difficult to use effectively.

To overcome these limitations, we propose RealCam-I2V, a video generation framework that integrates monocular depth estimation as a preprocessing step to construct a robust, absolute-scale 3D scene. Our approach leverages the Depth Anything v2 [[64]](#b64) model to predict the metric depth of a user-provided reference image, reprojecting its pixels back into camera space to create a stable 3D representation. This 3D scene serves as the foundation for camera control, providing a consistent and absolute scale that is critical for real-world applications.

In the training stage, we align the reconstructed 3D scene of the reference image with the point cloud of each video sample, reconstructed using COLMAP [[40]](#b39), a structurefrom-motion (SfM) method. This alignment allows us to rescale COLMAP-annotated camera parameters to the Depth Anything metric, providing an absolute, stable, and robust scale across training data. By aligning relative-scale camera parameters to absolute scales, we can condition the video generation model on accurately scaled camera trajectories, achieving greater control and scene consistency across diverse real-world images.

During inference, RealCam-I2V provides an interactive interface where users can intuitively design camera trajectories by drawing within the reconstructed 3D scene of the reference image. This interface renders preview videos of the trajectory in a static scene, offering users real-time feedback and greater control over camera movement. This interactive feature enhances usability, allowing precise trajectory control even for users without specialized knowledge of scene depth. To further improve video quality and control precision, we introduce scene-constrained noise initialization as a mechanism to shape the generation process in its high-noise stages. By using the preview video of the static 3D scene, RealCam-I2V injects scene-visible regions with controlled noise, guiding the video diffusion model's early generation stages. This high-noise feature constrains the initial layout and camera dynamics, providing a strong foundation for the remaining denoising stages. As denoising progresses, the condition-based approach, trained on absolute-scale camera trajectories, preserves global layout and completes the dynamic scene in previously unseen regions. This approach maintains the video diffusion model's capacity for dynamic content generation while ensuring accurate, coherent camera control.

Our experimental results show that RealCam-I2V achieves significant performance gains in video quality and controllability. When relative scales are aligned to absolute scales, models such as MotionCtrl, CameraCtrl, and CamI2V see substantial improvements in video quality. Furthermore, with the introduction of scene-constrained noise initialization, RealCam-I2V surpasses state-of-theart performance benchmarks, particularly on datasets like RealEstate10K [[78]](#b78) and out-of-domain images. These results demonstrate the effectiveness of our approach in both controlled and diverse real-world settings. In summary, our contributions are as follows:

• We identify scale inconsistencies and real-world usability challenges in existing trajectory-based methods and introduce a simple yet effective monocular 3D reconstruction into the preprocessing step of the generation pipeline, serving as a reliable intermediary reference for both training and inference. • With reconstructed 3D scene, we enable absolute-scale training and provide an interactive interface during inference to easily design camera trajectories with preview feedback, along with proposed scene-constrained noise shaping to significantly enhance scene consistency and camera controllability. • Our method overcomes critical real-world application challenges and achieves substantial improvements on the RealEstate10K dataset, establishing a new sota benchmark both in video quality and control precision.

## Related Works

Diffusion-based Video Generation. The advancement of diffusion models [[38,](#b37)[39,](#b38)[75]](#b75) has led to significant progress in video generation. Due to the scarcity of high-quality video-text datasets [[2,](#b1)[3]](#b2), researchers have adapted existing text-to-image (T2I) models to facilitate text-to-video (T2V) generation. Notable examples include AnimateDiff [[14]](#b13), Align your Latents [[3]](#b2), PYoCo [[11]](#b10), and Emu Video [[12]](#b11). Further advancements, such as LVDM [[18]](#b17), VideoCrafter [[7,](#b6)[8]](#b7), ModelScope [[47]](#b46), LAVIE [[51]](#b50), and VideoFactory [[49]](#b48), have refined these approaches by fine-tuning both spatial and temporal blocks, leveraging T2I models for initialization to improve video quality. Recently, Sora [[4]](#b3) and CogVideoX [[66]](#b66) enhance video generation by introducing Transformer-based diffusion backbones [[30,](#b29)[36,](#b35)[69]](#b69) and utilizing 3D-VAE, unlocking the potential for realistic world simulators. Additionally, SVD [[2]](#b1), SEINE [[9]](#b8), Pix-elDance [[70]](#b70) and PIA [[74]](#b74) have made significant strides in image-to-video generation, achieving notable improve-ments in quality and flexibility. Further, I2VGen-XL [[73]](#b73), DynamicCrafter [[59]](#b59), and Moonshot [[71]](#b71) incorporate additional cross-attention layers to strengthen conditional signals during generation. Controllable Generation. Controllable generation has become a central focus in both image [[25,](#b24)[33,](#b32)[37,](#b36)[42,](#b41)[56,](#b56)[58,](#b58)[67,](#b67)[72,](#b72)[76]](#b76) and video [[13,](#b12)[15,](#b14)[26,](#b25)[71]](#b71) generation, enabling users to direct the output through various types of control. A wide range of controllable inputs has been explored, including text descriptions, pose [[21,](#b20)[31,](#b30)[48,](#b47)[63]](#b63), audio [[17,](#b16)[43,](#b42)[44]](#b43), identity representations [[5,](#b4)[52,](#b51)[57]](#b57), trajectory [[6,](#b5)[29,](#b28)[34,](#b33)[55,](#b55)[68]](#b68).

Text-based Camera Control. Text-based camera control methods use natural language descriptions to guide camera motion in video generation. AnimateDiff [[14]](#b13) and SVD [[2]](#b1) fine-tune LoRAs [[20]](#b19) for specific camera movements based on text input. Image conductor [[28]](#b27) proposed to separate different camera and object motions through camera LoRA weight and object LoRA weight to achieve more precise motion control. In contrast, MotionMaster [[22]](#b21) and Peekaboo [[24]](#b23) offer training-free approaches for generating coarse-grained camera motions, though with limited precision. VideoComposer [[50]](#b49) adjusts pixel-level motion vectors to provide finer control, but challenges remain in achieving precise camera control.

Trajectory-based Camera Control. MotionCtrl [[53]](#b52), CameraCtrl [[16]](#b15), and Direct-a-Video [[65]](#b65) use camera pose as input to enhance control, while CVD [[27]](#b26) extends Cam-eraCtrl for multi-view generation, though still limited by motion complexity. To improve geometric consistency, Pose-guided diffusion [[45]](#b44), CamCo [[61]](#b61), and CamI2V [[77]](#b77) apply epipolar constraints for consistent viewpoints. VD3D [[1]](#b0) introduces a ControlNet [[72]](#b72)-like conditioning mechanism with spatiotemporal camera embeddings, enabling more precise control. CamTrol [[19]](#b18) offers a training-free approach that renders static point clouds into multi-view frames for video generation. Cavia [[60]](#b60) introduces viewintegrated attention mechanisms to improve viewpoint and temporal consistency, while I2VControl-Camera [[10]](#b9) refines camera movement by employing point trajectories in the camera coordinate system. Despite these advancements, challenges in maintaining camera control and scene-scale consistency remain, which our method seeks to address. It is noted that 4Dim [[54]](#b54) introduces absolute scale but in 4D novel view synthesis (NVS) of scenes.

## Method

## Metric Depth Estimation for 3D Reconstrcution

To obtain a depth map from a given input image, we use a metric depth predictor f depth , which takes the RGB image I as input and outputs the corresponding depth map D(u, v).  The prediction process is formulated as:

$D(u, v) = f depth (I),$where I is the input RGB image and D(u, v) is the predicted depth value for each pixel at coordinates (u, v). This predicted depth map D(u, v) serves as the foundation for projecting the image into 3D space, allowing us to construct a point cloud in the camera coordinate system. The camera intrinsics matrix K is defined as:

$K =   f x 0 c x 0 f y c y 0 0 1   ,$where f x and f y are the focal lengths along the x and y axes, (c x , c y ) is the principal point of the camera. Given a depth map D(u, v), the projected 3D coordinates in the camera coordinate system, p c = (x c , y c , z c ) T , are computed as:

$p c = D(u, v) • K -1 •   u v 1   .$Here u, v, 1 represents the homogeneous coordinates of the pixel, K -1 is the inverse of the intrinsic matrix, which maps pixel coordinates to normalized image coordinates. By applying this transformation to all pixels in the depth map, we obtain a set of 3D points {p c } in the camera coordinate system.

## Absolute-Scale Training

Camera-controlled Image-to-Video Model. Instead of directly modeling the video x, the latent representation z = E(x) is used for training. The diffusion model ϵ θ learns to estimate the noise ϵ added at each timestep t, conditioned on both a text prompt c txt , a reference image c img , and camera condition c cam , with t ∈ U(0, 1). The training objective simplifies to a reconstruction loss defined as:

$L = E z,ctxt,cimg,ccam,ϵ,t ∥ϵ -ϵ θ (z t , c txt , c img , c cam , t)∥ 2 2 ,(1)$where z ∈ R F ×H×W ×C represents the latent code of a video, with F, H, W, C corresponding to frame count, height, width, and channel dimensions.

The noise-corrupted latent code z t , derived from the ground-truth latent z 0 , is expressed as:

$z t = α t z 0 + σ t ϵ,(2)$where σ t = 1 -α 2 t . Here, α t and σ t are hyperparameters governing the diffusion process. Aligning from Relative Scale to Absolute Scale. To convert camera extrinsics from world-to-camera to an absolutescale camera-to-world representation, we defines that the world-to-camera extrinsics matrix F w2c ∈ R 4×4 is inverted to obtain the corresponding camera-to-world matrix:

$F c2w = F -1 w2c .$To express the transformations relative to the first frame, each F c2w is left-multiplied by the camera-to-world matrix of the inverse of first frame F c2w, 1 :

$c cam = F -1 c2w,1 • F c2w .$Here, c cam ∈ R F ×4×4 represents the camera-to-world transformations aligned relative to the first frame. However, the translation component of c cam remains in a relative scene scale. To convert the relative translation to an absolute scale, we align the metric 3D point cloud reconstructed by Depth Anything with the 3D point cloud reconstructed by COLMAP (Structure-from-Motion), as shown in Fig. [4](#fig_2).

The alignment process yields a scale factor a and is applied to the translation component of c cam , resulting in an absolute-scale camera-to-world transformation:

$c abs cam = R a • T 0 1 ,$where R is the rotation matrix, T is the relative translation vector. The resulting c abs cam ∈ R F ×4×4 represents the camera-to-world transformations with absolute scene scale, enabling robust and accurate real-world applications.

## Scene-Constrained Noise Shaping

Inspired by SDEdit [[32]](#b31) and DDIM [[41]](#b40) inversion, noised features z t can be used for shaping the layout, camera control of the entire image, especially at timestep with highlevel noise. We propose scene-constrained noise shaping, which utilizes preview videos generated along user-defined trajectories in the interactive 3D scene. Each frame of the preview video is treated as a reference frame and provided to the generation process during the high-noise stage. The reference frame's pixels are overlaid onto the modelpredicted z 0 to achieve the shaping effect.

Next, we detail the process for selecting the pixels to be referenced. As illustrated in Fig. [5](#fig_3), the primary criterion is that a pixel must be visible under the current camera viewpoint in the preview video. To mitigate issues such as holes where they are pasted onto the clean part (predicted z0) of a noised latent zt, typically at high noise levels 0.9 < t < 1.0 is enough for camera control and maintain dynamics.

## Basic Mode

Interpolation Mode caused by inaccurate depth predictions, we apply an additional filtering rule: if a visible pixel's k × k neighborhood contains any invisible pixels, it is considered to lie on an object's edge and potentially affected by depth prediction errors. Such pixels are excluded from selection. Finally, we define the noise shaping process with the following formula:

$… !! " !! # !! $ ! ! %&" ! ! % … !' " !' " !' " !' " !' " … !! " !! # !! $ ! ! %&" ! ! % … !' " !' " !' " !' " ! ' % … !! " !! # !! $ ! ! %&" ! ! % … !' " !' # !' $ !' $ !' $ Continuation Mode ! ! ∶ B, C, F, H, W ! " ∶ B, C, F, H, W concatenation B, 2×C, F, H, W$$z predict = mask • z preview + (1 -mask) • z predict ,$where the mask identifies the selected reference pixels, z preview represents the latent features from the preview video, and z predict is the model-predicted latent representation.

## Interploatation, Loop and Continueation

To support different tasks, including interpolation, looping, and continuation for long video generation, we train video diffusion model with different input concatenation mode, as shown in Fig. [6](#fig_4). Given a video latents z ∈ R F ×H×W ×C , we define the noised latents of f -th frame at timestep t as z f t . We then select i-th clean frame as the condition frame z i 0 . For interpolation mode, we define z f -1 0 as the end condition frame. For continuation mode, we define all 1 i-th as condition frame.

## Experiments

## Setup

Dataset. We train our model on RealEstate10K [[78]](#b78), which contains ∼ 70, 000 video clips with well-annotated camera poses. For metric depth alignment of absolute scene scale, we run COLMAP [[40]](#b39) point triangulator on each video clip with fixed camera intrinsics and extrinsics directly from RealEstate10K, obtaining the sparse point cloud of the reconstructed scene. We then calculate per-point depth scale against the metric depth from depth predictor. We term the median value of per-point depth scales in a frame as the frame-level depth scale. To make stable the training, we discard outliers of video clip whose maximum frame-level depth scale of the whole scene is among the top 2% for too small values or the last 2% for too large values, assuming sorted in ascending order. The same quantile filtering strategy is also applied on the minimum frame-level depth scales of video clips. It remains 58, 000 video clips for training and another 6, 000 for test. During training, we follow Dy-namiCrafter to sample 16 frames from each single video clip while perform resizing, keeping the aspect ratio, and center cropping to fit in our training scheme. We train the model with a random frame stride ranging from 1 to 10 and take random condition frame as data augmentation. We fix the frame stride to 8 and always use the first frame as the condition frame for inference. Implementation Details. We choose DynamiCrafter [[59]](#b59) as our image-to-video (I2V) base model and seamlessly integrate proposed RealCam-I2V into it as a plugin. For metric depth predictor, we choose Depth Anything V2 [[64]](#b64) Large Indoor, which is fine-tuned on metric depth estimation. During depth-aligned training, we freeze all parameters of the base model and the depth predictor, while only parameters of proposed method are trainable. More details are listed in dataset section of Appendix. We supervise ϵ-prediction on the model of 256 × 256 resolution and vprediction on the model of 512×320 resolution respectively, following the pre-training scheme of DynamiCrafter. We apply the Adam optimizer with a constant learning rate of 1 × 10 -4 with mixed-precision fp16 and DeepSpeed ZeRO-1. We train proposed method and variants on 8 NVIDIA H100 GPUs with an effective batch size of 64 for 50, 000 steps. More details are listed in implementation section of Appendix.

## Metrics

We follow previous works [[16,](#b15)[53,](#b52)[62,](#b62)[77]](#b77) to evaluate camera-controllablity by RotErr, TransErr and CamMC on their estimated camera poses using structure-from-motion (SfM) methods, e.g. COLMAP [[40]](#b39) and GLOMAP [[35]](#b34). We convert the camera pose of each frame in a video clip to be relative to the first frame as canonicalization. We denote the i-th frame relative camera-to-world matrix of ground truth as {R 3×3 i , T 3×1 i }, and that of generated video as { R3×3 i , T 3×1 i }. We randomly select 1, 000 samples from test set for evaluation. We sum up per-frame errors as the scene-level result for camera metrics. Inspired by Zheng et al. [[77]](#b77), we repetitively conduct 5 individual trials on each video clips for camera-control metrics to reduce the randomness introduced by SfM tools. Metrics of one video clip are averaged on successful trials at first for later samplewise average to get final results. RotError. We calculate camera rotation errors by the relative angle between generated videos and ground truths in radians for rotation accuracy.

$RotErr = n i=1 arccos tr( Ri R T i ) -1 2(3)$TransError. For relative TransErr, we perform scene scale normalization on the camera positions of each video clip. The scene scale of generated video si and grouth truth s i are individually calculated as the L 2 distance from the first camera to the farthest one for each video clip. For absolute TransErr, we normalize both the video clip to the scene scale of ground truth video, i.e. si = s i .

$TransErr = n i=1 Ti si - T i s i 2(4)$CamMC. We perform the same scene scale normalization for relative metrics and absolute metrics as TransError, and evaluate the overall camera pose accuracy by directly calculating L 2 similarity on camera-to-world matrices.

$CamMC = n i=1 Ri Ti si 3×4 -R i T i s i 3×4 2(5)$FVD. We also assess the visual quality of generative videos by the distribution distance FVD [[46]](#b45) between generated videos and ground-truths.

## Comparison with SOTA Methods

We compare our proposed method against models that either lack camera-condition training (DynamiCrafter [[59]](#b59)) or incorporate camera-condition training, namely Dynami-Crafter+MotionCtrl [[53]](#b52) (3 × 4 camera extrinsics), Dynam-iCrafter+CameraCtrl [[16]](#b15)  Table [2](#). Evaluation results on Vbench-I2V [[23]](#b22), a widely used benchmark suite with dynamic scenes and various types. Due to the efficient frozen parameters finetuning on Dynamicrafter, our method obtains the ability of camera control but decreases little in other metrics despite only training on static RealEstate10K.

from camera extrinsics and intrinsics as side input), and DynamiCrafter+CamI2V [[77]](#b77) (current SOTA using Plücker embedding and epipolar attention between all frames), as shown in Tab. 1.

Our method demonstrates significant improvements in visual quality (FVD) and camera control metrics (TransErr, RotErr, CamMC), particularly on absolute metrics. Specifically, absolute camera metrics improve by +27%, relative camera metrics by +14%, and FVD by +13%. However, these improvements are not fully captured by the RealEstate10K dataset, which has limitations on movement speed and contains mostly static scenes. It's strongly recommended to view the dynamic visualizations in the supplementary materials for a more comprehensive evaluation.

## Ablation Study

Effect of absolute-scale training only. As shown in Tab. 3, compared to relative-scale training, absolute-scale training yields notable improvements, especially on absolute metrics. It implies that models trained on absolute-scale data can more accurately capture true-to-scale translations and better understand camera rotations within a realistic spatial framework. The absolute scene scale enhances robustness and compatibility, ensuring that the framework adapts effectively to real-world images and applications. This approach allows for interaction within a unified scale, enabling intuitive user control over camera actions. Effect of absolute-scale training + scene-constrained noise shaping. Adding scene-constrained noise shaping to a model trained with absolute-scale yields substantial gains in video quality and camera controllability. This improvement is evident across both camera metrics and FVD. The synergy of absolute-scale training and sceneconstrained noise shaping ensures robust and precise control in diverse scenarios. As illustrated in Fig. [7](#fig_5), this combined approach delivers noticeably better dynamics compared to using scene-constrained noise shaping alone. Large camera movements, rotations, and rapid transitions, which previously struggled to maintain consistency and realism, now work seamlessly. This improvement underscores the strength of integrating absolute-scale training with noise  shaping for complex motion scenarios.

Effect of scene-constrained noise shaping only. As shown in Tab. 3, scene-constrained noise shaping can be used as the sole method for camera control when applied to a base model not trained with any camera conditions. It provides notable improvements in metrics, exemplified by nearly 50% reduction on DynamiCrafter. However, this method underperforms compared to the combined method with absolute-scale training. It also introduces challenges in parameter selection. Applying shaping only in the highnoise phase limits camera contrl in lower noise stages, while extending shaping to mid-noise phase can suppress dynamic elements, resulting in static video output. This limitation affects the fluidity and responsiveness of generated camera movements, making the combination approach preferable for applications requiring natural dynamics.

## Rotation Zoom in+Transition

Close-up 

Complex Trace
## Application

As illustrated in Fig. [9](#fig_7), we demonstrate the versatility of our method through visualization results across various applications, including videos generated at resolutions of 512×320 and 1024 × 576 with camera control under complex scenarios, such as large movements or rotations. Additionally, our results include camera-controlled loop video generation, generative frame interpolation, and smooth scene transitions, highlighting the robustness of our approach. These visualizations showcase two major breakthroughs: first, our method achieves a real-world application breakthrough by addressing challenges like training-inference scale inconsistency and low usability, ensuring improved robustness and compatibility with real-world images. Second, our framework exhibits superior performance in complex camera motions, handling large and rapid movements, rotations, and dynamics more effectively than existing methods. More ex-tensive results are provided in the supplementary materials.

## Limitation Analysis and Future Work

The model was trained on datasets such as RealEstate10K, which consists primarily of real-world, indoor and outdoor videos collected from YouTube. This dataset's content focuses heavily on realistic, static scenes, resulting in a model that excels in these contexts but performs less effectively when applied to scenes with significantly different visual styles, such as anime, oil paintings, or cartoon-like aesthetics. Better data quality, designing algorithm or improving the ability of fundamental model especially in long video generation will be considered into the future research.

## Potential Negative Societal Impacts

The image-to-video generation technology developed in this work, with its enhanced camera controllability and breakthrough in real-world applications holds the potential for misuse, particularly in the creation of falsified or deceptive video content. The ability to precisely control camera movements and generate realistic sequences from single images could be exploited to produce convincing yet fabricated videos, leading to ethical concerns around misinformation and privacy violations. To mitigate these risks, we advocate for responsible usage and adherence to ethical guidelines when deploying the RealCam-I2V model.

## Conclusion

In this paper, we address the scale inconsistencies and real-world usability challenges in existing trajectory-based camera-controlled image-to-video generation methods. We introduce a simple yet effective monocular 3D reconstruction into the preprocessing step of the generation pipeline, serving as a reliable intermediary reference for both training and inference. With reconstructed 3D scene, we enable absolute-scale training and provide an interactive interface during inference to easily design camera trajectories with preview feedback, along with proposed scene-constrained noise shaping to significantly enhance scene consistency and camera controllability. Our method overcomes critical real-world application challenges and achieves substantial improvements on the RealEstate10K dataset, establishing a new sota both in video quality and control precision. 

## C. Training

We choose DynamiCrafter[foot_2](#foot_2)[[59]](#b59) as our image-to-video (I2V) base model. We trained proposed method on 4 publicly accessible variants of DynamiCrafter, namely 256, 512, 512 interp and 1024. We conduct ablation study on resolution 256 × 256, due to the limitation of computing resource. For resolution 256 × 256, we train all models on ϵ-prediction with effective batch size 64 on 8 NVIDIA H100 GPUs for 50, 000 steps, taking about 25 hours. For resolution 512 × 320 and 1024 × 576, we train RealCam-I2V on v-prediction while enable perframe ae and gradient checkpoint to reduce peak GPU memory consumption. We apply the Adam optimizer with a constant learning rate of 1 × 10 -4 with mixed-precision fp16 and DeepSpeed ZeRO-1.

For MotionCtrl [[53]](#b52) and CameraCtrl [[16]](#b15), we reproduce all results on DynamiCrafter for fair comparison. For CamI2V [[77]](#b77), we implement hard mask epipolar attention and set 2 register tokens, aligned with the original paper. In quantitative comparison and ablation study, we set fixed text image CFG to 7.5 and camera CFG to 1.0.

## D. Camera Keyframe Interpolation

In real-world applications, user-provided camera trajectories often consist of a limited number of keyframes (e.g., 4 keyframes). To ensure smooth and continuous motion across the trajectory while adhering to the user's input, we perform linear interpolation in SE(3) space to expand the trajectory to a higher number of frames (e.g., 16 interpolated frames), as shown in Fig. [10](#fig_8). This step ensures that our model generates consistent and visually coherent videos without compromising the accuracy of user-defined camera movements.

![Figure 2. Comparision between text, relative trajectory, and absolute trajectory based camera-controlled image-to-video generation methods on aspects of camera control precision and usability.]()

![Figure 3. Pipeline. In training, we align camera parameters in RealEstate10K from relative scale to absolute scale. In inference, we use metric depth estimation method to construct a 3d point cloud for users to interactively drawing camera traces.]()

![Figure 4. 3D point cloud reconstructed from metric depth estimation (RGB) is robust and unified, whereas the SFM-based reconstruction by methods like COLMAP (Yellow) used in RealEstate10K annotations is in relative scale and may vary across images. Aligning these two 3D scenes enables the transformation from relative to absolute scale (real-world scale).]()

![Figure 5. Pixels selected for scene-constrained noise shaping, where they are pasted onto the clean part (predicted z0) of a noised latent zt, typically at high noise levels 0.9 < t < 1.0 is enough for camera control and maintain dynamics.]()

![Figure 6. Concatenation for different tasks, including basic mode, interpolation mode, and continuation mode.]()

![Figure 7. (a) Without Scene-Constrained Noise Shaping. (b) With Scene-Constrained Noise Shaping. The left shows failures in large movements without scene-constrained noise shaping, while the right illustrates a loss of dynamics when noise shaping is extended to lower noise stages.]()

![Figure 8. Without kernel size≥3 in noise shaping, unvisible regions will be wrongly pasted to the generated video.]()

![Figure 9. Visualization of various applications. Best viewed as dynamic videos in the supplementary materials.]()

![Figure 10. Camera Trajectory Interpolation.choose Large as the model size, which has 335.3M parameters, and the indoor version. The scene scale of our model is aligned to the metric depth space of Depth Anything V2 Large Indoor, i.e. absolute scene scale.]()

![Quantitative comparison with SOTA methods. * denotes the results we reproduced using DynamiCrafter as base I2V model. Our method achieves the state-of-the-art performance on both relative and absolute camera-controllable metrics, while coherently improve visual quality of generated videos, witnessed by a further drop of FVD. Best and second best results are highlighted respectively. We observe nearly +30% improvement on absolute metrics while over +10% improvement on relative metrics and FVD.]()

![Ablation study. * denotes the results we reproduced using DynamiCrafter as base I2V model. Absolute scene-scale training resolves scale inconsistencies for real-world applications and its improvement on relative metrics indicates a more stable and unified camera control for video generation. Scene-constrained noise shaping can provides substantial improvements in dynamics and large camearabut is less effective than the combined approach, struggling with parameter tuning and dynamic consistency in lower noise stages. Best and second best results are highlighted respectively.]()

https://google.github.io/realestate10k/

https : / / github . com / DepthAnything / Depth -Anything-V2/tree/main/metric_depth

https://github.com/Doubiiu/DynamiCrafter

