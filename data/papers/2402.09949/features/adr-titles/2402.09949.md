- Decision to implement Multi-Word Tokenization (MWT) for sequence compression
- Choice of n-gram size (N) set to 2 for better generalization
- Method for selecting top-K most frequent n-grams for vocabulary enrichment
- Adoption of Fast Vocabulary Transfer (FVT) for embedding initialization
- Decision to use Masked-Language Modeling (MLM) for fine-tuning after FVT
- Selection of datasets for evaluating MWT effectiveness (ADE, LEDGAR, PATENT)
- Choice of metrics for performance evaluation (macro-F1, inference speedup)
- Implementation of early stopping during model fine-tuning
- Decision to compare multiple tokenizers (generic vs. adapted)
- Strategy for measuring inference speedup across different tokenizers
- Approach for handling underrepresented languages in tokenization
- Decision to make the code publicly available for reproducibility
- Choice of hardware for conducting experiments (V100-PCIE GPU)
- Decision to use a specific batch size for MLM and downstream fine-tuning
- Strategy for analyzing the interaction between performance and speedup
- Decision to document the impact of MWT on various linguistic domains