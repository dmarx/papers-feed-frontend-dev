<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-word Tokenization for Sequence Compression</title>
				<funder ref="#_uVP5dqN #_athP5kB">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-04-04">4 Apr 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Leonidas</forename><surname>Gee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<addrLine>expert.ai expert.ai expert.ai</addrLine>
									<settlement>Siena Siena Siena</settlement>
									<country>United Kingdom Italy Italy Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonardo</forename><surname>Rigutini</surname></persName>
							<email>lrigutini@expert.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<addrLine>expert.ai expert.ai expert.ai</addrLine>
									<settlement>Siena Siena Siena</settlement>
									<country>United Kingdom Italy Italy Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Ernandes</surname></persName>
							<email>mernandes@expert.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<addrLine>expert.ai expert.ai expert.ai</addrLine>
									<settlement>Siena Siena Siena</settlement>
									<country>United Kingdom Italy Italy Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Zugarini</surname></persName>
							<email>azugarini@expert.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<addrLine>expert.ai expert.ai expert.ai</addrLine>
									<settlement>Siena Siena Siena</settlement>
									<country>United Kingdom Italy Italy Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-word Tokenization for Sequence Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-04-04">4 Apr 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">804038B904FE5933CDD71C42098D1ADB</idno>
					<idno type="arXiv">arXiv:2402.09949v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits:</p><p>(1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of Natural Language Processing (NLP) has seen major breakthroughs with the advent of Large Language Models (LLMs) <ref type="bibr" target="#b25">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b3">Devlin et al., 2018;</ref><ref type="bibr">Touvron et al., 2023;</ref><ref type="bibr" target="#b14">OpenAI, 2023)</ref>. Despite their successes, LLMs like ChatGPT <ref type="bibr" target="#b14">(OpenAI, 2023;</ref><ref type="bibr" target="#b0">Brown et al., 2020)</ref> possess hundreds of billions of parameters that entail enormous computational cost by design. Traditional model compression methods such as Knowledge Distillation <ref type="bibr" target="#b7">(Hinton et al., 2015)</ref>, Pruning <ref type="bibr" target="#b12">(Michel et al., 2019;</ref><ref type="bibr" target="#b27">Zhu and Gupta, 2017)</ref>, and Quantization <ref type="bibr" target="#b21">(Shen et al., 2020;</ref><ref type="bibr" target="#b5">Gupta et al., 2015)</ref> have focused on creating lighter models either by shrinking the architectural size or by reducing the number of FLOPs.</p><p>Recently, LLMs have been shown to produce impressive performance on inputs that have been carefully designed to contain all the necessary information for a given instruction. As such, there is an increasing trend in designing longer and longer prompts that has led to a significant rise in computational cost. To address this, interest has grown in compressing the input sequences from the tokenizer <ref type="bibr" target="#b4">(Gee et al., 2022;</ref><ref type="bibr" target="#b13">Mu et al., 2023;</ref><ref type="bibr" target="#b16">Petrov et al., 2023)</ref>. Indeed, various works have shown the importance of tokenization in determining the length of a sequence in specialized domains <ref type="bibr" target="#b4">(Gee et al., 2022)</ref> or on underrepresented languages <ref type="bibr" target="#b16">(Petrov et al., 2023)</ref>.</p><p>In this paper, we propose a method for reducing the computational cost of a LLM by compressing the textual inputs using Multi-Word Tokenizers (MWTs). To achieves this, we enrich the vocabulary of the tokenizer with statistically determined multi-word expressions. By encoding the frequent n-grams with single tokens, the sequences produced are both shorter and more informative, thus allowing for major speedups via early sequence truncation. Additionally, MWTs are shown to be compatible with the aforementioned traditional compression methods. Experimentally, we assess MWTs on three text classification datasets. We show how our approach still performs well when combined with distilled models <ref type="bibr" target="#b17">(Sanh et al., 2019)</ref> and other sequence compression techniques <ref type="bibr" target="#b4">(Gee et al., 2022)</ref>. The code for our paper is publicly available<ref type="foot" target="#foot_0">foot_0</ref> .</p><p>The rest of the paper is organized as follows. First, we review the related works in Section 2. Then, we describe our approach in Section 3 and present the experiments in Section 4. Finally, we draw our conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Most model compression research falls into one of the following categories: Knowledge Distillation <ref type="bibr" target="#b7">(Hinton et al., 2015;</ref><ref type="bibr" target="#b17">Sanh et al., 2019;</ref><ref type="bibr" target="#b8">Jiao et al., 2020;</ref><ref type="bibr" target="#b26">Wang et al., 2020;</ref><ref type="bibr" target="#b22">Sun et al., 2020)</ref>, Pruning <ref type="bibr" target="#b27">(Zhu and Gupta, 2017;</ref><ref type="bibr" target="#b12">Michel et al., 2019)</ref>, and Quantization <ref type="bibr" target="#b21">(Shen et al., 2020)</ref>. The family of approaches is somewhat complementary and Input: an energizable member is operably coupled to the outer sleeve .</p><p>Tgen: an, en, ##er, ##gi, ##zable, member, is, opera, ##bly, coupled, to, the, outer, sleeve, .</p><p>T 1000 gen : an, en, ##er, ##gi, ##zable, member_is, opera, ##bly, coupled_to, the_outer, sleeve, .</p><p>T100: an, energizable, member, is, operably, coupled, to, the, outer, sleeve, .</p><p>T 1000 100 : an, energizable, member_is, operably, coupled_to, the_outer, sleeve, .</p><p>Figure 1: Tokenization using generic T gen and adapted T 100 tokenizers. T 1000 gen and T 1000 100 are extended with the top-1000 bigrams. Tokens obtained with domain-adaptation or MWT are highlighted in orange and blue respectively. MWTs are shown to be highly complementary to existing tokenizers for sequence compression. can be applied individually or jointly. Each approach alters the model's size to obtain a more efficient architecture. Differently, other works such as FlashAttention <ref type="bibr" target="#b2">(Dao et al., 2022)</ref> seek to optimize a model's implementation. In particular, LLMs are sped up by reducing the number of memory accesses for the self-attention mechanism.</p><p>Sequence Compression. An emerging direction for reducing the cost of LLMs involves the designing of shorter input sequences. Prompting techniques such as <ref type="bibr" target="#b13">Mu et al. (2023)</ref> compress repetitive lengthy prompts into gist tokens. Other works emphasize the role of tokenization in sequence compression. In <ref type="bibr" target="#b16">Petrov et al. (2023)</ref>, the authors show how the tokenizer of most LLMs strongly favor the English language over other languages. For underrepresented languages, the same translated sentence may consist of inputs that are up to 15 times longer. Analogously, <ref type="bibr" target="#b4">Gee et al. (2022)</ref> investigated the tokenization efficiency of general-purpose tokenizers in vertical domains such as medicine and law. They proposed a transfer learning technique that adapts the vocabulary of a LLM to specific language domains. An effect of a dedicated vocabulary is a more efficient tokenization that reduces the number of sub-word tokens in a sequence.</p><p>In this work, we push this effect further, going beyond word boundaries by introducing Multi-Word Expressions (MWEs) in the form of n-grams into the tokenizer as shown in Figure <ref type="figure">1</ref>. The underlying intuition behind this is that a more compact tokenization can save computations by allowing the model to process shorter sequences without a significant loss of information. The usage of MWEs is not novel with several works <ref type="bibr" target="#b11">(Lample et al., 2018;</ref><ref type="bibr" target="#b15">Otani et al., 2020;</ref><ref type="bibr" target="#b10">Kumar and Thawani, 2022)</ref> introducing phrases or n-grams to improve the quality of machine translation. In <ref type="bibr" target="#b10">Kumar and Thawani (2022)</ref>, the authors generalized BPE <ref type="bibr" target="#b19">(Sennrich et al., 2016)</ref> to multi-word tokens. However, to the best of our knowledge, we are the first to investigate MWEs in the context of sequence compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-word Tokenizer</head><p>Tokenization is a necessary step in the feeding of textual data to a LLM. Typically, tokenizers split a text into a sequence of symbols which can be entire words or only subparts. To do this, a vocabulary is first constructed by statistically learning the most frequent tokens from a large general-purpose corpus <ref type="bibr" target="#b19">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b18">Schuster and Nakajima, 2012;</ref><ref type="bibr" target="#b9">Kudo and Richardson, 2018)</ref>. The resulting tokenizer can then be used to segment an input text by greedily looking for the solution with the least number of tokens. Building upon this, we inject into the tokenizer new symbols formed by n-grams of words. We do this by first selecting the most frequent n-grams to include in its vocabulary. Then, we place an n-gram merging step within the tokenization pipeline as sketched in Figure <ref type="figure">2</ref>. The added n-grams will be treated as single tokens further down the tokenization pipeline.</p><p>N-gram Selection. In order to maximize the sequence reduction, we statistically estimate the top-K most frequent n-grams in a reference training corpus. Although the approach is greedy, hence sub-optimal, it still effectively yields significant compression while being extremely fast and easy to compute. More formally, given a corpus D and N ≥ 2, we compute all the possible n-grams g n ∈ D, where n = 2, . . . , N . Then, we count their frequency f (g n ), ∀g n ∈ D. The K most frequent n-grams G K are included in the vocabulary V ← V ∪ G K of the tokenizer T . Fast Vocabulary Transfer. Given that the vocabulary of the tokenizer has changed, the newly added symbols G K must be included into the embedding matrix of the language model as well. To avoid retraining the entire model from scratch which is highly resource-demanding, or a random initialization of new tokens which would perform poorly, we make use of Fast Vocabulary Transfer (FVT) instead <ref type="bibr" target="#b4">(Gee et al., 2022)</ref>.</p><p>FVT is a transfer learning technique that assigns embeddings to new tokens by combining existing elements of the embedding matrix as shown in Figure <ref type="figure">3</ref>. After initializing the multi-word embeddings with FVT, we found it beneficial to tune the model with Masked-Language Modeling (MLM) as done by <ref type="bibr" target="#b4">Gee et al. (2022)</ref>. We believe this is helpful as it aids the model in further readjusting the embeddings of the new tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Given a fixed number of tokens, a more compact input sequence preserves a greater amount of infor-mation. This can be used to either achieve a better performance with limited benefits in speedup, or vice versa, i.e. making the model faster with negligible drops in performance. The experiments aim to analyze how these two aspects interact with one another. We focus on text classification as it is a problem of particular interest for many industryoriented applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Our experiments were conducted on the cased versions of BERT base <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and DistilBERT base <ref type="bibr" target="#b17">(Sanh et al., 2019)</ref>. Additionally, we consider an adapted tokenizer with a vocabulary size equal to that of the generic tokenizer from a pre-trained model as done by <ref type="bibr" target="#b4">Gee et al. (2022)</ref>. We refer to the generic and adapted tokenizers as T gen and T 100 respectively. Both tokenizers are extended with the top-K n-grams of 1000, 2500, and 5000. Overall, we compare eight different tokenizers indicated as: T gen , T 1000 gen , T 2500 gen , T 5000 gen and T 100 , T 1000 100 , T 2500 100 , T 5000 100 . Implementation Details. We train each model with 5 different random initializations. The macro-F1 and inference speedup are measured as metrics. The average of all 5 initializations is taken as the final value of each metric. The inference speedup measurements were done on a V100-PCIE GPU with 16GBs of dedicated RAM.</p><p>Following <ref type="bibr" target="#b4">Gee et al. (2022)</ref>, we first apply one epoch of MLM using the in-domain dataset. Next, the model is fine-tuned for 10 epochs with early stopping on the downstream task. We set the initial learning rate to 3 • 10 -5 for both MLM and downstream fine-tuning, while the batch size is set to 8 and 32 for MLM and downstream fine-tuning respectively.</p><p>Choice of N. An important hyperparameter is N, i.e. the maximum number of words constituting an n-gram. In our experiments, N is set to 2 as we believe that using bigrams only provides better generalization properties. Increasing the value of N may lead to an overspecialization of n-grams which could overfit on small textual corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>To determine the effectiveness of MWTs, we select 3 different text classification tasks from diverse linguistic domains, namely medical (ADE), legal (LEDGAR), and tech (PATENT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE.</head><p>A sentence classification dataset of determining whether a sentence is Adverse Drug Event (ADE)-related or not <ref type="bibr" target="#b6">(Gurulingappa et al., 2012)</ref>. The sentences are characterized by the presence of medical terminologies of drugs and their adverse effects. We use the same train, validation, and test splits as in <ref type="bibr" target="#b4">Gee et al. (2022)</ref>.</p><p>LEDGAR. A document classification dataset of contracts obtained from the US Securities and Exchange Commission (SEC) filings <ref type="bibr" target="#b24">(Tuggener et al., 2020)</ref>. The task is to determine whether the main topic of the contract provision from a set of 100 mutually-exclusive labels. The dataset is also part of LexGLUE <ref type="bibr" target="#b1">(Chalkidis et al., 2022)</ref>, which is a benchmark for legal language understanding.</p><p>PATENT. A document classification dataset 2 of US patent applications filed under the Cooperative Patent Classification (CPC) code <ref type="bibr" target="#b20">(Sharma et al., 2019)</ref>. A human written abstractive summary is provided for each patent application. The task is to determine the category that a patent application belongs to from 9 unbalanced classes.</p><p>2 <ref type="url" target="https://huggingface.co/datasets/ccdv/patent-classification">https://huggingface.co/datasets/ccdv/ patent-classification</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Preliminary Analysis. Before measuring the effects of MWTs on LLMs, we analyze how the average sequence length changes for each dataset depending on the tokenizer. From Table <ref type="table" target="#tab_0">1</ref>, increasing the top-K most frequent n-grams naturally yields a greater compression. However, even a 1000 bigrams is enough to achieve a reduction of about 20%. When multi-words are combined with an adapted tokenizer T 100 , the joint sequence narrowing effects appear to be highly complementary, achieving a compression rate close to 50% in ADE. In practice, a 50% reduction means that on average we can store the same amount of text in half the sequence length. Consequently, we could in principle reduce a LLM's maximum sequence length by a factor of 2.</p><p>Multi-word Tokenization. As a first evaluation, we assess the macro-F1 and inference speedups achieved by fine-tuned BERT models with multiword tokenizers: T 1000 gen , T 2500 gen , T 5000 gen . The pretrained BERT with a generic tokenizer T gen is considered as the reference model. From Table <ref type="table" target="#tab_2">2</ref>, MWTs are shown to either improve the reference performance or induce a relatively negligible degradation. At the same time, the sequence compression from MWTs yields a natural speedup that depending on the dataset varies from about x1.1 to x1.4.</p><p>MWT and Domain Adaptation. Additionally, we investigate the application of MWTs with tokenizers adapted to the dataset: T 1000 100 , T 2500 100 , T 5000 100 . With the exception of PATENT, most models are shown to achieve significant inference speedups of up to x1.8 with minimal degradation in performance from Table <ref type="table" target="#tab_2">2</ref>. We hypothesize that this is due to the fact that the language domain of PATENT is not as specialized as ADE and LEDGAR, which reduces the benefits of using an adapted tokenizer. ~x1.8 ~x2.4 ~x4.4 ADE 100 200 300 400 500 Maximum Sequence Length 78 79 80 81 82 83 Macro-F1 ~x2.1 ~x4.4 ~x9.4 LEDGAR 50 100 150 200 250 Maximum Sequence Length 57 58 59 60 61 62 Macro-F1 ~x2.0 ~x4.2 ~x8.6 PATENT gen 1000 gen 2500 gen 5000 gen 100 1000 100 2500 100 5000 100 max speedup Figure 4: Plot of macro-F1 against maximum sequence length. The generic T gen and adapted T 100 tokenizers are represented by solid and dashed lines respectively. MWTs are shown to be more robust on shorter sequence lengths, thus allowing for major speedups via early sequence truncation. Method ADE LEDGAR PATENT ∆F1 Speedup ∆F1 Speedup ∆F1 Speedup T gen 90.74 ± 0.84 1.00 82.12 ± 0.33 1.00 61.44 ± 0.38 1.00 T 1000 gen -0.09 ± 0.70 1.32 0.54 ± 0.24 1.14 -0.42 ± 0.54 1.11 T 2500 gen 0.37 ± 0.54 1.38 0.05 ± 0.44 1.23 -0.07 ± 0.46 1.16 T 5000 gen 0.29 ± 0.68 1.43 -0.05 ± 0.41 1.33 -0.46 ± 0.69 1.19 T 100 0.24 ± 0.67 1.51 0.00 ± 0.41 1.10 -1.27 ± 0.39 1.06 T 1000 100 -0.86 ± 1.21 1.71 0.32 ± 0.58 1.36 -0.78 ± 0.62 1.24 T 2500 100 -0.88 ± 0.72 1.78 -0.19 ± 0.57 1.47 -1.04 ± 0.42 1.30 T 5000 100 -0.51 ± 0.65 1.79 0.02 ± 0.58 1.57 -1.66 ± 0.44 1.34 From Figure <ref type="figure">4</ref>, we can see the performance of T gen dropping more rapidly than MWTs as truncation increases (maximum sequence length decreases). In the extreme 8-times truncation, the performance of T gen falls dramatically for both ADE and LEDGAR. However, MWTs are shown to be more robust to truncation, hence their degradation in performance is smoother and without sudden collapses. In both ADE and LEDGAR, a 4times truncation leads to nearly identical or better performance, while bringing significant inference speedups of ∼x2.4 and ∼x4.4 respectively. If a certain performance degradation is acceptable, the inference speedup can be maximized, reaching up to ∼x9.4 in LEDGAR.</p><p>MWT and Distillation. Additionally, we investigate the interaction between sequence compression and knowledge distillation in Table <ref type="table">3</ref>. To this end, we utilize a DistilBERT model with MWTs. For simplicity, we restrict our analysis to LEDGAR and to a single multi-word tokenizer T 2500 gen on different maximum sequence lengths. From the table, our MWT is shown to retain most of its performance with a quarter of the sequence length and an in-ference speedup of ∼x8.8. Even with an extreme sequence truncation to only 64 tokens, we can still achieve a ∼x18.1 inference speedup with only a 2.7% drop in relative performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Length ∆F1 Speedup T gen 512 82.12 1.00 Distil. + T gen 512 -0.78 2.43 Distil. + T 2500 gen 128 -0.32 8.81 Distil. + T 2500 gen 64 -2.70 18.13</p><p>Table 3: The macro-F1 and inference speedup results on LEDGAR with DistilBERT. MWTs are shown to be highly compatible with distilled models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a sequence compression approach that reduces textual inputs by exploiting the use of multi-word expressions drawn from the training set according to their top-K frequencies. We conducted an investigation on 3 different datasets by evaluating each model in conjunction with other compression methods <ref type="bibr" target="#b4">(Gee et al., 2022;</ref><ref type="bibr" target="#b17">Sanh et al., 2019)</ref>. Our approach is shown to be highly robust to shorter sequence lengths, thus yielding a more than x4 reduction in computational cost with negligible drops in performance. In the future, we expect to extend our analysis to other language models and tasks such as language generation in the scope of sequence compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>As demonstrated in the paper, MWTs work well on text classification problems. Despite not having conducted experiments on generative tasks, there are no limitations in extending MWTs to them. Differently, the application of MWTs to token classification problems can be challenging. Specifically, when merging multiple words together, it is unclear how to label such fused tokens.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Sketch of the Multi-word Tokenizer pipeline. First, n-grams are statistically learned from the training set. Then, the top-K n-grams are added to the vocabulary of the tokenizer. N-grams are merged from left to right within a sequence after pre-tokenization.</figDesc><graphic coords="3,73.02,70.87,449.23,155.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average sequence length from tokenization. The generic T gen and adapted T 100 tokenizers are extended with varying top-Ks of 1000, 2500, and 5000.</figDesc><table><row><cell>Dataset</cell><cell cols="2">T gen T 1000 gen</cell><cell>T 2500 gen</cell><cell>T 5000 gen</cell><cell cols="2">T 100 T 1000 100</cell><cell>T 2500 100</cell><cell>T 5000 100</cell></row><row><cell>ADE</cell><cell>31</cell><cell>26</cell><cell>25</cell><cell>23</cell><cell>21</cell><cell>18</cell><cell>17</cell><cell>16</cell></row><row><cell cols="2">LEDGAR 155</cell><cell>118</cell><cell>107</cell><cell>98</cell><cell>131</cell><cell>97</cell><cell>90</cell><cell>84</cell></row><row><cell cols="2">PATENT 134</cell><cell>110</cell><cell>105</cell><cell>100</cell><cell>118</cell><cell>94</cell><cell>90</cell><cell>86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Absolute values of BERT fine-tuned on the downstream task using a sequence length of 128, 512 and 256 for ADE, LEDGAR and PATENT respectively. T gen is shown on the first row, while relative values to T gen are shown on subsequent rows.</figDesc><table><row><cell>MWT and Truncation. Based on the prelim-</cell></row><row><cell>inary analysis, we analyze how truncating se-</cell></row><row><cell>quences with different maximum lengths affects</cell></row><row><cell>both the performance and inference speedup. Re-</cell></row><row><cell>ducing the maximum sequence length has a dou-</cell></row><row><cell>ble impact on the inference speedup given a fixed</cell></row><row><cell>amount of resources. First, latency linearly grows</cell></row><row><cell>with respect to the sequence length. Second,</cell></row><row><cell>reducing the sequence length releases GPU re-</cell></row><row><cell>sources that can be used to enlarge the batch size.</cell></row><row><cell>We consider 4 maximum sequence lengths for</cell></row><row><cell>each dataset by progressively halving the initial</cell></row><row><cell>maximum sequence length, i.e. {128, 64, 32, 16}</cell></row><row><cell>for ADE, {256, 128, 64, 32} for LEDGAR, and</cell></row><row><cell>{512, 256, 128, 64} for PATENT.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/LeonidasY/ fast-vocabulary-transfer/tree/emnlp2023</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="projectName">IBRIDAI</rs> project, a project financed by the <rs type="programName">Regional Operational Program</rs> "<rs type="programName">FESR 2014-2020</rs>" of <rs type="institution">Emilia Romagna (Italy)</rs>, resolution of the Regional Council n. <rs type="grantNumber">863/2021</rs>.</p></div>
<div><head>A.1 Results</head><p>We tabulate the complete results for BERT and DistilBERT on ADE, LEDGAR, and PATENT in Tables <ref type="table">4</ref> and <ref type="table">5</ref> respectively. The values in each table are averaged across 5 seeds.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_uVP5dqN">
					<orgName type="project" subtype="full">IBRIDAI</orgName>
					<orgName type="program" subtype="full">Regional Operational Program</orgName>
				</org>
				<org type="funding" xml:id="_athP5kB">
					<idno type="grant-number">863/2021</idno>
					<orgName type="program" subtype="full">FESR 2014-2020</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LexGLUE: A benchmark dataset for legal language understanding in English</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hartung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bommarito</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.297</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4310" to="4330" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, Daniel Katz, and Nikolaos Aletras Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16344" to="16359" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast vocabulary transfer for language model compression</title>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Zugarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Rigutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>UAE. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pritish</forename><surname>Narayanan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drugrelated adverse effects from medical case reports</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Mateen Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2012.04.008</idno>
	</analytic>
	<monogr>
		<title level="m">Text Mining and Natural Language Processing in Pharmacogenomics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="885" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bpe beyond word boundary: How not to use multi word expressions in neural machine translation</title>
		<author>
			<persName><forename type="first">Dipesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avijit</forename><surname>Thawani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Insights from Negative Results in NLP</title>
		<meeting>the Third Workshop on Insights from Negative Results in NLP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Phrasebased &amp; neural unsupervised machine translation</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07755</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Are sixteen heads really better than one? Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08467</idno>
		<title level="m">Learning to compress prompts with gist tokens</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pretokenization of multi-word expressions in crosslingual word embeddings</title>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoru</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micaelah</forename><forename type="middle">St</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4451" to="4464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Emanuele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Malfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Bibi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15425</idno>
		<title level="m">Language model tokenizers introduce unfairness between languages</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BIG-PATENT: A large-scale dataset for abstractive and coherent summarization</title>
		<author>
			<persName><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1212</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Q-BERT: Hessian based ultra low precision quantization of BERT</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8815" to="8821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02984</idno>
		<title level="m">MobileBERT: a compact task-agnostic BERT for resource-limited devices</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ledgar: A large-scale multilabel corpus for text classification of legal provisions in contracts</title>
		<author>
			<persName><forename type="first">Don</forename><surname>Tuggener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Pius Von Däniken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Peetz</surname></persName>
		</author>
		<author>
			<persName><surname>Cieliebak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1235" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5776" to="5788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
