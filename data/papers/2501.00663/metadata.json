{
  "arxivId": "2501.00663",
  "title": "Titans: Learning to Memorize at Test Time",
  "authors": "Ali Behrouz, Peilin Zhong, Vahab Mirrokni",
  "abstract": "Over more than a decade there has been an extensive research effort on how to\neffectively utilize recurrent models and attention. While recurrent models aim\nto compress the data into a fixed-size memory (called hidden state), attention\nallows attending to the entire context window, capturing the direct\ndependencies of all tokens. This more accurate modeling of dependencies,\nhowever, comes with a quadratic cost, limiting the model to a fixed-length\ncontext. We present a new neural long-term memory module that learns to\nmemorize historical context and helps attention to attend to the current\ncontext while utilizing long past information. We show that this neural memory\nhas the advantage of fast parallelizable training while maintaining a fast\ninference. From a memory perspective, we argue that attention due to its\nlimited context but accurate dependency modeling performs as a short-term\nmemory, while neural memory due to its ability to memorize the data, acts as a\nlong-term, more persistent, memory. Based on these two modules, we introduce a\nnew family of architectures, called Titans, and present three variants to\naddress how one can effectively incorporate memory into this architecture. Our\nexperimental results on language modeling, common-sense reasoning, genomics,\nand time series tasks show that Titans are more effective than Transformers and\nrecent modern linear recurrent models. They further can effectively scale to\nlarger than 2M context window size with higher accuracy in needle-in-haystack\ntasks compared to baselines.",
  "url": "https://arxiv.org/abs/2501.00663",
  "issue_number": 984,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/984",
  "created_at": "2025-01-10T08:07:02.306242",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 11,
  "last_read": "2025-01-10T08:07:02.307560",
  "last_visited": "2025-01-17T02:36:23.950Z",
  "main_tex_file": null,
  "published_date": "2024-12-31T22:32:03Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ]
}