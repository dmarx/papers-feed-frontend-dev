<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HunyuanVideo: A Systematic Framework For Large Video Generative Models &quot;Bridging the gap between closed-source and open-source video foundation models to accelerate community exploration.&quot; -Hunyuan Foundation Model Team</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-02">2 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">HunyuanVideo: A Systematic Framework For Large Video Generative Models &quot;Bridging the gap between closed-source and open-source video foundation models to accelerate community exploration.&quot; -Hunyuan Foundation Model Team</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-02">2 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">D26CF6D9DB0BA30A4BF34AD26139A967</idno>
					<idno type="arXiv">arXiv:2412.03603v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in video generation have profoundly transformed daily life for individuals and industries alike. However, the leading video generation models remain closed-source, creating a substantial performance disparity in video generation capabilities between the industry and the public community. In this report, we present HunyuanVideo, a novel open-source video foundation model that exhibits performance in video generation that is comparable to, if not superior to, leading closed-source models. HunyuanVideo features a comprehensive framework that integrates several key contributions, including data curation, advanced architecture design, progressive model scaling and training, and an efficient infrastructure designed to facilitate large-scale model training and inference. With those, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to professional human evaluation results, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and 3 top performing Chinese video generative models. By releasing the code of the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower everyone in the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at <ref type="url" target="https://github.com/Tencent/HunyuanVideo">https://github.com/Tencent/HunyuanVideo</ref>. Figure 1: Non-curated multi-ratio generation samples with HunyuanVideo, showing realistic, concept generalization and automatic scene-cut features. Hunyuan Foundation Model Team Contributors are listed at the end of the report .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With extensive pre-training and advanced architectures, diffusion models <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b46">47]</ref> have demonstrated superior performance in generating high-quality images and videos compared to previous generative adversarial network (GAN) methods <ref type="bibr" target="#b5">[6]</ref>. However, unlike the image generation field, which has seen a proliferation of novel algorithms and applications across various open platforms, diffusion-based video generative models remain relatively inactive. We contend that one of the primary reasons for this stagnation is the lack of robust open-source foundation models as in T2I filed <ref type="bibr" target="#b46">[47]</ref>. In contrast to the image generative model community, a significant gap has emerged between open-source and closed-source video generation models. Closed-source models tend to overshadow publicly available open-source alternatives, severely limiting the potential for algorithmic innovation from the public community. While the recent state-of-the-art model MovieGen <ref type="bibr" target="#b66">[67]</ref> has demonstrated promising performance, its milestone for open-source release has yet to be established.   <ref type="bibr" target="#b64">[65]</ref> trained with Flow Matching <ref type="bibr" target="#b51">[52]</ref> was not sufficiently efficient. Consequently, we explored an effective scaling strategy that can reduce computational resource requirements by up to 5× while achieving the desired model performance. With this optimal scaling approach and dedicated infrastructure, we successfully trained a large video model comprising 13 billion parameters, pre-training it on internetscale images and videos. After a dedicated progressive fine-tuning strategy, HunyuanVideo excels in four critical aspects of video generation: visual quality, motion dynamics, video-text alignment, and semantic scene cut. We conducted a comprehensive comparison of HunyuanVideo with leading global video generation models, including Gen-3 and Luma 1.6 and 3 top performing commercial models in China, using over 1,500 representative text prompts accessed by a group of 60 people.</p><p>The results indicate that HunyuanVideo achieves the highest overall satisfaction rates, particularly excelling in motion dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>HunyuanVideo is a comprehensive video training system encompassing all aspects from data processing to model deployment. This technical report is structured as follows:</p><p>• In Section 3, we introduce our data preprocessing techniques, including filtering and recaptioning models. • Section 4 presents detailed information about the architecture of all components of Hun-yuanVideo, along with our training and inference strategies. • In Section 5, we discuss methods for accelerating model training and inference, enabling the development of a large model with 13 billion parameters. • Section 6 evaluates the performance of our text-to-video foundation models and compares them with state-of-the-art video generation models, both open-source and proprietary.</p><p>• Finally, in Section 7, we showcase various applications built on the pre-trained foundation model, accompanied by relevant visualizations as well as some video related functional models such as video to audio generative model. 3 Data Pre-processing</p><p>We use an image-video joint training strategy. The videos are meticulously divided into five distinct groups, while images are categorized into two groups, each tailored to fit the specific requirements of their respective training processes. This section will primarily delve into the intricacies of video data curation.</p><p>Our data acquisition process is rigorously governed by the principles outlined in the General Data Protection Regulation (GDPR) <ref type="bibr" target="#b38">[39]</ref> framework. Furthermore, we employ advanced techniques such as data synthesis and privacy computing to guarantee compliance with these stringent standards.</p><p>Our raw data pool initially comprised videos spanning a wide range of domains including people, animals, plants, landscapes, vehicles, objects, buildings, and animation. Each video was acquired with a set of basic thresholds, including minimum duration requirements. Additionally, a subset of the data was collected based on more stringent criteria, such as spatial quality, adherence to a specific aspect ratio, and professional standards in composition, color, and exposure. These rigorous standards ensure that our videos possess technical quality and aesthetic appeal. We experimentally verified that incorporating high-quality data is instrumental in significantly enhancing model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Filtering</head><p>Our raw data from different sources exhibits varying durations and levels of quality. To address this, we employ a series of techniques to pre-process the raw data. Firstly, we utilize PySceneDetect <ref type="bibr" target="#b18">[19]</ref> to split raw videos into single-shot video clips. Next, we employ the Laplacian operator from OpenCV <ref type="bibr" target="#b17">[18]</ref> to identify a clear frame, serving as the starting frame of each video clip. Using an internal VideoCLIP model, we calculate embeddings for these video clips. These embeddings serve two purposes: (i) we deduplicate similar clips based on the Cosine distance of their embeddings; (ii) we apply k-means <ref type="bibr" target="#b58">[59]</ref> to obtain ∼10K concept centroids for concept resampling and balancing.</p><p>To continuously enhance video aesthetics, motion, and concept range, we implement a hierarchical data filtering pipeline for constructing training datasets, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. This pipeline incorporates various filters to help us filter data from different perspectives which we introduce next.</p><p>We employ Dover <ref type="bibr" target="#b84">[85]</ref> to assess the visual aesthetics of video clips from both aesthetic and technical viewpoints. Additionally, we train a model to determine clarity and eliminate video clips with visual blurs. By predicting the motion speed of videos using estimated optical flow <ref type="bibr" target="#b17">[18]</ref>, we filter out static or slow-motion videos. We combine the results from PySceneDetect <ref type="bibr" target="#b18">[19]</ref> and Transnet v2 <ref type="bibr" target="#b75">[76]</ref> to get scene boundary information. We utilize an internal OCR model to remove video clips with excessive text, as well as to locate and crop subtitles. We also develop YOLOX <ref type="bibr" target="#b23">[24]</ref>-like visual models to detect and remove some occluded or sensitive information such as watermarks, borders, and logos. To assess the effectiveness of these filters, we perform simple experiments using a smaller HunyuanVideo</p><p>Pool Pool Pool Pool Pool 256p 256p 256p 256p 256p DeDuplication DeDuplication DeDuplication DeDuplication DeDuplication Motion Filter Motion Filter Motion Filter Motion Filter Motion Filter Other Filters Other Filters Other Filters Other Filters Other Filters 360p 360p 360p 360p 360p OCR Filter OCR Filter OCR Filter OCR Filter OCR Filter Clarity Filter Clarity Filter Clarity Filter Clarity Filter Clarity Filter Other Filters Other Filters Other Filters Other Filters Other Filters 540p 540p 540p 540p 540p Aesthetic Filter Aesthetic Filter Aesthetic Filter Aesthetic Filter Aesthetic Filter Other Filters Other Filters Other Filters Other Filters Other Filters 720p 720p 720p 720p 720p Source Filter Source Filter Source Filter Source Filter Source Filter SFT SFT SFT SFT SFT Manually Filter Manually Filter Manually Filter Manually Filter Manually Filter model and observe the performance changes. The results obtained from these experiments play an important role in guiding the building of our data filtering pipeline, which is introduced next.</p><p>Our hierarchical data filtering pipeline for video data yields five training datasets, corresponding to the five training stages (Section 4.5). These datasets (except for the last fine-tuning dataset) are curated by progressively improving the thresholds of the aforementioned filters. The video spatial resolution increases progressively from 256 × 256 × 65 to 720×1280 × 129. We apply varying levels of strictness to the filters during the threshold adjustment process at different stages (see Figure <ref type="figure" target="#fig_3">4</ref>). The last dataset used for fine-tuning is described next.</p><p>To improve the model's performance in the final stage (Section 4.7), we build a fine-tuning dataset comprising ∼1M samples. This dataset is meticulously curated through human annotation. Annotators are assigned the task of identifying video clips that exhibit high visual aesthetics and compelling content motion. Each video clip undergoes evaluation based on two perspectives: (i) decomposed aesthetical views, including color harmony, lighting, object emphasis, and spatial layout; (ii) decomposed motion views, encompassing motion speed, action integrity, and motion blurs. Finally, our fine-tuning dataset consists of visually appealing video clips with intricate motion details.</p><p>We also establish a hierarchical data filtering pipeline for images by reusing most of the filters, excluding the motion-related ones. Similarly, we build two image training datasets by progressively increasing the filtering thresholds applied to an image pool of billions of image-text pairs. The first dataset contains billions of samples and is used for the initial stage of text-to-image pre-training.</p><p>The second dataset contains hundreds of millions of samples and is utilized for the second stage of text-to-image pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Annotation</head><p>Structured Captioning. As evidenced in research <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4]</ref>, the precision and comprehensiveness of captions play a crucial role in improving the prompt following ability and output quality of generative models. Most previous work focus on providing either brief captions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50]</ref> or dense captions <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. However, these approaches are not without shortcomings, suffering from incomplete information, redundant discourse and inaccuracies. In pursuit of achieving captions with higher comprehensiveness, information density and accuracy, we develop and implement an in-house Vision Language Model(VLM) designed to generate structured captions for images and videos. These structured captions, formatted in JSON, provide multi-dimensional descriptive information from various perspectives, including: 1) Short Description: Capturing the main content of the scene. 2) Dense Description: Detailing the scene's content, which notably includes scene transitions and camera movements that are integrated with the visual content, such as camera follows some subject.</p><p>3) Background: Describing the environment in which the subject is situated. 4) Style: Characterizing the style of the video, such as documentary, cinematic, realistic, or sci-fi. 5) Shot Type: Identifying the type of video shot that highlights or emphasizes specific visual content, such as aerial shot, close-up shot, medium shot, or long shot. 6) Lighting: Describing the lighting conditions of the video. 7) Atmosphere: Conveying the atmosphere of the video, such as cozy, tense, or mysterious.</p><p>Moreover, we extend the JSON structure to incorporate additional metadata-derived elements, including source tags, quality tags, and other pertinent tags from meta information of images and videos.</p><p>Through the implementation of a carefully designed dropout mechanism coupled with permutation and combination strategies, we synthesize captions diverse in length and pattern by assembling these multi-dimensional descriptions for each image and video, aiming to improve the generalization ability of generative models and prevent overfitting. We utilize this captioner to provide structured captions for all images and videos in our training dataset.</p><p>Camera Movement Types. We also train a camera movement classifier capable of predicting 14 distinct camera movement types, including zoom in, zoom out, pan up, pan down, pan left, pan right, tilt up, tilt down, tilt left, tilt right, around left, around right, static shot and handheld shot. High-confidence predictions of camera movement types are integrated into the JSON-formatted structured captions, to enable camera movement control ability of generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Architecture Design</head><p>The overview of our HunyuanVideo model is shown in Fig. <ref type="figure" target="#fig_4">5</ref>. This section describes the Causal 3D VAE, diffusion backbone, and scaling laws experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">3D Variational Auto-encoder Design</head><p>Similar to previous work <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b92">93]</ref>, we train a 3DVAE to compress pixel-space videos and images into a compact latent space. To handle both videos and images, we adopt CausalConv3D <ref type="bibr" target="#b94">[95]</ref>. For a video of shape (T + 1) × 3 × H × W , our 3DVAE compresses it into latent features with shape</p><formula xml:id="formula_0">( T ct + 1) × C × ( H cs ) × ( W cs ).</formula><p>In our implementation, c t = 4, c s = 8, and C = 16. This compression significantly reduces the number of tokens for the subsequent diffusion transformer model, allowing us to train videos at the original resolution and frame rate. The model structure is illustrated in Figure <ref type="figure">6</ref>. Figure <ref type="figure">6</ref>: The architecture of our 3DVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training</head><p>In contrast to most previous work <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b103">104]</ref>, we do not rely on a pre-trained image VAE for parameter initialization; instead, we train our model from scratch. To balance the reconstruction quality of videos and images, we mix video and image data at a ratio of 4 : 1. Besides the routinely used L 1 reconstruction loss and KL loss L kl , we also incorporate perceptual loss L lpips and GAN adversarial loss L adv <ref type="bibr" target="#b21">[22]</ref> to enhance the reconstruction quality. The complete loss function is shown in Equation <ref type="formula" target="#formula_1">1</ref>.</p><formula xml:id="formula_1">Loss = L 1 + 0.1L lpips + 0.05L adv + 10 -6 L kl<label>(1)</label></formula><p>During training, we employ a curriculum learning strategy, gradually training from low-resolution short video to high-resolution long video. To improve the reconstruction of high-motion videos, we randomly choose a sampling interval from the range 1 ∼ 8 to sample frames evenly across video clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Inference</head><p>Encoding and decoding high-resolution long videos on a single GPU can lead to out-of-memory (OOM) errors. To address this, we use a spatial-temporal tiling strategy, splitting the input video into overlapping tiles along the spatial and temporal dimensions. Each tile is encoded/decoded separately, and the outputs are stitched together. For the overlapping regions, we utilize a linear combination for blending. This tiling strategy allows us to encode/decode videos in arbitrary resolutions and durations on a single GPU.</p><p>We observed that directly using the tiling strategy during inference can result in visible artifacts due to inconsistencies between training and inference. To solve this, we introduce an additional finetuning phase where the tiling strategy is randomly enabled/disabled during training. This ensures the model is compatible with both tiling and non-tiling strategies, maintaining consistency between training and inference.</p><p>Table 1 compares our VAE with open-source state-of-the-art VAEs. On video data, our VAE demonstrates a significantly higher PSNR compared to other video VAEs. On images, our performance surpasses both video VAEs and image VAE. Figure 7 shows several cases at 256 × 256 resolution. Our VAE demonstrates significant advantages in text, small faces, and complex textures.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unified Image and Video Generative Architecture</head><p>In this section, we introduce the Transformer design in HunyuanVideo, which employs a unified Full Attention mechanism for three main reasons: Firstly, it has demonstrated superior performance compared to divided spatiotemporal attention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b78">79]</ref>. Secondly, it supports unified generation for both images and videos, simplifying the training process and improving model scalability. Lastly, it leverages existing LLM-related acceleration capabilities more effectively, enhancing both training and inference efficiency. The model structure is illustrated in Figure <ref type="figure" target="#fig_7">8</ref>.</p><p>Inputs. For a given video-text pair, the model operates within the 3D latent space described in Section 4.1. Specifically, for the video branch, the input is first compressed into latents of shape T × C × H × W . To unify input processing, we treat images as single-frame videos. These latents are then patchified and unfolded into a 1D sequence of tokens with a length of T kt • H k h • W kw using a 3D convolution with a kernel size of k t × k h × k w . For the text branch, we first use an advanced LLM to encode the text into a sequence of embeddings that capture fine-grained semantic information. Concurrently, we employ the CLIP model to extract a pooled text representation containing global information. This representation is then expanded in dimensionality and added to the timestep embedding before being fed into the model.</p><p>Model Design. To integrate textual and visual information effectively, we follow a similar strategy of "Dual-stream to Single-stream" hybrid model design as introduced in <ref type="bibr" target="#b46">[47]</ref> for video generation.</p><p>In the dual-stream phase, video and text tokens are processed independently through multiple Transformer blocks, enabling each modality to learn its own appropriate modulation mechanisms without interference. In the single-stream phase, we concatenate the video and text tokens and feed them into subsequent Transformer blocks for effective multimodal information fusion. This design captures complex interactions between visual and semantic information, enhancing overall model performance.</p><p>Position Embedding. To support multi-resolution, multi-aspect ratio, and varying duration generation, we use Rotary Position Embedding (RoPE) <ref type="bibr" target="#b76">[77]</ref> in each Transformer block. RoPE applies a rotary frequency matrix to the embeddings, enhancing the model's ability to capture both absolute and relative positional relationships, and demonstrating some extrapolation capability in LLMs. Given the added complexity of the temporal dimension in video data, we extend RoPE to three dimensions. Specifically, we compute the rotary frequency matrix separately for the coordinates of time (T ), height (H), and width (W ). We then partition the feature channels of the query and key into three segments (d t , d h , d w ), multiply each segment by the corresponding coordinate frequencies and concatenate the segments. This process yields position-aware query and key embeddings, which are used for attention computation.</p><p>For detailed model settings, please refer to Table <ref type="table" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Text encoder</head><p>In generation tasks like text-to-image and text-to-video, the text encoder plays a crucial role by providing guidance information in the latent space. Some representative works <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">51]</ref> typically use pre-trained CLIP <ref type="bibr" target="#b68">[69]</ref> and T5-XXL <ref type="bibr" target="#b70">[71]</ref> as text encoders where CLIP uses Transformer Encoder and T5 uses an Encoder-Decoder structure. In contrast, we utilize a pre-trained Multimodal Large Language Model (MLLM) with a Decoder-Only structure as our text encoder, which has following advantages: (i) Compared with T5, MLLM after visual instruction finetuning has better image-text alignment in the feature space, which alleviates the difficulty of instruction following in diffusion models; (ii) Compared with CLIP, MLLM has been demonstrated superior ability in image detail description and complex reasoning <ref type="bibr" target="#b52">[53]</ref>; (iii) MLLM can play as a zero-shot learner <ref type="bibr" target="#b7">[8]</ref> by following system instructions prepended to user prompts, helping text features pay more attention to key information. In addition, as shown in Fig. <ref type="figure" target="#fig_8">9</ref>, MLLM is based on causal attention while T5-XXL utilizes bidirectional attention that produces better text guidance for diffusion models. Therefore, we follow <ref type="bibr" target="#b54">[55]</ref> to introduce an extra bidirectional token refiner for enhancing text features. We have configured HunyuanVideo with a series of MLLMs <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref> for different purposes. Under each setting, MLLMs have shown superior performance over conventional text encoder.</p><p>In addition, CLIP text features are also valuable as the summary of the text information. As shown in Fig. <ref type="figure" target="#fig_7">8</ref> We adopt the final non-padded token of CLIP-Large text features as a global guidance, integrating into the dual-stream and single-stream DiT blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Scaling</head><p>Neural scaling laws <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b35">36]</ref> in language model training offer a powerful tool for understanding and optimizing the performance of machine learning models. By elucidating the relationships between model size (N ), dataset size (D), and computational resources (C), these laws help drive the development of more effective and efficient models, ultimately advancing the success of large model training.  In contrast to prior scaling laws on large language models [41, 36, 81, 1, 2] and image generation models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b41">42]</ref>, video generation models typically rely on pre-trained image models. Consequently, our initial step involved establishing the foundational scaling laws pertinent to text-to-image. Building upon these foundational scaling laws, we subsequently derived the scaling laws applicable to the text-to-video model. By integrating these two sets of scaling laws, we were able to systematically determine the appropriate model and data configuration for video generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Image model scaling law</head><p>Kaplan et.al <ref type="bibr" target="#b40">[41]</ref> and Hoffmann et.al <ref type="bibr" target="#b35">[36]</ref> explored emperical scaling laws for language models on cross-entropy loss. In the field of diffusion based visual generation, Li et.al <ref type="bibr" target="#b48">[49]</ref> study the scaling properties on UNet, while transformer based works such as DiT <ref type="bibr" target="#b64">[65]</ref>, U-ViT <ref type="bibr" target="#b2">[3]</ref>, Lumina-T2X <ref type="bibr" target="#b22">[23]</ref>, and SD3 <ref type="bibr" target="#b20">[21]</ref> only study the scaling behavior between sample quality and network complexity, leaving the power-laws about the computation resources and MSE loss used by diffusion models unexplored.</p><p>In order to fill the gap, we develop a family of DiT-like models, named as DiT-T2X to distinguish from the original DiT, where X can be the image (I) or the video (V). DiT-T2X applies T5-XXL <ref type="bibr" target="#b70">[71]</ref> as the text encoder and the aformentioned 3D VAE as the image encoder. The text information is injected to the model according to cross-attention layers. The DiT-T2X family has seven sizes ranging from 92M to 6.6B. The models were trained using DDPM <ref type="bibr" target="#b33">[34]</ref> and v-prediction <ref type="bibr" target="#b72">[73]</ref> with consistent hyperparameters and the same dataset with 256px resolution. We follow the experiment method introduced by <ref type="bibr" target="#b35">[36]</ref> and build the neural scaling laws to fit</p><formula xml:id="formula_2">N opt = a 1 C b1 , D opt = a 2 C b2 .<label>(2)</label></formula><p>As shown in Fig. <ref type="figure" target="#fig_9">10</ref> (a), the loss curve of each model decreases from top left to bottom right, and it always passes through the loss curve of the larger size model adjacent to it. It means that each curve will form two intersections with curves of the larger and the smaller models. Under the corresponding computation resources between the two intersections, the middle-sized model is optimal (with the lowest loss). After obtaining the envelope of lowest losses across all the x-axis values, we fill the Equation ( <ref type="formula" target="#formula_2">2</ref>) to find out that a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model-pretraining</head><p>We use Flow Matching <ref type="bibr" target="#b51">[52]</ref> for model training and split the training process into multiple stages.</p><p>We first pretrain our model on 256px and 512px images, then conduct joint training on images and videos from 256px to 960px.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Training Objective</head><p>In this work, we employ the Flow Matching framework <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13]</ref> to train our image and video generation model. Flow Matching transforms a complex probability distribution into a simple probability distribution through a series of variable transformations of the probability density function, and generates new data samples through inverse transformations.</p><p>During the training process, given an image or video latent representation x 1 in the training set. We first sample t ∈ [0, 1] from a logit-normal distribution <ref type="bibr" target="#b20">[21]</ref> and initialize a noise x 0 ∼ N (0, I) following Gaussion distribution. The training sample x t is then constructed using a linear interpolation method <ref type="bibr" target="#b51">[52]</ref>. The model is trained to predict the velocity u t = dx t /dt, which guides the sample x t towards the sample x 1 . The model parameters are optimized by minimizing the mean squared error between the predicted velocity v t and the ground truth velocity u t , expressed as the loss function</p><formula xml:id="formula_3">L generation = E t,x0,x1 ∥v t -u t ∥ 2 . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>During the inference process, a noise sample x 0 ∼ N (0, I) is drawn initially. The first-order Euler ordinary differential equation (ODE) solver is then used to compute x 1 by integrating the model's estimated values for dx t /dt. This process ultimately generates the final sample x 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Image Pre-training</head><p>At our early experiments, we found that a well pretrained model significantly accelerates the convergence of video training and improves video generation performance. Therefore, we introduce a two-stage progressive image pretraining strategy to serve as a warmup for video training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image stage 1 (256px training).</head><p>The model is first pretrained with low-resolution 256px images. Specifically, we follow previous work <ref type="bibr" target="#b65">[66]</ref> to enable multi-aspect training based on 256px, which helps the model learn to generate images with a wide range of aspect ratios while avoiding the textimage misalignments caused by the crop operation in image preprocessing. Meanwhile, pretraining with low resolution samples allows the model to learn more low-frequency concepts from a larger amount of samples.</p><p>Image stage 2 (mix-scale training). We introduce a second image-pretraining stage to further facilitate the model ability on higher resolutions, such as 512px. A trivial solution is to directly funetuning on images based on 512px. However, we found that the model performance finetuned on 512px images will degrade severely on 256px image generation, which may affect the following video pretraining on 256px videos. Therefore, we propose mix-scale training, where two or more scales of multi-aspect buckets are included for each training global batch. Each scale have an anchor size, and then the multi-aspect buckets are built based on the anchor size. We train the model on a two-scale dataset with anchor sizes 256px and 512px for learning higher resolution images while maintaining the ability on low resolutions. We also introduce dynamic batch sizes for micro batches with different image scales, maximaizing the GPU memory and computation utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Video-Image joint training</head><p>Multiple aspect ratios and durations bucketization. After the data filtering process described in Section 3.1, the videos have different aspect ratios and durations. To effectively utilize the data, we categorize the training data into buckets based on duration and aspect ratio. We create B T duration buckets and B AR aspect ratio buckets, resulting in a total of B T × B AR buckets. As the number of tokens varies across buckets, we assign each bucket a maximum batch size that can prevent out-of-memory (OOM) errors, to optimize GPU resource utilization. Before training, all data is allocated to the nearest bucket. During training, each rank randomly pre-fetches batch data from a bucket. This random selection ensures the model is trained on varying data sizes at each step, which helps maintain model generalization by avoiding the limitations of training on a single size.</p><p>Progressive Video-Image Joint Training. Generating high-quality, long-duration video sequences directly from text often leads to difficulties in model convergence and suboptimal results. Therefore, progressive curriculum learning has become a widely adopted strategy for training text-to-video models. In HunyuanVideo, we designed a comprehensive curriculum learning strategy, starting with model initialization using T2I parameters and progressively increasing video duration and resolution.</p><p>• Low-resolution, short video stage. The model establishes the basic mapping between text and visual content, ensuring consistency and coherence in short-term actions. • Low-resolution, long video stage. The model learns more complex temporal dynamics and scene changes, ensuring temporal and spatial consistency over a longer duration. • High-resolution, long video stage. The model enhances video resolution and detail quality while maintaining temporal coherence and managing complex temporal dynamics.</p><p>Additionally, at each stage, we incorporate images in varying proportions for video-image joint training. This approach addresses the scarcity of high-quality video data, enabling the model to learn more extensive and diverse world knowledge. It also effectively prevents catastrophic forgetting of image-space semantics due to distributional discrepancies between video and image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Prompt Rewrite</head><p>To address the variability in linguistic style and length of user-provided prompts, we employ the Hunyuan-Large model <ref type="bibr" target="#b77">[78]</ref> as our prompt rewrite model to adapt the original user prompt to the modelpreferred prompt. Utilized within a training-free framework, the prompt rewrite model capitalizes on detailed prompt instructions and in-context learning examples to enhance its performance. The key functionalities of this prompt rewrite module are as follows:</p><p>• Multilingual Input Adaptation: The module is designed to process and comprehend user prompts across various languages, ensuring that meaning and context are preserved. • Standardization of Prompt Structure: The module rephrases prompts to conform to a standardized information architecture, akin to training captions. • Simplification of Complex Terminology: The module simplifies complex user wording into more straightforward expressions, all while maintaining the user's original intent.</p><p>Furthermore, we implement a self-revision technique <ref type="bibr" target="#b42">[43]</ref> to refine the final prompt. This involves a comparative analysis between the original prompt and the rewritten version, ensuring that the output is both accurate and aligned with the model's capabilities.</p><p>To accelerate and simplify the application process, we also fine-tune a Hunyuan-Large model with LoRA for prompt rewriting. The training data for this LoRA tuning was sourced from the high-quality rewrite pairs collected through the training-free method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">High-performance Model Fine-tuning</head><p>In the pre-training stage, we utilized a large dataset for model training. While this dataset is rich in information, it displayed considerable variability in data quality. To create a robust generation model capable of producing high-quality, dynamic videos and improving its proficiency in continuous motion control and character animation, we carefully selected four specific subsets from the full dataset for fine-tuning. These subsets underwent an initial screening using automated data filtering techniques, followed by manual review. Additionally, we implemented various model optimization strategies to maximize generation performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Acceleration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Inference Step Reduction</head><p>To improve the inference efficiency, we firstly consider reducing the number of inference steps. Compared to image generation, it is more challenging to maintain the spatial and temporal quality of the generated videos with lower inference steps. Inspired by a previous observation that the first time-steps contribute to most changes during the generation process <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b98">99]</ref>, we utilize the time-step shifting to handle the case of lower inference steps. Specifically, given the inference step q ∈ {1, 2, ..., Q}, t = 1 -q Q is the input time condition for the generation model, where the noise is initialized at t = 1 and the generation process halts at t = 0. Instead of using t directly, we map t to t ′ with a shifting function t ′ = s * t 1+(s-1) * t , where t ′ is the input time condition and s is the shifting factor. If s &gt; 1, the flow model is conditioned more on early time steps. A critical observation is that a lower inference step requires a larger shifting factor s. Empirically, s is set as 7 for 50 inference steps, while s should be increased to 17 when the number of inference steps is smaller than 20. The time-step shifting strategy enables the generation model to match the results of numerous inference steps with a reduced number of steps.</p><p>MovieGen <ref type="bibr" target="#b66">[67]</ref> applies the linear-quadratic scheduler to achieve a similar goal. The schedulers are visualized in Figure <ref type="figure" target="#fig_12">11a</ref>. However, we find that our time-step shifting is more effective than the linear-quadratic scheduler in the case of extremely low number of inference steps, e.g., 10 steps. As shown in Figure <ref type="figure" target="#fig_12">11b</ref>, the linear-quadratic scheduler results in worse visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Text-guidance Distillation</head><p>Classifier-free guidance (CFG) <ref type="bibr" target="#b34">[35]</ref> significantly improves the sample quality and motion stability of text-conditioned diffusion models. However, it increases computational cost and inference latency since it requires extra output for the unconditional input at each inference step. Especially for the large video model and high-resolution video generation, the inference burden is extremely expensive when generating text-conditional and text-unconditional videos, simultaneously. To solve this limitation, we distill the combined output for unconditional and conditional inputs into a single student model <ref type="bibr" target="#b59">[60]</ref>. Specifically, the student model is conditioned on a guidance scale and shares the same structures and hyper-parameters as the teacher model. We initialize the student model with the same parameters as the teacher model and train with the guidance scale randomly sampled from 1 to 8. We experimentally find that text-guidance distillation approximatively brings 1.9x acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Efficient and Scalable Training</head><p>To achieve scalability and efficient training, we train HunyuanVideo on AngelPTM <ref type="bibr" target="#b61">[62]</ref>, the largescale pretraining framework from Tencent Angel machine learning team. In this part, we first outline the hardware and infrastructure used for training, and then give a detailed introduction to the model parallel method and its optimization methods, followed by the automatic fault tolerance mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Hardware Infrastucture</head><p>To ensure efficient communication in large-scale distributed training, we setup a dedicated distributed training framework termed Tencent XingMai network <ref type="bibr" target="#b47">[48]</ref> for highly efficient inter-server communication. The GPU scheduling for all training tasks is completed through the Tencent Angel machine learning platform, which provides powerful resource management and scheduling capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Parallel Strategy</head><p>HunyuanVideo training adopts 5D parallel strategies, including tensor parallelism (TP) <ref type="bibr" target="#b73">[74]</ref>, sequence parallelism (SP) <ref type="bibr" target="#b44">[45]</ref>, context parallelism (CP) <ref type="bibr" target="#b62">[63]</ref>, and data parallelism combined with Zero optimization (DP + ZeroCache <ref type="bibr" target="#b61">[62]</ref>). The tensor parallelism (TP) is based on the principle of block calculation of matrices. The model parameters (tensors) are divided into different GPUs to reduce GPU memory usage and accelerate the calculation. Each GPU is responsible for the calculation of different parts of tensors in the layer.</p><p>Sequence parallelism (SP) is based on TP. The input sequence dimension is sliced to reduce the repeated calculation of operators such as LayerNorm and Dropout, and reduce the storage of the same activations, which effectively reduces the waste of computing resources and GPU memory. In addition, for input data that does not meet the SP requirements, the engineering equivalent SP Padding capability is supported.</p><p>Context parallelism (CP) is sliced in the sequence dimension to support long-sequence training. Each GPU is responsible for calculating the Attention of different sequence slices. Specifically, Ring Attention <ref type="bibr" target="#b29">[30]</ref> is used to achieve efficient training of long sequences through multiple GPUs, breaking through the GPU memory limitation of a single GPU.</p><p>In addition, data parallelism + ZeroCache is leveraged to support horizontal expansion through data parallelism to meet the demand for increasing training data sets. Then, based on data parallelism, the ZeroCache optimization strategy is used to further reduce the redundancy of model states (model parameters, gradients, and optimizer states), and we unify the use of GPU memory to maximize the GPU memory usage efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Optimization</head><p>Attention optimization. As the sequence length increases, the attention calculation becomes the main bottleneck of training. We accelerated the attention calculation with FusedAttention.</p><p>Recomputation and activations offload optimization. Recomputation is a technology that trade calculations for storage. It is mainly made up of three parts: a) specifying certain layers or blocks for recalculation, b) releasing the activations in the forward calculation, and c) obtaining the dependent activations through recalculation in the backward calculation, which significantly reduces the use of GPU memory during training. In addition, considering the PCIe bandwidth and the host memory size, a layer-based activation offload strategy is adopted. Without reducing the training performance, the activations in the GPU memory are offloaded to the host memory, further saving GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Automatic fault tolerance</head><p>In terms of the large-scale training stability of HunyuanVideo, an automatic fault tolerance mechanism is used to quickly restore training for common hardware failures. This avoids frequent occurrence of the manual recovery of training tasks. By automatically detecting errors and quickly replacing healthy nodes to pull up training tasks, the training stability is 99.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Fundation Model Performance</head><p>Text Alignment One of the key metrics for video generative models is their ability to follow text prompts accurately. This capability is essential for the effectiveness of these models. However, some open-source models often struggle to capture all subjects or accurately represent the relationships between multiple subjects, particularly when the input text prompt is complex. HunyuanVideo demonstrates robust capabilities in generating videos that closely adhere to the provided text prompts. As illustrated in Figure <ref type="figure" target="#fig_1">12</ref>, it effectively manages multiple subjects within the scene.</p><p>Figure <ref type="figure" target="#fig_1">12</ref>: Prompt: A white cat sits on a white soft sofa like a person, while its long-haired male owner, with his hair tied up in a topknot, sits on the floor, gazing into the cat's eyes. His child stands nearby, observing the interaction between the cat and the man.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-quality</head><p>We also perform a fine-tuning process to enhance the spatial quality of the generated videos. As illustrated in Figure <ref type="figure" target="#fig_13">13</ref>, HunyuanVideo is capable of producing videos with ultra-detailed content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-motion Dynamics</head><p>In this part, we demonstrate HunyuanVideo's capabilities in producing high-dynamic videos based on given prompts. As shown in Figure <ref type="figure" target="#fig_15">14</ref>, our model excels in generating videos that encompass a wide range of scenes and various types of motion.</p><p>Concept Generalization One of the most desirable features of a generative model is its ability to generalize concepts. As illustrated in Figure <ref type="figure" target="#fig_16">15</ref>, the text prompt describes a scene: "In a distant galaxy, an astronaut floats on a shimmering, pink, gemstone-like lake that reflects the vibrant colors of the surrounding sky, creating a stunning scene. The astronaut gently drifts on the lake's surface, while the soft sounds of water whisper the planet's secrets. He reaches out, his fingertips gliding (a) Prompt: the ultra-wide-angle lens follows closely from the hood, with raindrops continuously splattering against the lens. Ahead, a sports car speeds around a corner, its tires violently skidding against the wet road, creating a mist of water. Neon lights refract in the rain, leaving colorful streaks on the car's surface. The camera swiftly shifts to the side of the car, capturing the wheels spinning at high speed, before finally moving to the rear.</p><p>(b) Prompt: a stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.   over the cool, smooth water." Notably, this specific scenario has not been encountered in the training dataset. Furthermore, it is evident that the depicted scene combines several concepts that are also absent from the training data. to the text prompts (1) 'In a distant galaxy, an astronaut floats on a shimmering, pink, gemstone-like lake that reflects the vibrant colors of the surrounding sky, creating a stunning scene. The astronaut gently drifts on the lake's surface, the soft sounds of water whispering the planet's secrets. He reaches out, his fingertips gliding over the cool, smooth water. ', (2) 'A macro lens captures a tiny orchestra of insects playing instruments.' and (3) 'The night-blooming cactus flowers in the evening, with a brief, rapid closure. Time-lapse shot, extreme close-up. Realistic, Night lighting, Mysterious.' respectively.</p><p>Action Reasoning and Planning Leveraging the capabilities of large language models, Hunyuan-Video can generate sequential movements based on a provided text prompt. As demonstrated in Figure <ref type="figure" target="#fig_17">16</ref>, HunyuanVideo effectively captures all actions in a photorealistic style.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison with SOTA Models</head><p>To evaluate the performance of HunyuanVideo, we selected five strong baselines from closed-source video generation models. In total, we utilized 1,533 text prompts, generating an equal number of video samples with HunyuanVideo in a single run. For a fair comparison, we conducted inference only once, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models, ensuring consistent video resolution. 60 professional evaluators performed the evaluation and the results are presented in Table <ref type="table" target="#tab_4">3</ref>. Videos  7 Applications</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Audio Generation based on Video</head><p>Our video-to-audio(V2A) module is designed to enhance generated video content by incorporating synchronized sound effects and contextually appropriate background music. Within the conventional film production pipeline, Foley sound design constitutes an integral component, significantly contributing to the auditory realism and emotional depth of visual media. However, the creation of Foley audio is both time-intensive and demands a high degree of professional expertise. With the advent of an increasing number of text-to-video (T2V) models, most of them lack the corresponding foley generation capabilities, thereby limiting their ability to produce fully immersive content. Our V2A module addresses this critical gap by autonomously generating cinematic-grade foley audio tailored to the input video and textual prompts, thus enabling the synthesis of a cohesive and holistically engaging multimedia experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Data</head><p>Unlike text-to-video (T2V) models, video-to-audio (V2A) models have different requirements for data. As mentioned above, we constructed a video dataset comprising of video-text pairs. However, not all data in this dataset are suitable for training the V2A model. For example, some videos lack an audio stream, others contain extensive voice-over content or their ambient audio tracks have been removed and replaced with unrelated elements. To address these challenges and ensure data quality, we designed a robust data filtering pipeline specifically tailored for V2A training.</p><p>First, we filter out videos without audio streams or those in which the silence ratio exceeds 80%. Next, we employ a frame-level audio detection model, like <ref type="bibr" target="#b37">[38]</ref>, to detect speech, music, and general sound in the audio stream. Based on this analysis, we classify the data into four distinct categories: pure sound, sound with speech, sound with music, and pure music. Subsequently, to prioritize high-quality data, we train a model inspired by CAVP <ref type="bibr" target="#b53">[54]</ref> to compute a visual-audio consistency score, which quantifies the alignment between the visual and auditory components of each video. Using this scoring system in conjunction with the audio category labels, we systematically sample portions of data from each category, retaining approximately 250,000 hours from the original dataset for pre-training. For the supervised fine-tuning stage, we further refine our selection, curating a subset of millions of high-quality clips (80,000 hours).</p><p>For feature extraction, we use CLIP <ref type="bibr" target="#b69">[70]</ref> to obtain visual features at a temporal resolution of 4 fps and subsequently resample these features to align with the audio frame rate. To generate captions, we employ <ref type="bibr" target="#b28">[29]</ref> as the sound captioning model and <ref type="bibr" target="#b19">[20]</ref> as the music captioning model. When both sound and music captions are available, we merge them into a structured caption format, following the approach detailed in <ref type="bibr" target="#b66">[67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Model</head><p>Just like the above-mentioned text-to-video model, our video-to-audio generation model also adopts a flow-matching-based diffusion transformer (DiT) as its architectural backbone. The detailed design of the model is depicted in Figure <ref type="figure" target="#fig_20">18</ref>, illustrating a transition from a triple-stream structure to a single-stream DiT framework.</p><p>The model operates within a latent space encoded by a variational autoencoder (VAE) trained on mel-spectrograms. Specifically, the audio waveform is first converted into a 2D mel-spectrogram representation. This spectrogram is subsequently encoded into a latent space using a pretrained VAE.</p><p>For feature extraction, we leverage pretrained CLIP <ref type="bibr" target="#b69">[70]</ref> and T5 <ref type="bibr" target="#b70">[71]</ref> encoders to independently extract visual and textual features, respectively. These features are subsequently projected into the DiT-compatible latent space using independent linear projections followed by SwiGLU activation, as depicted in Figure <ref type="figure" target="#fig_20">18</ref>.</p><p>To effectively integrate multimodal information, we incorporate stacked triple-stream transformer blocks, which independently process visual, audio, and textual modalities. These are later followed by single-stream transformer blocks to ensure seamless fusion and alignment across modalities. This design enhances the alignment between audio-video and audio-text representations, facilitating improved multimodal coherence.</p><p>Once the latent representation is generated by the diffusion transformer, the VAE decoder reconstructs the corresponding mel-spectrogram. Finally, the mel-spectrogram is converted back into an audio waveform using a pre-trained HifiGAN vocoder <ref type="bibr" target="#b43">[44]</ref>. This framework ensures a high-fidelity reconstruction of audio signals while maintaining strong multimodal alignment.  Image-to-video (I2V) task is a common application in video generation tasks. It usually means that given an image and a caption, the model uses this image as the first frame to generate a video that matches the caption. Although the naive HunyuanVideo is a text-to-video (T2V) model, it can be easily extended to an I2V model. Specifically, as mentioned in Sec. 4.2, the T2V model's input is a latent X with a shape of T × C × H × W , where T , C, H and W represent the frame, channel, height and width of the compressed video respectively. Similar to Emu <ref type="bibr" target="#b24">[25]</ref>, in order to introduce image condition I, we treat I as the first frame of a video and apply zero-padding to create a T × C × H × W tensor I o , as shown in Fig. <ref type="figure" target="#fig_22">19</ref>. Additionally, we employ a binary mask m with dimensions T × 1 × H × W , where the first temporal position is set to 1, and all other positions are set to zero. Then the latent X, the tensor I o and the mask m are concatenated along the channel dimension to form the input for the model. Note that since the channel of the input tensor has changed from C to 2C + 1, as shown in Fig. <ref type="figure" target="#fig_22">19</ref>, we need to adjust the parameters of the first convolutional module of the model from ϕ = (C in (= C), C out , s h , s w ) to ϕ ′ = (C ′ in (= 2C + 1), C out , s h , s w ), where each component corresponds to the input channel C in /C ′ in , output channel C out , height of the convolutional kernel s h , and width of the convolutional kernel s w . In order to retain the representation ability of the T2V model, the first C input channels of ϕ ′ are directly copied to ϕ, and the rest are initialized to zero. We pre-train the I2V model on the same data as T2V model, and the results are shown in Fig. <ref type="figure" target="#fig_23">20</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Downstream Task Fine-tuning: Portrait Image-to-Video Generation</head><p>We perform supervised finetuning of our I2V model on two million portrait videos to enhance human's motion and overall aesthetics. In addition to the standard data filtering pipeline described in section 3, we also apply face and body detectors to filter out the training videos which have more than five persons. We also remove the videos in which the main subjects are small. Finally, the rest videos will be manually inspected to obtain the final high-quality portrait training dataset.</p><p>Regarding training, we adopt a progressive fine-tuning strategy, gradually unfreezing the model parameters of the respective layers while keeping the rest frozen during finetuning. This approach allows the model to achieves high performance in the portrait domain without compromising much of its inherent generalization ability, guaranteeing commendable performance in natural landscapes, animals, and plants domains. Moreover, our model also supports video interpolation by using the first and last frames as conditions. We randomly drop the text conditions at certain probability during training to enhance the model's performance. Some demo results are shown in Fig. <ref type="figure" target="#fig_24">21</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Avatar animation</head><p>HunyuanVideo empowers controllable avatar animation in various aspects. It enables animating characters using explicit driving signals(e.g., speech signals, expression templates, and pose templates). In addition, it also integrates the implicit driving paradigm using text prompts. Fig. <ref type="figure" target="#fig_25">22</ref> shows how we leverage the power of HunyuanVideo to animate characters from multi-modal conditions. To maintain strict appearance consistency, we modify the HunyuanVideo architecture by inserting latent of reference image as strong guidance. As shown in Fig. <ref type="figure" target="#fig_25">22 (b,</ref> <ref type="figure">c</ref>), we encode reference image using 3DVAE obaining z ref ∈ R 1×c×h×w , where c = 16. Then we repeat it t times along temporal dimension and concatenate with z t in channel dimension to get the modified noise input ẑt ∈ R t×2c×h×w . To achieve controllable animation, various adapters are employed. We describe them in following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Upper-Body Talking Avatar Generation</head><p>In recent years, audio-driven digital human algorithms have made significant progress, especially in the performance of the talking head. Early algorithms, such as loopy <ref type="bibr" target="#b93">[94]</ref>, emo <ref type="bibr" target="#b79">[80]</ref>, and hallo <ref type="bibr" target="#b86">[87]</ref>, mainly focused on the head area, driving the digital human's facial expressions and lip shapes by analyzing audio signals. Even earlier algorithms, like wav2lip <ref type="bibr" target="#b67">[68]</ref> and DINet <ref type="bibr" target="#b96">[97]</ref>, concentrated on modifying the mouth region in the input video to achieve lip shape consistency with the audio. However, these algorithms are usually limited to the head area, neglecting other parts of the body. To achieve a more natural and vivid digital human performance, we propose an audio-driven algorithm extended to the upper body. In this algorithm, the digital human not only synchronizes facial expressions and lip shapes with the audio while speaking but also moves the body rhythmically with the audio.</p><p>Audio-Driven Based on the input audio signal, our model can adaptively predict the digital human's facial expressions and posture action information . This allows the driven character to speak with emotion and expression, enhancing the digital human's expressiveness and realism. As shown in Fig. <ref type="figure" target="#fig_25">22 (b)</ref>, for the single audio signal-driven part, the audio passes through the whisper feature extraction module to obtain audio features, which are then injected into the main network in a cross-attention manner. It should be noted that the injection process will be multiplied by the face-mask to control the audio's effect area. While enhancing the head and shoulder control ability, it will also greatly reduce the probability of body deformation. To obtain more lively head movements, head pose motion parameters and expression motion parameters are introduced and added to the time step in an embedding manner. During training, the head motion parameters are given by the variance of the nose tip keypoint sequence, and the expression parameters are given by the variance of the facial keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Fully-Controlled Whole-Body Avatar Generation</head><p>Controlling digital character's motion and expression explicitly has been a long-standing problem in both academia and industry, and recent advancement of diffusion models paved the first step to realistic avatar animation. However, current avatar animation solutions suffer from partial controllability due to limited capability of foundation video generation model. We demonstrate that a stronger T2V model boosts the avatar video generation to fully-controllable stage. We show how HunyuanVideo serves as strong foundation with limited modifications to extent general T2V model to fully-controllable avatar generation model in Fig. <ref type="figure" target="#fig_25">22 (c)</ref>.</p><p>Pose-Driven We can control the digital character's body movements explicitly using pose templates. We use Dwpose <ref type="bibr" target="#b91">[92]</ref> to detect skeletal video from any source video, and use 3DVAE to transform it to latent space as z pose . We argue that this eases the fine-tuning process because both input and driving videos are in image representation, and are encoded with shared VAE, resulting same latent space. We then inject the driving signals to the model by element-wise add as ẑt + z pose . Note that ẑt contains the appearance information of reference image. We use full-parameters finetune with pretrained T2V weights as initialization. Expression-Driven We can also control the facial expressions of digital character using implicit expression representations. Although facial landmarks are widely adopted in this area <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b15">16]</ref>, we argue using landmarks brings ID leak due to cross-ID misalignment. Instead, we use implicit representations as driving signals for their ID and expression disentanglement capabilities. In this work, we use VASA <ref type="bibr" target="#b87">[88]</ref> as expression extractor. As shown in Fig. <ref type="figure" target="#fig_25">22</ref> (c), we adopt a light-weight expression encoder to transform the expression representation to token sequence in latent space as z exp ∈ R t×n×c , where n is the number of tokens per frame. Typically, we set n = 16. Unlike pose condition, we inject z exp using cross-attention because ẑt and z exp are not naturally aligned in spatial aspect. We add cross-attention layer Attn exp (q, k, v) every K double and single-stream DiT layers to inject expression latent. Denote the hidden states after i-th DiT layer as h i , the injection of expression z exp to h i could be derived as: h i + Attn exp (h i , z exp , z exp ) * M face , where M face is the face region mask that guides where z exp should be applied at, and * stands for element-wise multiplication. Also, full-parameters tuning strategy is adopted.</p><p>Hybrid Condition Driven Combining both pose and expression driven strategies derives hybrid control approach. In this scenario, the body motion is controlled by explicit skeletal pose sequence, and the facial expression is determined by implicit expression representation. We jointly fine-tune T2V modules and added modules in an end-to-end fasion. During inference, the body motion and facial motion could be controlled by separate driving signals, empowering richer editability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Application Demos</head><p>We present extensive results of avatar animations to show the superiority and potential of bringing avatar animation empowered by HunyuanVideo to next generation.</p><p>Audio-Driven Fig. <ref type="figure" target="#fig_26">23</ref> shows that HunyuanVideo serves as a strong foundation model for audiodriven avatar animation, which can synthesize vivid and high-fidelity videos. We summarize the superiority of our method in three folds: • Upper-body Animation. Our method can drive not only portrait characters, but also upper-body avatar images, enlarging its range of application scenarios. • Dynamic Scene Modelling. Our method can generate videos with vivid and realistic background motion, such as the wave undulation, crowd movement, and breeze stirring leaves. • Vivid Avatar Movements. Our method is able to animate the character talking while gesturing vividly with audio solely.</p><p>Pose-Driven We also show that HunyuanVideo boosts the performance of pose-driven animation largely in many aspects in Fig. <ref type="figure" target="#fig_3">24:</ref> • High ID-Consistency. Our method maintains the ID-consistency well over the frames even with large poses, making it face-swapping free, thereby, could be used as real end-to-end animation solution. • Following Complex Poses Accurately. Our method is able to handle very complex poses such as turning around and hands crossed. • High Motion Quality. Our method has remarkable capability in dynamic modelling. For instance, the results show promising performance in terms of garment dynamics and texture consistency.</p><p>• Generalizability. Our method presents surprisingly high generalizability. It can animate wide variety of avatar images, such as real human, anime, pottery figurine, and even animals.</p><p>Expression-Driven Fig. <ref type="figure" target="#fig_4">25</ref> presents how HunyuanVideo enhances the portrait expression animating in three folds:</p><p>• Exaggerated Expression. Our method is able to animate given portrait to mimic any facial movements even with large poses and exaggerated expressions.</p><p>• Mimicing Eye Gaze Accurately. We can control the portraits' eye movements acurately given any expression template, even with extreme and large eye balls movements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Computation resources used for closed-source and open-source video generation models. Right: Performance comparison between HunyuanVideo and other selected strong baselines.</figDesc><graphic coords="2,309.96,228.47,178.20,106.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall training system for HunyuanVideo.</figDesc><graphic coords="3,108.00,125.07,395.96,122.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our hierarchical data filtering pipeline. We employ various filters for data filtering and progressively increase their thresholds to build 4 training datasets, i.e., 256p, 360p, 540p, and 720p, while the final SFT dataset is built through manual annotation. This figure highlights some of the most important filters to use at each stage. A large portion of data will be removed at each stage, ranging from half to one-fifth of the data from the previous stage. Here, gray bars represent the amount of data filtered out by each filter while colored bars indicate the amount of remaining data at each stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The overall architecture of HunyuanVideo. The model is trained on a spatial-temporally compressed latent space, which is compressed through Causal 3D VAE. Text prompts are encoded using a large language model, and used as the condition. Gaussian noise and condition are taken as input, our model generates a output latent, which is decoded into images or videos through the 3D VAE decoder.</figDesc><graphic coords="5,117.90,72.00,376.20,163.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: VAE reconstruction case comparison.</figDesc><graphic coords="7,108.35,72.31,395.55,197.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The architecture of our HunyuanVideo Diffusion Backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Text encoder comparison between T5 XXL and the instruction-guided MLLM introduced by HunyuanVideo.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Scaling laws of DiT-T2X model family. On the top-left (a) we show the loss curves of the T2X(I) model on a log-log scale for a range of model sizes from 92M to 6.6B. We follow<ref type="bibr" target="#b35">[36]</ref> to plot the envelope in gray points, which are used to estimate the power-law coefficients of the amount of computation (C) vs model parameters (N ) (b) and the computation vs tokens (D) (c). Based on the scaling law of the T2X(I) model, we plot the scaling law of the corresponding T2X(V) model in (d), (e), and (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>= 5.48 × 10 -4 , b 1 = 0.5634, a 2 = 0.324 and b 2 = 0.4325, where the units of a 1 , a 2 , N opt , D opt are billions while C has a unit of Peta FLOPs. The Fig.10 (b) and Fig.10 (c) show that DiT-T2X(I) family fits the power law quite well. Finally, given computation budgets, we can calculate the optimal model size and dataset size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>4. 4 . 2</head><label>42</label><figDesc>Video model scaling lawBased on the scaling law of the T2X(I) model, we select the optimal image checkpoint (i.e., the model on the envelope) corresponding to each size model to serve as the initialization model for the video scaling law experiment. Fig. 10 (d), Fig. 10 (e), and Fig. 10 (f) illustrate the scaling law results of the T2X(V) model, where a 1 = 0.0189, b 1 = 0.3618, a 2 = 0.0108 and b 2 = 0.6289. Based on the results of Fig. 10 (b) and Fig. 10 (e), and taking into account the training consumption and inference cost, we finally set the model size to 13B. Then the number of tokens for image and video training can be calculated as shown in Fig. 10 (c) and Fig. 10 (f). It is worth noting that the amount of training tokens calculated by image and video scaling laws is only related to the first stage of training for images and videos respectively. The scaling property of progressive training from low-resolution to high-resolution will be left explored in future work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: (a) Different time-step schedulers. For our shifting stragty, we set a larger shifting factor s for a lower inference step. (b) Generated videos with only 10 inference steps. The shifting stragty leads to significantly better visual quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: High-quality videos generated by HunyuanVideo.</figDesc><graphic coords="15,108.00,384.32,396.00,236.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>(a) Prompt: At sunset, a modified Ford F-150 Raptor roared past on the off-road track. The raised suspension allowed the huge explosion-proof tires to flip freely on the mud, and the mud splashed on the roll cage. (b) Prompt: The panning camera moves forward slowly, with a depth of field in the middle focus, and warm sunset light covers the screen. The girl in the picture runs with her skirt fluttering, turns and jumps. (c) Prompt: In the gym, a woman in workout clothes runs on a treadmill. Side angle. Realistic, Indoor lighting, Professional. (d) Prompt: Swimmer swimming underwater, in slow motion. Realistic, Underwater lighting, Peaceful. (e) Prompt: On the rooftop, there is an open-air basketball court, and five male students are playing basketball. Realistic, Natural lighting, Casual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: High-motion dynamics videos generated by HunyuanVideo.</figDesc><graphic coords="16,108.00,614.39,395.95,111.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: HunyuanVideo's performance on concept generalization. The results of the three rows correspond</figDesc><graphic coords="17,108.00,118.38,396.00,178.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Prompt: The woman walks over and opens the red wooden door. As the door swings open, seawater bursts forth, in a realistic style.</figDesc><graphic coords="17,108.00,437.42,396.00,116.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><figDesc>17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: High text-video alignment videos generated by HunyuanVideo. Top row: Prompt: A close-up of a wave crashing against the beach, the sea foam spells out "WAKE UP" on the sand. Bottom row: Prompt: In a garden filled with blooming flowers, "GROW LOVE" has been spelled out with colorful petals.</figDesc><graphic coords="18,108.00,72.00,396.00,115.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: The architecture of sound effect and music generation model.</figDesc><graphic coords="19,127.80,71.99,356.39,194.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><figDesc>𝜙 = 𝐶!" = 𝐶 , 𝐶#$%, 𝑠&amp;, 𝑠' 𝜙 ( = 𝐶 !" ( = 𝐶 + 𝐶 + 1 , 𝐶#$%, 𝑠&amp;, 𝑠'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Differences between text-to-video (T2V) model and image-to-video (I2V) model.</figDesc><graphic coords="20,128.17,72.49,342.95,164.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Sample results of the I2V pre-training model.</figDesc><graphic coords="21,108.00,72.00,396.00,197.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Sample results of our portrait I2V model.</figDesc><graphic coords="21,108.00,300.25,396.00,197.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Overview of Avatar Animation built on top of HunyuanVideo. We adopt 3D VAE to encode and inject reference and pose condition, and use additional cross-attention layers to inject audio and expression signals. Masks are employed to explicitly guide where they are affecting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Audio-Driven. HunyuanVideo can generate vivid talking avatar videos.</figDesc><graphic coords="23,148.52,250.18,355.49,108.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 24 :Figure 25 :</head><label>2425</label><figDesc>Figure 24: Pose-Driven. HunyuanVideo can animate wide variety of characters with high quality and appearance consistency under various poses.</figDesc><graphic coords="24,159.10,520.03,344.91,104.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: Hybrid Condition-Driven. HunyuanVideo supports full control with multiple driving sources across various avatar characters.</figDesc><graphic coords="26,145.73,402.70,358.28,117.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>VAE reconstruction metrics comparison.</figDesc><table><row><cell>Model</cell><cell cols="2">Downsample |z| Factor</cell><cell cols="2">ImageNet (256×256) MCL-JCV (33×360×640) PSNR↑ PSNR↑</cell></row><row><cell>FLUX-VAE [47]</cell><cell>1 × 8 × 8</cell><cell>16</cell><cell>32.70</cell><cell>-</cell></row><row><cell>OpenSora-1.2 [102]</cell><cell>4 × 8 × 8</cell><cell>4</cell><cell>28.11</cell><cell>30.15</cell></row><row><cell>CogvideoX-1.5 [93]</cell><cell>4 × 8 × 8</cell><cell>16</cell><cell>31.73</cell><cell>33.22</cell></row><row><cell>Cosmos-VAE [64]</cell><cell>4 × 8 × 8</cell><cell>16</cell><cell>30.07</cell><cell>32.76</cell></row><row><cell>Ours</cell><cell>4 × 8 × 8</cell><cell>16</cell><cell>33.14</cell><cell>35.39</cell></row></table><note><p>FLUX OpenSora-1.2 CogvideoX-1.5 Cosmos-VAE Ours Original</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Architecture hyperparameters for the HunyuanVideo 13B parameter foundation model.</figDesc><table><row><cell>Dual-stream Blocks</cell><cell>Single-stream Blocks</cell><cell>Model Dimension</cell><cell>FFN Dimension</cell><cell>Attention Heads</cell><cell cols="2">Head dim (dt, d h , dw)</cell></row><row><cell>20</cell><cell>40</cell><cell>3072</cell><cell>12288</cell><cell>24</cell><cell>128</cell><cell>(16, 56, 56)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Model Performance Evaluation</figDesc><table><row><cell>Model Name</cell><cell cols="6">Duration Text Alignment Motion Quality Visual Quality Overall Ranking</cell></row><row><cell cols="2">HunyuanVideo (Ours) 5s</cell><cell>61.8%</cell><cell>66.5%</cell><cell>95.7%</cell><cell>41.3%</cell><cell>1</cell></row><row><cell>CNTopA (API)</cell><cell>5s</cell><cell>62.6%</cell><cell>61.7%</cell><cell>95.6%</cell><cell>37.7%</cell><cell>2</cell></row><row><cell>CNTopB (Web)</cell><cell>5s</cell><cell>60.1%</cell><cell>62.9%</cell><cell>97.7%</cell><cell>37.5%</cell><cell>3</cell></row><row><cell>GEN-3 alpha (Web)</cell><cell>6s</cell><cell>47.7%</cell><cell>54.7%</cell><cell>97.5%</cell><cell>27.4%</cell><cell>4</cell></row><row><cell>Luma1.6 (API)</cell><cell>5s</cell><cell>57.6%</cell><cell>44.2%</cell><cell>94.1%</cell><cell>24.8%</cell><cell>5</cell></row><row><cell>CNTopC (Web)</cell><cell>5s</cell><cell>48.4%</cell><cell>47.2%</cell><cell>96.3%</cell><cell>24.6%</cell><cell>6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/Tencent/HunyuanVideo</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• Generalizability. Our method has high generalizability. It can animate not only real human portraits, but also anime or CGI characters.</p><p>Hybrid-Driven Lastly, we show that hybrid condition control reveals the potential of fully controllable and editable avatars in Fig. <ref type="figure">26</ref>. We highlight the superiority as follow:</p><p>• Hybrid Condition Control. For the first time, our method is able to conduct full control over body and facial motions with siloed or multiple signals, paving the route from demo to applications for avatar animation. • Half-body Animation. Our method supports upper-body full control, enabling rich editability while maintaining high quality and fidelity. • Generalizability. Our method generalize to both real human images and CGI characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Works</head><p>Due to the success of diffusion models in the field of image generation <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b33">34]</ref>, the exploration in the domain of video generation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b56">57]</ref> is also becoming popular. VDM <ref type="bibr" target="#b31">[32]</ref> is among the first that extends the 2D U-Net from image diffusion models to a 3D U-Net to achieve text-based generation. Later works, such as MagicVideo <ref type="bibr" target="#b102">[103]</ref> and Mindscope <ref type="bibr" target="#b81">[82]</ref>, introduce 1D temporal attention mechanisms, reducing computations by building upon latent diffusion models. In this report, we do not use the 2D + 1D temporal block manner for motion learning. Instead, we use similar dual flow attention blocks as in FLUX <ref type="bibr" target="#b46">[47]</ref>, which are used for processing all video frames. Following Imagen, Imagen Video <ref type="bibr" target="#b32">[33]</ref> employs a cascaded sampling pipeline that generates videos through multiple stages. In addition to traditional end-to-end text-to-video (T2V) generation, video generation using other conditions is also an important direction. This type of methods generates videos with other auxiliary controls, such as depth maps <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>, pose maps <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b55">56]</ref>, RGB images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b60">61]</ref>, or other guided motion videos <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b85">86]</ref>. Despite the excellent generation performance of the recent open-source models such as Stable video diffusion <ref type="bibr" target="#b4">[5]</ref>, Open-sora <ref type="bibr" target="#b101">[102]</ref>, Open-sora-plan <ref type="bibr" target="#b45">[46]</ref>, Mochi-1 <ref type="bibr" target="#b78">[79]</ref> and Allegro <ref type="bibr" target="#b103">[104]</ref>, their performance still falls far behind the closed-source state-of-the-art video generation models such as Sora <ref type="bibr" target="#b6">[7]</ref> and MovieGen <ref type="bibr" target="#b66">[67]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Project Contributors</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<title level="m">Palm 2 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All are worth words: A vit backbone for diffusion models</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22669" to="22679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving image generation with better captions</title>
		<author>
			<persName><forename type="first">James</forename><surname>Betker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://cdn.openai.com/papers/dall-e-3.pdf" />
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Homes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Wing Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Ramesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><surname>Tom B Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12793</idno>
		<title level="m">Sharegpt4v: Improving large multi-modal models with better captions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sharegpt4video: Improving video understanding and generation with better captions</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04325</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Od-vae: An omni-dimensional video compressor for improving latent video diffusion model</title>
		<author>
			<persName><forename type="first">Liuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.01199</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Follow-your-canvas: Higher-resolution video outpainting with extensive content generation</title>
		<author>
			<persName><forename type="first">Qihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.01055</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Panda-70m: Captioning 70m videos with multiple cross-modality teachers</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Tsai-Shien Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Wei</forename><surname>Deyneka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Eun Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">June 2024. 4</date>
			<biblScope unit="page" from="13320" to="13331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Seine: Short-to-long video diffusion model for generative transition and prediction</title>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08136</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Xtuner</forename><surname>Contributors</surname></persName>
		</author>
		<author>
			<persName><surname>Xtuner</surname></persName>
		</author>
		<ptr target="https://github.com/InternLM/xtuner,2023.8" />
		<title level="m">A toolkit for efficiently fine-tuning llm</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Opencv</forename><surname>Developers</surname></persName>
		</author>
		<author>
			<persName><surname>Opencv</surname></persName>
		</author>
		<ptr target="https://opencv.org/.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Pyscenedetect</forename><surname>Developers</surname></persName>
		</author>
		<author>
			<persName><surname>Pyscenedetect</surname></persName>
		</author>
		<ptr target="https://www.scenedetect.com/.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Seungheon</forename><surname>Doh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keunwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongpil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.16372</idno>
		<title level="m">Lp-musiccaps: Llm-based pseudo music captioning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaling rectified flow transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahim</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Boesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2024. 2, 8, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longtian</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.05945</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Yolox: Exceeding yolo series in 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Emu video: Factorizing text-to-video generation by explicit image conditioning</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akbar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10709</idno>
		<imprint>
			<date type="published" when="1920">2023. 2, 20</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Glm</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiadai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiale</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingdao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shudan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Lam Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xixuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuantao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Chatglm: A family of large language models from glm-130b to glm-4 all tools</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<title level="m">Sparsectrl: Adding sparse controls to text-to-video diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Taming data and transformers for audio generation</title>
		<author>
			<persName><forename type="first">Moayed</forename><surname>Haji-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.19388</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ring attention with blockwise transformers for nearinfinite context</title>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01889</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Animate-a-story: Storytelling with retrieval-augmented video generation</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Video diffusion models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><surname>Fleet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Imagen video: High definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><surname>Fleet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Animate anyone: Consistent and controllable image-to-video synthesis for character animation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A large tv dataset for speech and music activity detection</title>
		<author>
			<persName><forename type="first">Yun-Ning</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iroro</forename><surname>Orife</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Hipple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wolcott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">General data protection regulation (gdpr</title>
		<author>
			<persName><surname>Investopedia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">October 10, 2023. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Text2performer: Text-driven human video generation</title>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Liang Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Computational tradeoffs in image synthesis</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Japan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.13218</idno>
	</analytic>
	<monogr>
		<title level="m">Diffusion, masked-token, and next-token prediction</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Re-ex: Revising after explanation reduces the factual errors in llm responses</title>
		<author>
			<persName><forename type="first">Juyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongeun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyeol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junseong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jy</forename><surname>Yong Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaekyoung</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reducing activation recomputation in large transformer models</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkug</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Lym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Andersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="341" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Open-sora-plan</title>
		<author>
			<persName><forename type="first">Pku-Yuan</forename><surname>Lab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Tuzhan</surname></persName>
		</author>
		<author>
			<persName><surname>Etc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-04">April 2024</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Black Forest Labs. Flux</title>
		<imprint>
			<date type="published" when="2024">2024. 2, 6, 8, 27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Tccl: Co-optimizing collective communication and traffic routing for gpu-centric clusters</title>
		<author>
			<persName><forename type="first">Baojia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingzhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhuo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 SIGCOMM Workshop on Networks for AI Computing</title>
		<meeting>the 2024 SIGCOMM Workshop on Networks for AI Computing</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the scalability of diffusionbased text-to-image generation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orchid</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9400" to="9409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR, 2022. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Hunyuan-dit: A powerful multi-resolution diffusion transformer</title>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangfeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxin</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingfang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zedong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongwei</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingtao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglin</forename><surname>Lu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>with fine-grained chinese understanding, 2024. 2, 8</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02747</idno>
		<title level="m">Maximilian Nickel, and Matt Le. Flow matching for generative modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Diff-foley: Synchronized videoto-audio synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Simian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2019">2024. 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Exploring the role of large language models in prompt encoding for diffusion models</title>
		<author>
			<persName><forename type="first">Bingqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuofan</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11831</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Follow your pose: Pose-guided text-to-video generation using pose-free videos</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Follow-your-click: Open-domain regional image animation via short prompts</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08268</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01900</idno>
		<title level="m">Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>5-th Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On distillation of guided diffusion models</title>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14297" to="14306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Conditional image-to-video generation with latent flow diffusion models</title>
		<author>
			<persName><forename type="first">Haomiao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Renqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangcheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.02868</idno>
		<title level="m">Yangyu Tao, and Bin Cui. Angel-ptm: A scalable and economical large-scale pre-training system in tencent</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Context parallelism overview</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Cosmos-tokenizer</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Movie gen: A cast of media foundation models</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13720</idno>
		<imprint>
			<date type="published" when="2024">2024. 2, 5, 6, 7, 13</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on multimedia</title>
		<meeting>the 28th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 19</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00512</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Transnet v2: An effective deep network architecture for fast shot transition detection</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Souček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Lokoč</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04838</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xingwu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaipeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonny</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengzong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saiyong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lulu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Decheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yigeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengcheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiong</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianglu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberts</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ze</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fusheng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuebin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxiang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xirui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinben</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winston</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanhui</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Mochi 1: A new sota in open-source video generation</title>
		<author>
			<persName><forename type="first">Genmo</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/genmoai/models" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Emo: Emote portrait alive -generating expressive portrait videos with audio2video diffusion model under weak conditions</title>
		<author>
			<persName><forename type="first">Linrui</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Modelscope text-to-video technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Disentangled control for referring human dance generation in real world</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Lavie: High-quality video generation with cascaded latent diffusion models</title>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqing</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Exploring video quality assessment on user generated contents from aesthetic and technical perspectives</title>
		<author>
			<persName><forename type="first">Haoning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20144" to="20154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Lamp: Learn a motion pattern for few-shot-based video generation</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Hallo: Hierarchical audio-driven visual synthesis for portrait image animation</title>
		<author>
			<persName><forename type="first">Mingwang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Vasa-1: Lifelike audio-driven talking faces generated in real time</title>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10667</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">Zhongcong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><surname>Magicanimate</surname></persName>
		</author>
		<title level="m">Temporally consistent human image animation using diffusion model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Follow-your-pose v2: Multiple-condition guided character image animation for stable pose control</title>
		<author>
			<persName><forename type="first">Jingyun</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.03035</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Probabilistic adaptation of text-to-video models</title>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Effective whole-body pose estimation with two-stages distillation</title>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2007">2024. 4, 5, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Mimictalk: Mimicking a personalized and expressive 3d talking face in minutes</title>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinzheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Language model beats diffusion-tokenizer is key to visual generation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Versari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vighnesh</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05737</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Show-1: Marrying pixel and latent diffusion models for text-to-video generation</title>
		<author>
			<persName><forename type="first">David Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">Zhangjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingmin</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Dinet: Deformation inpainting network for realistic face visually dubbing on high resolution video</title>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tangjie</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning from pretrained diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22117" to="22130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Shiftddpms: exploring conditional diffusion models by shifting diffusion trajectories</title>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3552" to="3560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Motiondirector: Motion customization of text-to-video diffusion models</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">Zhangjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jussi</forename><surname>Keppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Real-time video generation with pyramid attention broadcast</title>
		<author>
			<persName><forename type="first">Xuanlei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.12588</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Open-sora: Democratizing efficient video production for all</title>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-03">March 2024</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Magicvideo: Efficient video generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Allegro: Open the black box of commercial-level video generation model</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.15458</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
