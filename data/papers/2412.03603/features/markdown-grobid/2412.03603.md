# HunyuanVideo: A Systematic Framework For Large Video Generative Models "Bridging the gap between closed-source and open-source video foundation models to accelerate community exploration." -Hunyuan Foundation Model Team

## Abstract

## 

Recent advancements in video generation have profoundly transformed daily life for individuals and industries alike. However, the leading video generation models remain closed-source, creating a substantial performance disparity in video generation capabilities between the industry and the public community. In this report, we present HunyuanVideo, a novel open-source video foundation model that exhibits performance in video generation that is comparable to, if not superior to, leading closed-source models. HunyuanVideo features a comprehensive framework that integrates several key contributions, including data curation, advanced architecture design, progressive model scaling and training, and an efficient infrastructure designed to facilitate large-scale model training and inference. With those, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to professional human evaluation results, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and 3 top performing Chinese video generative models. By releasing the code of the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower everyone in the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at [https://github.com/Tencent/HunyuanVideo](https://github.com/Tencent/HunyuanVideo). Figure 1: Non-curated multi-ratio generation samples with HunyuanVideo, showing realistic, concept generalization and automatic scene-cut features. Hunyuan Foundation Model Team Contributors are listed at the end of the report .

## Introduction

With extensive pre-training and advanced architectures, diffusion models [[51,](#b50)[65,](#b64)[21,](#b20)[72,](#b71)[5,](#b4)[25,](#b24)[67,](#b66)[47]](#b46) have demonstrated superior performance in generating high-quality images and videos compared to previous generative adversarial network (GAN) methods [[6]](#b5). However, unlike the image generation field, which has seen a proliferation of novel algorithms and applications across various open platforms, diffusion-based video generative models remain relatively inactive. We contend that one of the primary reasons for this stagnation is the lack of robust open-source foundation models as in T2I filed [[47]](#b46). In contrast to the image generative model community, a significant gap has emerged between open-source and closed-source video generation models. Closed-source models tend to overshadow publicly available open-source alternatives, severely limiting the potential for algorithmic innovation from the public community. While the recent state-of-the-art model MovieGen [[67]](#b66) has demonstrated promising performance, its milestone for open-source release has yet to be established.   [[65]](#b64) trained with Flow Matching [[52]](#b51) was not sufficiently efficient. Consequently, we explored an effective scaling strategy that can reduce computational resource requirements by up to 5× while achieving the desired model performance. With this optimal scaling approach and dedicated infrastructure, we successfully trained a large video model comprising 13 billion parameters, pre-training it on internetscale images and videos. After a dedicated progressive fine-tuning strategy, HunyuanVideo excels in four critical aspects of video generation: visual quality, motion dynamics, video-text alignment, and semantic scene cut. We conducted a comprehensive comparison of HunyuanVideo with leading global video generation models, including Gen-3 and Luma 1.6 and 3 top performing commercial models in China, using over 1,500 representative text prompts accessed by a group of 60 people.

The results indicate that HunyuanVideo achieves the highest overall satisfaction rates, particularly excelling in motion dynamics.

## Overview

HunyuanVideo is a comprehensive video training system encompassing all aspects from data processing to model deployment. This technical report is structured as follows:

• In Section 3, we introduce our data preprocessing techniques, including filtering and recaptioning models. • Section 4 presents detailed information about the architecture of all components of Hun-yuanVideo, along with our training and inference strategies. • In Section 5, we discuss methods for accelerating model training and inference, enabling the development of a large model with 13 billion parameters. • Section 6 evaluates the performance of our text-to-video foundation models and compares them with state-of-the-art video generation models, both open-source and proprietary.

• Finally, in Section 7, we showcase various applications built on the pre-trained foundation model, accompanied by relevant visualizations as well as some video related functional models such as video to audio generative model. 3 Data Pre-processing

We use an image-video joint training strategy. The videos are meticulously divided into five distinct groups, while images are categorized into two groups, each tailored to fit the specific requirements of their respective training processes. This section will primarily delve into the intricacies of video data curation.

Our data acquisition process is rigorously governed by the principles outlined in the General Data Protection Regulation (GDPR) [[39]](#b38) framework. Furthermore, we employ advanced techniques such as data synthesis and privacy computing to guarantee compliance with these stringent standards.

Our raw data pool initially comprised videos spanning a wide range of domains including people, animals, plants, landscapes, vehicles, objects, buildings, and animation. Each video was acquired with a set of basic thresholds, including minimum duration requirements. Additionally, a subset of the data was collected based on more stringent criteria, such as spatial quality, adherence to a specific aspect ratio, and professional standards in composition, color, and exposure. These rigorous standards ensure that our videos possess technical quality and aesthetic appeal. We experimentally verified that incorporating high-quality data is instrumental in significantly enhancing model performance.

## Data Filtering

Our raw data from different sources exhibits varying durations and levels of quality. To address this, we employ a series of techniques to pre-process the raw data. Firstly, we utilize PySceneDetect [[19]](#b18) to split raw videos into single-shot video clips. Next, we employ the Laplacian operator from OpenCV [[18]](#b17) to identify a clear frame, serving as the starting frame of each video clip. Using an internal VideoCLIP model, we calculate embeddings for these video clips. These embeddings serve two purposes: (i) we deduplicate similar clips based on the Cosine distance of their embeddings; (ii) we apply k-means [[59]](#b58) to obtain ∼10K concept centroids for concept resampling and balancing.

To continuously enhance video aesthetics, motion, and concept range, we implement a hierarchical data filtering pipeline for constructing training datasets, as shown in Figure [4](#fig_3). This pipeline incorporates various filters to help us filter data from different perspectives which we introduce next.

We employ Dover [[85]](#b84) to assess the visual aesthetics of video clips from both aesthetic and technical viewpoints. Additionally, we train a model to determine clarity and eliminate video clips with visual blurs. By predicting the motion speed of videos using estimated optical flow [[18]](#b17), we filter out static or slow-motion videos. We combine the results from PySceneDetect [[19]](#b18) and Transnet v2 [[76]](#b75) to get scene boundary information. We utilize an internal OCR model to remove video clips with excessive text, as well as to locate and crop subtitles. We also develop YOLOX [[24]](#b23)-like visual models to detect and remove some occluded or sensitive information such as watermarks, borders, and logos. To assess the effectiveness of these filters, we perform simple experiments using a smaller HunyuanVideo

Pool Pool Pool Pool Pool 256p 256p 256p 256p 256p DeDuplication DeDuplication DeDuplication DeDuplication DeDuplication Motion Filter Motion Filter Motion Filter Motion Filter Motion Filter Other Filters Other Filters Other Filters Other Filters Other Filters 360p 360p 360p 360p 360p OCR Filter OCR Filter OCR Filter OCR Filter OCR Filter Clarity Filter Clarity Filter Clarity Filter Clarity Filter Clarity Filter Other Filters Other Filters Other Filters Other Filters Other Filters 540p 540p 540p 540p 540p Aesthetic Filter Aesthetic Filter Aesthetic Filter Aesthetic Filter Aesthetic Filter Other Filters Other Filters Other Filters Other Filters Other Filters 720p 720p 720p 720p 720p Source Filter Source Filter Source Filter Source Filter Source Filter SFT SFT SFT SFT SFT Manually Filter Manually Filter Manually Filter Manually Filter Manually Filter model and observe the performance changes. The results obtained from these experiments play an important role in guiding the building of our data filtering pipeline, which is introduced next.

Our hierarchical data filtering pipeline for video data yields five training datasets, corresponding to the five training stages (Section 4.5). These datasets (except for the last fine-tuning dataset) are curated by progressively improving the thresholds of the aforementioned filters. The video spatial resolution increases progressively from 256 × 256 × 65 to 720×1280 × 129. We apply varying levels of strictness to the filters during the threshold adjustment process at different stages (see Figure [4](#fig_3)). The last dataset used for fine-tuning is described next.

To improve the model's performance in the final stage (Section 4.7), we build a fine-tuning dataset comprising ∼1M samples. This dataset is meticulously curated through human annotation. Annotators are assigned the task of identifying video clips that exhibit high visual aesthetics and compelling content motion. Each video clip undergoes evaluation based on two perspectives: (i) decomposed aesthetical views, including color harmony, lighting, object emphasis, and spatial layout; (ii) decomposed motion views, encompassing motion speed, action integrity, and motion blurs. Finally, our fine-tuning dataset consists of visually appealing video clips with intricate motion details.

We also establish a hierarchical data filtering pipeline for images by reusing most of the filters, excluding the motion-related ones. Similarly, we build two image training datasets by progressively increasing the filtering thresholds applied to an image pool of billions of image-text pairs. The first dataset contains billions of samples and is used for the initial stage of text-to-image pre-training.

The second dataset contains hundreds of millions of samples and is utilized for the second stage of text-to-image pre-training.

## Data Annotation

Structured Captioning. As evidenced in research [[7,](#b6)[4]](#b3), the precision and comprehensiveness of captions play a crucial role in improving the prompt following ability and output quality of generative models. Most previous work focus on providing either brief captions [[14,](#b13)[50]](#b49) or dense captions [[93,](#b92)[9,](#b8)[10]](#b9). However, these approaches are not without shortcomings, suffering from incomplete information, redundant discourse and inaccuracies. In pursuit of achieving captions with higher comprehensiveness, information density and accuracy, we develop and implement an in-house Vision Language Model(VLM) designed to generate structured captions for images and videos. These structured captions, formatted in JSON, provide multi-dimensional descriptive information from various perspectives, including: 1) Short Description: Capturing the main content of the scene. 2) Dense Description: Detailing the scene's content, which notably includes scene transitions and camera movements that are integrated with the visual content, such as camera follows some subject.

3) Background: Describing the environment in which the subject is situated. 4) Style: Characterizing the style of the video, such as documentary, cinematic, realistic, or sci-fi. 5) Shot Type: Identifying the type of video shot that highlights or emphasizes specific visual content, such as aerial shot, close-up shot, medium shot, or long shot. 6) Lighting: Describing the lighting conditions of the video. 7) Atmosphere: Conveying the atmosphere of the video, such as cozy, tense, or mysterious.

Moreover, we extend the JSON structure to incorporate additional metadata-derived elements, including source tags, quality tags, and other pertinent tags from meta information of images and videos.

Through the implementation of a carefully designed dropout mechanism coupled with permutation and combination strategies, we synthesize captions diverse in length and pattern by assembling these multi-dimensional descriptions for each image and video, aiming to improve the generalization ability of generative models and prevent overfitting. We utilize this captioner to provide structured captions for all images and videos in our training dataset.

Camera Movement Types. We also train a camera movement classifier capable of predicting 14 distinct camera movement types, including zoom in, zoom out, pan up, pan down, pan left, pan right, tilt up, tilt down, tilt left, tilt right, around left, around right, static shot and handheld shot. High-confidence predictions of camera movement types are integrated into the JSON-formatted structured captions, to enable camera movement control ability of generative models.

## Model Architecture Design

The overview of our HunyuanVideo model is shown in Fig. [5](#fig_4). This section describes the Causal 3D VAE, diffusion backbone, and scaling laws experiments.

## 3D Variational Auto-encoder Design

Similar to previous work [[67,](#b66)[93]](#b92), we train a 3DVAE to compress pixel-space videos and images into a compact latent space. To handle both videos and images, we adopt CausalConv3D [[95]](#b94). For a video of shape (T + 1) × 3 × H × W , our 3DVAE compresses it into latent features with shape

$( T ct + 1) × C × ( H cs ) × ( W cs ).$In our implementation, c t = 4, c s = 8, and C = 16. This compression significantly reduces the number of tokens for the subsequent diffusion transformer model, allowing us to train videos at the original resolution and frame rate. The model structure is illustrated in Figure [6](#). Figure [6](#): The architecture of our 3DVAE.

## Training

In contrast to most previous work [[67,](#b66)[11,](#b10)[104]](#b103), we do not rely on a pre-trained image VAE for parameter initialization; instead, we train our model from scratch. To balance the reconstruction quality of videos and images, we mix video and image data at a ratio of 4 : 1. Besides the routinely used L 1 reconstruction loss and KL loss L kl , we also incorporate perceptual loss L lpips and GAN adversarial loss L adv [[22]](#b21) to enhance the reconstruction quality. The complete loss function is shown in Equation [1](#formula_1).

$Loss = L 1 + 0.1L lpips + 0.05L adv + 10 -6 L kl(1)$During training, we employ a curriculum learning strategy, gradually training from low-resolution short video to high-resolution long video. To improve the reconstruction of high-motion videos, we randomly choose a sampling interval from the range 1 ∼ 8 to sample frames evenly across video clips.

## Inference

Encoding and decoding high-resolution long videos on a single GPU can lead to out-of-memory (OOM) errors. To address this, we use a spatial-temporal tiling strategy, splitting the input video into overlapping tiles along the spatial and temporal dimensions. Each tile is encoded/decoded separately, and the outputs are stitched together. For the overlapping regions, we utilize a linear combination for blending. This tiling strategy allows us to encode/decode videos in arbitrary resolutions and durations on a single GPU.

We observed that directly using the tiling strategy during inference can result in visible artifacts due to inconsistencies between training and inference. To solve this, we introduce an additional finetuning phase where the tiling strategy is randomly enabled/disabled during training. This ensures the model is compatible with both tiling and non-tiling strategies, maintaining consistency between training and inference.

Table 1 compares our VAE with open-source state-of-the-art VAEs. On video data, our VAE demonstrates a significantly higher PSNR compared to other video VAEs. On images, our performance surpasses both video VAEs and image VAE. Figure 7 shows several cases at 256 × 256 resolution. Our VAE demonstrates significant advantages in text, small faces, and complex textures.   

## Unified Image and Video Generative Architecture

In this section, we introduce the Transformer design in HunyuanVideo, which employs a unified Full Attention mechanism for three main reasons: Firstly, it has demonstrated superior performance compared to divided spatiotemporal attention [[7,](#b6)[67,](#b66)[93,](#b92)[79]](#b78). Secondly, it supports unified generation for both images and videos, simplifying the training process and improving model scalability. Lastly, it leverages existing LLM-related acceleration capabilities more effectively, enhancing both training and inference efficiency. The model structure is illustrated in Figure [8](#fig_7).

Inputs. For a given video-text pair, the model operates within the 3D latent space described in Section 4.1. Specifically, for the video branch, the input is first compressed into latents of shape T × C × H × W . To unify input processing, we treat images as single-frame videos. These latents are then patchified and unfolded into a 1D sequence of tokens with a length of T kt • H k h • W kw using a 3D convolution with a kernel size of k t × k h × k w . For the text branch, we first use an advanced LLM to encode the text into a sequence of embeddings that capture fine-grained semantic information. Concurrently, we employ the CLIP model to extract a pooled text representation containing global information. This representation is then expanded in dimensionality and added to the timestep embedding before being fed into the model.

Model Design. To integrate textual and visual information effectively, we follow a similar strategy of "Dual-stream to Single-stream" hybrid model design as introduced in [[47]](#b46) for video generation.

In the dual-stream phase, video and text tokens are processed independently through multiple Transformer blocks, enabling each modality to learn its own appropriate modulation mechanisms without interference. In the single-stream phase, we concatenate the video and text tokens and feed them into subsequent Transformer blocks for effective multimodal information fusion. This design captures complex interactions between visual and semantic information, enhancing overall model performance.

Position Embedding. To support multi-resolution, multi-aspect ratio, and varying duration generation, we use Rotary Position Embedding (RoPE) [[77]](#b76) in each Transformer block. RoPE applies a rotary frequency matrix to the embeddings, enhancing the model's ability to capture both absolute and relative positional relationships, and demonstrating some extrapolation capability in LLMs. Given the added complexity of the temporal dimension in video data, we extend RoPE to three dimensions. Specifically, we compute the rotary frequency matrix separately for the coordinates of time (T ), height (H), and width (W ). We then partition the feature channels of the query and key into three segments (d t , d h , d w ), multiply each segment by the corresponding coordinate frequencies and concatenate the segments. This process yields position-aware query and key embeddings, which are used for attention computation.

For detailed model settings, please refer to Table [2](#tab_3).

## Text encoder

In generation tasks like text-to-image and text-to-video, the text encoder plays a crucial role by providing guidance information in the latent space. Some representative works [[66,](#b65)[21,](#b20)[51]](#b50) typically use pre-trained CLIP [[69]](#b68) and T5-XXL [[71]](#b70) as text encoders where CLIP uses Transformer Encoder and T5 uses an Encoder-Decoder structure. In contrast, we utilize a pre-trained Multimodal Large Language Model (MLLM) with a Decoder-Only structure as our text encoder, which has following advantages: (i) Compared with T5, MLLM after visual instruction finetuning has better image-text alignment in the feature space, which alleviates the difficulty of instruction following in diffusion models; (ii) Compared with CLIP, MLLM has been demonstrated superior ability in image detail description and complex reasoning [[53]](#b52); (iii) MLLM can play as a zero-shot learner [[8]](#b7) by following system instructions prepended to user prompts, helping text features pay more attention to key information. In addition, as shown in Fig. [9](#fig_8), MLLM is based on causal attention while T5-XXL utilizes bidirectional attention that produces better text guidance for diffusion models. Therefore, we follow [[55]](#b54) to introduce an extra bidirectional token refiner for enhancing text features. We have configured HunyuanVideo with a series of MLLMs [[78,](#b77)[17,](#b16)[26]](#b25) for different purposes. Under each setting, MLLMs have shown superior performance over conventional text encoder.

In addition, CLIP text features are also valuable as the summary of the text information. As shown in Fig. [8](#fig_7) We adopt the final non-padded token of CLIP-Large text features as a global guidance, integrating into the dual-stream and single-stream DiT blocks.

## Model Scaling

Neural scaling laws [[41,](#b40)[36]](#b35) in language model training offer a powerful tool for understanding and optimizing the performance of machine learning models. By elucidating the relationships between model size (N ), dataset size (D), and computational resources (C), these laws help drive the development of more effective and efficient models, ultimately advancing the success of large model training.  In contrast to prior scaling laws on large language models [41, 36, 81, 1, 2] and image generation models [[49,](#b48)[42]](#b41), video generation models typically rely on pre-trained image models. Consequently, our initial step involved establishing the foundational scaling laws pertinent to text-to-image. Building upon these foundational scaling laws, we subsequently derived the scaling laws applicable to the text-to-video model. By integrating these two sets of scaling laws, we were able to systematically determine the appropriate model and data configuration for video generation tasks.

## Image model scaling law

Kaplan et.al [[41]](#b40) and Hoffmann et.al [[36]](#b35) explored emperical scaling laws for language models on cross-entropy loss. In the field of diffusion based visual generation, Li et.al [[49]](#b48) study the scaling properties on UNet, while transformer based works such as DiT [[65]](#b64), U-ViT [[3]](#b2), Lumina-T2X [[23]](#b22), and SD3 [[21]](#b20) only study the scaling behavior between sample quality and network complexity, leaving the power-laws about the computation resources and MSE loss used by diffusion models unexplored.

In order to fill the gap, we develop a family of DiT-like models, named as DiT-T2X to distinguish from the original DiT, where X can be the image (I) or the video (V). DiT-T2X applies T5-XXL [[71]](#b70) as the text encoder and the aformentioned 3D VAE as the image encoder. The text information is injected to the model according to cross-attention layers. The DiT-T2X family has seven sizes ranging from 92M to 6.6B. The models were trained using DDPM [[34]](#b33) and v-prediction [[73]](#b72) with consistent hyperparameters and the same dataset with 256px resolution. We follow the experiment method introduced by [[36]](#b35) and build the neural scaling laws to fit

$N opt = a 1 C b1 , D opt = a 2 C b2 .(2)$As shown in Fig. [10](#fig_9) (a), the loss curve of each model decreases from top left to bottom right, and it always passes through the loss curve of the larger size model adjacent to it. It means that each curve will form two intersections with curves of the larger and the smaller models. Under the corresponding computation resources between the two intersections, the middle-sized model is optimal (with the lowest loss). After obtaining the envelope of lowest losses across all the x-axis values, we fill the Equation ( [2](#formula_2)) to find out that a  

## Model-pretraining

We use Flow Matching [[52]](#b51) for model training and split the training process into multiple stages.

We first pretrain our model on 256px and 512px images, then conduct joint training on images and videos from 256px to 960px.

## Training Objective

In this work, we employ the Flow Matching framework [[52,](#b51)[21,](#b20)[13]](#b12) to train our image and video generation model. Flow Matching transforms a complex probability distribution into a simple probability distribution through a series of variable transformations of the probability density function, and generates new data samples through inverse transformations.

During the training process, given an image or video latent representation x 1 in the training set. We first sample t ∈ [0, 1] from a logit-normal distribution [[21]](#b20) and initialize a noise x 0 ∼ N (0, I) following Gaussion distribution. The training sample x t is then constructed using a linear interpolation method [[52]](#b51). The model is trained to predict the velocity u t = dx t /dt, which guides the sample x t towards the sample x 1 . The model parameters are optimized by minimizing the mean squared error between the predicted velocity v t and the ground truth velocity u t , expressed as the loss function

$L generation = E t,x0,x1 ∥v t -u t ∥ 2 . (3$$)$During the inference process, a noise sample x 0 ∼ N (0, I) is drawn initially. The first-order Euler ordinary differential equation (ODE) solver is then used to compute x 1 by integrating the model's estimated values for dx t /dt. This process ultimately generates the final sample x 1 .

## Image Pre-training

At our early experiments, we found that a well pretrained model significantly accelerates the convergence of video training and improves video generation performance. Therefore, we introduce a two-stage progressive image pretraining strategy to serve as a warmup for video training.

## Image stage 1 (256px training).

The model is first pretrained with low-resolution 256px images. Specifically, we follow previous work [[66]](#b65) to enable multi-aspect training based on 256px, which helps the model learn to generate images with a wide range of aspect ratios while avoiding the textimage misalignments caused by the crop operation in image preprocessing. Meanwhile, pretraining with low resolution samples allows the model to learn more low-frequency concepts from a larger amount of samples.

Image stage 2 (mix-scale training). We introduce a second image-pretraining stage to further facilitate the model ability on higher resolutions, such as 512px. A trivial solution is to directly funetuning on images based on 512px. However, we found that the model performance finetuned on 512px images will degrade severely on 256px image generation, which may affect the following video pretraining on 256px videos. Therefore, we propose mix-scale training, where two or more scales of multi-aspect buckets are included for each training global batch. Each scale have an anchor size, and then the multi-aspect buckets are built based on the anchor size. We train the model on a two-scale dataset with anchor sizes 256px and 512px for learning higher resolution images while maintaining the ability on low resolutions. We also introduce dynamic batch sizes for micro batches with different image scales, maximaizing the GPU memory and computation utilization.

## Video-Image joint training

Multiple aspect ratios and durations bucketization. After the data filtering process described in Section 3.1, the videos have different aspect ratios and durations. To effectively utilize the data, we categorize the training data into buckets based on duration and aspect ratio. We create B T duration buckets and B AR aspect ratio buckets, resulting in a total of B T × B AR buckets. As the number of tokens varies across buckets, we assign each bucket a maximum batch size that can prevent out-of-memory (OOM) errors, to optimize GPU resource utilization. Before training, all data is allocated to the nearest bucket. During training, each rank randomly pre-fetches batch data from a bucket. This random selection ensures the model is trained on varying data sizes at each step, which helps maintain model generalization by avoiding the limitations of training on a single size.

Progressive Video-Image Joint Training. Generating high-quality, long-duration video sequences directly from text often leads to difficulties in model convergence and suboptimal results. Therefore, progressive curriculum learning has become a widely adopted strategy for training text-to-video models. In HunyuanVideo, we designed a comprehensive curriculum learning strategy, starting with model initialization using T2I parameters and progressively increasing video duration and resolution.

• Low-resolution, short video stage. The model establishes the basic mapping between text and visual content, ensuring consistency and coherence in short-term actions. • Low-resolution, long video stage. The model learns more complex temporal dynamics and scene changes, ensuring temporal and spatial consistency over a longer duration. • High-resolution, long video stage. The model enhances video resolution and detail quality while maintaining temporal coherence and managing complex temporal dynamics.

Additionally, at each stage, we incorporate images in varying proportions for video-image joint training. This approach addresses the scarcity of high-quality video data, enabling the model to learn more extensive and diverse world knowledge. It also effectively prevents catastrophic forgetting of image-space semantics due to distributional discrepancies between video and image data.

## Prompt Rewrite

To address the variability in linguistic style and length of user-provided prompts, we employ the Hunyuan-Large model [[78]](#b77) as our prompt rewrite model to adapt the original user prompt to the modelpreferred prompt. Utilized within a training-free framework, the prompt rewrite model capitalizes on detailed prompt instructions and in-context learning examples to enhance its performance. The key functionalities of this prompt rewrite module are as follows:

• Multilingual Input Adaptation: The module is designed to process and comprehend user prompts across various languages, ensuring that meaning and context are preserved. • Standardization of Prompt Structure: The module rephrases prompts to conform to a standardized information architecture, akin to training captions. • Simplification of Complex Terminology: The module simplifies complex user wording into more straightforward expressions, all while maintaining the user's original intent.

Furthermore, we implement a self-revision technique [[43]](#b42) to refine the final prompt. This involves a comparative analysis between the original prompt and the rewritten version, ensuring that the output is both accurate and aligned with the model's capabilities.

To accelerate and simplify the application process, we also fine-tune a Hunyuan-Large model with LoRA for prompt rewriting. The training data for this LoRA tuning was sourced from the high-quality rewrite pairs collected through the training-free method.

## High-performance Model Fine-tuning

In the pre-training stage, we utilized a large dataset for model training. While this dataset is rich in information, it displayed considerable variability in data quality. To create a robust generation model capable of producing high-quality, dynamic videos and improving its proficiency in continuous motion control and character animation, we carefully selected four specific subsets from the full dataset for fine-tuning. These subsets underwent an initial screening using automated data filtering techniques, followed by manual review. Additionally, we implemented various model optimization strategies to maximize generation performance. 

## Model Acceleration

## Inference Step Reduction

To improve the inference efficiency, we firstly consider reducing the number of inference steps. Compared to image generation, it is more challenging to maintain the spatial and temporal quality of the generated videos with lower inference steps. Inspired by a previous observation that the first time-steps contribute to most changes during the generation process [[101,](#b100)[67,](#b66)[98,](#b97)[99]](#b98), we utilize the time-step shifting to handle the case of lower inference steps. Specifically, given the inference step q ∈ {1, 2, ..., Q}, t = 1 -q Q is the input time condition for the generation model, where the noise is initialized at t = 1 and the generation process halts at t = 0. Instead of using t directly, we map t to t ′ with a shifting function t ′ = s * t 1+(s-1) * t , where t ′ is the input time condition and s is the shifting factor. If s > 1, the flow model is conditioned more on early time steps. A critical observation is that a lower inference step requires a larger shifting factor s. Empirically, s is set as 7 for 50 inference steps, while s should be increased to 17 when the number of inference steps is smaller than 20. The time-step shifting strategy enables the generation model to match the results of numerous inference steps with a reduced number of steps.

MovieGen [[67]](#b66) applies the linear-quadratic scheduler to achieve a similar goal. The schedulers are visualized in Figure [11a](#fig_12). However, we find that our time-step shifting is more effective than the linear-quadratic scheduler in the case of extremely low number of inference steps, e.g., 10 steps. As shown in Figure [11b](#fig_12), the linear-quadratic scheduler results in worse visual quality.

## Text-guidance Distillation

Classifier-free guidance (CFG) [[35]](#b34) significantly improves the sample quality and motion stability of text-conditioned diffusion models. However, it increases computational cost and inference latency since it requires extra output for the unconditional input at each inference step. Especially for the large video model and high-resolution video generation, the inference burden is extremely expensive when generating text-conditional and text-unconditional videos, simultaneously. To solve this limitation, we distill the combined output for unconditional and conditional inputs into a single student model [[60]](#b59). Specifically, the student model is conditioned on a guidance scale and shares the same structures and hyper-parameters as the teacher model. We initialize the student model with the same parameters as the teacher model and train with the guidance scale randomly sampled from 1 to 8. We experimentally find that text-guidance distillation approximatively brings 1.9x acceleration.

## Efficient and Scalable Training

To achieve scalability and efficient training, we train HunyuanVideo on AngelPTM [[62]](#b61), the largescale pretraining framework from Tencent Angel machine learning team. In this part, we first outline the hardware and infrastructure used for training, and then give a detailed introduction to the model parallel method and its optimization methods, followed by the automatic fault tolerance mechanism.

## Hardware Infrastucture

To ensure efficient communication in large-scale distributed training, we setup a dedicated distributed training framework termed Tencent XingMai network [[48]](#b47) for highly efficient inter-server communication. The GPU scheduling for all training tasks is completed through the Tencent Angel machine learning platform, which provides powerful resource management and scheduling capabilities.

## Parallel Strategy

HunyuanVideo training adopts 5D parallel strategies, including tensor parallelism (TP) [[74]](#b73), sequence parallelism (SP) [[45]](#b44), context parallelism (CP) [[63]](#b62), and data parallelism combined with Zero optimization (DP + ZeroCache [[62]](#b61)). The tensor parallelism (TP) is based on the principle of block calculation of matrices. The model parameters (tensors) are divided into different GPUs to reduce GPU memory usage and accelerate the calculation. Each GPU is responsible for the calculation of different parts of tensors in the layer.

Sequence parallelism (SP) is based on TP. The input sequence dimension is sliced to reduce the repeated calculation of operators such as LayerNorm and Dropout, and reduce the storage of the same activations, which effectively reduces the waste of computing resources and GPU memory. In addition, for input data that does not meet the SP requirements, the engineering equivalent SP Padding capability is supported.

Context parallelism (CP) is sliced in the sequence dimension to support long-sequence training. Each GPU is responsible for calculating the Attention of different sequence slices. Specifically, Ring Attention [[30]](#b29) is used to achieve efficient training of long sequences through multiple GPUs, breaking through the GPU memory limitation of a single GPU.

In addition, data parallelism + ZeroCache is leveraged to support horizontal expansion through data parallelism to meet the demand for increasing training data sets. Then, based on data parallelism, the ZeroCache optimization strategy is used to further reduce the redundancy of model states (model parameters, gradients, and optimizer states), and we unify the use of GPU memory to maximize the GPU memory usage efficiency.

## Optimization

Attention optimization. As the sequence length increases, the attention calculation becomes the main bottleneck of training. We accelerated the attention calculation with FusedAttention.

Recomputation and activations offload optimization. Recomputation is a technology that trade calculations for storage. It is mainly made up of three parts: a) specifying certain layers or blocks for recalculation, b) releasing the activations in the forward calculation, and c) obtaining the dependent activations through recalculation in the backward calculation, which significantly reduces the use of GPU memory during training. In addition, considering the PCIe bandwidth and the host memory size, a layer-based activation offload strategy is adopted. Without reducing the training performance, the activations in the GPU memory are offloaded to the host memory, further saving GPU memory.

## Automatic fault tolerance

In terms of the large-scale training stability of HunyuanVideo, an automatic fault tolerance mechanism is used to quickly restore training for common hardware failures. This avoids frequent occurrence of the manual recovery of training tasks. By automatically detecting errors and quickly replacing healthy nodes to pull up training tasks, the training stability is 99.5%.

## Fundation Model Performance

Text Alignment One of the key metrics for video generative models is their ability to follow text prompts accurately. This capability is essential for the effectiveness of these models. However, some open-source models often struggle to capture all subjects or accurately represent the relationships between multiple subjects, particularly when the input text prompt is complex. HunyuanVideo demonstrates robust capabilities in generating videos that closely adhere to the provided text prompts. As illustrated in Figure [12](#fig_1), it effectively manages multiple subjects within the scene.

Figure [12](#fig_1): Prompt: A white cat sits on a white soft sofa like a person, while its long-haired male owner, with his hair tied up in a topknot, sits on the floor, gazing into the cat's eyes. His child stands nearby, observing the interaction between the cat and the man.

## High-quality

We also perform a fine-tuning process to enhance the spatial quality of the generated videos. As illustrated in Figure [13](#fig_13), HunyuanVideo is capable of producing videos with ultra-detailed content.

## High-motion Dynamics

In this part, we demonstrate HunyuanVideo's capabilities in producing high-dynamic videos based on given prompts. As shown in Figure [14](#fig_15), our model excels in generating videos that encompass a wide range of scenes and various types of motion.

Concept Generalization One of the most desirable features of a generative model is its ability to generalize concepts. As illustrated in Figure [15](#fig_16), the text prompt describes a scene: "In a distant galaxy, an astronaut floats on a shimmering, pink, gemstone-like lake that reflects the vibrant colors of the surrounding sky, creating a stunning scene. The astronaut gently drifts on the lake's surface, while the soft sounds of water whisper the planet's secrets. He reaches out, his fingertips gliding (a) Prompt: the ultra-wide-angle lens follows closely from the hood, with raindrops continuously splattering against the lens. Ahead, a sports car speeds around a corner, its tires violently skidding against the wet road, creating a mist of water. Neon lights refract in the rain, leaving colorful streaks on the car's surface. The camera swiftly shifts to the side of the car, capturing the wheels spinning at high speed, before finally moving to the rear.

(b) Prompt: a stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.   over the cool, smooth water." Notably, this specific scenario has not been encountered in the training dataset. Furthermore, it is evident that the depicted scene combines several concepts that are also absent from the training data. to the text prompts (1) 'In a distant galaxy, an astronaut floats on a shimmering, pink, gemstone-like lake that reflects the vibrant colors of the surrounding sky, creating a stunning scene. The astronaut gently drifts on the lake's surface, the soft sounds of water whispering the planet's secrets. He reaches out, his fingertips gliding over the cool, smooth water. ', (2) 'A macro lens captures a tiny orchestra of insects playing instruments.' and (3) 'The night-blooming cactus flowers in the evening, with a brief, rapid closure. Time-lapse shot, extreme close-up. Realistic, Night lighting, Mysterious.' respectively.

Action Reasoning and Planning Leveraging the capabilities of large language models, Hunyuan-Video can generate sequential movements based on a provided text prompt. As demonstrated in Figure [16](#fig_17), HunyuanVideo effectively captures all actions in a photorealistic style.  

## Comparison with SOTA Models

To evaluate the performance of HunyuanVideo, we selected five strong baselines from closed-source video generation models. In total, we utilized 1,533 text prompts, generating an equal number of video samples with HunyuanVideo in a single run. For a fair comparison, we conducted inference only once, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models, ensuring consistent video resolution. 60 professional evaluators performed the evaluation and the results are presented in Table [3](#tab_4). Videos  7 Applications

## Audio Generation based on Video

Our video-to-audio(V2A) module is designed to enhance generated video content by incorporating synchronized sound effects and contextually appropriate background music. Within the conventional film production pipeline, Foley sound design constitutes an integral component, significantly contributing to the auditory realism and emotional depth of visual media. However, the creation of Foley audio is both time-intensive and demands a high degree of professional expertise. With the advent of an increasing number of text-to-video (T2V) models, most of them lack the corresponding foley generation capabilities, thereby limiting their ability to produce fully immersive content. Our V2A module addresses this critical gap by autonomously generating cinematic-grade foley audio tailored to the input video and textual prompts, thus enabling the synthesis of a cohesive and holistically engaging multimedia experience.

## Data

Unlike text-to-video (T2V) models, video-to-audio (V2A) models have different requirements for data. As mentioned above, we constructed a video dataset comprising of video-text pairs. However, not all data in this dataset are suitable for training the V2A model. For example, some videos lack an audio stream, others contain extensive voice-over content or their ambient audio tracks have been removed and replaced with unrelated elements. To address these challenges and ensure data quality, we designed a robust data filtering pipeline specifically tailored for V2A training.

First, we filter out videos without audio streams or those in which the silence ratio exceeds 80%. Next, we employ a frame-level audio detection model, like [[38]](#b37), to detect speech, music, and general sound in the audio stream. Based on this analysis, we classify the data into four distinct categories: pure sound, sound with speech, sound with music, and pure music. Subsequently, to prioritize high-quality data, we train a model inspired by CAVP [[54]](#b53) to compute a visual-audio consistency score, which quantifies the alignment between the visual and auditory components of each video. Using this scoring system in conjunction with the audio category labels, we systematically sample portions of data from each category, retaining approximately 250,000 hours from the original dataset for pre-training. For the supervised fine-tuning stage, we further refine our selection, curating a subset of millions of high-quality clips (80,000 hours).

For feature extraction, we use CLIP [[70]](#b69) to obtain visual features at a temporal resolution of 4 fps and subsequently resample these features to align with the audio frame rate. To generate captions, we employ [[29]](#b28) as the sound captioning model and [[20]](#b19) as the music captioning model. When both sound and music captions are available, we merge them into a structured caption format, following the approach detailed in [[67]](#b66).

## Model

Just like the above-mentioned text-to-video model, our video-to-audio generation model also adopts a flow-matching-based diffusion transformer (DiT) as its architectural backbone. The detailed design of the model is depicted in Figure [18](#fig_20), illustrating a transition from a triple-stream structure to a single-stream DiT framework.

The model operates within a latent space encoded by a variational autoencoder (VAE) trained on mel-spectrograms. Specifically, the audio waveform is first converted into a 2D mel-spectrogram representation. This spectrogram is subsequently encoded into a latent space using a pretrained VAE.

For feature extraction, we leverage pretrained CLIP [[70]](#b69) and T5 [[71]](#b70) encoders to independently extract visual and textual features, respectively. These features are subsequently projected into the DiT-compatible latent space using independent linear projections followed by SwiGLU activation, as depicted in Figure [18](#fig_20).

To effectively integrate multimodal information, we incorporate stacked triple-stream transformer blocks, which independently process visual, audio, and textual modalities. These are later followed by single-stream transformer blocks to ensure seamless fusion and alignment across modalities. This design enhances the alignment between audio-video and audio-text representations, facilitating improved multimodal coherence.

Once the latent representation is generated by the diffusion transformer, the VAE decoder reconstructs the corresponding mel-spectrogram. Finally, the mel-spectrogram is converted back into an audio waveform using a pre-trained HifiGAN vocoder [[44]](#b43). This framework ensures a high-fidelity reconstruction of audio signals while maintaining strong multimodal alignment.  Image-to-video (I2V) task is a common application in video generation tasks. It usually means that given an image and a caption, the model uses this image as the first frame to generate a video that matches the caption. Although the naive HunyuanVideo is a text-to-video (T2V) model, it can be easily extended to an I2V model. Specifically, as mentioned in Sec. 4.2, the T2V model's input is a latent X with a shape of T × C × H × W , where T , C, H and W represent the frame, channel, height and width of the compressed video respectively. Similar to Emu [[25]](#b24), in order to introduce image condition I, we treat I as the first frame of a video and apply zero-padding to create a T × C × H × W tensor I o , as shown in Fig. [19](#fig_22). Additionally, we employ a binary mask m with dimensions T × 1 × H × W , where the first temporal position is set to 1, and all other positions are set to zero. Then the latent X, the tensor I o and the mask m are concatenated along the channel dimension to form the input for the model. Note that since the channel of the input tensor has changed from C to 2C + 1, as shown in Fig. [19](#fig_22), we need to adjust the parameters of the first convolutional module of the model from ϕ = (C in (= C), C out , s h , s w ) to ϕ ′ = (C ′ in (= 2C + 1), C out , s h , s w ), where each component corresponds to the input channel C in /C ′ in , output channel C out , height of the convolutional kernel s h , and width of the convolutional kernel s w . In order to retain the representation ability of the T2V model, the first C input channels of ϕ ′ are directly copied to ϕ, and the rest are initialized to zero. We pre-train the I2V model on the same data as T2V model, and the results are shown in Fig. [20](#fig_23).

## Downstream Task Fine-tuning: Portrait Image-to-Video Generation

We perform supervised finetuning of our I2V model on two million portrait videos to enhance human's motion and overall aesthetics. In addition to the standard data filtering pipeline described in section 3, we also apply face and body detectors to filter out the training videos which have more than five persons. We also remove the videos in which the main subjects are small. Finally, the rest videos will be manually inspected to obtain the final high-quality portrait training dataset.

Regarding training, we adopt a progressive fine-tuning strategy, gradually unfreezing the model parameters of the respective layers while keeping the rest frozen during finetuning. This approach allows the model to achieves high performance in the portrait domain without compromising much of its inherent generalization ability, guaranteeing commendable performance in natural landscapes, animals, and plants domains. Moreover, our model also supports video interpolation by using the first and last frames as conditions. We randomly drop the text conditions at certain probability during training to enhance the model's performance. Some demo results are shown in Fig. [21](#fig_24).  

## Avatar animation

HunyuanVideo empowers controllable avatar animation in various aspects. It enables animating characters using explicit driving signals(e.g., speech signals, expression templates, and pose templates). In addition, it also integrates the implicit driving paradigm using text prompts. Fig. [22](#fig_25) shows how we leverage the power of HunyuanVideo to animate characters from multi-modal conditions. To maintain strict appearance consistency, we modify the HunyuanVideo architecture by inserting latent of reference image as strong guidance. As shown in Fig. [22 (b,](#fig_25)[c](#)), we encode reference image using 3DVAE obaining z ref ∈ R 1×c×h×w , where c = 16. Then we repeat it t times along temporal dimension and concatenate with z t in channel dimension to get the modified noise input ẑt ∈ R t×2c×h×w . To achieve controllable animation, various adapters are employed. We describe them in following.

## Upper-Body Talking Avatar Generation

In recent years, audio-driven digital human algorithms have made significant progress, especially in the performance of the talking head. Early algorithms, such as loopy [[94]](#b93), emo [[80]](#b79), and hallo [[87]](#b86), mainly focused on the head area, driving the digital human's facial expressions and lip shapes by analyzing audio signals. Even earlier algorithms, like wav2lip [[68]](#b67) and DINet [[97]](#b96), concentrated on modifying the mouth region in the input video to achieve lip shape consistency with the audio. However, these algorithms are usually limited to the head area, neglecting other parts of the body. To achieve a more natural and vivid digital human performance, we propose an audio-driven algorithm extended to the upper body. In this algorithm, the digital human not only synchronizes facial expressions and lip shapes with the audio while speaking but also moves the body rhythmically with the audio.

Audio-Driven Based on the input audio signal, our model can adaptively predict the digital human's facial expressions and posture action information . This allows the driven character to speak with emotion and expression, enhancing the digital human's expressiveness and realism. As shown in Fig. [22 (b)](#fig_25), for the single audio signal-driven part, the audio passes through the whisper feature extraction module to obtain audio features, which are then injected into the main network in a cross-attention manner. It should be noted that the injection process will be multiplied by the face-mask to control the audio's effect area. While enhancing the head and shoulder control ability, it will also greatly reduce the probability of body deformation. To obtain more lively head movements, head pose motion parameters and expression motion parameters are introduced and added to the time step in an embedding manner. During training, the head motion parameters are given by the variance of the nose tip keypoint sequence, and the expression parameters are given by the variance of the facial keypoints.

## Fully-Controlled Whole-Body Avatar Generation

Controlling digital character's motion and expression explicitly has been a long-standing problem in both academia and industry, and recent advancement of diffusion models paved the first step to realistic avatar animation. However, current avatar animation solutions suffer from partial controllability due to limited capability of foundation video generation model. We demonstrate that a stronger T2V model boosts the avatar video generation to fully-controllable stage. We show how HunyuanVideo serves as strong foundation with limited modifications to extent general T2V model to fully-controllable avatar generation model in Fig. [22 (c)](#fig_25).

Pose-Driven We can control the digital character's body movements explicitly using pose templates. We use Dwpose [[92]](#b91) to detect skeletal video from any source video, and use 3DVAE to transform it to latent space as z pose . We argue that this eases the fine-tuning process because both input and driving videos are in image representation, and are encoded with shared VAE, resulting same latent space. We then inject the driving signals to the model by element-wise add as ẑt + z pose . Note that ẑt contains the appearance information of reference image. We use full-parameters finetune with pretrained T2V weights as initialization. Expression-Driven We can also control the facial expressions of digital character using implicit expression representations. Although facial landmarks are widely adopted in this area [[58,](#b57)[16]](#b15), we argue using landmarks brings ID leak due to cross-ID misalignment. Instead, we use implicit representations as driving signals for their ID and expression disentanglement capabilities. In this work, we use VASA [[88]](#b87) as expression extractor. As shown in Fig. [22](#fig_25) (c), we adopt a light-weight expression encoder to transform the expression representation to token sequence in latent space as z exp ∈ R t×n×c , where n is the number of tokens per frame. Typically, we set n = 16. Unlike pose condition, we inject z exp using cross-attention because ẑt and z exp are not naturally aligned in spatial aspect. We add cross-attention layer Attn exp (q, k, v) every K double and single-stream DiT layers to inject expression latent. Denote the hidden states after i-th DiT layer as h i , the injection of expression z exp to h i could be derived as: h i + Attn exp (h i , z exp , z exp ) * M face , where M face is the face region mask that guides where z exp should be applied at, and * stands for element-wise multiplication. Also, full-parameters tuning strategy is adopted.

Hybrid Condition Driven Combining both pose and expression driven strategies derives hybrid control approach. In this scenario, the body motion is controlled by explicit skeletal pose sequence, and the facial expression is determined by implicit expression representation. We jointly fine-tune T2V modules and added modules in an end-to-end fasion. During inference, the body motion and facial motion could be controlled by separate driving signals, empowering richer editability.

## Application Demos

We present extensive results of avatar animations to show the superiority and potential of bringing avatar animation empowered by HunyuanVideo to next generation.

Audio-Driven Fig. [23](#fig_26) shows that HunyuanVideo serves as a strong foundation model for audiodriven avatar animation, which can synthesize vivid and high-fidelity videos. We summarize the superiority of our method in three folds: • Upper-body Animation. Our method can drive not only portrait characters, but also upper-body avatar images, enlarging its range of application scenarios. • Dynamic Scene Modelling. Our method can generate videos with vivid and realistic background motion, such as the wave undulation, crowd movement, and breeze stirring leaves. • Vivid Avatar Movements. Our method is able to animate the character talking while gesturing vividly with audio solely.

Pose-Driven We also show that HunyuanVideo boosts the performance of pose-driven animation largely in many aspects in Fig. [24:](#fig_3) • High ID-Consistency. Our method maintains the ID-consistency well over the frames even with large poses, making it face-swapping free, thereby, could be used as real end-to-end animation solution. • Following Complex Poses Accurately. Our method is able to handle very complex poses such as turning around and hands crossed. • High Motion Quality. Our method has remarkable capability in dynamic modelling. For instance, the results show promising performance in terms of garment dynamics and texture consistency.

• Generalizability. Our method presents surprisingly high generalizability. It can animate wide variety of avatar images, such as real human, anime, pottery figurine, and even animals.

Expression-Driven Fig. [25](#fig_4) presents how HunyuanVideo enhances the portrait expression animating in three folds:

• Exaggerated Expression. Our method is able to animate given portrait to mimic any facial movements even with large poses and exaggerated expressions.

• Mimicing Eye Gaze Accurately. We can control the portraits' eye movements acurately given any expression template, even with extreme and large eye balls movements.

![Figure 2: Left: Computation resources used for closed-source and open-source video generation models. Right: Performance comparison between HunyuanVideo and other selected strong baselines.]()

![Figure 3: The overall training system for HunyuanVideo.]()

![Figure 4: Our hierarchical data filtering pipeline. We employ various filters for data filtering and progressively increase their thresholds to build 4 training datasets, i.e., 256p, 360p, 540p, and 720p, while the final SFT dataset is built through manual annotation. This figure highlights some of the most important filters to use at each stage. A large portion of data will be removed at each stage, ranging from half to one-fifth of the data from the previous stage. Here, gray bars represent the amount of data filtered out by each filter while colored bars indicate the amount of remaining data at each stage.]()

![Figure 5: The overall architecture of HunyuanVideo. The model is trained on a spatial-temporally compressed latent space, which is compressed through Causal 3D VAE. Text prompts are encoded using a large language model, and used as the condition. Gaussian noise and condition are taken as input, our model generates a output latent, which is decoded into images or videos through the 3D VAE decoder.]()

![Figure 7: VAE reconstruction case comparison.]()

![Figure 8: The architecture of our HunyuanVideo Diffusion Backbone.]()

![Figure 9: Text encoder comparison between T5 XXL and the instruction-guided MLLM introduced by HunyuanVideo.]()

![Figure10: Scaling laws of DiT-T2X model family. On the top-left (a) we show the loss curves of the T2X(I) model on a log-log scale for a range of model sizes from 92M to 6.6B. We follow[36] to plot the envelope in gray points, which are used to estimate the power-law coefficients of the amount of computation (C) vs model parameters (N ) (b) and the computation vs tokens (D) (c). Based on the scaling law of the T2X(I) model, we plot the scaling law of the corresponding T2X(V) model in (d), (e), and (f).]()

![= 5.48 × 10 -4 , b 1 = 0.5634, a 2 = 0.324 and b 2 = 0.4325, where the units of a 1 , a 2 , N opt , D opt are billions while C has a unit of Peta FLOPs. The Fig.10 (b) and Fig.10 (c) show that DiT-T2X(I) family fits the power law quite well. Finally, given computation budgets, we can calculate the optimal model size and dataset size.]()

![Video model scaling lawBased on the scaling law of the T2X(I) model, we select the optimal image checkpoint (i.e., the model on the envelope) corresponding to each size model to serve as the initialization model for the video scaling law experiment. Fig. 10 (d), Fig. 10 (e), and Fig. 10 (f) illustrate the scaling law results of the T2X(V) model, where a 1 = 0.0189, b 1 = 0.3618, a 2 = 0.0108 and b 2 = 0.6289. Based on the results of Fig. 10 (b) and Fig. 10 (e), and taking into account the training consumption and inference cost, we finally set the model size to 13B. Then the number of tokens for image and video training can be calculated as shown in Fig. 10 (c) and Fig. 10 (f). It is worth noting that the amount of training tokens calculated by image and video scaling laws is only related to the first stage of training for images and videos respectively. The scaling property of progressive training from low-resolution to high-resolution will be left explored in future work.]()

![Figure 11: (a) Different time-step schedulers. For our shifting stragty, we set a larger shifting factor s for a lower inference step. (b) Generated videos with only 10 inference steps. The shifting stragty leads to significantly better visual quality.]()

![Figure 13: High-quality videos generated by HunyuanVideo.]()

![(a) Prompt: At sunset, a modified Ford F-150 Raptor roared past on the off-road track. The raised suspension allowed the huge explosion-proof tires to flip freely on the mud, and the mud splashed on the roll cage. (b) Prompt: The panning camera moves forward slowly, with a depth of field in the middle focus, and warm sunset light covers the screen. The girl in the picture runs with her skirt fluttering, turns and jumps. (c) Prompt: In the gym, a woman in workout clothes runs on a treadmill. Side angle. Realistic, Indoor lighting, Professional. (d) Prompt: Swimmer swimming underwater, in slow motion. Realistic, Underwater lighting, Peaceful. (e) Prompt: On the rooftop, there is an open-air basketball court, and five male students are playing basketball. Realistic, Natural lighting, Casual.]()

![Figure 14: High-motion dynamics videos generated by HunyuanVideo.]()

![Figure 15: HunyuanVideo's performance on concept generalization. The results of the three rows correspond]()

![Figure 16: Prompt: The woman walks over and opens the red wooden door. As the door swings open, seawater bursts forth, in a realistic style.]()

![17.]()

![Figure 17: High text-video alignment videos generated by HunyuanVideo. Top row: Prompt: A close-up of a wave crashing against the beach, the sea foam spells out "WAKE UP" on the sand. Bottom row: Prompt: In a garden filled with blooming flowers, "GROW LOVE" has been spelled out with colorful petals.]()

![Figure 18: The architecture of sound effect and music generation model.]()

![𝜙 = 𝐶!" = 𝐶 , 𝐶#$%, 𝑠&, 𝑠' 𝜙 ( = 𝐶 !" ( = 𝐶 + 𝐶 + 1 , 𝐶#$%, 𝑠&, 𝑠']()

![Figure 19: Differences between text-to-video (T2V) model and image-to-video (I2V) model.]()

![Figure 20: Sample results of the I2V pre-training model.]()

![Figure 21: Sample results of our portrait I2V model.]()

![Figure 22: Overview of Avatar Animation built on top of HunyuanVideo. We adopt 3D VAE to encode and inject reference and pose condition, and use additional cross-attention layers to inject audio and expression signals. Masks are employed to explicitly guide where they are affecting.]()

![Figure 23: Audio-Driven. HunyuanVideo can generate vivid talking avatar videos.]()

![Figure 24: Pose-Driven. HunyuanVideo can animate wide variety of characters with high quality and appearance consistency under various poses.]()

![Figure 26: Hybrid Condition-Driven. HunyuanVideo supports full control with multiple driving sources across various avatar characters.]()

![VAE reconstruction metrics comparison.]()

![Architecture hyperparameters for the HunyuanVideo 13B parameter foundation model.]()

![Model Performance Evaluation]()

https://github.com/Tencent/HunyuanVideo

