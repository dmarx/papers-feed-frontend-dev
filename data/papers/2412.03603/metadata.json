{
  "arxivId": "2412.03603",
  "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
  "authors": "Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong",
  "abstract": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo.",
  "url": "https://arxiv.org/abs/2412.03603",
  "issue_number": 591,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/591",
  "created_at": "2025-01-04T06:53:33.619685",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 74,
  "last_read": "2025-01-04T14:48:27.533139",
  "last_visited": "2024-12-30T15:16:27.366Z",
  "main_tex_file": null,
  "published_date": "2024-12-03T23:52:37Z",
  "arxiv_tags": [
    "cs.CV"
  ]
}