# Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control

## Abstract

## 

Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoreticallysound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.

## 

Figure [1](#) We introduce Adjoint Matching, a theoretically-driven yet simple algorithm for reward fine-tuning that works for a large family of dynamical generative models, including for the first time, Flow Matching models. Text prompts: "Beautiful colorful sunset midst of building in Bangkok Thailand ", "Beautiful grandma and granddaughter are mixing salad and smiling while cooking in kitchen", "The beautiful young woman in sunglasses is standing at the background of field and hill. She is smiling and looking over shoulder ", "Chess, intellectual games, figure horse, chess board ".

## Introduction

Flow Matching [(Lipman et al., 2023;](#b46)[Albergo and Vanden-Eijnden, 2023;](#)[Liu et al., 2023)](#b49) and denoising diffusion [(Song and Ermon, 2019;](#b73)[Ho et al., 2020;](#b36)[Song et al., 2021b;](#)[Kingma et al., 2021)](#) models are being used for many generative modeling applications, including text-to-image [(Rombach et al., 2022;](#b65)[Esser et al., 2024)](#b23), text-to-video [(Singer et al., 2022)](#b70), and text-to-audio [(Le et al., 2024;](#b43)[Vyas et al., 2023)](#b85). In most cases, the base generative model does not achieve the desired sample quality. To improve the generated samples, it is common to resort to techniques such as classifier-free guidance [(Ho and Salimans, 2022;](#b35)[Zheng et al., 2023)](#b95) to get better text-to-sample alignment, or to fine-tune using human preference reward models to improve sample quality and realism [(Wallace et al., 2023a;](#)[Clark et al., 2024)](#b17).

In the adjacent field of large language models, the behavior of the model is aligned to human preferences through fine-tuning with reinforcement learning from human feedback (RLHF). Either explicitly or implicitly, RLHF methods [(Ziegler et al., 2020;](#)[Stiennon et al., 2020;](#b75)[Ouyang et al., 2022;](#b56)[Bai et al., 2022](#b4)) assume a reward model r(x) that captures human preferences, with the goal of modifying the base generative model such that it generates the following tilted distribution:

$p * (x) ∝ p base (x) exp(r(x)),(1)$where p base is the base generative model's sample distribution.

Inspired by this, fine-tuning methods have been developed to improve denoising diffusion models based on human preference data; either using a reward-based approach [(Fan and Lee, 2023;](#)[Black et al., 2024;](#b11)[Fan et al., 2023;](#)[Xu et al., 2023;](#b91)[Clark et al., 2024;](#b17)[Uehara et al., 2024a,b)](#), or direct preference optimization [(Wallace et al., 2023a)](#). However, unlike the fine-tuning methods designed for large language models, most of the existing methods to a large degree ignore p base and focus solely on the reward model. Reward models can range from standard evaluation metrics such as ClipScore [(Hessel et al., 2021;](#b34)[Kirstain et al., 2023)](#b42) to specialized models that have been trained on human preferences [(Schuhmann and Beaumont, 2022;](#b68)[Xu et al., 2023;](#b91)[Wu et al., 2023c)](#). As these are parameterized by neural networks, they fall pray to adversarial examples which lead to the generation of undesirable artifacts [(Goodfellow et al., 2014;](#b30)[Mordvintsev et al., 2015)](#b54). This has led some works to consider adding regularization during fine-tuning [(Fan et al., 2024;](#b26)[Uehara et al., 2024b)](#) to incentivize staying close to the base model distribution; however, there does not yet exist a simple approach which actually provably generates from the tilted distribution (1).

The main contributions of our paper are as follows:

(i) We present a stochastic optimal control (SOC) formulation for reward fine-tuning of dynamical generative models. Importantly, we prove that the naïve approach considered by prior works lead to a value function bias problem that biases the fine-tuned model away from the tilted distribution (1). This problem has also been observed by [Uehara et al. (2024b)](#) but they propose a more complicated solution which involves training a separate generative model for the optimal noise distribution.

(ii) Instead, we propose a very simple solution: the memoryless noise schedule. This is a unique noise schedule that completely removes the dependency between noise variables and the generated samples, resulting in provable convergence to the tilted distribution. This allows us to fine-tune dynamical generative models in full generality, including being the first to fine-tune noiseless Flow Matching models.

(iii) We also propose a new method for solving SOC problems, called Adjoint Matching, which combines the scalability of gradient-based methods and the simplicity of a least-squares regression objective. This is orthogonal to the reward fine-tuning application and can be applied to general SOC problems.

(iv) We perform extensive comparisons to baseline approaches, and analyze them from multiple perspectives such as realism, consistency, and diversity. We find that our proposed method provides generalization to unseen human preference reward models, better text-to-sample consistency, and retains good diversity.

In the following, sections are broken down as follows: Section 2 summarizes the algorithms used for sampling from pre-trained Flow Matching and diffusion models, while Section 3 provides a common notation that we will use throughout. Sections 4 and 5 form the core of our contributions. Section 4 details the value function bias problem and our proposed solution via the memoryless noise schedule. Section 5 details the new Adjoint Matching algorithm for solving SOC problems.

## Preliminaries on dynamical generative models

We are interested in fine-tuning base generative models p base (X 1 ) where samples are generated through the simulation of a stochastic process. That is, these models transform noise variables into a sample through an iterative process. In particular, we discuss the specific constructions and sampling processes of Flow Matching [(Lipman et al., 2023;](#b46)[Liu et al., 2023;](#b49)[Liu, 2022;](#b48)[Albergo and Vanden-Eijnden, 2023)](#) and Denoising Diffusion Models [(Ho et al., 2020;](#b36)[Song et al., 2021b,a)](#). The goal of this section is to provide background information on these methods, which we will later unify into a single consistent notation in Section 3.

Given random variables from an initial distribution X0 ∼ p 0 = N (0, I), and X1 which are distributed according to some data distribution, we define the reference flow X = ( Xt ) t∈ [[0,](#)[1]](#) where

$Xt = β t X0 + α t X1 ,(2)$where (α t ) t∈[0,1] , (β t ) t∈[0,1] are functions such that α 0 = β 1 = 0 and α 1 = β 0 = 1. Diffusion models and Flow Matching construct generative Markov processes X t with initial distribution X 0 ∼ N (0, I) that result in flows X = (X t ) t∈[0,1] with the same time marginals as the reference flow X, i.e., the random variables X t and Xt have identical distribution for all times t ∈ [0, 1]. This implies X 1 has the same distribution as the data distribution, so simulating the Markov process from random noise X 0 is a way to generate artificial samples 1 .

## Flow Matching

In its simplest form, the generative Markov process of a Flow Matching model is an ordinary differential equation (ODE) of the form:

$dX t = v(X t , t) dt, X 0 ∼ N (0, I).(3)$where v(X t , t) is a parametric velocity that is optimized to match the derivative of the reference flow, i.e., v(X t , t) = argmin v E v( Xt , t)-d dt Xt 2 (see e.g. [Lipman et al. (2023)](#b46) for details on pre-training Flow Matching models). It can then be proven that the solution of the generative process (3) has the same time marginals as the reference flow [(Lipman et al., 2023;](#b46)[Liu, 2022;](#b48)[Albergo and Vanden-Eijnden, 2023)](#), and a commonly used choice is α t = t and β t = 1 -t. One can also consider a family of stochastic differential equations (SDEs) with an arbitrary state-independent diffusion coefficient 2 :

$dX t = v(X t , t) + σ(t) 2 2βt( αt α t βt-βt) v(X t , t) -αt αt X t dt + σ(t) dB t , X 0 ∼ N (0, I),(4)$where (B t ) t≥0 is a Brownian motion. The generative processes in (3) and ( [4](#formula_3)) have the same time marginals. This can be seen by writing down the Fokker-Planck equations for (3) and (4), and observing that they are the same up to a cancellation of terms [(Maoutsa et al., 2020)](#b52). The diffusion coefficient σ(t) in ( [4](#formula_3)) is compensated by the second term in the drift which scales proportionally as σ(t) 2 .

## Denoising Diffusion Models

We next discuss diffusion models, in particular the sampling scheme proposed by Denoising Diffusion Implicit Model (DDIM; [Song et al. (2021a)](#)) which we will later relate to Denoising Diffusion Probabilistic Models (DDPM; [Ho et al. (2020)](#b36)) as a particular case of the former. For sampling from a diffusion model, the DDIM update rule 3 [(Song et al. (2021a)](#), Eq. 12), typically stated in discrete time with k ∈ {0, . . . , K}, is:

$X k+1 = √ ᾱk+1 X k - √ 1-ᾱk ϵ(X k ,k) √ ᾱk + 1 -ᾱk+1 -σ 2 k ϵ(X k , k) + σ k ε k , ε k ∼ N (0, I), X 0 ∼ N (0, I), (5$$)$where ᾱk is an increasing sequence such that ᾱ0 = 0, ᾱK = 1, and the sequence σ k is arbitrary. That is, one samples an initial Gaussian random variable x 0 , and applies the stochastic update (5) iteratively K times in order to obtain an artificial sample X K . Updates can be interpreted as progressively denoising the iterate:

x 0 is completely noisy and x K is fully denoised. The noise predictor model ϵ(x k , k) is trained to predict the noise of x k (see e.g. [Ho et al. (2020)](#b36) for details on pre-training denoising diffusion models).

## Flow Matching and diffusion models from a common perspective

We formulate Flow Matching and diffusion models in a unified framework, which we will later use throughout the paper. Firstly, to simplify notation, we will be using continuous-time formulations. This will also directly enable fine-tuning methods inspired by the continuous-time paradigm, which we find tends to perform better than discrete-time counterparts in our empirical validations. Secondly, by consolidating notation, we will be able to discuss fine-tuning of dynamical generative models that follow the same time marginals as the reference flow (2), pre-trained with either the Denoising Diffusion or Flow Matching framework, in full generality.

To convert DDIM to a continuous-time stochastic process, we can show that the DDIM update rule (5), up to a first-order approximation, is equivalent to the Euler-Maruyama discretization of the following SDE:

$dX t = αt 2 ᾱt X t -αt 2 ᾱt + σ(t) 2 2 ϵ base (Xt,t) √ 1-ᾱt dt + σ(t)dB t , X 0 ∼ N (0, I).(6)$See Appendix B.1 for the full derivation. To go from ( [5](#formula_4)) to ( [6](#formula_6)), we assumed a uniform discretization of time, i.e. t = k K . This results in identifying the discrete-time process (X k ) k∈{0,...,K} with a continuous-time process (X t ) t∈[0,1] , where ᾱk := ᾱt , σ k := 1 √ K σ(t), and ϵ(X k , k) with ϵ base (X k , t). In relation to the reference flow (2), the generative process in (6) has the same time marginals when α t = √ ᾱt and β t = √ 1 -ᾱt [(Ho et al., 2020)](#b36).

Furthermore, when viewed up to first order approximations, the DDPM sampling scheme [(Ho et al. (2020)](#b36); Algorithm 2) can be seen as special instance of the DDIM sampling scheme when σ(t) = αt /ᾱt. This results in the following generative process:

$dX t = αt 2 ᾱt X t -αt ᾱt ϵ base (Xt,t) √ 1-ᾱt dt + αt ᾱt dB t , X 0 ∼ N (0, I),(7)$We can further consolidate notation by converting all quantities to the score function s(x, t)-defined as the gradient of the log density of the random variable X t -which is possible when X 0 is Normal-distributed and under the affine reference flow (2). In particular, the velocity v base from Flow Matching can be expressed in terms of the score function (see Appendix B.4):

$v base (x, t) = αt αt x + β t ( αt αt β t -βt )s(x, t). (8$$)$And the noise predictor ϵ base also admits an expression in terms of the score function (see Appendix B.3):

$s(x, t) = -ϵ base (x,t) √ 1-ᾱt .(9)$Plugging these two equations into (4) and ( [6](#formula_6)), respectively, and rewriting them in terms of only the α t and β t in (2), we can unify both the Flow Matching and continuous-time DDIM generative processes as:

$dX t = b(X t , t) dt + σ(t) dB t , X 0 ∼ N (0, I),(10)$where b(x, t)

$= κ t x + σ(t) 2 2 + η t s(x, t), κ t = αt αt , η t = β t ( αt αt β t -βt )(11)$where (α t , β t ) are coefficients of the reference flow (2). We have hence expressed the generative process of a base model, whether it is a Flow Matching or a diffusion model, as an SDE of the form ( [10](#formula_11))-( [11](#formula_12)), unified by the choice of reference flow. This expression has been written before for DDIM, e.g. [Bartosh et al. (2024a,b)](#).

4 Fine-tuning as "memoryless" stochastic optimal control

We now discuss the crux of the problem: how to produce a fine-tuned generative model that produces samples X 1 which follow the tilted distribution involving a reward model (1). An obvious direction is to construct a fine-tuning objective involving both the base generative model and the reward model, where the optimal solution results in a fine-tuned generative model for the tilted distribution. However, as we will explain, this turns out to be non-trivial, because a naïve formulation will introduce bias into the solution.

In Section 4.1, we discuss the problem formulation of stochastic optimal control, a general framework for optimizing SDEs, and its relation to the maximum entropy reinforcement learning framework commonly used for RLHF fine-tuning. Next, in Section 4.2, we discuss the initial value function bias problem which plagues existing approaches and so far has seen no simple solution. Finally, in Section 4.3, we propose a novel simple solution that circumvents the bias problem, by enforcing a particular diffusion coefficient, the memoryless noise schedule, to be used during fine-tuning. This results in an extremely simple fine-tuning objective that provably converges to a model which generates the tilted distribution (1) without any statistical bias.

## Preliminaries on the stochastic optimal control problem formulation

Stochastic optimal control (SOC; [Bellman (1957)](#b7); [Fleming and Rishel (2012)](#b27); [Sethi (2018)](#b69)) considers general optimization problems over stochastic differential equations, but we only need to consider a common instantiation, the quadratic cost control-affine problem formulation:

$min u∈U E 1 0 1 2 ∥u(X u t , t)∥ 2 + f (X u t , t) dt + g(X u 1 ) ,(12)$$s.t. dX u t = b(X u t , t) + σ(t)u(X u t , t) dt + σ(t)dB t , X u 0 ∼ p 0 (13$$)$where in (13), X u t ∈ R d is the state of the stochastic process, u : R d × [0, 1] → R d is commonly referred to as the control vector field, b : R d × [0, 1] → R d is a base drift, and σ : [0, 1] → R d×d is the diffusion coefficient. These jointly define the controlled process X u ∼ p u that we are interested in optimizing; often both b and σ are fixed and we only optimize over the control u.

As part of the objective functional (12), we have an affine control cost 1 2 ∥u(X u t , t)∥ 2 , a running state cost f : R d × [0, 1] → R and a terminal state cost g : R d → R.

The stochastic optimal control (SOC) objective ( [12](#formula_13)) can be decomposed recursively from the final time value. It is common to define the cost functional which is the expected future cost starting from state x at time t:

$J(u; x, t) := E X∼p u 1 t 1 2 ∥u(X s , s)∥ 2 + f (X s , s) ds + g(X 1 ) X t = x .(14)$From here, the value function is the optimal value of the cost functional[foot_0](#foot_0) :

$V (x, t) := min u∈U J(u; x, t) = J(u * ; x, t),(15)$where u * is the optimal control, i.e., minimizer of (12). Furthermore, a classical result is that the value function can be expressed in terms of the uncontrolled base process p base [(Kappen (2005)](#b39), see Domingo-Enrich et al. 2023, Eq. 8, App. B for a self-contained proof):

$V (x, t) = -log E X∼p base exp(- 1 t f (X s , s)ds -g(X 1 )) X t = x .(16)$A useful expression for the optimal control (which we will make use of in deriving the Adjoint Matching objective in Section 5) is that it is related to the gradient of the value function:

$u * (x, t) = -σ(t) ⊤ ∇ x V (x, t) = -σ(t) ⊤ ∇ x J(u * , x, t). (17$$)$Relation to MaxEnt RL. Stochastic optimal control with the control-affine formulation (12) is the continuoustime equivalence of maximum entropy reinforcement learning (MaxEnt RL; [Todorov (2006)](#b78); [Ziebart et al. (2008)](#b96)) with a KL regularization instead of only an entropy regularization. In particular, by the Girsanov theorem (Theorem 2), the affine control cost is equivalent to a Kullback-Leibler (KL) divergence between the base process p base , when u = 0, and the controlled process p u , when conditioned on the same initial state X 0 (see Appendix C.4):

$D KL p u (X|X 0 ) p base (X|X 0 ) = E X u ∼p u 1 0 1 2 ∥u(X u t , t)∥ 2 dt ,(18)$resulting in the KL-regularized RL interpretation of (12):

$max u∈U E X0∼p0 E X∼p u (•|X0) 1 0 -f (X u t , t)dt -g(X u 1 ) -D KL (p u (X|X 0 ) ∥ p base (X|X 0 )) ,(19)$where the negative state costs correspond to intermediate and terminal rewards in the RL interpretation. The KL divergence incentivizes the optimal solution to stay close to the distribution of the base process.

## The initial value function bias problem

We next discuss why naïvely adding a KL regularization does not lead to the tilted distribution (1). From ( [19](#formula_22)), we can also show that the optimal distribution conditioned on X 0 is[foot_1](#foot_1)

$p * (X|X 0 ) ∝ p base (X|X 0 ) exp - 1 0 f (X t , t) dt -g(X 1 ) . (20$$)$This is analogous to the exponentiated reward distribution in MaxEnt RL [(Rawlik et al., 2013)](#b62), but since we generalize the entropy regularization to a KL regularization, p base acts as a prior distribution.

In order to relate this to the tilted distribution (1) that we want to achieve for fine-tuning, first notice that the normalization constant of the right-hand side (RHS) of ( [20](#formula_23)) is exactly the value function at t = 0:

$E X∼p base (X|X0) exp - 1 0 f (X t , t) dt -g(X 1 ) = exp -V (X 0 , 0) ,(21)$where the equality is due to (16). Dividing the RHS of ( [20](#formula_23)) by ( [21](#formula_25)) and multiplying by p 0 (X 0 ), we obtain the normalized distribution over the full path X,

$p * (X) = p base (X) exp - 1 0 f (X t , t) dt -g(X 1 ) + V (X 0 , 0) . (22$$)$Setting f = 0 and g = -r, we arrive at an expression for the optimal distribution

$p * (X 0 , X 1 ) = p base (X 0 , X 1 ) exp r(X 1 ) + V (X 0 , 0) . (23$$)$This unfortunately does not lead to the tilted distribution (1) because we have a bias in the optimal distribution that is due to the value function of the initial distribution V (X 0 , 0). That is to say, naïvely adding a KL regularization (18) to the fine-tuning objective in the sense of ( [19](#formula_22)) leads to a biased distribution ( [22](#formula_26)) after fine-tuning and is not equivalent to the tilted distribution (1). For instance, when the sampling procedure is noiseless, i.e., σ(t) = 0, fine-tuning naïvely will not have any effect because X 0 completely determines X 1 . This is unlike the situation for large language models [(Ouyang et al., 2022;](#b56)[Rafailov et al., 2023)](#b61), where there is no dynamical process that samples X 1 iteratively and hence no dependence on the initial noise variable X 0 . Although this KL regularization is a common objective for RLHF of large language models, it has seen seldom use in fine-tuning diffusion models, likely due to this issue of the initial value function bias.

In the context of diffusion models, KL regularization (19) has been explored in prior works [(Fan et al., 2024)](#b26), but its behavior was not well-understood and they did not relate the fine-tuned model to the tilted distribution (1). Another direction that has been proposed is to learn the initial distribution p 0 to cancel out the bias [(Uehara et al., 2024b;](#)[Tang, 2024)](#b77) but this simply shifts the work into tilting the initial distribution and requires an auxiliary model for parameterizing the optimal initial distribution. In contrast, we show in the next section that it is possible to remove the value function bias by simply choosing a very particular noise schedule during the fine-tuning procedure.

## The memoryless noise schedule for fine-tuning dynamical generative models

In this section, we propose a very simple method of turning (23) into the tilted distribution (1) through the use of a particular memoryless noise schedule. Throughout, we provide an intuitive explanation of why this noise schedule is sufficient for fine-tuning while discussing the full theoretical result where we show that the memoryless noise schedule is actually not only sufficient but also necessary.

Intuitively, the main reason we cannot arrive at the tilted distribution from ( [23](#formula_28)) is due to the p base (X 0 , X 1 ) distribution not factoring into X 0 and X 1 . Hence, we define a memoryless generative process as follows:

Definition 1 (Memoryless generative process). A generative process of the form (10)-( [11](#formula_12)) is memoryless if X 0 and X 1 are independent, i.e., p base (X 0 , X 1 ) = p base (X 0 )p base (X 1 ). 

$(t) = √ 2ηt$, the generative process is memoryless, i.e., samples X1 will be independent of the initial noise X0.

When the base generative process is memoryless, this implies:

$p * (X 1 ) = p base (X 0 )p base (X 1 ) exp(r(X 1 ) + V (X 0 , 0))dX 0 ∝ p base (X 1 ) exp(r(X 1 )). (24$$)$That is, solving the SOC problem ( [12](#formula_13))-( [13](#formula_14)) with a memoryless base model will result in a fine-tuned model that generates samples p * (X 1 ) according to the tilted distribution (1). This memoryless property is not satisfied generally by the family of generative processes captured by ( [12](#formula_13))-( [13](#formula_14)). For instance, the Flow Matching and DDIM generative processes with zero diffusion coefficient (i.e., σ(t) = 0) are definitely not memoryless due to X 0 and X 1 being theoretically invertible. Below, we provide the sufficient and neccessary condition for the noise schedule in order to have a memoryless generative process.

Proposition 1 (Memoryless noise schedules). Within the family of generative processes (10)-( [11](#formula_12)), a generative process is memoryless if and only if the noise schedule is chosen as:

$σ(t) 2 = 2η t + χ(t), where χ : [0, 1] → R is s.t. ∀t ∈ (0, 1], lim t ′ →0 + α t ′ exp - t t ′ χ(s) 2β 2 s ds = 0. (25$$)$where η t is the coefficient defined in (11) (see also Table [1](#tab_0)). In particular, we refer to σ(t) = √ 2η t as the memoryless noise schedule.

Due to the endpoint constraints of (α t , β t ) for the reference flow (2), the memoryless noise schedule σ(t) is infinite at t = 0 and approaches zero at t = 1. This provides a way for the generative process to mix when close to noise X 0 while stay steadying when close to the sample X 1 . Hence, the sample will have no information about X 0 due to the enormous amount of mixing with a large diffusion coefficient. Furthermore, while we have intuitively justified the memoryless noise schedule through its independence property, our theoretical result is actually even stronger: all generative models of the form (10)-( [11](#formula_12)) must be fine-tuned using the memoryless noise schedule. We formalize this in the following theorem, which we prove in Appendix D.2:

Theorem 1 (Fine-tuning recipe for general noise schedule sampling). Within the family of generative processes (10)-( [11](#formula_12)), in order to allow the use of arbitrary noise schedules and still generate samples according to the tilted distribution (1), the fine-tuning problem (12)-( [13](#formula_14)) with f = 0 and g = -r must be done with the memoryless noise schedule σ(t) = √ 2η t .

Theorem 1 states that we need to use the memoryless noise schedule for fine-tuning with the SOC objectiveor equivalently, the KL regularized reward objective (19). This is the only noise schedule that retains the relationship between the velocity and score function, allowing the conversion to arbitrary noise schedules (e.g., σ(t) = 0) after fine-tuning. It is worth noting that when using the memoryless noise schedule for DDIM, this recovers what we derived as the continuous-time limit of the DDPM generative process (7). However, the DDPM sampler [(Ho et al., 2020)](#b36) is not commonly used while the DDIM sampler [(Song et al., 2021a)](#) and Flow Matching models typically generate samples using σ(t) = 0, so an explicit conversion to the memoryless noise schedule is necessary for fine-tuning. To the best of our knowledge, we are not aware of any existing works that have proposed a time-varying diffusion coefficient with theoretical guarantees. Table [1](#tab_0) summarizes the memoryless schedule for diffusion and Flow Matching models, which we refer to as Memoryless Flow Matching. In Figure [2](#), we visualize fine-tuning a 1D model, where we see that constant σ(t) leads to biased distributions whereas the memoryless noise schedule perfectly converges to the tilted distribution (1). For convenience, we plug the memoryless noise schedule into the controlled process for fine-tuning (13), and express them in terms of each respective framework. Let ϵ base , v base denote the pre-trained vector fields and ϵ finetune , v finetune the fine-tuned vector fields.

Then we have the following expressions for the full drift b(x, t) + σ(t)u(x, t) and control u(x, t) when σ(t) = √ 2η t : DDIM / DDPM : b(x, t) + σ(t)u(x, t) = αt 2 ᾱt x -αt ᾱt ϵ finetune (x,t) √ 1-ᾱt , u(x, t) = -αt ᾱt(1-ᾱt) (ϵ finetune (x, t) -ϵ base (x, t)). (26) Memoryless Flow Matching: b(x, t) + σ(t)u(x, t) = 2v finetune (x, t) -αt αt x, u(x, t) = 2 βt( αt α t βt-βt)

$(v finetune (x, t) -v base (x, t)). (27)$Thus, to solve the SOC problem (12)-( [13](#formula_14)) in practice, we parameterize the control u in terms of ϵ finetune or v finetune and optimize these vector fields instead. After plugging in ( [26](#))-( [27](#)), the SOC problem ( [12](#formula_13))-( [13](#formula_14)) can then be solved using any SOC algorithm in order to perform fine-tuning, and we proposed an especially effective algorithm next in Section 5. After fine-tuning, ϵ finetune and v finetune can simply be plugged back into their respective generative processes (3)-( [7](#formula_7)) to sample from the tilted distribution (1) using any choice of diffusion coefficient.

## Adjoint Matching for control-affine stochastic optimal control

We discuss existing methods and also propose a new method for optimizing control-affine SOC problems. The new Adjoint Matching method is a combination of the time-tested continuous adjoint method [(Pontryagin, 1962)](#b59) with recent developments on constructing least-squares objectives for solving SOC problems [(Domingo-Enrich et al., 2023)](#b21). In this section, we briefly discuss preliminaries on existing methods, their pros and cons, then detail the Adjoint Matching algorithm and its surprising connections to the prior methods. For numerical optimization, we now assume that the control u is a parametric model with parameters θ.

## Existing methods for stochastic optimal control

## The adjoint method

The most basic method of optimizing the simulation of an SDE is to directly differentiate through the simulation using gradients from the SOC objective function [(Han and E, 2016)](#b32). The adjoint method simply uses the objective:

$L(u; X) := 1 0 1 2 ∥u(X t , t)∥ 2 +f (X t , t) dt+g(X 1 ), X ∼ p u . (28$$)$This is a stochastic estimate of the control objective in (12), and the goal is to take compute the gradient of L(u; X) with respect to the parameters θ of the control u. Due to the continuous-time nature of SDEs, there are two main approaches to implementing this numerically. Firstly, the Discrete Adjoint method uses a "discretize-then-differentiate" approach, where the numerical solver for simulating the SDE is simply stored in memory then differentiated through, and it has been studied extensively (e.g., [Bierkens and Kappen (2014)](#b10); [Gómez et al. (2014)](#b29); [Hartmann and Schütte (2012)](#b33); [Kappen et al. (2012)](#b40); [Rawlik et al. (2013)](#b62); [Haber and Ruthotto (2017)](#b31)). This approach, however, uses an extremely large amount of memory as the full computational graph of the numerical solver must be stored in memory and implementations often must rely on gradient checkpointing [(Chen et al., 2016)](#b15) to reduce memory usage.

Secondly, the Continuous Adjoint method exploits the continuous-time nature of SDEs and uses an analytical expression for the gradient of the control objective with respect to the intermediate states X t , expressed as an adjoint ODE, and then applies a numerical method to simulate this gradient itself, hence it is referred to as a "differentiate-then-discretize" approach [(Pontryagin, 1962;](#b59)[Chen et al., 2018;](#b13)[Li et al., 2020)](#b45). We first define the adjoint state as:

$a(t; X, u) := ∇ Xt 1 t 1 2 ∥u(X t ′ , t ′ )∥ 2 +f (X t ′ , t ′ ) dt ′ +g(X 1 ) , where X solves dX t = b(X t , t) + σ(t)u(X t , t) dt + σ(t)dB t .(29)$This implies that E X∼p u a(t; X, u) | X t = x = ∇ x J(u; x, t), where J denotes the cost functional defined in (14). It can then be shown that this adjoint state satisfies[foot_2](#foot_2) :

$d dt a(t; X, u) = -a(t; X, u) T ∇ Xt (b(X t , t) + σ(t)u(X t , t)) + ∇ Xt f (X t , t) + 1 2 ∥u(X t , t)∥ 2 ,(30)$a(1; X, u) = ∇g(X 1 ).

The adjoint state is solved backwards in time, starting from the terminal condition (31). Computation of (30) can be efficiently done as a vector-Jacobian product on automatic differentiation software [(Paszke et al., 2019)](#b57). Once the adjoint state has been solved for t ∈ [0, 1], then the gradient of L(u; X) with respect to the parameters θ can be obtained by integrating over the entire time interval:

$dL dθ = 1 2 1 0 ∂ ∂θ ∥u(X t , t)∥ 2 dt + 1 0 ∂u(Xt,t) ∂θ T σ(t) T a(t; X, u)dt,(32)$where the first term is the partial derivative of L w.r.t. θ and the second term is the partial derivative through the sample trajectory X. See Proposition 6 in Appendix E.1 for a statement and proof of this result. The discrete and continuous adjoint methods converge to the same gradient as the step size of the numerical solvers go to zero. Both are scalable to high dimensions and have seen their fair share of usage in optimizing neural ODE/SDEs [(Chen et al., 2018](#b13)[(Chen et al., , 2021;;](#b14)[Li et al., 2020)](#b45). As the adjoint methods are essentially gradient-based optimization algorithms applied on a highly non-convex problem, many have also reported they can be unstable empirically [(Mohamed et al., 2020;](#b53)[Suh et al., 2022;](#b76)[Domingo-Enrich et al., 2023)](#b21).

## Importance-weighted matching objectives for regressing onto the optimal control

An alternative is to consider regressing onto the optimal control u * , which is the approach of the cross-entropy method [(Rubinstein and Kroese, 2013;](#b67)[Zhang et al., 2014)](#b94) and stochastic optimal control matching (SOCM; Domingo-Enrich et al. ( [2023](#))). These methods make use of path integral theory [(Kappen, 2005)](#b39) to express the optimal control through importance sampling, resulting in an importance-weighted least-squares objective function

$L SOCM (u; X) := 1 0 ∥u(X t , t) -û * (X t , t)∥ 2 dt × ω(u, X), X ∼ p u , (33$$)$where ω is an importance weighting that approximates sampling from the optimal distribution p * , and û * is a stochastic estimator of the optimal control relying on having sampled from the optimal process. We defer to [Domingo-Enrich et al. (2023)](#b21) for the exact details. The functional landscape of this objective is convex, which is argued to help yield stable training. However, the need for importance sampling renders this impractical for high dimensional applications: the variance of the importance weighting ω grows exponentially with dimension of the stochastic process, leading to catastrophic failure. This unfortunately means that such importance-weighted matching objectives are impractical for fine-tuning dynamical generative models; however, a least-squares objective is greatly coveted as it can lead to stable training and simple interpretations.

## Adjoint Matching

We make two important observations which lead to our proposed method: (i ) it is possible to construct a matching objective without any importance weighting, and (ii ) there are unnecessary terms in the adjoint differential equation ( [30](#formula_39)) that can lead to higher variance at convergence.

Firstly, we notice that we can simply match the gradient of the cost functional under the current control.

That is, while SOCM carefully constructs an importance-weighted estimator of the optimal control u * = -σ(t) T ∇J(u * ; x, t) (17), we claim that we can actually just regress onto the target vector field -σ(t) T ∇J(u; x, t) where u is the current control, and furthermore, this results in a gradient equal in expectation to the continuous adjoint method. We formalize this in the following proposition, proven in Appendix E.2:

Proposition 2. Let us define, for now, the basic Adjoint Matching objective as:

$L Basic-Adj-Match (u; X) := 1 2 1 0 u(X t , t) + σ(t) T a(t; X, ū) 2 dt, X ∼ p ū, ū = stopgrad(u),(34)$where ū = stopgrad(u) means that the gradients of ū with respect to the parameters θ of the control u are artificially set to zero. The gradient of L Basic-Adj-Match (u; X) with respect to θ is equal to the gradient dL dθ in equation (32). Importantly, the only critical point of E L Basic-Adj-Match is the optimal control u * .

Critical points of L are controls u such that δ δu L(u) = 0, where δ δu L denotes the first variation of the functional L. In other words, Proposition 2 states that the only control that satisfies the first-order optimality condition for the basic Adjoint Matching objective is the optimal control, which provides theoretical grounding for gradient-based optimization algorithms.

An intuitive way to understand the basic Adjoint Matching objective is that it is a consistency loss. The Adjoint Matching objective is based off of the observation that the optimal control u * (x, t) is the unique fixed-point of the relation u(x, t) = -σ(t) T ∇ x J(u; x, t) (see Lemma 6 in Appendix E.2) and so we are directly optimizing for a control that fits this relation, while using the adjoint state as a stochastic estimator of ∇ x J(u; x, t) (29).

The basic Adjoint Matching objective in Proposition 2 does not yet yield a novel algorithm for stochastic optimal control, because it produces the same gradient as the continuous adjoint method. This can be seen by taking the gradient w.r.t. θ after expanding the square in (34) and removing terms that do not depend on θ to arrive exactly at the continuous adjoint method (32). However, it provides the means of deriving a simpler leaner objective function.

The "Lean" Adjoint. The minimizer of a least-squares objective is the conditional expectation of the regression target, so for the Adjoint Matching objective, at the optimum we have that

$u * (x, t) = E X∼p * -σ(t) T a(t; X, u * )|X t = x . (35$$)$Multiplying both sides by the Jacobian ∇ x u * (x, t) and re-arranging, we get the relation

$E X∼p * u * (x, t) T ∇ x u * (x, t) + a(t; X, u * ) T σ(t)∇ x u * (x, t) | X t = x = 0. (36$$)$Algorithm 1 Adjoint Matching for fine-tuning Flow Matching models

Input: Pre-trained FM velocity field v base , step size h, number of fine-tuning iterations N . Initialize fine-tuned vector fields: v finetune = v base with parameters θ.

for n ∈ {0, . . . , N -1} do Sample m trajectories X = (Xt) t∈{0,...,1} with memoryless noise schedule σ(t) = 2βt( αt α t βt -βt), e.g.:

$X t+h = Xt + h 2v finetune θ (Xt, t) -αt α t Xt + √ hσ(t)εt, εt ∼ N (0, I), X0 ∼ N (0, I). (40$$)$For each trajectory, solve the lean adjoint ODE (38)-( [39](#formula_56)) backwards in time from t = 1 to 0, e.g.:

$ãt-h = ãt + hã T t ∇X t 2v base (Xt, t) -αt α t Xt , ã1 = -∇X 1 r(X1).(41)$Note that Xt and ãt should be computed without gradients, i.e., Xt = stopgrad(Xt), ãt = stopgrad(ãt).

For each trajectory, compute the Adjoint Matching objective (37):

$L Adj-Match (θ) = t∈{0,...,1-h} 2 σ(t) v finetune θ (Xt, t) -v base (Xt, t) + σ(t)ãt 2 . (42$$)$Compute the gradient ∇ θ L(θ) and update θ using favorite gradient descent algorithm. end Output: Fine-tuned vector field v finetune Notice that the terms inside the expectation in (36) show up as part of the adjoint differential equation ( [30](#formula_39)), which we have now shown to have expectation zero at the optimal solution. Therefore, we motivate the definition of a lean adjoint state ã with the terms in (36) removed. Plugging this lean adjoint back into the least-squares objective, we obtain our final proposed Adjoint Matching objective:

$L Adj-Match (u; X) := 1 2 1 0 u(X t , t) + σ(t) T ã(t; X) 2 dt, X ∼ p ū, ū = stopgrad(u),(37)$where

$d dt ã(t; X) = -(ã(t; X) ⊤ ∇ x b(X t , t) + ∇ x f (X t , t)),(38)$$ã(1; X) = ∇ x g(X 1 ).(39)$Equations ( [38](#formula_55))-( [39](#formula_56)) define the lean adjoint state, and (37) is the complete Adjoint Matching objective. The unique critical point of E[L Adj-Match ] is the optimal control, which we prove relying on Proposition 2 and equation (36) (see Proposition 7 in Appendix E.3).

Compared to the importance sampling methods (Section 5.1.2), Adjoint Matching is a simple least-squares regression objective and has no importance weighting. This allows it to avoid the pitfalls of high variance importance weights and makes it as scalable as the adjoint methods while retaining the interpretation of matching a target vector field.

Compared to the adjoint method (Section 5.1.1), Adjoint Matching produces a different gradient in expectation than the continuous adjoint. This is because the lean adjoint state is not related to the gradient of the cost functional anymore, i.e., (29) is not true, except at the optimum when u = u * . Even at the optimal solution, since Adjoint Matching removes terms that have expectation zero, it can potentially exhibit better convergence and lower variance than the continuous adjoint method. Additionally, computation of the lean adjoint state (38) also exhibits a smaller computational cost due to the removal of the extra terms (no longer need the Jacobian of the control ∇ x u). We provide a rigorous derivation of Adjoint Matching and the above claims in Appendix E.3.

Adjoint Matching can be applied to reward fine-tuning of dynamical generative models through the memoryless SOC formulation discussed in Section 4. We provide pseudo-code for this in Algorithm 1 for Flow Matching models and in Algorithm 2 in Appendix E.4 for denoising diffusion models.

## Related work

Fine-tuning from human feedback. There are two main overarching approaches to RLHF: the reward-based approach [(Ziegler et al., 2020;](#)[Stiennon et al., 2020;](#b75)[Ouyang et al., 2022;](#b56)[Bai et al., 2022)](#b4) and direct preference optimization (DPO; [Rafailov et al. (2023)](#b61)). The reward-based approach [(Ziegler et al., 2020;](#)[Stiennon et al., 2020;](#b75)[Ouyang et al., 2022;](#b56)[Bai et al., 2022)](#b4) consists in learning the reward model r(x) from human preference data, and then solving a maximum entropy RL problem with rewards produced by r(x). DPO merges the two previous steps into one: there is no need to learn r(x) as human preference data is directly used to fine-tune the model. However, DPO is typically only applied with a filtered dataset, and does not work explicitly with a reward model. Furthermore, for flow and diffusion models specifically, it is possible to differentiate the reward function, so there is a larger emphasis on reward-based approaches.

Fine-tuning for diffusion models. Among existing reward-based diffusion fine-tuning methods, Fan and Lee (2023) interpret the denoising process as a multi-step decision-making task and use policy gradient algorithms to fine-tune diffusion samplers. [Black et al. (2024)](#b11) makes use of proximal policy gradients for fine-tuning but this does not make use of the differentiability of the reward model.

Fan et al. (2023) also consider KL-regularized rewards (19) but do not make the critical connection to the tilted distribution (1) that we flesh out in Section 4.2. The fine-tuning algorithms of Xu et al. (2023); Clark et al. (2024) directly take gradients of the reward model and use heuristics to try to stay close to the original base generative model, but their behavior is not well understood and unrelated to the tilted distribution: Xu et al. (2023) takes gradients of the reward applied on the denoised sample at different points in time, and Clark et al. (2024) backpropagates the reward function through all or part of the diffusion trajectory. Finally, Uehara et al.

(2024b) also fine-tune diffusion models with the goal of sampling from the tilted distribution (1), but their approach is much more involved than ours as it requires learning a value function, and solving two stochastic optimal control problems. Additional reward fine-tuning works include [Bruna and Han (2024)](#b12), that provide theoretical guarantees to sample from the tilted distribution when the reward is a quadratic function, and [Zhang et al. (2024)](#b92), that propose a reward fine-tuning algorithm for the GFlowNet architecture.

Inference-time optimization methods. Some have proposed methods that do not update the base model but instead modify the generation process directly. One approach is to add a guidance term to the velocity [(Chung et al., 2022;](#b16)[Song et al., 2023;](#b72)[Pokle et al., 2023)](#b58); however, this is a heuristic and it is not well-understood what particular distribution is being generated. Another approach is to directly optimize the initial noise distribution [(Li, 2021;](#b44)[Wallace et al., 2023b;](#)[Ben-Hamu et al., 2024)](#b8); this is taking an opposite approach to the inital value bias problem than us by moving all of the work into optimizing the initial distribution. A more computationally intensive approach is to perform online estimation of the optimal control, for the purpose of heuristically solving an optimal control problem within the sampling process [(Huang et al., 2024;](#b37)[Rout et al., 2024)](#b66); these approaches aim to solve a separate control problem for each generated sample, instead of performing amortization [(Amos et al., 2023)](#b2) to learn a fine-tuned generative model.

Optimal control in generative modeling. Methods from optimal control have been used to train dynamical generative models parameterized by ODEs [(Chen et al., 2018)](#b13), SDEs [(Li et al., 2020)](#b45), and jump processes [(Chen et al., 2021)](#b14), enabled through the adjoint method. They can be used to train arbitrary generative processes, but for simplified constructions these have fallen in favor of simulation-free matching objectives such as denoising score matching [(Vincent, 2011)](#b84) and Flow Matching [(Lipman et al., 2023)](#b46). The optimal control formalism also has significance in sampling from un-normalized distributions [(Zhang and Chen, 2022;](#b93)[Berner et al., 2023;](#b9)[Vargas et al., 2023](#b83)[Vargas et al., , 2022;;](#b82)[Richter and Berner, 2024;](#b63)[Tzen and Raginsky, 2019)](#b79). The inclusion of a state cost has been used to solve transport problems where intermediate path distributions are of importance [(Liu et al., 2024;](#b47)[Pooladian et al., 2024)](#b60). These collective advances naturally lead to the consideration of the optimal control formalism for reward fine-tuning.

Conditional sampling in inverse problems. [Denker et al. (2024)](#b19) and [Wu et al. (2023a)](#) independently consider a pre-trained diffusion model p(x), and an observation y on the generated sample x, as well as the analytic likelihood p(y|x). Their aim is to sample from the posterior p(x)p(y|x), and their applications include inpainting, class-conditional generation, super-resolution, phase retrieval, non-linear deblurring, computed

Fine-tuning Fine-tuning Sampling ClipScore ↑ PickScore ↑ HPS v2 ↑ DreamSim Method σ(t) σ(t) Diversity ↑ None N/A √ 2ηt 24.15±0.26 17.25±0.06 16.19±0.17 53.60±1.37 (Base model) 0 28.32±0.22 18.15±0.07 17.89±0.16 56.53±1.52 Baselines DRaFT-1 √ 2ηt √ 2ηt 30.18±0.24 19.38±0.08 24.61±0.17 25.54±0.99 0 0 30.95±0.28 19.37±0.06 24.37±0.17 27.39±1.14 DRaFT-40 √ 2ηt √ 2ηt 26.94±0.28 18.34±0.19 19.98±1.02 41.98±2.14 0 0 30.07±0.39 19.45±0.08 24.06±0.24 36.53±1.69 DPO √ 2ηt √ 2ηt 24.11±0.22 17.24±0.06 16.15±0.14 53.27±1.36 0 0 27.77±0.18 17.92±0.07 17.30±0.20 54.11±1.50 ReFL √ 2ηt √ 2ηt 28.59±0.31 18.68±0.10 22.24±0.46 32.71±2.76 0 0 30.06±0.63 19.07±0.21 23.06±0.41 32.69±1.28 Memoryless SOC Cont. Adjoint √ 2ηt √ 2ηt 26.99±0.43 18.33±0.16 20.83±0.63 46.59±1.40 λ = 12500 0 29.49±0.32 18.98±0.16 21.34±0.53 48.41±1.44 Disc. Adjoint √ 2ηt √ 2ηt 28.04±0.57 18.44±0.21 20.04±0.39 54.90±2.03 λ = 12500 0 29.28±0.17 18.82±0.14 19.73±0.17 53.36±2.48 Adj.-Matching √ 2ηt √ 2ηt 30.36±0.22 19.29±0.08 24.12±0.17 40.89±1.50 λ = 1000 0 31.41±0.22 19.57±0.09 23.29±0.18 43.10±1.76 Adj.-Matching √ 2ηt √ 2ηt 30.59±0.40 19.49±0.10 24.85±0.23 37.07±1.47 λ = 2500 0 31.64±0.21 19.71±0.09 24.12±0.27 39.88±1.59 Adj.-Matching √ 2ηt √ 2ηt 30.62±0.30 19.50±0.09 24.95±0.28 34.50±1.33 λ = 12500 0 31.65±0.19 19.76±0.08 24.49±0.27 37.24±1.57

Table [2](#) Evaluation metrics of different fine-tuning methods for text-to-image generation. The second and third columns show the noise schedules σ(t) used for fine-tuning and for sampling: σ(t) = √ 2ηt corresponds to Memoryless Flow Matching, and σ(t) = 0 to the Flow Matching ODE (3). We report standard errors estimated over 3 runs of the fine-tuning algorithm on random sets of 40000 training prompts, each evaluated over a random set of 1000 test prompts.

tomography, and protein design. Their setting reduces to a particular case of our reward fine-tuning framework by setting r(x) = log p(y|x). [Denker et al. (2024)](#b19) formulate an SOC problem, and they solve it via the log-variance loss [(Richter et al. (2020)](#b64); [Nüsken and Richter (2021)](#b55)), and the moment loss [(Nüsken and Richter, 2021)](#b55)[foot_3](#foot_3) , which they refer to as the trajectory balance loss [(Malkin et al., 2023)](#b51). [Wu et al. (2023a)](#) propose Twisted Diffusion Sampler, an algorithm based on Sequential Monte Carlo that uses increased inference-time compute to reduce bias. A third work that also tackles the conditional sampling problem is [Du et al. (2024)](#b22), which use a Lagrangian formulation that they solve approximately using Gaussian paths.

## Experiments

We experimentally validate our proposed method on reward fine-tuning a Flow Matching base model [(Lipman et al., 2023)](#b46). In particular, we use the usual setup of pre-training an autoencoder for 512×512 resolution images, then training a text-conditional Flow Matching model on the latent variables with a U-net architecture [(Long et al., 2015)](#b50), similar to the setup in [Rombach et al. (2022)](#b65). We pre-trained our base model using a dataset of licensed text and image pairs. Then for fine-tuning, we consider the reward function:

$r(x) := λ × RewardModel(x)(43)$corresponding to a scaled version of the reward model, which we take to be ImageReward [(Xu et al., 2023)](#b91). Different values of λ provide different tradeoffs between the KL regularization and the reward model ( [19](#formula_22)). For evaluation and benchmarking purposes, we report metrics that separately quantify text-to-image consistency, human preference, and sample diversity, capturing the tradeoff between each aspect of generative models [(Astolfi et al., 2024)](#b3). For consistency, we make use of the standard ClipScore [(Hessel et al., 2021)](#b34) and PickScore [(Kirstain et al., 2023)](#b42); for generalization to unseen human preferences, we use the HPSv2 model [(Wu et al., 2023b)](#); and for diversity, we compute averages of pairwise distances of the DreamSim features [(Fu et al., 2023)](#b28). More details are provided in Appendix G.4.

As our baselines, we consider the DPO [(Wallace et al., 2023a)](#), ReFL [(Xu et al., 2023)](#b91), and DRaFT-K algorithms [(Clark et al., 2024)](#b17). DPO does not use gradients from the reward function, while ReFL and DRaFT make use of heuristic gradient stopping approaches to stay close to the base generative model. Out of these baseline methods, we find that DRaFT-1 performs the best, so we perform additional ablation experiments comparing to this method. Within the same SOC formulation as our method, we also consider the discrete and continuous adjoint methods. We provide full experimental details in Appendix G; an important implementation detail is that we slightly offset σ(t) in order to avoid division by zero.

Evaluation results. In Table [2](#) we report the evaluation metrics for the baselines as well as our proposed Adjoint Matching approach. We compare each method at roughly the same wall clock time (see the times and number of iterations in Table [4](#), and comments in Appendix G.5). We find that across all metrics, our proposed memoryless SOC formulation outperforms existing baseline methods. The choice of SOC algorithms also obviously favors Adjoint Matching over continuous and discrete adjoint methods, which result in poorer consistency and human preference metrics.

Ablation: base model vs. reward tradeoff. We note that the scaling in front of the reward model λ determines how strongly the we should prefer the reward model over the base model. As such, we see a natural tradeoff curve: higher λ results in better consistency and human preference, but lower diversity in the generated samples. Overall, we find that Adjoint Matching performs stably across all values of λ. Our method of regularizing the fine-tuning procedure through memoryless SOC works much better than baseline methods which often must employ early stopping. We show the qualitative effect of varying λ in Figure [3](#), while for the DRaFT-1 baseline we show the effect of varying the number of fine-tuning iterations.

Ablation: classifier-free guidance. We note that it is possible to apply classifier-free guidance (CFG; Ho and Salimans (2022); Zheng et al. ( [2023](#))) after fine-tuning. We use the formula (1 + w)v(x, t|y) -wv(x, t), where w is the guidance weight, v(x, t|y) is a fine-tuned text-to-image model while v(x, t) is an unconditional image model. This is not principled as only the conditional model is fine-tuned, but generally it is unclear what distribution guided models sample from anyhow. In Figure [5](#fig_2) we show the evaluation metrics with classifier-free guidance applied. Comparing three different guidance weight values, we see a higher weight does improve text-to-image consistency, and to some extent, human preference, but this comes at the cost of being worse in terms of diversity. We show qualitative differences in Figure [4](#).

## Conclusion

We investigate the problem of fine-tuning dynamical generative models such as Flow Matching and propose the use of a stochastic optimal control (SOC) formulation with a memoryless noise schedule. This ensures we converge to the same tilted distribution that the large language modeling literature uses for learning from human feedback. In particular, the memoryless noise schedule corresponds to DDPM sampling for diffusion models and a new Memoryless Flow Matching generative process for flow models. In conjunction, we propose a novel training algorithm for solving stochastic optimal control problems, by casting SOC as a regression problem, which we call the Adjoint Matching objective. Empirically, we find that our memoryless SOC formulation works better than multiple existing works on fine-tuning diffusion models, and our Adjoint Matching algorithm outperforms related gradient-based methods. In summary, we are the first to provide a theoretically-driven algorithm for fine-tuning Flow Matching models, and we find that our approach significantly outperforms baseline methods across multiple axes of evaluation-text-to-image consistency, generalization to unseen human preference, and sample diversity-on large-scale text-to-image generation.   Table 4 Additional metrics for various fine-tuning methods for text-to-image generation, which complement the ones in Table 2 (both tables correspond to the same runs). The second and third columns show the noise schedules σ(t) used for fine-tuning and for inference: σ(t) = √ 2ηt corresponds to Memoryless Flow Matching, and σ(t) = 0 to the Flow Matching ODE (3). w Fine-tuning #iter. Fine-tun. Sampl. ImageReward ↑ ClipScore ↑ PickScore ↑ HPS v2 ↑ DreamSim loss / λ σ(t) σ(t) diversity ↑ 0.0 None N/A N/A √ 2ηt -1.384±0.040 24.15±0.26 17.25±0.06 16.19±0.17 53.60±1.37 0 -0.920±0.042 28.32±0.22 18.15±0.07 17.89±0.16 56.53±1.52 0.0 DRaFT-1 √ 2ηt √ 2ηt 0.913±0.068 29.80±0.22 19.16±0.06 23.63±0.16 35.21±1.93 0 0 0.626±0.195 30.48±0.32 18.91±0.34 21.92±1.63 38.52±2.01 √ 2ηt √ 2ηt 1.204±0.046 29.90±0.43 19.29±0.12 24.40±0.27 28.51±1.68 0 0 1.052±0.088 30.65±0.24 19.27±0.11 23.81±0.44 32.11±2.37 √ 2ηt √ 2ηt 1.307±0.041 29.96±0.22 19.31±0.06 24.42±0.13 26.57±1.32 0 0 1.173±0.058 30.86±0.25 19.37±0.06 24.17±0.23 29.69±1.30 √ 2ηt √ 2ηt 1.357±0.039 30.18±0.24 19.38±0.08 24.61±0.17 25.54±0.99 0 0 1.251±0.040 30.95±0.28 19.37±0.06 24.37±0.17 27.39±1.14 0.0 Adj.-Match. √ 2ηt √ 2ηt 0.550±0.043 30.36±0.22 19.29±0.08 24.12±0.17 40.89±1.50 0 0 0.454±0.055 31.41±0.22 19.57±0.09 23.29±0.18 43.10±1.76 √ 2ηt √ 2ηt 0.755±0.040 30.59±0.40 19.49±0.10 24.85±0.23 37.07±1.47 0 0 0.671±0.047 31.64±0.21 19.71±0.09 24.12±0.27 39.88±1.59 12500 √ 2ηt √ 2ηt 0.882±0.058 30.62±0.30 19.50±0.09 24.95±0.28 34.50±1.33 0 0 0.778±0.050 31.65±0.19 19.76±0.08 24.49±0.27 37.24±1.57 1.0 None N/A N/A √ 2ηt -0.269±0.050 30.41±0.22 18.74±0.07 20.47±0.18 43.82±1.24 0 -0.123±0.041 31.83±0.17 19.28±0.07 20.95±0.16 42.59±1.23 1.0 DRaFT-1 √ 2ηt √ 2ηt 1.123±0.051 32.06±0.19 19.69±0.06 24.56±0.17 28.25±1.55 0 0 0.856±0.167 32.32±0.25 19.38±0.34 22.88±1.54 29.98±1.86 0 0 1.177±0.053 32.36±0.18 19.67±0.08 24.48±0.28 25.09±1.82 0 0 1.255±0.038 32.36±0.19 19.70±0.06 24.64±0.17 23.24±1.19 0 0 1.296±0.033 32.30±0.19 19.68±0.06 24.71±0.14 21.54±0.96 1.0 Adj.-Match. 0 0 0.782±0.044 33.05±0.22 20.20±0.09 24.81±0.18 32.67±1.26 √ 2ηt √ 2ηt 1.027±0.038 32.85±0.21 20.08±0.08 25.88±0.20 29.83±1.00 0 0 0.910±0.040 33.20±0.17 20.29±0.09 25.39±0.24 30.34±1.51 12500 0 0 0.985±0.041 33.10±0.18 20.28±0.08 25.61±0.27 28.86±1.37 4.0 None N/A N/A √ 2ηt 0.277±0.043 32.68±0.18 19.50±0.07 22.29±0.16 35.12±0.92 0 0.209±0.046 32.83±0.17 19.79±0.07 22.30±0.17 32.05±1.05 4.0 DRaFT-1 √ 2ηt √ 2ηt 1.062±0.045 32.29±0.16 19.48±0.06 23.67±0.13 25.03±1.32 0 0 0.604±0.395 31.80±0.86 19.09±0.53 21.69±2.10 25.92±2.57 0 0 1.112±0.046 32.29±0.20 19.34±0.11 23.31±0.22 21.02±1.67 0 0 1.151±0.036 32.31±0.21 19.36±0.06 23.29±0.14 19.53±1.24 0 0 1.172±0.040 32.20±0.22 19.30±0.07 23.20±0.15 18.45±1.06 4.0 Adj.-Match. 0 0 0.852±0.046 33.50±0.22 20.31±0.08 24.97±0.19 25.83±0.82 √ 2ηt √ 2ηt 1.052±0.039 33.51±0.19 20.15±0.07 25.56±0.18 26.21±0.73 0 0 0.942±0.042 33.61±0.19 20.35±0.08 25.34±0.21 24.30±0.86 12500 0 0 1.007±0.052 33.48±0.20 20.29±0.08 25.50±0.29 23.48±0.81   

## A Additional Figures & Tables

## B Results on DDIM and Flow Matching

## B.1 The continuous-time limit of DDIM

The DDIM inference update [(Song et al., 2021a, Eq. 12)](#) is

$x k+1 = √ ᾱk+1 x k - √ 1-ᾱk ϵ(x k ,k) √ ᾱk + 1 -ᾱk+1 -σ 2 k ϵ(x k , k) + σ k ϵ k , x K ∼ N (0, I).(44)$If we let ∆ ᾱk = ᾱk+1 -ᾱk , we have that

$ᾱk+1 ᾱk = ᾱk + ᾱk+1 -ᾱk ᾱk = 1 + ᾱk+1 -ᾱk ᾱk = 1 + ∆ ᾱk ᾱk ≈ 1 + ∆ ᾱk 2 ᾱk ,(45)$where we used the first-order Taylor approximation of √ 1 + x. And

$-ᾱk+1 ᾱk (1 -ᾱk ) + 1 -ᾱk+1 -σ 2 k = -1 + ∆ ᾱk ᾱk (1 -ᾱk ) + 1 -ᾱk+1 -σ 2 k = -1 + ∆ ᾱk ᾱk -ᾱk -∆ ᾱk + 1 -ᾱk+1 -σ 2 k = -1 -ᾱk+1 + ∆ ᾱk ᾱk + 1 -ᾱk+1 -σ 2 k = √ 1 -ᾱk+1 -1 + ∆ ᾱk ᾱk (1-ᾱk+1 ) + 1 - σ 2 k 1-ᾱk+1 ≈ √ 1 -ᾱk+1 -1 + ∆ ᾱk 2 ᾱk (1-ᾱk+1 ) + 1 - σ 2 k 2(1-ᾱk+1 ) = -∆ ᾱk 2 ᾱk + σ 2 k 2 1 √ 1-ᾱk+1 ,(46)$where we used the same first-order Taylor approximation. Thus, up to first-order approximations, ( [44](#formula_58)) is equivalent to

$x k-1 = 1 + ∆ ᾱk 2 ᾱk x k -∆ ᾱk 2 ᾱk + σ 2 k 2 ϵ(x k ,k) √ 1-ᾱk+1 + σ k ϵ k , x K ∼ N (0, I).(47)$If we modify our notation slightly, we can rewrite this as

$X (k+1)h = 1 -h αkh 2 ᾱkh X kh + h αkh 2 ᾱkh -hσ(kh) 2 2 ϵ(X kh ,kh) √ 1-ᾱkh + √ hσ(kh)ϵ k , X 0 ∼ N (0, I). (48$$)$To go from (47) to (48), we introduced a continuous time variable and a stepsize h = 1/K, and we regard the increment hᾱ k as approximately equal to h times the derivative of ᾱ. We also identified σ k with √ hσ(kh), where σ(kh) plays the role of a diffusion coefficient. Note that equation ( [48](#formula_62)) can be reverse-engineered as the Euler-Maruyama discretization of the SDE

$dX t = -αt 2 ᾱt + αt 2 ᾱt -σ(t) 2 2 ϵ(Xt,t) √ 1-ᾱt dt + σ(t)dB t , X 0 ∼ N (0, I).(49)$B.2 Forward and backward stochastic differential equations

$Let (κ t ) t∈[0,1] and (η t ) t∈[0,1] such that ∀t ∈ [0, 1], η t ≥ 0, 1 0 κ 1-s ds = +∞, 2 1 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ = 1. (50$$)$As shown in Table [1](#tab_0), DDIM corresponds to κ t = αt 2 ᾱt , η t = αt 2 ᾱt , and Flow Matching corresponds to κ t = αt αt , η t = β t αt αt β t -βt . Lemma 1 (DDIM and Flow Matching fulfill the conditions (50)). The choices of (κ t ) t∈[0,1] and (η t ) t∈[0,1] for DDIM and Flow Matching fulfill the conditions (50). For DDIM, we have that

$t 0 κ 1-s ds = -1 2 log ᾱ1-t =⇒ 1 0 κ 1-s ds = +∞, 2 t 0 η t ′ exp -2 t t ′ κ s ds dt ′ = 1 -ᾱ1-t =⇒ 2 1 0 η t ′ exp -2 t t ′ κ s ds dt ′ = 1. (51) For Flow Matching, t 0 κ 1-s ds = -log α 1-t =⇒ 1 0 κ 1-s ds = +∞, (52$$)$$2 t 0 η t ′ exp -2 t t ′ κ s ds dt ′ = β 2 1-t =⇒ 2 1 0 η t ′ exp -2 t t ′ κ s ds dt ′ = 1. (53$$)$Forward and backward SDEs Consider the forward and backward SDEs

$d ⃗ X t = -κ 1-t ⃗ X t dt + √ 2η 1-t dB t , ⃗ X 0 ∼ p data , (54$$) dX t = κ t X t + 2η t s(X t , t) dt + √ 2η t dB t , X 0 ∼ N (0, I),(55)$where we let ⃗ p t be the density of ⃗ X t , and we define the score function as s(x, t) := ∇ log ⃗ p 1-t (x). Similarly, we let p t be the density of X t . ⃗ p t and p t solve the Fokker-Planck equations:

$∂ t ⃗ p t = ∇ • κ 1-t x⃗ p t + η 1-t ∆⃗ p t , ⃗ p 0 = p data ,(56)$$∂ t p t = ∇ • -κ t x -2η t ∇ log ⃗ p 1-t (X t ) p t + η t ∆p t , p 0 = N (0, I). (57$$)$Lemma 2 (Solution of the forward SDE). Let (κ t ) t≥0 , (η t ) t≥0 with η t ≥ 0, and (ξ t ) t≥0 be arbitrary. The solution ⃗ X t of the SDE

$d ⃗ X t = -κ 1-t ⃗ X t + ξ t dt + √ 2η 1-t dB t , ⃗ X 0 ∼ p data (58) is ⃗ X t = ⃗ X 0 exp - t 0 κ 1-s ds + t 0 exp - t t ′ κ 1-s ds ξ 1-t ′ dt ′ + t 0 √ 2η 1-t ′ exp - t t ′ κ 1-s ds dB t ′ , (59$$)$which has the same distribution as the random variable

$Xt = ⃗ X 0 exp - t 0 κ 1-s ds + t 0 exp - t t ′ κ 1-s ds ξ 1-t ′ dt ′ + 2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ ϵ, ϵ ∼ N (0, I).(60)$Applying Lemma 2 with ξ t ≡ 0, we obtain that ⃗ p 1 is also the distribution of

$X1 = ⃗ X 0 exp - t 0 κ 1-s ds + 2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ ϵ = ϵ,(61)$where ϵ ∼ N (0, I). The third equality in (61) holds by (50). Hence we obtain that ⃗ p 1 = N (0, I). Note also that

$∂ t ⃗ p 1-t = -∇ • κ t x⃗ p 1-t -η t ∆⃗ p 1-t = -∇ • -κ t x -2η t ∇ log ⃗ p 1-t (x) ⃗ p 1-t + η t ∆⃗ p 1-t(62)$Thus, ⃗ p 1-t is a solution of the backward Fokker-Planck equation ( [57](#formula_74)), which proves the following:

Proposition 3 (Equality of marginal distributions). For any time t ∈ [0, 1], the densities of the solutions ⃗ X t , X t of the forward and backward SDEs are equal up to a time flip:

$p t = ⃗ p 1-t .$Forward and backward SDEs with arbitrary noise schedule Next, we look at the following pair of forwardbackward SDEs:

$d ⃗ X t = -κ 1-t ⃗ X t + σ(1-t) 2 2 -η 1-t s( ⃗ X t , 1 -t) dt + σ(1 -t) dB t , ⃗ X 0 ∼ p data , (63$$)$$dX t = κ t X t + σ(t) 2 2 + η t s(X t , t) dt + σ(t) dB t , X 0 ∼ N (0, I),(64)$Here, the score function s is the same vector field as in (64). Remark that equations ( [54](#formula_71))-( [55](#formula_72)) are a particular case of ( [63](#formula_82))-( [64](#formula_84)) for which σ(t) = √ 2η t . The Fokker-Planck equations for (63)-( [64](#formula_84)) are:

$∂ t ⃗ p t = ∇ • κ 1-t x + -σ(1-t) 2 2 + η 1-t s(X t , t) ⃗ p t + η 1-t ∆⃗ p t , ⃗ p 0 = p data ,(65)$$∂ t p t = ∇ • -κ t x -σ(t) 2 2 + η t s(X t , t) p t + σ(t) 2 2 ∆p t , p 0 = N (0, I). (66$$)$It is straight-forward to see that for any σ, the solutions ⃗ p t and p t of ( [65](#formula_85))-( [66](#formula_86)) are also solutions of ( [56](#formula_73))-( [57](#formula_74)). Hence, the marginals ⃗ X t and X t are equally distributed for all noise schedules σ, and they are equal to each other up to a time flip.

## Equality of distributions over trajectories

The result in Proposition 3 can be made even stronger:

Proposition 4 (Equality of distributions over trajectories). Let ⃗ X, X be the solutions of the SDEs (63)-( [64](#formula_84)) with arbitrary noise schedule. For any sequence of times (t i ) 0≤i≤I , the joint distribution of ( ⃗ X ti ) 0≤i≤I is equal to the joint distribution of (X 1-ti ) 0≤i≤I , or equivalently, that the probability measures ⃗ P, P of the forward and backward processes ⃗ X, X are equal, up to a flip in the time direction.

This result states that sampling trajectories from the backward process is equivalent to sampling them from the forward process and then flipping their order.

## B.2.1 Proof of Lemma 1

As shown in Table [1](#tab_0), DDIM corresponds to κ t = αt 2 ᾱt , η t = αt 2 ᾱt . Thus, η t ≥ 0 because ᾱt is increasing, and

$t 0 κ 1-s ds = t 0 α1-s 2 ᾱ1-s ds = -1 2 t 0 ∂ s log ᾱ1-s ds = -1 2 (log ᾱ1-t -log ᾱ1 ) = -1 2 log ᾱ1-t , =⇒ 1 0 κ 1-s ds = -1 2 log ᾱ0 = +∞ (67) 2 t 0 η t ′ exp -2 t t ′ κ s ds dt ′ = t 0 α1-t ′ ᾱ1-t ′ exp - t t ′ α1-s ᾱ1-s ds dt ′ = t 0 α1-t ′ ᾱ1-t ′ ᾱ1-t ᾱ1-t ′ dt ′ = ᾱ1-t t 0 ∂ t ′ 1 ᾱ1-t ′ dt ′ = ᾱ1-t 1 ᾱ1-t -1 ᾱ1 = 1 -ᾱ1-t , =⇒ 2 1 0 η t ′ exp -2 t t ′ κ s ds dt ′ = 1 -ᾱ0 = 1. (68$$)$where we used that ᾱ1 = 1 and ᾱ0 = 0. And Flow Matching corresponds to κ t = αt αt , η t = β t αt αt β t -βt . We have that η t ≥ 0 because α t is increasing and β t is decreasing, and

$t 0 κ 1-s ds = t 0 α1-s α1-s ds = - t 0 ∂ s log α 1-s ds = -(log α 1-t -log α 1 ) = -log α 1-t , =⇒ 1 0 κ 1-s ds = -log α 0 = +∞,(69)$and

$2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ = 2 t 0 β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ exp -2 t t ′ α1-s α1-s ds dt ′ = 2 t 0 β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ α1-t α 1-t ′ 2 dt ′ ,(70)$To develop the right-hand side, note that by integration by parts,

$t 0 β1-t ′ β 1-t ′ α1-t α 1-t ′ 2 dt ′ = - t 0 ∂ t ′ β 2 1-t ′ 2 α1-t α 1-t ′ 2 dt ′ = - β 2 1-t ′ 2 α1-t α 1-t ′ 2 1 0 + t 0 β 2 1-t ′ 2 ∂ t ′ α1-t α 1-t ′ 2 dt ′ = - β 2 1-t ′ 2 α1-t α 1-t ′ 2 t 0 + t 0 β 2 1-t ′ α 2 1-t α1-t ′ α 3 1-t ′ dt ′ . (71$$)$And if we plug this into the right-hand side of (70), we obtain

$2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ = β 2 1-t ′ α1-t α 1-t ′ 2 t 0 = β 2 1-t -β 2 1 α1-t α1 2 = β 2 1-t ,(72)$=⇒ 2

$1 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ = β 2 1 = 1. (73$$)$where we used that β 1 = 0, α 1 = 1.

## B.2.2 Proof of Lemma 2

We can solve this equation by variation of parameters. To simplify the notation, we replace κ 1-s , η 1-s and ξ 1-s by κ s , η s and ξ

$s . Defining f ( ⃗ X t , t) = ⃗ X t exp t 0 κ 1-s ds , we get that df ( ⃗ X t , t) = κ 1-t ⃗ X t exp t 0 κ 1-s ds dt + exp t 0 κ 1-s ds d ⃗ X t = κ 1-t ⃗ X t exp t 0 κ 1-s ds dt + exp t 0 κ 1-s ds (-κ 1-t ⃗ X t + ξ 1-t ) dt + √ 2η 1-t dB t = exp t 0 κ 1-s ds ξ 1-t dt + √ 2η t exp t 0 κ 1-s ds dB t . (74$$)$Integrating from 0 to t, we get that

$⃗ X t exp t 0 κ 1-s ds = ⃗ X 0 + t 0 exp t ′ 0 κ 1-s ds ξ 1-t ′ dt ′ + t 0 √ 2η 1-t ′ exp t ′ 0 κ 1-s ds dB t ′ , (75) ⇐⇒ ⃗ X t = ⃗ X 0 exp - t 0 κ 1-s ds + t 0 exp - t t ′ κ 1-s ds ξ 1-t ′ dt ′ + t 0 √ 2η 1-t ′ exp - t t ′ κ 1-s ds dB t ′ . (76) Since E t 0 √ 2η 1-t ′ exp - t t ′ κ 1-s ds dB t ′ 2 = 2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ ,(77)$we obtain that

$t 0 √ 2η 1-t ′ exp - t t ′ κ 1-s ds dB t ′ has the same distribution as 2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ ϵ, where ϵ ∼ N (0, 1).$
## B.2.3 Proof of Proposition 4

This is a result that has been used by previous works, e.g. [(De Bortoli et al., 2021, Sec. 2.1)](#), but their derivation lacks rigor as it uses some unexplained approximations. While natural, the result is not common knowledge in the area. We provide a derivation which is still in discrete time, and hence not completely formal, but that corrects the gaps in the proof of De [Bortoli et al. (2021)](#b18).

We introduce the short-hand

$⃗ b(x, t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t s(x, 1 -t), (78$$) b(x, t) = κ t X t + σ(t) 2 2 + η t s(X t , t),(79)$$⃗ σ(t) = σ(1 -t). (80$$) Remark that b(x, t) = -⃗ b(x, 1 -t) + σ(t) 2 s(X t , t).$Suppose that we discretize the forward process ⃗ X using K + 1 equispaced timesteps:

$x k+1 = x k + h ⃗ b(x k , kh) + √ h⃗ σ(kh)ϵ k , with ϵ k ∼ N (0, 1). (81$$)$It is important to remark that x k+1 -x k = O(h 1/2 ). Throughout the proof we will keep track of all terms up to linear order in h, while neglecting terms of order O(h 3/2 ) and higher. The distribution of the discretized forward process is:

$⃗ p(x 0:K ) = ⃗ p 0 (x 0 ) K-1 k=0 ⃗ p k+1|k (x k+1 |x k ), where ⃗ p k+1|k (x k+1 |x k ) = exp - ∥x k+1 -x k -h ⃗ b(x k ,kh)∥ 2 2h⃗ σ(kh) 2 (2πh⃗ σ(kh) 2 ) d/2$(82)

Using telescoping products, we have that

$⃗ p(x 0:K ) = ⃗ p K (x K ) K-1 k=0 ⃗ p k+1|k (x k+1 |x k ) ⃗ p k (x k ) ⃗ p k+1 (x k+1 ) = ⃗ p K (x K ) K-1 k=0 ⃗ p k+1|k (x k+1 |x k ) exp log(⃗ p k (x k )) -log(⃗ p k+1 (x k+1 ))(83)$We can use a discrete time version of Ito's lemma:

$log ⃗ p(x k+1 , (k + 1)h) ≈ log ⃗ p(x k , kh) + h ∂ t log ⃗ p(x k , kh) + ⃗ σ(kh) 2 2 ∆ log ⃗ p(x k , kh)(84)$$+ ⟨∇ log ⃗ p(x k , kh), x k+1 -x k ⟩ + O(h 3/2 ). (85$$)$Using equation ( [81](#formula_105)) and a Taylor approximation, observe that

$⟨∇ log p(x k , kh), x k+1 -x k ⟩ = ⟨∇ log p(x k+1 , (k + 1)h) -∇ 2 log p(x k+1 , (k + 1)h)(x k+1 -x k ), x k+1 -x k ⟩ + O(h 3/2 ) = ⟨∇ log p(x k+1 , (k + 1)h), x k+1 -x k ⟩ -⟨h ⃗ b(x k , kh) + √ h⃗ σ(kh)ϵ k , ∇ 2 log p(x k+1 , (k + 1)h) h ⃗ b(x k , kh) + √ h⃗ σ(kh)ϵ k ⟩ + O(h 3/2 ) = ⟨∇ log p(x k+1 , (k + 1)h), x k+1 -x k ⟩ -h⃗ σ(kh) 2 ∆ log p(x k+1 , (k + 1)h) + O(h 3/2 ). (86$$)$And since ⃗ p satisfies the Fokker-Planck equation

$∂ t ⃗ p t = ∇ • (-⃗ b(x, t) + ⃗ σ(t) 2 2 ∇ log ⃗ p t (x))⃗ p t ,(87)$we have that

$∂ t log ⃗ p t = ∂t⃗ pt ⃗ pt = ∇• (-⃗ b(x,t)+ ⃗ σ(t) 2 2 ∇ log ⃗ pt(x))⃗ pt ⃗ pt = -∇ • ⃗ b(x, t) + ⃗ σ(t) 2 2 ∆ log ⃗ p t (x) + ⟨-⃗ b(x, t) + ⃗ σ(t) 2 2 ∇ log ⃗ p t (x), ∇ log ⃗ p t (x)⟩.(88)$Hence,

$∂ t log p(x k , kh) = ∂ t log p(x k+1 , (k + 1)h) + O(h 1/2 ) = -∇ • ⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k+1)h) 2 2 ∆ log ⃗ p(x k+1 , (k + 1)h) + ⟨-⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k+1)h) 2 2 ∇ log ⃗ p(x k+1 , (k + 1)h), ∇ log ⃗ p(x k+1 , (k + 1)h)⟩ + O(h 1/2 ). (89$$)$If we plug ( [86](#formula_112)) and ( [89](#formula_116)) into ( [84](#formula_109)), we obtain

$log p(x k+1 , (k + 1)h) -log p(x k , kh) = h -∇ • ⃗ b(x k+1 , (k + 1)h)+⟨-⃗ b(x k+1 , (k + 1)h)+ ⃗ σ((k+1)h) 2 2 ∇ log ⃗ p(x k+1 , (k + 1)h), ∇ log ⃗ p(x k+1 , (k + 1)h)⟩ + ⟨∇ log p(x k+1 , (k + 1)h), x k+1 -x k ⟩ + O(h 3/2 ) = ⟨2h⃗ σ(kh) 2 ∇ log p(x k+1 ,(k+1)h),x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)⟩ 2h⃗ σ(kh) 2 + h -∇ • ⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k+1)h) 2 2 ∥∇ log ⃗ p(x k+1 , (k + 1)h)∥ 2 + O(h 3/2 ).(90)$Applying a discrete time version of Ito's lemma again, we have that

$⃗ b(x k , kh) = ⃗ b(x k+1 , (k + 1)h) -h ∂ t ⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k+1)h) 2 2 ∆ ⃗ b(x k+1 , (k + 1)h) + ∇ ⃗ b(x k+1 , (k + 1)h) ⊤ (x k -x k+1 ) + O(h 3/2 ) = ⃗ b(x k+1 , (k + 1)h) + ∇ ⃗ b(x k+1 , (k + 1)h) ⊤ (x k -x k+1 ) + O(h). (91$$)$where ∆ ⃗ b denotes the component-wise Laplacian of ⃗ b. Thus,

$log ⃗ p k+1|k (x k+1 |x k ) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k ,kh)∥ 2 2h⃗ σ(kh) 2 = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h( ⃗ b(x k+1 ,(k+1)h)+∇ ⃗ b(x k+1 ,(k+1)h) ⊤ (x k -x k+1 ))∥ 2 2h⃗ σ(kh) 2 + O(h 3/2 ) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)∥ 2 2h⃗ σ(kh) 2 + ⟨x k+1 -x k ,∇ ⃗ b(x k+1 ,(k+1)h) ⊤ (x k -x k+1 )⟩ ⃗ σ(kh) 2 + O(h 3/2 ) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)∥ 2 h⃗ σ(kh) 2 -h⃗ σ(kh) 2 ⟨ϵ k ,∇ ⃗ b(x k+1 ,(k+1)h) ⊤ ϵ k ⟩ ⃗ σ(kh) 2 + O(h 3/2 ) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)∥ 2 h⃗ σ(kh) 2 -h∆ ⃗ b(x k+1 , (k + 1)h) + O(h 3/2 ) (92)$Combining ( [90](#formula_118)) and ( [92](#)), we obtain that

$log ⃗ p k+1|k (x k+1 |x k ) -log p(x k+1 , (k + 1)h) -log p(x k , kh) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)+h⃗ σ(kh) 2 ∇ log p(x k+1 ,(k+1)h)∥ 2 h⃗ σ(kh) 2 + O(h 3/2 ) = -d 2 log 2πh⃗ σ((k + 1)h) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)+h⃗ σ((k+1)h) 2 ∇ log p(x k+1 ,(k+1)h)∥ 2 h⃗ σ((k+1)h) 2 + O(h 3/2 ). (93$$)$By Bayes rule, and taking the exponential of this equation, we obtain

$⃗ p k+1|k (x k+1 |x k ) := ⃗ p k+1|k (x k+1 |x k ) ⃗ p k (x k ) ⃗ p k+1 (x k+1 ) = exp - ∥x k -x k+1 +h ⃗ b(x k+1 ,(k+1)h)-h⃗ σ((k+1)h) 2 ∇ log p(x k+1 ,(k+1)h)∥ 2 2h⃗ σ((k+1)h) 2 (2πh⃗ σ((k+1)h) 2 ) d/2 + O(h 3/2 ).$(94) Up to the O(h 3/2 ) term, the right-hand side is the conditional Gaussian corresponding to the update

$x k = x k+1 + h -⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k + 1)h) 2 ∇ log p(x k+1 , (k + 1)h) + √ h⃗ σ((k + 1)h)ϵ k+1 , ϵ k+1 ∼ N (0, I). (95$$)$If we define y k = x K-k , and we use that b(x, t) = -⃗ b(x, 1 -t) + ⃗ σ(t) 2 ∇ log p(x, 1 -t), we can rewrite (95) as

$y K-k = y K-k-1 + h -⃗ b(y K-k-1 , (K -k -1)h) + ⃗ σ((K -k -1)h) 2 ∇ log p(y K-k-1 , (K -k -1)h) + √ h⃗ σ((K -k -1)h)ϵ k = y K-k-1 + hb(y K-k-1 , kh) + √ hσ(kh)ϵ K-k-1 , =⇒ y k+1 = y k + hb(y k , kh) + √ hσ(kh)ϵ k . (96$$)$And this is the Euler-Maruyama discretization of the backward process ⃗ X. If we plug ( [94](#)) into ( [83](#formula_108)), we obtain that

$⃗ p(x 0:K ) ≈ ⃗ p K (x K ) K-1 k=0 ⃗ p k+1|k (x k+1 |x k ). (97$$)$which concludes the proof, as ⃗ p K (x K ) is the initial distribution of the backward process, and ⃗ p k+1|k (x k+1 |x k ) are its transition kernels.

## B.3 The relationship between the noise predictor ϵ and the score function

Applying Lemma 2 with the choices of (κ t ) t≥0 and (η t ) t≥0 for DDIM, we obtain that ⃗ X t has the same distribution as

$Xt = √ ᾱ1-t ⃗ X 0 + √ 1 -ᾱ1-t ϵ, ϵ ∼ N (0, 1).(98)$Since ⃗ X t and Xt have the same distribution, predicting the noise of ⃗ X t is equivalent to predicting the noise of Xt . The noise predictor ϵ can be written as:

$ϵ(x, t) := E[ϵ| X1-t = x] = E ϵ| √ ᾱt ⃗ X 0 + √ 1 -ᾱt ϵ = x = E x- √ ᾱt ⃗ X0 √ 1-ᾱt | √ ᾱt ⃗ X 0 + √ 1 -ᾱt ϵ = x(99)$And the score function s(x, t) := ∇ log ⃗ p 1-t (x) admits the expression

$s(x, t) := ∇ log ⃗ p 1-t (x) = ∇⃗ p1-t(x) ⃗ p1-t(x) = ∇E[⃗ p 1-t|0 (x| ⃗ X0)] ⃗ p1-t(x) = E[∇ log ⃗ p 1-t|0 (x| ⃗ X0)⃗ p 1-t|0 (x| ⃗ X0)] ⃗ p1-t(x) ,(100)$where

$⃗ p 1-t|0 (x| ⃗ X 0 ) = exp(-∥x- √ ᾱtY1∥ 2 /(2(1-ᾱt))) (2π(1-ᾱt)) d/2 =⇒ ∇ log ⃗ p t|1 (x|Y 1 ) = -x- √ ᾱtY1 1-ᾱt .(101)$Plugging this into the right-hand side of (100) and using Bayes' rule, we get

$s(x, t) = E -x- √ ᾱt ⃗ X0 1-ᾱt | √ ᾱt ⃗ X 0 + √ 1 -ᾱt ϵ = x .(102)$Comparing the right-hand sides of ( [99](#formula_132)) and ( [102](#formula_135)), we obtain that s(x, t) = -ϵ(x,t) √ 1-ᾱt .

B.4 The relationship between the vector field v and the score function

By construction [(Lipman et al., 2023;](#b46)[Albergo and Vanden-Eijnden, 2023;](#)[Albergo et al., 2023)](#), we have that

$v(x, t) = E[ αt Y 1 + βt Y 0 |x = α t Y 1 + β t Y 0 ] = E[ αt(x-βtY0) αt + βt Y 0 |x = α t Y 1 + β t Y 0 ] = αt αt x + ( βt -αt αt β t )E[Y 0 |x = α t Y 1 + β t Y 0 ],(103)$where we used that Y 1 = (x -β t Y 0 )/α t . Also, we can write the score as follows

$s(x, t) := ∇ log p t (x) = ∇pt(x) pt(x) = ∇E[p t|1 (x|Y1)] pt(x) = E[∇p t|1 (x|Y1)] pt(x) = E[p t|1 (x|Y1)∇ log p t|1 (x|Y1)] pt(x) ,(104)$where

$p t|1 (x|Y 1 ) = exp(-∥x-αtY1∥ 2 /(2β 2 t )) (2πβ 2 t ) d/2 =⇒ ∇ log ⃗ p t|1 (x|Y 1 ) = -x-αtY1 β 2 t (105$$)$Plugging this back into the right-hand side of (104), we obtain

$s(x, t) = - E[p t|1 (x|Y1) x-α t Y 1 β 2 t ] pt(x) = - ⃗ p t|1 (x|Y1)p1(Y1) x-α t Y 1 β 2 t dY1 ⃗ pt(x) = -p 1|t (Y 1 |x) x-αtY1 β 2 t dY 1 = -E[ x-αtY1 β 2 t |x = α t Y 1 + β t Y 0 ] = -E[Y0|x=αtY1+βtY0] βt(106)$The last equality holds because (x -α t Y 1 )/β t = Y 0 . Putting together ( [103](#formula_136)) and ( [106](#formula_140)), we obtain that

$v(x, t) = αt αt x + β t ( αt αt β t -βt )s(x, t) ⇐⇒ s(x, t) = 1 βt( αt α t βt-βt) v(x, t) -αt αt x(107)$Thus, the ODE (3) can be rewritten like this:

$dXt dt = αt αt X t + β t ( αt αt β t -βt )s(X t , t), X 0 ∼ p 0 .(108)$To allow for an arbitrary diffusion coefficient, we need to add a correction term to the drift:

$dX t = αt αt X t + σ(t) 2 2 + β t ( αt αt β t -βt ) s(X t , t) dt + σ(t)dB t , X 0 ∼ p 0 .(109)$This can be easily shown by writing down the Fokker-Planck equations for ( [108](#formula_142)) and ( [109](#formula_143)), and observing that they are the same up to a cancellation of terms. Finally, if we plug the right-hand side of ( [107](#formula_141)) into (109), we obtain the SDE for Flow Matching with arbitrary noise schedule (equation ( [4](#formula_3))).

C Stochastic optimal control as maximum entropy RL in continuous space and time

In this section, we bridge KL-regularized (or MaxEnt) reinforcement learning and stochastic optimal control. We show that when the action space is Euclidean and the transition probabilities are conditional Gaussians, taking the limit in which the stepsize goes to zero on the KL-regularized RL problem gives rise to the SOC problem. A consequence of this connection is that all algorithms for KL-regularized RL admit an analog for diffusion fine-tuning. This is not novel, but it may be useful for researchers that are familiar with RL fine-tuning formulations.

Appendix C.4 is providing a more direct, rigorous, continuous-time connection between SOC and MaxEnt RL, as it shows that the expected control cost is equal to the KL divergence between the distributions over trajectories, conditioned on the starting points (see equation ( [18](#formula_21))).

## C.1 Maximum entropy RL

Several diffusion fine-tuning methods [(Black et al., 2024;](#b11)[Uehara et al., 2024b)](#) are based on KL-regularized RL, also known as maximum entropy RL, which we review in the following. In the classical reinforcement learning (RL) setting, we have an agent that, starting from state s 0 ∼ p 0 , iteratively observes a state s k , takes an action a k according to a policy π(a k ; s k , k) which leads to a new state s k+1 according to a fixed transition probability p(s k+1 |a k , s k ), and obtains rewards r k (s k , a k ). This can be summarized into a trajectory τ = ((s k , a k )) K k=0 . The goal is to optimize the policy π in order to maximize the expected total reward, i.e.

$max π E τ ∼π,p [ K k=0 r k (s k , a k )].$Maximum entropy RL (MaxEnt RL; [Ziebart et al. (2008)](#b96)) amounts to adding the entropy H(π) of the policy π(•; s k , k) to the reward for each step k, in order to encourage exploration and improve robustness to changes in the environment:

$max π E τ ∼π,p [ K k=0 r k (s k , a k ) + K-1 k=0 H(π(•; s k , k))] 8 .$As a generalization, one can regularize using the negative KL divergence between π(•; s k , k) and a base policy π base (•; s k , k):

$max π E τ ∼π,p [ K k=0 r k (s k , a k ) - K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))],(110)$which prevents the learned policy to deviate too much from the base policy. Each policy π induces a distribution q(τ ) over trajectories τ , and the MaxEnt RL problem (110) can be expressed solely in terms of such distributions (Lemma 3 in Appendix C.3):

$max q E τ ∼q [ K k=0 r k (s k , a k )] -KL(q||q base ),(111)$where q base is the distribution induced by the base policy π base , and the maximization is over all distributions q such that their marginal for s 0 is p 0 . We can further recast this problem as (Lemma 4 in Appendix C.3):

$min q KL(q||q * ),$where q * (τ ) := q base (τ ) exp

$K k=0 r k (s k , a k ) -V(s 0 , 0) ,(112)$where

$V(s k , k) := log E τ ∼π base ,p [exp K k ′ =k r k ′ (s k ′ , a k ′ ) |s k ] = max π E τ ∼π,p K k ′ =k r k ′ (s k ′ , a k ′ ) - K-1 k ′ =k KL(π(•; s k ′ , k ′ )||π base (•; s k ′ , k ′ ))|s k (113)$is the value function. Problem (112) directly implies that the distribution induced by the optimal policy π * is the tilted distribution q * (which has initial marginal p 0 ).

## C.2 From maximum entropy RL to stochastic optimal control

The following well-known result, which we prove in Appendix C.3, shows that in a natural sense, the continuous-time continuous-space version of MaxEnt RL is the SOC framework introduced in Section 4.1. In particular, when states and actions are vectors in R d , policies are specified by a vector field u (the control), and transition probabilities are conditional Gaussians, the MaxEnt RL problem becomes an SOC problem when the number of timesteps grows to infinity.

Proposition 5. Suppose that (i) The state space and the action space are R d , (ii) Policies π are specified as π(a k ; s k , k) = δ(a k -u(s k , kh)), where u : R d × [0, T ] → R d is a vector field, and δ denotes the Dirac delta, (iii) Transition probabilities are conditional Gaussian densities:

$p(s k+1 |a k , s k ) = N (s k + h(b(s k , kh) + σ(kh)a k ), hσ(kh)σ(kh) ⊤ )$, where h = T /K is the stepsize, and b and σ are defined as in Section 4.1.

Then, in the limit in which the number of steps K grows to infinity, the problem (110) is equivalent to the SOC problem (12)-( [13](#formula_14)), identifying

$• the sequence of states (s k ) k k=0 with the trajectory X u = (X u t ) t∈[0,1] , • the running reward K-1 k=0 r k (s k , a k ) with the negative running cost - T 0 f (X u t , t) dt, • the terminal reward r K (s K , a K ) with the negative terminal cost -g(X u T ), • the KL regularization E τ ∼π,p [ K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))] with 1 2 times the expected L 2 norm of the control 1 2 E T 0 ∥u(X u t , t)∥ 2 dt ,$• and the value function V(s k , k) defined in (113) with the negative value function -V (x, t) defined in Section 4.1.

A first consequence of this result is that every loss function designed for generic MaxEnt RL problems has a corresponding loss function for SOC problems. The geometric structure of the latter allows for additional losses that do not have an analog in the classical MaxEnt RL setting; in particular, we can differentiate the state and terminal costs.

A second consequence of Proposition 5 is that the characterization (112) can be translated to the SOC setting. The analogs of the distributions q * , q base induced by the optimal policy π * and the base policy π base are the distributions p * , p base induced by the optimal control u * and the null control. For an arbitrary trajectory X = (X t ) t∈[0,T ] , the relation between P * and P base is given by

$dP * dP base (X) = exp(- T 0 f (X t , t) dt -g(X T ) + V (X 0 , 0)) (114$$)$where V is the value function as defined in Section 4.1. Note that this matches the statement in (22).

## C.3 Proof of Proposition 5: from MaxEnt RL to SOC

Since the transition p(s k+1 |a k , s k ) is fixed, for each π we can define

$π(a k , s k+1 ; s k , k) = π(a k ; s k , k)p(s k+1 |a k , s k ) and πbase (a k , s k+1 ; s k , k) = π base (a k ; s k , k)p(s k+1 |a k , s k ),(115)$and reexpress ( [110](#formula_146)) as (see Lemma 3)

$min π E τ ∼π [ K k=0 r k (s k , a k ) - K-1 k=0 KL(π(•, •; s k , k)||π base (•, •; s k , k))].(116)$Using the hypothesis of the proposition, we can write

$π(a k , s k+1 ; s k , k) = δ(a k -u(s k , kη))N (s k + η(b(s k , kη) + σ(kη)a k ), ησ(kη)σ(kη) ⊤ ) = δ(a k -u(s k , kη))π(s k+1 ; s k , k),(117) where π$$(s k+1 ; s k , k) = N (s k + η(b(s k , kη) + σ(kη)u(s k , kη)), ησ(kη)σ(kη) ⊤ ) is the state transition kernel.$We set the base policy as π base (a k ; s k , k) = δ(a k ), and we obtain analogously that π(a k , s k+1 ; s k , k) = δ(a k )π base (s k+1 ; s k , k) with πbase (s k+1 ; s k , k) = N (s k + ηb(s k , kη), ησ(kη)σ(kη) ⊤ ). Now, if we take K large, the trajectory (s k ) K k=0 generated by π can be regarded as the Euler-Maruyama discretization of a solution X u of the controlled SDE (13), while the trajectory generated by πbase is the discretization of the uncontrolled process X 0 obtained by setting u = 0. As a consequence

$lim K→∞ E τ ∼π [ K-1 k=0 KL(π(•, •; s k , k)||π base (•, •; s k , k))] = lim K→∞ E τ ∼π [ K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))] = E X u ∼P u [log dP u dP 0 (X u )],(118)$where P u and P 0 are the measures of the processes X u and X 0 , respectively. The Girsanov theorem (Theorem 2) implies that

$log dP u dP 0 (X u ) = - T 0 ⟨u(X u t , t), dB t ⟩ -1 2 T 0 ∥u(X u t , t)∥ 2 dt, which implies that E X u ∼P u [log dP u dP 0 (X u )] = -1 2 E X u ∼P u [ T 0 ∥u(X u t , t)∥ 2 dt]. Setting the rewards r k (a k , s k ) = ηf (s k , kη) for k ∈ {0, . . . , K -1} and r K (a K , s K ) = ηg(s k )$, where f and g are as in Section 4.1, yields the following limiting object:

$lim K→∞ E τ ∼π [ K k=0 r k (s k , a k )] = E X u ∼P u [ T 0 f (X u t , t) dt + g(X u T )].(119)$Hence, the limit of the MaxEnt RL loss (116) is the SOC loss (12).

Lemma 3. Let π(a k , s k+1 ; s k , k) and πbase (a k , s k+1 ; s k , k) be as defined in (115). KL(π(•,

$•; s k , k)||π base (•, •; s k , k))]$and KL(π(•; s k , k)||π base (•; s k , k))] are equal. Moreover, if q, q base denote the distributions over trajectories induced by π, π base , we have that

$KL(q||q base ) = E[ K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))].(120)$Proof. We have that

$KL(π(•, •; s k , k)||π base (•, •; s k , k))] = a k ,s k+1 π(a k , s k+1 ; s k , k) log π(a k ,s k+1 ;s k ,k) πbase (a k ,s k+1 ;s k ,k) = a k ,s k+1 π(a k ; s k , k)p(s k+1 |a k , s k ) log π(a k ;s k ,k)p(s k+1 |a k ,s k ) π base (a k ;s k ,k)p(s k+1 |a k ,s k ) = a k ,s k+1 π(a k ; s k , k)p(s k+1 |a k , s k ) log π(a k ;s k ,k) π base (a k ;s k ,k) = a k π(a k ; s k , k) s k+1 p(s k+1 |a k , s k ) log π(a k ;s k ,k) π base (a k ;s k ,k) = a k π(a k ; s k , k) log π(a k ;s k ,k) π base (a k ;s k ,k) = KL(π(•; s k , k)||π base (•; s k , k))].(121)$To prove (120), by construction we can write

$q(τ ) = p 0 (s 0 ) K-1 k=0 π(a k , s k+1 ; s k , k), q base (τ ) = p 0 (s 0 ) K-1 k=0 πbase (a k , s k+1 ; s k , k),(122)$which means that

$KL(q||q base ) = E τ ∼q [log q(τ ) q base (τ ) ] = E τ ∼q [ K-1 k=0 log π(a k ,s k+1 ;s k ,k) πbase (a k ,s k+1 ;s k ,k) ] = K-1 k=0 E τ ∼q 0:(k+1) [log π(a k ,s k+1 ;s k ,k) πbase (a k ,s k+1 ;s k ,k) ] = K-1 k=0 E τ ∼q 0:k [ a k ,s k+1 π(a k , s k+1 ; s k , k) log π(a k ,s k+1 ;s k ,k) πbase (a k ,s k+1 ;s k ,k) ] = K-1 k=0 E τ ∼q 0:k [KL(π(•, •; s k , k)||π base (•, •; s k , k))] = K-1 k=0 E τ ∼q 0:k [KL(π(•; s k , k)||π base (•; s k , k))] = E τ ∼q 0:k [ K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))] (123)$Here, the notation q 0:k denotes the trajectory q up to the state s k .

Lemma 4. The distribution-based MaxEnt RL formulation in (111) is equivalent to the the following problem:

$min q KL(q||q * ),$where q * (τ ) :=

$q base (τ ) exp K k=0 r k (s k ,a k ) 1 p 0 (s 0 ) {τ ′ |s ′ 0 =s 0 } q base (τ ′ ) exp K k=0 r k (s ′ k ,a ′ k ) , (124$$)$where the minimization is over q with marginal p 0 at step zero. The optimum of the problem is q * , which satisfies the marginal constraint. The following alternative characterization of q * holds:

$q * (τ ) = q base (τ ) exp K k=0 r k (s k , a k ) -V(s 0 , 0) ,(125)$where

$V(x, k) = max π E τ ∼π,p K k ′ =k r k ′ (s k ′ , a k ′ ) - K-1 k ′ =k KL(π(•; s k ′ , k ′ )||π base (•; s k ′ , k ′ ))|s k = x . (126)$Proof. Let us expand KL(q||q * ):

$KL(q||q * ) = E τ ∼q log q(τ ) q * (τ ) = E τ ∼q log q(τ ) -log q base (τ ) - K k=0 r k (s k , a k ) + log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) = KL(q||q base ) -E τ ∼q K k=0 r k (s k , a k ) + E s0∼p0 log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) ,(127)$where the third equality holds because the marginal of q at step zero is p 0 by hypothesis. Since the third term in the right-hand side is independent of q, this proves the equivalence between ( [111](#formula_147)) and ( [124](#formula_168)).

Next, we prove that the marginal of q * at step zero is p 0 :

${τ |s0=x} q * (τ ) := {τ |s0=x} q base (τ ) exp K k=0 r k (s k ,a k ) 1 p 0 (x) {τ ′ |s ′ 0 =x} q base (τ ′ ) exp K k=0 r k (s ′ k ,a ′ k ) = p 0 (x).(128)$Now, for an arbitrary s 0 , let q s0 , q * s0 be the distributions q, q * conditioned on the initial state being s 0 . We can write an analog to equation (127) for q s0 , q * s0 :

$KL(q s0 ||q * s0 ) = E τ ∼qs 0 log qs 0 (τ ) q * s 0 (τ ) = E τ ∼qs 0 log q s0 (τ ) -log q base s0 (τ ) - K k=0 r k (s k , a k ) + log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base s0 (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) = KL(q s0 ||q base s0 ) -E τ ∼qs 0 K k=0 r k (s k , a k ) + log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) ,(129)$Hence,

$0 = min qs 0 KL(q s0 ||q * s0 ) = -max qs 0 {E τ ∼qs 0 K k=0 r k (s k , a k ) -KL(q s0 ||q base s0 )} + log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) .(130)$And applying ( [120](#formula_163)) from ( [120](#formula_163)), we obtain that

$log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) = max π E τ ∼π,p K k=0 r k (s k , a k ) - K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))|s 0 = V(s 0 , 0),(131)$which concludes the proof.

C.4 Proof of equation (18): the control cost is a KL regularizer Theorem 2 (Girsanov theorem for SDEs). If the two SDEs

$dX t = b 1 (X t , t) dt + σ(X t , t) dB t , X 0 = x init (132$$)$$dY t = (b 1 (Y t , t) + b 2 (Y t , t)) dt + σ(Y t , t) dB t , Y 0 = x init (133$$)$admit unique strong solutions on [0, T ], then for any bounded continuous functional Φ on C([0, T ]), we have that We consider the forward-backward SDEs ( [63](#formula_82))-( [64](#formula_84)) with arbitrary noise schedule. By Proposition 4, the trajectories ⃗ X, X of these two processes are equally distributed up to a time flip, which also means that their marginals satisfy ⃗ p t = p 1-t , for all t ∈ [0, 1]. First, we develop an explicit expression for the score function s(x, t) = ∇ log p t (x). By the properties of flow matching, we know that p t is the distribution of the interpolation variable Xt = β t X0 + α t X1 , where X0 ∼ N (0, I), X1 ∼ p data are independent. Thus, Xt-αt X1 βt ∼ N (0, I), which means that we can express the density p t as

$E[Φ(X)] = E Φ(Y ) exp - T 0 σ(Y t , t) -1 b 2 (Y t , t) dB t -1 2 T 0 ∥σ(Y t , t) -1 b 2 (Y t , t)∥ 2 dt = E Φ(Y ) exp - T 0 σ(Y t , t) -1 b 2 (Y t , t) d Bt + 1 2 T 0 ∥σ(Y t , t) -1 b 2 (Y t , t)∥ 2 dt ,(134$$p t (x) = R d exp - ∥x-α t y∥ 2 2β 2 t (2πβ 2 t ) d/2$p data (y) dy.

(139) Thus,

$s(x, t) = ∇ log p t (x) = -x β 2 t + αt β 2 t R d y exp - ∥x-α t y∥ 2 2β 2 t p data (y) dy R d exp - ∥x-α t y∥ 2 2β 2 t p data (y) dy := -x-αtξt(x) β 2 t ,(140)$where we defined

$ξ t (x) = R d y exp - ∥x-α t y∥ 2 2β 2 t p data (y) dy R d exp - ∥x-α t y∥ 2 2β 2 t p data (y) dy .(141)$Hence, we can rewrite the forward SDE (63) as

$d ⃗ X t = -κ 1-t ⃗ X t -σ(1-t) 2 2 -η 1-t ⃗ Xt-α1-tξ1-t( ⃗ Xt) β 2 1-t dt + σ(1 -t) dB t , ⃗ X 0 ∼ p data(142)$Hence, if we substitute κ 1-t ← κ 1-t + σ(1-t) 2 -2η1-t

$2β 2 1-t , ξ 1-t ← α1-t(σ(1-t) 2 -2η1-t) 2β 2 1-t ξ 1-t ( ⃗ X t$) (where we ignore the dependency on ⃗ X t ), √ 2η 1-t ← σ(1 -t), we can apply Lemma 2, which yields

$⃗ X t = ⃗ X 0 exp - t 0 κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds + t 0 exp - t t ′ κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds α 1-t ′ (σ(1-t ′ ) 2 -2η 1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ + t 0 σ(1 -t ′ ) exp - t t ′ κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds dB t ′ .(143)$We simplify the recurring expression:

$κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s = α1-s α1-s + σ(1-s) 2 -2β1-s α1-s α 1-s β1-s-β1-s 2β 2 1-s = σ(1-s) 2 2β 2 1-s + β1-s β1-s(144)$Thus,

$t t ′ κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds = t t ′ σ(1-s) 2 2β 2 1-s -∂ s log β 1-s ds = t t ′ σ(1-s) 2 2β 2 1-s ds -log β 1-t -log β 1-t ′ ,(145)$which means that exp -

$t t ′ κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds = exp - t t ′ σ(1-s) 2 2β 2 1-s ds β1-t β 1-t ′ ,(146)$$α 1-t ′ (σ(1-t ′ ) 2 -2η 1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) = α 1-t ′ σ(1-t ′ ) 2 2β 2 1-t ′ + β1-t ′ β 1-t ′ -α1-t ′ α 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ). (147$$) If we define χ(1 -s) such that σ 2 (1 -s) = 2β 1-s α1-s α1-s β 1-s -β1-s + χ(1 -s), we obtain that exp - t t ′ σ(1-s) 2 2β 2 1-s ds β1-t β 1-t ′ = exp - t t ′ α1-s α1-s -β1-s β1-s + χ(1-s) 2β 2 1-s ds β1-t β 1-t ′ = exp t t ′ ∂ s log α 1-s -∂ s log β 1-s -χ(1-s) 2β 2 1-s ds β1-t β 1-t ′ = exp - t t ′ χ(1-s) 2β 2 1-s ds α1-t α 1-t ′ ,(148)$$α 1-t ′ σ(1-t ′ ) 2 2β 2 1-t ′ + β1-t ′ β 1-t ′ -α1-t ′ α 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) = α 1-t ′ χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ )(149)$If we plug equations ( [148](#formula_192))-( [149](#formula_193)) into ( [146](#formula_190))-( [147](#formula_191)), and then those into (143), we obtain that

$⃗ X t = ⃗ X 0 exp - t 0 χ(1-s) 2β 2 1-s ds α1-t α1 + α 1-t t 0 exp - t t ′ χ(1-s) 2β 2 1-s ds χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ + t 0 2β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ + χ(1 -t ′ ) exp - t t ′ χ(1-s) 2β 2 1-s ds α1-t α 1-t ′ dB t ′ .$(150) and if we take the limit t → 1 -and use that

$α 1 = 1, ⃗ X 1 = ⃗ X 0 lim t→1 -exp - t 0 χ(1-s) 2β 2 1-s ds α 1-t + lim t→1 -α 1-t t 0 exp - t t ′ χ(1-s) 2β 2 1-s ds χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ + lim t→1 - t 0 2β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ + χ(1 -t ′ ) exp - t t ′ χ(1-s) 2β 2 1-s ds α1-t α 1-t ′ dB t ′ . (151$$)$The assumption on χ in ( [25](#formula_33)) is equivalent, up to a rearrangement of the notation and a flip in the time variable, to the statement that for all t ′ ∈ [0, 1),

$lim t→1 -exp - t t ′ χ(1-s) 2β 2 1-s ds α 1-t = 0.(152)$Hence, under assumption (25), the factor accompanying ⃗ X 0 in equation ( [151](#formula_195)) is zero. Moreover, this assumption also implies that

$lim t→1 -α 1-t t 0 exp - t t ′ χ(1-s) 2β 2 1-s ds χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ = 1 0 lim t→1 -exp - t t ′ χ(1-s) 2β 2 1-s ds α 1-t χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ = 0.(153)$If we plug ( [152](#formula_197)) and ( [153](#formula_198)) into (151), we obtain that

$⃗ X 1 = lim t→1 - t 0 2β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ + χ(1 -t ′ ) exp - t t ′ χ(1-s) 2β 2 1-s ds α1-t α 1-t ′ dB t ′ ,(154)$which shows that ⃗ X 1 is independent of ⃗ X 0 . Next, we leverage that ⃗ X and X have equal distributions over trajectories (Proposition 4). In particular, the joint distribution of ( ⃗ X 0 , ⃗ X 1 ) is equal to the joint distribution of (X 1 , X 0 ). We conclude that X 1 and X 0 are independent, which is the definition of the memorylessness property. Hence, the assumption ( [25](#formula_33)) is sufficient for memorylessness to hold.

It remains to prove that the assumption (25) is necessary. Looking at equation ( [150](#)) we deduce that generally, for any t ∈ [0, 1), ⃗ X 0 and ⃗ X t are not independent, because the first two terms in (150) are different from zero. Thus, if there existed a t ′ ∈ [0, 1) such that the limit ( [152](#formula_197)) is different from zero, then ⃗ X 1 would not be independent from ⃗ X t ′ , which means that in general it would not be independent of ⃗ X 0 either.

## D.2 Proof of Theorem 1: fine-tuning recipe for general noise schedules

The proof of this result relies heavily on the properties of the Hamilton-Jacobi-Bellman equation:

Theorem 3 (Hamilton-Jacobi-Bellman equation). If we define the infinitesimal generator

$L := 1 2 d i,j=1 (σσ ⊤ ) ij (t)∂ xi ∂ xj + d i=1 b i (x, t)∂ xi ,(155)$the value function V for the SOC problem (12)-( [13](#formula_14)) solves the following Hamilton-Jacobi-Bellman (HJB) partial differential equation:

$∂ t V (x, t) = -LV (x, t) + 1 2 ∥(σ ⊤ ∇V )(x, t)∥ 2 -f (x, t), V (x, T ) = g(x).(156)$Consider forward SDEs like (63), starting from the distributions p base and p * , where p * (x) ∝ p base (x) exp(r(x)).

$d ⃗ X t = ⃗ b( ⃗ X t , t) dt + σ(t) dB t , ⃗ X 0 ∼ p base , (157) d ⃗ X * t = ⃗ b * ( ⃗ X * t , t) dt + σ(t) dB t , ⃗ X 0 ∼ p * . (158$$)$where the drifts are defined as

$⃗ b(x, t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t s(x, 1 -t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t ∇ log ⃗ p t (x), ⃗ b * (x, t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t s * (x, 1 -t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t ∇ log ⃗ p * t (x),(159)$and ⃗ p t , ⃗ p * t are the densities of X t , ⃗ X t , respectively. ⃗ p t , ⃗ p * t satisfy Fokker-Planck equations:

$∂ t ⃗ p t = ∇ • ( ⃗ b(x, t)⃗ p t ) + ∇ • ( σ(1-t) 2 2 ∇⃗ p t ), ⃗ p 0 = p base , ∂ t ⃗ p * t = ∇ • ( ⃗ b * (x, t)⃗ p * t ) + ∇ • ( σ(1-t) 2 2 ∇⃗ p * t ), ⃗ p 0 = p * .(160)$Plugging ( [159](#formula_204)) into (160), we obtain

$∂ t ⃗ p t = ∇ • (κ 1-t x⃗ p t ) + ∇ • η 1-t ∇⃗ p t , ⃗ p 0 = p base , ∂ t ⃗ p * t = ∇ • (κ 1-t x⃗ p * t ) + ∇ • η 1-t ∇⃗ p * t , ⃗ p 0 = p * .(161)$We apply the Hopf-Cole transformation to obtain PDEs for -log ⃗ p t (and -log ⃗ p * t analogously):

$-∂ t (-log ⃗ p t ) = ∂tpt pt = ∇•(κ1-tx⃗ pt)+∇• η1-t∇⃗ pt pt = κ 1-t ∇ • x + κ 1-t ⟨x, ∇ log ⃗ p t ⟩ + η 1-t ∇•(∇ log ⃗ pt exp(log pt)) pt = κ 1-t d + κ 1-t ⟨x, ∇ log ⃗ p t ⟩ + η 1-t ∆ log ⃗ p t + ∥∇ log ⃗ p t ∥ 2 .(162)$Hence, if we define V (x, t) = -log ⃗ p t (x), V * (x, t) = -log ⃗ p * t (x), then V and V * satisfy the following Hamilton-Jacobi-Bellman equations:

$-∂ t V = κ 1-t d -κ 1-t ⟨x, ∇V ⟩ + η 1-t -∆V + ∥∇V ∥ 2 , V (x, 0) = -log p base (x),(163)$$-∂ t V * = κ 1-t d -κ 1-t ⟨x, ∇V * ⟩ + η 1-t -∆V * + ∥∇V * ∥ 2 , V * (x, 0) = -log p * (x).(164)$Now, define V (x, t) = V * (x, t) -V (x, t). Subtracting ( [164](#formula_209)) from ( [163](#formula_208)), we obtain

$-∂ t V = -κ 1-t ⟨x, ∇ V ⟩ + η 1-t -∆ V + ∥∇V * ∥ 2 -∥∇V ∥ 2 = -κ 1-t ⟨x, ∇ V ⟩ + η 1-t -∆ V + ∥∇( V + V )∥ 2 -∥∇V ∥ 2 = -κ 1-t ⟨x, ∇ V ⟩ + η 1-t -∆ V + ∥∇ V ∥ 2 + 2⟨∇V , ∇ V ⟩ = ⟨-κ 1-t x + 2η 1-t ∇V , ∇ V ⟩ + η 1-t -∆ V + ∥∇ V ∥ 2 = ⟨-κ 1-t x -2η 1-t s(x, 1 -t), ∇ V ⟩ + η 1-t -∆ V + ∥∇ V ∥ 2 , V (x, 0) = -log p * (x) + log p base (x) = -r(x) + log p base (y) exp(r(y)) dy .(165)$Hence, V also satisfies a Hamilton-Jacobi-Bellman equation. If we define V such that V (x, t) = V (x, 1 -t), we have that

$∂ t V = ⟨-κ t x -2η t s(x, t), ∇V ⟩ + η t -∆V + ∥∇V ∥ 2 , V (x, 1) = r(x) -log p base (y) exp(r(y)) dy . (166$$)$Using Theorem 3, we can reverse-engineer V as the value function of the following SOC problem:

$min u∈U E 1 2 1 0 ∥u(X u t , t)∥ 2 dt-r(x)+log p base (y) exp(r(y)) dy ,(167) s$$.t. dX u t = κ t x + 2η t s(x, t)+ √ 2η t u(X u t , t) dt+ √ 2η t dB t , X u 0 ∼ p 0 .(168)$Note that this SOC problem is equal to the problem ( [12](#formula_13))-( [13](#formula_14)) with the choices f = 0, g = -r, and σ(t) = √ 2η t . By equation ( [17](#formula_19)), the optimal control of the problem (167)-( [168](#formula_214)) is of the form:

$u * (x, t) = - √ 2η t ∇V (x, t) = - √ 2η t ∇ V (x, 1 -t) = - √ 2η t ∇V * (x, 1 -t) -∇V (x, 1 -t) = - √ 2η t -∇ log ⃗ p * 1-t (x) + ∇ log ⃗ p 1-t (x) = √ 2η t s * (x, t) -s(x, t) ,(169)$$⇐⇒ s * (x, t) = s(x, t) + u * (x, t)/ √ 2η t .(170)$As in (64), the backward SDEs corresponding to the forward SDEs (158) take the following form:

$dX * t = κ t X * t + σ(t) 2 2 + η t s * (X * t , t) dt + σ(t) dB t , X * 0 ∼ N (0, I).(171)$If we plug (170) into this equation, we obtain

$dX * t = κ t X * t + σ(t) 2 2 + η t s(X * t , t) + u * (X * t ,t) √ 2ηt dt + σ(t) dB t , X * 0 ∼ N (0, I),(172)$$⇐⇒ dX * t = b(X * t , t) + σ(t) 2 2 +ηt √ 2ηt u * (X * t , t) dt + σ(t) dB t , X * 0 ∼ N (0, I). (173$$)$where we used that b(x, t) = κ t x + σ(t) 2 2 + η t s(x, t) by definition in equation ( [11](#formula_12)).

The fine-tuned inference SDE for DDIM Now, for DDIM, we have that u * (x, t) = -αt αt(1-αt) (ϵ * (x, t)ϵ base (x, t)) by ( [26](#)). Hence,

$σ(t) 2 2 +ηt √ 2ηt u * (x, t) = - σ(t) 2 2 + αt 2α t αt α t αt αt(1-αt) (ϵ * (x, t) -ϵ base (x, t)) = - σ(t) 2 2 + αt 2α t √ 1-αt (ϵ * (x, t) -ϵ base (x, t)),(174)$$=⇒ b(x, t) + σ(t) 2 2 +ηt √ 2ηt u * (x, t) = αt 2αt X t -αt 2αt + σ(t) 2 2 ϵ base (Xt,t) √ 1-αt - σ(t) 2 2 + αt 2α t √ 1-αt (ϵ * (x, t) -ϵ base (x, t)) = αt 2αt X t -αt 2αt + σ(t) 2 2 ϵ * (Xt,t) √ 1-αt .(175)$We obtain that the fine-tuned inference SDE for DDIM is

$dX * t = αt 2αt X * t -αt 2αt + σ(t) 2 2 ϵ * (X * t ,t) √ 1-αt dt + σ(t) dB t , X * 0 ∼ N (0, I),(176)$which is matches the SDE (6) with the choice ϵ = ϵ * .

The fine-tuned inference SDE for Flow Matching For Flow Matching, we have that u

$* (x, t) = 2 βt( αt α t βt-βt) (v * (x, t)- v base (x, t)) by (27). Hence, σ(t) 2 2 +ηt √ 2ηt u * (x, t) = σ(t) 2 2 +βt( αt α t βt-βt) 2βt( αt α t βt-βt) 2 βt( αt α t βt-βt) (v * (x, t) -v base (x, t)) = 1 + σ(t) 2 2βt( αt α t βt-βt) (v * (x, t) -v base (x, t)). (177) =⇒ b(x, t) + σ(t) 2 2 +ηt √ 2ηt u * (x, t) = v base (x, t) + σ(t) 2 2βt( αt α t βt-βt) v base (x, t) -αt αt x + 1 + σ(t) 2 2βt( αt α t βt-βt) (v * (x, t) -v base (x, t)) = v * (x, t) + σ(t) 2 2βt( αt α t βt-βt) v * (x, t) -αt αt x .(178)$We obtain that the fine-tuned inference SDE for Flow Matching is

$dX * t = v(X * t , t) + σ(t) 2 2βt( αt α t βt-βt) v * (X * t , t) -αt αt X * t dt + σ(t) dB t , X * 0 ∼ N (0, I),(179)$which matches equation ( [4](#formula_3)) with the choice v = v * .

## E Loss function derivations

E.1 Derivation of the Continuous Adjoint method Proposition 6. The gradient dL dθ of the adjoint loss L(u; X) defined in (28) with respect to the parameters θ of the control can be expressed as in (32).

Proof. First, note that we can write

$∇ θ E T 0 1 2 ∥u θ (X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = E T 0 ∇ θ u θ (X u θ t , t)u θ (X u θ t , t) dt + ∇ θ E T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) | v=stopgrad(u θ ) .(180)$To develop the second term, we apply Lemma 5. Namely, by the Leibniz rule and equation ( [185](#formula_230)), we have that

$∇ θ E T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) | v=stopgrad(u θ ) = E ∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) | v=stopgrad(u θ ) = E T 0 (∇ θ u θ )(X u θ t (ω), t) ⊤ σ(t) ⊤ a t (ω) dt .(181)$Plugging the right-hand side of this equation into (180) concludes the proof.

Lemma 5. Let v be an arbitrary fixed vector field. The unique solution of the ODE

$d dt a(t; X u , u) = -∇ X u t (b(X u t , t) + σ(t)u(X u t , t)) T a(t; X u , u) + ∇ X u t f (X u t , t) + 1 2 ∥v(X u t , t)∥ 2 , (182) a(1; X u , u) = ∇g(X u 1 ),(183)$satisfies:

$a(t; X u , u) := ∇ X u t 1 t 1 2 ∥u(X u t ′ , t ′ )∥ 2 +f (X u t ′ , t ′ ) dt ′ +g(X u 1 ) , where X u solves dX u t = b(X u t , t) + σ(t)u(X u t , t) dt + σ(t)dB t .(184)$Moreover, when u = u θ is parameterized by θ we have that

$∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = T 0 (∇ θ u θ )(X u θ t (ω), t)σ(t) ⊤ a t (ω) dt.(185)$Proof. We use an approach based on Lagrange multipliers which mirrors and extends the derivation of the adjoint ODE [(Domingo-Enrich et al., 2023, Lemma 8)](#). For shortness, we use the notation bθ (x, t) :

$= b(x, t) + σ(t)u θ (x, t). Define a process a : Ω × [0, T ] → R d such that for any ω ∈ Ω, a(ω, •) is differentiable. For a given ω ∈ Ω, we can write T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) - T 0 ⟨a t (ω), (dX u θ t (ω) -bθ (X u θ t (ω), t) dt -σ(t) dB t )⟩.(186)$By stochastic integration by parts (Domingo-Enrich et al., 2023, Lemma 9), we have that

$T 0 ⟨a t (ω), dX u θ t (ω)⟩ = ⟨a T (ω), X u θ T (ω)⟩ -⟨a 0 (ω), X u θ 0 (ω)⟩ - T 0 ⟨X u θ t (ω), dat dt (ω)⟩ dt.(187)$Hence, if X u θ 0 = x 0 is the initial condition, we have that 9

$∇ x0 T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = ∇ x0 T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) -⟨a T (ω), X u θ T (ω)⟩ + ⟨a 0 (ω), X u θ 0 (ω)⟩ + T 0 ⟨a t (ω), bθ (X u θ t (ω), t)⟩ + ⟨ dat dt (ω), X u θ t (ω)⟩ dt + T 0 ⟨a t (ω), σ(t) dB t ⟩ = T 0 ∇ x0 X u θ t (ω) ⊤ ∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) dt + ∇ x0 X u θ T (ω) ⊤ ∇ x g(X u θ T (ω)) -∇ x0 X u θ T (ω) ⊤ a T (ω) + ∇ x0 X u θ 0 (ω) ⊤ a 0 (ω) + T 0 ∇ x0 X u θ t (ω) ⊤ ∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) + ∇ x0 X u θ t (ω) ⊤ dat dt (ω) dt = T 0 ∇ x0 X u θ t (ω) ⊤ ∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) + ∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) + dat dt (ω) dt + ∇ x0 X u θ T (ω) ⊤ ∇ x g(X u θ T (ω)) -a T (ω) + a 0 (ω).(188)$In the last line we used that

$∇ x0 X u θ 0 (ω) = ∇ x0 x 0 = I. If choose a such that da t (ω) = -∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) -∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) dt, a T (ω) = ∇ x g(X u θ T (ω)),(189)$which is the ODE ( [182](#))-( [183](#formula_228)), then we obtain that

$∇ x0 T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = a 0 (ω)(190)$Without loss of generality, this argument can be extended from t = 0 to an arbitrary t ∈ [0, 1], which proves the first statement of the lemma.

To prove (185), we similarly write ∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = ∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) -⟨a T (ω), X u θ T (ω)⟩ + ⟨a 0 (ω), X u θ 0 (ω)⟩ + T 0 ⟨a t (ω), bθ (X u θ t (ω), t)⟩ + ⟨ dat dt (ω), X u θ t (ω)⟩ dt + T 0 ⟨a t (ω), σ(t) dB t ⟩ = T 0 ∇ θ X u θ t (ω) ⊤ ∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) dt + ∇ θ X u θ T (ω) ⊤ ∇ x g(X u θ T (ω)) -∇ θ X u θ T (ω) ⊤ a T (ω) + ∇ θ X u θ 0 (ω) ⊤ a 0 (ω) + T 0 ∇ θ X u θ t (ω) ⊤ ∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) + ∇ θ bθ (X u θ t (ω), t) ⊤ a t (ω) + ∇ θ X u θ t (ω) ⊤ dat dt (ω) dt = T 0 ∇ θ X u θ t (ω) ⊤ ∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) + ∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) + dat dt (ω) dt + ∇ θ X u θ T (ω) ⊤ ∇ x g(X u θ T (ω)) -a T (ω) + T 0 (∇ θ bθ )(X u θ t (ω), t) ⊤ a t (ω) dt.

(191)

In the last line we used that ∇ θ X u θ 0 (ω) = ∇ θ x = 0. When a satisfies (189), we obtain that ∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = T 0 (∇ θ bθ )(X u θ t (ω), t)a t (ω) dt = T 0 (∇ θ u θ )(X u θ t (ω), t) ⊤ σ(t) ⊤ a t (ω) dt.

(192)

The last equality holds because bθ (x, t) := b(x, t) + σ(t)u θ (x, t).

9 Unlike (Domingo-Enrich et al., 2023, Lemma 8), we use the convention that a Jacobian matrix J = ∇xv(x) is defined as

$J ij = ∂v i (x) ∂x j$. Their definition of ∇xv is the transpose of ours.

Plugging this back into (200), we obtain that 0 = ∂ t J(u; x, t) + ⟨∇J(u; x, t), b(x, t)⟩ + Tr σ(t)σ(t) ⊤ 2 ∇ 2 J(u; x, t) -1 2 ∥σ(t) ⊤ ∇ x J(u; x, t)∥ 2 + f (x, t). (202) And since J(u; x, T ) = g(x) by construction, we conclude that J(u; x, t) satisfies the HJB equation ( [156](#formula_201)).

## E.3 Theoretical guarantees of the Adjoint Matching loss

Proposition 7 (Theoretical guarantee of the Adjoint Matching loss). The only critical point of the loss E[L Adj-Match ] is the optimal control u * .

Proof. Let v be an arbitrary control. If ã(t; X v ) is the solution of the Lean Adjoint ODE (38)-( [39](#formula_56)), it satisfies the integral equation

$ã(t; X v ) = T t ∇ x b(X v s , s) ⊤ ã(s; X v ) + ∇ x f (X v s , s) ds + ∇g(X v T ).(203)$Hence,

$E ã(t; X v ) X v t = E T t ∇ x b(X v s , s) ⊤ ã(s; X v ) + ∇ x f (X v s , s) ds + ∇g(X v T ) X v t = E T t ∇ x b(X v s , s) ⊤ E ã(s; X v ) X v s + ∇ x f (X v s , s) ds + ∇g(X v T ) X v t ,(204)$where we used the tower property of conditional expectation in the second equality.

Similarly, if a(t; X v , v) is the solution of the Adjoint ODE ( [30](#formula_39))-( [31](#formula_40)), it satisfies the integral equation

$a(t; X v , v) = T t ∇ x b(X v s , s) ⊤ a(s; X v , v) + σ(s)v(X v s , s) + ∇ x f (X v s , s) + 1 2 ∥v(X v s , s)∥ 2 ds + ∇g(X v T ),(205)$and its expected value satisfies E a(t; X v , v) X

v t = E T t ∇ x b(X v s , s) + σ(s)v(X v s , s) ⊤ a(s; X v , v) + ∇ x f (X v s , s) + 1 2 ∥v(X v s , s)∥ 2 ds + ∇g(X v T ) X v t = E T t ∇ x b(X v s , s)+σ(s)v(X v s , s) ⊤ E a(s; X v , v) X v s +∇ x f (X v s , s)+ 1 2 ∥v(X v s , s)∥ 2 ds+∇g(X v T ) X v t . (206) Let us rewrite E[L Adj-Match ] as follows: E[L Adj-Match (u)] := E T 0 u(X v t , t) + σ(t) ⊤ E ã(t, X v )|X v t 2 dt | v=stopgrad(u) + E T 0 σ(t) ⊤ E ã(t, X v )|X v t -ã(t, X v ) 2 dt | v=stopgrad(u) , (207) Now, suppose that û is a critical point of E[L Adj-Match ]. By definition, this implies that the first variation of E[L Adj-Match ] is zero. Using (207), we can write this as follows: 0 = δ δu E[L Adj-Match (û)](x) = 2 û(x, t) + σ(t) ⊤ E[ã(t, X û)|X û t = x] , (208) =⇒ û(x, t) = -σ(t) ⊤ E[ã(t, X û)|X û t = x]. (209) Hence, we have ∇ x û(X û t , t) ⊤ σ(t) ⊤ E[ã(t, X û)|X û t ] + ∇ x û(X û t , t) ⊤ û(X û t , t) = 0, (210) =⇒ E T t ∇ x σ(s)û(X û s , s) ⊤ E ã(s; X û) X û s +∇ x 1 2 ∥û(X û s , s)∥ 2 ds X û t = 0. (211) If we set v = û in equation (204), and add (211) to its right-hand side, we obtain that E[ã(t, X û)|X û t ] also solves the integral equation E ã(t; X û) X û t = E T t ∇ x b(X û s , s)+σ(s)û(X û s , s) ⊤ E ã(s; X û) X û s +∇ x f (X û s , s)+ 1 2 ∥û(X û s , s)∥ 2 ds+∇g(X û T ) X û t . (212) Note that this integral equation is the same one as equation (206) when we set v = û in the latter. Proposition 8 states that the solution of the integral equation is unique, which means that E ã(t; X û) X û t = E a(t; X û, û) X û t for all t ∈ [0, T ]. Since we can reexpress the basic Adjoint Matching loss as E[L Basic-Adj-Match (u)] := E T 0 u(X v t , t) + σ(t) ⊤ E a(t; X v , v)|X v t 2 dt | v=stopgrad(u) + E T 0 σ(t) ⊤ E a(t; X v , v)|X v t -a(t; X v , v) 2 dt | v=stopgrad(u) , (213) we obtain that when û is a critical point of E[L Adj-Match ], d du E[L Basic-Adj-Match (û)](x) = 2 û(x, t) + σ(t) ⊤ E[a(t; X û, û)|X û t = x] = 2 û(x, t) + σ(t) ⊤ E[ã(t; X û)|X û t

$= x] = 0,(214)$where the second equality holds because E ã(t; X û) X û t = E a(t; X û, û) X û t , and the third equality holds by equation ( [209](#)). Thus, we deduce that the critical points of E[L Adj-Match ] are critical points of E[L Basic-Adj-Match ]. By Proposition 2, E[L Basic-Adj-Match ] has a single critical point, which is the optimal control u * , which concludes the proof of the statement for E[L Adj-Match ].

Proposition 8. Let v be an arbitrary control. Consider the integral equation:

$Y t = E T t ∇ x b(X v s , s)+σ(s)v(X v s , s) ⊤ Y s +∇ x f (X v s , s)+ 1 2 ∥v(X v s , s)∥ 2 ds+∇g(X v T ) X v t ,(215)$where t ∈ [0, T ]. This equation has a unique solution, i.e. if Y 1 , Y 2 are two solutions then Y 1 = Y 2 .

Proof. Let Y 1 , Y 2 be two solutions of the integral equation. We have that

$Y 1 t -Y 2 t = E T t (Y 1 s -Y 2 s ) ⊤ ∇ x b(X * s , s) ds X * t .(216)$Thus,

$∥Y 1 t -Y 2 t ∥ ≤ E T t (Y 1 s -Y 2 s ) ⊤ ∇ x b(X * s , s) ds X * t ≤ E T t (Y 1 s -Y 2 s ) ⊤ ∇ x b(X * s , s) ds X * t ≤ E T t Y 1 s -Y 2 s • ∇ x b(X * s , s) ds X * t = T t E Y 1 s -Y 2 s • ∇ x b(X * s , s) X * t ds ≤ T t E Y 1 s -Y 2 s 2 X * t 1/2 • E ∇ x b(X * s , s) 2 X * t 1/2 ds (217)$And this implies that

$sup t ′ ∈[0,t] E[∥Y 1 t -Y 2 t ∥ 2 |X * t ′ ] 1/2 ≤ T t E Y 1 s -Y 2 s 2 X * t 1/2 • E ∇ x b(X * s , s) 2 X * t 1/2 ds ≤ T t sup t ′ ∈[0,s] E Y 1 s -Y 2 s 2 X * t ′ 1/2 • sup t ′ ∈[0,s] E ∇ x b(X * s , s) 2 X * t ′$1/2 ds.

(218) Applying Grönwall's inequality on the function f (t) = sup t ′ ∈[0,t] E[∥Y 1 t -Y 2 t ∥ 2 |X * t ′ ] 1/2 , we obtain that sup t ′ ∈[0,t] E[∥Y 1 t -Y 2 t ∥ 2 |X * t ′ ] 1/2 = 0 for all t ∈ [0, T ], which means that Y 1 t = Y 2 t almost surely. And since ∥Y 1 t -Y 2 t ∥ ≤ T t E Y 1 s -Y 2 s 2 |X * t 1/2 • E ∇ x b(X * s , s) 2 |X * t 1/2 ds = 0, we obtain that Y 1 = Y 2 .

## E.4 Pseudo-code of Adjoint Matching for DDIM fine-tuning

Note that for each pair of equations ( [219](#))-( [220](#formula_245)), ( [221](#formula_246))-( [222](#formula_247)), ( [223](#formula_248))-( [224](#formula_250)), the first equation corresponds to the updates in the DDPM paper, while the second equation is an Euler-Maruyama / Euler discretization of the continuous-time object. To check that both discretizations are equal up to first order, remark that 

For each trajectory, solve the lean adjoint ODE (38)-( [39](#formula_56)) backwards in time from k = K to 0, e.g.:

$ãk = ãk+1 + ãT k+1 ∇X k ᾱk+1 ᾱk X k - 1-ᾱk / ᾱk+1 √ 1-ᾱk ϵ base (X k , k) -X k , ãK = ∇X K r(XK ),(221)$or ãk = ãk+1 + ãT k+1 ∇X t ᾱk+1 -ᾱk 2 ᾱk

$X k -ᾱk+1 -ᾱk ᾱk √ 1-ᾱk ϵ base (X k , k) , ãK = ∇X K r(XK ).(222)$Note that X k and ãk should be computed without gradients, i.e., X k = stopgrad(X k ), ãk = stopgrad(ã k ).

For each trajectory, compute the Adjoint Matching objective (37):

L Adj-Match (θ) = k∈{0,...,K-1} ᾱk+1 ᾱk (1-ᾱk+1 ) 1 -ᾱk ᾱk+1 (ϵ finetune (X k , k) -ϵ base (X k , k))

$- 1-ᾱk+1 1-ᾱk 1 -ᾱk ᾱk+1 ãk 2 , (223$$)$or L Adj-Match (θ) = k∈{0,...,K-1}

$ᾱk+1 -ᾱk ᾱk (1-ᾱk ) (ϵ finetune (X k , k) -ϵ base (X k , k)) -ᾱk+1 -ᾱk ᾱk ãk 2 . (224$$)$Compute the gradient ∇ θ L(θ) and update θ using favorite gradient descent algorithm. end Output: Fine-tuned vector field v finetune F Adapting diffusion fine-tuning baselines to flow matching F.1 Adapting ReFL [(Xu et al., 2023)](#b91) to flow matching Reward Feedback Learning (ReFL) is a diffusion fine-tuning algorithm introduced by [Xu et al. (2023)](#b91) which tries to increase the reward on denoised samples. Namely, if X = (X t ) t∈[0,1] is the solution of the DDPM SDE (7), we can denoise X t as

$X1 (X t ) = Xt- √ 1-ᾱtϵ(Xt,t) √ ᾱt . (226$$)$This equation follows from the stochastic interpolant equation (2) if we replace X0 with the noise predictor ϵ(X t , t). And then, the ReFL optimization update is based on the gradient:

$∇ θ r( X1 (X t )) = ∇ θ r Xt- √ 1-ᾱtϵθ(Xt,t) √ ᾱt ,(227)$where the trajectories have been detached.

To adapt ReFL to Flow Matching, we need to express the denoiser map in terms of the vector field v.

We have that v(x, t) = E βt X0 + αt X1 β t X0 + α t X1 = x = E βt βt β t X0 + α t X1 + αt -βt βt α t X1 β t X0 + α t X1 = x = βt βt x + αt -βt βt α t X1 (x, t). (228) where we defined the denoiser map X1 (x, t) := E X1 |β t X0 + α t X1 = x . Hence, X1 (x, t) = v(x,t)-βt β t x αt-βt β t αt . (229)

![Figure 2 Visualization of Theorem 1 showing that fine-tuning must be done with the memoryless noise schedule to ensure convergence to the tilted distribution (1). (a) Shows the base Flow Matching model. (b, c) Fine-tuning using a constant σ(t) leads to biased distributions. (d) Fine-tuning using the memoryless noise schedule leads to the correct tilted distribution. Note that sample generation can use any noise schedule after fine-tuning, including σ(t) = 0.]()

![Figure3Our proposed Adjoint Matching using the memoryless SOC formulation introduces a much more principled way of trading off how close to stay to the base model while optimizing the reward model. In contrast, baseline methods such as DRaFT-1 only optimize the reward model and must rely on early stopping to perform this trade off, resulting in a much more sensitive hyperparameter. Samples are produced using σ(t) = 0 with the same noise sample. Text prompts: "Handsome Smiling man in blue jacket portrait" and "Quinoa and Feta Stuffed Baby Bell Peppers".]()

![Figure5Tradeoffs between different aspects of generative models: text-to-image consistency (ClipScore), sample diversity for each prompt (DreamSim Diversity), and generalization to unseen human preferences (HPS v2). Different points are obtained from varying values of λ for Adjoint Matching and varying number of fine-tuning iterations for the DRaFT-1 baseline. Overall, we find our proposed method Adjoint Matching has the best Pareto fronts.]()

![Figure 6 Average values of ImageReward (reward function), control cost ( t 0 1 2 ∥u(X u t , t)∥ 2 dt), and ClipScore vs. wall-clock time for Adjoint Matching and our baselines. Lines show averages over three fine-tuning runs, evaluating on separate test datasets of size 200. Confidence intervals show standard errors of estimates.]()

![]()

![Figure 8 Generated samples with classifier-free guidance (w = 1) and σ(t) = 0 across ten selected prompts. Each row corresponds to a different prompt and each image corresponds to a different random seed consistent across models.]()

![) where Bt = B t + t 0 σ(Y s , s) -1 b 2 (Y s , s) ds. More generally, b 1 and b 2 can be random processes that are adapted to filtration of B. D Proofs of Section 4.3: memoryless noise schedule and fine-tuning recipe D.1 Proof of Proposition 1: the memoryless noise schedule]()

![ᾱk+1 ᾱk = 1 + ᾱk+1 -ᾱk ᾱk ≈ 1 + ᾱk+1 -ᾱk 2 ᾱk + O((ᾱ k+1 -ᾱk ) 2 ). (225)Algorithm 2 Adjoint Matching for fine-tuning DDIM Input: Pre-trained denoiser ϵ base , number of fine-tuning iterations N . Initialize fine-tuned denoiser: ϵ finetune = ϵ base with parameters θ.for n ∈ {0, . . . , N -1} do Sample m trajectories X = (Xt) t∈{0,...,1} according to DDPM, e.g.:ᾱk+1 ε k , ε k ∼ N (0, I), X0 ∼ N (0, I),(219)orX k+1 = X k + ᾱk+1 -ᾱk 2 ᾱk X k -ᾱk+1 -ᾱk ᾱk √ 1-ᾱk ϵ finetune (X k , k) + ᾱk+1 -ᾱk ᾱk ε k .]()

![Diffusion coefficient σ(t) and the factors κt, ηt for the Flow Matching, Memoryless Flow Matching, DDIM, and DDPM generative processes. When the diffusion coefficient is σ]()

![Metrics for various fine-tuning methods for text-to-image generation. The second and third columns show the noise schedules σ(t) used for fine-tuning and for inference: σ(t) = √ 2ηt corresponds to Memoryless Flow Matching, and σ(t) = 0 to the Flow Matching ODE (3). Confidence intervals show standard errors of estimates; computed over 3 runs of the fine-tuning algorithm on separate fine-tuning prompt datasets of size 40000 each. Test prompt sets are of size 1000, and also different for each run.]()

![Evaluation metrics when using classifier-free guidance (CFG;Ho and Salimans (2022)).]()

![Metrics for alternative optimization hyperparameters (learning rate and Adam β1).]()

![Comparison with an alternative fine-tuning noise schedule σ(t) = 1. We see that the initial value function bias (Section 4.2) results in the model not having a high reward function (ImageReward is the reward function used for fine-tuning). Its performance on other metrics are also lower than when fine-tuning with the memoryless noise schedule, except for diversity.]()

Note that there is a slight difference in terminology between SOC and reinforcement learning, where our cost functional is referred to as the state value function and our value function is the optimal state value function in RL.

Note (20) is informal because densities over continuous-time processes are ill-defined; the formal statement is dP * dP base (X|X 0 ) = exp(-1 0 f (Xt, t) dt -g(X 1 )), where dP * dP base denotes the Radon-Nikodym derivative. We treat this formally in the proofs.

Note we use the convention that a Jacobian matrix J = ∇xv(x) is defined asJ ij = ∂v i (x) ∂x j.

See also[Domingo-Enrich (2024)](#b20) for a comparison among SOC losses.

The entropy terms are usually multiplied by a factor to tune their magnitude, but one can equivalently rescale the rewards, which is why we do not add any factor.

