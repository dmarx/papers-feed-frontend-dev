<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-07">7 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Carles</forename><surname>Domingo-Enrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">FAIR</orgName>
								<address>
									<settlement>Meta</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">FAIR</orgName>
								<address>
									<settlement>Meta</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Karrer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">FAIR</orgName>
								<address>
									<settlement>Meta</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">FAIR</orgName>
								<address>
									<settlement>Meta</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carles</forename><surname>Domingo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">FAIR</orgName>
								<address>
									<settlement>Meta</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-07">7 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">877ACFD1B2AA7CD9BE94E4796BB7C1DC</idno>
					<idno type="arXiv">arXiv:2409.08861v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoreticallysound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref> We introduce Adjoint Matching, a theoretically-driven yet simple algorithm for reward fine-tuning that works for a large family of dynamical generative models, including for the first time, Flow Matching models. Text prompts: "Beautiful colorful sunset midst of building in Bangkok Thailand ", "Beautiful grandma and granddaughter are mixing salad and smiling while cooking in kitchen", "The beautiful young woman in sunglasses is standing at the background of field and hill. She is smiling and looking over shoulder ", "Chess, intellectual games, figure horse, chess board ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Flow Matching <ref type="bibr" target="#b46">(Lipman et al., 2023;</ref><ref type="bibr">Albergo and Vanden-Eijnden, 2023;</ref><ref type="bibr" target="#b49">Liu et al., 2023)</ref> and denoising diffusion <ref type="bibr" target="#b73">(Song and Ermon, 2019;</ref><ref type="bibr" target="#b36">Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021b;</ref><ref type="bibr">Kingma et al., 2021)</ref> models are being used for many generative modeling applications, including text-to-image <ref type="bibr" target="#b65">(Rombach et al., 2022;</ref><ref type="bibr" target="#b23">Esser et al., 2024)</ref>, text-to-video <ref type="bibr" target="#b70">(Singer et al., 2022)</ref>, and text-to-audio <ref type="bibr" target="#b43">(Le et al., 2024;</ref><ref type="bibr" target="#b85">Vyas et al., 2023)</ref>. In most cases, the base generative model does not achieve the desired sample quality. To improve the generated samples, it is common to resort to techniques such as classifier-free guidance <ref type="bibr" target="#b35">(Ho and Salimans, 2022;</ref><ref type="bibr" target="#b95">Zheng et al., 2023)</ref> to get better text-to-sample alignment, or to fine-tune using human preference reward models to improve sample quality and realism <ref type="bibr">(Wallace et al., 2023a;</ref><ref type="bibr" target="#b17">Clark et al., 2024)</ref>.</p><p>In the adjacent field of large language models, the behavior of the model is aligned to human preferences through fine-tuning with reinforcement learning from human feedback (RLHF). Either explicitly or implicitly, RLHF methods <ref type="bibr">(Ziegler et al., 2020;</ref><ref type="bibr" target="#b75">Stiennon et al., 2020;</ref><ref type="bibr" target="#b56">Ouyang et al., 2022;</ref><ref type="bibr" target="#b4">Bai et al., 2022</ref>) assume a reward model r(x) that captures human preferences, with the goal of modifying the base generative model such that it generates the following tilted distribution:</p><formula xml:id="formula_0">p * (x) ∝ p base (x) exp(r(x)),<label>(1)</label></formula><p>where p base is the base generative model's sample distribution.</p><p>Inspired by this, fine-tuning methods have been developed to improve denoising diffusion models based on human preference data; either using a reward-based approach <ref type="bibr">(Fan and Lee, 2023;</ref><ref type="bibr" target="#b11">Black et al., 2024;</ref><ref type="bibr">Fan et al., 2023;</ref><ref type="bibr" target="#b91">Xu et al., 2023;</ref><ref type="bibr" target="#b17">Clark et al., 2024;</ref><ref type="bibr">Uehara et al., 2024a,b)</ref>, or direct preference optimization <ref type="bibr">(Wallace et al., 2023a)</ref>. However, unlike the fine-tuning methods designed for large language models, most of the existing methods to a large degree ignore p base and focus solely on the reward model. Reward models can range from standard evaluation metrics such as ClipScore <ref type="bibr" target="#b34">(Hessel et al., 2021;</ref><ref type="bibr" target="#b42">Kirstain et al., 2023)</ref> to specialized models that have been trained on human preferences <ref type="bibr" target="#b68">(Schuhmann and Beaumont, 2022;</ref><ref type="bibr" target="#b91">Xu et al., 2023;</ref><ref type="bibr">Wu et al., 2023c)</ref>. As these are parameterized by neural networks, they fall pray to adversarial examples which lead to the generation of undesirable artifacts <ref type="bibr" target="#b30">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b54">Mordvintsev et al., 2015)</ref>. This has led some works to consider adding regularization during fine-tuning <ref type="bibr" target="#b26">(Fan et al., 2024;</ref><ref type="bibr">Uehara et al., 2024b)</ref> to incentivize staying close to the base model distribution; however, there does not yet exist a simple approach which actually provably generates from the tilted distribution (1).</p><p>The main contributions of our paper are as follows:</p><p>(i) We present a stochastic optimal control (SOC) formulation for reward fine-tuning of dynamical generative models. Importantly, we prove that the naïve approach considered by prior works lead to a value function bias problem that biases the fine-tuned model away from the tilted distribution (1). This problem has also been observed by <ref type="bibr">Uehara et al. (2024b)</ref> but they propose a more complicated solution which involves training a separate generative model for the optimal noise distribution.</p><p>(ii) Instead, we propose a very simple solution: the memoryless noise schedule. This is a unique noise schedule that completely removes the dependency between noise variables and the generated samples, resulting in provable convergence to the tilted distribution. This allows us to fine-tune dynamical generative models in full generality, including being the first to fine-tune noiseless Flow Matching models.</p><p>(iii) We also propose a new method for solving SOC problems, called Adjoint Matching, which combines the scalability of gradient-based methods and the simplicity of a least-squares regression objective. This is orthogonal to the reward fine-tuning application and can be applied to general SOC problems.</p><p>(iv) We perform extensive comparisons to baseline approaches, and analyze them from multiple perspectives such as realism, consistency, and diversity. We find that our proposed method provides generalization to unseen human preference reward models, better text-to-sample consistency, and retains good diversity.</p><p>In the following, sections are broken down as follows: Section 2 summarizes the algorithms used for sampling from pre-trained Flow Matching and diffusion models, while Section 3 provides a common notation that we will use throughout. Sections 4 and 5 form the core of our contributions. Section 4 details the value function bias problem and our proposed solution via the memoryless noise schedule. Section 5 details the new Adjoint Matching algorithm for solving SOC problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries on dynamical generative models</head><p>We are interested in fine-tuning base generative models p base (X 1 ) where samples are generated through the simulation of a stochastic process. That is, these models transform noise variables into a sample through an iterative process. In particular, we discuss the specific constructions and sampling processes of Flow Matching <ref type="bibr" target="#b46">(Lipman et al., 2023;</ref><ref type="bibr" target="#b49">Liu et al., 2023;</ref><ref type="bibr" target="#b48">Liu, 2022;</ref><ref type="bibr">Albergo and Vanden-Eijnden, 2023)</ref> and Denoising Diffusion Models <ref type="bibr" target="#b36">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021b,a)</ref>. The goal of this section is to provide background information on these methods, which we will later unify into a single consistent notation in Section 3.</p><p>Given random variables from an initial distribution X0 ∼ p 0 = N (0, I), and X1 which are distributed according to some data distribution, we define the reference flow X = ( Xt ) t∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> where</p><formula xml:id="formula_1">Xt = β t X0 + α t X1 ,<label>(2)</label></formula><p>where (α t ) t∈[0,1] , (β t ) t∈[0,1] are functions such that α 0 = β 1 = 0 and α 1 = β 0 = 1. Diffusion models and Flow Matching construct generative Markov processes X t with initial distribution X 0 ∼ N (0, I) that result in flows X = (X t ) t∈[0,1] with the same time marginals as the reference flow X, i.e., the random variables X t and Xt have identical distribution for all times t ∈ [0, 1]. This implies X 1 has the same distribution as the data distribution, so simulating the Markov process from random noise X 0 is a way to generate artificial samples 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Flow Matching</head><p>In its simplest form, the generative Markov process of a Flow Matching model is an ordinary differential equation (ODE) of the form:</p><formula xml:id="formula_2">dX t = v(X t , t) dt, X 0 ∼ N (0, I).<label>(3)</label></formula><p>where v(X t , t) is a parametric velocity that is optimized to match the derivative of the reference flow, i.e., v(X t , t) = argmin v E v( Xt , t)-d dt Xt 2 (see e.g. <ref type="bibr" target="#b46">Lipman et al. (2023)</ref> for details on pre-training Flow Matching models). It can then be proven that the solution of the generative process (3) has the same time marginals as the reference flow <ref type="bibr" target="#b46">(Lipman et al., 2023;</ref><ref type="bibr" target="#b48">Liu, 2022;</ref><ref type="bibr">Albergo and Vanden-Eijnden, 2023)</ref>, and a commonly used choice is α t = t and β t = 1 -t. One can also consider a family of stochastic differential equations (SDEs) with an arbitrary state-independent diffusion coefficient 2 :</p><formula xml:id="formula_3">dX t = v(X t , t) + σ(t) 2 2βt( αt α t βt-βt) v(X t , t) -αt αt X t dt + σ(t) dB t , X 0 ∼ N (0, I),<label>(4)</label></formula><p>where (B t ) t≥0 is a Brownian motion. The generative processes in (3) and ( <ref type="formula" target="#formula_3">4</ref>) have the same time marginals. This can be seen by writing down the Fokker-Planck equations for (3) and (4), and observing that they are the same up to a cancellation of terms <ref type="bibr" target="#b52">(Maoutsa et al., 2020)</ref>. The diffusion coefficient σ(t) in ( <ref type="formula" target="#formula_3">4</ref>) is compensated by the second term in the drift which scales proportionally as σ(t) 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Denoising Diffusion Models</head><p>We next discuss diffusion models, in particular the sampling scheme proposed by Denoising Diffusion Implicit Model (DDIM; <ref type="bibr">Song et al. (2021a)</ref>) which we will later relate to Denoising Diffusion Probabilistic Models (DDPM; <ref type="bibr" target="#b36">Ho et al. (2020)</ref>) as a particular case of the former. For sampling from a diffusion model, the DDIM update rule 3 <ref type="bibr">(Song et al. (2021a)</ref>, Eq. 12), typically stated in discrete time with k ∈ {0, . . . , K}, is:</p><formula xml:id="formula_4">X k+1 = √ ᾱk+1 X k - √ 1-ᾱk ϵ(X k ,k) √ ᾱk + 1 -ᾱk+1 -σ 2 k ϵ(X k , k) + σ k ε k , ε k ∼ N (0, I), X 0 ∼ N (0, I), (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where ᾱk is an increasing sequence such that ᾱ0 = 0, ᾱK = 1, and the sequence σ k is arbitrary. That is, one samples an initial Gaussian random variable x 0 , and applies the stochastic update (5) iteratively K times in order to obtain an artificial sample X K . Updates can be interpreted as progressively denoising the iterate:</p><p>x 0 is completely noisy and x K is fully denoised. The noise predictor model ϵ(x k , k) is trained to predict the noise of x k (see e.g. <ref type="bibr" target="#b36">Ho et al. (2020)</ref> for details on pre-training denoising diffusion models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Flow Matching and diffusion models from a common perspective</head><p>We formulate Flow Matching and diffusion models in a unified framework, which we will later use throughout the paper. Firstly, to simplify notation, we will be using continuous-time formulations. This will also directly enable fine-tuning methods inspired by the continuous-time paradigm, which we find tends to perform better than discrete-time counterparts in our empirical validations. Secondly, by consolidating notation, we will be able to discuss fine-tuning of dynamical generative models that follow the same time marginals as the reference flow (2), pre-trained with either the Denoising Diffusion or Flow Matching framework, in full generality.</p><p>To convert DDIM to a continuous-time stochastic process, we can show that the DDIM update rule (5), up to a first-order approximation, is equivalent to the Euler-Maruyama discretization of the following SDE:</p><formula xml:id="formula_6">dX t = αt 2 ᾱt X t -αt 2 ᾱt + σ(t) 2 2 ϵ base (Xt,t) √ 1-ᾱt dt + σ(t)dB t , X 0 ∼ N (0, I).<label>(6)</label></formula><p>See Appendix B.1 for the full derivation. To go from ( <ref type="formula" target="#formula_4">5</ref>) to ( <ref type="formula" target="#formula_6">6</ref>), we assumed a uniform discretization of time, i.e. t = k K . This results in identifying the discrete-time process (X k ) k∈{0,...,K} with a continuous-time process (X t ) t∈[0,1] , where ᾱk := ᾱt , σ k := 1 √ K σ(t), and ϵ(X k , k) with ϵ base (X k , t). In relation to the reference flow (2), the generative process in (6) has the same time marginals when α t = √ ᾱt and β t = √ 1 -ᾱt <ref type="bibr" target="#b36">(Ho et al., 2020)</ref>.</p><p>Furthermore, when viewed up to first order approximations, the DDPM sampling scheme <ref type="bibr" target="#b36">(Ho et al. (2020)</ref>; Algorithm 2) can be seen as special instance of the DDIM sampling scheme when σ(t) = αt /ᾱt. This results in the following generative process:</p><formula xml:id="formula_7">dX t = αt 2 ᾱt X t -αt ᾱt ϵ base (Xt,t) √ 1-ᾱt dt + αt ᾱt dB t , X 0 ∼ N (0, I),<label>(7)</label></formula><p>We can further consolidate notation by converting all quantities to the score function s(x, t)-defined as the gradient of the log density of the random variable X t -which is possible when X 0 is Normal-distributed and under the affine reference flow (2). In particular, the velocity v base from Flow Matching can be expressed in terms of the score function (see Appendix B.4):</p><formula xml:id="formula_8">v base (x, t) = αt αt x + β t ( αt αt β t -βt )s(x, t). (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>And the noise predictor ϵ base also admits an expression in terms of the score function (see Appendix B.3):</p><formula xml:id="formula_10">s(x, t) = -ϵ base (x,t) √ 1-ᾱt .<label>(9)</label></formula><p>Plugging these two equations into (4) and ( <ref type="formula" target="#formula_6">6</ref>), respectively, and rewriting them in terms of only the α t and β t in (2), we can unify both the Flow Matching and continuous-time DDIM generative processes as:</p><formula xml:id="formula_11">dX t = b(X t , t) dt + σ(t) dB t , X 0 ∼ N (0, I),<label>(10)</label></formula><p>where b(x, t)</p><formula xml:id="formula_12">= κ t x + σ(t) 2 2 + η t s(x, t), κ t = αt αt , η t = β t ( αt αt β t -βt )<label>(11)</label></formula><p>where (α t , β t ) are coefficients of the reference flow (2). We have hence expressed the generative process of a base model, whether it is a Flow Matching or a diffusion model, as an SDE of the form ( <ref type="formula" target="#formula_11">10</ref>)-( <ref type="formula" target="#formula_12">11</ref>), unified by the choice of reference flow. This expression has been written before for DDIM, e.g. <ref type="bibr">Bartosh et al. (2024a,b)</ref>.</p><p>4 Fine-tuning as "memoryless" stochastic optimal control</p><p>We now discuss the crux of the problem: how to produce a fine-tuned generative model that produces samples X 1 which follow the tilted distribution involving a reward model (1). An obvious direction is to construct a fine-tuning objective involving both the base generative model and the reward model, where the optimal solution results in a fine-tuned generative model for the tilted distribution. However, as we will explain, this turns out to be non-trivial, because a naïve formulation will introduce bias into the solution.</p><p>In Section 4.1, we discuss the problem formulation of stochastic optimal control, a general framework for optimizing SDEs, and its relation to the maximum entropy reinforcement learning framework commonly used for RLHF fine-tuning. Next, in Section 4.2, we discuss the initial value function bias problem which plagues existing approaches and so far has seen no simple solution. Finally, in Section 4.3, we propose a novel simple solution that circumvents the bias problem, by enforcing a particular diffusion coefficient, the memoryless noise schedule, to be used during fine-tuning. This results in an extremely simple fine-tuning objective that provably converges to a model which generates the tilted distribution (1) without any statistical bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries on the stochastic optimal control problem formulation</head><p>Stochastic optimal control (SOC; <ref type="bibr" target="#b7">Bellman (1957)</ref>; <ref type="bibr" target="#b27">Fleming and Rishel (2012)</ref>; <ref type="bibr" target="#b69">Sethi (2018)</ref>) considers general optimization problems over stochastic differential equations, but we only need to consider a common instantiation, the quadratic cost control-affine problem formulation:</p><formula xml:id="formula_13">min u∈U E 1 0 1 2 ∥u(X u t , t)∥ 2 + f (X u t , t) dt + g(X u 1 ) ,<label>(12)</label></formula><formula xml:id="formula_14">s.t. dX u t = b(X u t , t) + σ(t)u(X u t , t) dt + σ(t)dB t , X u 0 ∼ p 0 (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where in (13), X u t ∈ R d is the state of the stochastic process, u : R d × [0, 1] → R d is commonly referred to as the control vector field, b : R d × [0, 1] → R d is a base drift, and σ : [0, 1] → R d×d is the diffusion coefficient. These jointly define the controlled process X u ∼ p u that we are interested in optimizing; often both b and σ are fixed and we only optimize over the control u.</p><p>As part of the objective functional (12), we have an affine control cost 1 2 ∥u(X u t , t)∥ 2 , a running state cost f : R d × [0, 1] → R and a terminal state cost g : R d → R.</p><p>The stochastic optimal control (SOC) objective ( <ref type="formula" target="#formula_13">12</ref>) can be decomposed recursively from the final time value. It is common to define the cost functional which is the expected future cost starting from state x at time t:</p><formula xml:id="formula_16">J(u; x, t) := E X∼p u 1 t 1 2 ∥u(X s , s)∥ 2 + f (X s , s) ds + g(X 1 ) X t = x .<label>(14)</label></formula><p>From here, the value function is the optimal value of the cost functional<ref type="foot" target="#foot_0">foot_0</ref> :</p><formula xml:id="formula_17">V (x, t) := min u∈U J(u; x, t) = J(u * ; x, t),<label>(15)</label></formula><p>where u * is the optimal control, i.e., minimizer of (12). Furthermore, a classical result is that the value function can be expressed in terms of the uncontrolled base process p base <ref type="bibr" target="#b39">(Kappen (2005)</ref>, see Domingo-Enrich et al. 2023, Eq. 8, App. B for a self-contained proof):</p><formula xml:id="formula_18">V (x, t) = -log E X∼p base exp(- 1 t f (X s , s)ds -g(X 1 )) X t = x .<label>(16)</label></formula><p>A useful expression for the optimal control (which we will make use of in deriving the Adjoint Matching objective in Section 5) is that it is related to the gradient of the value function:</p><formula xml:id="formula_19">u * (x, t) = -σ(t) ⊤ ∇ x V (x, t) = -σ(t) ⊤ ∇ x J(u * , x, t). (<label>17</label></formula><formula xml:id="formula_20">)</formula><p>Relation to MaxEnt RL. Stochastic optimal control with the control-affine formulation (12) is the continuoustime equivalence of maximum entropy reinforcement learning (MaxEnt RL; <ref type="bibr" target="#b78">Todorov (2006)</ref>; <ref type="bibr" target="#b96">Ziebart et al. (2008)</ref>) with a KL regularization instead of only an entropy regularization. In particular, by the Girsanov theorem (Theorem 2), the affine control cost is equivalent to a Kullback-Leibler (KL) divergence between the base process p base , when u = 0, and the controlled process p u , when conditioned on the same initial state X 0 (see Appendix C.4):</p><formula xml:id="formula_21">D KL p u (X|X 0 ) p base (X|X 0 ) = E X u ∼p u 1 0 1 2 ∥u(X u t , t)∥ 2 dt ,<label>(18)</label></formula><p>resulting in the KL-regularized RL interpretation of (12):</p><formula xml:id="formula_22">max u∈U E X0∼p0 E X∼p u (•|X0) 1 0 -f (X u t , t)dt -g(X u 1 ) -D KL (p u (X|X 0 ) ∥ p base (X|X 0 )) ,<label>(19)</label></formula><p>where the negative state costs correspond to intermediate and terminal rewards in the RL interpretation. The KL divergence incentivizes the optimal solution to stay close to the distribution of the base process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The initial value function bias problem</head><p>We next discuss why naïvely adding a KL regularization does not lead to the tilted distribution (1). From ( <ref type="formula" target="#formula_22">19</ref>), we can also show that the optimal distribution conditioned on X 0 is<ref type="foot" target="#foot_1">foot_1</ref> </p><formula xml:id="formula_23">p * (X|X 0 ) ∝ p base (X|X 0 ) exp - 1 0 f (X t , t) dt -g(X 1 ) . (<label>20</label></formula><formula xml:id="formula_24">)</formula><p>This is analogous to the exponentiated reward distribution in MaxEnt RL <ref type="bibr" target="#b62">(Rawlik et al., 2013)</ref>, but since we generalize the entropy regularization to a KL regularization, p base acts as a prior distribution.</p><p>In order to relate this to the tilted distribution (1) that we want to achieve for fine-tuning, first notice that the normalization constant of the right-hand side (RHS) of ( <ref type="formula" target="#formula_23">20</ref>) is exactly the value function at t = 0:</p><formula xml:id="formula_25">E X∼p base (X|X0) exp - 1 0 f (X t , t) dt -g(X 1 ) = exp -V (X 0 , 0) ,<label>(21)</label></formula><p>where the equality is due to (16). Dividing the RHS of ( <ref type="formula" target="#formula_23">20</ref>) by ( <ref type="formula" target="#formula_25">21</ref>) and multiplying by p 0 (X 0 ), we obtain the normalized distribution over the full path X,</p><formula xml:id="formula_26">p * (X) = p base (X) exp - 1 0 f (X t , t) dt -g(X 1 ) + V (X 0 , 0) . (<label>22</label></formula><formula xml:id="formula_27">)</formula><p>Setting f = 0 and g = -r, we arrive at an expression for the optimal distribution</p><formula xml:id="formula_28">p * (X 0 , X 1 ) = p base (X 0 , X 1 ) exp r(X 1 ) + V (X 0 , 0) . (<label>23</label></formula><formula xml:id="formula_29">)</formula><p>This unfortunately does not lead to the tilted distribution (1) because we have a bias in the optimal distribution that is due to the value function of the initial distribution V (X 0 , 0). That is to say, naïvely adding a KL regularization (18) to the fine-tuning objective in the sense of ( <ref type="formula" target="#formula_22">19</ref>) leads to a biased distribution ( <ref type="formula" target="#formula_26">22</ref>) after fine-tuning and is not equivalent to the tilted distribution (1). For instance, when the sampling procedure is noiseless, i.e., σ(t) = 0, fine-tuning naïvely will not have any effect because X 0 completely determines X 1 . This is unlike the situation for large language models <ref type="bibr" target="#b56">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b61">Rafailov et al., 2023)</ref>, where there is no dynamical process that samples X 1 iteratively and hence no dependence on the initial noise variable X 0 . Although this KL regularization is a common objective for RLHF of large language models, it has seen seldom use in fine-tuning diffusion models, likely due to this issue of the initial value function bias.</p><p>In the context of diffusion models, KL regularization (19) has been explored in prior works <ref type="bibr" target="#b26">(Fan et al., 2024)</ref>, but its behavior was not well-understood and they did not relate the fine-tuned model to the tilted distribution (1). Another direction that has been proposed is to learn the initial distribution p 0 to cancel out the bias <ref type="bibr">(Uehara et al., 2024b;</ref><ref type="bibr" target="#b77">Tang, 2024)</ref> but this simply shifts the work into tilting the initial distribution and requires an auxiliary model for parameterizing the optimal initial distribution. In contrast, we show in the next section that it is possible to remove the value function bias by simply choosing a very particular noise schedule during the fine-tuning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The memoryless noise schedule for fine-tuning dynamical generative models</head><p>In this section, we propose a very simple method of turning (23) into the tilted distribution (1) through the use of a particular memoryless noise schedule. Throughout, we provide an intuitive explanation of why this noise schedule is sufficient for fine-tuning while discussing the full theoretical result where we show that the memoryless noise schedule is actually not only sufficient but also necessary.</p><p>Intuitively, the main reason we cannot arrive at the tilted distribution from ( <ref type="formula" target="#formula_28">23</ref>) is due to the p base (X 0 , X 1 ) distribution not factoring into X 0 and X 1 . Hence, we define a memoryless generative process as follows:</p><p>Definition 1 (Memoryless generative process). A generative process of the form (10)-( <ref type="formula" target="#formula_12">11</ref>) is memoryless if X 0 and X 1 are independent, i.e., p base (X 0 , X 1 ) = p base (X 0 )p base (X 1 ). </p><formula xml:id="formula_30">(t) = √ 2ηt</formula><p>, the generative process is memoryless, i.e., samples X1 will be independent of the initial noise X0.</p><p>When the base generative process is memoryless, this implies:</p><formula xml:id="formula_31">p * (X 1 ) = p base (X 0 )p base (X 1 ) exp(r(X 1 ) + V (X 0 , 0))dX 0 ∝ p base (X 1 ) exp(r(X 1 )). (<label>24</label></formula><formula xml:id="formula_32">)</formula><p>That is, solving the SOC problem ( <ref type="formula" target="#formula_13">12</ref>)-( <ref type="formula" target="#formula_14">13</ref>) with a memoryless base model will result in a fine-tuned model that generates samples p * (X 1 ) according to the tilted distribution (1). This memoryless property is not satisfied generally by the family of generative processes captured by ( <ref type="formula" target="#formula_13">12</ref>)-( <ref type="formula" target="#formula_14">13</ref>). For instance, the Flow Matching and DDIM generative processes with zero diffusion coefficient (i.e., σ(t) = 0) are definitely not memoryless due to X 0 and X 1 being theoretically invertible. Below, we provide the sufficient and neccessary condition for the noise schedule in order to have a memoryless generative process.</p><p>Proposition 1 (Memoryless noise schedules). Within the family of generative processes (10)-( <ref type="formula" target="#formula_12">11</ref>), a generative process is memoryless if and only if the noise schedule is chosen as:</p><formula xml:id="formula_33">σ(t) 2 = 2η t + χ(t), where χ : [0, 1] → R is s.t. ∀t ∈ (0, 1], lim t ′ →0 + α t ′ exp - t t ′ χ(s) 2β 2 s ds = 0. (<label>25</label></formula><formula xml:id="formula_34">)</formula><p>where η t is the coefficient defined in (11) (see also Table <ref type="table" target="#tab_0">1</ref>). In particular, we refer to σ(t) = √ 2η t as the memoryless noise schedule.</p><p>Due to the endpoint constraints of (α t , β t ) for the reference flow (2), the memoryless noise schedule σ(t) is infinite at t = 0 and approaches zero at t = 1. This provides a way for the generative process to mix when close to noise X 0 while stay steadying when close to the sample X 1 . Hence, the sample will have no information about X 0 due to the enormous amount of mixing with a large diffusion coefficient. Furthermore, while we have intuitively justified the memoryless noise schedule through its independence property, our theoretical result is actually even stronger: all generative models of the form (10)-( <ref type="formula" target="#formula_12">11</ref>) must be fine-tuned using the memoryless noise schedule. We formalize this in the following theorem, which we prove in Appendix D.2:</p><p>Theorem 1 (Fine-tuning recipe for general noise schedule sampling). Within the family of generative processes (10)-( <ref type="formula" target="#formula_12">11</ref>), in order to allow the use of arbitrary noise schedules and still generate samples according to the tilted distribution (1), the fine-tuning problem (12)-( <ref type="formula" target="#formula_14">13</ref>) with f = 0 and g = -r must be done with the memoryless noise schedule σ(t) = √ 2η t .</p><p>Theorem 1 states that we need to use the memoryless noise schedule for fine-tuning with the SOC objectiveor equivalently, the KL regularized reward objective (19). This is the only noise schedule that retains the relationship between the velocity and score function, allowing the conversion to arbitrary noise schedules (e.g., σ(t) = 0) after fine-tuning. It is worth noting that when using the memoryless noise schedule for DDIM, this recovers what we derived as the continuous-time limit of the DDPM generative process (7). However, the DDPM sampler <ref type="bibr" target="#b36">(Ho et al., 2020)</ref> is not commonly used while the DDIM sampler <ref type="bibr">(Song et al., 2021a)</ref> and Flow Matching models typically generate samples using σ(t) = 0, so an explicit conversion to the memoryless noise schedule is necessary for fine-tuning. To the best of our knowledge, we are not aware of any existing works that have proposed a time-varying diffusion coefficient with theoretical guarantees. Table <ref type="table" target="#tab_0">1</ref> summarizes the memoryless schedule for diffusion and Flow Matching models, which we refer to as Memoryless Flow Matching. In Figure <ref type="figure">2</ref>, we visualize fine-tuning a 1D model, where we see that constant σ(t) leads to biased distributions whereas the memoryless noise schedule perfectly converges to the tilted distribution (1). For convenience, we plug the memoryless noise schedule into the controlled process for fine-tuning (13), and express them in terms of each respective framework. Let ϵ base , v base denote the pre-trained vector fields and ϵ finetune , v finetune the fine-tuned vector fields.</p><p>Then we have the following expressions for the full drift b(x, t) + σ(t)u(x, t) and control u(x, t) when σ(t) = √ 2η t : DDIM / DDPM : b(x, t) + σ(t)u(x, t) = αt 2 ᾱt x -αt ᾱt ϵ finetune (x,t) √ 1-ᾱt , u(x, t) = -αt ᾱt(1-ᾱt) (ϵ finetune (x, t) -ϵ base (x, t)). (26) Memoryless Flow Matching: b(x, t) + σ(t)u(x, t) = 2v finetune (x, t) -αt αt x, u(x, t) = 2 βt( αt α t βt-βt)</p><formula xml:id="formula_35">(v finetune (x, t) -v base (x, t)). (27)</formula><p>Thus, to solve the SOC problem (12)-( <ref type="formula" target="#formula_14">13</ref>) in practice, we parameterize the control u in terms of ϵ finetune or v finetune and optimize these vector fields instead. After plugging in ( <ref type="formula">26</ref>)-( <ref type="formula">27</ref>), the SOC problem ( <ref type="formula" target="#formula_13">12</ref>)-( <ref type="formula" target="#formula_14">13</ref>) can then be solved using any SOC algorithm in order to perform fine-tuning, and we proposed an especially effective algorithm next in Section 5. After fine-tuning, ϵ finetune and v finetune can simply be plugged back into their respective generative processes (3)-( <ref type="formula" target="#formula_7">7</ref>) to sample from the tilted distribution (1) using any choice of diffusion coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Adjoint Matching for control-affine stochastic optimal control</head><p>We discuss existing methods and also propose a new method for optimizing control-affine SOC problems. The new Adjoint Matching method is a combination of the time-tested continuous adjoint method <ref type="bibr" target="#b59">(Pontryagin, 1962)</ref> with recent developments on constructing least-squares objectives for solving SOC problems <ref type="bibr" target="#b21">(Domingo-Enrich et al., 2023)</ref>. In this section, we briefly discuss preliminaries on existing methods, their pros and cons, then detail the Adjoint Matching algorithm and its surprising connections to the prior methods. For numerical optimization, we now assume that the control u is a parametric model with parameters θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Existing methods for stochastic optimal control</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">The adjoint method</head><p>The most basic method of optimizing the simulation of an SDE is to directly differentiate through the simulation using gradients from the SOC objective function <ref type="bibr" target="#b32">(Han and E, 2016)</ref>. The adjoint method simply uses the objective:</p><formula xml:id="formula_36">L(u; X) := 1 0 1 2 ∥u(X t , t)∥ 2 +f (X t , t) dt+g(X 1 ), X ∼ p u . (<label>28</label></formula><formula xml:id="formula_37">)</formula><p>This is a stochastic estimate of the control objective in (12), and the goal is to take compute the gradient of L(u; X) with respect to the parameters θ of the control u. Due to the continuous-time nature of SDEs, there are two main approaches to implementing this numerically. Firstly, the Discrete Adjoint method uses a "discretize-then-differentiate" approach, where the numerical solver for simulating the SDE is simply stored in memory then differentiated through, and it has been studied extensively (e.g., <ref type="bibr" target="#b10">Bierkens and Kappen (2014)</ref>; <ref type="bibr" target="#b29">Gómez et al. (2014)</ref>; <ref type="bibr" target="#b33">Hartmann and Schütte (2012)</ref>; <ref type="bibr" target="#b40">Kappen et al. (2012)</ref>; <ref type="bibr" target="#b62">Rawlik et al. (2013)</ref>; <ref type="bibr" target="#b31">Haber and Ruthotto (2017)</ref>). This approach, however, uses an extremely large amount of memory as the full computational graph of the numerical solver must be stored in memory and implementations often must rely on gradient checkpointing <ref type="bibr" target="#b15">(Chen et al., 2016)</ref> to reduce memory usage.</p><p>Secondly, the Continuous Adjoint method exploits the continuous-time nature of SDEs and uses an analytical expression for the gradient of the control objective with respect to the intermediate states X t , expressed as an adjoint ODE, and then applies a numerical method to simulate this gradient itself, hence it is referred to as a "differentiate-then-discretize" approach <ref type="bibr" target="#b59">(Pontryagin, 1962;</ref><ref type="bibr" target="#b13">Chen et al., 2018;</ref><ref type="bibr" target="#b45">Li et al., 2020)</ref>. We first define the adjoint state as:</p><formula xml:id="formula_38">a(t; X, u) := ∇ Xt 1 t 1 2 ∥u(X t ′ , t ′ )∥ 2 +f (X t ′ , t ′ ) dt ′ +g(X 1 ) , where X solves dX t = b(X t , t) + σ(t)u(X t , t) dt + σ(t)dB t .<label>(29)</label></formula><p>This implies that E X∼p u a(t; X, u) | X t = x = ∇ x J(u; x, t), where J denotes the cost functional defined in (14). It can then be shown that this adjoint state satisfies<ref type="foot" target="#foot_2">foot_2</ref> :</p><formula xml:id="formula_39">d dt a(t; X, u) = -a(t; X, u) T ∇ Xt (b(X t , t) + σ(t)u(X t , t)) + ∇ Xt f (X t , t) + 1 2 ∥u(X t , t)∥ 2 ,<label>(30)</label></formula><p>a(1; X, u) = ∇g(X 1 ).</p><p>The adjoint state is solved backwards in time, starting from the terminal condition (31). Computation of (30) can be efficiently done as a vector-Jacobian product on automatic differentiation software <ref type="bibr" target="#b57">(Paszke et al., 2019)</ref>. Once the adjoint state has been solved for t ∈ [0, 1], then the gradient of L(u; X) with respect to the parameters θ can be obtained by integrating over the entire time interval:</p><formula xml:id="formula_41">dL dθ = 1 2 1 0 ∂ ∂θ ∥u(X t , t)∥ 2 dt + 1 0 ∂u(Xt,t) ∂θ T σ(t) T a(t; X, u)dt,<label>(32)</label></formula><p>where the first term is the partial derivative of L w.r.t. θ and the second term is the partial derivative through the sample trajectory X. See Proposition 6 in Appendix E.1 for a statement and proof of this result. The discrete and continuous adjoint methods converge to the same gradient as the step size of the numerical solvers go to zero. Both are scalable to high dimensions and have seen their fair share of usage in optimizing neural ODE/SDEs <ref type="bibr" target="#b13">(Chen et al., 2018</ref><ref type="bibr" target="#b14">(Chen et al., , 2021;;</ref><ref type="bibr" target="#b45">Li et al., 2020)</ref>. As the adjoint methods are essentially gradient-based optimization algorithms applied on a highly non-convex problem, many have also reported they can be unstable empirically <ref type="bibr" target="#b53">(Mohamed et al., 2020;</ref><ref type="bibr" target="#b76">Suh et al., 2022;</ref><ref type="bibr" target="#b21">Domingo-Enrich et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Importance-weighted matching objectives for regressing onto the optimal control</head><p>An alternative is to consider regressing onto the optimal control u * , which is the approach of the cross-entropy method <ref type="bibr" target="#b67">(Rubinstein and Kroese, 2013;</ref><ref type="bibr" target="#b94">Zhang et al., 2014)</ref> and stochastic optimal control matching (SOCM; Domingo-Enrich et al. ( <ref type="formula">2023</ref>)). These methods make use of path integral theory <ref type="bibr" target="#b39">(Kappen, 2005)</ref> to express the optimal control through importance sampling, resulting in an importance-weighted least-squares objective function</p><formula xml:id="formula_42">L SOCM (u; X) := 1 0 ∥u(X t , t) -û * (X t , t)∥ 2 dt × ω(u, X), X ∼ p u , (<label>33</label></formula><formula xml:id="formula_43">)</formula><p>where ω is an importance weighting that approximates sampling from the optimal distribution p * , and û * is a stochastic estimator of the optimal control relying on having sampled from the optimal process. We defer to <ref type="bibr" target="#b21">Domingo-Enrich et al. (2023)</ref> for the exact details. The functional landscape of this objective is convex, which is argued to help yield stable training. However, the need for importance sampling renders this impractical for high dimensional applications: the variance of the importance weighting ω grows exponentially with dimension of the stochastic process, leading to catastrophic failure. This unfortunately means that such importance-weighted matching objectives are impractical for fine-tuning dynamical generative models; however, a least-squares objective is greatly coveted as it can lead to stable training and simple interpretations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Adjoint Matching</head><p>We make two important observations which lead to our proposed method: (i ) it is possible to construct a matching objective without any importance weighting, and (ii ) there are unnecessary terms in the adjoint differential equation ( <ref type="formula" target="#formula_39">30</ref>) that can lead to higher variance at convergence.</p><p>Firstly, we notice that we can simply match the gradient of the cost functional under the current control.</p><p>That is, while SOCM carefully constructs an importance-weighted estimator of the optimal control u * = -σ(t) T ∇J(u * ; x, t) (17), we claim that we can actually just regress onto the target vector field -σ(t) T ∇J(u; x, t) where u is the current control, and furthermore, this results in a gradient equal in expectation to the continuous adjoint method. We formalize this in the following proposition, proven in Appendix E.2:</p><p>Proposition 2. Let us define, for now, the basic Adjoint Matching objective as:</p><formula xml:id="formula_44">L Basic-Adj-Match (u; X) := 1 2 1 0 u(X t , t) + σ(t) T a(t; X, ū) 2 dt, X ∼ p ū, ū = stopgrad(u),<label>(34)</label></formula><p>where ū = stopgrad(u) means that the gradients of ū with respect to the parameters θ of the control u are artificially set to zero. The gradient of L Basic-Adj-Match (u; X) with respect to θ is equal to the gradient dL dθ in equation (32). Importantly, the only critical point of E L Basic-Adj-Match is the optimal control u * .</p><p>Critical points of L are controls u such that δ δu L(u) = 0, where δ δu L denotes the first variation of the functional L. In other words, Proposition 2 states that the only control that satisfies the first-order optimality condition for the basic Adjoint Matching objective is the optimal control, which provides theoretical grounding for gradient-based optimization algorithms.</p><p>An intuitive way to understand the basic Adjoint Matching objective is that it is a consistency loss. The Adjoint Matching objective is based off of the observation that the optimal control u * (x, t) is the unique fixed-point of the relation u(x, t) = -σ(t) T ∇ x J(u; x, t) (see Lemma 6 in Appendix E.2) and so we are directly optimizing for a control that fits this relation, while using the adjoint state as a stochastic estimator of ∇ x J(u; x, t) (29).</p><p>The basic Adjoint Matching objective in Proposition 2 does not yet yield a novel algorithm for stochastic optimal control, because it produces the same gradient as the continuous adjoint method. This can be seen by taking the gradient w.r.t. θ after expanding the square in (34) and removing terms that do not depend on θ to arrive exactly at the continuous adjoint method (32). However, it provides the means of deriving a simpler leaner objective function.</p><p>The "Lean" Adjoint. The minimizer of a least-squares objective is the conditional expectation of the regression target, so for the Adjoint Matching objective, at the optimum we have that</p><formula xml:id="formula_45">u * (x, t) = E X∼p * -σ(t) T a(t; X, u * )|X t = x . (<label>35</label></formula><formula xml:id="formula_46">)</formula><p>Multiplying both sides by the Jacobian ∇ x u * (x, t) and re-arranging, we get the relation</p><formula xml:id="formula_47">E X∼p * u * (x, t) T ∇ x u * (x, t) + a(t; X, u * ) T σ(t)∇ x u * (x, t) | X t = x = 0. (<label>36</label></formula><formula xml:id="formula_48">)</formula><p>Algorithm 1 Adjoint Matching for fine-tuning Flow Matching models</p><p>Input: Pre-trained FM velocity field v base , step size h, number of fine-tuning iterations N . Initialize fine-tuned vector fields: v finetune = v base with parameters θ.</p><p>for n ∈ {0, . . . , N -1} do Sample m trajectories X = (Xt) t∈{0,...,1} with memoryless noise schedule σ(t) = 2βt( αt α t βt -βt), e.g.:</p><formula xml:id="formula_49">X t+h = Xt + h 2v finetune θ (Xt, t) -αt α t Xt + √ hσ(t)εt, εt ∼ N (0, I), X0 ∼ N (0, I). (<label>40</label></formula><formula xml:id="formula_50">)</formula><p>For each trajectory, solve the lean adjoint ODE (38)-( <ref type="formula" target="#formula_56">39</ref>) backwards in time from t = 1 to 0, e.g.:</p><formula xml:id="formula_51">ãt-h = ãt + hã T t ∇X t 2v base (Xt, t) -αt α t Xt , ã1 = -∇X 1 r(X1).<label>(41)</label></formula><p>Note that Xt and ãt should be computed without gradients, i.e., Xt = stopgrad(Xt), ãt = stopgrad(ãt).</p><p>For each trajectory, compute the Adjoint Matching objective (37):</p><formula xml:id="formula_52">L Adj-Match (θ) = t∈{0,...,1-h} 2 σ(t) v finetune θ (Xt, t) -v base (Xt, t) + σ(t)ãt 2 . (<label>42</label></formula><formula xml:id="formula_53">)</formula><p>Compute the gradient ∇ θ L(θ) and update θ using favorite gradient descent algorithm. end Output: Fine-tuned vector field v finetune Notice that the terms inside the expectation in (36) show up as part of the adjoint differential equation ( <ref type="formula" target="#formula_39">30</ref>), which we have now shown to have expectation zero at the optimal solution. Therefore, we motivate the definition of a lean adjoint state ã with the terms in (36) removed. Plugging this lean adjoint back into the least-squares objective, we obtain our final proposed Adjoint Matching objective:</p><formula xml:id="formula_54">L Adj-Match (u; X) := 1 2 1 0 u(X t , t) + σ(t) T ã(t; X) 2 dt, X ∼ p ū, ū = stopgrad(u),<label>(37)</label></formula><p>where</p><formula xml:id="formula_55">d dt ã(t; X) = -(ã(t; X) ⊤ ∇ x b(X t , t) + ∇ x f (X t , t)),<label>(38)</label></formula><formula xml:id="formula_56">ã(1; X) = ∇ x g(X 1 ).<label>(39)</label></formula><p>Equations ( <ref type="formula" target="#formula_55">38</ref>)-( <ref type="formula" target="#formula_56">39</ref>) define the lean adjoint state, and (37) is the complete Adjoint Matching objective. The unique critical point of E[L Adj-Match ] is the optimal control, which we prove relying on Proposition 2 and equation (36) (see Proposition 7 in Appendix E.3).</p><p>Compared to the importance sampling methods (Section 5.1.2), Adjoint Matching is a simple least-squares regression objective and has no importance weighting. This allows it to avoid the pitfalls of high variance importance weights and makes it as scalable as the adjoint methods while retaining the interpretation of matching a target vector field.</p><p>Compared to the adjoint method (Section 5.1.1), Adjoint Matching produces a different gradient in expectation than the continuous adjoint. This is because the lean adjoint state is not related to the gradient of the cost functional anymore, i.e., (29) is not true, except at the optimum when u = u * . Even at the optimal solution, since Adjoint Matching removes terms that have expectation zero, it can potentially exhibit better convergence and lower variance than the continuous adjoint method. Additionally, computation of the lean adjoint state (38) also exhibits a smaller computational cost due to the removal of the extra terms (no longer need the Jacobian of the control ∇ x u). We provide a rigorous derivation of Adjoint Matching and the above claims in Appendix E.3.</p><p>Adjoint Matching can be applied to reward fine-tuning of dynamical generative models through the memoryless SOC formulation discussed in Section 4. We provide pseudo-code for this in Algorithm 1 for Flow Matching models and in Algorithm 2 in Appendix E.4 for denoising diffusion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Fine-tuning from human feedback. There are two main overarching approaches to RLHF: the reward-based approach <ref type="bibr">(Ziegler et al., 2020;</ref><ref type="bibr" target="#b75">Stiennon et al., 2020;</ref><ref type="bibr" target="#b56">Ouyang et al., 2022;</ref><ref type="bibr" target="#b4">Bai et al., 2022)</ref> and direct preference optimization (DPO; <ref type="bibr" target="#b61">Rafailov et al. (2023)</ref>). The reward-based approach <ref type="bibr">(Ziegler et al., 2020;</ref><ref type="bibr" target="#b75">Stiennon et al., 2020;</ref><ref type="bibr" target="#b56">Ouyang et al., 2022;</ref><ref type="bibr" target="#b4">Bai et al., 2022)</ref> consists in learning the reward model r(x) from human preference data, and then solving a maximum entropy RL problem with rewards produced by r(x). DPO merges the two previous steps into one: there is no need to learn r(x) as human preference data is directly used to fine-tune the model. However, DPO is typically only applied with a filtered dataset, and does not work explicitly with a reward model. Furthermore, for flow and diffusion models specifically, it is possible to differentiate the reward function, so there is a larger emphasis on reward-based approaches.</p><p>Fine-tuning for diffusion models. Among existing reward-based diffusion fine-tuning methods, Fan and Lee (2023) interpret the denoising process as a multi-step decision-making task and use policy gradient algorithms to fine-tune diffusion samplers. <ref type="bibr" target="#b11">Black et al. (2024)</ref> makes use of proximal policy gradients for fine-tuning but this does not make use of the differentiability of the reward model.</p><p>Fan et al. (2023) also consider KL-regularized rewards (19) but do not make the critical connection to the tilted distribution (1) that we flesh out in Section 4.2. The fine-tuning algorithms of Xu et al. (2023); Clark et al. (2024) directly take gradients of the reward model and use heuristics to try to stay close to the original base generative model, but their behavior is not well understood and unrelated to the tilted distribution: Xu et al. (2023) takes gradients of the reward applied on the denoised sample at different points in time, and Clark et al. (2024) backpropagates the reward function through all or part of the diffusion trajectory. Finally, Uehara et al.</p><p>(2024b) also fine-tune diffusion models with the goal of sampling from the tilted distribution (1), but their approach is much more involved than ours as it requires learning a value function, and solving two stochastic optimal control problems. Additional reward fine-tuning works include <ref type="bibr" target="#b12">Bruna and Han (2024)</ref>, that provide theoretical guarantees to sample from the tilted distribution when the reward is a quadratic function, and <ref type="bibr" target="#b92">Zhang et al. (2024)</ref>, that propose a reward fine-tuning algorithm for the GFlowNet architecture.</p><p>Inference-time optimization methods. Some have proposed methods that do not update the base model but instead modify the generation process directly. One approach is to add a guidance term to the velocity <ref type="bibr" target="#b16">(Chung et al., 2022;</ref><ref type="bibr" target="#b72">Song et al., 2023;</ref><ref type="bibr" target="#b58">Pokle et al., 2023)</ref>; however, this is a heuristic and it is not well-understood what particular distribution is being generated. Another approach is to directly optimize the initial noise distribution <ref type="bibr" target="#b44">(Li, 2021;</ref><ref type="bibr">Wallace et al., 2023b;</ref><ref type="bibr" target="#b8">Ben-Hamu et al., 2024)</ref>; this is taking an opposite approach to the inital value bias problem than us by moving all of the work into optimizing the initial distribution. A more computationally intensive approach is to perform online estimation of the optimal control, for the purpose of heuristically solving an optimal control problem within the sampling process <ref type="bibr" target="#b37">(Huang et al., 2024;</ref><ref type="bibr" target="#b66">Rout et al., 2024)</ref>; these approaches aim to solve a separate control problem for each generated sample, instead of performing amortization <ref type="bibr" target="#b2">(Amos et al., 2023)</ref> to learn a fine-tuned generative model.</p><p>Optimal control in generative modeling. Methods from optimal control have been used to train dynamical generative models parameterized by ODEs <ref type="bibr" target="#b13">(Chen et al., 2018)</ref>, SDEs <ref type="bibr" target="#b45">(Li et al., 2020)</ref>, and jump processes <ref type="bibr" target="#b14">(Chen et al., 2021)</ref>, enabled through the adjoint method. They can be used to train arbitrary generative processes, but for simplified constructions these have fallen in favor of simulation-free matching objectives such as denoising score matching <ref type="bibr" target="#b84">(Vincent, 2011)</ref> and Flow Matching <ref type="bibr" target="#b46">(Lipman et al., 2023)</ref>. The optimal control formalism also has significance in sampling from un-normalized distributions <ref type="bibr" target="#b93">(Zhang and Chen, 2022;</ref><ref type="bibr" target="#b9">Berner et al., 2023;</ref><ref type="bibr" target="#b83">Vargas et al., 2023</ref><ref type="bibr" target="#b82">Vargas et al., , 2022;;</ref><ref type="bibr" target="#b63">Richter and Berner, 2024;</ref><ref type="bibr" target="#b79">Tzen and Raginsky, 2019)</ref>. The inclusion of a state cost has been used to solve transport problems where intermediate path distributions are of importance <ref type="bibr" target="#b47">(Liu et al., 2024;</ref><ref type="bibr" target="#b60">Pooladian et al., 2024)</ref>. These collective advances naturally lead to the consideration of the optimal control formalism for reward fine-tuning.</p><p>Conditional sampling in inverse problems. <ref type="bibr" target="#b19">Denker et al. (2024)</ref> and <ref type="bibr">Wu et al. (2023a)</ref> independently consider a pre-trained diffusion model p(x), and an observation y on the generated sample x, as well as the analytic likelihood p(y|x). Their aim is to sample from the posterior p(x)p(y|x), and their applications include inpainting, class-conditional generation, super-resolution, phase retrieval, non-linear deblurring, computed</p><p>Fine-tuning Fine-tuning Sampling ClipScore ↑ PickScore ↑ HPS v2 ↑ DreamSim Method σ(t) σ(t) Diversity ↑ None N/A √ 2ηt 24.15±0.26 17.25±0.06 16.19±0.17 53.60±1.37 (Base model) 0 28.32±0.22 18.15±0.07 17.89±0.16 56.53±1.52 Baselines DRaFT-1 √ 2ηt √ 2ηt 30.18±0.24 19.38±0.08 24.61±0.17 25.54±0.99 0 0 30.95±0.28 19.37±0.06 24.37±0.17 27.39±1.14 DRaFT-40 √ 2ηt √ 2ηt 26.94±0.28 18.34±0.19 19.98±1.02 41.98±2.14 0 0 30.07±0.39 19.45±0.08 24.06±0.24 36.53±1.69 DPO √ 2ηt √ 2ηt 24.11±0.22 17.24±0.06 16.15±0.14 53.27±1.36 0 0 27.77±0.18 17.92±0.07 17.30±0.20 54.11±1.50 ReFL √ 2ηt √ 2ηt 28.59±0.31 18.68±0.10 22.24±0.46 32.71±2.76 0 0 30.06±0.63 19.07±0.21 23.06±0.41 32.69±1.28 Memoryless SOC Cont. Adjoint √ 2ηt √ 2ηt 26.99±0.43 18.33±0.16 20.83±0.63 46.59±1.40 λ = 12500 0 29.49±0.32 18.98±0.16 21.34±0.53 48.41±1.44 Disc. Adjoint √ 2ηt √ 2ηt 28.04±0.57 18.44±0.21 20.04±0.39 54.90±2.03 λ = 12500 0 29.28±0.17 18.82±0.14 19.73±0.17 53.36±2.48 Adj.-Matching √ 2ηt √ 2ηt 30.36±0.22 19.29±0.08 24.12±0.17 40.89±1.50 λ = 1000 0 31.41±0.22 19.57±0.09 23.29±0.18 43.10±1.76 Adj.-Matching √ 2ηt √ 2ηt 30.59±0.40 19.49±0.10 24.85±0.23 37.07±1.47 λ = 2500 0 31.64±0.21 19.71±0.09 24.12±0.27 39.88±1.59 Adj.-Matching √ 2ηt √ 2ηt 30.62±0.30 19.50±0.09 24.95±0.28 34.50±1.33 λ = 12500 0 31.65±0.19 19.76±0.08 24.49±0.27 37.24±1.57</p><p>Table <ref type="table">2</ref> Evaluation metrics of different fine-tuning methods for text-to-image generation. The second and third columns show the noise schedules σ(t) used for fine-tuning and for sampling: σ(t) = √ 2ηt corresponds to Memoryless Flow Matching, and σ(t) = 0 to the Flow Matching ODE (3). We report standard errors estimated over 3 runs of the fine-tuning algorithm on random sets of 40000 training prompts, each evaluated over a random set of 1000 test prompts.</p><p>tomography, and protein design. Their setting reduces to a particular case of our reward fine-tuning framework by setting r(x) = log p(y|x). <ref type="bibr" target="#b19">Denker et al. (2024)</ref> formulate an SOC problem, and they solve it via the log-variance loss <ref type="bibr" target="#b64">(Richter et al. (2020)</ref>; <ref type="bibr" target="#b55">Nüsken and Richter (2021)</ref>), and the moment loss <ref type="bibr" target="#b55">(Nüsken and Richter, 2021)</ref> <ref type="foot" target="#foot_3">foot_3</ref> , which they refer to as the trajectory balance loss <ref type="bibr" target="#b51">(Malkin et al., 2023)</ref>. <ref type="bibr">Wu et al. (2023a)</ref> propose Twisted Diffusion Sampler, an algorithm based on Sequential Monte Carlo that uses increased inference-time compute to reduce bias. A third work that also tackles the conditional sampling problem is <ref type="bibr" target="#b22">Du et al. (2024)</ref>, which use a Lagrangian formulation that they solve approximately using Gaussian paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We experimentally validate our proposed method on reward fine-tuning a Flow Matching base model <ref type="bibr" target="#b46">(Lipman et al., 2023)</ref>. In particular, we use the usual setup of pre-training an autoencoder for 512×512 resolution images, then training a text-conditional Flow Matching model on the latent variables with a U-net architecture <ref type="bibr" target="#b50">(Long et al., 2015)</ref>, similar to the setup in <ref type="bibr" target="#b65">Rombach et al. (2022)</ref>. We pre-trained our base model using a dataset of licensed text and image pairs. Then for fine-tuning, we consider the reward function:</p><formula xml:id="formula_57">r(x) := λ × RewardModel(x)<label>(43)</label></formula><p>corresponding to a scaled version of the reward model, which we take to be ImageReward <ref type="bibr" target="#b91">(Xu et al., 2023)</ref>. Different values of λ provide different tradeoffs between the KL regularization and the reward model ( <ref type="formula" target="#formula_22">19</ref>). For evaluation and benchmarking purposes, we report metrics that separately quantify text-to-image consistency, human preference, and sample diversity, capturing the tradeoff between each aspect of generative models <ref type="bibr" target="#b3">(Astolfi et al., 2024)</ref>. For consistency, we make use of the standard ClipScore <ref type="bibr" target="#b34">(Hessel et al., 2021)</ref> and PickScore <ref type="bibr" target="#b42">(Kirstain et al., 2023)</ref>; for generalization to unseen human preferences, we use the HPSv2 model <ref type="bibr">(Wu et al., 2023b)</ref>; and for diversity, we compute averages of pairwise distances of the DreamSim features <ref type="bibr" target="#b28">(Fu et al., 2023)</ref>. More details are provided in Appendix G.4.</p><p>As our baselines, we consider the DPO <ref type="bibr">(Wallace et al., 2023a)</ref>, ReFL <ref type="bibr" target="#b91">(Xu et al., 2023)</ref>, and DRaFT-K algorithms <ref type="bibr" target="#b17">(Clark et al., 2024)</ref>. DPO does not use gradients from the reward function, while ReFL and DRaFT make use of heuristic gradient stopping approaches to stay close to the base generative model. Out of these baseline methods, we find that DRaFT-1 performs the best, so we perform additional ablation experiments comparing to this method. Within the same SOC formulation as our method, we also consider the discrete and continuous adjoint methods. We provide full experimental details in Appendix G; an important implementation detail is that we slightly offset σ(t) in order to avoid division by zero.</p><p>Evaluation results. In Table <ref type="table">2</ref> we report the evaluation metrics for the baselines as well as our proposed Adjoint Matching approach. We compare each method at roughly the same wall clock time (see the times and number of iterations in Table <ref type="table">4</ref>, and comments in Appendix G.5). We find that across all metrics, our proposed memoryless SOC formulation outperforms existing baseline methods. The choice of SOC algorithms also obviously favors Adjoint Matching over continuous and discrete adjoint methods, which result in poorer consistency and human preference metrics.</p><p>Ablation: base model vs. reward tradeoff. We note that the scaling in front of the reward model λ determines how strongly the we should prefer the reward model over the base model. As such, we see a natural tradeoff curve: higher λ results in better consistency and human preference, but lower diversity in the generated samples. Overall, we find that Adjoint Matching performs stably across all values of λ. Our method of regularizing the fine-tuning procedure through memoryless SOC works much better than baseline methods which often must employ early stopping. We show the qualitative effect of varying λ in Figure <ref type="figure">3</ref>, while for the DRaFT-1 baseline we show the effect of varying the number of fine-tuning iterations.</p><p>Ablation: classifier-free guidance. We note that it is possible to apply classifier-free guidance (CFG; Ho and Salimans (2022); Zheng et al. ( <ref type="formula">2023</ref>)) after fine-tuning. We use the formula (1 + w)v(x, t|y) -wv(x, t), where w is the guidance weight, v(x, t|y) is a fine-tuned text-to-image model while v(x, t) is an unconditional image model. This is not principled as only the conditional model is fine-tuned, but generally it is unclear what distribution guided models sample from anyhow. In Figure <ref type="figure" target="#fig_2">5</ref> we show the evaluation metrics with classifier-free guidance applied. Comparing three different guidance weight values, we see a higher weight does improve text-to-image consistency, and to some extent, human preference, but this comes at the cost of being worse in terms of diversity. We show qualitative differences in Figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We investigate the problem of fine-tuning dynamical generative models such as Flow Matching and propose the use of a stochastic optimal control (SOC) formulation with a memoryless noise schedule. This ensures we converge to the same tilted distribution that the large language modeling literature uses for learning from human feedback. In particular, the memoryless noise schedule corresponds to DDPM sampling for diffusion models and a new Memoryless Flow Matching generative process for flow models. In conjunction, we propose a novel training algorithm for solving stochastic optimal control problems, by casting SOC as a regression problem, which we call the Adjoint Matching objective. Empirically, we find that our memoryless SOC formulation works better than multiple existing works on fine-tuning diffusion models, and our Adjoint Matching algorithm outperforms related gradient-based methods. In summary, we are the first to provide a theoretically-driven algorithm for fine-tuning Flow Matching models, and we find that our approach significantly outperforms baseline methods across multiple axes of evaluation-text-to-image consistency, generalization to unseen human preference, and sample diversity-on large-scale text-to-image generation.   Table 4 Additional metrics for various fine-tuning methods for text-to-image generation, which complement the ones in Table 2 (both tables correspond to the same runs). The second and third columns show the noise schedules σ(t) used for fine-tuning and for inference: σ(t) = √ 2ηt corresponds to Memoryless Flow Matching, and σ(t) = 0 to the Flow Matching ODE (3). w Fine-tuning #iter. Fine-tun. Sampl. ImageReward ↑ ClipScore ↑ PickScore ↑ HPS v2 ↑ DreamSim loss / λ σ(t) σ(t) diversity ↑ 0.0 None N/A N/A √ 2ηt -1.384±0.040 24.15±0.26 17.25±0.06 16.19±0.17 53.60±1.37 0 -0.920±0.042 28.32±0.22 18.15±0.07 17.89±0.16 56.53±1.52 0.0 DRaFT-1 √ 2ηt √ 2ηt 0.913±0.068 29.80±0.22 19.16±0.06 23.63±0.16 35.21±1.93 0 0 0.626±0.195 30.48±0.32 18.91±0.34 21.92±1.63 38.52±2.01 √ 2ηt √ 2ηt 1.204±0.046 29.90±0.43 19.29±0.12 24.40±0.27 28.51±1.68 0 0 1.052±0.088 30.65±0.24 19.27±0.11 23.81±0.44 32.11±2.37 √ 2ηt √ 2ηt 1.307±0.041 29.96±0.22 19.31±0.06 24.42±0.13 26.57±1.32 0 0 1.173±0.058 30.86±0.25 19.37±0.06 24.17±0.23 29.69±1.30 √ 2ηt √ 2ηt 1.357±0.039 30.18±0.24 19.38±0.08 24.61±0.17 25.54±0.99 0 0 1.251±0.040 30.95±0.28 19.37±0.06 24.37±0.17 27.39±1.14 0.0 Adj.-Match. √ 2ηt √ 2ηt 0.550±0.043 30.36±0.22 19.29±0.08 24.12±0.17 40.89±1.50 0 0 0.454±0.055 31.41±0.22 19.57±0.09 23.29±0.18 43.10±1.76 √ 2ηt √ 2ηt 0.755±0.040 30.59±0.40 19.49±0.10 24.85±0.23 37.07±1.47 0 0 0.671±0.047 31.64±0.21 19.71±0.09 24.12±0.27 39.88±1.59 12500 √ 2ηt √ 2ηt 0.882±0.058 30.62±0.30 19.50±0.09 24.95±0.28 34.50±1.33 0 0 0.778±0.050 31.65±0.19 19.76±0.08 24.49±0.27 37.24±1.57 1.0 None N/A N/A √ 2ηt -0.269±0.050 30.41±0.22 18.74±0.07 20.47±0.18 43.82±1.24 0 -0.123±0.041 31.83±0.17 19.28±0.07 20.95±0.16 42.59±1.23 1.0 DRaFT-1 √ 2ηt √ 2ηt 1.123±0.051 32.06±0.19 19.69±0.06 24.56±0.17 28.25±1.55 0 0 0.856±0.167 32.32±0.25 19.38±0.34 22.88±1.54 29.98±1.86 0 0 1.177±0.053 32.36±0.18 19.67±0.08 24.48±0.28 25.09±1.82 0 0 1.255±0.038 32.36±0.19 19.70±0.06 24.64±0.17 23.24±1.19 0 0 1.296±0.033 32.30±0.19 19.68±0.06 24.71±0.14 21.54±0.96 1.0 Adj.-Match. 0 0 0.782±0.044 33.05±0.22 20.20±0.09 24.81±0.18 32.67±1.26 √ 2ηt √ 2ηt 1.027±0.038 32.85±0.21 20.08±0.08 25.88±0.20 29.83±1.00 0 0 0.910±0.040 33.20±0.17 20.29±0.09 25.39±0.24 30.34±1.51 12500 0 0 0.985±0.041 33.10±0.18 20.28±0.08 25.61±0.27 28.86±1.37 4.0 None N/A N/A √ 2ηt 0.277±0.043 32.68±0.18 19.50±0.07 22.29±0.16 35.12±0.92 0 0.209±0.046 32.83±0.17 19.79±0.07 22.30±0.17 32.05±1.05 4.0 DRaFT-1 √ 2ηt √ 2ηt 1.062±0.045 32.29±0.16 19.48±0.06 23.67±0.13 25.03±1.32 0 0 0.604±0.395 31.80±0.86 19.09±0.53 21.69±2.10 25.92±2.57 0 0 1.112±0.046 32.29±0.20 19.34±0.11 23.31±0.22 21.02±1.67 0 0 1.151±0.036 32.31±0.21 19.36±0.06 23.29±0.14 19.53±1.24 0 0 1.172±0.040 32.20±0.22 19.30±0.07 23.20±0.15 18.45±1.06 4.0 Adj.-Match. 0 0 0.852±0.046 33.50±0.22 20.31±0.08 24.97±0.19 25.83±0.82 √ 2ηt √ 2ηt 1.052±0.039 33.51±0.19 20.15±0.07 25.56±0.18 26.21±0.73 0 0 0.942±0.042 33.61±0.19 20.35±0.08 25.34±0.21 24.30±0.86 12500 0 0 1.007±0.052 33.48±0.20 20.29±0.08 25.50±0.29 23.48±0.81   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Figures &amp; Tables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results on DDIM and Flow Matching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 The continuous-time limit of DDIM</head><p>The DDIM inference update <ref type="bibr">(Song et al., 2021a, Eq. 12)</ref> is</p><formula xml:id="formula_58">x k+1 = √ ᾱk+1 x k - √ 1-ᾱk ϵ(x k ,k) √ ᾱk + 1 -ᾱk+1 -σ 2 k ϵ(x k , k) + σ k ϵ k , x K ∼ N (0, I).<label>(44)</label></formula><p>If we let ∆ ᾱk = ᾱk+1 -ᾱk , we have that</p><formula xml:id="formula_59">ᾱk+1 ᾱk = ᾱk + ᾱk+1 -ᾱk ᾱk = 1 + ᾱk+1 -ᾱk ᾱk = 1 + ∆ ᾱk ᾱk ≈ 1 + ∆ ᾱk 2 ᾱk ,<label>(45)</label></formula><p>where we used the first-order Taylor approximation of √ 1 + x. And</p><formula xml:id="formula_60">-ᾱk+1 ᾱk (1 -ᾱk ) + 1 -ᾱk+1 -σ 2 k = -1 + ∆ ᾱk ᾱk (1 -ᾱk ) + 1 -ᾱk+1 -σ 2 k = -1 + ∆ ᾱk ᾱk -ᾱk -∆ ᾱk + 1 -ᾱk+1 -σ 2 k = -1 -ᾱk+1 + ∆ ᾱk ᾱk + 1 -ᾱk+1 -σ 2 k = √ 1 -ᾱk+1 -1 + ∆ ᾱk ᾱk (1-ᾱk+1 ) + 1 - σ 2 k 1-ᾱk+1 ≈ √ 1 -ᾱk+1 -1 + ∆ ᾱk 2 ᾱk (1-ᾱk+1 ) + 1 - σ 2 k 2(1-ᾱk+1 ) = -∆ ᾱk 2 ᾱk + σ 2 k 2 1 √ 1-ᾱk+1 ,<label>(46)</label></formula><p>where we used the same first-order Taylor approximation. Thus, up to first-order approximations, ( <ref type="formula" target="#formula_58">44</ref>) is equivalent to</p><formula xml:id="formula_61">x k-1 = 1 + ∆ ᾱk 2 ᾱk x k -∆ ᾱk 2 ᾱk + σ 2 k 2 ϵ(x k ,k) √ 1-ᾱk+1 + σ k ϵ k , x K ∼ N (0, I).<label>(47)</label></formula><p>If we modify our notation slightly, we can rewrite this as</p><formula xml:id="formula_62">X (k+1)h = 1 -h αkh 2 ᾱkh X kh + h αkh 2 ᾱkh -hσ(kh) 2 2 ϵ(X kh ,kh) √ 1-ᾱkh + √ hσ(kh)ϵ k , X 0 ∼ N (0, I). (<label>48</label></formula><formula xml:id="formula_63">)</formula><p>To go from (47) to (48), we introduced a continuous time variable and a stepsize h = 1/K, and we regard the increment hᾱ k as approximately equal to h times the derivative of ᾱ. We also identified σ k with √ hσ(kh), where σ(kh) plays the role of a diffusion coefficient. Note that equation ( <ref type="formula" target="#formula_62">48</ref>) can be reverse-engineered as the Euler-Maruyama discretization of the SDE</p><formula xml:id="formula_64">dX t = -αt 2 ᾱt + αt 2 ᾱt -σ(t) 2 2 ϵ(Xt,t) √ 1-ᾱt dt + σ(t)dB t , X 0 ∼ N (0, I).<label>(49)</label></formula><p>B.2 Forward and backward stochastic differential equations</p><formula xml:id="formula_65">Let (κ t ) t∈[0,1] and (η t ) t∈[0,1] such that ∀t ∈ [0, 1], η t ≥ 0, 1 0 κ 1-s ds = +∞, 2 1 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ = 1. (<label>50</label></formula><formula xml:id="formula_66">)</formula><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, DDIM corresponds to κ t = αt 2 ᾱt , η t = αt 2 ᾱt , and Flow Matching corresponds to κ t = αt αt , η t = β t αt αt β t -βt . Lemma 1 (DDIM and Flow Matching fulfill the conditions (50)). The choices of (κ t ) t∈[0,1] and (η t ) t∈[0,1] for DDIM and Flow Matching fulfill the conditions (50). For DDIM, we have that</p><formula xml:id="formula_67">t 0 κ 1-s ds = -1 2 log ᾱ1-t =⇒ 1 0 κ 1-s ds = +∞, 2 t 0 η t ′ exp -2 t t ′ κ s ds dt ′ = 1 -ᾱ1-t =⇒ 2 1 0 η t ′ exp -2 t t ′ κ s ds dt ′ = 1. (51) For Flow Matching, t 0 κ 1-s ds = -log α 1-t =⇒ 1 0 κ 1-s ds = +∞, (<label>52</label></formula><formula xml:id="formula_68">)</formula><formula xml:id="formula_69">2 t 0 η t ′ exp -2 t t ′ κ s ds dt ′ = β 2 1-t =⇒ 2 1 0 η t ′ exp -2 t t ′ κ s ds dt ′ = 1. (<label>53</label></formula><formula xml:id="formula_70">)</formula><p>Forward and backward SDEs Consider the forward and backward SDEs</p><formula xml:id="formula_71">d ⃗ X t = -κ 1-t ⃗ X t dt + √ 2η 1-t dB t , ⃗ X 0 ∼ p data , (<label>54</label></formula><formula xml:id="formula_72">) dX t = κ t X t + 2η t s(X t , t) dt + √ 2η t dB t , X 0 ∼ N (0, I),<label>(55)</label></formula><p>where we let ⃗ p t be the density of ⃗ X t , and we define the score function as s(x, t) := ∇ log ⃗ p 1-t (x). Similarly, we let p t be the density of X t . ⃗ p t and p t solve the Fokker-Planck equations:</p><formula xml:id="formula_73">∂ t ⃗ p t = ∇ • κ 1-t x⃗ p t + η 1-t ∆⃗ p t , ⃗ p 0 = p data ,<label>(56)</label></formula><formula xml:id="formula_74">∂ t p t = ∇ • -κ t x -2η t ∇ log ⃗ p 1-t (X t ) p t + η t ∆p t , p 0 = N (0, I). (<label>57</label></formula><formula xml:id="formula_75">)</formula><p>Lemma 2 (Solution of the forward SDE). Let (κ t ) t≥0 , (η t ) t≥0 with η t ≥ 0, and (ξ t ) t≥0 be arbitrary. The solution ⃗ X t of the SDE</p><formula xml:id="formula_76">d ⃗ X t = -κ 1-t ⃗ X t + ξ t dt + √ 2η 1-t dB t , ⃗ X 0 ∼ p data (58) is ⃗ X t = ⃗ X 0 exp - t 0 κ 1-s ds + t 0 exp - t t ′ κ 1-s ds ξ 1-t ′ dt ′ + t 0 √ 2η 1-t ′ exp - t t ′ κ 1-s ds dB t ′ , (<label>59</label></formula><formula xml:id="formula_77">)</formula><p>which has the same distribution as the random variable</p><formula xml:id="formula_78">Xt = ⃗ X 0 exp - t 0 κ 1-s ds + t 0 exp - t t ′ κ 1-s ds ξ 1-t ′ dt ′ + 2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ ϵ, ϵ ∼ N (0, I).<label>(60)</label></formula><p>Applying Lemma 2 with ξ t ≡ 0, we obtain that ⃗ p 1 is also the distribution of</p><formula xml:id="formula_79">X1 = ⃗ X 0 exp - t 0 κ 1-s ds + 2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ ϵ = ϵ,<label>(61)</label></formula><p>where ϵ ∼ N (0, I). The third equality in (61) holds by (50). Hence we obtain that ⃗ p 1 = N (0, I). Note also that</p><formula xml:id="formula_80">∂ t ⃗ p 1-t = -∇ • κ t x⃗ p 1-t -η t ∆⃗ p 1-t = -∇ • -κ t x -2η t ∇ log ⃗ p 1-t (x) ⃗ p 1-t + η t ∆⃗ p 1-t<label>(62)</label></formula><p>Thus, ⃗ p 1-t is a solution of the backward Fokker-Planck equation ( <ref type="formula" target="#formula_74">57</ref>), which proves the following:</p><p>Proposition 3 (Equality of marginal distributions). For any time t ∈ [0, 1], the densities of the solutions ⃗ X t , X t of the forward and backward SDEs are equal up to a time flip:</p><formula xml:id="formula_81">p t = ⃗ p 1-t .</formula><p>Forward and backward SDEs with arbitrary noise schedule Next, we look at the following pair of forwardbackward SDEs:</p><formula xml:id="formula_82">d ⃗ X t = -κ 1-t ⃗ X t + σ(1-t) 2 2 -η 1-t s( ⃗ X t , 1 -t) dt + σ(1 -t) dB t , ⃗ X 0 ∼ p data , (<label>63</label></formula><formula xml:id="formula_83">)</formula><formula xml:id="formula_84">dX t = κ t X t + σ(t) 2 2 + η t s(X t , t) dt + σ(t) dB t , X 0 ∼ N (0, I),<label>(64)</label></formula><p>Here, the score function s is the same vector field as in (64). Remark that equations ( <ref type="formula" target="#formula_71">54</ref>)-( <ref type="formula" target="#formula_72">55</ref>) are a particular case of ( <ref type="formula" target="#formula_82">63</ref>)-( <ref type="formula" target="#formula_84">64</ref>) for which σ(t) = √ 2η t . The Fokker-Planck equations for (63)-( <ref type="formula" target="#formula_84">64</ref>) are:</p><formula xml:id="formula_85">∂ t ⃗ p t = ∇ • κ 1-t x + -σ(1-t) 2 2 + η 1-t s(X t , t) ⃗ p t + η 1-t ∆⃗ p t , ⃗ p 0 = p data ,<label>(65)</label></formula><formula xml:id="formula_86">∂ t p t = ∇ • -κ t x -σ(t) 2 2 + η t s(X t , t) p t + σ(t) 2 2 ∆p t , p 0 = N (0, I). (<label>66</label></formula><formula xml:id="formula_87">)</formula><p>It is straight-forward to see that for any σ, the solutions ⃗ p t and p t of ( <ref type="formula" target="#formula_85">65</ref>)-( <ref type="formula" target="#formula_86">66</ref>) are also solutions of ( <ref type="formula" target="#formula_73">56</ref>)-( <ref type="formula" target="#formula_74">57</ref>). Hence, the marginals ⃗ X t and X t are equally distributed for all noise schedules σ, and they are equal to each other up to a time flip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equality of distributions over trajectories</head><p>The result in Proposition 3 can be made even stronger:</p><p>Proposition 4 (Equality of distributions over trajectories). Let ⃗ X, X be the solutions of the SDEs (63)-( <ref type="formula" target="#formula_84">64</ref>) with arbitrary noise schedule. For any sequence of times (t i ) 0≤i≤I , the joint distribution of ( ⃗ X ti ) 0≤i≤I is equal to the joint distribution of (X 1-ti ) 0≤i≤I , or equivalently, that the probability measures ⃗ P, P of the forward and backward processes ⃗ X, X are equal, up to a flip in the time direction.</p><p>This result states that sampling trajectories from the backward process is equivalent to sampling them from the forward process and then flipping their order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Proof of Lemma 1</head><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, DDIM corresponds to κ t = αt 2 ᾱt , η t = αt 2 ᾱt . Thus, η t ≥ 0 because ᾱt is increasing, and</p><formula xml:id="formula_88">t 0 κ 1-s ds = t 0 α1-s 2 ᾱ1-s ds = -1 2 t 0 ∂ s log ᾱ1-s ds = -1 2 (log ᾱ1-t -log ᾱ1 ) = -1 2 log ᾱ1-t , =⇒ 1 0 κ 1-s ds = -1 2 log ᾱ0 = +∞ (67) 2 t 0 η t ′ exp -2 t t ′ κ s ds dt ′ = t 0 α1-t ′ ᾱ1-t ′ exp - t t ′ α1-s ᾱ1-s ds dt ′ = t 0 α1-t ′ ᾱ1-t ′ ᾱ1-t ᾱ1-t ′ dt ′ = ᾱ1-t t 0 ∂ t ′ 1 ᾱ1-t ′ dt ′ = ᾱ1-t 1 ᾱ1-t -1 ᾱ1 = 1 -ᾱ1-t , =⇒ 2 1 0 η t ′ exp -2 t t ′ κ s ds dt ′ = 1 -ᾱ0 = 1. (<label>68</label></formula><formula xml:id="formula_89">)</formula><p>where we used that ᾱ1 = 1 and ᾱ0 = 0. And Flow Matching corresponds to κ t = αt αt , η t = β t αt αt β t -βt . We have that η t ≥ 0 because α t is increasing and β t is decreasing, and</p><formula xml:id="formula_90">t 0 κ 1-s ds = t 0 α1-s α1-s ds = - t 0 ∂ s log α 1-s ds = -(log α 1-t -log α 1 ) = -log α 1-t , =⇒ 1 0 κ 1-s ds = -log α 0 = +∞,<label>(69)</label></formula><p>and</p><formula xml:id="formula_91">2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ = 2 t 0 β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ exp -2 t t ′ α1-s α1-s ds dt ′ = 2 t 0 β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ α1-t α 1-t ′ 2 dt ′ ,<label>(70)</label></formula><p>To develop the right-hand side, note that by integration by parts,</p><formula xml:id="formula_92">t 0 β1-t ′ β 1-t ′ α1-t α 1-t ′ 2 dt ′ = - t 0 ∂ t ′ β 2 1-t ′ 2 α1-t α 1-t ′ 2 dt ′ = - β 2 1-t ′ 2 α1-t α 1-t ′ 2 1 0 + t 0 β 2 1-t ′ 2 ∂ t ′ α1-t α 1-t ′ 2 dt ′ = - β 2 1-t ′ 2 α1-t α 1-t ′ 2 t 0 + t 0 β 2 1-t ′ α 2 1-t α1-t ′ α 3 1-t ′ dt ′ . (<label>71</label></formula><formula xml:id="formula_93">)</formula><p>And if we plug this into the right-hand side of (70), we obtain</p><formula xml:id="formula_94">2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ = β 2 1-t ′ α1-t α 1-t ′ 2 t 0 = β 2 1-t -β 2 1 α1-t α1 2 = β 2 1-t ,<label>(72)</label></formula><p>=⇒ 2</p><formula xml:id="formula_95">1 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ = β 2 1 = 1. (<label>73</label></formula><formula xml:id="formula_96">)</formula><p>where we used that β 1 = 0, α 1 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Proof of Lemma 2</head><p>We can solve this equation by variation of parameters. To simplify the notation, we replace κ 1-s , η 1-s and ξ 1-s by κ s , η s and ξ</p><formula xml:id="formula_97">s . Defining f ( ⃗ X t , t) = ⃗ X t exp t 0 κ 1-s ds , we get that df ( ⃗ X t , t) = κ 1-t ⃗ X t exp t 0 κ 1-s ds dt + exp t 0 κ 1-s ds d ⃗ X t = κ 1-t ⃗ X t exp t 0 κ 1-s ds dt + exp t 0 κ 1-s ds (-κ 1-t ⃗ X t + ξ 1-t ) dt + √ 2η 1-t dB t = exp t 0 κ 1-s ds ξ 1-t dt + √ 2η t exp t 0 κ 1-s ds dB t . (<label>74</label></formula><formula xml:id="formula_98">)</formula><p>Integrating from 0 to t, we get that</p><formula xml:id="formula_99">⃗ X t exp t 0 κ 1-s ds = ⃗ X 0 + t 0 exp t ′ 0 κ 1-s ds ξ 1-t ′ dt ′ + t 0 √ 2η 1-t ′ exp t ′ 0 κ 1-s ds dB t ′ , (75) ⇐⇒ ⃗ X t = ⃗ X 0 exp - t 0 κ 1-s ds + t 0 exp - t t ′ κ 1-s ds ξ 1-t ′ dt ′ + t 0 √ 2η 1-t ′ exp - t t ′ κ 1-s ds dB t ′ . (76) Since E t 0 √ 2η 1-t ′ exp - t t ′ κ 1-s ds dB t ′ 2 = 2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ ,<label>(77)</label></formula><p>we obtain that</p><formula xml:id="formula_100">t 0 √ 2η 1-t ′ exp - t t ′ κ 1-s ds dB t ′ has the same distribution as 2 t 0 η 1-t ′ exp -2 t t ′ κ 1-s ds dt ′ ϵ, where ϵ ∼ N (0, 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Proof of Proposition 4</head><p>This is a result that has been used by previous works, e.g. <ref type="bibr">(De Bortoli et al., 2021, Sec. 2.1)</ref>, but their derivation lacks rigor as it uses some unexplained approximations. While natural, the result is not common knowledge in the area. We provide a derivation which is still in discrete time, and hence not completely formal, but that corrects the gaps in the proof of De <ref type="bibr" target="#b18">Bortoli et al. (2021)</ref>.</p><p>We introduce the short-hand</p><formula xml:id="formula_101">⃗ b(x, t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t s(x, 1 -t), (<label>78</label></formula><formula xml:id="formula_102">) b(x, t) = κ t X t + σ(t) 2 2 + η t s(X t , t),<label>(79)</label></formula><formula xml:id="formula_103">⃗ σ(t) = σ(1 -t). (<label>80</label></formula><formula xml:id="formula_104">) Remark that b(x, t) = -⃗ b(x, 1 -t) + σ(t) 2 s(X t , t).</formula><p>Suppose that we discretize the forward process ⃗ X using K + 1 equispaced timesteps:</p><formula xml:id="formula_105">x k+1 = x k + h ⃗ b(x k , kh) + √ h⃗ σ(kh)ϵ k , with ϵ k ∼ N (0, 1). (<label>81</label></formula><formula xml:id="formula_106">)</formula><p>It is important to remark that x k+1 -x k = O(h 1/2 ). Throughout the proof we will keep track of all terms up to linear order in h, while neglecting terms of order O(h 3/2 ) and higher. The distribution of the discretized forward process is:</p><formula xml:id="formula_107">⃗ p(x 0:K ) = ⃗ p 0 (x 0 ) K-1 k=0 ⃗ p k+1|k (x k+1 |x k ), where ⃗ p k+1|k (x k+1 |x k ) = exp - ∥x k+1 -x k -h ⃗ b(x k ,kh)∥ 2 2h⃗ σ(kh) 2 (2πh⃗ σ(kh) 2 ) d/2</formula><p>(82)</p><p>Using telescoping products, we have that</p><formula xml:id="formula_108">⃗ p(x 0:K ) = ⃗ p K (x K ) K-1 k=0 ⃗ p k+1|k (x k+1 |x k ) ⃗ p k (x k ) ⃗ p k+1 (x k+1 ) = ⃗ p K (x K ) K-1 k=0 ⃗ p k+1|k (x k+1 |x k ) exp log(⃗ p k (x k )) -log(⃗ p k+1 (x k+1 ))<label>(83)</label></formula><p>We can use a discrete time version of Ito's lemma:</p><formula xml:id="formula_109">log ⃗ p(x k+1 , (k + 1)h) ≈ log ⃗ p(x k , kh) + h ∂ t log ⃗ p(x k , kh) + ⃗ σ(kh) 2 2 ∆ log ⃗ p(x k , kh)<label>(84)</label></formula><formula xml:id="formula_110">+ ⟨∇ log ⃗ p(x k , kh), x k+1 -x k ⟩ + O(h 3/2 ). (<label>85</label></formula><formula xml:id="formula_111">)</formula><p>Using equation ( <ref type="formula" target="#formula_105">81</ref>) and a Taylor approximation, observe that</p><formula xml:id="formula_112">⟨∇ log p(x k , kh), x k+1 -x k ⟩ = ⟨∇ log p(x k+1 , (k + 1)h) -∇ 2 log p(x k+1 , (k + 1)h)(x k+1 -x k ), x k+1 -x k ⟩ + O(h 3/2 ) = ⟨∇ log p(x k+1 , (k + 1)h), x k+1 -x k ⟩ -⟨h ⃗ b(x k , kh) + √ h⃗ σ(kh)ϵ k , ∇ 2 log p(x k+1 , (k + 1)h) h ⃗ b(x k , kh) + √ h⃗ σ(kh)ϵ k ⟩ + O(h 3/2 ) = ⟨∇ log p(x k+1 , (k + 1)h), x k+1 -x k ⟩ -h⃗ σ(kh) 2 ∆ log p(x k+1 , (k + 1)h) + O(h 3/2 ). (<label>86</label></formula><formula xml:id="formula_113">)</formula><p>And since ⃗ p satisfies the Fokker-Planck equation</p><formula xml:id="formula_114">∂ t ⃗ p t = ∇ • (-⃗ b(x, t) + ⃗ σ(t) 2 2 ∇ log ⃗ p t (x))⃗ p t ,<label>(87)</label></formula><p>we have that</p><formula xml:id="formula_115">∂ t log ⃗ p t = ∂t⃗ pt ⃗ pt = ∇• (-⃗ b(x,t)+ ⃗ σ(t) 2 2 ∇ log ⃗ pt(x))⃗ pt ⃗ pt = -∇ • ⃗ b(x, t) + ⃗ σ(t) 2 2 ∆ log ⃗ p t (x) + ⟨-⃗ b(x, t) + ⃗ σ(t) 2 2 ∇ log ⃗ p t (x), ∇ log ⃗ p t (x)⟩.<label>(88)</label></formula><p>Hence,</p><formula xml:id="formula_116">∂ t log p(x k , kh) = ∂ t log p(x k+1 , (k + 1)h) + O(h 1/2 ) = -∇ • ⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k+1)h) 2 2 ∆ log ⃗ p(x k+1 , (k + 1)h) + ⟨-⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k+1)h) 2 2 ∇ log ⃗ p(x k+1 , (k + 1)h), ∇ log ⃗ p(x k+1 , (k + 1)h)⟩ + O(h 1/2 ). (<label>89</label></formula><formula xml:id="formula_117">)</formula><p>If we plug ( <ref type="formula" target="#formula_112">86</ref>) and ( <ref type="formula" target="#formula_116">89</ref>) into ( <ref type="formula" target="#formula_109">84</ref>), we obtain</p><formula xml:id="formula_118">log p(x k+1 , (k + 1)h) -log p(x k , kh) = h -∇ • ⃗ b(x k+1 , (k + 1)h)+⟨-⃗ b(x k+1 , (k + 1)h)+ ⃗ σ((k+1)h) 2 2 ∇ log ⃗ p(x k+1 , (k + 1)h), ∇ log ⃗ p(x k+1 , (k + 1)h)⟩ + ⟨∇ log p(x k+1 , (k + 1)h), x k+1 -x k ⟩ + O(h 3/2 ) = ⟨2h⃗ σ(kh) 2 ∇ log p(x k+1 ,(k+1)h),x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)⟩ 2h⃗ σ(kh) 2 + h -∇ • ⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k+1)h) 2 2 ∥∇ log ⃗ p(x k+1 , (k + 1)h)∥ 2 + O(h 3/2 ).<label>(90)</label></formula><p>Applying a discrete time version of Ito's lemma again, we have that</p><formula xml:id="formula_119">⃗ b(x k , kh) = ⃗ b(x k+1 , (k + 1)h) -h ∂ t ⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k+1)h) 2 2 ∆ ⃗ b(x k+1 , (k + 1)h) + ∇ ⃗ b(x k+1 , (k + 1)h) ⊤ (x k -x k+1 ) + O(h 3/2 ) = ⃗ b(x k+1 , (k + 1)h) + ∇ ⃗ b(x k+1 , (k + 1)h) ⊤ (x k -x k+1 ) + O(h). (<label>91</label></formula><formula xml:id="formula_120">)</formula><p>where ∆ ⃗ b denotes the component-wise Laplacian of ⃗ b. Thus,</p><formula xml:id="formula_121">log ⃗ p k+1|k (x k+1 |x k ) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k ,kh)∥ 2 2h⃗ σ(kh) 2 = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h( ⃗ b(x k+1 ,(k+1)h)+∇ ⃗ b(x k+1 ,(k+1)h) ⊤ (x k -x k+1 ))∥ 2 2h⃗ σ(kh) 2 + O(h 3/2 ) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)∥ 2 2h⃗ σ(kh) 2 + ⟨x k+1 -x k ,∇ ⃗ b(x k+1 ,(k+1)h) ⊤ (x k -x k+1 )⟩ ⃗ σ(kh) 2 + O(h 3/2 ) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)∥ 2 h⃗ σ(kh) 2 -h⃗ σ(kh) 2 ⟨ϵ k ,∇ ⃗ b(x k+1 ,(k+1)h) ⊤ ϵ k ⟩ ⃗ σ(kh) 2 + O(h 3/2 ) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)∥ 2 h⃗ σ(kh) 2 -h∆ ⃗ b(x k+1 , (k + 1)h) + O(h 3/2 ) (92)</formula><p>Combining ( <ref type="formula" target="#formula_118">90</ref>) and ( <ref type="formula">92</ref>), we obtain that</p><formula xml:id="formula_122">log ⃗ p k+1|k (x k+1 |x k ) -log p(x k+1 , (k + 1)h) -log p(x k , kh) = -d 2 log 2πh⃗ σ(kh) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)+h⃗ σ(kh) 2 ∇ log p(x k+1 ,(k+1)h)∥ 2 h⃗ σ(kh) 2 + O(h 3/2 ) = -d 2 log 2πh⃗ σ((k + 1)h) 2 -∥x k+1 -x k -h ⃗ b(x k+1 ,(k+1)h)+h⃗ σ((k+1)h) 2 ∇ log p(x k+1 ,(k+1)h)∥ 2 h⃗ σ((k+1)h) 2 + O(h 3/2 ). (<label>93</label></formula><formula xml:id="formula_123">)</formula><p>By Bayes rule, and taking the exponential of this equation, we obtain</p><formula xml:id="formula_124">⃗ p k+1|k (x k+1 |x k ) := ⃗ p k+1|k (x k+1 |x k ) ⃗ p k (x k ) ⃗ p k+1 (x k+1 ) = exp - ∥x k -x k+1 +h ⃗ b(x k+1 ,(k+1)h)-h⃗ σ((k+1)h) 2 ∇ log p(x k+1 ,(k+1)h)∥ 2 2h⃗ σ((k+1)h) 2 (2πh⃗ σ((k+1)h) 2 ) d/2 + O(h 3/2 ).</formula><p>(94) Up to the O(h 3/2 ) term, the right-hand side is the conditional Gaussian corresponding to the update</p><formula xml:id="formula_125">x k = x k+1 + h -⃗ b(x k+1 , (k + 1)h) + ⃗ σ((k + 1)h) 2 ∇ log p(x k+1 , (k + 1)h) + √ h⃗ σ((k + 1)h)ϵ k+1 , ϵ k+1 ∼ N (0, I). (<label>95</label></formula><formula xml:id="formula_126">)</formula><p>If we define y k = x K-k , and we use that b(x, t) = -⃗ b(x, 1 -t) + ⃗ σ(t) 2 ∇ log p(x, 1 -t), we can rewrite (95) as</p><formula xml:id="formula_127">y K-k = y K-k-1 + h -⃗ b(y K-k-1 , (K -k -1)h) + ⃗ σ((K -k -1)h) 2 ∇ log p(y K-k-1 , (K -k -1)h) + √ h⃗ σ((K -k -1)h)ϵ k = y K-k-1 + hb(y K-k-1 , kh) + √ hσ(kh)ϵ K-k-1 , =⇒ y k+1 = y k + hb(y k , kh) + √ hσ(kh)ϵ k . (<label>96</label></formula><formula xml:id="formula_128">)</formula><p>And this is the Euler-Maruyama discretization of the backward process ⃗ X. If we plug ( <ref type="formula">94</ref>) into ( <ref type="formula" target="#formula_108">83</ref>), we obtain that</p><formula xml:id="formula_129">⃗ p(x 0:K ) ≈ ⃗ p K (x K ) K-1 k=0 ⃗ p k+1|k (x k+1 |x k ). (<label>97</label></formula><formula xml:id="formula_130">)</formula><p>which concludes the proof, as ⃗ p K (x K ) is the initial distribution of the backward process, and ⃗ p k+1|k (x k+1 |x k ) are its transition kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 The relationship between the noise predictor ϵ and the score function</head><p>Applying Lemma 2 with the choices of (κ t ) t≥0 and (η t ) t≥0 for DDIM, we obtain that ⃗ X t has the same distribution as</p><formula xml:id="formula_131">Xt = √ ᾱ1-t ⃗ X 0 + √ 1 -ᾱ1-t ϵ, ϵ ∼ N (0, 1).<label>(98)</label></formula><p>Since ⃗ X t and Xt have the same distribution, predicting the noise of ⃗ X t is equivalent to predicting the noise of Xt . The noise predictor ϵ can be written as:</p><formula xml:id="formula_132">ϵ(x, t) := E[ϵ| X1-t = x] = E ϵ| √ ᾱt ⃗ X 0 + √ 1 -ᾱt ϵ = x = E x- √ ᾱt ⃗ X0 √ 1-ᾱt | √ ᾱt ⃗ X 0 + √ 1 -ᾱt ϵ = x<label>(99)</label></formula><p>And the score function s(x, t) := ∇ log ⃗ p 1-t (x) admits the expression</p><formula xml:id="formula_133">s(x, t) := ∇ log ⃗ p 1-t (x) = ∇⃗ p1-t(x) ⃗ p1-t(x) = ∇E[⃗ p 1-t|0 (x| ⃗ X0)] ⃗ p1-t(x) = E[∇ log ⃗ p 1-t|0 (x| ⃗ X0)⃗ p 1-t|0 (x| ⃗ X0)] ⃗ p1-t(x) ,<label>(100)</label></formula><p>where</p><formula xml:id="formula_134">⃗ p 1-t|0 (x| ⃗ X 0 ) = exp(-∥x- √ ᾱtY1∥ 2 /(2(1-ᾱt))) (2π(1-ᾱt)) d/2 =⇒ ∇ log ⃗ p t|1 (x|Y 1 ) = -x- √ ᾱtY1 1-ᾱt .<label>(101)</label></formula><p>Plugging this into the right-hand side of (100) and using Bayes' rule, we get</p><formula xml:id="formula_135">s(x, t) = E -x- √ ᾱt ⃗ X0 1-ᾱt | √ ᾱt ⃗ X 0 + √ 1 -ᾱt ϵ = x .<label>(102)</label></formula><p>Comparing the right-hand sides of ( <ref type="formula" target="#formula_132">99</ref>) and ( <ref type="formula" target="#formula_135">102</ref>), we obtain that s(x, t) = -ϵ(x,t) √ 1-ᾱt .</p><p>B.4 The relationship between the vector field v and the score function</p><p>By construction <ref type="bibr" target="#b46">(Lipman et al., 2023;</ref><ref type="bibr">Albergo and Vanden-Eijnden, 2023;</ref><ref type="bibr">Albergo et al., 2023)</ref>, we have that</p><formula xml:id="formula_136">v(x, t) = E[ αt Y 1 + βt Y 0 |x = α t Y 1 + β t Y 0 ] = E[ αt(x-βtY0) αt + βt Y 0 |x = α t Y 1 + β t Y 0 ] = αt αt x + ( βt -αt αt β t )E[Y 0 |x = α t Y 1 + β t Y 0 ],<label>(103)</label></formula><p>where we used that Y 1 = (x -β t Y 0 )/α t . Also, we can write the score as follows</p><formula xml:id="formula_137">s(x, t) := ∇ log p t (x) = ∇pt(x) pt(x) = ∇E[p t|1 (x|Y1)] pt(x) = E[∇p t|1 (x|Y1)] pt(x) = E[p t|1 (x|Y1)∇ log p t|1 (x|Y1)] pt(x) ,<label>(104)</label></formula><p>where</p><formula xml:id="formula_138">p t|1 (x|Y 1 ) = exp(-∥x-αtY1∥ 2 /(2β 2 t )) (2πβ 2 t ) d/2 =⇒ ∇ log ⃗ p t|1 (x|Y 1 ) = -x-αtY1 β 2 t (<label>105</label></formula><formula xml:id="formula_139">)</formula><p>Plugging this back into the right-hand side of (104), we obtain</p><formula xml:id="formula_140">s(x, t) = - E[p t|1 (x|Y1) x-α t Y 1 β 2 t ] pt(x) = - ⃗ p t|1 (x|Y1)p1(Y1) x-α t Y 1 β 2 t dY1 ⃗ pt(x) = -p 1|t (Y 1 |x) x-αtY1 β 2 t dY 1 = -E[ x-αtY1 β 2 t |x = α t Y 1 + β t Y 0 ] = -E[Y0|x=αtY1+βtY0] βt<label>(106)</label></formula><p>The last equality holds because (x -α t Y 1 )/β t = Y 0 . Putting together ( <ref type="formula" target="#formula_136">103</ref>) and ( <ref type="formula" target="#formula_140">106</ref>), we obtain that</p><formula xml:id="formula_141">v(x, t) = αt αt x + β t ( αt αt β t -βt )s(x, t) ⇐⇒ s(x, t) = 1 βt( αt α t βt-βt) v(x, t) -αt αt x<label>(107)</label></formula><p>Thus, the ODE (3) can be rewritten like this:</p><formula xml:id="formula_142">dXt dt = αt αt X t + β t ( αt αt β t -βt )s(X t , t), X 0 ∼ p 0 .<label>(108)</label></formula><p>To allow for an arbitrary diffusion coefficient, we need to add a correction term to the drift:</p><formula xml:id="formula_143">dX t = αt αt X t + σ(t) 2 2 + β t ( αt αt β t -βt ) s(X t , t) dt + σ(t)dB t , X 0 ∼ p 0 .<label>(109)</label></formula><p>This can be easily shown by writing down the Fokker-Planck equations for ( <ref type="formula" target="#formula_142">108</ref>) and ( <ref type="formula" target="#formula_143">109</ref>), and observing that they are the same up to a cancellation of terms. Finally, if we plug the right-hand side of ( <ref type="formula" target="#formula_141">107</ref>) into (109), we obtain the SDE for Flow Matching with arbitrary noise schedule (equation ( <ref type="formula" target="#formula_3">4</ref>)).</p><p>C Stochastic optimal control as maximum entropy RL in continuous space and time</p><p>In this section, we bridge KL-regularized (or MaxEnt) reinforcement learning and stochastic optimal control. We show that when the action space is Euclidean and the transition probabilities are conditional Gaussians, taking the limit in which the stepsize goes to zero on the KL-regularized RL problem gives rise to the SOC problem. A consequence of this connection is that all algorithms for KL-regularized RL admit an analog for diffusion fine-tuning. This is not novel, but it may be useful for researchers that are familiar with RL fine-tuning formulations.</p><p>Appendix C.4 is providing a more direct, rigorous, continuous-time connection between SOC and MaxEnt RL, as it shows that the expected control cost is equal to the KL divergence between the distributions over trajectories, conditioned on the starting points (see equation ( <ref type="formula" target="#formula_21">18</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Maximum entropy RL</head><p>Several diffusion fine-tuning methods <ref type="bibr" target="#b11">(Black et al., 2024;</ref><ref type="bibr">Uehara et al., 2024b)</ref> are based on KL-regularized RL, also known as maximum entropy RL, which we review in the following. In the classical reinforcement learning (RL) setting, we have an agent that, starting from state s 0 ∼ p 0 , iteratively observes a state s k , takes an action a k according to a policy π(a k ; s k , k) which leads to a new state s k+1 according to a fixed transition probability p(s k+1 |a k , s k ), and obtains rewards r k (s k , a k ). This can be summarized into a trajectory τ = ((s k , a k )) K k=0 . The goal is to optimize the policy π in order to maximize the expected total reward, i.e.</p><formula xml:id="formula_144">max π E τ ∼π,p [ K k=0 r k (s k , a k )].</formula><p>Maximum entropy RL (MaxEnt RL; <ref type="bibr" target="#b96">Ziebart et al. (2008)</ref>) amounts to adding the entropy H(π) of the policy π(•; s k , k) to the reward for each step k, in order to encourage exploration and improve robustness to changes in the environment:</p><formula xml:id="formula_145">max π E τ ∼π,p [ K k=0 r k (s k , a k ) + K-1 k=0 H(π(•; s k , k))] 8 .</formula><p>As a generalization, one can regularize using the negative KL divergence between π(•; s k , k) and a base policy π base (•; s k , k):</p><formula xml:id="formula_146">max π E τ ∼π,p [ K k=0 r k (s k , a k ) - K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))],<label>(110)</label></formula><p>which prevents the learned policy to deviate too much from the base policy. Each policy π induces a distribution q(τ ) over trajectories τ , and the MaxEnt RL problem (110) can be expressed solely in terms of such distributions (Lemma 3 in Appendix C.3):</p><formula xml:id="formula_147">max q E τ ∼q [ K k=0 r k (s k , a k )] -KL(q||q base ),<label>(111)</label></formula><p>where q base is the distribution induced by the base policy π base , and the maximization is over all distributions q such that their marginal for s 0 is p 0 . We can further recast this problem as (Lemma 4 in Appendix C.3):</p><formula xml:id="formula_148">min q KL(q||q * ),</formula><p>where q * (τ ) := q base (τ ) exp</p><formula xml:id="formula_149">K k=0 r k (s k , a k ) -V(s 0 , 0) ,<label>(112)</label></formula><p>where</p><formula xml:id="formula_150">V(s k , k) := log E τ ∼π base ,p [exp K k ′ =k r k ′ (s k ′ , a k ′ ) |s k ] = max π E τ ∼π,p K k ′ =k r k ′ (s k ′ , a k ′ ) - K-1 k ′ =k KL(π(•; s k ′ , k ′ )||π base (•; s k ′ , k ′ ))|s k (113)</formula><p>is the value function. Problem (112) directly implies that the distribution induced by the optimal policy π * is the tilted distribution q * (which has initial marginal p 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 From maximum entropy RL to stochastic optimal control</head><p>The following well-known result, which we prove in Appendix C.3, shows that in a natural sense, the continuous-time continuous-space version of MaxEnt RL is the SOC framework introduced in Section 4.1. In particular, when states and actions are vectors in R d , policies are specified by a vector field u (the control), and transition probabilities are conditional Gaussians, the MaxEnt RL problem becomes an SOC problem when the number of timesteps grows to infinity.</p><p>Proposition 5. Suppose that (i) The state space and the action space are R d , (ii) Policies π are specified as π(a k ; s k , k) = δ(a k -u(s k , kh)), where u : R d × [0, T ] → R d is a vector field, and δ denotes the Dirac delta, (iii) Transition probabilities are conditional Gaussian densities:</p><formula xml:id="formula_151">p(s k+1 |a k , s k ) = N (s k + h(b(s k , kh) + σ(kh)a k ), hσ(kh)σ(kh) ⊤ )</formula><p>, where h = T /K is the stepsize, and b and σ are defined as in Section 4.1.</p><p>Then, in the limit in which the number of steps K grows to infinity, the problem (110) is equivalent to the SOC problem (12)-( <ref type="formula" target="#formula_14">13</ref>), identifying</p><formula xml:id="formula_152">• the sequence of states (s k ) k k=0 with the trajectory X u = (X u t ) t∈[0,1] , • the running reward K-1 k=0 r k (s k , a k ) with the negative running cost - T 0 f (X u t , t) dt, • the terminal reward r K (s K , a K ) with the negative terminal cost -g(X u T ), • the KL regularization E τ ∼π,p [ K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))] with 1 2 times the expected L 2 norm of the control 1 2 E T 0 ∥u(X u t , t)∥ 2 dt ,</formula><p>• and the value function V(s k , k) defined in (113) with the negative value function -V (x, t) defined in Section 4.1.</p><p>A first consequence of this result is that every loss function designed for generic MaxEnt RL problems has a corresponding loss function for SOC problems. The geometric structure of the latter allows for additional losses that do not have an analog in the classical MaxEnt RL setting; in particular, we can differentiate the state and terminal costs.</p><p>A second consequence of Proposition 5 is that the characterization (112) can be translated to the SOC setting. The analogs of the distributions q * , q base induced by the optimal policy π * and the base policy π base are the distributions p * , p base induced by the optimal control u * and the null control. For an arbitrary trajectory X = (X t ) t∈[0,T ] , the relation between P * and P base is given by</p><formula xml:id="formula_153">dP * dP base (X) = exp(- T 0 f (X t , t) dt -g(X T ) + V (X 0 , 0)) (<label>114</label></formula><formula xml:id="formula_154">)</formula><p>where V is the value function as defined in Section 4.1. Note that this matches the statement in (22).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Proposition 5: from MaxEnt RL to SOC</head><p>Since the transition p(s k+1 |a k , s k ) is fixed, for each π we can define</p><formula xml:id="formula_155">π(a k , s k+1 ; s k , k) = π(a k ; s k , k)p(s k+1 |a k , s k ) and πbase (a k , s k+1 ; s k , k) = π base (a k ; s k , k)p(s k+1 |a k , s k ),<label>(115)</label></formula><p>and reexpress ( <ref type="formula" target="#formula_146">110</ref>) as (see Lemma 3)</p><formula xml:id="formula_156">min π E τ ∼π [ K k=0 r k (s k , a k ) - K-1 k=0 KL(π(•, •; s k , k)||π base (•, •; s k , k))].<label>(116)</label></formula><p>Using the hypothesis of the proposition, we can write</p><formula xml:id="formula_157">π(a k , s k+1 ; s k , k) = δ(a k -u(s k , kη))N (s k + η(b(s k , kη) + σ(kη)a k ), ησ(kη)σ(kη) ⊤ ) = δ(a k -u(s k , kη))π(s k+1 ; s k , k),<label>(117) where π</label></formula><formula xml:id="formula_158">(s k+1 ; s k , k) = N (s k + η(b(s k , kη) + σ(kη)u(s k , kη)), ησ(kη)σ(kη) ⊤ ) is the state transition kernel.</formula><p>We set the base policy as π base (a k ; s k , k) = δ(a k ), and we obtain analogously that π(a k , s k+1 ; s k , k) = δ(a k )π base (s k+1 ; s k , k) with πbase (s k+1 ; s k , k) = N (s k + ηb(s k , kη), ησ(kη)σ(kη) ⊤ ). Now, if we take K large, the trajectory (s k ) K k=0 generated by π can be regarded as the Euler-Maruyama discretization of a solution X u of the controlled SDE (13), while the trajectory generated by πbase is the discretization of the uncontrolled process X 0 obtained by setting u = 0. As a consequence</p><formula xml:id="formula_159">lim K→∞ E τ ∼π [ K-1 k=0 KL(π(•, •; s k , k)||π base (•, •; s k , k))] = lim K→∞ E τ ∼π [ K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))] = E X u ∼P u [log dP u dP 0 (X u )],<label>(118)</label></formula><p>where P u and P 0 are the measures of the processes X u and X 0 , respectively. The Girsanov theorem (Theorem 2) implies that</p><formula xml:id="formula_160">log dP u dP 0 (X u ) = - T 0 ⟨u(X u t , t), dB t ⟩ -1 2 T 0 ∥u(X u t , t)∥ 2 dt, which implies that E X u ∼P u [log dP u dP 0 (X u )] = -1 2 E X u ∼P u [ T 0 ∥u(X u t , t)∥ 2 dt]. Setting the rewards r k (a k , s k ) = ηf (s k , kη) for k ∈ {0, . . . , K -1} and r K (a K , s K ) = ηg(s k )</formula><p>, where f and g are as in Section 4.1, yields the following limiting object:</p><formula xml:id="formula_161">lim K→∞ E τ ∼π [ K k=0 r k (s k , a k )] = E X u ∼P u [ T 0 f (X u t , t) dt + g(X u T )].<label>(119)</label></formula><p>Hence, the limit of the MaxEnt RL loss (116) is the SOC loss (12).</p><p>Lemma 3. Let π(a k , s k+1 ; s k , k) and πbase (a k , s k+1 ; s k , k) be as defined in (115). KL(π(•,</p><formula xml:id="formula_162">•; s k , k)||π base (•, •; s k , k))]</formula><p>and KL(π(•; s k , k)||π base (•; s k , k))] are equal. Moreover, if q, q base denote the distributions over trajectories induced by π, π base , we have that</p><formula xml:id="formula_163">KL(q||q base ) = E[ K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))].<label>(120)</label></formula><p>Proof. We have that</p><formula xml:id="formula_164">KL(π(•, •; s k , k)||π base (•, •; s k , k))] = a k ,s k+1 π(a k , s k+1 ; s k , k) log π(a k ,s k+1 ;s k ,k) πbase (a k ,s k+1 ;s k ,k) = a k ,s k+1 π(a k ; s k , k)p(s k+1 |a k , s k ) log π(a k ;s k ,k)p(s k+1 |a k ,s k ) π base (a k ;s k ,k)p(s k+1 |a k ,s k ) = a k ,s k+1 π(a k ; s k , k)p(s k+1 |a k , s k ) log π(a k ;s k ,k) π base (a k ;s k ,k) = a k π(a k ; s k , k) s k+1 p(s k+1 |a k , s k ) log π(a k ;s k ,k) π base (a k ;s k ,k) = a k π(a k ; s k , k) log π(a k ;s k ,k) π base (a k ;s k ,k) = KL(π(•; s k , k)||π base (•; s k , k))].<label>(121)</label></formula><p>To prove (120), by construction we can write</p><formula xml:id="formula_165">q(τ ) = p 0 (s 0 ) K-1 k=0 π(a k , s k+1 ; s k , k), q base (τ ) = p 0 (s 0 ) K-1 k=0 πbase (a k , s k+1 ; s k , k),<label>(122)</label></formula><p>which means that</p><formula xml:id="formula_166">KL(q||q base ) = E τ ∼q [log q(τ ) q base (τ ) ] = E τ ∼q [ K-1 k=0 log π(a k ,s k+1 ;s k ,k) πbase (a k ,s k+1 ;s k ,k) ] = K-1 k=0 E τ ∼q 0:(k+1) [log π(a k ,s k+1 ;s k ,k) πbase (a k ,s k+1 ;s k ,k) ] = K-1 k=0 E τ ∼q 0:k [ a k ,s k+1 π(a k , s k+1 ; s k , k) log π(a k ,s k+1 ;s k ,k) πbase (a k ,s k+1 ;s k ,k) ] = K-1 k=0 E τ ∼q 0:k [KL(π(•, •; s k , k)||π base (•, •; s k , k))] = K-1 k=0 E τ ∼q 0:k [KL(π(•; s k , k)||π base (•; s k , k))] = E τ ∼q 0:k [ K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))] (123)</formula><p>Here, the notation q 0:k denotes the trajectory q up to the state s k .</p><p>Lemma 4. The distribution-based MaxEnt RL formulation in (111) is equivalent to the the following problem:</p><formula xml:id="formula_167">min q KL(q||q * ),</formula><p>where q * (τ ) :=</p><formula xml:id="formula_168">q base (τ ) exp K k=0 r k (s k ,a k ) 1 p 0 (s 0 ) {τ ′ |s ′ 0 =s 0 } q base (τ ′ ) exp K k=0 r k (s ′ k ,a ′ k ) , (<label>124</label></formula><formula xml:id="formula_169">)</formula><p>where the minimization is over q with marginal p 0 at step zero. The optimum of the problem is q * , which satisfies the marginal constraint. The following alternative characterization of q * holds:</p><formula xml:id="formula_170">q * (τ ) = q base (τ ) exp K k=0 r k (s k , a k ) -V(s 0 , 0) ,<label>(125)</label></formula><p>where</p><formula xml:id="formula_171">V(x, k) = max π E τ ∼π,p K k ′ =k r k ′ (s k ′ , a k ′ ) - K-1 k ′ =k KL(π(•; s k ′ , k ′ )||π base (•; s k ′ , k ′ ))|s k = x . (126)</formula><p>Proof. Let us expand KL(q||q * ):</p><formula xml:id="formula_172">KL(q||q * ) = E τ ∼q log q(τ ) q * (τ ) = E τ ∼q log q(τ ) -log q base (τ ) - K k=0 r k (s k , a k ) + log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) = KL(q||q base ) -E τ ∼q K k=0 r k (s k , a k ) + E s0∼p0 log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) ,<label>(127)</label></formula><p>where the third equality holds because the marginal of q at step zero is p 0 by hypothesis. Since the third term in the right-hand side is independent of q, this proves the equivalence between ( <ref type="formula" target="#formula_147">111</ref>) and ( <ref type="formula" target="#formula_168">124</ref>).</p><p>Next, we prove that the marginal of q * at step zero is p 0 :</p><formula xml:id="formula_173">{τ |s0=x} q * (τ ) := {τ |s0=x} q base (τ ) exp K k=0 r k (s k ,a k ) 1 p 0 (x) {τ ′ |s ′ 0 =x} q base (τ ′ ) exp K k=0 r k (s ′ k ,a ′ k ) = p 0 (x).<label>(128)</label></formula><p>Now, for an arbitrary s 0 , let q s0 , q * s0 be the distributions q, q * conditioned on the initial state being s 0 . We can write an analog to equation (127) for q s0 , q * s0 :</p><formula xml:id="formula_174">KL(q s0 ||q * s0 ) = E τ ∼qs 0 log qs 0 (τ ) q * s 0 (τ ) = E τ ∼qs 0 log q s0 (τ ) -log q base s0 (τ ) - K k=0 r k (s k , a k ) + log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base s0 (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) = KL(q s0 ||q base s0 ) -E τ ∼qs 0 K k=0 r k (s k , a k ) + log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) ,<label>(129)</label></formula><p>Hence,</p><formula xml:id="formula_175">0 = min qs 0 KL(q s0 ||q * s0 ) = -max qs 0 {E τ ∼qs 0 K k=0 r k (s k , a k ) -KL(q s0 ||q base s0 )} + log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) .<label>(130)</label></formula><p>And applying ( <ref type="formula" target="#formula_163">120</ref>) from ( <ref type="formula" target="#formula_163">120</ref>), we obtain that</p><formula xml:id="formula_176">log 1 p0(s0) {τ ′ |s ′ 0 =s0} q base (τ ′ ) exp K k=0 r k (s ′ k , a ′ k ) = max π E τ ∼π,p K k=0 r k (s k , a k ) - K-1 k=0 KL(π(•; s k , k)||π base (•; s k , k))|s 0 = V(s 0 , 0),<label>(131)</label></formula><p>which concludes the proof.</p><p>C.4 Proof of equation (18): the control cost is a KL regularizer Theorem 2 (Girsanov theorem for SDEs). If the two SDEs</p><formula xml:id="formula_177">dX t = b 1 (X t , t) dt + σ(X t , t) dB t , X 0 = x init (<label>132</label></formula><formula xml:id="formula_178">)</formula><formula xml:id="formula_179">dY t = (b 1 (Y t , t) + b 2 (Y t , t)) dt + σ(Y t , t) dB t , Y 0 = x init (<label>133</label></formula><formula xml:id="formula_180">)</formula><p>admit unique strong solutions on [0, T ], then for any bounded continuous functional Φ on C([0, T ]), we have that We consider the forward-backward SDEs ( <ref type="formula" target="#formula_82">63</ref>)-( <ref type="formula" target="#formula_84">64</ref>) with arbitrary noise schedule. By Proposition 4, the trajectories ⃗ X, X of these two processes are equally distributed up to a time flip, which also means that their marginals satisfy ⃗ p t = p 1-t , for all t ∈ [0, 1]. First, we develop an explicit expression for the score function s(x, t) = ∇ log p t (x). By the properties of flow matching, we know that p t is the distribution of the interpolation variable Xt = β t X0 + α t X1 , where X0 ∼ N (0, I), X1 ∼ p data are independent. Thus, Xt-αt X1 βt ∼ N (0, I), which means that we can express the density p t as</p><formula xml:id="formula_181">E[Φ(X)] = E Φ(Y ) exp - T 0 σ(Y t , t) -1 b 2 (Y t , t) dB t -1 2 T 0 ∥σ(Y t , t) -1 b 2 (Y t , t)∥ 2 dt = E Φ(Y ) exp - T 0 σ(Y t , t) -1 b 2 (Y t , t) d Bt + 1 2 T 0 ∥σ(Y t , t) -1 b 2 (Y t , t)∥ 2 dt ,<label>(134</label></formula><formula xml:id="formula_182">p t (x) = R d exp - ∥x-α t y∥ 2 2β 2 t (2πβ 2 t ) d/2</formula><p>p data (y) dy.</p><p>(139) Thus,</p><formula xml:id="formula_183">s(x, t) = ∇ log p t (x) = -x β 2 t + αt β 2 t R d y exp - ∥x-α t y∥ 2 2β 2 t p data (y) dy R d exp - ∥x-α t y∥ 2 2β 2 t p data (y) dy := -x-αtξt(x) β 2 t ,<label>(140)</label></formula><p>where we defined</p><formula xml:id="formula_184">ξ t (x) = R d y exp - ∥x-α t y∥ 2 2β 2 t p data (y) dy R d exp - ∥x-α t y∥ 2 2β 2 t p data (y) dy .<label>(141)</label></formula><p>Hence, we can rewrite the forward SDE (63) as</p><formula xml:id="formula_185">d ⃗ X t = -κ 1-t ⃗ X t -σ(1-t) 2 2 -η 1-t ⃗ Xt-α1-tξ1-t( ⃗ Xt) β 2 1-t dt + σ(1 -t) dB t , ⃗ X 0 ∼ p data<label>(142)</label></formula><p>Hence, if we substitute κ 1-t ← κ 1-t + σ(1-t) 2 -2η1-t</p><formula xml:id="formula_186">2β 2 1-t , ξ 1-t ← α1-t(σ(1-t) 2 -2η1-t) 2β 2 1-t ξ 1-t ( ⃗ X t</formula><p>) (where we ignore the dependency on ⃗ X t ), √ 2η 1-t ← σ(1 -t), we can apply Lemma 2, which yields</p><formula xml:id="formula_187">⃗ X t = ⃗ X 0 exp - t 0 κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds + t 0 exp - t t ′ κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds α 1-t ′ (σ(1-t ′ ) 2 -2η 1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ + t 0 σ(1 -t ′ ) exp - t t ′ κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds dB t ′ .<label>(143)</label></formula><p>We simplify the recurring expression:</p><formula xml:id="formula_188">κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s = α1-s α1-s + σ(1-s) 2 -2β1-s α1-s α 1-s β1-s-β1-s 2β 2 1-s = σ(1-s) 2 2β 2 1-s + β1-s β1-s<label>(144)</label></formula><p>Thus,</p><formula xml:id="formula_189">t t ′ κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds = t t ′ σ(1-s) 2 2β 2 1-s -∂ s log β 1-s ds = t t ′ σ(1-s) 2 2β 2 1-s ds -log β 1-t -log β 1-t ′ ,<label>(145)</label></formula><p>which means that exp -</p><formula xml:id="formula_190">t t ′ κ 1-s + σ(1-s) 2 -2η1-s 2β 2 1-s ds = exp - t t ′ σ(1-s) 2 2β 2 1-s ds β1-t β 1-t ′ ,<label>(146)</label></formula><formula xml:id="formula_191">α 1-t ′ (σ(1-t ′ ) 2 -2η 1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) = α 1-t ′ σ(1-t ′ ) 2 2β 2 1-t ′ + β1-t ′ β 1-t ′ -α1-t ′ α 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ). (<label>147</label></formula><formula xml:id="formula_192">) If we define χ(1 -s) such that σ 2 (1 -s) = 2β 1-s α1-s α1-s β 1-s -β1-s + χ(1 -s), we obtain that exp - t t ′ σ(1-s) 2 2β 2 1-s ds β1-t β 1-t ′ = exp - t t ′ α1-s α1-s -β1-s β1-s + χ(1-s) 2β 2 1-s ds β1-t β 1-t ′ = exp t t ′ ∂ s log α 1-s -∂ s log β 1-s -χ(1-s) 2β 2 1-s ds β1-t β 1-t ′ = exp - t t ′ χ(1-s) 2β 2 1-s ds α1-t α 1-t ′ ,<label>(148)</label></formula><formula xml:id="formula_193">α 1-t ′ σ(1-t ′ ) 2 2β 2 1-t ′ + β1-t ′ β 1-t ′ -α1-t ′ α 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) = α 1-t ′ χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ )<label>(149)</label></formula><p>If we plug equations ( <ref type="formula" target="#formula_192">148</ref>)-( <ref type="formula" target="#formula_193">149</ref>) into ( <ref type="formula" target="#formula_190">146</ref>)-( <ref type="formula" target="#formula_191">147</ref>), and then those into (143), we obtain that</p><formula xml:id="formula_194">⃗ X t = ⃗ X 0 exp - t 0 χ(1-s) 2β 2 1-s ds α1-t α1 + α 1-t t 0 exp - t t ′ χ(1-s) 2β 2 1-s ds χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ + t 0 2β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ + χ(1 -t ′ ) exp - t t ′ χ(1-s) 2β 2 1-s ds α1-t α 1-t ′ dB t ′ .</formula><p>(150) and if we take the limit t → 1 -and use that</p><formula xml:id="formula_195">α 1 = 1, ⃗ X 1 = ⃗ X 0 lim t→1 -exp - t 0 χ(1-s) 2β 2 1-s ds α 1-t + lim t→1 -α 1-t t 0 exp - t t ′ χ(1-s) 2β 2 1-s ds χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ + lim t→1 - t 0 2β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ + χ(1 -t ′ ) exp - t t ′ χ(1-s) 2β 2 1-s ds α1-t α 1-t ′ dB t ′ . (<label>151</label></formula><formula xml:id="formula_196">)</formula><p>The assumption on χ in ( <ref type="formula" target="#formula_33">25</ref>) is equivalent, up to a rearrangement of the notation and a flip in the time variable, to the statement that for all t ′ ∈ [0, 1),</p><formula xml:id="formula_197">lim t→1 -exp - t t ′ χ(1-s) 2β 2 1-s ds α 1-t = 0.<label>(152)</label></formula><p>Hence, under assumption (25), the factor accompanying ⃗ X 0 in equation ( <ref type="formula" target="#formula_195">151</ref>) is zero. Moreover, this assumption also implies that</p><formula xml:id="formula_198">lim t→1 -α 1-t t 0 exp - t t ′ χ(1-s) 2β 2 1-s ds χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ = 1 0 lim t→1 -exp - t t ′ χ(1-s) 2β 2 1-s ds α 1-t χ(1-t ′ ) 2β 2 1-t ′ ξ 1-t ′ ( ⃗ X t ′ ) dt ′ = 0.<label>(153)</label></formula><p>If we plug ( <ref type="formula" target="#formula_197">152</ref>) and ( <ref type="formula" target="#formula_198">153</ref>) into (151), we obtain that</p><formula xml:id="formula_199">⃗ X 1 = lim t→1 - t 0 2β 1-t ′ α1-t ′ α 1-t ′ β 1-t ′ -β1-t ′ + χ(1 -t ′ ) exp - t t ′ χ(1-s) 2β 2 1-s ds α1-t α 1-t ′ dB t ′ ,<label>(154)</label></formula><p>which shows that ⃗ X 1 is independent of ⃗ X 0 . Next, we leverage that ⃗ X and X have equal distributions over trajectories (Proposition 4). In particular, the joint distribution of ( ⃗ X 0 , ⃗ X 1 ) is equal to the joint distribution of (X 1 , X 0 ). We conclude that X 1 and X 0 are independent, which is the definition of the memorylessness property. Hence, the assumption ( <ref type="formula" target="#formula_33">25</ref>) is sufficient for memorylessness to hold.</p><p>It remains to prove that the assumption (25) is necessary. Looking at equation ( <ref type="formula">150</ref>) we deduce that generally, for any t ∈ [0, 1), ⃗ X 0 and ⃗ X t are not independent, because the first two terms in (150) are different from zero. Thus, if there existed a t ′ ∈ [0, 1) such that the limit ( <ref type="formula" target="#formula_197">152</ref>) is different from zero, then ⃗ X 1 would not be independent from ⃗ X t ′ , which means that in general it would not be independent of ⃗ X 0 either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Proof of Theorem 1: fine-tuning recipe for general noise schedules</head><p>The proof of this result relies heavily on the properties of the Hamilton-Jacobi-Bellman equation:</p><p>Theorem 3 (Hamilton-Jacobi-Bellman equation). If we define the infinitesimal generator</p><formula xml:id="formula_200">L := 1 2 d i,j=1 (σσ ⊤ ) ij (t)∂ xi ∂ xj + d i=1 b i (x, t)∂ xi ,<label>(155)</label></formula><p>the value function V for the SOC problem (12)-( <ref type="formula" target="#formula_14">13</ref>) solves the following Hamilton-Jacobi-Bellman (HJB) partial differential equation:</p><formula xml:id="formula_201">∂ t V (x, t) = -LV (x, t) + 1 2 ∥(σ ⊤ ∇V )(x, t)∥ 2 -f (x, t), V (x, T ) = g(x).<label>(156)</label></formula><p>Consider forward SDEs like (63), starting from the distributions p base and p * , where p * (x) ∝ p base (x) exp(r(x)).</p><formula xml:id="formula_202">d ⃗ X t = ⃗ b( ⃗ X t , t) dt + σ(t) dB t , ⃗ X 0 ∼ p base , (157) d ⃗ X * t = ⃗ b * ( ⃗ X * t , t) dt + σ(t) dB t , ⃗ X 0 ∼ p * . (<label>158</label></formula><formula xml:id="formula_203">)</formula><p>where the drifts are defined as</p><formula xml:id="formula_204">⃗ b(x, t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t s(x, 1 -t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t ∇ log ⃗ p t (x), ⃗ b * (x, t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t s * (x, 1 -t) = -κ 1-t x + σ(1-t) 2 2 -η 1-t ∇ log ⃗ p * t (x),<label>(159)</label></formula><p>and ⃗ p t , ⃗ p * t are the densities of X t , ⃗ X t , respectively. ⃗ p t , ⃗ p * t satisfy Fokker-Planck equations:</p><formula xml:id="formula_205">∂ t ⃗ p t = ∇ • ( ⃗ b(x, t)⃗ p t ) + ∇ • ( σ(1-t) 2 2 ∇⃗ p t ), ⃗ p 0 = p base , ∂ t ⃗ p * t = ∇ • ( ⃗ b * (x, t)⃗ p * t ) + ∇ • ( σ(1-t) 2 2 ∇⃗ p * t ), ⃗ p 0 = p * .<label>(160)</label></formula><p>Plugging ( <ref type="formula" target="#formula_204">159</ref>) into (160), we obtain</p><formula xml:id="formula_206">∂ t ⃗ p t = ∇ • (κ 1-t x⃗ p t ) + ∇ • η 1-t ∇⃗ p t , ⃗ p 0 = p base , ∂ t ⃗ p * t = ∇ • (κ 1-t x⃗ p * t ) + ∇ • η 1-t ∇⃗ p * t , ⃗ p 0 = p * .<label>(161)</label></formula><p>We apply the Hopf-Cole transformation to obtain PDEs for -log ⃗ p t (and -log ⃗ p * t analogously):</p><formula xml:id="formula_207">-∂ t (-log ⃗ p t ) = ∂tpt pt = ∇•(κ1-tx⃗ pt)+∇• η1-t∇⃗ pt pt = κ 1-t ∇ • x + κ 1-t ⟨x, ∇ log ⃗ p t ⟩ + η 1-t ∇•(∇ log ⃗ pt exp(log pt)) pt = κ 1-t d + κ 1-t ⟨x, ∇ log ⃗ p t ⟩ + η 1-t ∆ log ⃗ p t + ∥∇ log ⃗ p t ∥ 2 .<label>(162)</label></formula><p>Hence, if we define V (x, t) = -log ⃗ p t (x), V * (x, t) = -log ⃗ p * t (x), then V and V * satisfy the following Hamilton-Jacobi-Bellman equations:</p><formula xml:id="formula_208">-∂ t V = κ 1-t d -κ 1-t ⟨x, ∇V ⟩ + η 1-t -∆V + ∥∇V ∥ 2 , V (x, 0) = -log p base (x),<label>(163)</label></formula><formula xml:id="formula_209">-∂ t V * = κ 1-t d -κ 1-t ⟨x, ∇V * ⟩ + η 1-t -∆V * + ∥∇V * ∥ 2 , V * (x, 0) = -log p * (x).<label>(164)</label></formula><p>Now, define V (x, t) = V * (x, t) -V (x, t). Subtracting ( <ref type="formula" target="#formula_209">164</ref>) from ( <ref type="formula" target="#formula_208">163</ref>), we obtain</p><formula xml:id="formula_210">-∂ t V = -κ 1-t ⟨x, ∇ V ⟩ + η 1-t -∆ V + ∥∇V * ∥ 2 -∥∇V ∥ 2 = -κ 1-t ⟨x, ∇ V ⟩ + η 1-t -∆ V + ∥∇( V + V )∥ 2 -∥∇V ∥ 2 = -κ 1-t ⟨x, ∇ V ⟩ + η 1-t -∆ V + ∥∇ V ∥ 2 + 2⟨∇V , ∇ V ⟩ = ⟨-κ 1-t x + 2η 1-t ∇V , ∇ V ⟩ + η 1-t -∆ V + ∥∇ V ∥ 2 = ⟨-κ 1-t x -2η 1-t s(x, 1 -t), ∇ V ⟩ + η 1-t -∆ V + ∥∇ V ∥ 2 , V (x, 0) = -log p * (x) + log p base (x) = -r(x) + log p base (y) exp(r(y)) dy .<label>(165)</label></formula><p>Hence, V also satisfies a Hamilton-Jacobi-Bellman equation. If we define V such that V (x, t) = V (x, 1 -t), we have that</p><formula xml:id="formula_211">∂ t V = ⟨-κ t x -2η t s(x, t), ∇V ⟩ + η t -∆V + ∥∇V ∥ 2 , V (x, 1) = r(x) -log p base (y) exp(r(y)) dy . (<label>166</label></formula><formula xml:id="formula_212">)</formula><p>Using Theorem 3, we can reverse-engineer V as the value function of the following SOC problem:</p><formula xml:id="formula_213">min u∈U E 1 2 1 0 ∥u(X u t , t)∥ 2 dt-r(x)+log p base (y) exp(r(y)) dy ,<label>(167) s</label></formula><formula xml:id="formula_214">.t. dX u t = κ t x + 2η t s(x, t)+ √ 2η t u(X u t , t) dt+ √ 2η t dB t , X u 0 ∼ p 0 .<label>(168)</label></formula><p>Note that this SOC problem is equal to the problem ( <ref type="formula" target="#formula_13">12</ref>)-( <ref type="formula" target="#formula_14">13</ref>) with the choices f = 0, g = -r, and σ(t) = √ 2η t . By equation ( <ref type="formula" target="#formula_19">17</ref>), the optimal control of the problem (167)-( <ref type="formula" target="#formula_214">168</ref>) is of the form:</p><formula xml:id="formula_215">u * (x, t) = - √ 2η t ∇V (x, t) = - √ 2η t ∇ V (x, 1 -t) = - √ 2η t ∇V * (x, 1 -t) -∇V (x, 1 -t) = - √ 2η t -∇ log ⃗ p * 1-t (x) + ∇ log ⃗ p 1-t (x) = √ 2η t s * (x, t) -s(x, t) ,<label>(169)</label></formula><formula xml:id="formula_216">⇐⇒ s * (x, t) = s(x, t) + u * (x, t)/ √ 2η t .<label>(170)</label></formula><p>As in (64), the backward SDEs corresponding to the forward SDEs (158) take the following form:</p><formula xml:id="formula_217">dX * t = κ t X * t + σ(t) 2 2 + η t s * (X * t , t) dt + σ(t) dB t , X * 0 ∼ N (0, I).<label>(171)</label></formula><p>If we plug (170) into this equation, we obtain</p><formula xml:id="formula_218">dX * t = κ t X * t + σ(t) 2 2 + η t s(X * t , t) + u * (X * t ,t) √ 2ηt dt + σ(t) dB t , X * 0 ∼ N (0, I),<label>(172)</label></formula><formula xml:id="formula_219">⇐⇒ dX * t = b(X * t , t) + σ(t) 2 2 +ηt √ 2ηt u * (X * t , t) dt + σ(t) dB t , X * 0 ∼ N (0, I). (<label>173</label></formula><formula xml:id="formula_220">)</formula><p>where we used that b(x, t) = κ t x + σ(t) 2 2 + η t s(x, t) by definition in equation ( <ref type="formula" target="#formula_12">11</ref>).</p><p>The fine-tuned inference SDE for DDIM Now, for DDIM, we have that u * (x, t) = -αt αt(1-αt) (ϵ * (x, t)ϵ base (x, t)) by ( <ref type="formula">26</ref>). Hence,</p><formula xml:id="formula_221">σ(t) 2 2 +ηt √ 2ηt u * (x, t) = - σ(t) 2 2 + αt 2α t αt α t αt αt(1-αt) (ϵ * (x, t) -ϵ base (x, t)) = - σ(t) 2 2 + αt 2α t √ 1-αt (ϵ * (x, t) -ϵ base (x, t)),<label>(174)</label></formula><formula xml:id="formula_222">=⇒ b(x, t) + σ(t) 2 2 +ηt √ 2ηt u * (x, t) = αt 2αt X t -αt 2αt + σ(t) 2 2 ϵ base (Xt,t) √ 1-αt - σ(t) 2 2 + αt 2α t √ 1-αt (ϵ * (x, t) -ϵ base (x, t)) = αt 2αt X t -αt 2αt + σ(t) 2 2 ϵ * (Xt,t) √ 1-αt .<label>(175)</label></formula><p>We obtain that the fine-tuned inference SDE for DDIM is</p><formula xml:id="formula_223">dX * t = αt 2αt X * t -αt 2αt + σ(t) 2 2 ϵ * (X * t ,t) √ 1-αt dt + σ(t) dB t , X * 0 ∼ N (0, I),<label>(176)</label></formula><p>which is matches the SDE (6) with the choice ϵ = ϵ * .</p><p>The fine-tuned inference SDE for Flow Matching For Flow Matching, we have that u</p><formula xml:id="formula_224">* (x, t) = 2 βt( αt α t βt-βt) (v * (x, t)- v base (x, t)) by (27). Hence, σ(t) 2 2 +ηt √ 2ηt u * (x, t) = σ(t) 2 2 +βt( αt α t βt-βt) 2βt( αt α t βt-βt) 2 βt( αt α t βt-βt) (v * (x, t) -v base (x, t)) = 1 + σ(t) 2 2βt( αt α t βt-βt) (v * (x, t) -v base (x, t)). (177) =⇒ b(x, t) + σ(t) 2 2 +ηt √ 2ηt u * (x, t) = v base (x, t) + σ(t) 2 2βt( αt α t βt-βt) v base (x, t) -αt αt x + 1 + σ(t) 2 2βt( αt α t βt-βt) (v * (x, t) -v base (x, t)) = v * (x, t) + σ(t) 2 2βt( αt α t βt-βt) v * (x, t) -αt αt x .<label>(178)</label></formula><p>We obtain that the fine-tuned inference SDE for Flow Matching is</p><formula xml:id="formula_225">dX * t = v(X * t , t) + σ(t) 2 2βt( αt α t βt-βt) v * (X * t , t) -αt αt X * t dt + σ(t) dB t , X * 0 ∼ N (0, I),<label>(179)</label></formula><p>which matches equation ( <ref type="formula" target="#formula_3">4</ref>) with the choice v = v * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Loss function derivations</head><p>E.1 Derivation of the Continuous Adjoint method Proposition 6. The gradient dL dθ of the adjoint loss L(u; X) defined in (28) with respect to the parameters θ of the control can be expressed as in (32).</p><p>Proof. First, note that we can write</p><formula xml:id="formula_226">∇ θ E T 0 1 2 ∥u θ (X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = E T 0 ∇ θ u θ (X u θ t , t)u θ (X u θ t , t) dt + ∇ θ E T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) | v=stopgrad(u θ ) .<label>(180)</label></formula><p>To develop the second term, we apply Lemma 5. Namely, by the Leibniz rule and equation ( <ref type="formula" target="#formula_230">185</ref>), we have that</p><formula xml:id="formula_227">∇ θ E T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) | v=stopgrad(u θ ) = E ∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) | v=stopgrad(u θ ) = E T 0 (∇ θ u θ )(X u θ t (ω), t) ⊤ σ(t) ⊤ a t (ω) dt .<label>(181)</label></formula><p>Plugging the right-hand side of this equation into (180) concludes the proof.</p><p>Lemma 5. Let v be an arbitrary fixed vector field. The unique solution of the ODE</p><formula xml:id="formula_228">d dt a(t; X u , u) = -∇ X u t (b(X u t , t) + σ(t)u(X u t , t)) T a(t; X u , u) + ∇ X u t f (X u t , t) + 1 2 ∥v(X u t , t)∥ 2 , (182) a(1; X u , u) = ∇g(X u 1 ),<label>(183)</label></formula><p>satisfies:</p><formula xml:id="formula_229">a(t; X u , u) := ∇ X u t 1 t 1 2 ∥u(X u t ′ , t ′ )∥ 2 +f (X u t ′ , t ′ ) dt ′ +g(X u 1 ) , where X u solves dX u t = b(X u t , t) + σ(t)u(X u t , t) dt + σ(t)dB t .<label>(184)</label></formula><p>Moreover, when u = u θ is parameterized by θ we have that</p><formula xml:id="formula_230">∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = T 0 (∇ θ u θ )(X u θ t (ω), t)σ(t) ⊤ a t (ω) dt.<label>(185)</label></formula><p>Proof. We use an approach based on Lagrange multipliers which mirrors and extends the derivation of the adjoint ODE <ref type="bibr">(Domingo-Enrich et al., 2023, Lemma 8)</ref>. For shortness, we use the notation bθ (x, t) :</p><formula xml:id="formula_231">= b(x, t) + σ(t)u θ (x, t). Define a process a : Ω × [0, T ] → R d such that for any ω ∈ Ω, a(ω, •) is differentiable. For a given ω ∈ Ω, we can write T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) - T 0 ⟨a t (ω), (dX u θ t (ω) -bθ (X u θ t (ω), t) dt -σ(t) dB t )⟩.<label>(186)</label></formula><p>By stochastic integration by parts (Domingo-Enrich et al., 2023, Lemma 9), we have that</p><formula xml:id="formula_232">T 0 ⟨a t (ω), dX u θ t (ω)⟩ = ⟨a T (ω), X u θ T (ω)⟩ -⟨a 0 (ω), X u θ 0 (ω)⟩ - T 0 ⟨X u θ t (ω), dat dt (ω)⟩ dt.<label>(187)</label></formula><p>Hence, if X u θ 0 = x 0 is the initial condition, we have that 9</p><formula xml:id="formula_233">∇ x0 T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = ∇ x0 T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) -⟨a T (ω), X u θ T (ω)⟩ + ⟨a 0 (ω), X u θ 0 (ω)⟩ + T 0 ⟨a t (ω), bθ (X u θ t (ω), t)⟩ + ⟨ dat dt (ω), X u θ t (ω)⟩ dt + T 0 ⟨a t (ω), σ(t) dB t ⟩ = T 0 ∇ x0 X u θ t (ω) ⊤ ∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) dt + ∇ x0 X u θ T (ω) ⊤ ∇ x g(X u θ T (ω)) -∇ x0 X u θ T (ω) ⊤ a T (ω) + ∇ x0 X u θ 0 (ω) ⊤ a 0 (ω) + T 0 ∇ x0 X u θ t (ω) ⊤ ∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) + ∇ x0 X u θ t (ω) ⊤ dat dt (ω) dt = T 0 ∇ x0 X u θ t (ω) ⊤ ∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) + ∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) + dat dt (ω) dt + ∇ x0 X u θ T (ω) ⊤ ∇ x g(X u θ T (ω)) -a T (ω) + a 0 (ω).<label>(188)</label></formula><p>In the last line we used that</p><formula xml:id="formula_234">∇ x0 X u θ 0 (ω) = ∇ x0 x 0 = I. If choose a such that da t (ω) = -∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) -∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) dt, a T (ω) = ∇ x g(X u θ T (ω)),<label>(189)</label></formula><p>which is the ODE ( <ref type="formula">182</ref>)-( <ref type="formula" target="#formula_228">183</ref>), then we obtain that</p><formula xml:id="formula_235">∇ x0 T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = a 0 (ω)<label>(190)</label></formula><p>Without loss of generality, this argument can be extended from t = 0 to an arbitrary t ∈ [0, 1], which proves the first statement of the lemma.</p><p>To prove (185), we similarly write ∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = ∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) -⟨a T (ω), X u θ T (ω)⟩ + ⟨a 0 (ω), X u θ 0 (ω)⟩ + T 0 ⟨a t (ω), bθ (X u θ t (ω), t)⟩ + ⟨ dat dt (ω), X u θ t (ω)⟩ dt + T 0 ⟨a t (ω), σ(t) dB t ⟩ = T 0 ∇ θ X u θ t (ω) ⊤ ∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) dt + ∇ θ X u θ T (ω) ⊤ ∇ x g(X u θ T (ω)) -∇ θ X u θ T (ω) ⊤ a T (ω) + ∇ θ X u θ 0 (ω) ⊤ a 0 (ω) + T 0 ∇ θ X u θ t (ω) ⊤ ∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) + ∇ θ bθ (X u θ t (ω), t) ⊤ a t (ω) + ∇ θ X u θ t (ω) ⊤ dat dt (ω) dt = T 0 ∇ θ X u θ t (ω) ⊤ ∇ x 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t (ω), t) + ∇ x bθ (X u θ t (ω), t) ⊤ a t (ω) + dat dt (ω) dt + ∇ θ X u θ T (ω) ⊤ ∇ x g(X u θ T (ω)) -a T (ω) + T 0 (∇ θ bθ )(X u θ t (ω), t) ⊤ a t (ω) dt.</p><p>(191)</p><p>In the last line we used that ∇ θ X u θ 0 (ω) = ∇ θ x = 0. When a satisfies (189), we obtain that ∇ θ T 0 1 2 ∥v(X u θ t , t)∥ 2 +f (X u θ t , t) dt+g(X u θ T ) = T 0 (∇ θ bθ )(X u θ t (ω), t)a t (ω) dt = T 0 (∇ θ u θ )(X u θ t (ω), t) ⊤ σ(t) ⊤ a t (ω) dt.</p><p>(192)</p><p>The last equality holds because bθ (x, t) := b(x, t) + σ(t)u θ (x, t).</p><p>9 Unlike (Domingo-Enrich et al., 2023, Lemma 8), we use the convention that a Jacobian matrix J = ∇xv(x) is defined as</p><formula xml:id="formula_236">J ij = ∂v i (x) ∂x j</formula><p>. Their definition of ∇xv is the transpose of ours.</p><p>Plugging this back into (200), we obtain that 0 = ∂ t J(u; x, t) + ⟨∇J(u; x, t), b(x, t)⟩ + Tr σ(t)σ(t) ⊤ 2 ∇ 2 J(u; x, t) -1 2 ∥σ(t) ⊤ ∇ x J(u; x, t)∥ 2 + f (x, t). (202) And since J(u; x, T ) = g(x) by construction, we conclude that J(u; x, t) satisfies the HJB equation ( <ref type="formula" target="#formula_201">156</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Theoretical guarantees of the Adjoint Matching loss</head><p>Proposition 7 (Theoretical guarantee of the Adjoint Matching loss). The only critical point of the loss E[L Adj-Match ] is the optimal control u * .</p><p>Proof. Let v be an arbitrary control. If ã(t; X v ) is the solution of the Lean Adjoint ODE (38)-( <ref type="formula" target="#formula_56">39</ref>), it satisfies the integral equation</p><formula xml:id="formula_237">ã(t; X v ) = T t ∇ x b(X v s , s) ⊤ ã(s; X v ) + ∇ x f (X v s , s) ds + ∇g(X v T ).<label>(203)</label></formula><p>Hence,</p><formula xml:id="formula_238">E ã(t; X v ) X v t = E T t ∇ x b(X v s , s) ⊤ ã(s; X v ) + ∇ x f (X v s , s) ds + ∇g(X v T ) X v t = E T t ∇ x b(X v s , s) ⊤ E ã(s; X v ) X v s + ∇ x f (X v s , s) ds + ∇g(X v T ) X v t ,<label>(204)</label></formula><p>where we used the tower property of conditional expectation in the second equality.</p><p>Similarly, if a(t; X v , v) is the solution of the Adjoint ODE ( <ref type="formula" target="#formula_39">30</ref>)-( <ref type="formula" target="#formula_40">31</ref>), it satisfies the integral equation</p><formula xml:id="formula_239">a(t; X v , v) = T t ∇ x b(X v s , s) ⊤ a(s; X v , v) + σ(s)v(X v s , s) + ∇ x f (X v s , s) + 1 2 ∥v(X v s , s)∥ 2 ds + ∇g(X v T ),<label>(205)</label></formula><p>and its expected value satisfies E a(t; X v , v) X</p><p>v t = E T t ∇ x b(X v s , s) + σ(s)v(X v s , s) ⊤ a(s; X v , v) + ∇ x f (X v s , s) + 1 2 ∥v(X v s , s)∥ 2 ds + ∇g(X v T ) X v t = E T t ∇ x b(X v s , s)+σ(s)v(X v s , s) ⊤ E a(s; X v , v) X v s +∇ x f (X v s , s)+ 1 2 ∥v(X v s , s)∥ 2 ds+∇g(X v T ) X v t . (206) Let us rewrite E[L Adj-Match ] as follows: E[L Adj-Match (u)] := E T 0 u(X v t , t) + σ(t) ⊤ E ã(t, X v )|X v t 2 dt | v=stopgrad(u) + E T 0 σ(t) ⊤ E ã(t, X v )|X v t -ã(t, X v ) 2 dt | v=stopgrad(u) , (207) Now, suppose that û is a critical point of E[L Adj-Match ]. By definition, this implies that the first variation of E[L Adj-Match ] is zero. Using (207), we can write this as follows: 0 = δ δu E[L Adj-Match (û)](x) = 2 û(x, t) + σ(t) ⊤ E[ã(t, X û)|X û t = x] , (208) =⇒ û(x, t) = -σ(t) ⊤ E[ã(t, X û)|X û t = x]. (209) Hence, we have ∇ x û(X û t , t) ⊤ σ(t) ⊤ E[ã(t, X û)|X û t ] + ∇ x û(X û t , t) ⊤ û(X û t , t) = 0, (210) =⇒ E T t ∇ x σ(s)û(X û s , s) ⊤ E ã(s; X û) X û s +∇ x 1 2 ∥û(X û s , s)∥ 2 ds X û t = 0. (211) If we set v = û in equation (204), and add (211) to its right-hand side, we obtain that E[ã(t, X û)|X û t ] also solves the integral equation E ã(t; X û) X û t = E T t ∇ x b(X û s , s)+σ(s)û(X û s , s) ⊤ E ã(s; X û) X û s +∇ x f (X û s , s)+ 1 2 ∥û(X û s , s)∥ 2 ds+∇g(X û T ) X û t . (212) Note that this integral equation is the same one as equation (206) when we set v = û in the latter. Proposition 8 states that the solution of the integral equation is unique, which means that E ã(t; X û) X û t = E a(t; X û, û) X û t for all t ∈ [0, T ]. Since we can reexpress the basic Adjoint Matching loss as E[L Basic-Adj-Match (u)] := E T 0 u(X v t , t) + σ(t) ⊤ E a(t; X v , v)|X v t 2 dt | v=stopgrad(u) + E T 0 σ(t) ⊤ E a(t; X v , v)|X v t -a(t; X v , v) 2 dt | v=stopgrad(u) , (213) we obtain that when û is a critical point of E[L Adj-Match ], d du E[L Basic-Adj-Match (û)](x) = 2 û(x, t) + σ(t) ⊤ E[a(t; X û, û)|X û t = x] = 2 û(x, t) + σ(t) ⊤ E[ã(t; X û)|X û t</p><formula xml:id="formula_240">= x] = 0,<label>(214)</label></formula><p>where the second equality holds because E ã(t; X û) X û t = E a(t; X û, û) X û t , and the third equality holds by equation ( <ref type="formula">209</ref>). Thus, we deduce that the critical points of E[L Adj-Match ] are critical points of E[L Basic-Adj-Match ]. By Proposition 2, E[L Basic-Adj-Match ] has a single critical point, which is the optimal control u * , which concludes the proof of the statement for E[L Adj-Match ].</p><p>Proposition 8. Let v be an arbitrary control. Consider the integral equation:</p><formula xml:id="formula_241">Y t = E T t ∇ x b(X v s , s)+σ(s)v(X v s , s) ⊤ Y s +∇ x f (X v s , s)+ 1 2 ∥v(X v s , s)∥ 2 ds+∇g(X v T ) X v t ,<label>(215)</label></formula><p>where t ∈ [0, T ]. This equation has a unique solution, i.e. if Y 1 , Y 2 are two solutions then Y 1 = Y 2 .</p><p>Proof. Let Y 1 , Y 2 be two solutions of the integral equation. We have that</p><formula xml:id="formula_242">Y 1 t -Y 2 t = E T t (Y 1 s -Y 2 s ) ⊤ ∇ x b(X * s , s) ds X * t .<label>(216)</label></formula><p>Thus,</p><formula xml:id="formula_243">∥Y 1 t -Y 2 t ∥ ≤ E T t (Y 1 s -Y 2 s ) ⊤ ∇ x b(X * s , s) ds X * t ≤ E T t (Y 1 s -Y 2 s ) ⊤ ∇ x b(X * s , s) ds X * t ≤ E T t Y 1 s -Y 2 s • ∇ x b(X * s , s) ds X * t = T t E Y 1 s -Y 2 s • ∇ x b(X * s , s) X * t ds ≤ T t E Y 1 s -Y 2 s 2 X * t 1/2 • E ∇ x b(X * s , s) 2 X * t 1/2 ds (217)</formula><p>And this implies that</p><formula xml:id="formula_244">sup t ′ ∈[0,t] E[∥Y 1 t -Y 2 t ∥ 2 |X * t ′ ] 1/2 ≤ T t E Y 1 s -Y 2 s 2 X * t 1/2 • E ∇ x b(X * s , s) 2 X * t 1/2 ds ≤ T t sup t ′ ∈[0,s] E Y 1 s -Y 2 s 2 X * t ′ 1/2 • sup t ′ ∈[0,s] E ∇ x b(X * s , s) 2 X * t ′</formula><p>1/2 ds.</p><p>(218) Applying Grönwall's inequality on the function f (t) = sup t ′ ∈[0,t] E[∥Y 1 t -Y 2 t ∥ 2 |X * t ′ ] 1/2 , we obtain that sup t ′ ∈[0,t] E[∥Y 1 t -Y 2 t ∥ 2 |X * t ′ ] 1/2 = 0 for all t ∈ [0, T ], which means that Y 1 t = Y 2 t almost surely. And since ∥Y 1 t -Y 2 t ∥ ≤ T t E Y 1 s -Y 2 s 2 |X * t 1/2 • E ∇ x b(X * s , s) 2 |X * t 1/2 ds = 0, we obtain that Y 1 = Y 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Pseudo-code of Adjoint Matching for DDIM fine-tuning</head><p>Note that for each pair of equations ( <ref type="formula">219</ref>)-( <ref type="formula" target="#formula_245">220</ref>), ( <ref type="formula" target="#formula_246">221</ref>)-( <ref type="formula" target="#formula_247">222</ref>), ( <ref type="formula" target="#formula_248">223</ref>)-( <ref type="formula" target="#formula_250">224</ref>), the first equation corresponds to the updates in the DDPM paper, while the second equation is an Euler-Maruyama / Euler discretization of the continuous-time object. To check that both discretizations are equal up to first order, remark that </p><p>For each trajectory, solve the lean adjoint ODE (38)-( <ref type="formula" target="#formula_56">39</ref>) backwards in time from k = K to 0, e.g.:</p><formula xml:id="formula_246">ãk = ãk+1 + ãT k+1 ∇X k ᾱk+1 ᾱk X k - 1-ᾱk / ᾱk+1 √ 1-ᾱk ϵ base (X k , k) -X k , ãK = ∇X K r(XK ),<label>(221)</label></formula><p>or ãk = ãk+1 + ãT k+1 ∇X t ᾱk+1 -ᾱk 2 ᾱk</p><formula xml:id="formula_247">X k -ᾱk+1 -ᾱk ᾱk √ 1-ᾱk ϵ base (X k , k) , ãK = ∇X K r(XK ).<label>(222)</label></formula><p>Note that X k and ãk should be computed without gradients, i.e., X k = stopgrad(X k ), ãk = stopgrad(ã k ).</p><p>For each trajectory, compute the Adjoint Matching objective (37):</p><p>L Adj-Match (θ) = k∈{0,...,K-1} ᾱk+1 ᾱk (1-ᾱk+1 ) 1 -ᾱk ᾱk+1 (ϵ finetune (X k , k) -ϵ base (X k , k))</p><formula xml:id="formula_248">- 1-ᾱk+1 1-ᾱk 1 -ᾱk ᾱk+1 ãk 2 , (<label>223</label></formula><formula xml:id="formula_249">)</formula><p>or L Adj-Match (θ) = k∈{0,...,K-1}</p><formula xml:id="formula_250">ᾱk+1 -ᾱk ᾱk (1-ᾱk ) (ϵ finetune (X k , k) -ϵ base (X k , k)) -ᾱk+1 -ᾱk ᾱk ãk 2 . (<label>224</label></formula><formula xml:id="formula_251">)</formula><p>Compute the gradient ∇ θ L(θ) and update θ using favorite gradient descent algorithm. end Output: Fine-tuned vector field v finetune F Adapting diffusion fine-tuning baselines to flow matching F.1 Adapting ReFL <ref type="bibr" target="#b91">(Xu et al., 2023)</ref> to flow matching Reward Feedback Learning (ReFL) is a diffusion fine-tuning algorithm introduced by <ref type="bibr" target="#b91">Xu et al. (2023)</ref> which tries to increase the reward on denoised samples. Namely, if X = (X t ) t∈[0,1] is the solution of the DDPM SDE (7), we can denoise X t as</p><formula xml:id="formula_252">X1 (X t ) = Xt- √ 1-ᾱtϵ(Xt,t) √ ᾱt . (<label>226</label></formula><formula xml:id="formula_253">)</formula><p>This equation follows from the stochastic interpolant equation (2) if we replace X0 with the noise predictor ϵ(X t , t). And then, the ReFL optimization update is based on the gradient:</p><formula xml:id="formula_254">∇ θ r( X1 (X t )) = ∇ θ r Xt- √ 1-ᾱtϵθ(Xt,t) √ ᾱt ,<label>(227)</label></formula><p>where the trajectories have been detached.</p><p>To adapt ReFL to Flow Matching, we need to express the denoiser map in terms of the vector field v.</p><p>We have that v(x, t) = E βt X0 + αt X1 β t X0 + α t X1 = x = E βt βt β t X0 + α t X1 + αt -βt βt α t X1 β t X0 + α t X1 = x = βt βt x + αt -βt βt α t X1 (x, t). (228) where we defined the denoiser map X1 (x, t) := E X1 |β t X0 + α t X1 = x . Hence, X1 (x, t) = v(x,t)-βt β t x αt-βt β t αt . (229)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )Figure 2</head><label>a2</label><figDesc>Figure 2 Visualization of Theorem 1 showing that fine-tuning must be done with the memoryless noise schedule to ensure convergence to the tilted distribution (1). (a) Shows the base Flow Matching model. (b, c) Fine-tuning using a constant σ(t) leads to biased distributions. (d) Fine-tuning using the memoryless noise schedule leads to the correct tilted distribution. Note that sample generation can use any noise schedule after fine-tuning, including σ(t) = 0.</figDesc><graphic coords="8,70.87,88.29,117.56,134.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 Figure 4</head><label>34</label><figDesc>Figure3Our proposed Adjoint Matching using the memoryless SOC formulation introduces a much more principled way of trading off how close to stay to the base model while optimizing the reward model. In contrast, baseline methods such as DRaFT-1 only optimize the reward model and must rely on early stopping to perform this trade off, resulting in a much more sensitive hyperparameter. Samples are produced using σ(t) = 0 with the same noise sample. Text prompts: "Handsome Smiling man in blue jacket portrait" and "Quinoa and Feta Stuffed Baby Bell Peppers".</figDesc><graphic coords="14,78.95,325.34,55.30,55.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5</head><label>5</label><figDesc>Figure5Tradeoffs between different aspects of generative models: text-to-image consistency (ClipScore), sample diversity for each prompt (DreamSim Diversity), and generalization to unseen human preferences (HPS v2). Different points are obtained from varying values of λ for Adjoint Matching and varying number of fine-tuning iterations for the DRaFT-1 baseline. Overall, we find our proposed method Adjoint Matching has the best Pareto fronts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 Figure 7</head><label>67</label><figDesc>Figure 6 Average values of ImageReward (reward function), control cost ( t 0 1 2 ∥u(X u t , t)∥ 2 dt), and ClipScore vs. wall-clock time for Adjoint Matching and our baselines. Lines show averages over three fine-tuning runs, evaluating on separate test datasets of size 200. Confidence intervals show standard errors of estimates.</figDesc><graphic coords="23,197.84,259.40,216.33,159.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fine</head></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>BaseFigure 8 Figure 9 Figure 10 Figure 11</head><label>891011</label><figDesc>Figure 8 Generated samples with classifier-free guidance (w = 1) and σ(t) = 0 across ten selected prompts. Each row corresponds to a different prompt and each image corresponds to a different random seed consistent across models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>) where Bt = B t + t 0 σ(Y s , s) -1 b 2 (Y s , s) ds. More generally, b 1 and b 2 can be random processes that are adapted to filtration of B. D Proofs of Section 4.3: memoryless noise schedule and fine-tuning recipe D.1 Proof of Proposition 1: the memoryless noise schedule</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>ᾱk+1 ᾱk = 1 + ᾱk+1 -ᾱk ᾱk ≈ 1 + ᾱk+1 -ᾱk 2 ᾱk + O((ᾱ k+1 -ᾱk ) 2 ). (225)Algorithm 2 Adjoint Matching for fine-tuning DDIM Input: Pre-trained denoiser ϵ base , number of fine-tuning iterations N . Initialize fine-tuned denoiser: ϵ finetune = ϵ base with parameters θ.for n ∈ {0, . . . , N -1} do Sample m trajectories X = (Xt) t∈{0,...,1} according to DDPM, e.g.:ᾱk+1 ε k , ε k ∼ N (0, I), X0 ∼ N (0, I),(219)orX k+1 = X k + ᾱk+1 -ᾱk 2 ᾱk X k -ᾱk+1 -ᾱk ᾱk √ 1-ᾱk ϵ finetune (X k , k) + ᾱk+1 -ᾱk ᾱk ε k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Diffusion coefficient σ(t) and the factors κt, ηt for the Flow Matching, Memoryless Flow Matching, DDIM, and DDPM generative processes. When the diffusion coefficient is σ</figDesc><table><row><cell></cell><cell>κ t</cell><cell>η t</cell><cell cols="3">Diffusion coefficient σ(t) Memoryless X t</cell></row><row><cell>Flow Matching (3) Memoryless Flow Matching (4)</cell><cell>αt αt αt αt</cell><cell>β t αt αt β t -βt β t αt αt β t -βt</cell><cell cols="2">General (commonly 0) √ 2η t</cell><cell>No Yes</cell></row><row><cell>DDIM (6)</cell><cell>αt 2 ᾱt</cell><cell>αt 2 ᾱt</cell><cell cols="2">General (commonly 0)</cell><cell>No</cell></row><row><cell>DDPM (7)</cell><cell>αt 2 ᾱt</cell><cell>αt 2 ᾱt</cell><cell>√</cell><cell>2η t</cell><cell>Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Metrics for various fine-tuning methods for text-to-image generation. The second and third columns show the noise schedules σ(t) used for fine-tuning and for inference: σ(t) = √ 2ηt corresponds to Memoryless Flow Matching, and σ(t) = 0 to the Flow Matching ODE (3). Confidence intervals show standard errors of estimates; computed over 3 runs of the fine-tuning algorithm on separate fine-tuning prompt datasets of size 40000 each. Test prompt sets are of size 1000, and also different for each run.</figDesc><table><row><cell>loss</cell><cell cols="2">-tuning</cell><cell></cell><cell cols="5">Fine-tuning Sampling ImageReward ↑ σ(t) σ(t)</cell><cell cols="3">ClipScore diversity ↑ diversity ↑ PickScore Total time (s) / # iterations</cell></row><row><cell cols="3">None (CFG = 1.0)</cell><cell></cell><cell cols="2">N/A</cell><cell>√</cell><cell>2ηt 0</cell><cell>-1.384±0.040 -0.920±0.042</cell><cell>28.07±1.40 30.29±1.53</cell><cell>1.63±0.08 1.82±0.09</cell><cell>N/A</cell></row><row><cell cols="2">DRaFT-1</cell><cell></cell><cell></cell><cell cols="2">√ 2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>1.357±0.039 1.251±0.040</cell><cell>16.86±0.98 16.76±1.06</cell><cell>1.21±0.07 1.27±0.07</cell><cell>140k±5.9k / 4000</cell></row><row><cell cols="3">DRaFT-40</cell><cell></cell><cell cols="2">√ 2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>-0.560±0.138 0.424±0.042</cell><cell>24.07±1.37 20.99±1.54</cell><cell>1.64±0.12 1.67±0.08</cell><cell>148k±4.2k / 1500</cell></row><row><cell cols="2">DPO</cell><cell></cell><cell></cell><cell cols="2">√ 2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>-1.386±0.033 -0.957±0.040</cell><cell>27.80±1.40 29.81±1.43</cell><cell>1.62±0.08 1.68±0.10</cell><cell>118k±0.6k / 1000</cell></row><row><cell cols="2">ReFL</cell><cell></cell><cell></cell><cell cols="2">√ 2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>0.687±0.085 0.709±0.080</cell><cell>19.49±1.76 18.39±1.11</cell><cell>1.22±0.08 1.31±0.10</cell><cell>173k±10.9k / 6000</cell></row><row><cell cols="4">Cont. Adjoint λ = 12500</cell><cell cols="2">√ 2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>-0.448±0.135 -0.249±0.116</cell><cell>26.97±1.37 26.25±1.30</cell><cell>1.82±0.09 1.90±0.10</cell><cell>153k±0.9k / 750</cell></row><row><cell cols="3">Disc. Adjoint λ = 12500</cell><cell></cell><cell cols="2">√ 2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>-0.557±0.113 -0.552±0.041</cell><cell>30.40±2.39 28.37±2.26</cell><cell>1.91±0.09 1.97±0.09</cell><cell>152k±1.5k / 1000</cell></row><row><cell cols="4">Adj.-Matching λ = 1000</cell><cell cols="2">√ 2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>0.550±0.043 0.454±0.055</cell><cell>23.00±1.27 22.76±1.40</cell><cell>1.65±0.08 1.73±0.09</cell></row><row><cell cols="4">Adj.-Matching λ = 2500</cell><cell cols="2">√ 2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>0.755±0.040 0.671±0.047</cell><cell>21.33±1.71 21.42±1.54</cell><cell>1.55±0.08 1.64±0.08</cell><cell>156k±1.9k / 1000</cell></row><row><cell cols="4">Adj.-Matching λ = 12500</cell><cell cols="2">√ 2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>0.882±0.058 0.778±0.050</cell><cell>20.49±1.48 20.34±1.49</cell><cell>1.50±0.09 1.57±0.09</cell></row><row><cell cols="2">Fine-tun. loss</cell><cell cols="4">Fine-tun. Generat. σ(t) σ(t)</cell><cell cols="3">ImageReward ↑ ClipScore ↑</cell><cell>PickScore ↑</cell><cell>HPS v2 ↑</cell><cell>DreamSim Runtime/ diversity ↑ #iter.</cell></row><row><cell>ReFL</cell><cell></cell><cell>√</cell><cell>2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell cols="2">0.459±0.096 0.330±0.114</cell><cell>28.46±0.25 29.63±0.61</cell><cell>18.77±0.09 19.08±0.18</cell><cell>22.54±0.17 22.46±0.77</cell><cell>37.51±3.50 39.51±1.30</cell><cell>43k±2.7k / 1500</cell></row><row><cell cols="2">DRaFT-1</cell><cell>√</cell><cell>2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell cols="2">0.913±0.068 0.626±0.195</cell><cell>29.80±0.22 30.48±0.32</cell><cell>19.16±0.06 18.91±0.34</cell><cell>23.63±0.16 21.92±1.63</cell><cell>35.21±1.93 38.52±2.01</cell><cell>35k±1.5k / 1000</cell></row><row><cell>Draft-40</cell><cell></cell><cell>√</cell><cell>2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell cols="2">-1.427±0.267 -0.097±0.052</cell><cell>23.39±1.72 29.12±0.41</cell><cell>17.24±0.45 18.97±0.14</cell><cell>15.72±1.80 21.93±0.20</cell><cell>41.98±2.14 46.35±1.34</cell><cell>49k±1.4k / 500</cell></row><row><cell cols="2">Adj.-Match. λ = 1000</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell cols="2">0.107±0.046 0.051±0.044</cell><cell>29.37±0.25 30.58±0.17</cell><cell>19.05±0.07 19.31±0.07</cell><cell>22.79±0.20 21.93±0.23</cell><cell>46.38±1.36 48.12±1.56</cell></row><row><cell cols="2">Adj.-Match. λ = 2500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell cols="2">0.199±0.068 0.106±0.067</cell><cell>29.27±0.21 30.43±0.24</cell><cell>19.07±0.10 19.32±0.11</cell><cell>22.98±0.30 22.16±0.33</cell><cell>45.03±1.61 47.61±1.49</cell><cell>39k±0.5k / 250</cell></row><row><cell cols="2">Adj.-Match. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell cols="2">0.299±0.095 0.224±0.051</cell><cell>29.61±0.37 30.70±0.23</cell><cell>19.26±0.14 19.52±0.11</cell><cell>23.67±0.27 22.93±0.21</cell><cell>43.36±1.93 44.62±1.79</cell></row><row><cell cols="2">Cont. Adj. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell cols="2">-0.910±0.116 -0.681±0.051</cell><cell>26.29±0.44 28.50±0.19</cell><cell>18.06±0.16 18.69±0.11</cell><cell>18.86±0.88 19.90±0.50</cell><cell>51.60±1.97 50.87±1.52</cell><cell>51k±0.3k / 250</cell></row><row><cell cols="2">Disc. Adj. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell cols="2">-0.978±0.123 -0.791±0.065</cell><cell>26.68±0.76 28.66±0.33</cell><cell>18.51±0.11 18.51±0.11</cell><cell>18.53±0.28 18.53±0.28</cell><cell>55.95±1.70 54.78±2.00</cell><cell>38k±0.4k / 250</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Evaluation metrics when using classifier-free guidance (CFG;<ref type="bibr" target="#b35">Ho and Salimans (2022)</ref>).</figDesc><table><row><cell>LR / Adam β 1</cell><cell>Fine-tuning loss</cell><cell cols="5">Fine-tun. Generat. ImageReward ↑ σ(t) σ(t)</cell><cell cols="2">ClipScore ↑ PickScore ↑</cell><cell>HPS v2 ↑</cell><cell>DreamSim diversity ↑</cell></row><row><cell>3 × 10 -5</cell><cell>DRaFT-1</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt</cell><cell>1.467±0.029</cell><cell>30.28±0.56</cell><cell>19.37±0.09</cell><cell>24.70±0.15</cell><cell>21.20±0.93</cell></row><row><cell>/ 0.97</cell><cell>Adj.-Match. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt</cell><cell>1.130±0.034</cell><cell>31.01±0.27</cell><cell>19.60±0.08</cell><cell>25.01±0.25</cell><cell>26.73±0.88</cell></row><row><cell>2 × 10 -5</cell><cell>Disc. Adj.</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt</cell><cell>-1.186±0.553</cell><cell>21.95±4.29</cell><cell>16.94±0.95</cell><cell>12.34±4.40</cell><cell>28.33±10.26</cell></row><row><cell>/ 0.95</cell><cell>λ = 12500</cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell><cell>-0.961±0.653</cell><cell>24.07±4.71</cell><cell>17.86±1.17</cell><cell>15.93±5.80</cell><cell>33.62±7.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Metrics for alternative optimization hyperparameters (learning rate and Adam β1).</figDesc><table><row><cell>Fine-tuning loss</cell><cell cols="5">Fine-tuning Generative ImageReward ↑ σ(t) σ(t)</cell><cell cols="2">ClipScore ↑ PickScore ↑</cell><cell>HPS v2 ↑</cell><cell>DreamSim diversity ↑</cell></row><row><cell>Adj.-Matching λ = 12500</cell><cell></cell><cell>1</cell><cell></cell><cell>1 0</cell><cell>0.009±0.077 0.454±0.055</cell><cell>29.18±0.51 31.41±0.22</cell><cell>18.66±0.09 19.57±0.09</cell><cell>20.75±0.32 23.29±0.18</cell><cell>41.33±1.24 43.10±1.76</cell></row><row><cell>Adj.-Matching λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>0.882±0.058 0.778±0.050</cell><cell>30.62±0.30 31.65±0.19</cell><cell>19.50±0.09 19.76±0.08</cell><cell>24.95±0.28 24.49±0.27</cell><cell>34.50±1.33 37.24±1.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Comparison with an alternative fine-tuning noise schedule σ(t) = 1. We see that the initial value function bias (Section 4.2) results in the model not having a high reward function (ImageReward is the reward function used for fine-tuning). Its performance on other metrics are also lower than when fine-tuning with the memoryless noise schedule, except for diversity.</figDesc><table><row><cell cols="2">#sampl. timesteps loss Fine-tuning</cell><cell cols="4">Fine-tun. Sampl. σ(t) σ(t)</cell><cell>ImageReward ↑</cell><cell cols="2">ClipScore ↑ PickScore ↑</cell><cell>HPS v2 ↑</cell><cell>DreamSim diversity ↑</cell></row><row><cell></cell><cell>None (Base)</cell><cell cols="2">N/A</cell><cell>√</cell><cell>2ηt 0</cell><cell>-2.279±0.001 -1.386±0.040</cell><cell>13.99±0.12 26.26±0.24</cell><cell>14.98±0.05 17.64±0.07</cell><cell>7.37±0.10 14.92±0.17</cell><cell>5.07±0.13 51.26±1.38</cell></row><row><cell>10</cell><cell>DRaFT-1</cell><cell>√</cell><cell>2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>1.033±0.051 1.236±0.038</cell><cell>25.98±0.25 31.54±0.27</cell><cell>18.28±0.07 19.53±0.07</cell><cell>22.08±0.18 24.47±0.19</cell><cell>14.47±0.67 24.78±0.88</cell></row><row><cell></cell><cell>Adj.-Match. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>-2.104±0.074 0.607±0.055</cell><cell>17.12±0.56 31.36±0.20</cell><cell>15.76±0.20 19.56±0.08</cell><cell>11.48±1.03 23.23±0.28</cell><cell>9.88±0.81 33.75±1.48</cell></row><row><cell></cell><cell>None (Base)</cell><cell cols="2">N/A</cell><cell>√</cell><cell>2ηt 0</cell><cell>-2.275±0.002 -1.017±0.055</cell><cell>14.58±0.13 27.92±0.19</cell><cell>15.07±0.05 18.01±0.07</cell><cell>7.47±0.10 17.17±0.15</cell><cell>11.27±0.33 54.69±1.45</cell></row><row><cell>20</cell><cell>DRaFT-1</cell><cell>√</cell><cell>2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>1.301±0.039 1.255±0.038</cell><cell>27.09±0.24 31.14±0.25</cell><cell>18.93±0.07 19.43±0.06</cell><cell>23.78±0.20 24.52±0.16</cell><cell>21.05±1.12 26.15±1.11</cell></row><row><cell></cell><cell>Adj.-Match. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>-0.032±0.072 0.768±0.048</cell><cell>25.07±0.27 31.70±0.17</cell><cell>18.01±0.07 19.73±0.08</cell><cell>20.75±0.23 24.30±0.26</cell><cell>29.06±2.34 35.90±1.52</cell></row><row><cell></cell><cell>None (Base)</cell><cell cols="2">N/A</cell><cell>√</cell><cell>2ηt 0</cell><cell>-1.384±0.040 -0.920±0.042</cell><cell>24.15±0.26 28.32±0.22</cell><cell>17.25±0.06 18.15±0.07</cell><cell>16.19±0.17 17.89±0.16</cell><cell>53.60±1.37 56.53±1.52</cell></row><row><cell>40</cell><cell>DRaFT-1</cell><cell>√</cell><cell>2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>1.357±0.039 1.251±0.040</cell><cell>30.18±0.24 30.95±0.28</cell><cell>19.38±0.08 19.37±0.06</cell><cell>24.61±0.17 24.37±0.17</cell><cell>25.54±0.99 27.39±1.14</cell></row><row><cell></cell><cell>Adj.-Match. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>0.882±0.058 0.778±0.050</cell><cell>30.62±0.30 31.65±0.19</cell><cell>19.50±0.09 19.76±0.08</cell><cell>24.95±0.28 24.49±0.27</cell><cell>34.50±1.33 37.24±1.57</cell></row><row><cell></cell><cell>None (Base)</cell><cell cols="2">N/A</cell><cell>√</cell><cell>2ηt 0</cell><cell>-0.881±0.041 -0.881±0.036</cell><cell>27.83±0.19 28.65±0.18</cell><cell>18.10±0.07 18.22±0.06</cell><cell>18.43±0.17 18.20±0.17</cell><cell>57.21±1.50 57.73±1.68</cell></row><row><cell>100</cell><cell>DRaFT-1</cell><cell>√</cell><cell>2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>1.343±0.040 1.239±0.037</cell><cell>30.64±0.20 30.74±0.28</cell><cell>19.38±0.08 19.33±0.06</cell><cell>24.37±0.17 24.24±0.17</cell><cell>25.51±1.10 28.70±1.11</cell></row><row><cell></cell><cell>Adj.-Match. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>0.892±0.044 0.779±0.048</cell><cell>31.23±0.23 31.64±0.17</cell><cell>19.65±0.08 19.76±0.08</cell><cell>24.92±0.23 24.57±0.25</cell><cell>35.13±1.40 38.26±1.65</cell></row><row><cell></cell><cell>None (Base)</cell><cell cols="2">N/A</cell><cell>√</cell><cell>2ηt 0</cell><cell>-0.848±0.048 -0.871±0.036</cell><cell>28.37±0.21 28.50±0.18</cell><cell>18.27±0.08 18.23±0.06</cell><cell>18.56±0.19 18.25±0.14</cell><cell>58.00±1.58 57.84±1.60</cell></row><row><cell>200</cell><cell>DRaFT-1</cell><cell>√</cell><cell>2ηt 0</cell><cell>√</cell><cell>2ηt 0</cell><cell>1.331±0.044 1.222±0.042</cell><cell>30.69±0.23 30.77±0.27</cell><cell>19.36±0.07 19.32±0.06</cell><cell>24.21±0.17 24.18±0.16</cell><cell>26.41±1.18 29.09±1.07</cell></row><row><cell></cell><cell>Adj.-Match. λ = 12500</cell><cell>√</cell><cell>2ηt</cell><cell>√</cell><cell>2ηt 0</cell><cell>0.869±0.062 0.766±0.050</cell><cell>31.33±0.21 31.61±0.16</cell><cell>19.68±0.09 19.75±0.08</cell><cell>24.81±0.30 24.52±0.24</cell><cell>35.90±1.55 38.60±1.38</cell></row></table><note><p>Table8</p><p>Performance metrics for different number of sampling steps. Only the number of sampling steps is ablated; the fine-tuned models used in all cases are the ones fine-tuned using 40 steps.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Note that there is a slight difference in terminology between SOC and reinforcement learning, where our cost functional is referred to as the state value function and our value function is the optimal state value function in RL.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Note (20) is informal because densities over continuous-time processes are ill-defined; the formal statement is dP * dP base (X|X 0 ) = exp(-1 0 f (Xt, t) dt -g(X 1 )), where dP * dP base denotes the Radon-Nikodym derivative. We treat this formally in the proofs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>Note we use the convention that a Jacobian matrix J = ∇xv(x) is defined asJ ij = ∂v i (x) ∂x j.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>See also<ref type="bibr" target="#b20">Domingo-Enrich (2024)</ref> for a comparison among SOC losses.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>The entropy terms are usually multiplied by a factor to tune their magnitude, but one can equivalently rescale the rewards, which is why we do not add any factor.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider the SDEs</head><p>If we let P| x0 , P u | x0 be the probability measures of the solutions of ( <ref type="formula">135</ref>) and ( <ref type="formula">136</ref>), Theorem 2 implies that</p><p>Hence,</p><p>where we used that stochastic integrals are martingales.</p><p>E.2 Proof of Proposition 2: Theoretical guarantees of the basic Adjoint Matching loss</p><p>Let ū = stopgrad(u θ ). We can rewrite equation (32) as:</p><p>This proves the first statement of the proposition. To prove that the only critical point of the expected basic Adjoint Matching loss is the optimal control, we first compute the first variation of E[L Basic-Adj-Match ].</p><p>Letting v : R d × [0, T ] → R d be arbitrary, we have that</p><p>Hence, critical points satisfy that</p><p>In this equation, the second equality holds by equation ( <ref type="formula">184</ref>) from Lemma 5, and the third equality holds by the Leibniz rule.</p><p>Lemma 6 shows that any control u that satisfies (196) is equal to the optimal control, which concludes the proof.</p><p>Lemma 6. Suppose that for any x ∈ R d , t ∈ [0, T ], u(x, t) = -σ(t) ⊤ ∇ x J(u; x, t). Then, J(u; •, •) satisfies the Hamilton-Jacobi-Bellman equation (156). By the uniqueness of the solution to the HJB equation, we have that</p><p>which means that</p><p>Recall that the generator T u of the controlled SDE (13) takes the form:</p><p>Hence, if we take the limit ∆t → 0 on equation (198), we obtain that:</p><p>Now using that u(x, t) = -σ(t) ⊤ ∇ x J(u; x, t), we have that</p><p>F.2 Adapting Diffusion-DPO <ref type="bibr">(Wallace et al., 2023a)</ref> to flow matching</p><p>The Diffusion-DPO loss assumes access to ranked pairs of generated samples x w 1 ≻ x l 1 , where x w and x l are the winning and losing samples. For DDPM, the loss implemented in practice reads <ref type="bibr">(Wallace et al., 2023a, Eq. 46)</ref>:</p><p>where S(x) = 1 1+e -x denotes the sigmoid function, and q(x * kh |x * 1 ) is the conditional distribution of the forward process, i.e. x * kh is sampled as</p><p>Following the derivation of the Diffusion-DPO loss in <ref type="bibr">(Wallace et al., 2023a</ref>, Sec. S4), we observe that the termβ 2 ∥ε w -ϵ θ (x w kh , kh)∥ 2 arises from</p><p>up to a constant term in θ. If we switch to the more general flow matching scheme, the analog of this term is</p><p>Using the expression of the denoiser map in terms of the vector field v in equation ( <ref type="formula">229</ref>), we can rewrite (232) as:</p><p>-</p><p>Thus, the Diffusion-DPO loss for Flow Matching reads When we have access to the reward function r, instead of a winning sample x w 1 and a losing sample x l 1 , we have a pair of samples (x a 1 , x b 1 ) with winning weights S(r(</p><p>. Hence, the loss (234) becomes:</p><p>We want to emphasize that despite the similarities, even though the loss L DPO that we use (equation ( <ref type="formula">235</ref>)) is very similar to the one implemented by <ref type="bibr">Wallace et al. (2023a)</ref>, the preference data pairs that we use are very different from theirs. We sample the preference data from the current model, which results in imperfect samples, while they consider off-policy, high-quality, curated preference samples. The reason for this discrepancy is that the starting point of our work is a reward model, not a set of preference data, and we only benchmark against approaches that leverage reward models for an apples-to-apples comparison. Our experimental results on DPO (Table <ref type="table">2</ref>, Figure <ref type="figure">6</ref>, Table <ref type="table">3</ref>) show that the resulting model performs like the base model, or a bit worse according to some metrics. Hence, we conclude that DPO is not a competitive alternative for on-policy fine-tune when the base model is not already good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Experimental details</head><p>Unless otherwise specified, we used the same hyperparameters across all fine-tuning methods. Namely, we used:</p><p>• K = 40 timesteps.</p><p>• Adam optimizer with learning rate 2 × 10 -5 and parameters β 1 = 0.95, β 2 = 0.999, ϵ = 1 × 10 -8 , weight decay 1 × 10 -2 , gradient norm clipping value 1. For Discrete Adjoint, these hyperparameters resulted in fine-tuning instability (see Table <ref type="table">6</ref>); the results that we report in all other tables for Discrete Adjoint were obtained with learning rate 1 × 10 -5 .</p><p>• Bfloat16 precision.</p><p>• Effective batch size 40; for each run we used two 80GB A100 GPUs with batch size 20 each.</p><p>• A set of 40k fine-tuning prompts taken from a licensed dataset consisting of text and image pairs (note that we disregarded the images). Thus, each epoch lasts 1000 iterations; see the total amount of fine-tuning iterations for each algorithm in Table <ref type="table">3</ref>. For each of the three runs that we perform for each data point that we report, the set of 40k prompts is sampled independently among a total set of 100k prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Noise schedule details</head><p>Since we use K = 40 discretization steps, the timesteps are t ∈ {0, 0.025, 0.05, 0.075, 0.1, . . . , 0.95, 0.975}.</p><p>To sample X t+h from X t we use equation ( <ref type="formula">40</ref>). We use the choices α t = t, β t = 1 -t, which means that</p><p>Note that if we plug t = 0 into this expression, we obtain infinity, and if we plug t ⪅ 1, we obtain σ(t) ≈ 0.</p><p>For obvious reasons, the former issue requires a fix: we simply add a small offset to the denominator of σ(t), replacing 1/t by 1/(t + h) (note that h := 1/K = 0.025). But the latter issue is also not completely satisfactory from a practical standpoint, because looking at the adjoint matching loss (37), we observe that u(X ū t , t) is trained to approximate the conditional expectation of σ(t) T ã(t; X ū). Thus, if we set σ(t) very close to zero for t ⪅ 1, we are forcing the control u to be close to zero as well, or equivalently preventing v finetune from deviating from v base . While this is the right thing to do from a theoretical perspective, we concluded experimentally that setting σ(t) just slightly larger results in substantially faster fine-tuning, thanks to the additional leeway provided to v finetune to deviate from v base . In particular, we added a small offset to the factor 1 -t in the numerator 1 -t of σ(t): we replaced 1 -t by 1 -t + h. Thus, the expression that we used to compute the diffusion coefficient in our experiments is</p><p>When solving the lean adjoint ODE ( <ref type="formula">38</ref>)-( <ref type="formula">39</ref>) backwards in time via the Euler scheme (41), the timesteps we use are t ∈ {1, 0.975, 0.95, 0.925, 0.9, . . . , 0.05, 0.025}. We do not actually initialize the adjoint state as ∇ x g(X 1 ), but rather as ∇ x g( X1 ), where X1 := X 1-h + hv base (X 1-h , 1 -h). That is, X1 is obtained by performing a final noiseless update, instead of using noise σ(1 -h) = √ 4h given by equation ( <ref type="formula">236</ref>). The reason for this is that the regular final iterate X 1 contains some noise that was added in the final step, and that can distort the gradient ∇ x g(X 1 ). By setting ã(1; X) = ∇ x g(X 1 ), we get rid of this bias. Note that in the continuous time limit h → 0, X1 = X 1 , which means that this small trick is consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Selection of gradient evaluation timesteps</head><p>In Algorithm 1, equation ( <ref type="formula">42</ref>), we state that the term</p><p>2 must be computed for all K steps in {0, . . . , 1 -h}. However, the gradient signal provided by backpropagating through this expression for consecutive times t and t + h is quite similar. In the interest of computational efficiency, we sample a subset K of timesteps, and we only compute and backpropagate the terms</p><p>for those timesteps. We construct K by sampling ten timesteps uniformly without repetition among {0, . . . , 0.725}, and always sampling the last ten timesteps {0.75, . . . , 0.975}. This is because fine-tuning the last ten steps (25% of the total) well is critical for good empirical performance, while the initial steps are not as important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Loss function clipping: the LCT hyperparameter</head><p>Note that the magnitude of σ(t) T a(t; X ū, ū) is much larger for times t ⪆ 0 than for times t ⪅ 1. The reason is two-fold:</p><p>• As discussed in Appendix G.1, σ(t) is much larger for t ⪆ 0 than for t ⪅ 1.</p><p>• The magnitude of the lean adjoint state ã grows roughly exponentially as t goes backward in time. In fact, if we assumed that ∇ x b(X t , t) is constant in time, this statement would be exact.</p><p>Observe that when σ(t) T a(t; X ū, ū) is large, the gradient</p><p>also has a high magnitude. Including such terms in our gradient computation decreases the signal to noise ratio of the gradient. Even more so, as discussed in Appendix G.2 for good practical performance it is critical to get a good gradient signal from the last 25% steps. Hence, including the high-magnitude terms for t ⪅ 0 in our gradients can muffle these other important, low-magnitude terms.</p><p>To fix this issue, we clip the terms such that</p><p>LCT stands for the loss clipping threshold. That is, the adjoint matching loss that we use in our experiments is of the form:</p><p>where K is the random timestep subset described in Appendix G.2.</p><p>For adjoint matching, we set LCT = 1.6 × λ 2 . Remark that LCT needs to grow quadratically with λ, because the magnitude of the lean adjoint ã grows quadratically with λ. We set the constant 1.6 through experimentation; all or almost all of the terms for the last ten timesteps fall below LCT, but only a fraction of the terms (≈ 25%) for the first ten steps fall below LCT. The constant for LCT is a relevant hyperparameter that needs to be tuned to obtain a similar behavior.</p><p>We also used loss function clipping on the continuous adjoint loss. For that loss we set LCT = 1600 × λ 2 . The reason is that the magnitude of the regular adjoint states is significantly larger than the magnitude of the lean adjoint states (which is a big reason why adjoint matching outperforms the continuous adjoint).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Computation of evaluation metrics</head><p>We used the open_clip library <ref type="bibr" target="#b38">(Ilharco et al., 2021)</ref> to compute ClipScores. We computed ClipScore diversity as the variance of Clip embeddings of 40 generations for a given prompt, averaged across 25 prompts. Namely,</p><p>where g k i denotes the i-th generation for the k-th prompt. We used the transformers library to compute the PickScore processor and model <ref type="bibr" target="#b42">(Kirstain et al., 2023)</ref>. PickScore diversity is computed in analogy with ClipScore diversity. We used the hps library to compute values of Human Preference Score v2 <ref type="bibr">(Wu et al., 2023b)</ref>.</p><p>To compute Dreamsim diversity we use the dreamsim library <ref type="bibr" target="#b28">(Fu et al., 2023)</ref>. Dreamsim diversity is computed in analogy with ClipScore diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5 Remarks on computational costs</head><p>Observe from the figures reported in Table <ref type="table">3</ref> that the per iteration wall-clock time of Adjoint Matching (156 seconds) is very similar to that of the Discrete Adjoint loss (152 seconds). The reason is that both algorithms perform a similar amount of forward and backward passes on the flow matching model and the reward model. Namely, for each sample in the batch, both algorithms perform K forward passes on the flow model to obtain the trajectories. In order to compute the gradient of the training loss, the Discrete Adjoint loss does K additional forward passes to evaluate the base flow model, one forward and backward pass on the reward model, and K backward passes on the current flow model, which typically use gradient checkpointing to avoid memory overflow. In the case of Adjoint Matching, solving the lean adjoint ODE requires one forward and backward pass on the reward model, and K backward passes on the base flow model. Finally, computing the gradient of the loss takes K/2 additional backward passes if we evaluate at only half of the timesteps as we do, although this computation is much quicker because it can be fully parallelized.</p><p>Meanwhile, computing the gradient of the Continuous Adjoint loss takes 204 seconds per iteration. With respect to Adjoint Matching, Continuous Adjoint performs additional backward passes to compute the gradients ∇ Xt ∥u(X t , t)∥ 2 when solving the adjoint ODE. Finally, we observe that models that directly fine-tune the reward are quicker, but that comes with its own set of issues that we discuss throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.6 Remarks on number of sampling timesteps</head><p>In our experiments and all baselines, we used 40 timesteps in the fine-tuning procedure (h = 1/40 in Algorithm 1). The experiments reported in all tables and figures except for Table <ref type="table">8</ref> were performed at 40 inference timesteps. In Table 8 (Appendix A), we show experimental results at 10, 20, 40, 100, and 200 inference timesteps, for the base model and the models fine-tuned with adjoint matching and DRaFT-1. We make the following observations about the results:</p><p>• The metrics for Adjoint Matching at 100 and 200 timesteps are statistically equal to the ones for 40 timesteps, with slight increases in Dreamsim diversity. This suggests that fine-tuning at large numbers of timesteps is a good idea if we want to perform inference at a large number of timesteps, as otherwise the capabilities of the model are limited by the number of fine-tuning timesteps instead of the inference compute. Also, at 100 and 200 timesteps the difference in performance of Adjoint Matching relative to DRaFT-1 increases.</p><p>• The metrics for Adjoint Matching at 10 and 20 timesteps are worse than at 40 timesteps, especially for 10. The difference in performance between Adjoint Matching and DRaFT-1 vanishes at 10 timesteps for all metrics except for diversity, for which Adjoint Matching is still clearly better.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">M</forename><surname>Michael S Albergo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Boffi</surname></persName>
		</author>
		<author>
			<persName><surname>Vanden-Eijnden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08797</idno>
		<title level="m">Stochastic interpolants: A unifying framework for flows and diffusions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building normalizing flows with stochastic interpolants</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albergo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 3, and 36</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tutorial on amortized optimization</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Consistency-diversity-realism pareto fronts of conditional image generative models</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Astolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Careil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Mañas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Muckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><forename type="middle">Romero</forename><surname>Soriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10429</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><surname>El-Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<title level="m">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Bartosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">A</forename><surname>Naesseth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08337</idno>
		<title level="m">Neural diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural flow diffusion models: Learnable forward process for improved diffusion modelling</title>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Bartosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">A</forename><surname>Naesseth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.12940</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Princeton Landmarks in Mathematics</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">2010. 1957</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>Dynamic programming Cited on page 5</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">D-flow: Differentiating through flows for controlled generation</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itai</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14017</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An optimal control perspective on diffusion-based generative modeling</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenz</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01364</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explicit solution of relative entropy weighted control</title>
		<author>
			<persName><forename type="first">Joris</forename><surname>Bierkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hilbert</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Control Letters</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training diffusion models with reinforcement learning</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 12, and 37</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Posterior sampling with denoising oracles via tilted transport</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiequn</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.00745</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning neural event functions for ordinary differential equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongsol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">L</forename><surname>Michael T Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Chul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14687</idno>
		<title level="m">Diffusion posterior sampling for general noisy inverse problems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Directly fine-tuning diffusion models on differentiable rewards</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 12, and 14</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diffusion schrödinger bridge with applications to score-based generative modeling</title>
		<author>
			<persName><forename type="first">James</forename><surname>Valentin De Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deft: Efficient finetuning of conditional diffusion models by learning the generalised h-transform</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreyas</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Didi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dutordoir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Barbano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emile</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urszula</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Komorowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01781</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Carles</forename><surname>Domingo-Enrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.00345</idno>
		<title level="m">A taxonomy of loss functions for stochastic optimal control</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Carles</forename><surname>Domingo-Enrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiequn</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02027</idno>
		<title level="m">Stochastic optimal control matching</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on pages 5, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Doob&apos;s lagrangian: A sample-efficient variational approach to transition path sampling</title>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Plainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Brekelmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Apsuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirill</forename><surname>Neklyudov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07974</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scaling rectified flow transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahim</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Boesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing ddpm sampling with shortcut fine-tuning</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moonkyung</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16381</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reinforcement learning for fine-tuning text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moonkyung</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2" to="6" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deterministic and Stochastic Optimal Control. Stochastic Modelling and Applied Probability</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Rishel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Netanel</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobhita</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><surname>Dreamsim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09344</idno>
		<title level="m">Learning new dimensions of human visual similarity using synthetic data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14" to="54" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Policy search for path integral control</title>
		<author>
			<persName><forename type="first">Vicenç</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hilbert</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="482" to="497" />
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning approximation for stochastic control problems</title>
		<author>
			<persName><forename type="first">Jiequn</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07422</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient rare event simulation by optimal nonequilibrium forcing</title>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Schütte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2012">11004. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Clipscore: A reference-free evaluation metric for image captioning</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08718</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on pages 2, 15, and 25</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 3, 4, and 7</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adishree</forename><surname>Ghatare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sageev</forename><surname>Gururani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Oore</surname></persName>
		</author>
		<author>
			<persName><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14285</idno>
		<title level="m">Symbolic music generation with non-differentiable rule guided diffusion</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><surname>Openclip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
	</monogr>
	<note>Cited on page 54</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Path integrals and symmetry breaking for optimal control theory</title>
		<author>
			<persName><forename type="first">H J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5" to="9" />
			<date type="published" when="2005-11">2005. nov 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimal control as a graphical model inference problem</title>
		<author>
			<persName><forename type="first">Vicenç</forename><surname>Hilbert J Kappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On density estimation with diffusion models</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pick-a-pic: An open dataset of user preferences for text-to-image generation</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Kirstain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahbuland</forename><surname>Matiana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 14, and 54</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leda</forename><surname>Sari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashel</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Mahadeokar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Differentiable gaussianization layers for inverse problems regularized by deep generative models</title>
		<author>
			<persName><forename type="first">Dongzhuo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03860</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable gradients for stochastic differential equations</title>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Kam Leonard</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Flow matching for generative modeling</title>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 3, 12, 13, and 36</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generalized schrödinger bridge matching</title>
		<author>
			<persName><forename type="first">Guan-Horng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14577</idno>
		<title level="m">Rectified flow: A marginal preserving approach to optimal transport</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
	<note>Chengyue Gong, and qiang liu</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13259</idno>
		<title level="m">Trajectory balance: Improved credit assignment in gflownets</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Interacting particle solutions of fokker-planck equations through gradient-log-density estimation</title>
		<author>
			<persName><forename type="first">Dimitra</forename><surname>Maoutsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">802</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monte carlo gradient estimation in machine learning</title>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">132</biblScope>
			<biblScope unit="page" from="1" to="62" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tyka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google research blog</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Solving high-dimensional Hamilton-Jacobi-Bellman pdes using neural networks: perspectives from the theory of controlled diffusions and measures on path space</title>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Nüsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenz</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Partial differential equations and applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
		</imprint>
	</monogr>
	<note>Cited on pages 2, 6, and 12</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Ashwini</forename><surname>Pokle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Muckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04432</idno>
		<title level="m">Training-free linear image inversion via flows</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The Mathematical Theory of Optimal Processes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Pontryagin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962">1962</date>
			<publisher>Interscience Publishers</publisher>
			<biblScope unit="page" from="8" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Aram-Alexandre</forename><surname>Pooladian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carles</forename><surname>Domingo-Enrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.00288</idno>
		<title level="m">Neural optimal transport with lagrangian costs</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On stochastic optimal control and reinforcement learning by approximate inference</title>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Rawlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sethu</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Third International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improved sampling via learned diffusions</title>
		<author>
			<persName><forename type="first">Lorenz</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Berner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">VarGrad: A low-variance gradient estimator for variational inference</title>
		<author>
			<persName><forename type="first">Lorenz</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayman</forename><surname>Boustati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Nüsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><forename type="middle">Deniz</forename><surname>Akyildiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control</title>
		<author>
			<persName><forename type="first">Litu</forename><surname>Rout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17401</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Reuven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><surname>Kroese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Laion-aesthetics</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Optimal Control Theory: Applications to Management Science and Economics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Sethi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
	<note>Cited on page 5</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pseudoinverse-guided diffusion models for inverse problems</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05600</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Scorebased generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR 2021)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Do differentiable simulators give better policy gradients?</title>
		<author>
			<persName><forename type="first">Ju</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqing</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20668" to="20696" />
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Wenpin</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.06279</idno>
		<title level="m">Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 6</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>Linearly-solvable markov decision problems Cited on page 5</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Theoretical guarantees for sampling and inference in generative models with latent diffusions</title>
		<author>
			<persName><forename type="first">Belinda</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Raginsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01608</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Understanding reinforcement learning-based fine-tuning of diffusion models: A tutorial and review</title>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Biancalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.13734</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Scalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">Lee</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Biancalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15194</idno>
		<title level="m">Fine-tuning of continuous-time diffusion models as entropyregularized control</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on pages 2, 6, 12, and 37</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Bayesian learning via neural schrödinger-föllmer flows</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrius</forename><surname>Ovsianas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Lopes</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Nüsken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Symposium on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Denoising diffusion samplers</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Sussman Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Audiobox: Unified audio generation with natural language prompts</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Chiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiemin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Ngan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.15821</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Caiming Xiong, Shafiq Joty, and Nikhil Naik</title>
		<author>
			<persName><forename type="first">Bram</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meihua</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12908</idno>
	</analytic>
	<monogr>
		<title level="m">Diffusion model alignment using direct preference optimization</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on pages 2, 14, 22, and 52</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">End-to-end diffusion latent optimization improves classifier guidance</title>
		<author>
			<persName><forename type="first">Bram</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13703</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Practical and asymptotically exact conditional sampling in diffusion models</title>
		<author>
			<persName><forename type="first">Luhuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Naesseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Xiaoshi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09341</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14" to="54" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Xiaoshi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09341</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Imagereward: Learning and evaluating human preferences for text-to-image generation</title>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 12, 13, 14, 22, and 51</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Improving gflownets for text-to-image diffusion alignment</title>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.00633</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Path integral sampler: A stochastic control approach for sampling</title>
		<author>
			<persName><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Applications of the cross-entropy method to importance sampling and optimal control of diffusions</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Schütte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2654" to="A2672" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">Qinqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neta</forename><surname>Shaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13443</idno>
		<title level="m">Guided flows for generative modeling and decision making</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2" to="15" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andrew Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aaai</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="5" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<title level="m">Fine-tuning language models from human preferences</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
