- **Adjoint Matching Algorithm**: A new method for reward fine-tuning of dynamical generative models, formulated as a regression problem, improving consistency, realism, and generalization.
  
- **Stochastic Optimal Control (SOC) Framework**: Casts reward fine-tuning as SOC, addressing the value function bias problem that affects existing methods.

- **Memoryless Noise Schedule**: A critical component that removes the dependency between noise variables and generated samples, ensuring convergence to the tilted distribution \( p^*(x) \propto p_{\text{base}}(x) \exp(r(x)) \).

- **Value Function Bias Problem**: Existing naive approaches lead to bias away from the desired tilted distribution; this paper provides a solution through the memoryless noise schedule.

- **Generative Models**: Focus on Flow Matching and Denoising Diffusion Models, which generate samples through iterative stochastic processes.

- **Flow Matching Process**: Described by the ODE \( dX_t = v(X_t, t) dt \) with \( X_0 \sim N(0, I) \), where \( v(X_t, t) \) is optimized to match the reference flow.

- **Denoising Diffusion Model (DDIM)**: Sampling rule given by \( X_{k+1} = \sqrt{\bar{\alpha}_{k+1}} X_k - \sqrt{1 - \bar{\alpha}_k} \epsilon(X_k, k) \) with \( \bar{\alpha}_k \) as an increasing sequence.

- **Unified Framework**: Both Flow Matching and diffusion models can be expressed in a common notation as \( dX_t = b(X_t, t) dt + \sigma(t) dB_t \), where \( b(x, t) \) is defined in terms of the score function.

- **Human Preference Reward Models**: Fine-tuning aims to align generative models with human preferences, utilizing reward models \( r(x) \) to adjust the sample distribution.

- **Evaluation Metrics**: Comparison of methods based on realism, consistency, and diversity, demonstrating the advantages of the proposed Adjoint Matching approach.

- **Notation Summary**: Common notation used throughout the paper for clarity in discussing algorithms and processes.

- **Key References**: Include foundational works on Flow Matching and diffusion models, as well as recent advancements in reward fine-tuning techniques.