{
  "arxivId": "2409.08861",
  "title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with\n  Memoryless Stochastic Optimal Control",
  "authors": "Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, Ricky T. Q. Chen",
  "abstract": "Dynamical generative models that produce samples through an iterative\nprocess, such as Flow Matching and denoising diffusion models, have seen\nwidespread use, but there have not been many theoretically-sound methods for\nimproving these models with reward fine-tuning. In this work, we cast reward\nfine-tuning as stochastic optimal control (SOC). Critically, we prove that a\nvery specific memoryless noise schedule must be enforced during fine-tuning, in\norder to account for the dependency between the noise variable and the\ngenerated samples. We also propose a new algorithm named Adjoint Matching which\noutperforms existing SOC algorithms, by casting SOC problems as a regression\nproblem. We find that our approach significantly improves over existing methods\nfor reward fine-tuning, achieving better consistency, realism, and\ngeneralization to unseen human preference reward models, while retaining sample\ndiversity.",
  "url": "https://arxiv.org/abs/2409.08861",
  "issue_number": 958,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/958",
  "created_at": "2025-01-13T06:45:25.929648",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 21,
  "last_read": "2025-01-13T06:52:55.474406",
  "last_visited": "2025-01-13T06:50:13.853000+00:00",
  "main_tex_file": null,
  "published_date": "2024-09-13T14:22:14Z",
  "arxiv_tags": [
    "cs.LG",
    "math.OC",
    "stat.ML"
  ]
}