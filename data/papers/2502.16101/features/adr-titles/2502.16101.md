- Decision to focus on evaluating RAG systems against misleading retrievals
- Choice of dataset source (Reddit discussions) for real-world misinformation
- Definition and categorization of evidence types (supporting, misleading, irrelevant)
- Methodology for annotating retrieved documents using LLM-guided approaches
- Structure of the dataset (claims, verdicts, associated documents)
- Evaluation metrics for assessing RAG system performance
- Selection of LLMs and RAG systems for benchmark experiments
- Configuration of task setups (Zero-Context Prediction, Standard RAG, Oracle Retrieval)
- Approach to handling conflicting evidence in the dataset
- Decision to prioritize naturally occurring misinformation over synthetic noise
- Rationale for including misleading documents in the dataset
- Strategy for qualitative analysis of RAG system performance
- Framework for future research directions based on benchmark results
- Consideration of real-world implications of RAG system vulnerabilities
- Decision to advocate for a shift in RAG evaluation focus toward noisy information scenarios