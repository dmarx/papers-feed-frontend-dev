<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-22">22 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linda</forename><surname>Zeng</surname></persName>
							<email>26lindaz@students.harker.org</email>
						</author>
						<author>
							<persName><forename type="first">Rithwik</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Diji</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><forename type="middle">2025</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Divij</forename><surname>Motwani</surname></persName>
							<email>divijmotwani@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Harker School San Jose</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Irvington High School Fremont</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Palo Alto High School Palo Alto</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of California Santa Cruz Santa Cruz</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of California Santa Cruz Santa Cruz</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-22">22 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">7360D509F432623130049E267FF3C296</idno>
					<idno type="arXiv">arXiv:2502.16101v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-26T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>• Information systems → Question answering</term>
					<term>Language models</term>
					<term>• Computing methodologies → Natural language processing Retrieval-Augmented Generation (RAG) benchmark, Fact-checking dataset, Noisy retrieval corpus, Misleading retrievals</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation.</p><p>In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance.</p><p>To bridge this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: supporting, misleading, and irrelevant, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information.</p><p>Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments. To the best of our knowledge, RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence. We expect this benchmark will drive future research toward improving RAG * Equal contribution. † Co-advising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Retrieval-augmented generation (RAG) systems have shown significant promise in mitigating LLM hallucination and enhancing trustworthiness. By combining the generative capabilities of large language models (LLMs) with the retrieval power of external corpora, RAG aims to ground responses in relevant, contextually appropriate information, thereby improving factual consistency and output credibility <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. However, while existing RAG approaches primarily focus on optimizing retrieval relevance and maximizing the amount of information in retrieved-context <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43]</ref>, a critical challenge remains largely unaddressed: how to handle cases where retrieved content is misleading or irrelevant. This issue is particularly concerning when misinformation, adversarial perturbations, or biased sources influence the retrieval process, potentially degrading the reliability of LLM outputs. Addressing this robustness gap is essential for ensuring the trustworthiness of RAG systems, especially in high-stakes applications such as fact-checking <ref type="bibr" target="#b32">[33]</ref> and legal or medical domains <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Prior work has mitigated noisy retrievals by prompting models to justify relevance, aggregating sources, or using debate-based selection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. However, most approaches align retrieved content with LLMs' prior knowledge rather than addressing real-world contradictions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>. Furthermore, current datasets overly rely on curating reliable documents, limiting robustness testing against misinformation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>. While some introduce counterfactuals or retrieval noise <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>, they rely on artificial perturbations or costly human annotation. This highlights the need for an evaluation framework that challenges RAG systems with real-world contradictions, exposing their limitations and improving resilience in complex retrieval scenarios.</p><p>Fact-checking plays a crucial role in combating misinformation, yet most existing datasets assume the availability of gold-standard evidence that aligns with the verdict <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>. In reality, retrieved information often presents conflicting perspectives, making automated verification more challenging. The political domain is particularly rich in such complexities, as controversial claims generate both supporting and opposing narratives from diverse sources <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. To develop fact-checking systems capable of handling real-world misinformation, it is essential to move beyond idealized settings and expose models to the conflicting and misleading evidence that humans work with in the real-world.</p><p>To bridge this gap, we introduce RAGuard, a benchmark dataset based on political discourse claims and their verifications from Poli-tiFact incorporating real-world misinformation. Given the prevalence of polarizing and deceptive information in political discourse, we develop an automated pipeline that retrieves relevant yet potentially misleading documents from Reddit via Google Search. Reddit, with its diverse and often controversial user-generated content, serves as a realistic source for challenging retrieval scenarios.</p><p>We introduce a novel LLM-guided approach to annotate retrieved documents by simulating a fact-checking exam. This method labels documents as supporting, misleading, or irrelevant based on their impact on the LLM's decision, providing a scalable benchmark to evaluate RAG systems in real-world, noisy retrieval scenarios. Each data point in our dataset consists of a claim, a fact-checking verdict, and multiple labeled associated documents. This structure enables a rigorous evaluation of the ability of RAG systems to navigate situations with both noisy and supporting information, reflecting real-world conditions where accurate retrieval cannot be guaranteed. Our benchmark supports verifying robustness on documents solely labeled as misleading or on the full dataset to systematize generalization capabilities in complex scenarios.</p><p>We evaluate widely used LLMs and RAG systems, testing their ability to predict the correct fact-checking verdict across three task configurations: Zero-Context Prediction (given claims with no retrieved documents), Standard RAG (given claims with retrieved documents), and Oracle Retrieval (given claims with their associated documents). Our results reveal that current LLMs are highly vulnerable and lack robustness in real-world scenarios. Performance drops significantly across all configurations when using the RAGuard knowledge base. Notably, incorporating associated documents as context leads to an even steeper decline compared to dynamically retrieved documents, demonstrating how our dataset effectively assigns impactful misinformation to claims. This further exposes the limitations of LLMs in handling misleading content. Qualitative analysis shows that RAG systems are particularly susceptible to overtly misleading information, falling short of human reasoning. Figure <ref type="figure" target="#fig_0">1</ref> highlights the motivation behind our dataset and the susceptibility of current RAG systems to misleading retrievals.</p><p>In summary, our work advocates for a shift in focus from developing idealized RAG settings to those that better simulate real-world noisy information. We provide a benchmark to evaluate the robustness of RAG systems against misleading retrievals, addressing the gap in naturally-occurring misinformation RAG datasets. Our baseline results reveal the current shortcomings of RAG systems, showing performance worse than zero-shot. We expect our dataset to contribute to the development of more reliable and resilient RAG systems in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DATASET</head><p>We introduce RAGuard, a benchmark for evaluating the robustness of RAG systems in political fact-checking. RAGuard simulates noisy real-world retrieval settings, where systems must navigate supporting, misleading, and irrelevant evidence. The dataset comprises 2,648 political claims, corresponding fact-checking verdicts, and 16,331 associated documents labeled by their agreement with the verdicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definitions</head><p>The main task in RAGuard is retrieval-augmented fact-checking, where claims are verified as true or false based on retrieved documents that may support, mislead, or be irrelevant to the claim. Prior works employ varying terminology to describe the presence of such noise in retrieved contexts or retrieval corpora <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>. To establish consistency, we define a structured taxonomy and align existing definitions (See Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>Typical RAG datasets, including all prior fact-checking datasets to our knowledge, exclusively contain non-noisy supporting documents as associated evidence, leading to overly optimistic performance <ref type="bibr" target="#b7">[8]</ref>. Instead of relying solely on answer-containing documents, our dataset adopts a broader notion of supporting evidence. Specifically, we consider a document to be supporting if it provides information that enables an LLM to infer the correct answer, even if it does not explicitly state the ground-truth output. This reflects real-world fact-checking, where human verifiers rely on contextual information rather than single authoritative documents.</p><p>We categorize different types of noisy evidence based on whether the information directly conflicts with aspects of the correct prediction. As in prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>, we include non-conflicting documents in RAGuard, such as irrelevant texts that may hurt performance. However, our primary focus is conflicting documents, which include misleading, fabricated, and unambiguous evidence. Previous datasets primarily include conflicting evidence as fabricated or unambiguous documents, oversimplifying real-world complexity and ambiguity (see Section 2.2 for further discussion) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>. Notably, no prior work has introduced misleading documents.</p><p>In RAGuard, misleading documents distort facts through selective framing, omission, or biased presentation, leading the system toward incorrect predictions while still containing partial truths. Unlike fabricated evidence, which is explicitly engineered to contradict the correct prediction (i.e., adversarial perturbations), misleading evidence subtly misguides the model rather than directly opposing it. Additionally, while prior work such as QACC <ref type="bibr" target="#b24">[25]</ref> introduces unambiguous evidence-a term we adopt to ensure consistency with past research-which includes some naturally conflicting evidence but only for a limited set of unambiguous questions, we focus on more natural yet scalable conflicting evidence.</p><p>For reference, we provide a list of all defined terms. Each term defines a type of document or piece of evidence.</p><p>(1) Associated: any document linked to a claim, regardless of label (2) Supporting: aids the system in producing a correct prediction through containing the correct answer explicitly or providing contextual support</p><p>Dataset Evidence Conflicting Real Domain Claims Retrieval Evidence World Fact-Checking FEVER [34] ✓ ✗ ✗ General 185K FEVEROUS [2] ✓ ✗ ✗ General 87K Liar [37] ✗ ✗ ✓ Political 12.8K Mocheg [45] ✓ ✗ ✓ Political 15.6K Snopes [17] ✓ ✗ ✓ Political 6.4K PUBHEALTH [22] ✓ ✗ ✓ Health 11.8K MultiFC [4] ✓ ✗ ✓ Political 43.8K Noisy Contexts Power of Noise [8]</p><formula xml:id="formula_0">✓ ✗ ✓ General 10K RAAT [11] ✓ ✓ ✗ General 7.8K NoiserBench [39] ✓ ✓ ✗ General 4K QACC [25] ✓ ✓ ✓ General 1.5K RAGuard ✓ ✓ ✓ Political 2.6K</formula><p>Table <ref type="table">1</ref>: Comparison of RAGuard with related fact-checking and RAG datasets. The columns indicate whether the dataset requires automatic evidence retrieval, contains conflicting evidence documents, and consists of naturally occurring realworld claims and evidence, as well as their domain and size.</p><p>(3) Noisy: challenges or disrupt system performance, thereby enhancing robustness (4) Conflicting: contradicts either the correct answer or some aspect of the prediction (5) Misleading: introduces factual distortions through selective framing, omission, or biased presentation; may contain partial truths (6) Fabricated: synthetically constructed to include factual errors (e.g., adversarial perturbations) (7) Unambiguous: naturally conflicting evidence but only for a limited set of unambiguous questions (special case of <ref type="bibr" target="#b24">[25]</ref>) (8) Non-Conflicting: does not directly contradict the correct answer but still introduces noise by distracting the model (9) Irrelevant: does not contain specific enough information to determine the correct prediction, despite being topically or semantically related to the query (10) Random: unrelated; often introduced through random selection or artificial generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Comparison with Existing Datasets</head><p>Table <ref type="table">1</ref> depicts a comparison of RAGuard with other fact-checking and RAG datasets.</p><p>Fact-Checking Datasets. All existing fact-checking datasets that include evidence retrieval contain only supporting documents. While FEVEROUS <ref type="bibr" target="#b1">[2]</ref> labels some documents as refute, this terminology is misleading-these documents actually support the falsehood of the claim rather than providing conflicting evidence. Therefore, while FEVEROUS categorizes evidence into support for true claims and refute for false claims, it does not include documents that actively contradict the claim's verdict. Additionally, both FEVER <ref type="bibr" target="#b33">[34]</ref> and FEVEROUS <ref type="bibr" target="#b1">[2]</ref> rely on rewritten Wikipedia statements rather than naturally occurring claims, as noted by <ref type="bibr" target="#b3">[4]</ref>.</p><p>Liar <ref type="bibr" target="#b36">[37]</ref> and Mocheg <ref type="bibr" target="#b44">[45]</ref> are the most similar to our dataset since they also source claims from Politifact. However, Liar <ref type="bibr" target="#b36">[37]</ref> does not support evidence retrieval as it lacks evidence documents. Mocheg <ref type="bibr" target="#b44">[45]</ref> includes only documents cited by Politifact fact-checkers, Statistic Number Total Claims 2,648 True 1,333 (50.3%) False 1,315 (49.7%) Avg. Claim Length (words) 17.6 Total Documents 16,331 Supporting 2,685 (16.4%) Misleading 1,812 (11.1%) Irrelevant 11,834 (72.5%) Avg. Document Length (words) 161 Avg. Documents Per Claim 6.2 Claims with Supporting Docs 955 (36.1%) Claims with Misleading Docs 788 (29.8%) (a) Main statistics of RAGuard. Says The We I In I f J o h n B a ra ck Th er e Wh en Hillary Obama Preside nt Unde r Beca use For Mit t As Ove r Joe Sen . Sen ato r A At You Afte r By Illeg al It More "Joe Americ a During First, Last On Right Since Speaking Thanks Video According Gas George Health I'm ISIS Many McCain Oil "For "If "We "When Don ald H ill ar y the he Mitt th a t Jo e hi s Pre sid ent Barack Bernie Obama a Romney she Paul of Scott Sen. Ted in John Repu blican s as U n it e d O b a m a U .S . Tr um p fe de ra l nu m be r Re pu bli ca n av er ag e Bu sh he alt h las t Dem ocra ts Sen ate bord er cost fact five med ia med ian only presi dent presi dent' s price unem ploym ent vast ha ve are now don't got spend have was don't neve r did didn't want the Ne w 202 0, Chic ago, y o u th e we yo u'r e M c C a in McC ain' s O b a m a Ob am a's ar e is was have I Mitt President the Clinton says voted has Barack Obam a the Presid ent of th e Ro m ne y a gove rnor , th e Bi de n O ba m a Ob am a McC ain few lot th e can kno w, th e th e im mi gra tio n is tha n Bid en America Best First Official is do a re d o e s d id do es n' t w as ha s hav e don 't the don'tnot aren't didn't would can't isn'tDo were America aren't Are Is am didn't isn't should Did can't hasn't won't couldn't hasn't people is do are if would exactly was  which support the verdict rather than introducing conflicting or misleading evidence. Other datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref> primarily use journalistwritten explanations from fact-checking websites, which are structured to justify the verdict rather than reflect the complexity of real-world misinformation.</p><p>In contrast, RAGuard explicitly incorporates conflicting evidence, making it more representative of real-world misinformation challenges. Unlike curated fact-checking content, our dataset sources evidence from Reddit discussions, which naturally contain misleading information through diverse viewpoints. This increases the difficulty of RAG by better aligning ot with real-world misinformation.</p><p>Datasets with Noisy Contexts. Prior datasets that include noisy evidence primarily build on open-domain question answering (QA) datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>. As these datasets differ in how they define and introduce noise, we use our definition framework in Figure <ref type="figure" target="#fig_1">2</ref> to better distinguish each dataset.</p><p>Power of Noise <ref type="bibr" target="#b7">[8]</ref> classifies evidence into gold, relevant, distracting, and random categories. Gold and relevant documents serve as supporting evidence, with gold documents being preexisting gold-standard documents and relevant documents being newly retrieved documents that explicitly contain the correct answer. Nonconflicting evidence includes distracting documents, which are simply non-gold retrievals and therefore irrelevant. Additionally, Power of Noise is the only dataset that introduces random evidence, which is entirely unrelated to the query. Notably, it does not include any conflicting evidence.</p><p>RAG-Bench <ref type="bibr" target="#b10">[11]</ref> classifies evidence into golden context, irrelevant retrieval noise, relevant retrieval noise, and counterfactual retrieval noise. It constructs supporting evidence using gold-standard documents from non-noisy QA datasets. It introduces fabricated evidence by modifying documents to contain incorrect answers. While this results in factually incorrect conflicting evidence, it does not capture misleading evidence, which may contain partial truths but manipulates the information through selective framing or omission. Both relevant and irrelevant retrieval noise are considered irrelevant, as their retrieval via semantic search implies a degree of semantic relatedness to the query while not directly conflicting with the task content. Nonetheless, they can still distract RAG systems.</p><p>NoiserBench <ref type="bibr" target="#b38">[39]</ref> introduces a wide range of types of noise, including inserting counterfactual noise. While this introduces conflicting evidence, all noise is artificially constructed, limiting its reflection of real-world misleading information. Like RAG-Bench, it focuses on fabricated evidence rather than capturing more complex distortions such as selective framing or omission.</p><p>QACC <ref type="bibr" target="#b24">[25]</ref> employs human annotators to label retrieved documents as conflicting or non-conflicting with answers from Am-bigQA. Rather than artificially injecting errors, it includes conflicting evidence that directly contradicts the correct answer. However, its reliance on human annotation limits scalability, and its approach does not fully capture real-world ambiguity, as it focuses on clear-cut conflicts (i.e., questions labeled as "unambiguous" from AmbigQA) rather than more nuanced misleading evidence.</p><p>Unlike these datasets, our work focuses on the political domain, which presents distinct challenges. Political misinformation has tangible consequences, influencing public opinion, policy decisions, and elections <ref type="bibr" target="#b36">[37]</ref>. Misleading evidence in political discourse is often more nuanced, relying on selective framing rather than outright falsehoods. Furthermore, political fact-checking requires domainspecific reasoning, as claims are frequently shaped by ideological bias and rhetorical strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset Structure</head><p>RAGuard consists of 2,648 political claims made by U.S. presidential candidates (2000-2024), each labeled as either true or false, and a knowledge base comprising 16,331 documents. The dataset's key statistics are presented in Table <ref type="table">3a</ref>. Each claim is linked to a set of associated documents, categorized as supporting, misleading, or irrelevant, with an average of 6.2 documents per claim. Notably, the dataset contains more supporting documents than misleading ones, reflecting that political discussions online are more often aligned with factual information. However, not every claim has both misleading and supporting documents, highlighting the imbalanced nature of political discourse, where certain narratives dominate while others lack counterpoints. The dataset is provided in two  Table 3: Example of data structure of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Supported Tasks</head><p>To benchmark the performance of current RAG systems in realworld fact-checking scenarios, we define a series of tasks using RAGuard. Each task evaluates a different aspect of RAG system robustness against misleading or conflicting contextual data.</p><p>Zero-Context Prediction. This task assesses a RAG system's ability to fact-check claims without external contextual information. The goal is to evaluate the intrinsic knowledge encoded in each model during pre-training and its effectiveness in verifying claims. This serves as a baseline to measure the impact of external retrieval on performance.</p><p>Standard RAG. This task simulates a real-time RAG system retrieving documents from the entire dataset corpus. The retrieved documents may include supporting, misleading, or irrelevant information to the claim, introducing retrieval noise. In extreme cases of poor retrieval, models may receive documents unrelated to the claim, mimicking real-world retrieval limitations.</p><p>Oracle Retrieval. This task provides RAG systems with the ssociated documents for each claim, isolating the impact of the associated documents labeled in our dataset. Unlike the previous task, where retrieval noise is unpredictable, this setting ensures systems receive only the supporting, misleading, and irrelevant documents associated with each claim. This setup evaluates how well models can filter out deceptive content when retrieval errors are controlled. We include two specific evaluations for this task. In the first, each model receives an associated document for a claim, regardless of the categorization as supporting, misleading, or irrelevant, to assess its ability to generalize to complex scenarios where documents have potential to support, mislead, or be irrelevant. In the second, we isolate only instances where the associated document is labeled as misleading, which conflicts with the claim's ground truth factchecking verdict, testing susceptibility to misleading information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET CONSTRUCTION</head><p>We construct RAGuard in three stages, depicted in Figure <ref type="figure" target="#fig_5">4</ref>. First, we collect political claims and fact-checking labels from PolitiFact, a reputable source for verified political fact-checking information. Next, we construct a knowledge base by retrieving relevant Reddit documents via a search engine, leveraging its diverse, real-time content to reflect real-world discourse. Finally, we introduce a novel, scalable LLM-guided approach to classify documents as misleading, supporting, or irrelevant by simulating the LLM taking an exam. The following sections outline the details of each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Claim and Verdict Collection</head><p>To collect claims and fact-checking verdicts for RAGuard, we scrape PolitiFact,<ref type="foot" target="#foot_1">foot_1</ref> a reputable platform where expert journalists assess the truthfulness of a wide range of political claims. We focus on claims made by major U.S. presidential candidates from 2000 to 2024 to ensure the inclusion of widely discussed statements that have been frequently fact-checked and debated, generating substantial online discourse and reflecting politically significant information. To facilitate document retrieval, we condense PolitiFact's six-point truth scale (true, mostly true, half true, mostly false, false, pants on fire) into binary labels-true and false, as it is challenging for a document to specifically mislead a half true verdict, undermining our core contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Base Construction</head><p>To construct the RAGuard knowledge base, we employ a multi-step retrieval process that balances coverage, diversity, and realism. First,</p><p>Gemini 1.5 Flash GPT-4o Mini Claude 3.5 Sonnet Llama 3 Mistral Task 1: Zero-Context Prediction 61.06 67.33 74.51 62.50 63.97 Task 2: Standard RAG RAG-1 56.68 ↓ -4.38% / -7.2% 64.80 ↓ -2.53% / -3.8% 70.09 ↓ -4.42% / -5.9% 59.40 ↓ -3.10% / -5.0% 59.14 ↓ -4.83% / -7.5% RAG-5 57.59 ↓ -3.47% / -5.7% 65.90 ↓ -1.43% / -2.1% 68.58 ↓ -5.93% / -8.0% 61.37 ↓ -1.13% / -1.8% 58.91 ↓ -5.06% / -7.9% Task 3: Oracle Retrieval All Documents 52.38 ↓ -8.68% / -14.2% 53.22 ↓ -14.11% / -20.9% 51.17 ↓ -23.34% / -31.3% 61.09 ↓ -1.41% / -2.3% 51.61 ↓ -12.36% / -19.3% Misleading-Only 30.57 ↓ -30.49% / -49.9% 45.97 ↓ -21.36% / -31.7% 37.05 ↓ -37.46% / -50.3% 36.81 ↓ -25.69% / -41.1% 28.22 ↓ -35.75% / -55.9%</p><p>Table 4: Performance of various LLM backbones in RAG setup on three tasks, reported in Accuracy (%). The red numbers indicate the absolute/relative accuracy drop compared to Zero-Context Prediction (Task 1) under each setting.</p><p>GPT-4 extracts the keywords from each claim. This keyword expansion ensures a broader and more nuanced search space, increasing the likelihood of retrieving both corroborating and contradicting information. Next, we perform a keyword-based Google Search to retrieve up to ten relevant Reddit posts per claim. Google's ranking ensures contextual relevance, while Reddit's user-generated content introduces diverse perspectives, from speculative theories to well-supported arguments. Unlike curated fact-checking datasets, Reddit captures real-world discourse, including misinformation and conflicting viewpoints. This retrieval pipeline creates a realistic testbed for fact-checking, combining GPT-4-assisted keyword expansion with search engine retrieval to mirror the complexities of real-world misinformation challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LLM-Guided Document Annotation</head><p>Our work defines a document's role in a RAG system based on its influence on the LLM's decision-making. Unlike prior studies that introduce counterfactual evidence or rely on human annotators, our approach directly evaluates whether a document aids or misleads the LLM in real time. We achieve this by simulating a fact-checking scenario during annotation, treating the LLM as an exam taker.</p><p>To generate labels, we simulate the RAG fact-checking process at inference time. Given a claim (Section 3.1) and a retrieved document (Section 3.2), GPT-4 classifies the claim as true or false based on the document's content. If the classification aligns with the ground-truth, the document is labeled supporting; if it contradicts the gold label, it is misleading; and if it does not contribute to verification, it is irrelevant. By basing labels on the LLM's actual behavior, our approach ensures document annotations reflect their real impact on fact-checking, providing a scalable, empirically grounded alternative to human annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BASELINES 4.1 Experimental Setup</head><p>Evaluation. We frame fact-checking as a binary classification task where the model must generate a response that aligns with one of the predefined options. Accuracy, calculated with the gold label serving as the reference, is used to evaluate performance. If a model generates an out-of-scope response that does not match any of the given options, it is treated as an incorrect prediction.</p><p>Implementation Details. We evaluate RAG systems using both open-source and closed-source LLMs to assess their capabilities for real-world applications. For closed-source benchmarks, we test Google's Gemini 1.5 Flash <ref type="bibr" target="#b13">[14]</ref>, OpenAI's GPT-4o Mini <ref type="bibr" target="#b27">[28]</ref>, and Anthropic's Claude 3.5 Sonnet <ref type="bibr" target="#b2">[3]</ref> via their respective APIs. For open-source benchmarks, we evaluate Meta's LLama3 8B Instruct <ref type="bibr" target="#b8">[9]</ref> and Mistral's Mistral 7B Instruct <ref type="bibr" target="#b17">[18]</ref> by running local inference.</p><p>In all settings, two-shot examples-one true and one false claim from RAGuard training data-are provided in the context. In the Standard RAG setting, we employ OpenAI's text-embedding-ada-002 for semantic search using the original claim as the query and provide the top one and five retrieved documents as context to the LLM. In the Oracle Retrieval setting, our system only takes one document at a time to find the impact of each document to the result. In our prompt, we specify that contextual documents may not be relevant or correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table">4</ref> displays baseline results on RAGuard for three tasks using two open and three closed-source LLMs.</p><p>Zero-Context Prediction. In the zero-context prediction, RAG systems operate without context documents from the RAGuard knowledge base. We use this as our baseline (a.k.a, zero-shot baseline). All systems achieve the highest accuracy scores, which is counterintuitive, considering this setting does not benefit from retrieval.</p><p>Standard RAG. Performance decreases for all models when integrating retrieval, with all scores falling below the zero-shot baseline. The decline is consistent across both RAG-1 and RAG-5 settings, though the magnitude varies. GPT-4 remains the most robust, exhibiting only a minor drop, while other models, particularly Mistral and Gemini, experience more pronounced declines. Increasing retrieval (RAG-1 to RAG-5) does not consistently improve performance and sometimes worsens it. This suggests that retrieval introduces both useful and misleading information, and when retrieval quality is not optimal, the additional context can confuse rather than help. These findings challenge the assumption that retrieval always benefits downstream performance and reinforce prior research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref> on the risks of noisy retrieval in high-stakes tasks.</p><p>Oracle Retrieval. The results in the Oracle Retrieval setting reveal a striking trend: incorporating associated documents from RAGuard leads to a significant drop in performance compared to the zero-context baseline. This suggests that models are highly sensitive to misleading or irrelevant information. This trend holds across all models except Llama3, indicating that randomly retrieved irrelevant documents are less harmful than misleading content. This finding underscores the challenge posed by RAGuard, which systematically tests model robustness misleading information.</p><p>In this task's All Documents setting, where models receive all associated documents for a claim, performance generally declines, suggesting that models struggle to reconcile conflicting evidence. The impact is even more severe in the Misleading-Only setting, where models are provided only with misleading documents that contradict the claim's ground truth. Most models falling to around 30% accuracy despite the binary nature of the task. This confirms that models are highly susceptible to misleading information and struggle to distinguish factual content from misinformation, highlighting LLM limitations in handling misleading evidence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Comparison of Model Robustness. Figure <ref type="figure" target="#fig_6">5</ref> displays the relative performance decreases across different models. Notably, Claude 3.5 Sonnet, which achieved the highest accuracy in the zero-context baseline, experienced the greatest decreases when exposed to noisy retrieval. In the All Documents Oracle Retrieval setting, Claude suffered one of the steepest declines, and its performance dropped even more drastically in the Misleading-Only condition. This suggests that while Claude performs well in ideal conditions, it is particularly susceptible to misleading evidence, struggling to filter out incorrect information when retrieval introduces contradictory or noisy context. This may also be due to its high baseline performance.</p><p>Conversely, GPT-4o Mini demonstrated the highest robustness against misleading evidence. Its relative performance drop in the Misleading-Only setting was 31.7%, significantly lower than the approximate 50% declines observed for Gemini, Claude, and Mistral. This suggests that GPT-4o Mini is better at handling misleading content and maintaining accuracy in noisy retrieval conditions, highlighting differences in model sensitivity to retrieval-induced noise.</p><p>Effect of RAG on Accuracy. Figure <ref type="figure" target="#fig_6">5</ref> challenges the common assumption that RAG consistently enhances model performance. When retrieval introduces misleading or irrelevant evidence, it actively degrades accuracy, often leading to worse performance than zero-shot baselines. This is particularly evident in the Misleading-Only condition from Task 3, where each model is intentionally provided with documents contradicting the claim's ground truth. Across all models, accuracy drops significantly, with an average performance decrease of 45.8%, highlighting the substantial risks posed by retrieval-induced misinformation. These findings emphasize that retrieval, when not carefully controlled, can be detrimental rather than beneficial, further reinforcing the importance of robust filtering and sufficiency evaluation in RAG systems.</p><p>Retrieval Performance. Retrieval performance is a standard metric in RAG benchmarks, but our dataset focuses on how models handle misleading or conflicting evidence. High retrieval accuracy alone does not ensure reliable answers due to misleading information in the corpus. Nonetheless, to To provide a full view of system behavior, we report both conventional retrieval metrics and a tailored measurement called Misleading Retrieval Recall.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows Retrieval Precision, Recall, and Normalized Discounted Cumulative Gain (NDCG) for Task 2 (Standard RAG). Recall naturally rises with 𝐾, while precision decreases. NDCG follows a non-monotonic trend, dipping around 𝐾 = 10 before recovering due to relevant items being unevenly distributed across ranked positions, causing reordering as 𝐾 changes.</p><p>We also report Misleading Retrieval Recall-the fraction of claims retrieving at least one misleading document. Task 1 (Zero-Context) scores 0%, while Task 3 (Oracle Retrieval) is 100%. In Task 2, RAG-1 scores 21.3%, increasing to 44.8% for RAG-5, showing a higher risk of retrieving misleading content when retrieving more documents. As seen in Table <ref type="table">4</ref>, this correlates with lower overall accuracy.</p><p>Qualitative Example. Figure <ref type="figure" target="#fig_8">7</ref> presents example system predictions on RAGuard, illustrating the impact of misleading documents. The left example highlights how misleading documents negatively affect the classification of a true claim. While misleading documents generally degrade system performance compared to zero-shot predictions, their specific influence varies based on their complexity. We distinguish three categories of misleading documents:</p><p>(1) Overtly Misleading Document: This category includes documents that are evidently misleading to humans but still lead to incorrect predictions by all RAG systems. For example, in Figure <ref type="figure" target="#fig_8">7</ref>, the document falsely comparing California's job growth to the national average misleads all systems (1b), despite their correct zero-shot predictions (1a). This suggests a form of selective bias, where the systems prioritize the provided information simply because it is included in the prompt, even though the instructions explicitly caution against assuming its correctness. (2) Partially True Misleading Document: These documents contain partial truths, making it necessary to apply reasoning to recognize their misleading nature. For example, as shown in Figure <ref type="figure" target="#fig_8">7</ref>, one document criticizes unemployment but also states that "official job reports are reporting jobs added" (1c). While this statement supports the claim that 500,000 jobs were added, the document's overall tone suggests rising unemployment. However, this suggestion is more of an opinion than a fact. Some LLMs, such as GPT-4 and Mistral, were able to reason through this contradiction and classify the claim correctly. (3) Challenging Misleading Document: These documents present significant challenges, even for human annotators. For example, a claim referencing job growth in the 2000s is incorrectly classified because the RAG system retrieves data from 2024, which accurately reports lower job creation (1d). The temporal misalignment in retrieved documents presents a fundamental challenge in this dataset and task.</p><p>The middle example demonstrates GPT-4's ability to filter out noise from retrieved documents that are not associated with the claim but could be considered misleading documents in our dataset (e.g., documents using the same phrasing but referring to different individuals, such as "Harris" instead of "Clinton" in example 2b). Even when five irrelevant documents are retrieved (2c), GPT-4 remains robust. However, when presented with a misleading document from the dataset (2d), GPT-4 fails, reinforcing the dataset's effectiveness in challenging model performance beyond conventional RAG noise. This further explains the lower accuracy observed in the Oracle Retrieval setting in our baseline experiments.</p><p>The right example shows how GPT-4 tends to assign disproportionate weight to misleading documents, allowing them to override even non-associated supporting evidence. In the example, a nonassociated document that contains supporting information (3b) enables GPT-4 to correct its initially incorrect zero-shot prediction (3a). However, when a misleading document is retrieved alongside other non-associated supporting documents (3c), the system incorrectly classifies the claim, similar to its behavior when only the misleading document is retrieved (3d). This demonstrates that misleading documents can have a stronger influence on the model's classification, regardless of the presence of supporting evidence, highlighting a significant vulnerability in RAG systems.</p><p>These examples highlight three key findings: (1) LLMs remain highly susceptible to misleading documents, even when their content is transparently incorrect, (2) misleading documents retrieved from the dataset exert a stronger influence than non-associated documents retrieved erroneously, and (3) when misleading documents are present, they can significantly outweigh supporting evidence, leading to incorrect predictions. These findings emphasize the strength and uniqueness of our dataset in evaluating and challenging RAG-based model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Retrieval-Augmented Generation with Noisy Contexts. Retrieval-Augmented Language Models (RALMs) have demonstrated strong performance across various NLP tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>. However, their effectiveness is constrained by the retriever's ability to find supporting information. In real-world applications, retrieval often introduces irrelevant or misleading content, which can significantly degrade model performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref>. Prior work has identified two primary effects of such noise.</p><p>The first is the impact of irrelevant documents on RAG performance <ref type="bibr" target="#b6">[7]</ref>. To make RAG systems more robust to this issue, researchers have explored several strategies, including prompting the language model to generate a rationale connecting retrieved documents to the query <ref type="bibr" target="#b37">[38]</ref>, employing multi-agent debate systems to identify the most relevant information <ref type="bibr" target="#b35">[36]</ref>, and aggregating multiple documents to produce a more reliable final response <ref type="bibr" target="#b39">[40]</ref>.</p><p>The second challenge arises when retrieved documents conflict with an LLM's internal knowledge <ref type="bibr" target="#b20">[21]</ref>. To address this, AstuteRAG introduced an iterative system that consolidates internal and external knowledge, reducing inconsistencies in generated responses <ref type="bibr" target="#b34">[35]</ref>. However, this study focuses on disagreements between documents in existing datasets, which do not intentionally mislead RAG systems and do not represent conflicting information in the real world.</p><p>Researchers have increasingly attempted to improve RAG robustness through developing datasets that expose the model to conflicting contexts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> and retrieval noises <ref type="bibr" target="#b38">[39]</ref> that may later be used for adversarial training <ref type="bibr" target="#b10">[11]</ref>. However, existing datasets fail to fully capture the complexities of real-world misinformation.</p><p>In contrast, RAGuard is the first to capture misleading context that reflects real-world ambiguities, polarized opinions, and partial truths. Unlike prior datasets that primarily focus on synthetic conflicts or document-model disagreements, RAGuard captures naturally occurring misinformation, making it a more realistic benchmark for evaluating RAG robustness.</p><p>The Limitations of Open-Book QA Datasets. RAG is studied primarily through open-book question answering (QA), where models answer questions based on retrieved knowledge <ref type="bibr" target="#b12">[13]</ref>. However, most Open-Book QA datasets carefully curate their documents, avoiding noisy information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>. This leads to strong performance in controlled settings, but poor generalization in real-world scenarios, where conflicting documents often degrade performance <ref type="bibr" target="#b7">[8]</ref>.</p><p>Some datasets attempt to address this issue by synthetically introducing counterfactual information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>. However, synthetic noise may not fully capture the complexities of real-world misinformation. Others rely on human annotators to identify conflicting documents <ref type="bibr" target="#b24">[25]</ref>, but this approach is costly and difficult to scale. <ref type="bibr" target="#b7">[8]</ref> classifies retrieved documents as distracting based on their equivalence to a gold-standard document, but this approach may not reflect the existence of truly deceptive or contradictory information.</p><p>In contrast to these approaches, we leverage real-world misinformation by using political fact-checking data, which contains misleading and conflicting information. This allows us to construct a dataset that better reflects the challenges RAG systems face.</p><p>Fact-Checking and RAG. Fact-checking is a well-studied task with datasets sourced from platforms like Twitter <ref type="bibr" target="#b26">[27]</ref>, Wikipedia <ref type="bibr" target="#b33">[34]</ref>, and PolitiFact <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. Given that fact-checking often relies on external evidence, it aligns well with RAG, where retrieval can support the verification or negation of a claim. However, existing fact-checking datasets typically use gold-standard evidence from the same source as the verdict <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>, meaning there is no exposure to noisy or contradictory evidence. Despite the inherently polarizing nature of online and political discourse, the retrieved evidence in these datasets rarely contradicts the final verdict. Our work introduces noisy and conflicting information into fact-checking datasets, ensuring that retrieved evidence is not always aligned with the verdict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we introduce RAGuard, a challenging and diverse fact-checking dataset designed to assess the robustness of RAG systems against misleading retrievals. RAGuard comprises 2,648 claims-1,333 true and 1,315 false-paired with 16,331 documents, averaging 6.2 documents per claim. These documents are labeled using a novel LLM-guided approach that simulates an exam-like evaluation, analyzing how the model processes and interprets retrieved evidence at inference time to determine whether the documents support, mislead, or are irrelevant to the claim. Unlike prior RAG benchmarks that rely on synthetically noisy data or curated gold-standard documents, RAGuard utilizes real-world evidence, even in cases where no gold-standard documents exist. By incorporating naturally occurring misleading data from Reddit discussions alongside verified evidence and claims from PolitiFact, it mirrors the complexities of real-world misinformation, which is necessary for more robust systems.</p><p>Our findings show that the performance of current RAG systems deteriorates significantly when exposed to misleading evidence, challenging the assumption that retrieval always enhances model accuracy. These results highlight the need for more resilient factchecking pipelines. Future research should focus on enhancing retrieval robustness through methods such as adversarial retrieval training, which exposes models to misleading evidence during training to improve resilience, and uncertainty-aware retrieval, which prioritizes evidence credibility over mere relevance. Additionally, fact-verification mechanisms that incorporate multi-step reasoning and cross-document consistency checks can mitigate the impact of misleading sources, while confidence calibration techniques may further refine the model's ability to discern factual inconsistencies.</p><p>By providing a challenging yet realistic benchmark, RAGuard encourages the development of more sophisticated retrieval-based fact-checking methodologies. We hope this dataset will facilitate progress in designing retrieval pipelines that are not only effective but also resistant to misinformation, ultimately contributing to more reliable and trustworthy AI systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a false claim initially classified correctly but later misclassified as true due to misleading retrievedcontext, alongside the ideal human judgment.</figDesc><graphic coords="2,53.80,83.69,240.25,277.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Taxonomy of terminology to classify different types of evidence, labeled with prior works' contributions (left), and our dataset's composition (right).</figDesc><graphic coords="3,53.80,83.69,240.23,110.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>election the your going on deficit employment federal salary the Trump'in / a a 7 (c) Word distribution for documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Key statistics and word distributions of RAGuard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: RAGuard dataset construction, consisting of three stages to obtain claims and verdicts; associated documents; and labels for the each document's relationship to the claim and verdict.</figDesc><graphic coords="5,53.80,83.69,504.37,70.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance decreases from the Zero-Context baseline to Task 2 and 3 when using RAGuard across various models. Results are measured in Accuracy and include relative percent decreases.</figDesc><graphic coords="7,65.81,282.40,216.22,157.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Retrieval Accuracy, Recall, and NDCG at Different Top K Levels</figDesc><graphic coords="7,53.80,518.03,240.25,144.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example predictions on RAGuard, compared to the expected human response. Note that each column compares different prediction scenarios based on varying retrieved contexts for the same claim rather than a multi-turn process. Left: Each system's classification of a true claim with three progressively misleading documents. Middle: GPT-4-based system's classification of a false claim with one noisy non-associated document, many noisy non-associated documents, and a misleading document. Right: GPT-4-based system's classification of a true claim with a supporting non-associated document, one misleading document along with other supporting non-associated documents, and a misleading document.</figDesc><graphic coords="8,53.80,83.68,504.40,175.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Example of data structure of claims.</figDesc><table><row><cell>Claims.csv</cell><cell></cell></row><row><cell>ID</cell><cell>1517</cell></row><row><cell>Claim</cell><cell>Insulin for Medicare beneficiaries dec...</cell></row><row><cell>Verdict</cell><cell>True</cell></row><row><cell>Associated Docs</cell><cell>[9400, 9402, 9405, . . . ]</cell></row><row><cell cols="2">Document Labels [irrelevant, supporting, misleading, ...]</cell></row><row><cell>Documents.csv</cell><cell></cell></row><row><cell>ID</cell><cell>9405</cell></row><row><cell>Title</cell><cell>My dad is spending $700/mo on insulin...</cell></row><row><cell>Full Text</cell><cell>...</cell></row><row><cell>Document Label</cell><cell>Misleading</cell></row><row><cell cols="2">Associated Claim 1517</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The dataset is available at https://huggingface.co/datasets/UCSC-IRKM/RAGuard.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.politifact.com/</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the <rs type="institution">Robustness of RAG Against Misleading Retrievals. In Preprint, 2025. ACM, New York, NY, USA</rs>, 11 pages. <ref type="url" target="https://doi.org/XXX.XXX">https://doi.org/XXX.XXX</ref> </p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Where is Your Evidence: Improving Fact-checking by Justification Modeling</title>
		<author>
			<persName><forename type="first">Savvas</forename><surname>Tariq Alhindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smaranda</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><surname>Muresan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5513</idno>
		<ptr target="https://doi.org/10.18653/v1/W18-5513" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), James Thorne, Andreas Vlachos, Oana Cocarascu</title>
		<editor>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</editor>
		<meeting>the First Workshop on Fact Extraction and VERification (FEVER), James Thorne, Andreas Vlachos, Oana Cocarascu<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Rami</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05707[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2106.05707" />
		<title level="m">Oana Cocarascu, and Arpit Mittal. 2021. FEVER-OUS: Fact Extraction and VERification Over Unstructured and Structured information</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<ptr target="https://www.anthropic.com/news/claude-3-5-sonnet" />
		<title level="m">Claude 3.5 sonnet</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><forename type="middle">Chaves</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1475</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1475" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4685" to="4697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic Parsing on Freebase from Question-Answer Pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D13-1160" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</editor>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Bm</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2206" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.01431[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2309.01431" />
		<title level="m">Benchmarking Large Language Models in Retrieval-Augmented Generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems</title>
		<author>
			<persName><forename type="first">Florin</forename><surname>Cuconasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Trappolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Siciliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<idno type="DOI">10.1145/3626772.3657834</idno>
		<ptr target="https://doi.org/10.1145/3626772.3657834" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024)</title>
		<meeting>the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ugur Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training</title>
		<author>
			<persName><forename type="first">Feiteng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuelin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20978[cs.AI</idno>
		<ptr target="https://arxiv.org/abs/2405.20978" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangxiang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10997</idno>
		<title level="m">Retrieval-augmented generation for large language models: A survey</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangxiang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10997[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2312.10997" />
		<title level="m">Retrieval-Augmented Generation for Large Language Models: A Survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><surname>Gemini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05530</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models</title>
		<author>
			<persName><forename type="first">Neel</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Nyarko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chohlas-Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Waldon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rockmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Zambrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Retrieval Augmented Language Model Pre-Training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v119/guu20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1046</idno>
		<ptr target="https://doi.org/10.18653/v1/K19-1046" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<editor>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</editor>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="493" to="503" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">El</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><surname>Sayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2310.06825" />
		<title level="m">Mistral 7B</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/1705.03551" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dense Passage Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.550" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Kortukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16032[cs.LG</idno>
		<ptr target="https://arxiv.org/abs/2404.16032" />
		<title level="m">Studying Large Language Model Behaviors Under Context-Memory Conflicts With Real Documents</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explainable Automated Fact-Checking for Public Health Claims</title>
		<author>
			<persName><forename type="first">Neema</forename><surname>Kotonya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Toni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.623</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.623" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7740" to="7754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural Questions: A Benchmark for Question Answering Research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00276" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2005.11401" />
		<title level="m">Generation for Knowledge-Intensive NLP Tasks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Siyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishaloy</forename><surname>Halder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mon</forename><surname>Phu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Neha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonan</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yassine</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.12311[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2410.12311" />
		<title level="m">Open Domain Question Answering with Conflicting Contexts</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.09513[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2209.09513" />
		<title level="m">Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mumin: A large-scale multilingual multimodal fact-checked misinformation social network dataset</title>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">S</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcconville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 45th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3141" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.21276</idno>
		<title level="m">Gpt-4o system card</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The PolitiFact-Oslo Corpus: A New Dataset for Fake News Analysis and Detection</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Nele Põldvere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleena</forename><surname>Zia Uddin</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="j">Inf</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">265420523</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/1606.05250" />
		<title level="m">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the Role of Relevance in Natural Language Processing Tasks</title>
		<author>
			<persName><forename type="first">Artsiom</forename><surname>Sauchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3532034</idno>
		<ptr target="https://doi.org/10.1145/3477495.3532034" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Madrid, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1785" to="1789" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;22)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FakeNewsNet: A Data Repository with News Content</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Mahudeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01286[cs.SI</idno>
		<ptr target="https://arxiv.org/abs/1809.01286" />
	</analytic>
	<monogr>
		<title level="m">Social Context and Spatialtemporal Information for Studying Fake News on Social Media</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated Fact Checking: Task Formulations, Methods and Future Directions</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/C18-1283/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pierre</forename><surname>Isabelle</surname></persName>
		</editor>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3346" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">FEVER: a Large-scale Dataset for Fact Extraction and VERification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1074</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sercan</forename><forename type="middle">Ö</forename><surname>Arık</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07176[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2410.07176" />
		<title level="m">Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyuan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianglong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lian</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.04854[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2312.04854" />
		<title level="m">Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Liar, Liar Pants on Fire&quot;: A New Benchmark Dataset for Fake News Detection</title>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2067</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-2067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</editor>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.13629[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2406.13629" />
		<title level="m">InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pandora&apos;s Box or Aladdin&apos;s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models</title>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feihu</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.13533[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2408.13533" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Chong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.15556[cs.LG</idno>
		<ptr target="https://arxiv.org/abs/2405.15556" />
		<title level="m">Certifiably Robust RAG against Retrieval Corruption</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13300[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2305.13300" />
		<title level="m">Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Benchmarking Retrieval-Augmented Generation for Medicine</title>
		<author>
			<persName><forename type="first">Guangzhi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.13178[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2402.13178" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Im-rag: Multi-round retrieval-augmented generation through learning inner monologues</title>
		<author>
			<persName><forename type="first">Diji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kezhen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="730" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HotpotQA: A Dataset for Diverse</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/1809.09600" />
	</analytic>
	<monogr>
		<title level="m">Explainable Multi-hop Question Answering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models</title>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Hee</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3539618.3591879</idno>
		<ptr target="https://doi.org/10.1145/3539618.3591879" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;23)</title>
		<meeting>the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;23)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2733" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Xunjian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.14820[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2310.14820" />
		<title level="m">ALCUNA: Large Language Models Meet New Knowledge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14010[cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2305.14010" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
