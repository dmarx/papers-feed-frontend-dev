# Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals

## Abstract

## 

Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation.

In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance.

To bridge this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: supporting, misleading, and irrelevant, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information.

Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments. To the best of our knowledge, RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence. We expect this benchmark will drive future research toward improving RAG * Equal contribution. ‚Ä† Co-advising.

## INTRODUCTION

Retrieval-augmented generation (RAG) systems have shown significant promise in mitigating LLM hallucination and enhancing trustworthiness. By combining the generative capabilities of large language models (LLMs) with the retrieval power of external corpora, RAG aims to ground responses in relevant, contextually appropriate information, thereby improving factual consistency and output credibility [[12,](#b11)[16,](#b15)[24]](#b23). However, while existing RAG approaches primarily focus on optimizing retrieval relevance and maximizing the amount of information in retrieved-context [[6,](#b5)[20,](#b19)[43]](#b42), a critical challenge remains largely unaddressed: how to handle cases where retrieved content is misleading or irrelevant. This issue is particularly concerning when misinformation, adversarial perturbations, or biased sources influence the retrieval process, potentially degrading the reliability of LLM outputs. Addressing this robustness gap is essential for ensuring the trustworthiness of RAG systems, especially in high-stakes applications such as fact-checking [[33]](#b32) and legal or medical domains [[15,](#b14)[42]](#b41).

Prior work has mitigated noisy retrievals by prompting models to justify relevance, aggregating sources, or using debate-based selection [[36,](#b35)[38,](#b37)[40]](#b39). However, most approaches align retrieved content with LLMs' prior knowledge rather than addressing real-world contradictions [[21,](#b20)[35]](#b34). Furthermore, current datasets overly rely on curating reliable documents, limiting robustness testing against misinformation [[19,](#b18)[23,](#b22)[30,](#b29)[44]](#b43). While some introduce counterfactuals or retrieval noise [[8,](#b7)[25]](#b24), they rely on artificial perturbations or costly human annotation. This highlights the need for an evaluation framework that challenges RAG systems with real-world contradictions, exposing their limitations and improving resilience in complex retrieval scenarios.

Fact-checking plays a crucial role in combating misinformation, yet most existing datasets assume the availability of gold-standard evidence that aligns with the verdict [[2,](#b1)[4,](#b3)[17,](#b16)[22,](#b21)[34,](#b33)[45]](#b44). In reality, retrieved information often presents conflicting perspectives, making automated verification more challenging. The political domain is particularly rich in such complexities, as controversial claims generate both supporting and opposing narratives from diverse sources [[27,](#b26)[29,](#b28)[32,](#b31)[34]](#b33). To develop fact-checking systems capable of handling real-world misinformation, it is essential to move beyond idealized settings and expose models to the conflicting and misleading evidence that humans work with in the real-world.

To bridge this gap, we introduce RAGuard, a benchmark dataset based on political discourse claims and their verifications from Poli-tiFact incorporating real-world misinformation. Given the prevalence of polarizing and deceptive information in political discourse, we develop an automated pipeline that retrieves relevant yet potentially misleading documents from Reddit via Google Search. Reddit, with its diverse and often controversial user-generated content, serves as a realistic source for challenging retrieval scenarios.

We introduce a novel LLM-guided approach to annotate retrieved documents by simulating a fact-checking exam. This method labels documents as supporting, misleading, or irrelevant based on their impact on the LLM's decision, providing a scalable benchmark to evaluate RAG systems in real-world, noisy retrieval scenarios. Each data point in our dataset consists of a claim, a fact-checking verdict, and multiple labeled associated documents. This structure enables a rigorous evaluation of the ability of RAG systems to navigate situations with both noisy and supporting information, reflecting real-world conditions where accurate retrieval cannot be guaranteed. Our benchmark supports verifying robustness on documents solely labeled as misleading or on the full dataset to systematize generalization capabilities in complex scenarios.

We evaluate widely used LLMs and RAG systems, testing their ability to predict the correct fact-checking verdict across three task configurations: Zero-Context Prediction (given claims with no retrieved documents), Standard RAG (given claims with retrieved documents), and Oracle Retrieval (given claims with their associated documents). Our results reveal that current LLMs are highly vulnerable and lack robustness in real-world scenarios. Performance drops significantly across all configurations when using the RAGuard knowledge base. Notably, incorporating associated documents as context leads to an even steeper decline compared to dynamically retrieved documents, demonstrating how our dataset effectively assigns impactful misinformation to claims. This further exposes the limitations of LLMs in handling misleading content. Qualitative analysis shows that RAG systems are particularly susceptible to overtly misleading information, falling short of human reasoning. Figure [1](#fig_0) highlights the motivation behind our dataset and the susceptibility of current RAG systems to misleading retrievals.

In summary, our work advocates for a shift in focus from developing idealized RAG settings to those that better simulate real-world noisy information. We provide a benchmark to evaluate the robustness of RAG systems against misleading retrievals, addressing the gap in naturally-occurring misinformation RAG datasets. Our baseline results reveal the current shortcomings of RAG systems, showing performance worse than zero-shot. We expect our dataset to contribute to the development of more reliable and resilient RAG systems in the future.

## DATASET

We introduce RAGuard, a benchmark for evaluating the robustness of RAG systems in political fact-checking. RAGuard simulates noisy real-world retrieval settings, where systems must navigate supporting, misleading, and irrelevant evidence. The dataset comprises 2,648 political claims, corresponding fact-checking verdicts, and 16,331 associated documents labeled by their agreement with the verdicts.

## Definitions

The main task in RAGuard is retrieval-augmented fact-checking, where claims are verified as true or false based on retrieved documents that may support, mislead, or be irrelevant to the claim. Prior works employ varying terminology to describe the presence of such noise in retrieved contexts or retrieval corpora [[8,](#b7)[11,](#b10)[25,](#b24)[39]](#b38). To establish consistency, we define a structured taxonomy and align existing definitions (See Figure [2](#fig_1)).

Typical RAG datasets, including all prior fact-checking datasets to our knowledge, exclusively contain non-noisy supporting documents as associated evidence, leading to overly optimistic performance [[8]](#b7). Instead of relying solely on answer-containing documents, our dataset adopts a broader notion of supporting evidence. Specifically, we consider a document to be supporting if it provides information that enables an LLM to infer the correct answer, even if it does not explicitly state the ground-truth output. This reflects real-world fact-checking, where human verifiers rely on contextual information rather than single authoritative documents.

We categorize different types of noisy evidence based on whether the information directly conflicts with aspects of the correct prediction. As in prior work [[8,](#b7)[11]](#b10), we include non-conflicting documents in RAGuard, such as irrelevant texts that may hurt performance. However, our primary focus is conflicting documents, which include misleading, fabricated, and unambiguous evidence. Previous datasets primarily include conflicting evidence as fabricated or unambiguous documents, oversimplifying real-world complexity and ambiguity (see Section 2.2 for further discussion) [[11,](#b10)[25,](#b24)[39]](#b38). Notably, no prior work has introduced misleading documents.

In RAGuard, misleading documents distort facts through selective framing, omission, or biased presentation, leading the system toward incorrect predictions while still containing partial truths. Unlike fabricated evidence, which is explicitly engineered to contradict the correct prediction (i.e., adversarial perturbations), misleading evidence subtly misguides the model rather than directly opposing it. Additionally, while prior work such as QACC [[25]](#b24) introduces unambiguous evidence-a term we adopt to ensure consistency with past research-which includes some naturally conflicting evidence but only for a limited set of unambiguous questions, we focus on more natural yet scalable conflicting evidence.

For reference, we provide a list of all defined terms. Each term defines a type of document or piece of evidence.

(1) Associated: any document linked to a claim, regardless of label (2) Supporting: aids the system in producing a correct prediction through containing the correct answer explicitly or providing contextual support

Dataset Evidence Conflicting Real Domain Claims Retrieval Evidence World Fact-Checking FEVER [34] ‚úì ‚úó ‚úó General 185K FEVEROUS [2] ‚úì ‚úó ‚úó General 87K Liar [37] ‚úó ‚úó ‚úì Political 12.8K Mocheg [45] ‚úì ‚úó ‚úì Political 15.6K Snopes [17] ‚úì ‚úó ‚úì Political 6.4K PUBHEALTH [22] ‚úì ‚úó ‚úì Health 11.8K MultiFC [4] ‚úì ‚úó ‚úì Political 43.8K Noisy Contexts Power of Noise [8]

$‚úì ‚úó ‚úì General 10K RAAT [11] ‚úì ‚úì ‚úó General 7.8K NoiserBench [39] ‚úì ‚úì ‚úó General 4K QACC [25] ‚úì ‚úì ‚úì General 1.5K RAGuard ‚úì ‚úì ‚úì Political 2.6K$Table [1](#): Comparison of RAGuard with related fact-checking and RAG datasets. The columns indicate whether the dataset requires automatic evidence retrieval, contains conflicting evidence documents, and consists of naturally occurring realworld claims and evidence, as well as their domain and size.

(3) Noisy: challenges or disrupt system performance, thereby enhancing robustness (4) Conflicting: contradicts either the correct answer or some aspect of the prediction (5) Misleading: introduces factual distortions through selective framing, omission, or biased presentation; may contain partial truths (6) Fabricated: synthetically constructed to include factual errors (e.g., adversarial perturbations) (7) Unambiguous: naturally conflicting evidence but only for a limited set of unambiguous questions (special case of [[25]](#b24)) (8) Non-Conflicting: does not directly contradict the correct answer but still introduces noise by distracting the model (9) Irrelevant: does not contain specific enough information to determine the correct prediction, despite being topically or semantically related to the query (10) Random: unrelated; often introduced through random selection or artificial generation

## Comparison with Existing Datasets

Table [1](#) depicts a comparison of RAGuard with other fact-checking and RAG datasets.

Fact-Checking Datasets. All existing fact-checking datasets that include evidence retrieval contain only supporting documents. While FEVEROUS [[2]](#b1) labels some documents as refute, this terminology is misleading-these documents actually support the falsehood of the claim rather than providing conflicting evidence. Therefore, while FEVEROUS categorizes evidence into support for true claims and refute for false claims, it does not include documents that actively contradict the claim's verdict. Additionally, both FEVER [[34]](#b33) and FEVEROUS [[2]](#b1) rely on rewritten Wikipedia statements rather than naturally occurring claims, as noted by [[4]](#b3).

Liar [[37]](#b36) and Mocheg [[45]](#b44) are the most similar to our dataset since they also source claims from Politifact. However, Liar [[37]](#b36) does not support evidence retrieval as it lacks evidence documents. Mocheg [[45]](#b44) includes only documents cited by Politifact fact-checkers, Statistic Number Total Claims 2,648 True 1,333 (50.3%) False 1,315 (49.7%) Avg. Claim Length (words) 17.6 Total Documents 16,331 Supporting 2,685 (16.4%) Misleading 1,812 (11.1%) Irrelevant 11,834 (72.5%) Avg. Document Length (words) 161 Avg. Documents Per Claim 6.2 Claims with Supporting Docs 955 (36.1%) Claims with Misleading Docs 788 (29.8%) (a) Main statistics of RAGuard. Says The We I In I f J o h n B a ra ck Th er e Wh en Hillary Obama Preside nt Unde r Beca use For Mit t As Ove r Joe Sen . Sen ato r A At You Afte r By Illeg al It More "Joe Americ a During First, Last On Right Since Speaking Thanks Video According Gas George Health I'm ISIS Many McCain Oil "For "If "We "When Don ald H ill ar y the he Mitt th a t Jo e hi s Pre sid ent Barack Bernie Obama a Romney she Paul of Scott Sen. Ted in John Repu blican s as U n it e d O b a m a U .S . Tr um p fe de ra l nu m be r Re pu bli ca n av er ag e Bu sh he alt h las t Dem ocra ts Sen ate bord er cost fact five med ia med ian only presi dent presi dent' s price unem ploym ent vast ha ve are now don't got spend have was don't neve r did didn't want the Ne w 202 0, Chic ago, y o u th e we yo u'r e M c C a in McC ain' s O b a m a Ob am a's ar e is was have I Mitt President the Clinton says voted has Barack Obam a the Presid ent of th e Ro m ne y a gove rnor , th e Bi de n O ba m a Ob am a McC ain few lot th e can kno w, th e th e im mi gra tio n is tha n Bid en America Best First Official is do a re d o e s d id do es n' t w as ha s hav e don 't the don'tnot aren't didn't would can't isn'tDo were America aren't Are Is am didn't isn't should Did can't hasn't won't couldn't hasn't people is do are if would exactly was  which support the verdict rather than introducing conflicting or misleading evidence. Other datasets [[4,](#b3)[17,](#b16)[22]](#b21) primarily use journalistwritten explanations from fact-checking websites, which are structured to justify the verdict rather than reflect the complexity of real-world misinformation.

In contrast, RAGuard explicitly incorporates conflicting evidence, making it more representative of real-world misinformation challenges. Unlike curated fact-checking content, our dataset sources evidence from Reddit discussions, which naturally contain misleading information through diverse viewpoints. This increases the difficulty of RAG by better aligning ot with real-world misinformation.

Datasets with Noisy Contexts. Prior datasets that include noisy evidence primarily build on open-domain question answering (QA) datasets [[8,](#b7)[11,](#b10)[25,](#b24)[39]](#b38). As these datasets differ in how they define and introduce noise, we use our definition framework in Figure [2](#fig_1) to better distinguish each dataset.

Power of Noise [[8]](#b7) classifies evidence into gold, relevant, distracting, and random categories. Gold and relevant documents serve as supporting evidence, with gold documents being preexisting gold-standard documents and relevant documents being newly retrieved documents that explicitly contain the correct answer. Nonconflicting evidence includes distracting documents, which are simply non-gold retrievals and therefore irrelevant. Additionally, Power of Noise is the only dataset that introduces random evidence, which is entirely unrelated to the query. Notably, it does not include any conflicting evidence.

RAG-Bench [[11]](#b10) classifies evidence into golden context, irrelevant retrieval noise, relevant retrieval noise, and counterfactual retrieval noise. It constructs supporting evidence using gold-standard documents from non-noisy QA datasets. It introduces fabricated evidence by modifying documents to contain incorrect answers. While this results in factually incorrect conflicting evidence, it does not capture misleading evidence, which may contain partial truths but manipulates the information through selective framing or omission. Both relevant and irrelevant retrieval noise are considered irrelevant, as their retrieval via semantic search implies a degree of semantic relatedness to the query while not directly conflicting with the task content. Nonetheless, they can still distract RAG systems.

NoiserBench [[39]](#b38) introduces a wide range of types of noise, including inserting counterfactual noise. While this introduces conflicting evidence, all noise is artificially constructed, limiting its reflection of real-world misleading information. Like RAG-Bench, it focuses on fabricated evidence rather than capturing more complex distortions such as selective framing or omission.

QACC [[25]](#b24) employs human annotators to label retrieved documents as conflicting or non-conflicting with answers from Am-bigQA. Rather than artificially injecting errors, it includes conflicting evidence that directly contradicts the correct answer. However, its reliance on human annotation limits scalability, and its approach does not fully capture real-world ambiguity, as it focuses on clear-cut conflicts (i.e., questions labeled as "unambiguous" from AmbigQA) rather than more nuanced misleading evidence.

Unlike these datasets, our work focuses on the political domain, which presents distinct challenges. Political misinformation has tangible consequences, influencing public opinion, policy decisions, and elections [[37]](#b36). Misleading evidence in political discourse is often more nuanced, relying on selective framing rather than outright falsehoods. Furthermore, political fact-checking requires domainspecific reasoning, as claims are frequently shaped by ideological bias and rhetorical strategies.

## Dataset Structure

RAGuard consists of 2,648 political claims made by U.S. presidential candidates (2000-2024), each labeled as either true or false, and a knowledge base comprising 16,331 documents. The dataset's key statistics are presented in Table [3a](#). Each claim is linked to a set of associated documents, categorized as supporting, misleading, or irrelevant, with an average of 6.2 documents per claim. Notably, the dataset contains more supporting documents than misleading ones, reflecting that political discussions online are more often aligned with factual information. However, not every claim has both misleading and supporting documents, highlighting the imbalanced nature of political discourse, where certain narratives dominate while others lack counterpoints. The dataset is provided in two  Table 3: Example of data structure of documents.

## Supported Tasks

To benchmark the performance of current RAG systems in realworld fact-checking scenarios, we define a series of tasks using RAGuard. Each task evaluates a different aspect of RAG system robustness against misleading or conflicting contextual data.

Zero-Context Prediction. This task assesses a RAG system's ability to fact-check claims without external contextual information. The goal is to evaluate the intrinsic knowledge encoded in each model during pre-training and its effectiveness in verifying claims. This serves as a baseline to measure the impact of external retrieval on performance.

Standard RAG. This task simulates a real-time RAG system retrieving documents from the entire dataset corpus. The retrieved documents may include supporting, misleading, or irrelevant information to the claim, introducing retrieval noise. In extreme cases of poor retrieval, models may receive documents unrelated to the claim, mimicking real-world retrieval limitations.

Oracle Retrieval. This task provides RAG systems with the ssociated documents for each claim, isolating the impact of the associated documents labeled in our dataset. Unlike the previous task, where retrieval noise is unpredictable, this setting ensures systems receive only the supporting, misleading, and irrelevant documents associated with each claim. This setup evaluates how well models can filter out deceptive content when retrieval errors are controlled. We include two specific evaluations for this task. In the first, each model receives an associated document for a claim, regardless of the categorization as supporting, misleading, or irrelevant, to assess its ability to generalize to complex scenarios where documents have potential to support, mislead, or be irrelevant. In the second, we isolate only instances where the associated document is labeled as misleading, which conflicts with the claim's ground truth factchecking verdict, testing susceptibility to misleading information.

## DATASET CONSTRUCTION

We construct RAGuard in three stages, depicted in Figure [4](#fig_5). First, we collect political claims and fact-checking labels from PolitiFact, a reputable source for verified political fact-checking information. Next, we construct a knowledge base by retrieving relevant Reddit documents via a search engine, leveraging its diverse, real-time content to reflect real-world discourse. Finally, we introduce a novel, scalable LLM-guided approach to classify documents as misleading, supporting, or irrelevant by simulating the LLM taking an exam. The following sections outline the details of each stage.

## Claim and Verdict Collection

To collect claims and fact-checking verdicts for RAGuard, we scrape PolitiFact,[foot_1](#foot_1) a reputable platform where expert journalists assess the truthfulness of a wide range of political claims. We focus on claims made by major U.S. presidential candidates from 2000 to 2024 to ensure the inclusion of widely discussed statements that have been frequently fact-checked and debated, generating substantial online discourse and reflecting politically significant information. To facilitate document retrieval, we condense PolitiFact's six-point truth scale (true, mostly true, half true, mostly false, false, pants on fire) into binary labels-true and false, as it is challenging for a document to specifically mislead a half true verdict, undermining our core contribution.

## Knowledge Base Construction

To construct the RAGuard knowledge base, we employ a multi-step retrieval process that balances coverage, diversity, and realism. First,

Gemini 1.5 Flash GPT-4o Mini Claude 3.5 Sonnet Llama 3 Mistral Task 1: Zero-Context Prediction 61.06 67.33 74.51 62.50 63.97 Task 2: Standard RAG RAG-1 56.68 ‚Üì -4.38% / -7.2% 64.80 ‚Üì -2.53% / -3.8% 70.09 ‚Üì -4.42% / -5.9% 59.40 ‚Üì -3.10% / -5.0% 59.14 ‚Üì -4.83% / -7.5% RAG-5 57.59 ‚Üì -3.47% / -5.7% 65.90 ‚Üì -1.43% / -2.1% 68.58 ‚Üì -5.93% / -8.0% 61.37 ‚Üì -1.13% / -1.8% 58.91 ‚Üì -5.06% / -7.9% Task 3: Oracle Retrieval All Documents 52.38 ‚Üì -8.68% / -14.2% 53.22 ‚Üì -14.11% / -20.9% 51.17 ‚Üì -23.34% / -31.3% 61.09 ‚Üì -1.41% / -2.3% 51.61 ‚Üì -12.36% / -19.3% Misleading-Only 30.57 ‚Üì -30.49% / -49.9% 45.97 ‚Üì -21.36% / -31.7% 37.05 ‚Üì -37.46% / -50.3% 36.81 ‚Üì -25.69% / -41.1% 28.22 ‚Üì -35.75% / -55.9%

Table 4: Performance of various LLM backbones in RAG setup on three tasks, reported in Accuracy (%). The red numbers indicate the absolute/relative accuracy drop compared to Zero-Context Prediction (Task 1) under each setting.

GPT-4 extracts the keywords from each claim. This keyword expansion ensures a broader and more nuanced search space, increasing the likelihood of retrieving both corroborating and contradicting information. Next, we perform a keyword-based Google Search to retrieve up to ten relevant Reddit posts per claim. Google's ranking ensures contextual relevance, while Reddit's user-generated content introduces diverse perspectives, from speculative theories to well-supported arguments. Unlike curated fact-checking datasets, Reddit captures real-world discourse, including misinformation and conflicting viewpoints. This retrieval pipeline creates a realistic testbed for fact-checking, combining GPT-4-assisted keyword expansion with search engine retrieval to mirror the complexities of real-world misinformation challenges.

## LLM-Guided Document Annotation

Our work defines a document's role in a RAG system based on its influence on the LLM's decision-making. Unlike prior studies that introduce counterfactual evidence or rely on human annotators, our approach directly evaluates whether a document aids or misleads the LLM in real time. We achieve this by simulating a fact-checking scenario during annotation, treating the LLM as an exam taker.

To generate labels, we simulate the RAG fact-checking process at inference time. Given a claim (Section 3.1) and a retrieved document (Section 3.2), GPT-4 classifies the claim as true or false based on the document's content. If the classification aligns with the ground-truth, the document is labeled supporting; if it contradicts the gold label, it is misleading; and if it does not contribute to verification, it is irrelevant. By basing labels on the LLM's actual behavior, our approach ensures document annotations reflect their real impact on fact-checking, providing a scalable, empirically grounded alternative to human annotation.

## BASELINES 4.1 Experimental Setup

Evaluation. We frame fact-checking as a binary classification task where the model must generate a response that aligns with one of the predefined options. Accuracy, calculated with the gold label serving as the reference, is used to evaluate performance. If a model generates an out-of-scope response that does not match any of the given options, it is treated as an incorrect prediction.

Implementation Details. We evaluate RAG systems using both open-source and closed-source LLMs to assess their capabilities for real-world applications. For closed-source benchmarks, we test Google's Gemini 1.5 Flash [[14]](#b13), OpenAI's GPT-4o Mini [[28]](#b27), and Anthropic's Claude 3.5 Sonnet [[3]](#b2) via their respective APIs. For open-source benchmarks, we evaluate Meta's LLama3 8B Instruct [[9]](#b8) and Mistral's Mistral 7B Instruct [[18]](#b17) by running local inference.

In all settings, two-shot examples-one true and one false claim from RAGuard training data-are provided in the context. In the Standard RAG setting, we employ OpenAI's text-embedding-ada-002 for semantic search using the original claim as the query and provide the top one and five retrieved documents as context to the LLM. In the Oracle Retrieval setting, our system only takes one document at a time to find the impact of each document to the result. In our prompt, we specify that contextual documents may not be relevant or correct.

## Results

Table [4](#) displays baseline results on RAGuard for three tasks using two open and three closed-source LLMs.

Zero-Context Prediction. In the zero-context prediction, RAG systems operate without context documents from the RAGuard knowledge base. We use this as our baseline (a.k.a, zero-shot baseline). All systems achieve the highest accuracy scores, which is counterintuitive, considering this setting does not benefit from retrieval.

Standard RAG. Performance decreases for all models when integrating retrieval, with all scores falling below the zero-shot baseline. The decline is consistent across both RAG-1 and RAG-5 settings, though the magnitude varies. GPT-4 remains the most robust, exhibiting only a minor drop, while other models, particularly Mistral and Gemini, experience more pronounced declines. Increasing retrieval (RAG-1 to RAG-5) does not consistently improve performance and sometimes worsens it. This suggests that retrieval introduces both useful and misleading information, and when retrieval quality is not optimal, the additional context can confuse rather than help. These findings challenge the assumption that retrieval always benefits downstream performance and reinforce prior research [[8,](#b7)[31,](#b30)[41,](#b40)[46]](#b45) on the risks of noisy retrieval in high-stakes tasks.

Oracle Retrieval. The results in the Oracle Retrieval setting reveal a striking trend: incorporating associated documents from RAGuard leads to a significant drop in performance compared to the zero-context baseline. This suggests that models are highly sensitive to misleading or irrelevant information. This trend holds across all models except Llama3, indicating that randomly retrieved irrelevant documents are less harmful than misleading content. This finding underscores the challenge posed by RAGuard, which systematically tests model robustness misleading information.

In this task's All Documents setting, where models receive all associated documents for a claim, performance generally declines, suggesting that models struggle to reconcile conflicting evidence. The impact is even more severe in the Misleading-Only setting, where models are provided only with misleading documents that contradict the claim's ground truth. Most models falling to around 30% accuracy despite the binary nature of the task. This confirms that models are highly susceptible to misleading information and struggle to distinguish factual content from misinformation, highlighting LLM limitations in handling misleading evidence.  

## DISCUSSION

Comparison of Model Robustness. Figure [5](#fig_6) displays the relative performance decreases across different models. Notably, Claude 3.5 Sonnet, which achieved the highest accuracy in the zero-context baseline, experienced the greatest decreases when exposed to noisy retrieval. In the All Documents Oracle Retrieval setting, Claude suffered one of the steepest declines, and its performance dropped even more drastically in the Misleading-Only condition. This suggests that while Claude performs well in ideal conditions, it is particularly susceptible to misleading evidence, struggling to filter out incorrect information when retrieval introduces contradictory or noisy context. This may also be due to its high baseline performance.

Conversely, GPT-4o Mini demonstrated the highest robustness against misleading evidence. Its relative performance drop in the Misleading-Only setting was 31.7%, significantly lower than the approximate 50% declines observed for Gemini, Claude, and Mistral. This suggests that GPT-4o Mini is better at handling misleading content and maintaining accuracy in noisy retrieval conditions, highlighting differences in model sensitivity to retrieval-induced noise.

Effect of RAG on Accuracy. Figure [5](#fig_6) challenges the common assumption that RAG consistently enhances model performance. When retrieval introduces misleading or irrelevant evidence, it actively degrades accuracy, often leading to worse performance than zero-shot baselines. This is particularly evident in the Misleading-Only condition from Task 3, where each model is intentionally provided with documents contradicting the claim's ground truth. Across all models, accuracy drops significantly, with an average performance decrease of 45.8%, highlighting the substantial risks posed by retrieval-induced misinformation. These findings emphasize that retrieval, when not carefully controlled, can be detrimental rather than beneficial, further reinforcing the importance of robust filtering and sufficiency evaluation in RAG systems.

Retrieval Performance. Retrieval performance is a standard metric in RAG benchmarks, but our dataset focuses on how models handle misleading or conflicting evidence. High retrieval accuracy alone does not ensure reliable answers due to misleading information in the corpus. Nonetheless, to To provide a full view of system behavior, we report both conventional retrieval metrics and a tailored measurement called Misleading Retrieval Recall.

Figure [6](#fig_7) shows Retrieval Precision, Recall, and Normalized Discounted Cumulative Gain (NDCG) for Task 2 (Standard RAG). Recall naturally rises with ùêæ, while precision decreases. NDCG follows a non-monotonic trend, dipping around ùêæ = 10 before recovering due to relevant items being unevenly distributed across ranked positions, causing reordering as ùêæ changes.

We also report Misleading Retrieval Recall-the fraction of claims retrieving at least one misleading document. Task 1 (Zero-Context) scores 0%, while Task 3 (Oracle Retrieval) is 100%. In Task 2, RAG-1 scores 21.3%, increasing to 44.8% for RAG-5, showing a higher risk of retrieving misleading content when retrieving more documents. As seen in Table [4](#), this correlates with lower overall accuracy.

Qualitative Example. Figure [7](#fig_8) presents example system predictions on RAGuard, illustrating the impact of misleading documents. The left example highlights how misleading documents negatively affect the classification of a true claim. While misleading documents generally degrade system performance compared to zero-shot predictions, their specific influence varies based on their complexity. We distinguish three categories of misleading documents:

(1) Overtly Misleading Document: This category includes documents that are evidently misleading to humans but still lead to incorrect predictions by all RAG systems. For example, in Figure [7](#fig_8), the document falsely comparing California's job growth to the national average misleads all systems (1b), despite their correct zero-shot predictions (1a). This suggests a form of selective bias, where the systems prioritize the provided information simply because it is included in the prompt, even though the instructions explicitly caution against assuming its correctness. (2) Partially True Misleading Document: These documents contain partial truths, making it necessary to apply reasoning to recognize their misleading nature. For example, as shown in Figure [7](#fig_8), one document criticizes unemployment but also states that "official job reports are reporting jobs added" (1c). While this statement supports the claim that 500,000 jobs were added, the document's overall tone suggests rising unemployment. However, this suggestion is more of an opinion than a fact. Some LLMs, such as GPT-4 and Mistral, were able to reason through this contradiction and classify the claim correctly. (3) Challenging Misleading Document: These documents present significant challenges, even for human annotators. For example, a claim referencing job growth in the 2000s is incorrectly classified because the RAG system retrieves data from 2024, which accurately reports lower job creation (1d). The temporal misalignment in retrieved documents presents a fundamental challenge in this dataset and task.

The middle example demonstrates GPT-4's ability to filter out noise from retrieved documents that are not associated with the claim but could be considered misleading documents in our dataset (e.g., documents using the same phrasing but referring to different individuals, such as "Harris" instead of "Clinton" in example 2b). Even when five irrelevant documents are retrieved (2c), GPT-4 remains robust. However, when presented with a misleading document from the dataset (2d), GPT-4 fails, reinforcing the dataset's effectiveness in challenging model performance beyond conventional RAG noise. This further explains the lower accuracy observed in the Oracle Retrieval setting in our baseline experiments.

The right example shows how GPT-4 tends to assign disproportionate weight to misleading documents, allowing them to override even non-associated supporting evidence. In the example, a nonassociated document that contains supporting information (3b) enables GPT-4 to correct its initially incorrect zero-shot prediction (3a). However, when a misleading document is retrieved alongside other non-associated supporting documents (3c), the system incorrectly classifies the claim, similar to its behavior when only the misleading document is retrieved (3d). This demonstrates that misleading documents can have a stronger influence on the model's classification, regardless of the presence of supporting evidence, highlighting a significant vulnerability in RAG systems.

These examples highlight three key findings: (1) LLMs remain highly susceptible to misleading documents, even when their content is transparently incorrect, (2) misleading documents retrieved from the dataset exert a stronger influence than non-associated documents retrieved erroneously, and (3) when misleading documents are present, they can significantly outweigh supporting evidence, leading to incorrect predictions. These findings emphasize the strength and uniqueness of our dataset in evaluating and challenging RAG-based model performance.

## RELATED WORK

Retrieval-Augmented Generation with Noisy Contexts. Retrieval-Augmented Language Models (RALMs) have demonstrated strong performance across various NLP tasks [[16,](#b15)[24]](#b23). However, their effectiveness is constrained by the retriever's ability to find supporting information. In real-world applications, retrieval often introduces irrelevant or misleading content, which can significantly degrade model performance [[8,](#b7)[31,](#b30)[41,](#b40)[46]](#b45). Prior work has identified two primary effects of such noise.

The first is the impact of irrelevant documents on RAG performance [[7]](#b6). To make RAG systems more robust to this issue, researchers have explored several strategies, including prompting the language model to generate a rationale connecting retrieved documents to the query [[38]](#b37), employing multi-agent debate systems to identify the most relevant information [[36]](#b35), and aggregating multiple documents to produce a more reliable final response [[40]](#b39).

The second challenge arises when retrieved documents conflict with an LLM's internal knowledge [[21]](#b20). To address this, AstuteRAG introduced an iterative system that consolidates internal and external knowledge, reducing inconsistencies in generated responses [[35]](#b34). However, this study focuses on disagreements between documents in existing datasets, which do not intentionally mislead RAG systems and do not represent conflicting information in the real world.

Researchers have increasingly attempted to improve RAG robustness through developing datasets that expose the model to conflicting contexts [[8,](#b7)[25]](#b24) and retrieval noises [[39]](#b38) that may later be used for adversarial training [[11]](#b10). However, existing datasets fail to fully capture the complexities of real-world misinformation.

In contrast, RAGuard is the first to capture misleading context that reflects real-world ambiguities, polarized opinions, and partial truths. Unlike prior datasets that primarily focus on synthetic conflicts or document-model disagreements, RAGuard captures naturally occurring misinformation, making it a more realistic benchmark for evaluating RAG robustness.

The Limitations of Open-Book QA Datasets. RAG is studied primarily through open-book question answering (QA), where models answer questions based on retrieved knowledge [[13]](#b12). However, most Open-Book QA datasets carefully curate their documents, avoiding noisy information [[5,](#b4)[10,](#b9)[19,](#b18)[23,](#b22)[26,](#b25)[30,](#b29)[44,](#b43)[47]](#b46). This leads to strong performance in controlled settings, but poor generalization in real-world scenarios, where conflicting documents often degrade performance [[8]](#b7).

Some datasets attempt to address this issue by synthetically introducing counterfactual information [[11,](#b10)[39]](#b38). However, synthetic noise may not fully capture the complexities of real-world misinformation. Others rely on human annotators to identify conflicting documents [[25]](#b24), but this approach is costly and difficult to scale. [[8]](#b7) classifies retrieved documents as distracting based on their equivalence to a gold-standard document, but this approach may not reflect the existence of truly deceptive or contradictory information.

In contrast to these approaches, we leverage real-world misinformation by using political fact-checking data, which contains misleading and conflicting information. This allows us to construct a dataset that better reflects the challenges RAG systems face.

Fact-Checking and RAG. Fact-checking is a well-studied task with datasets sourced from platforms like Twitter [[27]](#b26), Wikipedia [[34]](#b33), and PolitiFact [[1,](#b0)[29,](#b28)[32,](#b31)[37,](#b36)[45]](#b44). Given that fact-checking often relies on external evidence, it aligns well with RAG, where retrieval can support the verification or negation of a claim. However, existing fact-checking datasets typically use gold-standard evidence from the same source as the verdict [[4,](#b3)[17,](#b16)[22,](#b21)[34,](#b33)[45]](#b44), meaning there is no exposure to noisy or contradictory evidence. Despite the inherently polarizing nature of online and political discourse, the retrieved evidence in these datasets rarely contradicts the final verdict. Our work introduces noisy and conflicting information into fact-checking datasets, ensuring that retrieved evidence is not always aligned with the verdict.

## CONCLUSION

In this paper, we introduce RAGuard, a challenging and diverse fact-checking dataset designed to assess the robustness of RAG systems against misleading retrievals. RAGuard comprises 2,648 claims-1,333 true and 1,315 false-paired with 16,331 documents, averaging 6.2 documents per claim. These documents are labeled using a novel LLM-guided approach that simulates an exam-like evaluation, analyzing how the model processes and interprets retrieved evidence at inference time to determine whether the documents support, mislead, or are irrelevant to the claim. Unlike prior RAG benchmarks that rely on synthetically noisy data or curated gold-standard documents, RAGuard utilizes real-world evidence, even in cases where no gold-standard documents exist. By incorporating naturally occurring misleading data from Reddit discussions alongside verified evidence and claims from PolitiFact, it mirrors the complexities of real-world misinformation, which is necessary for more robust systems.

Our findings show that the performance of current RAG systems deteriorates significantly when exposed to misleading evidence, challenging the assumption that retrieval always enhances model accuracy. These results highlight the need for more resilient factchecking pipelines. Future research should focus on enhancing retrieval robustness through methods such as adversarial retrieval training, which exposes models to misleading evidence during training to improve resilience, and uncertainty-aware retrieval, which prioritizes evidence credibility over mere relevance. Additionally, fact-verification mechanisms that incorporate multi-step reasoning and cross-document consistency checks can mitigate the impact of misleading sources, while confidence calibration techniques may further refine the model's ability to discern factual inconsistencies.

By providing a challenging yet realistic benchmark, RAGuard encourages the development of more sophisticated retrieval-based fact-checking methodologies. We hope this dataset will facilitate progress in designing retrieval pipelines that are not only effective but also resistant to misinformation, ultimately contributing to more reliable and trustworthy AI systems.

![Figure 1: Example of a false claim initially classified correctly but later misclassified as true due to misleading retrievedcontext, alongside the ideal human judgment.]()

![Figure 2: Taxonomy of terminology to classify different types of evidence, labeled with prior works' contributions (left), and our dataset's composition (right).]()

![election the your going on deficit employment federal salary the Trump'in / a a 7 (c) Word distribution for documents.]()

![Figure 3: Key statistics and word distributions of RAGuard.]()

![Figure 4: RAGuard dataset construction, consisting of three stages to obtain claims and verdicts; associated documents; and labels for the each document's relationship to the claim and verdict.]()

![Figure 5: Performance decreases from the Zero-Context baseline to Task 2 and 3 when using RAGuard across various models. Results are measured in Accuracy and include relative percent decreases.]()

![Figure 6: Retrieval Accuracy, Recall, and NDCG at Different Top K Levels]()

![Figure 7: Example predictions on RAGuard, compared to the expected human response. Note that each column compares different prediction scenarios based on varying retrieved contexts for the same claim rather than a multi-turn process. Left: Each system's classification of a true claim with three progressively misleading documents. Middle: GPT-4-based system's classification of a false claim with one noisy non-associated document, many noisy non-associated documents, and a misleading document. Right: GPT-4-based system's classification of a true claim with a supporting non-associated document, one misleading document along with other supporting non-associated documents, and a misleading document.]()

![Example of data structure of claims.]()

The dataset is available at https://huggingface.co/datasets/UCSC-IRKM/RAGuard.

https://www.politifact.com/

