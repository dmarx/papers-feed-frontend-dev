# Monarch: Expressive Structured Matrices for Efficient and Accurate Training

## Abstract

## 

Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute/memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency-quality tradeoffs, and (2) in denseto-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2× with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called "reverse sparsification," Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2× without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7× with comparable accuracy.

## Introduction

Large neural networks excel in many domains, but their training and fine-tuning demand extensive computation and memory [[54]](#b53). A natural approach to mitigate this cost is to replace dense weight matrices with structured ones, such as sparse & low-rank matrices and the Fourier transform. However, structured matrices (which can be viewed as a general form of sparsity) have not yet seen wide adoption to date, due to two main challenges. [(1)](#b0) In the end-to-end (E2E) training setting, they have shown unfavorable efficiency-quality tradeoffs. Model efficiency refers how efficient these structured matrices are on modern hardware (e.g., GPUs). Model quality (performance on tasks) is determined by how expressive they are (e.g., can they represent commonly used transforms such as convolution or Fourier/cosine transforms that encode domain-specific knowledge). Existing structured matrices are either not hardware-efficient, or not expressive enough. [(2)](#b1) In the setting of dense-to-sparse (D2S) fine-tuning of pretrained models, a long-standing problem for most classes of structured matrices is the lack of tractable algorithms to approximate dense pretrained weight matrices [[79]](#b78). Sparse matrices have seen advances in training deep learning models (e.g., pruning [[44]](#b43), lottery tickets [[30]](#b29)), but most work on (entrywise) sparsification focuses on reducing training or inference FLOPs, which do not necessarily map to E2E training time on modern hardware (e.g., GPUs). In fact, most sparse training methods slow down training in wall-clock time [[33,](#b32)[48]](#b47). Moreover, sparse matrices are not able to represent commonly used transforms such as convolution and the Fourier transform. Another class of structured matrices, such as Fourier, sine/cosine, Chebyshev, are used in specialized domains such as PDE solving [[100]](#b99) and medical imaging [[49]](#b48). However, they are difficult to use in E2E training since only specific instances of these structured matrices have fast GPU implementations (e.g., FFT). Moreover, their applications requires domain expertise to hand-pick the right transforms. Generalizations of these transforms (e.g., Toeplitz-like [[95]](#b94), orthogonal polynomial transforms [[25]](#b24), low-displacement rank [[53]](#b52), quasi-separable [[27]](#b26)), though learnable, often lack efficient implementation on GPUs [[98]](#b97) for E2E training as well. In addition, they have no known tractable algorithm to approximate a given dense matrix [[79]](#b78), making them difficult to use in D2S fine-tuning.

E2E training. The technical challenge in addressing the efficiency-quality tradeoff of structured matrices is to find a parameterization that is both efficient on block-oriented hardware (e.g., GPUs) and expressive (e.g., can represent many commonly used transforms). We propose a class of matrices called Monarch,[foot_0](#foot_0) parameterized as products of two block-diagonal matrices (up to permutation), to address this challenge. This parameterization leverages optimized batch-matrix-multiply (BMM) routines on GPUs, yielding up to 2× speedup compared to dense matrix multiply (Section 5.1.1). We show that the class of Monarch matrices contains the class of butterfly matrices [[80,](#b79)[12]](#b11), which can represent any low-depth arithmetic circuits in near optimal runtime and parameter size [[13]](#b12). Monarch matrices inherit this expressiveness and thus can represent many fast transforms (e.g., Fourier, sine/cosine/Chebyshev transforms, convolution) (Proposition 3.2).

Sparse-to-dense (S2D) training, aka "reverse sparsification". The hardware-efficiency and expressiveness of Monarch matrices unlock a new way to train dense models: training with Monarch weight matrices for most of the time and then transitioning to dense weight matrices (Fig. [3](#fig_3)). This technique can be used in cases where sparse training faces representation or optimization difficulties [[28]](#b27) or a dense model is necessary. One such application is language modeling on large datasets, where a massive number of parameters are required [[54]](#b53) to memorize the textual patterns [[35]](#b34). Monarch matrices can serve as a fast intermediate representation to speed up the training process of the dense model.

D2S fine-tuning. While transitioning from sparse to dense matrices is easy, the reverse direction is challenging. The main technical difficulty is the projection problem: finding a matrix in a class of structured matrices that is the closest to a given dense matrix. Only a few specific classes of structured matrices have a tractable projection solution, such as entrywise sparse matrices (magnitude pruning [[97]](#b96)), low-rank matrices (the Eckart-Young theorem [[26]](#b25)), and orthogonal matrices (the orthogonal Procrustes problem [[93]](#b92)). For more expressive classes of structured matrices, projection remains a long-standing problem [[79]](#b78). For example, De Sa et al. [[16]](#b15) show that all structured matrices (in the form of arithmetic circuits) can be written as products of sparse matrices, which can be represented as products of butterfly matrices [[13]](#b12). There have been numerous heuristics proposed to project on the set of butterfly matrices or products of sparse matrices, based on iterative first-order optimization [[63,](#b62)[12,](#b11)[55]](#b54) or alternating minimization [[67]](#b66). However, they lack theoretical guarantees. In contrast, we derive a projection algorithm for our Monarch parameterization and prove that it finds the optimal solution (Theorem 1). We also derive an algorithm to factorize matrices that are products of Monarch matrices (Section 3.4). These new algorithms allows us to easily finetune a pretrained model into a model with Monarch weight matrices (Section 5.3).

We validate our approach empirically in these three settings, showing that our Monarch matrix parameterization achieves a favorable efficiency-accuracy tradeoff compared to baselines on a wide range of domains: text, images, PDEs, MRI. • In the E2E sparse training setting (Section 5.1), our Monarch matrices model trains 2× faster than dense models while achieving the same accuracy / perplexity on benchmark tasks (ViT on ImageNet classification, GPT-2 on Wikitext-103 language modeling). On scientific and medical tasks relying on hand-crafted fast transforms (PDE solving, MRI reconstruction), Monarch reduces the error by up to 40% at the same training speed compared to domain-specific Fourier-based methods. • In the S2D training setting (Section 5.2), our "reverse sparsification" process with Monarch matrices speeds up GPT-2 pretraining on the large OpenWebText dataset by 2× compared to an optimized implementation from NVIDIA [[94]](#b93), with comparable upstream and downstream (text classification) quality.

When applied to BERT pretraining, our method is 23% faster than the implementation from Nvidia that set the MLPerf [[72]](#b71) 1.1 record. • In the D2S fine-tuning setting (Section 5.3), we show a proof of concept that our Monarch projection algorithm speeds up BERT fine-tuning. We project a pretrained BERT model to a Monarch matrix model and fine-tune on GLUE, with 2× fewer parameters, 1.7× faster fine-tuning speed, and similar average GLUE accuracy as the dense model. [2](#foot_1)2 Related Work and Background

## Related Work

Sparse Training. Sparse training is an active research topic. There has been inspiring work along the line of compressing models such as neural network pruning and lottery tickets [[44,](#b43)[45,](#b44)[30]](#b29). Pruning methods usually eliminate neurons and connections through iterative retraining [[44,](#b43)[45,](#b44)[92]](#b91) or at runtime [[66,](#b65)[23]](#b22).

Although both Monarch and pruning methods aim to produce sparse models, we differ in our emphasis on overall efficiency, whereas pruning mostly focuses on inference efficiency and disregards the cost of finding the smaller model. Lottery tickets [[30,](#b29)[31,](#b30)[32]](#b31) are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks in convergence speed and potentially in generalization.

Monarch can be roughly seen as a class of manually constructed lottery tickets. Structured Matrices. Structured matrices are those with subquadratic (o(n 2 ) for dimension n × n) number of parameters and runtime. Examples include sparse and low-rank matrices, and fast transforms (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). They are commonly used to replace the dense weight matrices of deep learning models, thus reducing the number of parameters and training/inference FLOPs. Large classes of structured matrices (e.g., Toeplitz-like [[95]](#b94), low-displacement rank [[53]](#b52), quasiseparable [[27]](#b26)) have been shown to be able to represent many commonly used fast transforms. For example, De Sa et al. [[16]](#b15) show that a simple divide-and-conquer scheme leads to a fast algorithm for a large class of structured matrices. Our work builds on butterfly matrices [[80,](#b79)[12]](#b11), which have been shown to be expressive but remain hardware-inefficient. Pixelated butterfly [[6]](#b5) has attempted to make butterfly matrices more hardware-friendly, but at the cost of reduced expressiveness. Furthermore, it is not known if one can directly decompose a dense pretrained model to a model with butterfly weight matrices without retraining.

## Butterfly Matrices

Our work builds on recent work on butterfly matrices. Dao et al. [[12]](#b11) introduced the notion of a butterfly matrix as a certain product of permuted block-diagonal matrices, inspired by the Cooley-Tukey fast Fourier transform algorithm [[11]](#b10). They encode the divide-and-conquer structure of many fast multiplication algorithms. Dao et al. [[13]](#b12) showed that all structured matrices can be written as products of such butterfly matrices, and this representation has optimal memory and runtime complexity up to polylogarithmic factors. We now review these definitions (following [[13]](#b12)).

A butterfly factor of size k (where k is even) is a matrix of the form

$D 1 D 2 D 3 D 4 where each D i is a k 2 × k 2 diagonal matrix.$We call this class of matrices BF (k,k) . A butterfly factor matrix of size n and block size k is a block diagonal matrix of n k butterfly factors of size k:

$diag B 1 , B 2 , . . . , B n k ,$where B i ∈ BF (k,k) . We call this class of matrices BF (n,k) . Finally, a butterfly matrix of size n = 2 s is a matrix M that can be expressed as a product of butterfly factor matrices:

$M = B n B n/2 . . . B 2 ,$where each B i ∈ BF (n,i) . We denote the set of size-n butterfly matrices by B (n) . Equivalently, M can be written in the following form:

$M = B n M 1 0 0 M 2 ,$where B n ∈ BF (n,n) and M 1 , M 2 ∈ B ( n 2 ) . Dao et al. [[13]](#b12) further introduce the kaleidoscope matrix hierarchy: the class BB * (n) is the set of matrices of the form n) , and the class (BB * (n) ) w e is the set of all matrices of the form n) . (A * denotes the conjugate transpose of A.) When the size n is clear from context, we will omit the superscript (n) (i.e., just write B, BB * , etc.). As shown by Theorem 1 of Dao et al. [[13]](#b12), the kaleidoscope hierarchy can represent any structured matrix with nearly-optimal parameters and runtime: if M is an n × n matrix such that multiplying any vector v by M can be represented as a linear arithmetic circuit with depth d and s total gates, then M ∈ (BB * (n) )

$M 1 M * 2 for M 1 , M 2 ∈ B ($$w i=1 M i [1:n, 1:n] where each M i ∈ BB * (e•$$O(d) O(s/n) .$
## Monarch: Definition & Algorithms

In Section 3.1, we introduce Monarch matrices, and describe how they relate to butterfly matrices. In Section 3.2 we show that the class of Monarch matrices is at least as expressive as the class of butterfly matrices, while admitting a practically efficient representation. In particular, many fast transforms (e.g., Fourier, convolution) can be represented as a Monarch matrix or as the product of two or four Monarch matrices (Proposition 3.2). In Section 3.3, we show how to project onto the set of Monarch matrices. This allows us to tractably approximate a given matrix (e.g., a dense pretrained weight matrix) with a Monarch matrix, unlocking new applications (cf. Section 5). In Section 3.4, we show how to recover the individual factors of the larger class of products of two Monarch matrices.

## Monarch Parametrization for Square Matrices

Inspired by the 4-step FFT algorithm [[3]](#b2), we propose the class of Monarch matrices, each parametrized as the product of two block-diagonal matrices up to permutation:

$Definition 3.1. Let n = m 2 .$An n × n Monarch matrix has the form:

$M = PLP R,$where L and R are block-diagonal matrices, each with m blocks of size m × m, and P is the permutation that maps [x 1 , .

. . , x n ] to [x 1 , x 1+m , . . . , x 1+(m-1)m , x 2 , x 2+m , . . . , x 2+(m-1)m , . . . , x m , x 2m , . . . , x n ]. We call this the Monarch parametrization. We denote the class of all matrices that can be written in this form as M (n) (dropping the superscript when clear from context). Fig. [2](#fig_1) illustrates this parametrization.

We now provide more intuition for this parametrization and connect it to butterfly matrices. For ease of exposition, suppose B ∈ B (n) where n is a power of 4. Then let L be obtained by multiplying together the first log The matrix L can also be written as block-diagonal with the same structure as R after permuting the rows and columns. Specifically, let P be the permutation of Definition 3.1. We can interpret P as follows: it reshapes the vector x of size n as a matrix of size m × m, transposes the matrix, then converts back into a vector of size n. Note that P = P . Then we can write L = PL P , where L = diag(L 1 , . . . , L m ).

Hence, up to permuting rows and columns, L is also a block-diagonal matrix of m dense blocks, each of size m × m.

Thus we can write B = PLP R, where L, R, and P are as in Definition 3.1. So, B ∈ B (n) implies that B ∈ M (n) .

Products of Monarch Matrices. Another important class of matrices (due to their expressiveness, cf. Proposition 3.2) is the class MM * : matrices that can be written as M 1 M * 2 for some M 1 , M 2 ∈ M. Further, (MM * ) 2 denotes the class of matrices that can be written

$M 1 M 2 for M 1 , M 2 ∈ MM * .$Extension to Rectangular Matrices. In practice, we also want a way to parametrize rectangular weight matrices, and to increase the number of parameters of Monarch matrices to fit different applications (analogous to the rank parameter in low-rank matrices and the number of nonzeros in sparse matrices). We make the simple choice to increase the block size of the block-diagonal matrices in the Monarch parametrization, and to allow rectangular blocks. More details are in Appendix C.

## Expressiveness and Efficiency

We remark on the expressiveness of Monarch matrices and their products (ability to represent many structured transforms), and on their computational and memory efficiency.

## Expressiveness

As described in Section 3.1, any matrix B ∈ B (n) can be written in the Monarch butterfly representation, by simply condensing the log 2 n total factors into two matrices. Thus, the Monarch butterfly representation is strictly more general than the original butterfly representation (as there also exist matrices in M (n) but not B (n) ). In other words, for a given size n, M ⊃ B; similarly MM * ⊃ BB * . In particular, Dao et al. [[13]](#b12) showed that the following matrix classes are contained in BB * , which implies they are in MM * as well: Proposition 3.2. The matrix class MM * can represent convolution, Hadamard transform, Toeplitz matrices [[37]](#b36), and AFDF matrices [[74]](#b73). The matrix class (MM * ) 2 can represent the Fourier transform, discrete sine and cosine transforms (DST/DCT), the (HD) 3 [106] class, Fastfood [[62]](#b61), and ACDC matrices [[74]](#b73).

## Efficiency

Parameters. A Monarch matrix M = PLP R is described by 2n √ n parameters: L, R both have √ n dense blocks of size √ n × √ n, for a total parameter count of n √ n each. The permutation P is fixed, and thus doesn't add any parameters. Speed. To multiply by M, we need to multiply by a block diagonal matrix R, permute, multiply by a block diagonal matrix L, and finally permute. All four of these steps can be implemented efficiently. The total number of FLOPs is O(n √ n), which is more the O(n log n) for a butterfly matrix. However, since we can leverage efficient block-diagonal multiplication (e.g., batch matrix multiply), Monarch multiplication is easy to implement and is fast in practice (2x faster than dense multiply, cf. Section 5).

## Projection on the Set M of Monarch Matrices

Given our class of structured matrices, a natural question is the projection problem: finding a Monarch matrix that is the closest to a given dense matrix. We show that this problem has an analytical optimal solution, and show how to compute it efficiently. This allows us to project dense models to Monarch models, enabling D2S fine-tuning (Section 5.3).

We formalize the problem: for a given matrix A, find

$argmin M∈M A -M 2 F .(1)$Even though this problem is nonconvex (as M is parametrized as the product of two matrices), in Theorem 1 we show that there exists an analytical solution (full proof in Appendix D). This is analogous to the Eckart-Young theorem that establishes that optimal low-rank approximation is obtained from the SVD [[26]](#b25).

Theorem 1. Given an n×n matrix A, there is an O(n 5/2 )-time algorithm that optimally solves the projection problem (1), and returns the Monarch factors L and R.

We now derive this algorithm (Algorithm 1) by examining the structure of a Monarch matrix M. We first rewrite the steps of Monarch matrix-vector multiplication (i.e., computing Mx). The main idea is to view the input x, which is a vector of size n = m 2 , as a 2D tensor of size m × m. Then the two matrices L and R in the Monarch parametrization M = PLP R correspond to batched matrix multiply along one dimension of x, followed by batched matrix multiply along the other dimension of x. Thus we view x as a 2D tensor of size m × m, and each of L and R as a 3D tensor of size m × m × m.

Steps to multiply x by a Monarch matrix M = PLP R: 1. Multiply R by x: y kj = i R kji x ki , to obtain an output y that is a 2D tensor of size m × m. 2. Multiply PLP by y: z j = k L j k y kj , to obtain an output that is a 2D tensor of size m × m. 3. Reshape z back into a vector of size n, and return this. We can thus write the output z as z j = k,i L j k R kji x ki . Since M = PLP R, we can write:

$M jki = L j k R kji .(2)$Note that here we view M as a 4D tensor of size m × m × m × m.

When viewed as a 4D tensor, the structure of the matrix M becomes apparent, and the solution to the projection problem is easy to see. Let's examine Eq. ( [2](#formula_11)): M jki = L j k R kji . We see that this reshaped tensor version of M is simply m • m batches of rank-1 matrices: we batch over the dimensions k and j, and each batch is simply a rank-1 matrix (p jk )(q jk ) for some length-m vectors p jk , q jk . Therefore, the projection objective (Eq. ( [1](#formula_10))) can be broken up into the sum of m • m independent terms, each term corresponding to a block of A of size m × m. As the structure of a Monarch matrix forces each block to have rank 1 as described above, the solution to the projection problem becomes apparent: given a matrix A, reshape it to a 4D tensor of size m × m × m × m, and take the rank-1 approximation of each batch with the SVD, which (after reshaping) yields the factors L, R of the desired matrix M ∈ M. (Note that if A ∈ M itself, this algorithm recovers the factors such that A = PLP R.)

## Algorithm 1 Projection on the set of Monarch matrices

$Require: Matrix A ∈ R n×n , with n = m 2 . Reshape A into a 4D tensor A of size m × m × m × m, where A jki = A ( -1)m+j,(k-1)m+i for , j, k, i = 1, . . . , m. for 1 ≤ j, k ≤ m do Let M jk = A :,j,k,: of size m × m.$Compute the best rank-1 approximation of M jk as u jk v jk with the SVD of A. end for Let R be the m × m × m tensor where R kji = (v jk ) i . Let L be the m × m × m tensor where L j k = (u jk ) . Return L, R as block-diagonal matrices L, R (where the b th block of L, R are L b,:,: , R b,:,: respectively)

## Factorization of MM * Matrices

In the previous section, we saw how to project onto the set M. As Theorem 3.2 shows, the broader class MM * also encompasses many important linear transforms. In this section, we present an algorithm to compute the Monarch factorization of a given matrix M ∈ MM * , under mild assumptions. This allows us to store and apply M efficiently.

Specifically, observe that if To understand how to do this, define M = P MP and observe that M 

$M ∈ MM * , we can write M = (PLP R)(R * PL * P ) = (PL 1 P )R(PL 2 P ) for block-diagonal L 1 , L 2 , R$$= L 1 (PRP )L 2 =    A1 A2 . . . Am       D11 D12 . . . D1m D21 D22 . . . D2m . . . . . . . . . . . . Dm1 Dm2 . . . Dmm       C1 C2 . . . Cm   $$M ij is equal to A i D ij C j .$Notice that M is invertible only if all the A i 's and C j 's are (since if any one of these is singular, then L 1 or L 2 is singular).

Thus, our goal is to find matrices Â1 , . . . , Âm , Ĉ1 , . . . , Ĉm and diagonal matrices D11 , . . . , Dmm such that M ij = Âi Dij Ĉj for all i, j; this represents a valid Monarch factorization of M.

To provide intuition for how to do this, let's analyze a simple case in which all the D ij 's are the identity matrix. Then we have the set of equations A i C j = M ij . Again assume the A i 's and C j 's are invertible, so each M ij is as well. Suppose we set Ĉ1 = I (identity matrix). Then we can immediately read off Âi = M i1 for all i. We can then set Ĉj = Â-1

1 M 1j for all j. Let's now check that this strategy gives a valid factorization, i.e., that M ij = Âi Ĉj for all i, j. We have Âi Ĉj = M i1 M -1 11 M 1j . Recalling that in the "true" factorization we have

$M ij = A i C j , this equals (A i C 1 )(A 1 C 1 ) -1 (A 1 C j ) = A i C j , as desired.$In the general case, we must deal with the diagonal D ij matrices as well. We will no longer be able to freely set Ĉ1 = I. However, once we find a proper choice of Ĉ1 , we can use it to find all the Âi 's and Ĉj 's. We can find such a Ĉ1 via the idea of simultaneous diagonalization; for space reasons, we defer a full description of our algorithm (Algorithm 2), and its analysis, to Appendix D.

## Using Monarch Matrices in Model Training

We can use our class of Monarch matrices to parameterize weight matrices of deep learning models in several settings.

• In the E2E sparse training setting, we replace the dense weight matrices of a baseline model with Monarch matrices with the same dimension, initialize them randomly, and train as usual. Most of our baseline models are Transformers, and we replace the projection matrices in the attention blocks, along with the weights of the feed-forward network (FFN) blocks, with Monarch matrices. The Monarch parameterization is differentiable, and we rely on autodifferentiation to train with first-order methods such as Adam [[57]](#b56). • In the S2D training setting, we first replace the dense weight matrices of a baseline model with Monarch matrices, then train the sparse model for about 90% of the usual number of iterations. We then convert the Monarch matrices to dense matrices (by simply multiplying the factors L and R along with permutations), and continue training for the remaining 10% of the iterations. Compared to dense end-to-end training, we train for the same number of iterations, but the first 90% of the iterations are faster due to the hardware efficiency of Monarch matrices. • In the D2S fine-tuning setting, we start with a dense pretrained model (e.g., BERT), and project the dense weight matrices (e.g., in the attention blocks and FFN blocks) on the set of Monarch matrices using the algorithm in Section 3.3. We then fine-tune the resulting model on downstream tasks (e.g., GLUE), using first-order methods. We typically set the number of blocks in the block-diagonal matrices to be between 2 and 4 based on the parameter budgets (25% -50% of the dense model).

## Experiments

We validate our approach empirically, showing that our Monarch matrix parametrization achieves a favorable efficiency-accuracy tradeoff compared to baselines on a wide range of domains (text, images, PDEs, MRI), in three settings (E2E training, S2D training, and D2S fine-tuning):

• In Section 5.1.1, on image classification and language modeling benchmarks, such as ViT / MLP Mixer on ImageNet and GPT-2 on Wikitext-103, Monarch is 2× faster to train than dense models, while achieving the same accuracy / perplexity. In Section 5.1.2, in scientific and medical domains where special transforms (Fourier) are common, Monarch outperforms Fourier transform based methods on PDE solving, with up to 40% lower error, and on MRI reconstruction attains up to 15% higher pSNR and 3.8% higher SSIM.

• In Section 5.1.2, we show that on the large OpenWebText dataset, reverse sparsification (training with Monarch weight matrices for most of the time, then transitioning to dense weight matrices) speeds up the pretraining of GPT-2 models by 2× compared to the dense model, with no loss in upstream or downstream quality. Moreover, reverse sparsification speeds up BERT pretraining by 23% even compared to the implementation from Nvidia that set the MLPerf [72] 1.1 record. • In Section 5.3, as a proof of concept, we demonstrate that our Monarch approximation algorithm can improve fine-tuning efficiency for pretrained models. We show that compressing BERT to a Monarch matrix model performs comparably to a finetuned dense model on GLUE, with 2× fewer parameters and 1.7× faster finetuning speed. 5.1 End-to-End Training 5.1.1 Benchmark Tasks: Image Classification, Language Modeling We show that replacing dense matrices with Monarch matrices in ViT, MLP-Mixer, and GPT-2 can speed up training by up to 2× without sacrificing model quality in Tables [1](#tab_2) and [2](#tab_3).

Setup. We use the popular vision benchmark, ImageNet [[17]](#b16). We choose recent popular Vision Transformer [[24]](#b23), and MLP-Mixer [[99]](#b98) as representative base dense models. For language modeling, we evaluate GPT-2 [[86]](#b85) on WikiText-103 [[73]](#b72).  

## PDE solving and multi-coil MRI reconstruction

Many scientific or medical imaging tasks rely on specialized transforms such as the Fourier transform. We show that replacing the fixed Fourier transform with the more expressive Monarch matrices yields higher model quality (lower reconstruction error) with comparable model speed.

Solving PDEs with Monarch Neural Operators. We follow the experimental setting in FNO [[65]](#b64) and apply a Monarch-based neural operator to the task of solving the Navier-Stokes PDE. Compared to baseline U-Nets [[90]](#b89), TF-Nets [[103]](#b102), ResNets [[47]](#b46) and FNOs [[65]](#b64), neural operators based on Monarch improve solution accuracy across spatial resolutions by up to 40% (Table [3](#)).

Non-periodic boundary conditions. Traditional spectral methods based on Fourier transform work best with periodic boundary conditions and forcing terms. However, PDEs of practical interest often exhibit non-periodic or even unknown boundary conditions. Monarch operators are not constrained to the Fourier transform and can thus still learn the solution operator with excellent accuracy.

Table 3: Benchmarks on Navier-Stokes (fixing resolution 64 × 64 for both training and testing). Decreasing the viscosity coefficient ν makes the dynamics more chaotic.

$Model v = 10 -3 v = 10 -4 v = 10$-5 U-Net 0.025 0.205 0.198 TF-Net 0.023 0.225 0.227 ResNet 0.070 0.287 0.275 FNO 0.017 0.178 0.155 Monarch-NO 0.010 0.145 0.136

Accelerated MRI Reconstruction. We characterize the utility of Monarch-based FFT operations for accelerated MRI reconstruction, a task which requires methods with both structured Fourier operators and dealiasing properties to recover high quality images. On the clinically-acquired 3D MRI SKM-TEA dataset [[20]](#b19), Monarch-SENSE (mSENSE) enhances image quality by over 1.5dB pSNR and 2.5% SSIM compared to zero-filled SENSE and up to 4.4dB and 3.8% SSIM compared to U-Net baselines in data-limited settings. Setup details are available in Appendix E.5.

Expressive FFT. By definition, standard IFFT in zero-filled SENSE cannot dealias the signal, resulting in artifacts in the reconstructed image. mSENSE replaces the inverse FFT (IFFT) operation in standard SENSE with learnable Monarch matrices. Thus, mSENSE preserves the structure of the Fourier transform while learning to reweight frequencies to suppress aliasing artifacts. Across multiple accelerations, mSENSE achieved up to +1.5dB and 2.5% improvement in peak signal-to-noise ratio (pSNR) and structural similarity (SSIM), respectively (Table [4](#tab_6)). Data Efficiency. While CNNs have shown promise for MRI reconstruction tasks, training these networks requires extensive amounts of labeled data to avoid overfitting. However, large data corpora are difficult to acquire in practice. mSENSE can be trained efficiently with limited supervised examples. In few shot settings, mSENSE can outperform U-Net by +4.4dB (≈15%) and 3.8% SSIM (Table [5](#tab_7)).  in 2× less time. We also evaluate its downstream quality on zero-shot generation from [[34]](#b33) and classification tasks from [[108]](#b107), achieving comparable performance to the dense counterparts (Table [6](#tab_8)). In Fig. [5](#fig_8) BERT pretraining. On the Wikipedia + BookCorpus datasets [[110]](#b109), we train a BERT-large model with Monarch weight matrices for 70% of the time and transition to dense weight matrices for the remaining 30% of the time, which yields the same pretraining loss as conventional dense training. In Table [7](#), we compare the total training time to several baseline implementations: the widely-used implementation from HuggingFace [[104]](#b103), the more optimized implementation from Megatron [[94]](#b93), and the most optimized implementation we know of from Nvidia that was used to set MLPerf 1.1 training speed record. Our method is 3.5x faster than HuggingFace and 23% faster than Nvidia's MLPerf 1.1 implementation [3](#foot_2) . Experiment details are in Appendix E.4.

## Dense-to-Sparse Fine-tuning

We show that our Monarch approximation algorithm allows us to efficiently use pretrained models, such as speeding up BERT finetuning on GLUE.

BERT finetuning. We take the BERT pretrained weights, approximate them with Monarch matrices, and finetune the resulting model on the 9 GLUE tasks. The results in Table [8](#tab_9) shows that we obtain a Monarch finetuned model with similar quality to the dense BERT model, but with 1.7× faster finetuning speed. This   

## A Extended Related Work

In this section, we extend the related works referenced in the main paper and discuss them in detail.

Sparse Training. Our work is loosely related to neural network pruning. By iteratively eliminating neurons and connections, pruning has seen great success in compressing complex models. Han et al. [[44,](#b43)[45]](#b44) put forth two naive but effective algorithms to compress models up to 49x and maintain comparable accuracy. Li et al. [[64]](#b63) employ filter pruning to reduce the cost of running convolution models up to 38 %, Lin et al. [[66]](#b65) prunes the network at runtime, hence retaining the flexibility of the full model. Dong et al. [[23]](#b22) prunes the network locally in a layer by layer manner. Sanh et al. [[92]](#b91) prunes with deterministic first-order information, which is more adaptive to pretrained model weights. Lagunas et al. [[60]](#b59) prunes transformers models with block sparsity pattern during fine-tuning, which leads to real hardware speed up while maintaining the accuracy.

Zhu & Gupta [[109]](#b108) finds large pruned sparse network consistently outperform the small dense networks with the same compute and memory footprints. Although both our and all the pruning methods are aiming to produce sparse models, we differ in our emphasis on the overall efficiency, whereas pruning mostly focuses on inference efficiency and disregards the cost in finding the smaller model. There has been more recent work on sparse methods that focuses on speeding up training and not just inference, such as SNFS [[21]](#b20), RigL [[21]](#b20), Top-KAST [[50]](#b49). These methods often focus on FLOP counts, which may not correlate well with wall-clock time on modern hardware (e.g., GPUs). Block-sparsity is another approach that exploits the block-oriented nature of GPUs [[38,](#b37)[7,](#b6)[41]](#b40). Sparse models have also been found useful to improve the training process of dense models. For example, sparsity can be used to regularize dense models to improve accuracy [[46]](#b45), or to alternate between sparse and dense training to ease deployment [[82]](#b81). Our sparse-to-dense reverse sparsification instead focuses on speeding up dense training, where the sparse model is used for efficiency and not regularization.

In addition, models proposed in our work can be roughly seen as a class of manually constructed lottery tickets. Lottery tickets Frankle & Carbin [[30]](#b29) are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks in convergence speed and potentially in generalization. A huge number of studies are carried out to analyze these tickets both empirically and theoretically: Morcos et al. [[75]](#b74) proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with the specialized lottery tickets; Frankle et al. [[31]](#b30) improves the stability of the lottery tickets by iterative pruning; Frankle et al. [[32]](#b31) found that subnetworks reach full accuracy only if they are stable against SGD noise during training; Orseau et al. [[78]](#b77) provides a logarithmic upper bound for the number of parameters it takes for the optimal sub-networks to exist; Pensia et al. [[81]](#b80) suggests a way to construct the lottery ticket by solving the subset sum problem and it's a proof by construction for the strong lottery ticket hypothesis. Furthermore, follow-up works [[68,](#b67)[102,](#b101)[96]](#b95) show that we can find tickets without any training labels.

Structured matrices and butterfly matrices. Structured matrices are those with asymptotically fast matrix-vector multiplication algorithm (o(n 2 ) time complexity) and few parameters (o(n 2 ) space complexity). Common examples include sparse & low-rank matrices, and fast transforms such as Fourier transform, Chebyshev transform, Legendre transform, and more generally orthogonal polynomial transforms. These transforms have been widely used in data preprocessing (e.g., DFT in speech processing [[52]](#b51)) and kernel approximation [[62,](#b61)[106]](#b105). Many generalizations of these transforms have been used in machine learning to replace dense weight matrices [[95,](#b94)[98,](#b97)[40]](#b39). De Sa et al. [[16]](#b15) shows that any structured matrix (in the form of arithmetic circuits) can be written as product of sparse matrices, and Dao et al. [[13]](#b12) shows that products of butterfly matrices can represent these structured matrices almost optimally in terms of runtime and memory. The class of butterfly matrices [[80]](#b79) have also been used in kernel models [[76,](#b75)[8]](#b7) and deep learning models [[101,](#b100)[67,](#b66)[1]](#b0).

Neural Operators for PDEs. Deep learning has found application in the domain of differential equations and scientific computing [[85]](#b84), with methods developed for prediction and control problems [[56,](#b55)[71]](#b70), as well as acceleration of numerical schemes [[83,](#b82)[51]](#b50). Specific to the partial differential equations (PDEs) are approaches designed to learn solution operators [[87,](#b86)[29,](#b28)[65]](#b64), and hybridized solvers [[59]](#b58), evaluated primarily on classical fluid dynamics.

The promise of these approaches is to offer, at the cost of an initial training procedure, accurate yet faster solutions than an appropriate numerical method tuned for a specific problem, which can then be leveraged for real-time forecasting or within larger feedback loops. Nonetheless, optimal design of neural operators remains an open problem, with most relying on fast Fourier transforms (FFT) or standard dense neural architectures. Instead, neural operators based on Monarch are capable of approximating all fast transforms, thus allowing automated optimization towards a suitable transform on a given PDE problem.

MRI. Accelerated multi-coil MRI is an essential mechanism for reducing long scan times and making certain scan types feasible. In multi-coil MRI, data is acquired in the spatial Fourier domain (a.k.a k-space) across multiple coils (sensors). To reduce scan time, this data is sampled below the required rate for recovering the underlying signal (i.e. Nyquist rate), which results in signal aliasing (see Appendix E.5). In these settings, direct application of the inverse fast Fourier transform (FFT) cannot suppress aliasing artifacts.

Classical MRI reconstruction approaches supplement the FFT by leveraging shared information across multiple coils and strong analytical priors to regularize image recovery objectives. SENSE-based methods jointly dealias images across multiple coils and reweight the final image based on the spatial sensitivity profile of each coil [[84]](#b83). Compressed sensing promotes image sparsity in transformation domains (e.g. Fourier, wavelet) while enforcing data consistency between the Fourier transform of the reconstructed image and the observed measurements [[69]](#b68). Low-rank methods enforce low rank structure across slowly-varying dimensions or local patches in the data [[77,](#b76)[89,](#b88)[42]](#b41). Additionally, GRAPPA-based techniques optimize kernels to directly interpolate missing k-space samples to promote smoothness in the Fourier domain [[39]](#b38). Despite their efficacy, these methods have long reconstruction times, require explicit analytical priors, and require careful hyperparameter fine-tuning.

CNNs have shown promise as a fast-at-inference, learnable alternative to classical MRI reconstruction methods [[58]](#b57). In supervised learning, fully convolutional networks (e.g. U-Net [[90]](#b89) or unrolled networks [[91,](#b90)[43]](#b42)) learn a mapping between paired zero-filled and fully-sampled, ground truth images. However, supervised methods require a large fully-sampled (labeled) data corpus and are sensitive to distribution drifts due to patient, hardware, and sequence heterogeneity [[15]](#b14). To reduce dependence on labeled data, unsupervised methods have used generative adversarial networks [[9,](#b8)[70]](#b69), self-supervised learning [[105]](#b104), dictionary learning [[61]](#b60), and untrained networks [[14]](#b13). Despite their label efficiency, these techniques still underperform supervised methods and are also sensitive to distribution shift. Recently, a family of semi-supervised reconstruction methods demonstrated label efficiency and robustness to physics-driven perturbations, such as changes in signalto-noise ratio or patient motion [[19,](#b18)[18]](#b17). However, these methods require large amounts of unlabeled data, which can be difficult to curate in few-shot settings. Thus, despite their success in controlled environments, prospective clinical deployment of these models has been stifled [[5]](#b4).

In our work, we propose a model with a single FFT-initialized factorized Monarch matrix. Such a matrix can provide the benefits of both a simple linearized transformation like FFT and a learnable mechanism to remove aliasing artifacts resulting from the undersampled k-space. The smaller learnable parameter set may reduce overfitting in data-limited settings while preserving the transformation structure of Fourier matrices. Thus, our approach can be interpreted as a hybrid between analytically-constrained classical methods and data-dependent CNNs.

## B Notation Review

Throughout this paper, we use lowercase to denote scalars (e.g., k), lowercase boldface to denote vectors (e.g., v), and uppercase boldface to denote matrices (e.g., A).

I denotes the identity matrix. We use A to denote the transpose of a matrix and A * to denote the conjugate transpose of a matrix. All results in this paper apply to matrices over the either the reals R or the complex numbers C; when the field under consideration can be either one of these, we denote it by F.

We use 1-indexing throughout this paper except where explicitly stated.

## C General Monarch Matrix Parametrization

In Section C.1, we define a parametrization for square Monarch matrices of different "block sizes" (i.e., not necessarily √ n), and prove some basic properties about them. In Section C.2, we further extend this to define rectangular Monarch matrices, and prove some basic properties about them.

Note: In this section, we use 0-indexing rather than 1-indexing, for notational convenience.

C.1 General square matrices

$C.1.$
## Parametrization

In this section, we define a more general Monarch parametrization for square matrices, allowing for different "block sizes." Like Definition 3.1, the parametrization involves the product of a permuted block-diagonal matrix with another block-diagonal matrix; the difference is that we now allow the matrices L and R to have diagonal blocks of different sizes. Thus, the permutations applied to L (to turn it into a block matrix where each block matrix is diagonal) will correspondingly also be different. First, in Definition C.1, we define notation for a class of block-diagonal matrices.

Definition C.1 (Class BD (b,n) ). Let b ∈ (1, n) be an integer that divides n. For 0 ≤ i < n b , let R i ∈ F b×b be a b × b "block" matrix. Then define the matrix R with block size b as follows:

$R = diag R 0 , . . . , R n b -1 . (3$$) (Note that the number of possible nonzero values in R is n b • b 2 = nb.)$We denote the class of all matrices R expressible in this form by BD (b,n) . Note that this class is closed under (conjugate) transposition and contains the identity matrix.

Next, in Definition C.2, we define notation for a class of block matrices whose blocks are diagonal.

Definition C.2 (Class DB (b,n) ). Let b ∈ (1, n) be an integer that divides n. For 0 ≤ i, j < b, let D i,j ∈ F b×b be a b × b diagonal matrix. Then let L be an n × n matrix with the following form:

$L =    D 0,0 . . . D 0, n b -1 . . . . . . . . . D n b -1,0 . . . D n b -1, n b -1   (4)$(Note that the number of possible nonzero values in

$L is n b 2 • b = n 2 b .)$We denote the class of all matrices L expressible in this form by DB (b,n) . Note that this class is closed under (conjugate) transposition and contains the identity matrix. As we show in Appendix C.1.2, L can be written as a block-diagonal matrix with b blocks of size n b × n b (i.e., a matrix in BD ( n b , n) ), multiplied on the left and right with appropriate permutation matrices. We denote the class of all matrices L expressible in this form by DB (b,n) . Note that this class is closed under (conjugate) transposition. As we show in Appendix C.1.2, L can be written as a block-diagonal matrix with b blocks of size n b × n b (i.e., a matrix in BD ( n b , n) ), multiplied on the left and right with appropriate permutation matrices.

Using these two definitions, we define the class of Monarch matrices with a given block size.

Definition C.3 (Class M (b,n) ). Let b ∈ (1, n) be an integer that divides n. A Monarch matrix of size n × n and "block size b" is a matrix of the form:

$M = LR(5)$where L ∈ DB (b,n) and R ∈ BD (b,n) .

We denote the class of all matrices M expressible in this form by M (b,n) . Observe that when b = √ n, this is exactly the matrix class M (n) in Definition 3.1. (In other words, M (n) is shorthand for M ( √ n,n) .) Note that a matrix in M (b,n) is represented by n 2 b + nb parameters. We remark that M (b,n) ⊃ B (n) for all block sizes b ∈ (1, n) that divide n. Based on Definition C.19, we define the classes MM * (b,n) and M * M (b,n) :: n) . We define MM * (b,n) to be the the class of all matrices M expressible in the form M = M 1 M * 2 . We define M * M (b,n) to be the the class of all matrices M expressible in the form

$Definition C.4 (Class MM * (b,n) , M * M (b,n) ). Let b ∈ (1, n) be an integer that divides n and suppose M 1 , M 2 ∈ M (b,$$M = M * 1 M 2 .$Observe that when b = √ n, MM * (b,n) is exactly the matrix class MM * (n) defined in Section 3. Note that a matrix in MM * (b,n) or M * M (b,n) . is represented by 2 n 2 b + 2nb parameters. Finally, we define the following "Monarch hierarchy" based on the kaleidoscope hierarchy of [[13]](#b12):

$Definition C.5 (Class (MM * (b,n) ) w e )$. Let b ∈ (1, n) be an integer that divides n. We define the matrix class (MM * (b,n) ) w e as the set of all matrices M that can be expressed as

$M =   w i=1 M i   [1 : n, 1 : n](6)$where each M i ∈ MM * (b,e•n) .

Note that a matrix in (MM * (b,n) ) w e is represented by 2w e 2 n 2 b + 2wenb parameters.

## C.1.2 Properties

Here we show some properties of the matrix classes defined above. We first show some basic equivalent ways to define these classes. We then show (Theorem 3) that the matrices in DB (b,n) are permuted blockdiagonal matrices; specifically, that they can be converted to matrices in BD ( n b ,n) by applying the appropriate permutation. Finally, we state an expressivity result for the general "Monarch hierarchy" which follows from Theorem 1 of [[13]](#b12).

First, we define a class of permutations. Let 1 ≤ b ≤ n be integers such that b divides n. We will need to express each index 0 ≤ i < n in "block form." More specifically: Definition C.6. Let i ≥ 0, b ≥ 1 be integers. Then define i 0 = i mod b, and

$i 1 = i b .$We use the notation i ≡ (i 1 , i 0 ) b to denote the representation above. In particular, if i ≡ (i 1 , i 0 ) b , then we have

$i = i 1 • b + i 0$Using this notation, we define the following class of permutations:

$Definition C.7. Let b ∈ [1, n] be an integer that divides n. Let i ≡ (i 1 , i 0 ) b . Define σ (b,n) (i) = i 0 • n b + i 1 .(7)$That is,

$σ (b,n) (i) ≡ (i 0 , i 1 ) n b$. Let P (b,n) denote the n × n permutation matrix defined by the permutation σ (b,n) .

Intuitively, P (b,n) can be interpreted as reshaping a length-n vector into an b × n b matrix in row-major order, transposing the result, and then flattening this back into a vector (again in row-major order). Now, we restate the formulation in Definition C.1 equivalently as:

Proposition C.8. A matrix R satisfies Equation (3) (i.e., R ∈ BD (b,n) ) if and only if the following holds for any 0 ≤ i, j < n. Let i ≡ (i 1 , i 0 ) b and j ≡ (j 1 , j 0 ) b . Then 2. Else (i.e., when

$i 1 = j 1 ), then R[i, j] = R i1 [i 0 , j 0 ].$We restate the formulation in Definition C.2 equivalently as:

Proposition C.9. A matrix L satisfies Equation (4) (i.e., L ∈ DB (b,n) ) if and only if the following holds for any 0 ≤ i, j < n. Let i ≡ (i 1 , i 0 ) b and j ≡ (j 1 , j 0 ) b . Then

$1. if i 0 = j 0 , then L[i, j] = 0.$2. Else, (i.e., when i 0 = j 0 ), then

$L[i, j] = D i1,j1 [i 0 , i 0 ].$We will argue the following:

$Theorem 3. Let 1 ≤ b ≤ n such that b divides n.$Recall that P (b,n) is the permutation matrix defined by the permutation σ (b,n) . Let L be a matrix in DB (b,n) . Then we have

$R = P (b,n) • L • P (b,n) ,$where R ∈ BD ( n b , n) .

Proof. We first note that multiplying an n × n matrix on the right (and left resp.) by P (b,n) = P ( n b ,n) (and P (b,n) resp.) permutes the columns (and rows resp.) of the matrix according to σ (b,n) . [4](#foot_4) This implies that for any 0 ≤ i, j < n:

$R [σ (b,n) (i), σ (b,n) (j)] = L[i, j].(8)$To complete the proof, we will argue that R satisfies the two conditions in Proposition C.8. Towards this end, let 0 ≤ i, j < n be arbitrary indices and further, define i

$= (i 1 , i 0 ) b and j = (j 1 , j 0 ) b . Then note that σ (b,n) (i) = (i 0 , i 1 ) n b and σ (b,n) (j) = (j 0 , j 1 ) n b .$By Proposition C.9, we have that if i 0 = j 0 , then L[i, j] = 0. Note that i 0 = j 0 satisfies the pre-condition for base size n b for indices (σ (b,n) (i), σ (b,n) (j)) in item 1 in Proposition C.8. Then by Eq. ( [8](#formula_37)), we have that R [σ (b,n) (i), σ (b,n) (j)] = 0, which satisfies item 1 in Proposition C.8. Now consider the case that i 0 = j 0 ; then by item 2 in Proposition C.9, we have that L[i, j] = D i1,j1 [i 0 , i 0 ]. Note that i 0 = j 0 satisfies the pre-condition for base size n b for indices (σ (b,n

$) (i), σ (b,n) (j)) in item 2 in Proposition C.8 if we define R i0 ∈ F n b × n b as follows: R i0 [i 1 , j 1 ] = D i1,j1 [i 0 , i 0 ]. Note that the above implies that R = diag R 0 , . . . , R b-1 ,$where R • is as defined in the above paragraph. This means R ∈ BD

$( n b ,n) , since each block R i0 is a matrix of size n b × n b .$We now briefly note some alternate ways to express matrices in MM * (b,n) .

Proposition C.10. For any M ∈ MM * (b,n) , we can write M = (P (b,n) n) . Notice that since R * 1 , R 2 are both block-diagonal with the same structure (i.e., both have blocks of size b × b), their product R is also in BD (b,n) . Also, by Theorem 3 we can write

$L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 ∈ BD ( n b ,n) and R ∈ BD (b,n) . Proof. By definition (see Definition C.1 and Definition C.2), if M ∈ MM * (b,n) , we can write M = (L 1 R 1 )(L 2 R 2 ) * = L 1 (R * 1 R 2 )L * 2 , where L 1 , L 2 ∈ DB (b,n) , R 1 , R 2 ∈ BD (b,$$L 1 = P (b,n) L 1 P (b,n) , L 2 = P (b,n) L 2 P (b,n) , where L 1 , L 2 are both in BD ( n b ,n) (i.e., block diagonal with blocks of size n b × n b ). Thus, we can write M = (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 ∈ BD ( n b ,n) and R ∈ BD (b,n) .$least n 2 b ≥ n 3/2 free parameters (the entries in the blocks of the block-diagonal matrix P (b,n) LP (b,n) can be arbitrary, and there are b such blocks each of size n b ). Similarly, in the case b > √ n, we can set L to the identity, and M has at least nb ≥ n 3/2 free parameters (the entries of the block-diagonal matrix R can be arbitrary, and there are nb total of these). Thus, at least n 3/2 parameters are required to uniquely describe any matrix in M (b,n) . However, a butterfly matrix in B (n) has only 2n log 2 n parameters. For n > 256, 2n log 2 n < n 3/2 . (Note that this analysis is not tight: a more careful analysis can show the inclusion is strict even for smaller values of n.)

We end this section with a theorem on the expressivity of the "monarch hierarchy" (products of monarch matrices), which follows from Theorem 1 of [[13]](#b12).

Theorem 5 (Monarch hierarchy expressivity). Let M be an n×n matrix such that matrix-vector multiplication of M and an arbitrary vector v (i.e., computation of Mv) can be represented as a linear arithmetic circuit with depth d and s total gates. Let b ∈ (1, n) be a power of 2 that divides n. Then, M ∈ (MM * (b,n) )

$O(d) O(s/n) .$Proof. Theorem 1 of Dao et al. [[13]](#b12) says that if n is a power of 2 and A is an n × n matrix such that multiplying any vector v by A can be represented as a linear arithmetic circuit with depth ≤ d and ≤ s total gates, then A ∈ (BB * (n) )

$O(d)$O(s/n) (this is the "kaleidoscope representation" of A). Recall from Theorem 4 that for any b ∈ (1, n) that is a power of 2 and divides n, M (b,n) ⊃ B (n) ; thus, this implies MM * (b,e•n) ⊃ BB * (e•n) , and in turn (MM * (b,n) ) w e ⊃ (BB

$* (n) ) w e . As A ∈ (BB * (n) ) O(d) O(s/n) , we thus have A ∈ (MM * (b,n) ) O(d) O(s/n) .$As per [[13]](#b12), the class of kaleidoscope matrices (BB * (n) )

$O(d)$O(s/n) has O(ds log s) parameters and runtime, compared to the O(s) parameters and runtime of the circuit. Note that at worst, s is O(n 2 ).

Define f (n, s) to be the largest power of 2 that is 

$≤ min n 2 , √ s . Note that f (n, s) = O( √ s), and since s = O(n 2 ), f (n, s) = Ω( √ s), so f (n, s) = Θ( √ s). We thus have A ∈ (MM * (f (n,s),n) ) O(d)$
## C.2 General rectangular matrices

In this section, we extend the Monarch parametrization to apply to rectangular matrices, and prove some basic properties of the relevant matrix classes. (Note that our subsequent theoretical results (Appendix D) do not depend on this section, as they focus on the square parametrization.) For the rest of the section, we will assume that n 1 , n 2 , n 3 , b 1 , b 2 , b 3 ≥ 1 are integers such that:

• b i divides n i for all 1 ≤ i ≤ 3, and

$• n1 b1 = n2 b2 .$We begin with the definition of the following class of rectangular block-diagonal matrices:

$Definition C.14. For 0 ≤ i < n b1 , let R i ∈ F b2×b1 be a b 2 × b 1 matrix. Then define the matrix R ∈ F n2×n1 as follows: R = diag R 0 , . . . , R n 1 b 1 -1 .(9)$We say that R has block size b 2 × b 1 . Recall that we have assumed n1 b1 = n2 b2 , so Eq. ( [9](#formula_49)) is well-defined. (Note that the number of possible nonzero values in R is

$n1 b1 • b 1 × b 2 = n 1 b 2 .$) We denote the class of all matrices R expressible in this form by BD (b2×b1,n2×n1) . Note that this class is only defined when n1 b1 = n2 n2 . We restate the above definition equivalently as:

$Proposition C.15. R ∈ F n2×n1 is in BD (b2×b1,n2×n1) (with n1 b1 = n2 n2$) if and only if the following holds for any 0 ≤ i < n 2 and 0 ≤ j < n 1 . Let i ≡ (i 1 , i 0 ) b2 and j ≡ (j 1 , j 0 ) b1 (recalling this notation from Definition C.6. Then 1. if i 1 = j 1 , then R[i, j] = 0.

## Else (i.e., when i

$1 = j 1 ), then R[i, j] = R i1 [i 0 , j 0 ].$Before we define the rectangular L, we first need to define the notion of a 'wrapped diagonal' matrix: We now define the following class of block matrices with each block a wrapped diagonal matrix.

Definition C.17. Let L ∈ F n3×n2 have the form:

$L =     S 0,0 . . . S 0, n 2 b 2 -1 . . . . . . . . . S n 3 b 3 -1,0 . . . S n 3 b 3 -1, n 2 b 2 -1     ,(10)$where each S •,• is a wrapped diagonal matrix in F b3×b2 .

We say that L has block size b 3 × b 2 . (Note that the number of possible nonzero values in L is

$n2 b2 • n3 b3 max(b 2 , b 3 ) = n2•n3 min(b2,b3$) .) We denote the class of all matrices L expressible in this form by DB (b3×b2,n3×n2) .

We restate the above definition equivalently as: n3×n2) if and only if the following holds for any 0 ≤ i < n 3 and 0 ≤ j < n 2 . Let i ≡ (i 1 , i 0 ) b3 and j ≡ (j 1 , j 0 ) b2 . Assuming b 2 ≤ b 3 , we have: 

$Proposition C.18. L ∈ F n3×n2 is in DB (b3×b2,$$1. if i 0 mod b 2 = j 0 , then L[i, j] = 0.$$M = LR(11)$where L ∈ DB (b3×b2,n3×n2) and R ∈ BD (b2×b1,n2×n1) .

(As mentioned before, we assume b i divides n i for i = 1, 2, 3 and that n 1 /b 1 = n 2 /b 2 .) We denote the class of all matrices M expressible in this form by M ((b1,b2,b3),(n1,n2,n3)) . Observe that when b 1 = b 2 = b 3 = b and n 1 = n 2 = n 3 = n, this is exactly the matrix class M (b,n) in Definition C. [19](#b18).

We are now ready to prove our main result in this section, which essentially follows from the observation that if we permute the rows and columns of L such that the row/column block size in L becomes the number of row/columns blocks in the permuted matrix (and vice-versa) then the permuted matrix has the form of R. Theorem 6. Let 1 ≤ b, n 2 , n 3 be such that b divides n 2 and n 3 . Suppose L ∈ F n3×n2 ∈ DB (b×b,n3×n2) . Then if we define

$R = P (b,n3) • L • P (b,n2) , we have that R ∈ BD ( n 3 b 3 × n 2 b 2 ,n3×n2) .$Proof. We recall that multiplying an m × n matrix on the right (and left resp.) by P (b,n) = P ( n b ,n) (and P (b,m) resp.) permutes the columns (and rows resp.) of the matrix according to σ (b,n) (and σ (b,m) ) respectively. [5](#foot_5)This implies that for any 0 ≤ i, j < n:

$R [σ (b,n3) (i), σ (b,n2) (j)] = L[i, j].(12)$Since the squared Frobenius norm objective A -M 2 F (Eq. ( [1](#formula_10))) only depends on the entries of A and M and not their shape, we can rewrite the objective after reshaping:

$A -M 2 F = jki (A jki -M jki ) 2 = jki (A jki -L j k R kji ) 2 = jk i (A jki -L j k R kji ) 2 .$We see that the objective decomposes into m × m independent terms (indexed by j and k). For each value of j and k, the objective is exactly the rank-1 approximation objective for the corresponding slice A :,j,k,: . Let u jk v jk be the best rank-1 approximation of A :,j,k,: (which we can compute using the SVD, by the Eckart-Young theorem [[26]](#b25) for Frobenius norm). Let R be the 3D tensor of size m × m × m where R kji = (v jk ) i , and let L be the 3D tensor of size m × m × m where L j k = (u jk ) . Then each of the terms in the objective is minimized, and thus the overall objective is minimized.

We see that the algorithm requires m • m SVD's, each of size m × m. Each SVD takes O(m 3 ) time [[100]](#b99), so the overall time complexity is O(m 5 ) = O(n 5/2 ).

## D.3 Monarch Factorizations for Matrices in MM *

In this section, we describe the algorithm for factorizing matrices in MM * previously outlined in Section 3.4 (Algorithm 2). Again, Algorithm 2 handles the general case where the block sizes of L and R can be different. We then prove Theorem 7, which has Theorem 2 as an immediate corollary.

Our goal is thus to compute the matrices L 1 , R, L 2 in the factorization of M. In order to compute this factorization, we require the following assumption on M: Assumption D.1. Assume that (1) M ∈ MM * (b,n) is invertible and (2) M can be written as (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) where L 1 , L 2 ∈ BD ( n b ,n) , R ∈ BD (b,n) , and R has no nonzero entries in its diagonal blocks. (Note that by Proposition C.10, we can write any M ∈ MM * (b,n) as (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ); thus, (2) is merely the assumption that R has no zero entries in its blocks.) This is analogous to Assumption 3.3, except applicable to the more general block size b. We now present Algorithm 2 to find factors L 1 , R, L 2 of matrices satisfying Assumption D.1.

First, observe that if we define M = P (b,n) MP (b,n) , we have M = L 1 (P (b,n) RP (b,n) )L 2 . By Theorem 3, the matrix P (b,n) RP (b,n) is in DB ( n b ,n) , i.e., is a block matrix with blocks of size n b × n b where each block is a diagonal matrix. Thus, we can write: Thus, we have the set of matrix equations A i D ij C j = M ij , for 1 ≤ i, j ≤ b. Notice that the assumption that the R has no nonzero entries in its blocks (Assumption D.1) is equivalent to assuming that none of the diagonal entries of any matrix D ij is equal to zero. Also, the assumption that M is invertible implies that L 1 , L 2 are invertible (since the product of square singular matrices is singular), which in turn implies that each block matrix A i and each block matrix C j is invertible (since a square block-diagonal matrix where one of the blocks is singular is itself singular). Taken together, this means that each matrix M ij is invertible, since M ij = A i D ij C j and each of the matrices on the RHS of the equation is invertible.

$      M 11 M 12 . . . M 1b M 21 M 22 . . . M 2b . . . . . . . . . . . . M b1 M b2 . . . M bb       =      A 1 A 2 . . . A b           D 11 D 12 . . . D 1b D 21 D 22 . . . D 2b . . . . . . . . . . . . D b1 D b2 . . . D bb           C 1 C 2 . . . C b      , whereA$Observe that given a solution to the set of equations A i D ij C j = M ij , if we rescale and permute the matrices A i , D ij , C j appropriately, the result is still a solution to the equations. Specifically, let P be any permutation matrix and {S i } b i=1 , {S j } b j=1 be any invertible diagonal matrices (i.e., diagonal matrices without any zeros on the diagonal). Define D ij = S i P D ij PS j for all i, j. Notice that P D ij P = P -1 D ij P is diagonal because D ij is diagonal. Thus, D ij is diagonal (and invertible) since the product of diagonal matrices is diagonal. Define A i = A i PS -1 i and C j = P S -1 j C j for all i, j. Thus, we have that

$M ij = A i D ij C j = (A i PS -1 i )D ij (P S -1 j C j ) = A i D ij C j$for all i, j: in other words, we can scale the A i 's on the right by any invertible diagonal matrix, the C j 's on the left by any invertible diagonal matrix, and apply a matching permutation to the rows of the C j 's and the columns of the A i 's, and apply matching transformations to the D ij 's and the result will still be a valid factorization. This implies that as long as we recover a "correct" Ĉ1 up to a permutation and scaling of its rows, we can set the Di1 's and D1j 's to the identity matrix, and then compute the remaining Âi 's and Ĉj 's via the equations Âi = M i1 Ĉ-1 1 and Ĉj = Â-1 1 M 1j . To understand how we can compute such a matrix Ĉ1 , define

$F(i, j) = M -1 i1 M ij M -1 1j M 11 and observe that F(i, j) = M -1 i1 M ij M -1 1j M 11 = (C -1 1 D -1 i1 A -1 i )(A i D ij C j )(C -1 j D -1 1j A -1 1 )(A 1 D 11 C 1 ) = C -1 1 (D -1 i1 D ij D -1 1j D 11 )C 1 for all 1 ≤ i, j ≤ b. Note that D -1 i1 D ij D -1 1j D 11 is a diagonal matrix; thus, C 1 F(i, j)C -1$1 is diagonal for all i, j, i.e., C 1 simultaneously diagonalizes all the matrices F(i, j). (Note: In this paper, we say that a matrix Q "simultaneously diagonalizes" a set of matrices G

$1 , . . . , G k if QG i Q -1 is a diagonal matrix for all 1 ≤ i ≤ k.$Note that sometimes the opposite convention [i.e., Q -1 G i Q must be diagonal] is used in the literature; we adopt the former for notational convenience.) Indeed, if any matrix simultaneously diagonalizes all these matrices, then it leads to a valid factorization, which we show in the proof of Theorem 7. Therefore, we compute some matrix that simultaneously diagonalizes all these matrices, and set Ĉ1 to that matrix.

These ideas form the basis of Algorithm 2, which is presented formally below. Algorithm 2 uses simultaneous diagonalization as a subroutine; we discuss how to solve simultaneous diagonalization problems below. maintains the property that QG j Q -1 is diagonal for all j < i, since PDP is diagonal for any permutation P and diagonal matrix D). Then for each block of size > 1, compute a matrix that diagonalizes that block; denoting the number of blocks (including size-1 blocks) by b, let Q 1 , . . . , Q b denote the corresponding diagonalizing transformations, or the scalar 1 when the block is of size 1. Finally set Q ← diag(Q 1 , . . . , Q b ) and Q ← Q -1 QQ . By construction, QG i Q -1 will now be diagonal; also, QG j Q -1 is still diagonal for all j < i, because any linear combination of a set of eigenvectors of a diagonalizable matrix corresponding to a repeated eigenvalue λ is itself an eigenvector of that matrix with eigenvalue λ. Thus, once we've processed all k of the G i 's, Q is a matrix that simultaneously diagonalizes all of them. At each step i, we compute diagonalizing transformations for square block matrices whose sizes s 1 , . . . , s k sum to n. As eigendecomposition (for a fixed desired precision) takes O(n 3 ) time for an n × n matrix, this means the total runtime of step i is O k j=1 s 3 i ≤ O(n 3 ). Thus the total runtime of the entire simultaneous diagonalization procedure is O(kn 3 ), where k is the number of matrices. (Note that iterative methods for simultaneous diagonalization also exist [[4,](#b3)[2]](#b1) and could be used to speed up this step in practice.)

Applying this to our problem, we have b for the entire simultaneous diagonalization procedure, and thus the runtime of Algorithm 2 is also O n 3 b , as desired. (Note: As can be seen from the above analysis, we don't actually need M itself to be invertible-we simply need all its blocks M ij to be, so that all the A i 's and C j 's are, which is a weaker assumption that invertibility of M given that we already assumed the D ij 's are invertible due to the nonzero assumption on the blocks of R.) 

## E.4 Details for BERT Pretraining

We follow the training procedure and hyperparameters of the reference implementation from Nvidia Deep Learning examples ([https://github.com/NVIDIA/DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples)). In particular, we use the LAMB optimizer with learning rate 4e-3. We use as large a minibatch size as possible that still fits in the GPU memory (A100-40GB), and use gradient accumulation to reach an effective batch size of 64k sequences for phase 1 (maximum sequence length 128) and 32k for phase 2 (maximum sequence legnth 512). We train is mixed precision (fp16 and fp32).

We use all the optimizations that were in Nvidia's BERT implementation in MLPerf 1.1:

1. Only compute the prediction scores (last layer) for masked tokens as the outputs of other tokens are not used to compute the masked language modeling loss.

2. Remove padding tokens and only compute the attention for non-padding tokens.

3. Use a fused CUDA kernel (FMHA) that combines 4 steps into one kernel: computes QK T , take softmax, apply dropout, multiply by V , where Q, K, V are the query, key, and value respectively.

4. Fuse matrix multiplication and adding bias into one CUDA kernel in the feed-forward network (FFN) layers. The gradient of the bias is also fused with the matrix multiplication the backward pass.

5. Fuse matrix multiplication and adding bias into one CUDA kernel in the attention output projection.

6. Fuse dropout and adding residual in the residual connection at the end on the attention and FFN blocks.

We train with DeepSpeed [[88]](#b87) ZeRO optimizer stage 1 to shard the optimizer states, thus reducing GPU memory usage and allowing us to use larger batch sizes. For the Nvidia MLPerf implementation, we report the speed for both Apex's automatic mix-precision (AMP) level O2 (as in the original implementation), and DeepSpeed ZeRO optimizer.

## E.5 Accelerated Multi-coil MRI Reconstruction

## E.5.1 Background

In multi-coil MRI, multiple receiver coils (i.e. sensors) acquire complex-valued measurements in the spatial frequency (a.k.a. k-space) domain. These measurements are modulated by the spatially-varying sensitivity maps, which characterize the sensitivity of each coil to the imaging target. In accelerated MRI, scan times are reduced by decreasing the number of samples acquired in k-space. Because the data is sampled below the Nyquist rate, reconstructing the underlying image is an ill-posed problem.

The forward problem for accelerated multi-coil MRI can be written as the matrix equation

$y = ΩF Sx +$where Ω is the binary undersampling mask that indexes acquired samples in k-space, y is the vectorized measured signal in k-space, F is the discrete Fourier transform matrix, S is the receiver coil sensitivity maps, x is the ground-truth signal in image-space, and is additive complex Gaussian noise. The acceleration factor is given by R =   

![Figure 1: Monarch matrices unlock several ways to train sparse and dense models: end-to-end training a sparse (Monarch) model can be 2x faster than dense training thanks to its hardware efficiency; sparse-to-dense "reverse sparsification" can speed up training of large models such as GPT-2; and our dense-to-sparse Monarch projection algorithm can transfer knowledge from pretrained dense model to Monarch model and speed up BERT fine-tuning.]()

![Figure 2: Monarch matrices are parametrized as products of two block-diagonal matrices up to permutation, allowing efficient multiplication algorithm that leverages batch matrix multiply.]()

![butterfly factor matrices in the butterfly factorization of B, and R by multiplying together the last log 2 n 2 butterfly factor matrices. (We detail this more rigorously in Theorem 4.) The matrix R is block-diagonal with m = √ n dense blocks, each block of size m×m: R = diag(R 1 , . . . , R m ). The matrix L is composed of m × m blocks of size m × m, where each block is a diagonal matrix:]()

![Figure 3: With the "reverse sparsification" process, Monarch matrices can speed up GPT-2 training by 2x.]()

![and the permutation P of Definition 3.1. Then, we can compute L 1 , L 2 , R in such a factorization under Assumption 3.3, as stated in Theorem 2. (Note that the factorization is not unique.) Assume that (1) M ∈ MM * is invertible and (2) M can be written as (PL 1 P )R(PL 2 P ) where the blocks of R have no zero entries.Given an n × n matrix M ∈ MM * satisfying Assumption 3.3, there is an O(n 5/2 )-time algorithm to find its Monarch factors L 1 , R, L 2 .]()

![Figure 4: With Algorithm 1 for our Monarch parameterization, we can convert a pretrained model into a model with Monarch weight matrices and speed up downstream fine-tuning. where m = √ n, the A i 's and C j 's denote the m × m diagonal blocks of L 1 , L 2 respectively, and each D ij is an m × m diagonal matrix. If we write M as a block matrix with m × m blocks each of size m × m, then we see that the blockM ij is equal to A i D ij C j .Notice that M is invertible only if all the A i 's and C j 's are (since if any one of these is singular, then L 1 or L 2 is singular).Thus, our goal is to find matrices Â1 , . . . , Âm , Ĉ1 , . . . , Ĉm and diagonal matrices D11 , . . . , Dmm such that M ij = Âi Dij Ĉj for all i, j; this represents a valid Monarch factorization of M.To provide intuition for how to do this, let's analyze a simple case in which all the D ij 's are the identity matrix. Then we have the set of equations A i C j = M ij . Again assume the A i 's and C j 's are invertible, so each M ij is as well. Suppose we set Ĉ1 = I (identity matrix). Then we can immediately read off Âi = M i1 for all i. We can then set Ĉj = Â-11 M 1j for all j. Let's now check that this strategy gives a valid factorization, i.e., that M ij = Âi Ĉj for all i, j. We have Âi Ĉj = M i1 M -1 11 M 1j . Recalling that in the "true" factorization we haveM ij = A i C j , this equals (A i C 1 )(A 1 C 1 ) -1 (A 1 C j ) = A i C j , as desired.In the general case, we must deal with the diagonal D ij matrices as well. We will no longer be able to freely set Ĉ1 = I. However, once we find a proper choice of Ĉ1 , we can use it to find all the Âi 's and Ĉj 's. We can find such a Ĉ1 via the idea of simultaneous diagonalization; for space reasons, we defer a full description of our algorithm (Algorithm 2), and its analysis, to Appendix D.]()

![, we show the training time of the dense GPT-2 model, along with the Monarch GPT-2 model. After training the Monarch model for 90% of the time, in the last 10% of the training steps, by transitioning to dense weight matrices, the model is able to reach the same performance of another model that was trained with dense weight matrices from scratch. By training with Monarch matrices for 90% of the time, we reduce the total training time by 2×.]()

![The total training time of BERT-large trained with Monarch reverse sparsification and with conventional dense training on 8 A100-40GB GPUs (DGX A100). Training consists of two phases, phase 1 with sequence length 128 and phase 2 with sequence length 512. Monarch training is 3.5x faster than HuggingFace and 23% faster than Nvidia's MLPerf 1.1 implementation.]()

![Figure 5: Time required (in A100 GPU hours) to reach the same perplexity (18.0) for GPT-2-small on OpenWebText. With "reverse sparsification", Monarch can speed up GPT-2 training by 2×. serves as a proof of concept, and we expect further speedup if additional model compression techniques are applied (e.g., quantization, kernel fusion).]()

![(s/n) . The class (MM * (f (n,s),n) ) O(d) O(s/n) has O(d s 2 f (n,s) + dsf (n, s)) = O(ds 3/2 ) parameters. Thus, the monarch representation of A is suboptimal by at most an O(d √ s) factor compared to the O(d log s) of kaleidoscope.]()

![A wrapped diagonal matrix S ∈ F b3×b2 is defined as follows. First assume b 2 ≤ b 3 . Then for any 0 ≤ i < b 3 and 0 ≤ j < b 2 , we have the following. If i mod b 2 = j, then S[i, j] = 0. (If b 2 > b 3 , then instead apply the previous definition to S .)]()

![Else, (i.e., when i 0 mod b 2 = j 0 ), thenL[i, j] = S i1,j1 [i 0 , j 0 ].If b 2 > b 3 , then in the above, the condition "i 0 mod b 2 = j 0 " gets replaced by "j 0 mod b 2 = i 0 ."Using the above definitions, we now define the class of rectangular Monarch matrices.Definition C.19 (Rectangular Monarch Matrix). Let M ∈ F n3×n1 be a matrix of the form:]()

![1 , . . . , A b are n b × n b matrices that are the diagonal blocks of L 1 ; C 1 , . . . , C b are n b × n b matrices that are the diagonal blocks of L 2 ; D 11 , . . . , D 1b , D 21 , . . . , D 2b , . . . , D b1 , . . . , D bb are n b × n b diagonal matrices that are the blocks of P (b,n) RP (b,n) ; and M 11 , . . . , M 1b , M 21 , . . . , M 2b , . . . , M b1 , . . . , M bb are n b × n b matrices that are the blocks of M = P (b,n) MP (b,n) .]()

![MM * FactorizationRequire: Block size b; matrix M ∈ MM * (b,n) satisfying Assumption D.10: Define M ij (of size n b × n b ) as the i, j block of P (b,n) MP (b,n) 1: for 1 ≤ i, j ≤ b do Compute F(i, j) := M -1 i1 M ij M -1 1j M 11 2: end for 2: Ĉ1 ← SIMULTANEOUS DIAG {F(i, j)} b,b i,j=1,Âi ← M i1 Ĉ-Ĉj ← Â-1 1 M 1j 6:end for 7: for 1 ≤ i, j ≤ b do Given an n × n matrix M ∈ MM * (b,n) satisfying Assumption 3.3, Algorithm 2 finds its Monarch factors L 1 , R, L 2 in time O n 3 b .]()

![matrices to simultaneously diagonalize, each of size n b × n b . This leads to a total runtime of O b 2 • ( n b ) 3 = O n 3 b]()

![Figure 6: Sample reconstructions at 2x acceleration for the first echo in the SKM-TEA dataset using SENSE, Monarch-SENSE (mSENSE), and U-Net. Both mSENSE and U-Net are trained with 1 training scan. SENSE is an untrained method.]()

![Figure 7: Sample reconstructions at 2x acceleration for the second echo in the SKM-TEA dataset using SENSE, Monarch SENSE (mSENSE), and U-Net. Both mSENSE and U-Net are trained with 1 training scan. SENSE is an untrained method.]()

![The performance of Monarch matrices and ViT / MLP-Mixer on ImageNet, including the number of parameters and FLOPs. We measure the Top-1 accuracy and the training time speedup compared to the corresponding dense model.]()

![Performance of Monarch matrices and GPT-2-Small/Medium on WikiText-103, including the # of parameters and FLOPs. Monarch achieves similar perplexity (ppl) but 2.0× faster.]()

![Mean ± standard error of the mean of conventional and Monarch-SENSE (mSENSE) on dual-echo (E1,E2) MRI reconstruction at multiple acceleration factors (Acc.).]()

![Impact of number of training examples (N ) on dual-echo MRI reconstruction at 2x acceleration.GPT-2 pretraining. On the large OpenWebtext dataset[36], we train a GPT-2 model with Monarch weight matrices for 90% of the training iterations, then relax the constraint on the weight matrices and train them as dense matrices for the remaining 10% of the iterations. We call this technique "reverse sparsification." Previous sparse training techniques often don't speed up training, whereas our hardware-efficient Monarch matrices do. Therefore we can use them as an intermediate step to pretrain a large language model (GPT-2)]()

![The performance (accuracy) of GPT-2-medium trained with Monarch reverse sparsification and with conventional dense training on text classification benchmarks.]()

![The performance of Monarch matrices in finetuning BERT on GLUE.We propose Monarch, a novel matrix parameterization that inherits the expressiveness of butterfly matrices and thus can represent many fast transforms. Our parameterization leverages optimized batch matrix multiply routines on GPUs, yielding up to 2× speedup compared to dense matrix multiply. We derive an efficient algorithm for projecting an arbitrary dense matrix on the set of Monarch factors. Our algorithm allows us to easily fine-tune a pretrained model into a model with Monarch weight matrices. As a result, Monarch matrices unlock new ways for faster end-to-end training, sparse-to-dense training, and dense-to-sparse fine-tuning of large neural networks. By making structured matrices practical, our work is a first step towards unlocking tremendous performance improvements in applying sparse models to wide-ranging ML applications (including science and medicine). We anticipate this work can inspire more future work on advancing machine learning models for interdisciplinary research with limited computational resources.No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.]()

![The performance (accuracy) of GPT-2-medium trained with Monarch reverse sparsification and with conventional dense training on text classification benchmarks.]()

They are named after the monarch butterfly.

Monarch code is available at https://github.com/HazyResearch/monarch

Our result is not an official MLPerf submission. We train BERT for both phase 1 (sequence length 128) and phase 2 (sequence length 512) according to the standard BERT training recipe[[22]](#b21), while MLPerf only measures training time for phase 2.

if i 1 = j 1 , then R[i, j] = 0.

This uses the fact that σ (b,n)-1 = σ ( n b ,n) (which means P ( n b ,n) = P (b,n) since the inverse of a permutation matrix is its transpose).

This uses the fact that σ (b,n)-1 = σ ( n b ,n) .

