- Decision to use structured matrices to reduce compute/memory requirements
- Choice of Monarch matrices as a parameterization for efficiency and expressiveness
- Adoption of block-diagonal matrix products for hardware optimization
- Selection of analytical optimal solution for approximating dense weight matrices
- Implementation of "reverse sparsification" technique for sparse-to-dense training
- Development of a projection algorithm for dense-to-sparse fine-tuning
- Validation of Monarch matrices in end-to-end training applications
- Comparison of Monarch matrices with existing structured matrix methods
- Decision to empirically validate efficiency-accuracy tradeoffs across multiple domains
- Choice of specific benchmarks (ViT, GPT-2, BERT) for performance evaluation
- Assumptions regarding the expressiveness of Monarch matrices in representing common transforms
- Decision to focus on both hardware efficiency and model quality in design
- Consideration of trade-offs between training speed and model accuracy
- Decision to derive algorithms for matrix factorization within the Monarch framework
- Choice to explore applications in scientific and medical domains for structured matrices