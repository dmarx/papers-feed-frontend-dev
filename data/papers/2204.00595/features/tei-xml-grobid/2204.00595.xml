<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monarch: Expressive Structured Matrices for Efficient and Accurate Training</title>
				<funder ref="#_nbbkj5J">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_pxuV77X">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-04-04">April 4, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<email>trid@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
							<email>beidic@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nimit</forename><surname>Sohoni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arjun</forename><surname>Desai</surname></persName>
							<email>arjundd@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
							<email>poli@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jessica</forename><surname>Grogan</surname></persName>
							<email>jrgrogan@buffalo.edu</email>
							<affiliation key="aff1">
								<orgName type="department">University at Buffalo</orgName>
								<address>
									<country>SUNY</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Liu</surname></persName>
							<email>avliu@umich.edu</email>
						</author>
						<author>
							<persName><forename type="first">Aniruddh</forename><surname>Rao</surname></persName>
							<email>anrao@umich.edu</email>
						</author>
						<author>
							<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">University at Buffalo</orgName>
								<address>
									<country>SUNY</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Monarch: Expressive Structured Matrices for Efficient and Accurate Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-04">April 4, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">7F45F5E50CEA0828BA2BDCB1008FEC41</idno>
					<idno type="arXiv">arXiv:2204.00595v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute/memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency-quality tradeoffs, and (2) in denseto-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2× with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called "reverse sparsification," Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2× without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7× with comparable accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large neural networks excel in many domains, but their training and fine-tuning demand extensive computation and memory <ref type="bibr" target="#b53">[54]</ref>. A natural approach to mitigate this cost is to replace dense weight matrices with structured ones, such as sparse &amp; low-rank matrices and the Fourier transform. However, structured matrices (which can be viewed as a general form of sparsity) have not yet seen wide adoption to date, due to two main challenges. <ref type="bibr" target="#b0">(1)</ref> In the end-to-end (E2E) training setting, they have shown unfavorable efficiency-quality tradeoffs. Model efficiency refers how efficient these structured matrices are on modern hardware (e.g., GPUs). Model quality (performance on tasks) is determined by how expressive they are (e.g., can they represent commonly used transforms such as convolution or Fourier/cosine transforms that encode domain-specific knowledge). Existing structured matrices are either not hardware-efficient, or not expressive enough. <ref type="bibr" target="#b1">(2)</ref> In the setting of dense-to-sparse (D2S) fine-tuning of pretrained models, a long-standing problem for most classes of structured matrices is the lack of tractable algorithms to approximate dense pretrained weight matrices <ref type="bibr" target="#b78">[79]</ref>. Sparse matrices have seen advances in training deep learning models (e.g., pruning <ref type="bibr" target="#b43">[44]</ref>, lottery tickets <ref type="bibr" target="#b29">[30]</ref>), but most work on (entrywise) sparsification focuses on reducing training or inference FLOPs, which do not necessarily map to E2E training time on modern hardware (e.g., GPUs). In fact, most sparse training methods slow down training in wall-clock time <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48]</ref>. Moreover, sparse matrices are not able to represent commonly used transforms such as convolution and the Fourier transform. Another class of structured matrices, such as Fourier, sine/cosine, Chebyshev, are used in specialized domains such as PDE solving <ref type="bibr" target="#b99">[100]</ref> and medical imaging <ref type="bibr" target="#b48">[49]</ref>. However, they are difficult to use in E2E training since only specific instances of these structured matrices have fast GPU implementations (e.g., FFT). Moreover, their applications requires domain expertise to hand-pick the right transforms. Generalizations of these transforms (e.g., Toeplitz-like <ref type="bibr" target="#b94">[95]</ref>, orthogonal polynomial transforms <ref type="bibr" target="#b24">[25]</ref>, low-displacement rank <ref type="bibr" target="#b52">[53]</ref>, quasi-separable <ref type="bibr" target="#b26">[27]</ref>), though learnable, often lack efficient implementation on GPUs <ref type="bibr" target="#b97">[98]</ref> for E2E training as well. In addition, they have no known tractable algorithm to approximate a given dense matrix <ref type="bibr" target="#b78">[79]</ref>, making them difficult to use in D2S fine-tuning.</p><p>E2E training. The technical challenge in addressing the efficiency-quality tradeoff of structured matrices is to find a parameterization that is both efficient on block-oriented hardware (e.g., GPUs) and expressive (e.g., can represent many commonly used transforms). We propose a class of matrices called Monarch,<ref type="foot" target="#foot_0">foot_0</ref> parameterized as products of two block-diagonal matrices (up to permutation), to address this challenge. This parameterization leverages optimized batch-matrix-multiply (BMM) routines on GPUs, yielding up to 2× speedup compared to dense matrix multiply (Section 5.1.1). We show that the class of Monarch matrices contains the class of butterfly matrices <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b11">12]</ref>, which can represent any low-depth arithmetic circuits in near optimal runtime and parameter size <ref type="bibr" target="#b12">[13]</ref>. Monarch matrices inherit this expressiveness and thus can represent many fast transforms (e.g., Fourier, sine/cosine/Chebyshev transforms, convolution) (Proposition 3.2).</p><p>Sparse-to-dense (S2D) training, aka "reverse sparsification". The hardware-efficiency and expressiveness of Monarch matrices unlock a new way to train dense models: training with Monarch weight matrices for most of the time and then transitioning to dense weight matrices (Fig. <ref type="figure" target="#fig_3">3</ref>). This technique can be used in cases where sparse training faces representation or optimization difficulties <ref type="bibr" target="#b27">[28]</ref> or a dense model is necessary. One such application is language modeling on large datasets, where a massive number of parameters are required <ref type="bibr" target="#b53">[54]</ref> to memorize the textual patterns <ref type="bibr" target="#b34">[35]</ref>. Monarch matrices can serve as a fast intermediate representation to speed up the training process of the dense model.</p><p>D2S fine-tuning. While transitioning from sparse to dense matrices is easy, the reverse direction is challenging. The main technical difficulty is the projection problem: finding a matrix in a class of structured matrices that is the closest to a given dense matrix. Only a few specific classes of structured matrices have a tractable projection solution, such as entrywise sparse matrices (magnitude pruning <ref type="bibr" target="#b96">[97]</ref>), low-rank matrices (the Eckart-Young theorem <ref type="bibr" target="#b25">[26]</ref>), and orthogonal matrices (the orthogonal Procrustes problem <ref type="bibr" target="#b92">[93]</ref>). For more expressive classes of structured matrices, projection remains a long-standing problem <ref type="bibr" target="#b78">[79]</ref>. For example, De Sa et al. <ref type="bibr" target="#b15">[16]</ref> show that all structured matrices (in the form of arithmetic circuits) can be written as products of sparse matrices, which can be represented as products of butterfly matrices <ref type="bibr" target="#b12">[13]</ref>. There have been numerous heuristics proposed to project on the set of butterfly matrices or products of sparse matrices, based on iterative first-order optimization <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b54">55]</ref> or alternating minimization <ref type="bibr" target="#b66">[67]</ref>. However, they lack theoretical guarantees. In contrast, we derive a projection algorithm for our Monarch parameterization and prove that it finds the optimal solution (Theorem 1). We also derive an algorithm to factorize matrices that are products of Monarch matrices (Section 3.4). These new algorithms allows us to easily finetune a pretrained model into a model with Monarch weight matrices (Section 5.3).</p><p>We validate our approach empirically in these three settings, showing that our Monarch matrix parameterization achieves a favorable efficiency-accuracy tradeoff compared to baselines on a wide range of domains: text, images, PDEs, MRI. • In the E2E sparse training setting (Section 5.1), our Monarch matrices model trains 2× faster than dense models while achieving the same accuracy / perplexity on benchmark tasks (ViT on ImageNet classification, GPT-2 on Wikitext-103 language modeling). On scientific and medical tasks relying on hand-crafted fast transforms (PDE solving, MRI reconstruction), Monarch reduces the error by up to 40% at the same training speed compared to domain-specific Fourier-based methods. • In the S2D training setting (Section 5.2), our "reverse sparsification" process with Monarch matrices speeds up GPT-2 pretraining on the large OpenWebText dataset by 2× compared to an optimized implementation from NVIDIA <ref type="bibr" target="#b93">[94]</ref>, with comparable upstream and downstream (text classification) quality.</p><p>When applied to BERT pretraining, our method is 23% faster than the implementation from Nvidia that set the MLPerf <ref type="bibr" target="#b71">[72]</ref> 1.1 record. • In the D2S fine-tuning setting (Section 5.3), we show a proof of concept that our Monarch projection algorithm speeds up BERT fine-tuning. We project a pretrained BERT model to a Monarch matrix model and fine-tune on GLUE, with 2× fewer parameters, 1.7× faster fine-tuning speed, and similar average GLUE accuracy as the dense model. <ref type="foot" target="#foot_1">2</ref>2 Related Work and Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>Sparse Training. Sparse training is an active research topic. There has been inspiring work along the line of compressing models such as neural network pruning and lottery tickets <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b29">30]</ref>. Pruning methods usually eliminate neurons and connections through iterative retraining <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b91">92]</ref> or at runtime <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Although both Monarch and pruning methods aim to produce sparse models, we differ in our emphasis on overall efficiency, whereas pruning mostly focuses on inference efficiency and disregards the cost of finding the smaller model. Lottery tickets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks in convergence speed and potentially in generalization.</p><p>Monarch can be roughly seen as a class of manually constructed lottery tickets. Structured Matrices. Structured matrices are those with subquadratic (o(n 2 ) for dimension n × n) number of parameters and runtime. Examples include sparse and low-rank matrices, and fast transforms (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). They are commonly used to replace the dense weight matrices of deep learning models, thus reducing the number of parameters and training/inference FLOPs. Large classes of structured matrices (e.g., Toeplitz-like <ref type="bibr" target="#b94">[95]</ref>, low-displacement rank <ref type="bibr" target="#b52">[53]</ref>, quasiseparable <ref type="bibr" target="#b26">[27]</ref>) have been shown to be able to represent many commonly used fast transforms. For example, De Sa et al. <ref type="bibr" target="#b15">[16]</ref> show that a simple divide-and-conquer scheme leads to a fast algorithm for a large class of structured matrices. Our work builds on butterfly matrices <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b11">12]</ref>, which have been shown to be expressive but remain hardware-inefficient. Pixelated butterfly <ref type="bibr" target="#b5">[6]</ref> has attempted to make butterfly matrices more hardware-friendly, but at the cost of reduced expressiveness. Furthermore, it is not known if one can directly decompose a dense pretrained model to a model with butterfly weight matrices without retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Butterfly Matrices</head><p>Our work builds on recent work on butterfly matrices. Dao et al. <ref type="bibr" target="#b11">[12]</ref> introduced the notion of a butterfly matrix as a certain product of permuted block-diagonal matrices, inspired by the Cooley-Tukey fast Fourier transform algorithm <ref type="bibr" target="#b10">[11]</ref>. They encode the divide-and-conquer structure of many fast multiplication algorithms. Dao et al. <ref type="bibr" target="#b12">[13]</ref> showed that all structured matrices can be written as products of such butterfly matrices, and this representation has optimal memory and runtime complexity up to polylogarithmic factors. We now review these definitions (following <ref type="bibr" target="#b12">[13]</ref>).</p><p>A butterfly factor of size k (where k is even) is a matrix of the form</p><formula xml:id="formula_0">D 1 D 2 D 3 D 4 where each D i is a k 2 × k 2 diagonal matrix.</formula><p>We call this class of matrices BF (k,k) . A butterfly factor matrix of size n and block size k is a block diagonal matrix of n k butterfly factors of size k:</p><formula xml:id="formula_1">diag B 1 , B 2 , . . . , B n k ,</formula><p>where B i ∈ BF (k,k) . We call this class of matrices BF (n,k) . Finally, a butterfly matrix of size n = 2 s is a matrix M that can be expressed as a product of butterfly factor matrices:</p><formula xml:id="formula_2">M = B n B n/2 . . . B 2 ,</formula><p>where each B i ∈ BF (n,i) . We denote the set of size-n butterfly matrices by B (n) . Equivalently, M can be written in the following form:</p><formula xml:id="formula_3">M = B n M 1 0 0 M 2 ,</formula><p>where B n ∈ BF (n,n) and M 1 , M 2 ∈ B ( n 2 ) . Dao et al. <ref type="bibr" target="#b12">[13]</ref> further introduce the kaleidoscope matrix hierarchy: the class BB * (n) is the set of matrices of the form n) , and the class (BB * (n) ) w e is the set of all matrices of the form n) . (A * denotes the conjugate transpose of A.) When the size n is clear from context, we will omit the superscript (n) (i.e., just write B, BB * , etc.). As shown by Theorem 1 of Dao et al. <ref type="bibr" target="#b12">[13]</ref>, the kaleidoscope hierarchy can represent any structured matrix with nearly-optimal parameters and runtime: if M is an n × n matrix such that multiplying any vector v by M can be represented as a linear arithmetic circuit with depth d and s total gates, then M ∈ (BB * (n) )</p><formula xml:id="formula_4">M 1 M * 2 for M 1 , M 2 ∈ B (</formula><formula xml:id="formula_5">w i=1 M i [1:n, 1:n] where each M i ∈ BB * (e•</formula><formula xml:id="formula_6">O(d) O(s/n) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monarch: Definition &amp; Algorithms</head><p>In Section 3.1, we introduce Monarch matrices, and describe how they relate to butterfly matrices. In Section 3.2 we show that the class of Monarch matrices is at least as expressive as the class of butterfly matrices, while admitting a practically efficient representation. In particular, many fast transforms (e.g., Fourier, convolution) can be represented as a Monarch matrix or as the product of two or four Monarch matrices (Proposition 3.2). In Section 3.3, we show how to project onto the set of Monarch matrices. This allows us to tractably approximate a given matrix (e.g., a dense pretrained weight matrix) with a Monarch matrix, unlocking new applications (cf. Section 5). In Section 3.4, we show how to recover the individual factors of the larger class of products of two Monarch matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monarch Parametrization for Square Matrices</head><p>Inspired by the 4-step FFT algorithm <ref type="bibr" target="#b2">[3]</ref>, we propose the class of Monarch matrices, each parametrized as the product of two block-diagonal matrices up to permutation:</p><formula xml:id="formula_7">Definition 3.1. Let n = m 2 .</formula><p>An n × n Monarch matrix has the form:</p><formula xml:id="formula_8">M = PLP R,</formula><p>where L and R are block-diagonal matrices, each with m blocks of size m × m, and P is the permutation that maps [x 1 , .</p><p>. . , x n ] to [x 1 , x 1+m , . . . , x 1+(m-1)m , x 2 , x 2+m , . . . , x 2+(m-1)m , . . . , x m , x 2m , . . . , x n ]. We call this the Monarch parametrization. We denote the class of all matrices that can be written in this form as M (n) (dropping the superscript when clear from context). Fig. <ref type="figure" target="#fig_1">2</ref> illustrates this parametrization.</p><p>We now provide more intuition for this parametrization and connect it to butterfly matrices. For ease of exposition, suppose B ∈ B (n) where n is a power of 4. Then let L be obtained by multiplying together the first log The matrix L can also be written as block-diagonal with the same structure as R after permuting the rows and columns. Specifically, let P be the permutation of Definition 3.1. We can interpret P as follows: it reshapes the vector x of size n as a matrix of size m × m, transposes the matrix, then converts back into a vector of size n. Note that P = P . Then we can write L = PL P , where L = diag(L 1 , . . . , L m ).</p><p>Hence, up to permuting rows and columns, L is also a block-diagonal matrix of m dense blocks, each of size m × m.</p><p>Thus we can write B = PLP R, where L, R, and P are as in Definition 3.1. So, B ∈ B (n) implies that B ∈ M (n) .</p><p>Products of Monarch Matrices. Another important class of matrices (due to their expressiveness, cf. Proposition 3.2) is the class MM * : matrices that can be written as M 1 M * 2 for some M 1 , M 2 ∈ M. Further, (MM * ) 2 denotes the class of matrices that can be written</p><formula xml:id="formula_9">M 1 M 2 for M 1 , M 2 ∈ MM * .</formula><p>Extension to Rectangular Matrices. In practice, we also want a way to parametrize rectangular weight matrices, and to increase the number of parameters of Monarch matrices to fit different applications (analogous to the rank parameter in low-rank matrices and the number of nonzeros in sparse matrices). We make the simple choice to increase the block size of the block-diagonal matrices in the Monarch parametrization, and to allow rectangular blocks. More details are in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expressiveness and Efficiency</head><p>We remark on the expressiveness of Monarch matrices and their products (ability to represent many structured transforms), and on their computational and memory efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Expressiveness</head><p>As described in Section 3.1, any matrix B ∈ B (n) can be written in the Monarch butterfly representation, by simply condensing the log 2 n total factors into two matrices. Thus, the Monarch butterfly representation is strictly more general than the original butterfly representation (as there also exist matrices in M (n) but not B (n) ). In other words, for a given size n, M ⊃ B; similarly MM * ⊃ BB * . In particular, Dao et al. <ref type="bibr" target="#b12">[13]</ref> showed that the following matrix classes are contained in BB * , which implies they are in MM * as well: Proposition 3.2. The matrix class MM * can represent convolution, Hadamard transform, Toeplitz matrices <ref type="bibr" target="#b36">[37]</ref>, and AFDF matrices <ref type="bibr" target="#b73">[74]</ref>. The matrix class (MM * ) 2 can represent the Fourier transform, discrete sine and cosine transforms (DST/DCT), the (HD) 3 [106] class, Fastfood <ref type="bibr" target="#b61">[62]</ref>, and ACDC matrices <ref type="bibr" target="#b73">[74]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Efficiency</head><p>Parameters. A Monarch matrix M = PLP R is described by 2n √ n parameters: L, R both have √ n dense blocks of size √ n × √ n, for a total parameter count of n √ n each. The permutation P is fixed, and thus doesn't add any parameters. Speed. To multiply by M, we need to multiply by a block diagonal matrix R, permute, multiply by a block diagonal matrix L, and finally permute. All four of these steps can be implemented efficiently. The total number of FLOPs is O(n √ n), which is more the O(n log n) for a butterfly matrix. However, since we can leverage efficient block-diagonal multiplication (e.g., batch matrix multiply), Monarch multiplication is easy to implement and is fast in practice (2x faster than dense multiply, cf. Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Projection on the Set M of Monarch Matrices</head><p>Given our class of structured matrices, a natural question is the projection problem: finding a Monarch matrix that is the closest to a given dense matrix. We show that this problem has an analytical optimal solution, and show how to compute it efficiently. This allows us to project dense models to Monarch models, enabling D2S fine-tuning (Section 5.3).</p><p>We formalize the problem: for a given matrix A, find</p><formula xml:id="formula_10">argmin M∈M A -M 2 F .<label>(1)</label></formula><p>Even though this problem is nonconvex (as M is parametrized as the product of two matrices), in Theorem 1 we show that there exists an analytical solution (full proof in Appendix D). This is analogous to the Eckart-Young theorem that establishes that optimal low-rank approximation is obtained from the SVD <ref type="bibr" target="#b25">[26]</ref>.</p><p>Theorem 1. Given an n×n matrix A, there is an O(n 5/2 )-time algorithm that optimally solves the projection problem (1), and returns the Monarch factors L and R.</p><p>We now derive this algorithm (Algorithm 1) by examining the structure of a Monarch matrix M. We first rewrite the steps of Monarch matrix-vector multiplication (i.e., computing Mx). The main idea is to view the input x, which is a vector of size n = m 2 , as a 2D tensor of size m × m. Then the two matrices L and R in the Monarch parametrization M = PLP R correspond to batched matrix multiply along one dimension of x, followed by batched matrix multiply along the other dimension of x. Thus we view x as a 2D tensor of size m × m, and each of L and R as a 3D tensor of size m × m × m.</p><p>Steps to multiply x by a Monarch matrix M = PLP R: 1. Multiply R by x: y kj = i R kji x ki , to obtain an output y that is a 2D tensor of size m × m. 2. Multiply PLP by y: z j = k L j k y kj , to obtain an output that is a 2D tensor of size m × m. 3. Reshape z back into a vector of size n, and return this. We can thus write the output z as z j = k,i L j k R kji x ki . Since M = PLP R, we can write:</p><formula xml:id="formula_11">M jki = L j k R kji .<label>(2)</label></formula><p>Note that here we view M as a 4D tensor of size m × m × m × m.</p><p>When viewed as a 4D tensor, the structure of the matrix M becomes apparent, and the solution to the projection problem is easy to see. Let's examine Eq. ( <ref type="formula" target="#formula_11">2</ref>): M jki = L j k R kji . We see that this reshaped tensor version of M is simply m • m batches of rank-1 matrices: we batch over the dimensions k and j, and each batch is simply a rank-1 matrix (p jk )(q jk ) for some length-m vectors p jk , q jk . Therefore, the projection objective (Eq. ( <ref type="formula" target="#formula_10">1</ref>)) can be broken up into the sum of m • m independent terms, each term corresponding to a block of A of size m × m. As the structure of a Monarch matrix forces each block to have rank 1 as described above, the solution to the projection problem becomes apparent: given a matrix A, reshape it to a 4D tensor of size m × m × m × m, and take the rank-1 approximation of each batch with the SVD, which (after reshaping) yields the factors L, R of the desired matrix M ∈ M. (Note that if A ∈ M itself, this algorithm recovers the factors such that A = PLP R.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Projection on the set of Monarch matrices</head><formula xml:id="formula_12">Require: Matrix A ∈ R n×n , with n = m 2 . Reshape A into a 4D tensor A of size m × m × m × m, where A jki = A ( -1)m+j,(k-1)m+i for , j, k, i = 1, . . . , m. for 1 ≤ j, k ≤ m do Let M jk = A :,j,k,: of size m × m.</formula><p>Compute the best rank-1 approximation of M jk as u jk v jk with the SVD of A. end for Let R be the m × m × m tensor where R kji = (v jk ) i . Let L be the m × m × m tensor where L j k = (u jk ) . Return L, R as block-diagonal matrices L, R (where the b th block of L, R are L b,:,: , R b,:,: respectively)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Factorization of MM * Matrices</head><p>In the previous section, we saw how to project onto the set M. As Theorem 3.2 shows, the broader class MM * also encompasses many important linear transforms. In this section, we present an algorithm to compute the Monarch factorization of a given matrix M ∈ MM * , under mild assumptions. This allows us to store and apply M efficiently.</p><p>Specifically, observe that if To understand how to do this, define M = P MP and observe that M </p><formula xml:id="formula_13">M ∈ MM * , we can write M = (PLP R)(R * PL * P ) = (PL 1 P )R(PL 2 P ) for block-diagonal L 1 , L 2 , R</formula><formula xml:id="formula_14">= L 1 (PRP )L 2 =    A1 A2 . . . Am       D11 D12 . . . D1m D21 D22 . . . D2m . . . . . . . . . . . . Dm1 Dm2 . . . Dmm       C1 C2 . . . Cm   </formula><formula xml:id="formula_15">M ij is equal to A i D ij C j .</formula><p>Notice that M is invertible only if all the A i 's and C j 's are (since if any one of these is singular, then L 1 or L 2 is singular).</p><p>Thus, our goal is to find matrices Â1 , . . . , Âm , Ĉ1 , . . . , Ĉm and diagonal matrices D11 , . . . , Dmm such that M ij = Âi Dij Ĉj for all i, j; this represents a valid Monarch factorization of M.</p><p>To provide intuition for how to do this, let's analyze a simple case in which all the D ij 's are the identity matrix. Then we have the set of equations A i C j = M ij . Again assume the A i 's and C j 's are invertible, so each M ij is as well. Suppose we set Ĉ1 = I (identity matrix). Then we can immediately read off Âi = M i1 for all i. We can then set Ĉj = Â-1</p><p>1 M 1j for all j. Let's now check that this strategy gives a valid factorization, i.e., that M ij = Âi Ĉj for all i, j. We have Âi Ĉj = M i1 M -1 11 M 1j . Recalling that in the "true" factorization we have</p><formula xml:id="formula_16">M ij = A i C j , this equals (A i C 1 )(A 1 C 1 ) -1 (A 1 C j ) = A i C j , as desired.</formula><p>In the general case, we must deal with the diagonal D ij matrices as well. We will no longer be able to freely set Ĉ1 = I. However, once we find a proper choice of Ĉ1 , we can use it to find all the Âi 's and Ĉj 's. We can find such a Ĉ1 via the idea of simultaneous diagonalization; for space reasons, we defer a full description of our algorithm (Algorithm 2), and its analysis, to Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Using Monarch Matrices in Model Training</head><p>We can use our class of Monarch matrices to parameterize weight matrices of deep learning models in several settings.</p><p>• In the E2E sparse training setting, we replace the dense weight matrices of a baseline model with Monarch matrices with the same dimension, initialize them randomly, and train as usual. Most of our baseline models are Transformers, and we replace the projection matrices in the attention blocks, along with the weights of the feed-forward network (FFN) blocks, with Monarch matrices. The Monarch parameterization is differentiable, and we rely on autodifferentiation to train with first-order methods such as Adam <ref type="bibr" target="#b56">[57]</ref>. • In the S2D training setting, we first replace the dense weight matrices of a baseline model with Monarch matrices, then train the sparse model for about 90% of the usual number of iterations. We then convert the Monarch matrices to dense matrices (by simply multiplying the factors L and R along with permutations), and continue training for the remaining 10% of the iterations. Compared to dense end-to-end training, we train for the same number of iterations, but the first 90% of the iterations are faster due to the hardware efficiency of Monarch matrices. • In the D2S fine-tuning setting, we start with a dense pretrained model (e.g., BERT), and project the dense weight matrices (e.g., in the attention blocks and FFN blocks) on the set of Monarch matrices using the algorithm in Section 3.3. We then fine-tune the resulting model on downstream tasks (e.g., GLUE), using first-order methods. We typically set the number of blocks in the block-diagonal matrices to be between 2 and 4 based on the parameter budgets (25% -50% of the dense model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We validate our approach empirically, showing that our Monarch matrix parametrization achieves a favorable efficiency-accuracy tradeoff compared to baselines on a wide range of domains (text, images, PDEs, MRI), in three settings (E2E training, S2D training, and D2S fine-tuning):</p><p>• In Section 5.1.1, on image classification and language modeling benchmarks, such as ViT / MLP Mixer on ImageNet and GPT-2 on Wikitext-103, Monarch is 2× faster to train than dense models, while achieving the same accuracy / perplexity. In Section 5.1.2, in scientific and medical domains where special transforms (Fourier) are common, Monarch outperforms Fourier transform based methods on PDE solving, with up to 40% lower error, and on MRI reconstruction attains up to 15% higher pSNR and 3.8% higher SSIM.</p><p>• In Section 5.1.2, we show that on the large OpenWebText dataset, reverse sparsification (training with Monarch weight matrices for most of the time, then transitioning to dense weight matrices) speeds up the pretraining of GPT-2 models by 2× compared to the dense model, with no loss in upstream or downstream quality. Moreover, reverse sparsification speeds up BERT pretraining by 23% even compared to the implementation from Nvidia that set the MLPerf [72] 1.1 record. • In Section 5.3, as a proof of concept, we demonstrate that our Monarch approximation algorithm can improve fine-tuning efficiency for pretrained models. We show that compressing BERT to a Monarch matrix model performs comparably to a finetuned dense model on GLUE, with 2× fewer parameters and 1.7× faster finetuning speed. 5.1 End-to-End Training 5.1.1 Benchmark Tasks: Image Classification, Language Modeling We show that replacing dense matrices with Monarch matrices in ViT, MLP-Mixer, and GPT-2 can speed up training by up to 2× without sacrificing model quality in Tables <ref type="table" target="#tab_2">1</ref> and <ref type="table" target="#tab_3">2</ref>.</p><p>Setup. We use the popular vision benchmark, ImageNet <ref type="bibr" target="#b16">[17]</ref>. We choose recent popular Vision Transformer <ref type="bibr" target="#b23">[24]</ref>, and MLP-Mixer <ref type="bibr" target="#b98">[99]</ref> as representative base dense models. For language modeling, we evaluate GPT-2 <ref type="bibr" target="#b85">[86]</ref> on WikiText-103 <ref type="bibr" target="#b72">[73]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">PDE solving and multi-coil MRI reconstruction</head><p>Many scientific or medical imaging tasks rely on specialized transforms such as the Fourier transform. We show that replacing the fixed Fourier transform with the more expressive Monarch matrices yields higher model quality (lower reconstruction error) with comparable model speed.</p><p>Solving PDEs with Monarch Neural Operators. We follow the experimental setting in FNO <ref type="bibr" target="#b64">[65]</ref> and apply a Monarch-based neural operator to the task of solving the Navier-Stokes PDE. Compared to baseline U-Nets <ref type="bibr" target="#b89">[90]</ref>, TF-Nets <ref type="bibr" target="#b102">[103]</ref>, ResNets <ref type="bibr" target="#b46">[47]</ref> and FNOs <ref type="bibr" target="#b64">[65]</ref>, neural operators based on Monarch improve solution accuracy across spatial resolutions by up to 40% (Table <ref type="table">3</ref>).</p><p>Non-periodic boundary conditions. Traditional spectral methods based on Fourier transform work best with periodic boundary conditions and forcing terms. However, PDEs of practical interest often exhibit non-periodic or even unknown boundary conditions. Monarch operators are not constrained to the Fourier transform and can thus still learn the solution operator with excellent accuracy.</p><p>Table 3: Benchmarks on Navier-Stokes (fixing resolution 64 × 64 for both training and testing). Decreasing the viscosity coefficient ν makes the dynamics more chaotic.</p><formula xml:id="formula_17">Model v = 10 -3 v = 10 -4 v = 10</formula><p>-5 U-Net 0.025 0.205 0.198 TF-Net 0.023 0.225 0.227 ResNet 0.070 0.287 0.275 FNO 0.017 0.178 0.155 Monarch-NO 0.010 0.145 0.136</p><p>Accelerated MRI Reconstruction. We characterize the utility of Monarch-based FFT operations for accelerated MRI reconstruction, a task which requires methods with both structured Fourier operators and dealiasing properties to recover high quality images. On the clinically-acquired 3D MRI SKM-TEA dataset <ref type="bibr" target="#b19">[20]</ref>, Monarch-SENSE (mSENSE) enhances image quality by over 1.5dB pSNR and 2.5% SSIM compared to zero-filled SENSE and up to 4.4dB and 3.8% SSIM compared to U-Net baselines in data-limited settings. Setup details are available in Appendix E.5.</p><p>Expressive FFT. By definition, standard IFFT in zero-filled SENSE cannot dealias the signal, resulting in artifacts in the reconstructed image. mSENSE replaces the inverse FFT (IFFT) operation in standard SENSE with learnable Monarch matrices. Thus, mSENSE preserves the structure of the Fourier transform while learning to reweight frequencies to suppress aliasing artifacts. Across multiple accelerations, mSENSE achieved up to +1.5dB and 2.5% improvement in peak signal-to-noise ratio (pSNR) and structural similarity (SSIM), respectively (Table <ref type="table" target="#tab_6">4</ref>). Data Efficiency. While CNNs have shown promise for MRI reconstruction tasks, training these networks requires extensive amounts of labeled data to avoid overfitting. However, large data corpora are difficult to acquire in practice. mSENSE can be trained efficiently with limited supervised examples. In few shot settings, mSENSE can outperform U-Net by +4.4dB (≈15%) and 3.8% SSIM (Table <ref type="table" target="#tab_7">5</ref>).  in 2× less time. We also evaluate its downstream quality on zero-shot generation from <ref type="bibr" target="#b33">[34]</ref> and classification tasks from <ref type="bibr" target="#b107">[108]</ref>, achieving comparable performance to the dense counterparts (Table <ref type="table" target="#tab_8">6</ref>). In Fig. <ref type="figure" target="#fig_8">5</ref> BERT pretraining. On the Wikipedia + BookCorpus datasets <ref type="bibr" target="#b109">[110]</ref>, we train a BERT-large model with Monarch weight matrices for 70% of the time and transition to dense weight matrices for the remaining 30% of the time, which yields the same pretraining loss as conventional dense training. In Table <ref type="table">7</ref>, we compare the total training time to several baseline implementations: the widely-used implementation from HuggingFace <ref type="bibr" target="#b103">[104]</ref>, the more optimized implementation from Megatron <ref type="bibr" target="#b93">[94]</ref>, and the most optimized implementation we know of from Nvidia that was used to set MLPerf 1.1 training speed record. Our method is 3.5x faster than HuggingFace and 23% faster than Nvidia's MLPerf 1.1 implementation <ref type="foot" target="#foot_2">3</ref> . Experiment details are in Appendix E.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dense-to-Sparse Fine-tuning</head><p>We show that our Monarch approximation algorithm allows us to efficiently use pretrained models, such as speeding up BERT finetuning on GLUE.</p><p>BERT finetuning. We take the BERT pretrained weights, approximate them with Monarch matrices, and finetune the resulting model on the 9 GLUE tasks. The results in Table <ref type="table" target="#tab_9">8</ref> shows that we obtain a Monarch finetuned model with similar quality to the dense BERT model, but with 1.7× faster finetuning speed. This   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extended Related Work</head><p>In this section, we extend the related works referenced in the main paper and discuss them in detail.</p><p>Sparse Training. Our work is loosely related to neural network pruning. By iteratively eliminating neurons and connections, pruning has seen great success in compressing complex models. Han et al. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> put forth two naive but effective algorithms to compress models up to 49x and maintain comparable accuracy. Li et al. <ref type="bibr" target="#b63">[64]</ref> employ filter pruning to reduce the cost of running convolution models up to 38 %, Lin et al. <ref type="bibr" target="#b65">[66]</ref> prunes the network at runtime, hence retaining the flexibility of the full model. Dong et al. <ref type="bibr" target="#b22">[23]</ref> prunes the network locally in a layer by layer manner. Sanh et al. <ref type="bibr" target="#b91">[92]</ref> prunes with deterministic first-order information, which is more adaptive to pretrained model weights. Lagunas et al. <ref type="bibr" target="#b59">[60]</ref> prunes transformers models with block sparsity pattern during fine-tuning, which leads to real hardware speed up while maintaining the accuracy.</p><p>Zhu &amp; Gupta <ref type="bibr" target="#b108">[109]</ref> finds large pruned sparse network consistently outperform the small dense networks with the same compute and memory footprints. Although both our and all the pruning methods are aiming to produce sparse models, we differ in our emphasis on the overall efficiency, whereas pruning mostly focuses on inference efficiency and disregards the cost in finding the smaller model. There has been more recent work on sparse methods that focuses on speeding up training and not just inference, such as SNFS <ref type="bibr" target="#b20">[21]</ref>, RigL <ref type="bibr" target="#b20">[21]</ref>, Top-KAST <ref type="bibr" target="#b49">[50]</ref>. These methods often focus on FLOP counts, which may not correlate well with wall-clock time on modern hardware (e.g., GPUs). Block-sparsity is another approach that exploits the block-oriented nature of GPUs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41]</ref>. Sparse models have also been found useful to improve the training process of dense models. For example, sparsity can be used to regularize dense models to improve accuracy <ref type="bibr" target="#b45">[46]</ref>, or to alternate between sparse and dense training to ease deployment <ref type="bibr" target="#b81">[82]</ref>. Our sparse-to-dense reverse sparsification instead focuses on speeding up dense training, where the sparse model is used for efficiency and not regularization.</p><p>In addition, models proposed in our work can be roughly seen as a class of manually constructed lottery tickets. Lottery tickets Frankle &amp; Carbin <ref type="bibr" target="#b29">[30]</ref> are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks in convergence speed and potentially in generalization. A huge number of studies are carried out to analyze these tickets both empirically and theoretically: Morcos et al. <ref type="bibr" target="#b74">[75]</ref> proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with the specialized lottery tickets; Frankle et al. <ref type="bibr" target="#b30">[31]</ref> improves the stability of the lottery tickets by iterative pruning; Frankle et al. <ref type="bibr" target="#b31">[32]</ref> found that subnetworks reach full accuracy only if they are stable against SGD noise during training; Orseau et al. <ref type="bibr" target="#b77">[78]</ref> provides a logarithmic upper bound for the number of parameters it takes for the optimal sub-networks to exist; Pensia et al. <ref type="bibr" target="#b80">[81]</ref> suggests a way to construct the lottery ticket by solving the subset sum problem and it's a proof by construction for the strong lottery ticket hypothesis. Furthermore, follow-up works <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b95">96]</ref> show that we can find tickets without any training labels.</p><p>Structured matrices and butterfly matrices. Structured matrices are those with asymptotically fast matrix-vector multiplication algorithm (o(n 2 ) time complexity) and few parameters (o(n 2 ) space complexity). Common examples include sparse &amp; low-rank matrices, and fast transforms such as Fourier transform, Chebyshev transform, Legendre transform, and more generally orthogonal polynomial transforms. These transforms have been widely used in data preprocessing (e.g., DFT in speech processing <ref type="bibr" target="#b51">[52]</ref>) and kernel approximation <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b105">106]</ref>. Many generalizations of these transforms have been used in machine learning to replace dense weight matrices <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b39">40]</ref>. De Sa et al. <ref type="bibr" target="#b15">[16]</ref> shows that any structured matrix (in the form of arithmetic circuits) can be written as product of sparse matrices, and Dao et al. <ref type="bibr" target="#b12">[13]</ref> shows that products of butterfly matrices can represent these structured matrices almost optimally in terms of runtime and memory. The class of butterfly matrices <ref type="bibr" target="#b79">[80]</ref> have also been used in kernel models <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b7">8]</ref> and deep learning models <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Neural Operators for PDEs. Deep learning has found application in the domain of differential equations and scientific computing <ref type="bibr" target="#b84">[85]</ref>, with methods developed for prediction and control problems <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b70">71]</ref>, as well as acceleration of numerical schemes <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b50">51]</ref>. Specific to the partial differential equations (PDEs) are approaches designed to learn solution operators <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b64">65]</ref>, and hybridized solvers <ref type="bibr" target="#b58">[59]</ref>, evaluated primarily on classical fluid dynamics.</p><p>The promise of these approaches is to offer, at the cost of an initial training procedure, accurate yet faster solutions than an appropriate numerical method tuned for a specific problem, which can then be leveraged for real-time forecasting or within larger feedback loops. Nonetheless, optimal design of neural operators remains an open problem, with most relying on fast Fourier transforms (FFT) or standard dense neural architectures. Instead, neural operators based on Monarch are capable of approximating all fast transforms, thus allowing automated optimization towards a suitable transform on a given PDE problem.</p><p>MRI. Accelerated multi-coil MRI is an essential mechanism for reducing long scan times and making certain scan types feasible. In multi-coil MRI, data is acquired in the spatial Fourier domain (a.k.a k-space) across multiple coils (sensors). To reduce scan time, this data is sampled below the required rate for recovering the underlying signal (i.e. Nyquist rate), which results in signal aliasing (see Appendix E.5). In these settings, direct application of the inverse fast Fourier transform (FFT) cannot suppress aliasing artifacts.</p><p>Classical MRI reconstruction approaches supplement the FFT by leveraging shared information across multiple coils and strong analytical priors to regularize image recovery objectives. SENSE-based methods jointly dealias images across multiple coils and reweight the final image based on the spatial sensitivity profile of each coil <ref type="bibr" target="#b83">[84]</ref>. Compressed sensing promotes image sparsity in transformation domains (e.g. Fourier, wavelet) while enforcing data consistency between the Fourier transform of the reconstructed image and the observed measurements <ref type="bibr" target="#b68">[69]</ref>. Low-rank methods enforce low rank structure across slowly-varying dimensions or local patches in the data <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b41">42]</ref>. Additionally, GRAPPA-based techniques optimize kernels to directly interpolate missing k-space samples to promote smoothness in the Fourier domain <ref type="bibr" target="#b38">[39]</ref>. Despite their efficacy, these methods have long reconstruction times, require explicit analytical priors, and require careful hyperparameter fine-tuning.</p><p>CNNs have shown promise as a fast-at-inference, learnable alternative to classical MRI reconstruction methods <ref type="bibr" target="#b57">[58]</ref>. In supervised learning, fully convolutional networks (e.g. U-Net <ref type="bibr" target="#b89">[90]</ref> or unrolled networks <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b42">43]</ref>) learn a mapping between paired zero-filled and fully-sampled, ground truth images. However, supervised methods require a large fully-sampled (labeled) data corpus and are sensitive to distribution drifts due to patient, hardware, and sequence heterogeneity <ref type="bibr" target="#b14">[15]</ref>. To reduce dependence on labeled data, unsupervised methods have used generative adversarial networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b69">70]</ref>, self-supervised learning <ref type="bibr" target="#b104">[105]</ref>, dictionary learning <ref type="bibr" target="#b60">[61]</ref>, and untrained networks <ref type="bibr" target="#b13">[14]</ref>. Despite their label efficiency, these techniques still underperform supervised methods and are also sensitive to distribution shift. Recently, a family of semi-supervised reconstruction methods demonstrated label efficiency and robustness to physics-driven perturbations, such as changes in signalto-noise ratio or patient motion <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. However, these methods require large amounts of unlabeled data, which can be difficult to curate in few-shot settings. Thus, despite their success in controlled environments, prospective clinical deployment of these models has been stifled <ref type="bibr" target="#b4">[5]</ref>.</p><p>In our work, we propose a model with a single FFT-initialized factorized Monarch matrix. Such a matrix can provide the benefits of both a simple linearized transformation like FFT and a learnable mechanism to remove aliasing artifacts resulting from the undersampled k-space. The smaller learnable parameter set may reduce overfitting in data-limited settings while preserving the transformation structure of Fourier matrices. Thus, our approach can be interpreted as a hybrid between analytically-constrained classical methods and data-dependent CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Notation Review</head><p>Throughout this paper, we use lowercase to denote scalars (e.g., k), lowercase boldface to denote vectors (e.g., v), and uppercase boldface to denote matrices (e.g., A).</p><p>I denotes the identity matrix. We use A to denote the transpose of a matrix and A * to denote the conjugate transpose of a matrix. All results in this paper apply to matrices over the either the reals R or the complex numbers C; when the field under consideration can be either one of these, we denote it by F.</p><p>We use 1-indexing throughout this paper except where explicitly stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C General Monarch Matrix Parametrization</head><p>In Section C.1, we define a parametrization for square Monarch matrices of different "block sizes" (i.e., not necessarily √ n), and prove some basic properties about them. In Section C.2, we further extend this to define rectangular Monarch matrices, and prove some basic properties about them.</p><p>Note: In this section, we use 0-indexing rather than 1-indexing, for notational convenience.</p><p>C.1 General square matrices</p><formula xml:id="formula_18">C.1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Parametrization</head><p>In this section, we define a more general Monarch parametrization for square matrices, allowing for different "block sizes." Like Definition 3.1, the parametrization involves the product of a permuted block-diagonal matrix with another block-diagonal matrix; the difference is that we now allow the matrices L and R to have diagonal blocks of different sizes. Thus, the permutations applied to L (to turn it into a block matrix where each block matrix is diagonal) will correspondingly also be different. First, in Definition C.1, we define notation for a class of block-diagonal matrices.</p><p>Definition C.1 (Class BD (b,n) ). Let b ∈ (1, n) be an integer that divides n. For 0 ≤ i &lt; n b , let R i ∈ F b×b be a b × b "block" matrix. Then define the matrix R with block size b as follows:</p><formula xml:id="formula_19">R = diag R 0 , . . . , R n b -1 . (<label>3</label></formula><formula xml:id="formula_20">) (Note that the number of possible nonzero values in R is n b • b 2 = nb.)</formula><p>We denote the class of all matrices R expressible in this form by BD (b,n) . Note that this class is closed under (conjugate) transposition and contains the identity matrix.</p><p>Next, in Definition C.2, we define notation for a class of block matrices whose blocks are diagonal.</p><p>Definition C.2 (Class DB (b,n) ). Let b ∈ (1, n) be an integer that divides n. For 0 ≤ i, j &lt; b, let D i,j ∈ F b×b be a b × b diagonal matrix. Then let L be an n × n matrix with the following form:</p><formula xml:id="formula_21">L =    D 0,0 . . . D 0, n b -1 . . . . . . . . . D n b -1,0 . . . D n b -1, n b -1   <label>(4)</label></formula><p>(Note that the number of possible nonzero values in</p><formula xml:id="formula_22">L is n b 2 • b = n 2 b .)</formula><p>We denote the class of all matrices L expressible in this form by DB (b,n) . Note that this class is closed under (conjugate) transposition and contains the identity matrix. As we show in Appendix C.1.2, L can be written as a block-diagonal matrix with b blocks of size n b × n b (i.e., a matrix in BD ( n b , n) ), multiplied on the left and right with appropriate permutation matrices. We denote the class of all matrices L expressible in this form by DB (b,n) . Note that this class is closed under (conjugate) transposition. As we show in Appendix C.1.2, L can be written as a block-diagonal matrix with b blocks of size n b × n b (i.e., a matrix in BD ( n b , n) ), multiplied on the left and right with appropriate permutation matrices.</p><p>Using these two definitions, we define the class of Monarch matrices with a given block size.</p><p>Definition C.3 (Class M (b,n) ). Let b ∈ (1, n) be an integer that divides n. A Monarch matrix of size n × n and "block size b" is a matrix of the form:</p><formula xml:id="formula_23">M = LR<label>(5)</label></formula><p>where L ∈ DB (b,n) and R ∈ BD (b,n) .</p><p>We denote the class of all matrices M expressible in this form by M (b,n) . Observe that when b = √ n, this is exactly the matrix class M (n) in Definition 3.1. (In other words, M (n) is shorthand for M ( √ n,n) .) Note that a matrix in M (b,n) is represented by n 2 b + nb parameters. We remark that M (b,n) ⊃ B (n) for all block sizes b ∈ (1, n) that divide n. Based on Definition C.19, we define the classes MM * (b,n) and M * M (b,n) :: n) . We define MM * (b,n) to be the the class of all matrices M expressible in the form M = M 1 M * 2 . We define M * M (b,n) to be the the class of all matrices M expressible in the form</p><formula xml:id="formula_24">Definition C.4 (Class MM * (b,n) , M * M (b,n) ). Let b ∈ (1, n) be an integer that divides n and suppose M 1 , M 2 ∈ M (b,</formula><formula xml:id="formula_25">M = M * 1 M 2 .</formula><p>Observe that when b = √ n, MM * (b,n) is exactly the matrix class MM * (n) defined in Section 3. Note that a matrix in MM * (b,n) or M * M (b,n) . is represented by 2 n 2 b + 2nb parameters. Finally, we define the following "Monarch hierarchy" based on the kaleidoscope hierarchy of <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_26">Definition C.5 (Class (MM * (b,n) ) w e )</formula><p>. Let b ∈ (1, n) be an integer that divides n. We define the matrix class (MM * (b,n) ) w e as the set of all matrices M that can be expressed as</p><formula xml:id="formula_27">M =   w i=1 M i   [1 : n, 1 : n]<label>(6)</label></formula><p>where each M i ∈ MM * (b,e•n) .</p><p>Note that a matrix in (MM * (b,n) ) w e is represented by 2w e 2 n 2 b + 2wenb parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Properties</head><p>Here we show some properties of the matrix classes defined above. We first show some basic equivalent ways to define these classes. We then show (Theorem 3) that the matrices in DB (b,n) are permuted blockdiagonal matrices; specifically, that they can be converted to matrices in BD ( n b ,n) by applying the appropriate permutation. Finally, we state an expressivity result for the general "Monarch hierarchy" which follows from Theorem 1 of <ref type="bibr" target="#b12">[13]</ref>.</p><p>First, we define a class of permutations. Let 1 ≤ b ≤ n be integers such that b divides n. We will need to express each index 0 ≤ i &lt; n in "block form." More specifically: Definition C.6. Let i ≥ 0, b ≥ 1 be integers. Then define i 0 = i mod b, and</p><formula xml:id="formula_28">i 1 = i b .</formula><p>We use the notation i ≡ (i 1 , i 0 ) b to denote the representation above. In particular, if i ≡ (i 1 , i 0 ) b , then we have</p><formula xml:id="formula_29">i = i 1 • b + i 0</formula><p>Using this notation, we define the following class of permutations:</p><formula xml:id="formula_30">Definition C.7. Let b ∈ [1, n] be an integer that divides n. Let i ≡ (i 1 , i 0 ) b . Define σ (b,n) (i) = i 0 • n b + i 1 .<label>(7)</label></formula><p>That is,</p><formula xml:id="formula_31">σ (b,n) (i) ≡ (i 0 , i 1 ) n b</formula><p>. Let P (b,n) denote the n × n permutation matrix defined by the permutation σ (b,n) .</p><p>Intuitively, P (b,n) can be interpreted as reshaping a length-n vector into an b × n b matrix in row-major order, transposing the result, and then flattening this back into a vector (again in row-major order). Now, we restate the formulation in Definition C.1 equivalently as:</p><p>Proposition C.8. A matrix R satisfies Equation (3) (i.e., R ∈ BD (b,n) ) if and only if the following holds for any 0 ≤ i, j &lt; n. Let i ≡ (i 1 , i 0 ) b and j ≡ (j 1 , j 0 ) b . Then 2. Else (i.e., when</p><formula xml:id="formula_32">i 1 = j 1 ), then R[i, j] = R i1 [i 0 , j 0 ].</formula><p>We restate the formulation in Definition C.2 equivalently as:</p><p>Proposition C.9. A matrix L satisfies Equation (4) (i.e., L ∈ DB (b,n) ) if and only if the following holds for any 0 ≤ i, j &lt; n. Let i ≡ (i 1 , i 0 ) b and j ≡ (j 1 , j 0 ) b . Then</p><formula xml:id="formula_33">1. if i 0 = j 0 , then L[i, j] = 0.</formula><p>2. Else, (i.e., when i 0 = j 0 ), then</p><formula xml:id="formula_34">L[i, j] = D i1,j1 [i 0 , i 0 ].</formula><p>We will argue the following:</p><formula xml:id="formula_35">Theorem 3. Let 1 ≤ b ≤ n such that b divides n.</formula><p>Recall that P (b,n) is the permutation matrix defined by the permutation σ (b,n) . Let L be a matrix in DB (b,n) . Then we have</p><formula xml:id="formula_36">R = P (b,n) • L • P (b,n) ,</formula><p>where R ∈ BD ( n b , n) .</p><p>Proof. We first note that multiplying an n × n matrix on the right (and left resp.) by P (b,n) = P ( n b ,n) (and P (b,n) resp.) permutes the columns (and rows resp.) of the matrix according to σ (b,n) . <ref type="foot" target="#foot_4">4</ref> This implies that for any 0 ≤ i, j &lt; n:</p><formula xml:id="formula_37">R [σ (b,n) (i), σ (b,n) (j)] = L[i, j].<label>(8)</label></formula><p>To complete the proof, we will argue that R satisfies the two conditions in Proposition C.8. Towards this end, let 0 ≤ i, j &lt; n be arbitrary indices and further, define i</p><formula xml:id="formula_38">= (i 1 , i 0 ) b and j = (j 1 , j 0 ) b . Then note that σ (b,n) (i) = (i 0 , i 1 ) n b and σ (b,n) (j) = (j 0 , j 1 ) n b .</formula><p>By Proposition C.9, we have that if i 0 = j 0 , then L[i, j] = 0. Note that i 0 = j 0 satisfies the pre-condition for base size n b for indices (σ (b,n) (i), σ (b,n) (j)) in item 1 in Proposition C.8. Then by Eq. ( <ref type="formula" target="#formula_37">8</ref>), we have that R [σ (b,n) (i), σ (b,n) (j)] = 0, which satisfies item 1 in Proposition C.8. Now consider the case that i 0 = j 0 ; then by item 2 in Proposition C.9, we have that L[i, j] = D i1,j1 [i 0 , i 0 ]. Note that i 0 = j 0 satisfies the pre-condition for base size n b for indices (σ (b,n</p><formula xml:id="formula_39">) (i), σ (b,n) (j)) in item 2 in Proposition C.8 if we define R i0 ∈ F n b × n b as follows: R i0 [i 1 , j 1 ] = D i1,j1 [i 0 , i 0 ]. Note that the above implies that R = diag R 0 , . . . , R b-1 ,</formula><p>where R • is as defined in the above paragraph. This means R ∈ BD</p><formula xml:id="formula_40">( n b ,n) , since each block R i0 is a matrix of size n b × n b .</formula><p>We now briefly note some alternate ways to express matrices in MM * (b,n) .</p><p>Proposition C.10. For any M ∈ MM * (b,n) , we can write M = (P (b,n) n) . Notice that since R * 1 , R 2 are both block-diagonal with the same structure (i.e., both have blocks of size b × b), their product R is also in BD (b,n) . Also, by Theorem 3 we can write</p><formula xml:id="formula_41">L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 ∈ BD ( n b ,n) and R ∈ BD (b,n) . Proof. By definition (see Definition C.1 and Definition C.2), if M ∈ MM * (b,n) , we can write M = (L 1 R 1 )(L 2 R 2 ) * = L 1 (R * 1 R 2 )L * 2 , where L 1 , L 2 ∈ DB (b,n) , R 1 , R 2 ∈ BD (b,</formula><formula xml:id="formula_42">L 1 = P (b,n) L 1 P (b,n) , L 2 = P (b,n) L 2 P (b,n) , where L 1 , L 2 are both in BD ( n b ,n) (i.e., block diagonal with blocks of size n b × n b ). Thus, we can write M = (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 ∈ BD ( n b ,n) and R ∈ BD (b,n) .</formula><p>least n 2 b ≥ n 3/2 free parameters (the entries in the blocks of the block-diagonal matrix P (b,n) LP (b,n) can be arbitrary, and there are b such blocks each of size n b ). Similarly, in the case b &gt; √ n, we can set L to the identity, and M has at least nb ≥ n 3/2 free parameters (the entries of the block-diagonal matrix R can be arbitrary, and there are nb total of these). Thus, at least n 3/2 parameters are required to uniquely describe any matrix in M (b,n) . However, a butterfly matrix in B (n) has only 2n log 2 n parameters. For n &gt; 256, 2n log 2 n &lt; n 3/2 . (Note that this analysis is not tight: a more careful analysis can show the inclusion is strict even for smaller values of n.)</p><p>We end this section with a theorem on the expressivity of the "monarch hierarchy" (products of monarch matrices), which follows from Theorem 1 of <ref type="bibr" target="#b12">[13]</ref>.</p><p>Theorem 5 (Monarch hierarchy expressivity). Let M be an n×n matrix such that matrix-vector multiplication of M and an arbitrary vector v (i.e., computation of Mv) can be represented as a linear arithmetic circuit with depth d and s total gates. Let b ∈ (1, n) be a power of 2 that divides n. Then, M ∈ (MM * (b,n) )</p><formula xml:id="formula_43">O(d) O(s/n) .</formula><p>Proof. Theorem 1 of Dao et al. <ref type="bibr" target="#b12">[13]</ref> says that if n is a power of 2 and A is an n × n matrix such that multiplying any vector v by A can be represented as a linear arithmetic circuit with depth ≤ d and ≤ s total gates, then A ∈ (BB * (n) )</p><formula xml:id="formula_44">O(d)</formula><p>O(s/n) (this is the "kaleidoscope representation" of A). Recall from Theorem 4 that for any b ∈ (1, n) that is a power of 2 and divides n, M (b,n) ⊃ B (n) ; thus, this implies MM * (b,e•n) ⊃ BB * (e•n) , and in turn (MM * (b,n) ) w e ⊃ (BB</p><formula xml:id="formula_45">* (n) ) w e . As A ∈ (BB * (n) ) O(d) O(s/n) , we thus have A ∈ (MM * (b,n) ) O(d) O(s/n) .</formula><p>As per <ref type="bibr" target="#b12">[13]</ref>, the class of kaleidoscope matrices (BB * (n) )</p><formula xml:id="formula_46">O(d)</formula><p>O(s/n) has O(ds log s) parameters and runtime, compared to the O(s) parameters and runtime of the circuit. Note that at worst, s is O(n 2 ).</p><p>Define f (n, s) to be the largest power of 2 that is </p><formula xml:id="formula_47">≤ min n 2 , √ s . Note that f (n, s) = O( √ s), and since s = O(n 2 ), f (n, s) = Ω( √ s), so f (n, s) = Θ( √ s). We thus have A ∈ (MM * (f (n,s),n) ) O<label>(d)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 General rectangular matrices</head><p>In this section, we extend the Monarch parametrization to apply to rectangular matrices, and prove some basic properties of the relevant matrix classes. (Note that our subsequent theoretical results (Appendix D) do not depend on this section, as they focus on the square parametrization.) For the rest of the section, we will assume that n 1 , n 2 , n 3 , b 1 , b 2 , b 3 ≥ 1 are integers such that:</p><p>• b i divides n i for all 1 ≤ i ≤ 3, and</p><formula xml:id="formula_48">• n1 b1 = n2 b2 .</formula><p>We begin with the definition of the following class of rectangular block-diagonal matrices:</p><formula xml:id="formula_49">Definition C.14. For 0 ≤ i &lt; n b1 , let R i ∈ F b2×b1 be a b 2 × b 1 matrix. Then define the matrix R ∈ F n2×n1 as follows: R = diag R 0 , . . . , R n 1 b 1 -1 .<label>(9)</label></formula><p>We say that R has block size b 2 × b 1 . Recall that we have assumed n1 b1 = n2 b2 , so Eq. ( <ref type="formula" target="#formula_49">9</ref>) is well-defined. (Note that the number of possible nonzero values in R is</p><formula xml:id="formula_50">n1 b1 • b 1 × b 2 = n 1 b 2 .</formula><p>) We denote the class of all matrices R expressible in this form by BD (b2×b1,n2×n1) . Note that this class is only defined when n1 b1 = n2 n2 . We restate the above definition equivalently as:</p><formula xml:id="formula_51">Proposition C.15. R ∈ F n2×n1 is in BD (b2×b1,n2×n1) (with n1 b1 = n2 n2</formula><p>) if and only if the following holds for any 0 ≤ i &lt; n 2 and 0 ≤ j &lt; n 1 . Let i ≡ (i 1 , i 0 ) b2 and j ≡ (j 1 , j 0 ) b1 (recalling this notation from Definition C.6. Then 1. if i 1 = j 1 , then R[i, j] = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Else (i.e., when i</head><formula xml:id="formula_52">1 = j 1 ), then R[i, j] = R i1 [i 0 , j 0 ].</formula><p>Before we define the rectangular L, we first need to define the notion of a 'wrapped diagonal' matrix: We now define the following class of block matrices with each block a wrapped diagonal matrix.</p><p>Definition C.17. Let L ∈ F n3×n2 have the form:</p><formula xml:id="formula_53">L =     S 0,0 . . . S 0, n 2 b 2 -1 . . . . . . . . . S n 3 b 3 -1,0 . . . S n 3 b 3 -1, n 2 b 2 -1     ,<label>(10)</label></formula><p>where each S •,• is a wrapped diagonal matrix in F b3×b2 .</p><p>We say that L has block size b 3 × b 2 . (Note that the number of possible nonzero values in L is</p><formula xml:id="formula_54">n2 b2 • n3 b3 max(b 2 , b 3 ) = n2•n3 min(b2,b3</formula><p>) .) We denote the class of all matrices L expressible in this form by DB (b3×b2,n3×n2) .</p><p>We restate the above definition equivalently as: n3×n2) if and only if the following holds for any 0 ≤ i &lt; n 3 and 0 ≤ j &lt; n 2 . Let i ≡ (i 1 , i 0 ) b3 and j ≡ (j 1 , j 0 ) b2 . Assuming b 2 ≤ b 3 , we have: </p><formula xml:id="formula_55">Proposition C.18. L ∈ F n3×n2 is in DB (b3×b2,</formula><formula xml:id="formula_56">1. if i 0 mod b 2 = j 0 , then L[i, j] = 0.</formula><formula xml:id="formula_57">M = LR<label>(11)</label></formula><p>where L ∈ DB (b3×b2,n3×n2) and R ∈ BD (b2×b1,n2×n1) .</p><p>(As mentioned before, we assume b i divides n i for i = 1, 2, 3 and that n 1 /b 1 = n 2 /b 2 .) We denote the class of all matrices M expressible in this form by M ((b1,b2,b3),(n1,n2,n3)) . Observe that when b 1 = b 2 = b 3 = b and n 1 = n 2 = n 3 = n, this is exactly the matrix class M (b,n) in Definition C. <ref type="bibr" target="#b18">19</ref>.</p><p>We are now ready to prove our main result in this section, which essentially follows from the observation that if we permute the rows and columns of L such that the row/column block size in L becomes the number of row/columns blocks in the permuted matrix (and vice-versa) then the permuted matrix has the form of R. Theorem 6. Let 1 ≤ b, n 2 , n 3 be such that b divides n 2 and n 3 . Suppose L ∈ F n3×n2 ∈ DB (b×b,n3×n2) . Then if we define</p><formula xml:id="formula_58">R = P (b,n3) • L • P (b,n2) , we have that R ∈ BD ( n 3 b 3 × n 2 b 2 ,n3×n2) .</formula><p>Proof. We recall that multiplying an m × n matrix on the right (and left resp.) by P (b,n) = P ( n b ,n) (and P (b,m) resp.) permutes the columns (and rows resp.) of the matrix according to σ (b,n) (and σ (b,m) ) respectively. <ref type="foot" target="#foot_5">5</ref>This implies that for any 0 ≤ i, j &lt; n:</p><formula xml:id="formula_59">R [σ (b,n3) (i), σ (b,n2) (j)] = L[i, j].<label>(12)</label></formula><p>Since the squared Frobenius norm objective A -M 2 F (Eq. ( <ref type="formula" target="#formula_10">1</ref>)) only depends on the entries of A and M and not their shape, we can rewrite the objective after reshaping:</p><formula xml:id="formula_60">A -M 2 F = jki (A jki -M jki ) 2 = jki (A jki -L j k R kji ) 2 = jk i (A jki -L j k R kji ) 2 .</formula><p>We see that the objective decomposes into m × m independent terms (indexed by j and k). For each value of j and k, the objective is exactly the rank-1 approximation objective for the corresponding slice A :,j,k,: . Let u jk v jk be the best rank-1 approximation of A :,j,k,: (which we can compute using the SVD, by the Eckart-Young theorem <ref type="bibr" target="#b25">[26]</ref> for Frobenius norm). Let R be the 3D tensor of size m × m × m where R kji = (v jk ) i , and let L be the 3D tensor of size m × m × m where L j k = (u jk ) . Then each of the terms in the objective is minimized, and thus the overall objective is minimized.</p><p>We see that the algorithm requires m • m SVD's, each of size m × m. Each SVD takes O(m 3 ) time <ref type="bibr" target="#b99">[100]</ref>, so the overall time complexity is O(m 5 ) = O(n 5/2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Monarch Factorizations for Matrices in MM *</head><p>In this section, we describe the algorithm for factorizing matrices in MM * previously outlined in Section 3.4 (Algorithm 2). Again, Algorithm 2 handles the general case where the block sizes of L and R can be different. We then prove Theorem 7, which has Theorem 2 as an immediate corollary.</p><p>Our goal is thus to compute the matrices L 1 , R, L 2 in the factorization of M. In order to compute this factorization, we require the following assumption on M: Assumption D.1. Assume that (1) M ∈ MM * (b,n) is invertible and (2) M can be written as (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) where L 1 , L 2 ∈ BD ( n b ,n) , R ∈ BD (b,n) , and R has no nonzero entries in its diagonal blocks. (Note that by Proposition C.10, we can write any M ∈ MM * (b,n) as (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ); thus, (2) is merely the assumption that R has no zero entries in its blocks.) This is analogous to Assumption 3.3, except applicable to the more general block size b. We now present Algorithm 2 to find factors L 1 , R, L 2 of matrices satisfying Assumption D.1.</p><p>First, observe that if we define M = P (b,n) MP (b,n) , we have M = L 1 (P (b,n) RP (b,n) )L 2 . By Theorem 3, the matrix P (b,n) RP (b,n) is in DB ( n b ,n) , i.e., is a block matrix with blocks of size n b × n b where each block is a diagonal matrix. Thus, we can write: Thus, we have the set of matrix equations A i D ij C j = M ij , for 1 ≤ i, j ≤ b. Notice that the assumption that the R has no nonzero entries in its blocks (Assumption D.1) is equivalent to assuming that none of the diagonal entries of any matrix D ij is equal to zero. Also, the assumption that M is invertible implies that L 1 , L 2 are invertible (since the product of square singular matrices is singular), which in turn implies that each block matrix A i and each block matrix C j is invertible (since a square block-diagonal matrix where one of the blocks is singular is itself singular). Taken together, this means that each matrix M ij is invertible, since M ij = A i D ij C j and each of the matrices on the RHS of the equation is invertible.</p><formula xml:id="formula_61">      M 11 M 12 . . . M 1b M 21 M 22 . . . M 2b . . . . . . . . . . . . M b1 M b2 . . . M bb       =      A 1 A 2 . . . A b           D 11 D 12 . . . D 1b D 21 D 22 . . . D 2b . . . . . . . . . . . . D b1 D b2 . . . D bb           C 1 C 2 . . . C b      , where<label>A</label></formula><p>Observe that given a solution to the set of equations A i D ij C j = M ij , if we rescale and permute the matrices A i , D ij , C j appropriately, the result is still a solution to the equations. Specifically, let P be any permutation matrix and {S i } b i=1 , {S j } b j=1 be any invertible diagonal matrices (i.e., diagonal matrices without any zeros on the diagonal). Define D ij = S i P D ij PS j for all i, j. Notice that P D ij P = P -1 D ij P is diagonal because D ij is diagonal. Thus, D ij is diagonal (and invertible) since the product of diagonal matrices is diagonal. Define A i = A i PS -1 i and C j = P S -1 j C j for all i, j. Thus, we have that</p><formula xml:id="formula_62">M ij = A i D ij C j = (A i PS -1 i )D ij (P S -1 j C j ) = A i D ij C j</formula><p>for all i, j: in other words, we can scale the A i 's on the right by any invertible diagonal matrix, the C j 's on the left by any invertible diagonal matrix, and apply a matching permutation to the rows of the C j 's and the columns of the A i 's, and apply matching transformations to the D ij 's and the result will still be a valid factorization. This implies that as long as we recover a "correct" Ĉ1 up to a permutation and scaling of its rows, we can set the Di1 's and D1j 's to the identity matrix, and then compute the remaining Âi 's and Ĉj 's via the equations Âi = M i1 Ĉ-1 1 and Ĉj = Â-1 1 M 1j . To understand how we can compute such a matrix Ĉ1 , define</p><formula xml:id="formula_63">F(i, j) = M -1 i1 M ij M -1 1j M 11 and observe that F(i, j) = M -1 i1 M ij M -1 1j M 11 = (C -1 1 D -1 i1 A -1 i )(A i D ij C j )(C -1 j D -1 1j A -1 1 )(A 1 D 11 C 1 ) = C -1 1 (D -1 i1 D ij D -1 1j D 11 )C 1 for all 1 ≤ i, j ≤ b. Note that D -1 i1 D ij D -1 1j D 11 is a diagonal matrix; thus, C 1 F(i, j)C -1</formula><p>1 is diagonal for all i, j, i.e., C 1 simultaneously diagonalizes all the matrices F(i, j). (Note: In this paper, we say that a matrix Q "simultaneously diagonalizes" a set of matrices G</p><formula xml:id="formula_64">1 , . . . , G k if QG i Q -1 is a diagonal matrix for all 1 ≤ i ≤ k.</formula><p>Note that sometimes the opposite convention [i.e., Q -1 G i Q must be diagonal] is used in the literature; we adopt the former for notational convenience.) Indeed, if any matrix simultaneously diagonalizes all these matrices, then it leads to a valid factorization, which we show in the proof of Theorem 7. Therefore, we compute some matrix that simultaneously diagonalizes all these matrices, and set Ĉ1 to that matrix.</p><p>These ideas form the basis of Algorithm 2, which is presented formally below. Algorithm 2 uses simultaneous diagonalization as a subroutine; we discuss how to solve simultaneous diagonalization problems below. maintains the property that QG j Q -1 is diagonal for all j &lt; i, since PDP is diagonal for any permutation P and diagonal matrix D). Then for each block of size &gt; 1, compute a matrix that diagonalizes that block; denoting the number of blocks (including size-1 blocks) by b, let Q 1 , . . . , Q b denote the corresponding diagonalizing transformations, or the scalar 1 when the block is of size 1. Finally set Q ← diag(Q 1 , . . . , Q b ) and Q ← Q -1 QQ . By construction, QG i Q -1 will now be diagonal; also, QG j Q -1 is still diagonal for all j &lt; i, because any linear combination of a set of eigenvectors of a diagonalizable matrix corresponding to a repeated eigenvalue λ is itself an eigenvector of that matrix with eigenvalue λ. Thus, once we've processed all k of the G i 's, Q is a matrix that simultaneously diagonalizes all of them. At each step i, we compute diagonalizing transformations for square block matrices whose sizes s 1 , . . . , s k sum to n. As eigendecomposition (for a fixed desired precision) takes O(n 3 ) time for an n × n matrix, this means the total runtime of step i is O k j=1 s 3 i ≤ O(n 3 ). Thus the total runtime of the entire simultaneous diagonalization procedure is O(kn 3 ), where k is the number of matrices. (Note that iterative methods for simultaneous diagonalization also exist <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref> and could be used to speed up this step in practice.)</p><p>Applying this to our problem, we have b for the entire simultaneous diagonalization procedure, and thus the runtime of Algorithm 2 is also O n 3 b , as desired. (Note: As can be seen from the above analysis, we don't actually need M itself to be invertible-we simply need all its blocks M ij to be, so that all the A i 's and C j 's are, which is a weaker assumption that invertibility of M given that we already assumed the D ij 's are invertible due to the nonzero assumption on the blocks of R.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Details for BERT Pretraining</head><p>We follow the training procedure and hyperparameters of the reference implementation from Nvidia Deep Learning examples (<ref type="url" target="https://github.com/NVIDIA/DeepLearningExamples">https://github.com/NVIDIA/DeepLearningExamples</ref>). In particular, we use the LAMB optimizer with learning rate 4e-3. We use as large a minibatch size as possible that still fits in the GPU memory (A100-40GB), and use gradient accumulation to reach an effective batch size of 64k sequences for phase 1 (maximum sequence length 128) and 32k for phase 2 (maximum sequence legnth 512). We train is mixed precision (fp16 and fp32).</p><p>We use all the optimizations that were in Nvidia's BERT implementation in MLPerf 1.1:</p><p>1. Only compute the prediction scores (last layer) for masked tokens as the outputs of other tokens are not used to compute the masked language modeling loss.</p><p>2. Remove padding tokens and only compute the attention for non-padding tokens.</p><p>3. Use a fused CUDA kernel (FMHA) that combines 4 steps into one kernel: computes QK T , take softmax, apply dropout, multiply by V , where Q, K, V are the query, key, and value respectively.</p><p>4. Fuse matrix multiplication and adding bias into one CUDA kernel in the feed-forward network (FFN) layers. The gradient of the bias is also fused with the matrix multiplication the backward pass.</p><p>5. Fuse matrix multiplication and adding bias into one CUDA kernel in the attention output projection.</p><p>6. Fuse dropout and adding residual in the residual connection at the end on the attention and FFN blocks.</p><p>We train with DeepSpeed <ref type="bibr" target="#b87">[88]</ref> ZeRO optimizer stage 1 to shard the optimizer states, thus reducing GPU memory usage and allowing us to use larger batch sizes. For the Nvidia MLPerf implementation, we report the speed for both Apex's automatic mix-precision (AMP) level O2 (as in the original implementation), and DeepSpeed ZeRO optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Accelerated Multi-coil MRI Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5.1 Background</head><p>In multi-coil MRI, multiple receiver coils (i.e. sensors) acquire complex-valued measurements in the spatial frequency (a.k.a. k-space) domain. These measurements are modulated by the spatially-varying sensitivity maps, which characterize the sensitivity of each coil to the imaging target. In accelerated MRI, scan times are reduced by decreasing the number of samples acquired in k-space. Because the data is sampled below the Nyquist rate, reconstructing the underlying image is an ill-posed problem.</p><p>The forward problem for accelerated multi-coil MRI can be written as the matrix equation</p><formula xml:id="formula_65">y = ΩF Sx +</formula><p>where Ω is the binary undersampling mask that indexes acquired samples in k-space, y is the vectorized measured signal in k-space, F is the discrete Fourier transform matrix, S is the receiver coil sensitivity maps, x is the ground-truth signal in image-space, and is additive complex Gaussian noise. The acceleration factor is given by R =   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Monarch matrices unlock several ways to train sparse and dense models: end-to-end training a sparse (Monarch) model can be 2x faster than dense training thanks to its hardware efficiency; sparse-to-dense "reverse sparsification" can speed up training of large models such as GPT-2; and our dense-to-sparse Monarch projection algorithm can transfer knowledge from pretrained dense model to Monarch model and speed up BERT fine-tuning.</figDesc><graphic coords="2,188.02,72.86,229.32,132.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Monarch matrices are parametrized as products of two block-diagonal matrices up to permutation, allowing efficient multiplication algorithm that leverages batch matrix multiply.</figDesc><graphic coords="5,199.04,72.86,210.60,100.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 n 2</head><label>2</label><figDesc>butterfly factor matrices in the butterfly factorization of B, and R by multiplying together the last log 2 n 2 butterfly factor matrices. (We detail this more rigorously in Theorem 4.) The matrix R is block-diagonal with m = √ n dense blocks, each block of size m×m: R = diag(R 1 , . . . , R m ). The matrix L is composed of m × m blocks of size m × m, where each block is a diagonal matrix:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: With the "reverse sparsification" process, Monarch matrices can speed up GPT-2 training by 2x.</figDesc><graphic coords="7,210.74,72.86,187.19,142.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Assumption 3 . 3 .Theorem 2 .</head><label>332</label><figDesc>and the permutation P of Definition 3.1. Then, we can compute L 1 , L 2 , R in such a factorization under Assumption 3.3, as stated in Theorem 2. (Note that the factorization is not unique.) Assume that (1) M ∈ MM * is invertible and (2) M can be written as (PL 1 P )R(PL 2 P ) where the blocks of R have no zero entries.Given an n × n matrix M ∈ MM * satisfying Assumption 3.3, there is an O(n 5/2 )-time algorithm to find its Monarch factors L 1 , R, L 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: With Algorithm 1 for our Monarch parameterization, we can convert a pretrained model into a model with Monarch weight matrices and speed up downstream fine-tuning. where m = √ n, the A i 's and C j 's denote the m × m diagonal blocks of L 1 , L 2 respectively, and each D ij is an m × m diagonal matrix. If we write M as a block matrix with m × m blocks each of size m × m, then we see that the blockM ij is equal to A i D ij C j .Notice that M is invertible only if all the A i 's and C j 's are (since if any one of these is singular, then L 1 or L 2 is singular).Thus, our goal is to find matrices Â1 , . . . , Âm , Ĉ1 , . . . , Ĉm and diagonal matrices D11 , . . . , Dmm such that M ij = Âi Dij Ĉj for all i, j; this represents a valid Monarch factorization of M.To provide intuition for how to do this, let's analyze a simple case in which all the D ij 's are the identity matrix. Then we have the set of equations A i C j = M ij . Again assume the A i 's and C j 's are invertible, so each M ij is as well. Suppose we set Ĉ1 = I (identity matrix). Then we can immediately read off Âi = M i1 for all i. We can then set Ĉj = Â-11 M 1j for all j. Let's now check that this strategy gives a valid factorization, i.e., that M ij = Âi Ĉj for all i, j. We have Âi Ĉj = M i1 M -1 11 M 1j . Recalling that in the "true" factorization we haveM ij = A i C j , this equals (A i C 1 )(A 1 C 1 ) -1 (A 1 C j ) = A i C j , as desired.In the general case, we must deal with the diagonal D ij matrices as well. We will no longer be able to freely set Ĉ1 = I. However, once we find a proper choice of Ĉ1 , we can use it to find all the Âi 's and Ĉj 's. We can find such a Ĉ1 via the idea of simultaneous diagonalization; for space reasons, we defer a full description of our algorithm (Algorithm 2), and its analysis, to Appendix D.</figDesc><graphic coords="8,199.04,72.86,210.60,80.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>, we show the training time of the dense GPT-2 model, along with the Monarch GPT-2 model. After training the Monarch model for 90% of the time, in the last 10% of the training steps, by transitioning to dense weight matrices, the model is able to reach the same performance of another model that was trained with dense weight matrices from scratch. By training with Monarch matrices for 90% of the time, we reduce the total training time by 2×.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Table 7 :</head><label>7</label><figDesc>The total training time of BERT-large trained with Monarch reverse sparsification and with conventional dense training on 8 A100-40GB GPUs (DGX A100). Training consists of two phases, phase 1 with sequence length 128 and phase 2 with sequence length 512. Monarch training is 3.5x faster than HuggingFace and 23% faster than Nvidia's MLPerf 1.1 implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Time required (in A100 GPU hours) to reach the same perplexity (18.0) for GPT-2-small on OpenWebText. With "reverse sparsification", Monarch can speed up GPT-2 training by 2×. serves as a proof of concept, and we expect further speedup if additional model compression techniques are applied (e.g., quantization, kernel fusion).</figDesc><graphic coords="12,234.14,198.86,140.41,116.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>O</head><figDesc>(s/n) . The class (MM * (f (n,s),n) ) O(d) O(s/n) has O(d s 2 f (n,s) + dsf (n, s)) = O(ds 3/2 ) parameters. Thus, the monarch representation of A is suboptimal by at most an O(d √ s) factor compared to the O(d log s) of kaleidoscope.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Definition C. 16 .</head><label>16</label><figDesc>A wrapped diagonal matrix S ∈ F b3×b2 is defined as follows. First assume b 2 ≤ b 3 . Then for any 0 ≤ i &lt; b 3 and 0 ≤ j &lt; b 2 , we have the following. If i mod b 2 = j, then S[i, j] = 0. (If b 2 &gt; b 3 , then instead apply the previous definition to S .)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 .</head><label>2</label><figDesc>Else, (i.e., when i 0 mod b 2 = j 0 ), thenL[i, j] = S i1,j1 [i 0 , j 0 ].If b 2 &gt; b 3 , then in the above, the condition "i 0 mod b 2 = j 0 " gets replaced by "j 0 mod b 2 = i 0 ."Using the above definitions, we now define the class of rectangular Monarch matrices.Definition C.19 (Rectangular Monarch Matrix). Let M ∈ F n3×n1 be a matrix of the form:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>1 , . . . , A b are n b × n b matrices that are the diagonal blocks of L 1 ; C 1 , . . . , C b are n b × n b matrices that are the diagonal blocks of L 2 ; D 11 , . . . , D 1b , D 21 , . . . , D 2b , . . . , D b1 , . . . , D bb are n b × n b diagonal matrices that are the blocks of P (b,n) RP (b,n) ; and M 11 , . . . , M 1b , M 21 , . . . , M 2b , . . . , M b1 , . . . , M bb are n b × n b matrices that are the blocks of M = P (b,n) MP (b,n) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Algorithm 2 1 : 1 3: for 1 ≤ i ≤ b do 3 : 1 14: end for 5 : for 2 ≤ j ≤ b do 5 :Theorem 7 .</head><label>2111315257</label><figDesc>MM * FactorizationRequire: Block size b; matrix M ∈ MM * (b,n) satisfying Assumption D.10: Define M ij (of size n b × n b ) as the i, j block of P (b,n) MP (b,n) 1: for 1 ≤ i, j ≤ b do Compute F(i, j) := M -1 i1 M ij M -1 1j M 11 2: end for 2: Ĉ1 ← SIMULTANEOUS DIAG {F(i, j)} b,b i,j=1,Âi ← M i1 Ĉ-Ĉj ← Â-1 1 M 1j 6:end for 7: for 1 ≤ i, j ≤ b do Given an n × n matrix M ∈ MM * (b,n) satisfying Assumption 3.3, Algorithm 2 finds its Monarch factors L 1 , R, L 2 in time O n 3 b .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2</head><label>2</label><figDesc>matrices to simultaneously diagonalize, each of size n b × n b . This leads to a total runtime of O b 2 • ( n b ) 3 = O n 3 b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sample reconstructions at 2x acceleration for the first echo in the SKM-TEA dataset using SENSE, Monarch-SENSE (mSENSE), and U-Net. Both mSENSE and U-Net are trained with 1 training scan. SENSE is an untrained method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sample reconstructions at 2x acceleration for the second echo in the SKM-TEA dataset using SENSE, Monarch SENSE (mSENSE), and U-Net. Both mSENSE and U-Net are trained with 1 training scan. SENSE is an untrained method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The performance of Monarch matrices and ViT / MLP-Mixer on ImageNet, including the number of parameters and FLOPs. We measure the Top-1 accuracy and the training time speedup compared to the corresponding dense model.</figDesc><table><row><cell>Model</cell><cell cols="4">ImageNet acc. Speedup Params FLOPs</cell></row><row><cell>Mixer-S/16</cell><cell>74.0</cell><cell>-</cell><cell>18.5M</cell><cell>3.8G</cell></row><row><cell>Monarch-Mixer-S/16</cell><cell>73.7</cell><cell>1.7×</cell><cell>7.0M</cell><cell>1.5G</cell></row><row><cell>Mixer-B/16</cell><cell>77.7</cell><cell>-</cell><cell cols="2">59.9M 12.6G</cell></row><row><cell>Monarch-Mixer-B/16</cell><cell>77.8</cell><cell>1.9×</cell><cell>20.9M</cell><cell>5.0G</cell></row><row><cell>ViT-S/16</cell><cell>79.4</cell><cell>-</cell><cell>48.8M</cell><cell>9.9G</cell></row><row><cell>Monarch-ViT-S/16</cell><cell>79.1</cell><cell>1.9×</cell><cell>19.6M</cell><cell>3.9G</cell></row><row><cell>ViT-B/16</cell><cell>78.5</cell><cell>-</cell><cell cols="2">86.6M 17.6G</cell></row><row><cell>Monarch-ViT-B/16</cell><cell>78.9</cell><cell>2.0×</cell><cell>33.0M</cell><cell>5.9G</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of Monarch matrices and GPT-2-Small/Medium on WikiText-103, including the # of parameters and FLOPs. Monarch achieves similar perplexity (ppl) but 2.0× faster.</figDesc><table><row><cell>Model</cell><cell cols="4">PPL Speedup Params FLOPs</cell></row><row><cell>GPT-2-Small</cell><cell>20.6</cell><cell>-</cell><cell>124M</cell><cell>106G</cell></row><row><cell>Monarch-GPT-2-Small</cell><cell>20.7</cell><cell>1.8×</cell><cell>72M</cell><cell>51G</cell></row><row><cell>GPT-2-Medium</cell><cell>20.9</cell><cell>-</cell><cell>355M</cell><cell>361G</cell></row><row><cell>Monarch-GPT-2-Medium</cell><cell>20.3</cell><cell>2.0×</cell><cell>165M</cell><cell>166G</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Mean ± standard error of the mean of conventional and Monarch-SENSE (mSENSE) on dual-echo (E1,E2) MRI reconstruction at multiple acceleration factors (Acc.).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">pSNR (dB) (↑)</cell><cell cols="2">SSIM (↑)</cell></row><row><cell>Acc.</cell><cell>Model</cell><cell>E1</cell><cell>E2</cell><cell>E1</cell><cell>E2</cell></row><row><cell>2</cell><cell>SENSE mSENSE</cell><cell>32.8±0.2 34.3±0.2</cell><cell>35.4±0.2 36.6±0.2</cell><cell>0.871±0.003 0.886±0.002</cell><cell>0.865±0.003 0.882±0.003</cell></row><row><cell>3</cell><cell>SENSE mSENSE</cell><cell>30.9±0.2 32.3±0.2</cell><cell>33.5±0.2 34.6±0.2</cell><cell>0.819±0.004 0.843±0.003</cell><cell>0.795±0.004 0.820±0.004</cell></row><row><cell>4</cell><cell>SENSE mSENSE</cell><cell>30.1±0.2 31.2±0.2</cell><cell>32.8±0.2 33.5±0.2</cell><cell>0.789±0.004 0.812±0.003</cell><cell>0.753±0.005 0.767±0.005</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Impact of number of training examples (N ) on dual-echo MRI reconstruction at 2x acceleration.GPT-2 pretraining. On the large OpenWebtext dataset<ref type="bibr" target="#b35">[36]</ref>, we train a GPT-2 model with Monarch weight matrices for 90% of the training iterations, then relax the constraint on the weight matrices and train them as dense matrices for the remaining 10% of the iterations. We call this technique "reverse sparsification." Previous sparse training techniques often don't speed up training, whereas our hardware-efficient Monarch matrices do. Therefore we can use them as an intermediate step to pretrain a large language model (GPT-2)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">pSNR (dB) (↑)</cell><cell cols="2">SSIM (↑)</cell></row><row><cell>N</cell><cell>Model</cell><cell>E1</cell><cell>E2</cell><cell>E1</cell><cell>E2</cell></row><row><cell>N/A</cell><cell>SENSE</cell><cell>32.8±0.2</cell><cell>35.4±0.2</cell><cell>0.871±0.003</cell><cell>0.865±0.003</cell></row><row><cell>1</cell><cell>U-Net mSENSE</cell><cell>29.4±0.2 33.8±0.2</cell><cell>34.4±0.3 36.0±0.2</cell><cell>0.848±0.004 0.886±0.003</cell><cell>0.857±0.004 0.867±0.003</cell></row><row><cell>2</cell><cell>U-Net mSENSE</cell><cell>29.9±0.3 34.0±0.2</cell><cell>35.1±0.3 36.4±0.2</cell><cell>0.858±0.003 0.883±0.002</cell><cell>0.871±0.003 0.877±0.003</cell></row><row><cell>3</cell><cell>U-Net mSENSE</cell><cell>31.0±0.3 33.9±0.2</cell><cell>35.2±0.3 36.5±0.2</cell><cell>0.866±0.003 0.882±0.002</cell><cell>0.867±0.004 0.878±0.003</cell></row><row><cell>5</cell><cell>U-Net mSENSE</cell><cell>31.4±0.3 33.9±0.2</cell><cell>35.6±0.2 36.5±0.2</cell><cell>0.877±0.002 0.881±0.002</cell><cell>0.870±0.003 0.877±0.003</cell></row><row><cell cols="5">5.2 Sparse-to-Dense Training (reverse sparsification)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The performance (accuracy) of GPT-2-medium trained with Monarch reverse sparsification and with conventional dense training on text classification benchmarks.</figDesc><table><row><cell>Model</cell><cell cols="3">OpenWebText (ppl) Speedup Classification (avg acc)</cell></row><row><cell>GPT-2m</cell><cell>18.0</cell><cell>-</cell><cell>38.9</cell></row><row><cell>Monarch-GPT-2m</cell><cell>18.0</cell><cell>2×</cell><cell>38.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The performance of Monarch matrices in finetuning BERT on GLUE.We propose Monarch, a novel matrix parameterization that inherits the expressiveness of butterfly matrices and thus can represent many fast transforms. Our parameterization leverages optimized batch matrix multiply routines on GPUs, yielding up to 2× speedup compared to dense matrix multiply. We derive an efficient algorithm for projecting an arbitrary dense matrix on the set of Monarch factors. Our algorithm allows us to easily fine-tune a pretrained model into a model with Monarch weight matrices. As a result, Monarch matrices unlock new ways for faster end-to-end training, sparse-to-dense training, and dense-to-sparse fine-tuning of large neural networks. By making structured matrices practical, our work is a first step towards unlocking tremendous performance improvements in applying sparse models to wide-ranging ML applications (including science and medicine). We anticipate this work can inspire more future work on advancing machine learning models for interdisciplinary research with limited computational resources.No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.</figDesc><table><row><cell>Model</cell><cell cols="4">GLUE (avg) Speedup Params FLOPs</cell></row><row><cell>BERT-base</cell><cell>78.6</cell><cell>-</cell><cell>109M</cell><cell>11.2G</cell></row><row><cell>Monarch-BERT-base</cell><cell>78.3</cell><cell>1.5×</cell><cell>55M</cell><cell>6.2G</cell></row><row><cell>BERT-large</cell><cell>80.4</cell><cell>-</cell><cell>335M</cell><cell>39.5G</cell></row><row><cell>Monarch-BERT-large</cell><cell>79.6</cell><cell>1.7×</cell><cell>144M</cell><cell>14.6G</cell></row><row><cell>6 Conclusion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>The performance (accuracy) of GPT-2-medium trained with Monarch reverse sparsification and with conventional dense training on text classification benchmarks.</figDesc><table><row><cell>Model</cell><cell cols="3">OpenWebText (ppl) Speedup Classification (avg acc)</cell><cell></cell></row><row><cell>GPT-2m</cell><cell>68.3</cell><cell>37.0</cell><cell>10.7</cell><cell>52.0 26.6</cell></row><row><cell>Monarch-GPT-2m</cell><cell>72</cell><cell>38.6</cell><cell>12.5</cell><cell>47.3 23.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>They are named after the monarch butterfly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Monarch code is available at https://github.com/HazyResearch/monarch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Our result is not an official MLPerf submission. We train BERT for both phase 1 (sequence length 128) and phase 2 (sequence length 512) according to the standard BERT training recipe<ref type="bibr" target="#b21">[22]</ref>, while MLPerf only measures training time for phase 2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3"><p>if i 1 = j 1 , then R[i, j] = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>This uses the fact that σ (b,n)-1 = σ ( n b ,n) (which means P ( n b ,n) = P (b,n) since the inverse of a permutation matrix is its transpose).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>This uses the fact that σ (b,n)-1 = σ ( n b ,n) .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Laurel Orr</rs>, <rs type="person">Xun Huang</rs>, <rs type="person">Trevor Gale</rs>, <rs type="person">Jian Zhang</rs>, <rs type="person">Victor Bittorf</rs>, <rs type="person">Sarah Hooper</rs>, <rs type="person">Neel Guha</rs>, and <rs type="person">Michael Zhang</rs> for their helpful discussions and feedback on early drafts of the paper.</p><p>We gratefully acknowledge the support of <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); <rs type="institution">ARL</rs> under</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nbbkj5J">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_pxuV77X">
					<idno type="grant-number">CCF1763315</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use the above to show a simple relationship between MM * (b,n) and M * M (b,n) .</p><p>Proposition C.11. If M ∈ MM * (b,n) , then P (b,n) MP (b,n) ∈ M * M ( n b ,n) . Conversely, if M ∈ M * M (b,n) , then P (b,n) MP (b,n) ∈ M * M ( n b ,n) .</p><p>Proof. Suppose M ∈ MM * (b,n) . By Proposition C.10 we can write M = (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 ∈ BD ( n b ,n) and R ∈ BD (b,n) . Thus P (b,n) MP (b,n) = L 1 (P (b,n) RP (b,n) )L 2 . Letting L 1 = L 1 , L 2 = L * 2 , R 1 = P (b,n) RP (b,n) , and R 2 = I, we have (b,n) and L 1 , L 2 ∈ DB (b,n) . Thus by Theorem 3 (and the fact that BD (b,n) is closed under conjugate transposition) we can write R * 1 = P ( n b ,n) R 1 P ( n b ,n) = P (b,n) R 1 P (b,n) for some R 1 ∈ DB ( n b ,n) , and similarly, can write</p><p>, where L 1 = P (b,n) L * 1 P (b,n) , L 2 = P (b,n) L 2 P (b,n) are in BD ( n b ,n) by Theorem 3. Thus letting</p><p>We now show that the class M (b,n) strictly contains the class B (n) of n × n butterfly matrices (as defined in Dao et al. <ref type="bibr" target="#b12">[13]</ref>). We first show two elementary "helper" results. Proof. Suppose L ∈ DB (c,n) . Then by Proposition C.9, L[i, j] = 0 whenever (i mod c) = (j mod c). Thus, whenever (i mod b) = (j mod b), L[i, j] = 0, since (i mod b) = (j mod b) implies (i mod c) = (j mod c) by the assumption that b divides c. Applying Proposition C.9 again, this means L ∈ DB (b,n) as well.</p><p>Theorem 4. Let n ≥ 4 be a power of 2. The class of matrices B (n) is a subset of the class M (b,n) , for all b ∈ (1, n) that divide n. When n ≥ 512 it is a strict subset.</p><p>Proof. Recall from Section 2.2 that if B ∈ B (n) , it has a butterfly factorization B = B n B n/2 . . . B 2 , where each B i ∈ BF (n,i) .</p><p>Consider multiplying together the factors (n,i) , by definition it is block diagonal with diagonal blocks of size i × i; in other words, B i ∈ BD (i,n) . Thus, each of the matrices B b , B b/2 , . . . , B 2 is in BD (b,n) (by Proposition C.12), i.e. block-diagonal with block size b × b. This means their product B b B b/2 . . . B 2 is also block diagonal with block size b × b, i.e., it is in BD (b,n) . Now, note that since B i ∈ BF (n,i) , by definition it is a block matrix with blocks of size i/2 × i/2, where each block is a diagonal matrix (note that some of these blocks are zero, except for the case of B n ). In other words, B i ∈ DB (i/2,n) . Thus, for all i ∈ {n, n/2, . . . , 2b}, B i ∈ DB ((2b)/2,n) = DB (b,n) (by Proposition C.13). So, their product B n B n/2 . . . B 2b is in DB (b,n) as well, as by Theorem 3 we can write B n B n/2 . . .</p><p>To show that the inclusion is strict, notice that any M ∈ M (b,n) is the product of L and R, where R ∈ BD (b,n) and P (b,n) LP (b,n) ∈ BD ( n b ,n) (by Theorem 3). Notice that the identity matrix is contained in both BD (b,n) and DB (b,n) . Suppose first that b ≤ √ n. Then even if we set R to the identity, M has at Recall that in the notation of Definition C.17 we have b 2 = b 3 = b, so we are in the b 2 ≤ b 3 case. To complete the proof, we will argue that R satisfies the two conditions in Proposition C. 15. 6  Towards this end, let 0 ≤ i, j &lt; n be arbitrary indices and further, define i = (i 1 , i 0 ) b and j = (j 1 , j 0 ) b . Then note that σ (b,n3) (i) = (i 0 , i 1 ) n 3 b and σ (b,n2) (j) = (j 0 , j 1 ) n 2 b . By Proposition C.18, we have that if i 0 mod b = j 0 , then L[i, j] = 0. Note that since i 0 , j 0 &lt; b by definition, the condition i 0 mod b = j 0 is equivalent to saying i 0 = j 0 . Note that i 0 = j 0 satisfies the pre-condition for base size n3 b × n2 b for indices (σ (b,n3) (i), σ (b,n2) (j)) in item 1 in Proposition C.15. Then by Eq. ( <ref type="formula">12</ref>), we have that R [σ (b,n3) (i), σ (b,n2) (j)] = 0, which satisfies item 1 in Proposition C. <ref type="bibr" target="#b14">15</ref>. Now consider the case that i 0 = j mod b, which by the observation in the above paragraph is the same as i 0 = j 0 . Then by item 2 in Proposition C.18, we have that</p><p>Note that the above implies that</p><p>where R • is as defined in the above paragraph. This means R ∈ BD </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Theory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Projection onto M</head><p>In Algorithm 1, we provide pseudocode for the algorithm outlined in Section 3.3. We now prove Theorem 1. Note that the rectangular matrix case generalizes naturally from the square matrix case, by replacing square blocks with rectangular blocks.</p><p>Proof of Theorem 1. As shown in Section 3.3, after reshaping the Monarch matrix M as a 4D tensor M jki and writing the two block-diagonal matrices L and R as 3D tensors L j k and R kji , we obtain:</p><p>We can similarly reshape the given matrix A into a 4D tensor A jki with size m × m × m × m. 6 Note that we also need that the ratios of the row/column length to the row/column block sizes are the same; i.e., in our case we need that n 3</p><p>Notice that by setting b = √ n, we immediately recover Theorem 2. Note also that by Proposition C.11, Theorem 7 implies that given an M ∈ M * M ( n b ,n) , we can find its Monarch factorization in time O( n 3 b ) as well (e.g., simply permute it to a matrix in MM * (b,n) and then run Algorithm 2). We now prove Theorem 7.</p><p>Proof. We first show that the factorization returned by Algorithm 2 is valid, which reduces to showing that (1) M ij = Âi Dij Ĉj and (2) Dij is diagonal, for all 1 ≤ i, j ≤ b as argued above.</p><p>As argued above, since M satisfies Assumption D.1, then there exists a matrix (C 1 ) that simultaneously diagonalizes all the F(i, j)'s. Thus, we can always compute some matrix that simultaneously diagonalizes these matrices (i.e., line 2 of Algorithm 2 will always return a valid solution); we discuss how to actually do this below. By definition of simultaneous diagonalization, this matrix (which we set Ĉ1 to) is invertible.</p><p>So, Âi = M i1 Ĉ-1 1 is invertible for all i. Thus Ĉj = Â-1 1 M 1j is invertible for all j as well. (Note that the equation Ĉj = Â-1</p><p>1 M 1j holds by construction of Ĉj for j ≥ 2, and by construction of Â1 when</p><p>by definition, we thus have that M ij = Âi Dij Ĉj for all i, j. It remains to show that Dij is diagonal.</p><p>is diagonal for all i, j by definition of Ĉ1 as a matrix that simultaneously diagonalizes the F(i, j)'s.</p><p>As for L 1 , R, L 2 , recall that we can simply set L 1 = diag( Â1 , . . . , Âb ), L 2 = diag( Ĉ1 , . . . , Ĉb ), and b 3 ) time. (Note that we can compute each of these faster using fast matrix multiplication / inversion; however, it turns out not to matter as the simultaneous diagonalization is the bottleneck.)</p><p>Finally, we analyze the simultaneous diagonalization runtime. Simultaneous diagonalization of a set of matrices {G 1 , . . . , G k } is equivalent to finding a mutual eigenbasis for the matrices, since if D i is a diagonal matrix and QG i Q -1 = D i , then the j th column of Q is an eigenvector of G i with eigenvalue equal to the j th entry of D i .</p><p>A simple algorithm for simultaneous diagonalizing a set of matrices, assuming that they are in fact simultaneously diagonalizable (which implies that each matrix is individually diagonalizable), is as follows (e.g. see <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref>): first, set i = 1 and diagonalize the first matrix G i = G 1 (i.e., find an eigenbasis), and set Q to be the diagonalizing matrix (i.e., the matrix of eigenvectors). So, QG 1 Q -1 is diagonal. By the assumption that the matrices are in fact simultaneously diagonalizable, QG j Q -1 will be permuted block diagonal for all j = i as well: the size of each block corresponds to the multiplicity of the corresponding eigenvalue of G 1 . (Note that if G 1 's has unique eigenvalues, then the eigenbasis is unique (up to permutation and nonzero scaling), and thus in this case G 1 uniquely determines the simultaneously diagonalizing matrix, up to arbitrary permutation and nonzero scaling of the rows. In other words, the block size will be 1 in this case, meaning that QG j Q -1 will be diagonal for all j, and we are done.)</p><p>So now, we repeat the following for all i up to k. Increment i and compute QG i Q -1 . If it is already diagonal, move on. Otherwise, first permute Q ← PQP so that it is block diagonal (observe that this</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experiment Details E.1 Model Configurations and Hyperparameters</head><p>We summarize the details required to replicate our experiments below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.1 Image Classification</head><p>Baseline Model: For dense models, we use standard implementations of ViT <ref type="bibr" target="#b23">[24]</ref>, MLP-Mixertolstikhin2021mlp from the timm library and from the T2T-ViT codebase <ref type="bibr" target="#b106">[107]</ref>.</p><p>The Monarch version of these models simply swap out the dense weight matrices in the attention blocks (projection matrices) and in the FFN block (linear layers) with Monarch matrices. We set the number of blocks in the block-diagonal matrices to 4. We also reduce the amount of regularization (stochastic depth) as our Monarch models are smaller than the dense models.</p><p>We adopt the hyperparameters (optimizer, learning rate, learning rate scheduler) from Yuan et al. <ref type="bibr" target="#b106">[107]</ref>. Details are in Table <ref type="table">9</ref>.</p><p>We measure the wall-clock training time on V100 GPUs. We follow the naming convention in the Vision Transformer paper and MLP-Mixer paper. In particular,</p><p>ViT-S and ViT-B refers to the small and base ViT models respectively, and 16 refers to the patch size of 16x16. The MLP-Mixer models follow the same convention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.2 Language Modeling</head><p>For dense models, we use standard implementations of GPT-2 <ref type="bibr" target="#b85">[86]</ref> from Huggingface transformers library and from Nvidia's Megatron-LM repo. We follow the training recipe of the Megatron-LM repo.</p><p>The Monarch version of these models simply swap out the dense weight matrices in the attention blocks (projection matrices) and in the FFN block (linear layers) with Monarch matrices. We set the number of blocks in the block-diagonal matrices to 4. We also reduce the regularization strength (dropout) as our model is smaller.</p><p>We report the hyperparameters used in Table <ref type="table">10</ref> and Table <ref type="table">11</ref>. We use an effective batch size of 512, and use gradient accumulation to fit into available GPU memory.</p><p>We measure the wall-clock training time on V100 GPUs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Details for PDE Solving</head><p>We adopt the experiment setting and data generation of Navier-Stokes Equation from FNO <ref type="bibr" target="#b64">[65]</ref>. It considers the 2-d Navier-Stokes equation for a viscous, incompressible fliud in vorticity form on the unit tortus:</p><p>∇w(x, t) = 0, x ∈ (0, 1) 2 , t ∈ (0, T ] (</p><p>w(x, 0) = w 0 (x), x ∈ (0, 1) 2</p><p>where u ∈ C([, T 0]);H per ((0, 1) 2 ; R 2 )) for any r &gt; 0 is the velocity field, w = ∇ × u is the vorticity, w 0 ∈ L 2 per ((0, 1) 2 ; R) is the initial vorticity, v ∈ R + is the viscosity coefficient, and f ∈ L 2 per ((0, 1) 2 ; R) is the forcing function. T represents the time interval since it is time-dependent equation. v represents the viscosity. N represents the number of training pairs or data. Table <ref type="table">3</ref> shows the results for viscosities v = 1e -3, 1e -4, 1e -5, T = 50, 30, 20 respectively and use N = 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Details for GPT-2 Downstream Tasks</head><p>We train Pixelfly-GPT2-small on a larger scale dataset, OpenWebText, and evaluate the downstream quality on zero-shot generation and classification tasks from <ref type="bibr" target="#b107">[108]</ref>, achieving comparable and even better performance to the dense model. Specifically, the datasets contains five popular classification tasks: SST2, Trec, CB, Agnews, and Dbpedia. We also adapated the calibrated metric from <ref type="bibr" target="#b107">[108]</ref> for evaluation. Results for each individual task are shown in Table <ref type="table">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5.2 Experimental Details</head><p>Dataset. We benchmark our method on the SKM-TEA Raw Data Track, which consists of dual-echo 3D MRI scans <ref type="bibr" target="#b19">[20]</ref>. Scans are accelerated using Poisson Disc undersampling masks distributed with the dataset. During training, Poisson Disc masks are generated, cached, and applied to mask the k-space data to simulate accelerated scans.</p><p>Matrix Shape. Like all matrices, Monarch matrices have an explicit shape constraint, which is a limitation of these matrices for MRI reconstruction tasks. Thus, the SKM-TEA dataset was filtered to include scans of shape 512 × 512 × 160, which is the most frequently occuring scan shape. A total of 3 scans were dropped from the original 155 scans in the dataset. Our method and all baselines were trained on this filtered dataset. Baselines. We compare our method to two baselines, SENSE and U-Net. Parameter count and hyperparameters are available in Table <ref type="table">13</ref>.</p><p>• SENSE : SENSE performs a linear combination of the images acquired on each coil <ref type="bibr" target="#b83">[84]</ref>. Here, the inverse fsat Fourier transform (IFFT) is applied to the acquired k-space for each coil. The resulting images are combined into a single complex image by weighting each coil image by corresponding coil sensitivity maps. In accelerated MRI, the unsampled frequencies are zero-valued; thus, SENSE produces a zero-filled image. Note, SENSE does not require any training.</p><p>• U-Net: U-Net is a popular fully convolutional neural network baseline for MRI reconstruction <ref type="bibr" target="#b89">[90]</ref>. We use the default implementation and hyperparameters used by Desai et al. <ref type="bibr" target="#b19">[20]</ref> to benchmark the SKM-TEA dataset. In this approach, the SENSE-reconstructed zero-filled image is mapped to SENSE-reconstructed ground truth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monarch-SENSE (mSENSE):</head><p>We propose a modification to the SENSE method, in which the (IFFT) is parameterized by a factorized Monarch matrix. This matrix is initialized to the IFFT but, unlike SENSE, is learnable. While mSENSE is trainable, it has 137x fewer trainable parameters than U-Net.</p><p>Metrics: We evaluate reconstruction performance using peak signal-to-noise ratio (pSNR) and structural similarity (SSIM) on both echoes (echo1 -E1, echo2 -E2) separately. Both metrics were computed on the 3D volume of each echo.</p><p>Extended Results. We provide sample reconstructions of SENSE, mSENSE, and U-Net in data-limited settings for first (Fig. <ref type="figure">6</ref>) and second (Fig. <ref type="figure">7</ref>) echoes. Both SENSE and U-Net reconstructed images have aliasing artifacts. Due to the random Poisson Disc undersampling pattern, these artifacts are incoherent, causing them to manifest as blurring around fine structures and edges. In contrast, mSENSE can recover these structures with higher fidelity. Even in the second echo, which has lower signal-to-noise ratio (SNR) than the first echo, mSENSE does not overblur the image.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse linear networks with a fixed butterfly structure: theory and practice</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Leibovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1174" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Approximate simultaneous diagonalization of matrices via structured low-rank approximation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Akema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06305</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FFTs in external or hierarchical memory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="35" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Numerical methods for simultaneous diagonalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bunse-Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prospective deployment of deep learning in MRI: A framework for important considerations, challenges, and recommendations for best practices</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sandino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vasanawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hargreaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pixelated butterfly: Simple and efficient sparse training for neural network models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unifying orthogonal Monte Carlo methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1203" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised MRI reconstruction with generative adversarial networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vasanawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13065</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The minimal polynomial and some applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Conrad</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An algorithm for the machine calculation of complex fourier series</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning fast algorithms for linear transforms using butterfly factorizations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kaleidoscope: An efficient, learnable representation for all structured linear maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blonder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accelerated MRI with un-trained neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Darestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="724" to="733" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Measuring robustness in deep learning based compressive sensing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Darestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06103</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A two-pronged progress in structured dense matrix vector multiplication</title>
		<author>
			<persName><forename type="first">C</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puttagunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1060" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Ozturkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Beg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasanawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hargreaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><surname>Vortex</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02549</idno>
		<title level="m">Physics-driven data augmentations for consistency training for robust accelerated MRI reconstruction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Noise2recon: A semi-supervised framework for joint MRI reconstruction and denoising</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Ozturkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sandino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasanawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hargreaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00075</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SKM-TEA: A dataset for accelerated MRI reconstruction with dense image labels for quantitative clinical evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sandino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mazzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<title level="m">Sparse networks from scratch: Faster training without losing performance</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to prune deep neural networks via layer-wise optimal brain surgeon</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07565</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast discrete polynomial transforms with applications to data analysis for distance transitive graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Driscoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Healy</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1066" to="1099" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On a new class of structured matrices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gohberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integral Equations and Operator Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="324" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10732</idno>
		<title level="m">The difficulty of training sparse neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Solving inverse problems in steady-state navier-stokes equations using deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13074</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01611</idno>
		<title level="m">Stabilizing the lottery ticket hypothesis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3259" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The state of sparsity in deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5371628</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5371628" />
		<imprint>
			<date type="published" when="2021-09">September 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Transformer feed-forward layers are key-value memories</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14913</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ellie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName><surname>Openwebtext Corpus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toeplitz and circulant matrices: A review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Communications and Information Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="155" to="239" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09224</idno>
		<title level="m">GPU kernels for block-sparse weights</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalized autocalibrating partially parallel acquisitions (grappa)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Griswold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nittka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jellus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Official Journal of the International Society for Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1202" to="1210" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Magnetic Resonance in Medicine: An</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accelerating sparse dnn models without hardware-support via tile-wise sparsity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Low-rank modeling of local k-space neighborhoods (loraks) for constrained MRI</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Haldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="668" to="681" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning a variational network for reconstruction of accelerated MRI data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hammernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klatzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Sodickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3055" to="3071" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02626</idno>
		<title level="m">Learning both weights and connections for efficient neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04381</idno>
		<title level="m">Dense-sparse-dense training for deep neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06489</idno>
		<title level="m">The hardware lottery</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Computed tomography: principles, design, artifacts, and recent advances</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>SPIE press</publisher>
			<biblScope unit="volume">114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Top-Kast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03517</idno>
		<title level="m">Top-K always sparse training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14080</idno>
		<title level="m">Gotta go fast when generating data with score-based models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Speech and language processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Pearson London</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Displacement ranks of matrices and linear equations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Khalitov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08184</idno>
		<title level="m">Sparse factorization of large square matrices</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<title level="m">Neural controlled differential equations for irregular time series</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep-learning methods for parallel magnetic resonance imaging reconstruction: A survey of the current approaches, trends, and issues</title>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hammernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Sodickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akcakaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="140" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Machine learningaccelerated computational fluid dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kochkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Lagunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Charlaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04838</idno>
		<title level="m">Block pruning for faster transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Blind primed supervised (blips) learning for mr image reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fessler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05028</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fastfood-computing hilbert space expansions in loglinear time</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Flexible multilayer sparse approximations of matrices and applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Le Magoarou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="688" to="700" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<title level="m">Pruning filters for efficient convnets</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fourier neural operator for parametric partial differential equations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>; Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deformable butterfly: A highly structured and sparse linear transform</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Finding trainable sparse networks through neural tangent transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6336" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The application of compressed sensing for rapid mr imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mri</forename><surname>Sparse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep generative adversarial neural networks for compressive sensing MRI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vasanawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03885</idno>
		<title level="m">Differentiable multiple shooting layers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Mlperf training benchmark</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bittorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="336" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">ACDC: a structured efficient linear layer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02773</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Quadrature-based features for kernel approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Munkhoeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kapushev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9165" to="9174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Beyond low rank+ sparse: Multiscale low rank matrix decomposition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of selected topics in signal processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="672" to="687" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Logarithmic pruning is all you need</title>
		<author>
			<persName><forename type="first">L</forename><surname>Orseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rivasplata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Structured matrices and polynomials: unified superfast algorithms</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Random butterfly transformations with applications in computational linear algebra</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Parker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Optimal lottery tickets via subsetsum: Logarithmic over-parameterization is sufficient</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pensia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vishwakarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07990</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Ac/dc: Alternating compressed/decompressed training of deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Peste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Iofinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Toward fast continuousdepth models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Sense: sensitivity encoding for fast MRI</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Pruessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boesiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="952" to="962" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Universal differential equations for scientific machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rackauckas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zubov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Supekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramadhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04385</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Low-rank and adaptive sparse signal (lassi) models for highly accelerated dynamic imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Nadakuditi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Compressed sensing: From research to clinical practice with deep neural networks: Shortening scan times for magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sandino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vasanawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07683</idno>
		<title level="m">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal procrustes problem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><surname>Megatron-Lm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Structured transforms for small-footprint deep learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3088" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05467</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Sparse matrices</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Tewarson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="volume">69</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Learning compressed transforms with low displacement rank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9052" to="9060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<title level="m">Mlp-Mixer: An all-mlp architecture for vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Spectral methods in MATLAB</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Trefethen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Butterfly transform: An efficient fft based neural architecture design</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Vahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12021" to="12030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07376</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Towards physics-informed deep learning for turbulent flow prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1457" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Self-supervised physics-based deep learning MRI reconstruction without fully-sampled data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A H</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ellermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ugurbil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akçakaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="921" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Orthogonal random features</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1975" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token ViT: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09690</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
