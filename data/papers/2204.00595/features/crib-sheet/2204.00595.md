- **Monarch Matrix Definition**: A Monarch matrix \( M \) is defined as \( M = PLPR \), where \( L \) and \( R \) are block-diagonal matrices, \( P \) is a permutation matrix, and \( n = m^2 \) (Definition 3.1).

- **Parameterization**: Monarch matrices are parameterized as products of two block-diagonal matrices, enhancing hardware efficiency on GPUs and allowing for optimized batch-matrix-multiply routines (Section 5.1.1).

- **Expressiveness**: Monarch matrices can represent many commonly used transforms (e.g., Fourier, convolution) and inherit expressiveness from butterfly matrices, which can represent low-depth arithmetic circuits (Proposition 3.2).

- **Efficiency-Quality Tradeoff**: Monarch matrices achieve favorable accuracy-efficiency tradeoffs, enabling 2× faster training on tasks like ViT and GPT-2 with comparable model quality (Section 5.1).

- **Reverse Sparsification Technique**: This technique allows for transitioning from sparse to dense weight matrices, speeding up GPT-2 pretraining by 2× without quality drop (Section 5.2).

- **Dense-to-Sparse Fine-Tuning**: The Monarch approximation algorithm enables BERT fine-tuning on GLUE with 1.7× faster speed and comparable accuracy, demonstrating effective D2S fine-tuning (Section 5.3).

- **Analytical Optimal Solution**: The problem of approximating a dense weight matrix with a Monarch matrix has an analytical optimal solution, despite being nonconvex (Theorem 1).

- **Empirical Validation**: Monarch matrices reduce error by up to 40% in PDE solving and MRI reconstruction tasks while maintaining training speed compared to traditional Fourier-based methods.

- **Comparison with Existing Methods**: Unlike pruning methods that focus on inference efficiency, Monarch emphasizes overall efficiency in both training and inference, functioning similarly to manually constructed lottery tickets.

- **Butterfly Matrices Relation**: Monarch matrices are related to butterfly matrices, which can be expressed as products of permuted block-diagonal matrices, optimizing memory and runtime complexity (Section 3).

- **Projection Algorithm**: A derived projection algorithm allows for tractable approximation of dense matrices with Monarch matrices, facilitating their use in various applications (Section 3.3).

- **Block-Diagonal Structure**: Each block-diagonal matrix \( L \) and \( R \) in the Monarch parameterization consists of \( m \) blocks of size \( m \times m \), enhancing the representation of structured matrices (Section 3.1).

- **Performance Metrics**: Empirical results show that Monarch matrices can achieve 23% faster BERT pretraining than Nvidia's optimized implementation, setting a new benchmark in efficiency (Section 5.2).