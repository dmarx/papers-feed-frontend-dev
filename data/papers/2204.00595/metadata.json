{
  "arxivId": "2204.00595",
  "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate\n  Training",
  "authors": "Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher R\u00e9",
  "abstract": "Large neural networks excel in many domains, but they are expensive to train\nand fine-tune. A popular approach to reduce their compute or memory\nrequirements is to replace dense weight matrices with structured ones (e.g.,\nsparse, low-rank, Fourier transform). These methods have not seen widespread\nadoption (1) in end-to-end training due to unfavorable efficiency--quality\ntradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable\nalgorithms to approximate a given dense weight matrix. To address these issues,\nwe propose a class of matrices (Monarch) that is hardware-efficient (they are\nparameterized as products of two block-diagonal matrices for better hardware\nutilization) and expressive (they can represent many commonly used transforms).\nSurprisingly, the problem of approximating a dense weight matrix with a Monarch\nmatrix, though nonconvex, has an analytical optimal solution. These properties\nof Monarch matrices unlock new ways to train and fine-tune sparse and dense\nmodels. We empirically validate that Monarch can achieve favorable\naccuracy-efficiency tradeoffs in several end-to-end sparse training\napplications: speeding up ViT and GPT-2 training on ImageNet classification and\nWikitext-103 language modeling by 2x with comparable model quality, and\nreducing the error on PDE solving and MRI reconstruction tasks by 40%. In\nsparse-to-dense training, with a simple technique called \"reverse\nsparsification,\" Monarch matrices serve as a useful intermediate representation\nto speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The\nsame technique brings 23% faster BERT pretraining than even the very optimized\nimplementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse\nfine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds\nup BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",
  "url": "https://arxiv.org/abs/2204.00595",
  "issue_number": 335,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/335",
  "created_at": "2025-01-04T15:02:51.854751",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 35,
  "last_read": "2025-01-04T15:02:51.856832",
  "last_visited": "2024-12-28T06:07:58.885Z",
  "main_tex_file": null,
  "published_date": "2022-04-01T17:37:29Z",
  "arxiv_tags": [
    "cs.LG"
  ]
}