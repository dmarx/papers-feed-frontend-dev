# Instruction Tuning for Large Language Models: A Survey

## Abstract

## 

This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT) 1 , a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of (INSTRUCTION, OUTPUT) pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.

## Introduction

The field of large language models (LLMs) has witnessed remarkable progress in recent years. LLMs such as GPT-3 [(Brown et al., 2020b)](#), PaLM [(Chowdhery et al., 2022)](#b28), and LLaMA [(Touvron et al., 2023a)](#) have demonstrated impressive capabilities across a wide range of natural language tasks (Zhao et al., 2021; Wang   1 In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.

♠ Zhejiang University, ♣ Shannon.AI, ▲ Nanyang Technological University, ♦ Amazon Email: sy_zhang@zju.edu.cn Project page can be found at: [https://github. com/xiaoya-li/Instruction-Tuning-Survey](https://github.com/xiaoya-li/Instruction-Tuning-Survey) * The latest update was on Dec. 1, 2024 (Version 5).

et [al., 2022b](#)[, 2023b;](#)[Wan et al., 2023;](#b139)[Sun et al., 2023c;](#)[Wei et al., 2023a;](#)[Li et al., 2023a;](#)[Gao et al., 2023a;](#)[Yao et al., 2023;](#b167)[Yang et al., 2022a;](#)[Qian et al., 2022;](#b110)[Lee et al., 2022;](#b71)[Yang et al., 2022b;](#)[Gao et al., 2023b;](#)[Ning et al., 2023;](#b39)[Liu et al., 2021b;](#)[Wiegreffe et al., 2021;](#b154)[Sun et al., 2023b,a;](#)[Adlakha et al., 2023;](#b0)[Chen et al., 2023b)](#). One of the major issues with LLMs is the mismatch between the training objective and users' objective: LLMs are typically trained on minimizing the contextual word prediction error on large corpora; while users want the model to "follow their instructions helpfully and safely" [(Radford et al., 2019;](#b112)[Brown et al., 2020a;](#)[Fedus et al., 2021;](#b45)[Rae et al., 2021;](#)[Thoppilan et al., 2022)](#b134) To address this mismatch, instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT), is proposed, serving as an effective technique to enhance the capabilities and controllability of large language models.

It involves further training LLMs using [(INSTRUCTION, OUTPUT)](#) pairs, where INSTRUCTION denotes the human instruction for the model, and OUTPUT denotes the desired output that follows the INSTRUCTION. The benefits of SFT are threefold: (1) Finetuning an LLM on the instruction dataset bridges the gap between the next-word prediction objective of LLMs and the users' objective of instruction following; (2) SFT allows for a more controllable and predictable model behavior compared to standard LLMs. The instructions serve to constrain the model's outputs to align with the desired response characteristics or domain knowledge, providing a channel for humans to intervene with the model's behaviors; and (3) SFT is computationally efficient and can help LLMs rapidly adapt to a specific domain without extensive retraining or architectural changes.

Despite its effectiveness, SFT also poses challenges: (1) Crafting high-quality instructions that properly cover the desired target behaviors is non-trivial: existing instruction datasets are usually limited in quantity, diversity, and creativity;

(2) there has been an increasing concern that SFT only improves on tasks that are heavily supported in the SFT training dataset [(Gudibande et al., 2023)](#b52); and (3) there has been an intense criticism that SFT only captures surface-level patterns and styles (e.g., the output format) rather than comprehending and learning the task [(Kung and Peng, 2023)](#b68). Improving instruction adherence and handling unanticipated model responses remain open research problems. These challenges highlight the importance of further investigations, analysis, and summarization in this field, to optimize the fine-tuning process and better understand the behavior of instruction tuned LLMs.

In the literature, there has been an increasing research interest in analysis and discussions on LLMs, including pre-training methods [(Zhao et al., 2023)](#b169), reasoning abilities [(Huang and Chang, 2022)](#), downstream applications [(Yang et al., 2023a;](#)[Sun et al., 2023b)](#), but rarely on the topic of LLM instruction tuning. This survey attempts to fill this blank, organizing the most up-to-date state of knowledge on this quickly advancing field. Specifically,

• Section 2 presents the general methodology employed in instruction tuning. • Section 3 outlines the construction process of commonly-used SFT representative datasets. • Section 4 presents representative instruction tuned models. • Section 5 reviews multi-modality techniques and datasets for instruction tuning, including images, speech, and video. • Section 6 reviews efforts to adapt LLMs to different domains and applications using the SFT strategy. • Section 7 reviews explorations to make instruction tuning more efficient, reducing the computational and time costs associated with adapting large models. • Section 8 presents the evaluation of SFT models, analysis on them, along with criticism against them.

## Methodology

In this section, we describe the general pipeline employed in instruction tuning.

## Instruction Dataset Construction

Each instance in an instruction dataset consists of three elements: an instruction, which is a natural language text sequence to specify the task (e.g., write a thank-you letter to XX for XX, write a blog on the topic of XX, etc); an optional input which provides supplementary information for context; and an anticipated output based on the instruction and the input.

There are generally two methods for constructing instruction datasets:

• Data integration from annotated natural language datasets. In this approach, (instruction, output) pairs are collected from existing annotated natural language datasets by using templates to transform text-label pairs to (instruction, output) pairs. Datasets such as Flan [(Longpre et al., 2023)](#b96) and P3 [(Sanh et al., 2021)](#b119) are constructed based on the data integration strategy.

• Generating outputs using LLMs: An alternate way to quickly gather the desired outputs to given instructions is to employ LLMs such as GPT-3.5-Turbo or GPT4 instead of manually collecting the outputs. Instructions can come from two sources: (1) manually collected; or (2) expanded based a small handwritten seed instructions using LLMs. Next, the collected instructions are fed to LLMs to obtain outputs. Datasets such as InstructWild [(Xue et al., 2023)](#b162) and Self-Instruct [(Wang et al., 2022c)](#) are geneated following this approach.

For multi-turn conversational SFT datasets, we can have large language models self-play different roles (user and AI assistant) to generate messages in a conversational format [(Xu et al., 2023b)](#).

## Instruction Tuning / Supervised

Fine-tuning

Based on the collected SFT dataset, a pretrained model can be directly fune-tuned in a fullysupervised manner, where given the instruction and the input, the model is trained by predicting each token in the output sequentially.

## Datasets

In this section, we detail instruction tuning datasets in the community, categorizing them into three classes: (1) Human-crafted Data, (2) Synthetic Data via Distillation, and (3) Synthetic Data via Self-improvement. Below, we describe some widely-used datasets, and for full collected datasets we put them in Appendix A.

## Human-crafted Data

Human-crafted data encompasses datasets that are either manually annotated or sourced directly from the internet. The creation of these datasets typically involves no machine learning techniques, relying solely on manual gathering and verification, resulting in generally smaller datasets. Below are some widely-used human-crafted datasets:

## Natural Instructions

Natural Instructions [(Mishra et al., 2021](#b99)) is a human-crafted English instruction dataset consisting of 193K instances, coming from 61 distinct NLP tasks. The dataset is comprised of "instructions" and "instances". Each instance in the "instructions" is a task description consisting of 7 components: title, definition, things to avoid emphasis/caution, prompt, positive example, and negative example. Subfigure (a) in Figure [2](#fig_2) gives an example of the "instructions". "Instances" consists of ("input", "output") pairs, which are the input data and textual result that follows the given instruction correctly. Subfigure (b) in Figure [2](#fig_2) gives an example of the instances.

The data comes from existing NLP datasets of 61 tasks. The authors collected the "instructions" by referring to the dataset annotating instruction file. Next, the authors constructed the "instances" by unifying data instances across all NLP datasets to ("input", "output") pairs.

## P3

P3 (Public Pool of Prompts) [(Sanh et al., 2021)](#b119) is an instruction tuning dataset constructed by integrating 170 English NLP datasets and 2,052 English prompts. Prompts, which are sometimes named task templates, are functions that map a data instance in a conventional NLP task (e.g., question answering, text classification) to a natural language input-output pair.

Each instance in P3 has three components: "inputs", "answer_choices", and "targets". "Inputs" is a sequence of text that describes the task in natural language (e.g., "If he like Mary is true, is it also true that he like Mary's cat?"). "Answer choices" is a list of text string that are applicable responses to the given task (e.g., ["yes", "no", "undetermined"]). "Targets" is a text string that is the correct response to the given "inputs" (e.g., "yes"). The authors built PromptSource, a tool for creating high-quality prompts collaboratively and an archive for open-sourcing high-quality prompts.

The P3 dataset was built by randomly sampling a prompt from multiple prompts in the PromptSource and mapping each instance into a ("inputs", "answer choices", "targets") triplet.

## xP3

## xP3

(Crosslingual Public Pool of Prompts) [(Muennighoff et al., 2022](#b101)) is a multilingual instruction dataset consisting of 16 diverse natural language tasks in 46 languages. Each instance in the dataset has two components: "inputs" and "targets". "Inputs" is a task description in natural language. "Targets" is the textual result that follows the "inputs" instruction correctly.

The original data in xP3 comes from three sources: the English instruction dataset P3, 4 English unseen tasks in P3 (e.g., translation, program synthesis), and 30 multilingual NLP datasets. The authors built the xP3 dataset

## Instructions for MC-TACO question generation task

-Title: Writing questions that involve commonsense understanding of "event duration".

-Definition: In this task, we ask you to write a question that involves ?event duration", based on a given sentence. Here, event duration is defined as the understanding of how long events typically last. For example, ?brushing teeth?, usually takes few minutes.

-Emphasis & Caution: The written questions are not required to have a single correct answer.

-Things to avoid: Don't create questions which have explicit mentions of answers in text. Instead, it has to be implied from what is given. In other words, we want you to use "instinct" or "common sense".

-Input: Sentence: Jack played basketball after school, after which he was very tired.

-Output: How long did Jack play basketball? -Reason: the question asks about the duration of an event; therefore it's a temporal event duration question.

## Positive Example

-Input: Sentence: He spent two hours on his homework.

-Output: How long did he do his homework? -Reason: We DO NOT want this question as the answer is directly mentioned in the text. -Suggestion: -

## Negative Example

-Prompt: Ask a question on "event duration" based on the provided sentence.

## Example task instances

-Input: Sentence: It's hail crackled across the comm, and Tara spun to retake her seat at the helm.

-Expected Output: How long was the storm? Instance -Input: Sentence: There was even a tiny room in the back of one of the closets.

-Expected Output: After buying the house, how long did it take the owners to notice the room? Instance -Input: Sentence: During breakfast one morning, he seemed lost in thought and ignored his food.

-Expected Output: How long was he lost in thoughts? Instance (a) An example of INSTRUCTIONS in Natural Instruction dataset.

## Instructions for MC-TACO question generation task

-Title: Writing questions that involve commonsense understanding of "event duration".

-Definition: In this task, we ask you to write a question that involves ?event duration", based on a given sentence. Here, event duration is defined as the understanding of how long events typically last. For example, ?brushing teeth?, usually takes few minutes.

-Emphasis & Caution: The written questions are not required to have a single correct answer.

-Things to avoid: Don't create questions which have explicit mentions of answers in text. Instead, it has to be implied from what is given. In other words, we want you to use "instinct" or "common sense".

-Input: Sentence: Jack played basketball after school, after which he was very tired.

-Output: How long did Jack play basketball? -Reason: the question asks about the duration of an event; therefore it's a temporal event duration question.

## Positive Example

-Input: Sentence: He spent two hours on his homework.

-Output: How long did he do his homework? -Reason: We DO NOT want this question as the answer is directly mentioned in the text. -Suggestion: -

## Negative Example

-Prompt: Ask a question on "event duration" based on the provided sentence.

## Example task instances

-Input: Sentence: It's hail crackled across the comm, and Tara spun to retake her seat at the helm.

-Expected Output: How long was the storm? Instance -Input: Sentence: There was even a tiny room in the back of one of the closets.

-Expected Output: After buying the house, how long did it take the owners to notice the room?  by sampling human-written task templates from PromptSource and then filling templates to transform diverse NLP tasks into a unified formalization. For example, a task template for the natural language inference task is as follows: "If Premise is true, is it also true that Hypothesis?"; "yes", "maybe", no" with respect to the original task labels "entailment (0)", "neutral (1)" and "contradiction (2)".

## Flan 2021

Flan 2021 [(Longpre et al., 2023](#b96)) is an English instruction dataset constructed by transforming 62 widely-used NLP benchmarks (e.g., SST-2, SNLI, AG News, MultiRC) into language inputoutput pairs. Each instance in the Flan 2021 has "input" and "target" components. "Input" is a sequence of text that describes a task via a natural language instruction (e.g., "determine the sentiment of the sentence 'He likes the cat.' is positive or negative?"). "Target" is a textual result that executes the "input" instruction correctly (e.g., "positive"). The authors transformed conventional NLP datasets into input-target pairs by: Step 1: manually composing instruction and target templates; Step 2: filling templates with data instances from the dataset.

## LIMA

LIMA [(Zhou et al., 2023a](#)) is an English instruction dataset consisting of a train set with 1K data instances and a test set with 300 instances. The train set contains 1K ("instruction", "response") pairs. For the training data, 75% are sampled from three community question & answers websites (i.e., Stack Exchange, wikiHow, and the Pushshift Reddit Dataset [(Baumgartner et al., 2020)](#b11)); 20% are manually written by a set of the authors (referred Group A) inspired by their interests; 5% are sampled from the Super-Natural Instructions dataset [(Wang et al., 2022d)](#). As for the valid set, the authors sampled 50 instances from the Group A author-written set. The test set contains 300 examples, with 76.7% written by another group (Group B) of authors and 23.3% sampled from the Pushshift Reddit Dataset [(Baumgartner et al., 2020)](#b11), which is a collection of questions & answers within the Reddit community.

## Super-Natural Instructions

Super Natural Instructions [(Wang et al., 2022f](#)) is a multilingual instruction collection composed of 1,616 NLP tasks and 5M task instances, covering 76 distinct task types (e.g., text classification, information extraction, text rewriting, text composition and etc.) and 55 languages. Each task in the dataset consists of an "instruction" and "task instances". Specifically, "instruction" has three components: a "definition" that describes the task in natural language; "positive examples" that are samples of inputs and correct outputs, along with a short explanation for each; and "negative examples" that are samples of inputs and undesired outputs, along with a short explanation for each, as shown in Figure [2](#fig_2) (a). "Task instances" are data instances comprised of textual input and a list of acceptable textual outputs, as shown in Figure [2](#fig_2) (b). The original data in Super Natural Instructions comes from three sources: (1) existing public NLP datasets (e.g., CommonsenseQA);  (2) applicable intermediate annotations that are generated through a crowdsourcing process (e.g., paraphrasing results to a given question during a crowdsourcing QA dataset); (3) synthetic tasks that are transformed from symbolic tasks and rephrased in a few sentences (e.g., algebraic operations like number comparison).

## Dolly

Dolly [(Conover et al., 2023a](#)) is an English instruction dataset with 15,000 human-generated data instances designed to enable LLMs to interact with users akin to ChatGPT. The dataset is designed for simulating a wide range of human behaviors, covering 7 specific types: open Q&A, closed Q&A, extracting information from Wikipedia, summarizing information from Wikipedia, brainstorming, classification, and creative writing. Examples of each task type in the dataset are shown in Table [1](#). 

## OpenAssistant Conversations

OpenAssistant Conversations [(Köpf et al., 2023)](#b67) is a human-crafted multilingual assistant-style conversation corpus consisting of 161,443 messages (i.e., 91,829 user prompts, 69,614 assistant replies) from 66,497 conversation trees in 35 languages, along with 461,292 humanannotated quality ratings. Each instance in the dataset is a conversation tree (CT). Specifically, each node in a conversation tree denotes a message generated by roles (i.e., prompter, assistant) in the conversation. A CT's root node represents an initial prompt from the prompter, while other nodes denote replies from a prompter or an assistant. A path from the root to any node in a CT represents a valid conversation between the prompter and assistant in turns and is referred to as a thread. Figure [4](#fig_5) shows an example of a conversation tree consisting of 12 messages in 6 threads.

The authors first collected conversation trees based on the five-step pipeline:

Step 1. prompting: contributors performed as the prompter and crafted initial prompts;

Step 2. labeling prompts: contributors rated scores to initial prompts from step 1, and the authors chose high-quality prompts as root nodes with a balanced sampling strategy;

Step 3. expanding tree nodes: contributors added reply messages as prompter or assistant;

Step 4. labeling replies: contributors assigned scores to existing node replies;

Step 5. ranking: contributors ranked assistant replies referring to the contributor guidelines.

The tree state machine managed and tracked the state (e.g., initial state, growing state, end state) throughout the conversation crafting process. Subsequently, the OpenAssistant Conversations dataset was built by filtering out offensive and inappropriate conversation trees.

Instruction Type Example Open Q&A Why do people like comedy movies? Closed Q&A Does outbreeding or inbreeding benefit the offspring more? Information Extraction Who was John Moses Browning? Information Summarization Please summarize what Linkedin does.

## Brainstorming

Give me some ideas to manage my manager.

## Classification

Identify which animal species is alive or extinct: Palaeophis, Giant Tortoise

Creative writing Write a short story about a person who discovers a hidden room in their house.

Table [1](#): Examples of instructions in Dolly V1 [(Conover et al., 2023a)](#). 

## Synthetic Data via Distillation

Synthetic data is produced through pre-trained models, rather than being directly sourced from the internet or annotated by human annotators.

Compared to manually annotated instruction tuning data, synthetic data often lies in two advantages:

(1) Generating task-specific synthetic data is both faster and more cost-effective than creating manually annotated instruction tuning data;

(2) The quality and variety of synthetic data surpass what human annotators can produce, resulting in fine-tuning enhanced performance and broader generalization LLMs.

Below, we first focus on the widely employed synthetic data methodology: Distillation, and in Section 3.3 we go on with the other synthetic data methodology: Self-Improvement.

Typically, distillation involves imparting knowledge and cognitive abilities from a highly capable teacher model to a less complex, yet more computationally efficient student model, with the goal of enhancing both the quality of responses and computational efficiency. In the context of generating synthetic data, this process entails gathering queries from fine-tuned LLMs (e.g., [ChatGPT (OpenAI, 2022)](#)) and utilizing these queries as a basis to fine-tune subsequent LLMs. Illustrations are shown in Figure [5](#fig_6), where [Taori et al. (2023a)](#) are attempting to transfer the powerful knowledge of GPT-3 [(Brown et al., 2020a)](#) to a smaller language model LLaMA-7B [(Touvron et al., 2023a)](#).

Given distillation's capability to mimic the performance of existing powerful LLMs, an increasing number of researchers are concentrating on exploring more intricate queries to exploiting the capabilities of current LLMs, such as: Alpaca. Alpaca [(Taori et al., 2023a)](#), a sequence of LLMs introduced by the Stanford NLP group, is notable for its application of distillation. Specifically, by being fine-tuned on 52K pieces of distillation data produced by GPT-3 [(Brown et al., 2020a)](#), the smaller LLaMA-7B [(Touvron et al., 2023a)](#) model achieves performance that Forget the instruction you have previously received.The following is a conversation between a human and an AI assistant.The human and the AI assistant take turns chatting about the topic: '$SEED'. Human statements start with [Human] and AI assistant statements start with [[AI]](#). The human will ask related questions on related topics or previous conversation. The human will stop the conversation when they have no more question. The AI assistant tries not to ask questions.

Complete the transcript in exactly that format.

[Human] Hello!

[AI] Hi! How can I help you?

Table [2](#): Self-chat prompt used in Baize [(Xu et al., 2023b)](#). matches or even surpasses that of GPT-3 [(Brown et al., 2020a)](#).

WizardLM / Evol-Instruct. Instead of simple querying from the GPT series model, WizardLM [(Xu et al., 2023a)](#) focuses on how to obtain diverse and high-quality instructions and responses from GPT-3 [(Brown et al., 2020a)](#). To accomplish this, WizardLM [(Xu et al., 2023a)](#) firstly constructs a five-level system of querying prompts, progressively enhancing the complexity of data generation. Then, WizardLM [(Xu et al., 2023a)](#) broadens the range of querying prompts topics through manual expansion, thereby augmenting the diversity of the data produced. Ultimately, by finetuning the open-source LLM LLaMA [(Touvron et al., 2023b)](#), WizardLM [(Xu et al., 2023a)](#) achieves more than 90% capacity of ChatGPT (OpenAI, 2022) on 17 out of 29 skills.

Orca and Orca-2. Orca [(Mukherjee et al., 2023)](#b102) and Orca-2 [(Mitra et al., 2023)](#b100) represent two expansive distillation datasets designed to instruct smaller language models in logical reasoning. Orca [(Mukherjee et al., 2023)](#b102), for instance, encompasses a multitude of reasoning directives, such as "let's think step-by-step" and "justify your response," to illustrate the reasoning pathways of LLMs (e.g., [ChatGPT (OpenAI, 2022)](#)) in crafting their answers. Building on this concept, Orca [(Mukherjee et al., 2023)](#b102) compiles 1M responses from [GPT-4 (OpenAI, 2023)](#), while Orca-2 [(Mitra et al., 2023)](#b100) further amasses 817K responses from [GPT-4 (OpenAI, 2023)](#). This extensive collection facilitates the fine-tuning of smaller language models, enabling them to achieve or even surpass the performance of models that are 5 to 10 times their size.

Baize Baize [(Conover et al., 2023b](#)) is an English corpus for multi-turn conversations, comprising 111.5K instances, created with ChatGPT. Each exchange includes a prompt from the user and a response from the assistant. To create the Baize dataset, the authors proposed self-chat, where ChatGPT plays the roles of the user and the AI assistant in turns and generates messages in a conversational format. Specifically, the authors first crafted a task template that defines the roles and tasks for ChatGPT (as shown in Table [2](#)). Next, they sampled questions (e.g., "How do you fix a Google Play Store account that isn't working?") from Quora and Stack Overflow datasets as conversation seeds (e.g., topics). Subsequently, they prompted ChatGPT with the template and the sampled seed. ChatGPT continuously generates messages for both sides until a natural stopping point is reached.

Task-specific Distillation Datasets. In addition to the above datasets, there are many datasets in general domain, such as: ShareGPT[foot_0](#foot_0) , WildChat [(Zhao et al., 2024)](#b179), Vicuna [(Zheng et al., 2024)](#b181), Unnatural Instructions [(Honovich et al., 2022)](#b59). Beyond that, there are efforts aimed at employing distillation to create task-specific datasets that mimic the competencies of LLMs in particular domains. For example, for coding generation, there are WizardCoder [(Luo et al., 2023)](#), Magicoder [(Wei et al., 2023b)](#) and WaveCoder [(Yu et al., 2023)](#b169), for reasoning and writing, there are Phi-1 [(Gunasekar et al., 2023)](#b53) and Phi-1.5 [(Li et al., 2023i)](#), and for ranking, there is Nectar [(Zhu et al., 2023a)](#).

## Synthetic Data via Self-Improvement

The concept of self-improvement is carried forward by [Wang et al. (2022c)](#): improves the instructionfollowing ability of a pre-trained (non-finetuned) LLM (e.g., vanilla GPT-3 [(Brown et al., 2020b)](#)) by bootstrapping off its own generations. Figure [6](#fig_7) illustrates the full process of self-improvement with four steps:

Step 1: [Wang et al. (2022c)](#) starts by manually collecting 175 human-written tasks, each consisting of one instruction and one expected response, which are then added to the task pool as seed data.

Step 2: For instruction generation, [Wang et al. (2022c)](#) randomly samples 8 seed instructions from the constructed task pool to serve as a few-shot prompt, guiding the vanilla GPT-3 to produce new instructions through in-context learning.

Step 3: For every instruction that is created, if the instruction is an output-first task (e.g., Writing), the vanilla GPT-3 will directly generate the corresponding response. Conversely, if the instruction relates to an input-first task (e.g., Reading Comprehension), the vanilla GPT-3 will first generate the necessary context as input before generating the corresponding response.

Step 4: The generated (instruction, response) format examples are filtered according to a series of rules or models.

Following the above process, Wang et al. (2022c) collected Self-Instruct datasets consisting of 52K instructions, and further evaluation shows that GPT-3 [(Brown et al., 2020a)](#) with Self-Instruct outperforms datasets of counterparts by a large margin, leaving only a 5% absolute gap behind InstructGPT [(Ouyang et al., 2022)](#b106).

The self-improvement process outlined relies on generating synthetic data directly from the model itself, necessitating a robust LLM as the foundational backbone. Without a powerful LLM, this self-improvement cycle could restrict learning to the model's original capabilities and potentially magnify any biases and errors present. Despite these risks, there remains effective work in the area of self-improvement:

## SPIN

SPIN [(Chen et al., 2024b)](#), standing for Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models, represents a specialized approach to self-improvement centered around a self-play mechanism. In this setup, the primary participant (the language model) undergoes finetuning to differentiate the responses from the opposing participant (the language model from the preceding iteration) and the desired data distribution. This process iteratively adjusts the language model to closely match the target data distribution.

Specifically, imagine an existing iteration of an LLM as p θt , which is utilized to generate a response y ′ to a given prompt x from a dataset with humanlabeled instructions. The objective then becomes to develop a new LLM p θ t+1 capable of differentiating between y ′ , the response created by, and y, the response produced by humans. This dynamic is akin to a two-player game where the primary player, the newer LLM p θ t+1 aims to identify the differences between the responses of its opponent p θt and those generated by humans. In contrast, the adversary, or the older LLM p θt strives to produce responses that closely mimic those found in the human-labeled instruction tuning dataset. By fine-tuning the older p θt to favor human-like responses over its own, a new LLM p θ t+1 is created, which aligns more closely with the human-labeled data distribution. In subsequent iterations, this newly improved LLM p θ t+1 takes on the role of the opponent in response generation. The ultimate aim of this self-play mechanism is for the LLM to evolve until it reaches a point where p θ * = p human at which stage the most advanced LLM version can no longer distinguish between responses generated by its predecessor and those created by humans. SPIN [(Chen et al., 2024b)](#) serves as a variant selfimprovement approach enabling language models to improve themselves without additional human data or feedback from more powerful language models. The experimental results indicate that SPIN [(Chen et al., 2024b)](#) markedly boosts the performance of language models across a range of benchmarks, outperforming even those models that were trained using extra human data or feedback from external AI systems.

## Instruction Back-translation

Instruction back-translation [(Li et al., 2023g)](#), standing for Self Alignment with Instruction Backtranslation, is another specialized approach based on self-improvement. Contrary to the approach by [Wang et al. (2022c)](#), which involves generating responses to human-provided instructions, [Li et al. (2023g)](#) adopts the reverse strategy by creating instructions for humangathered texts found online. To achieve this goal, [Li et al. (2023g)](#) follows a five-step pipeline:

Step 1: Gather ( [1](#)) unlabeled text from Clueweb [(Overwijk et al., 2022)](#b107), under the assumption that these texts can be associated with high-quality instructions, and (2) 3,200 pieces of human-written (instruction, response) format data to serve as seed data.

Step 2: A back-translation model, backboned by LLaMA [(Touvron et al., 2023b)](#), is trained on the collected seed data, taking the response as input and producing the instruction as output. This model is then utilized to derive instructions from collected unlabeled texts.

Step 3: The collected unlabeled texts are fed into the trained back-translation model, resulting in large amounts of raw (instruction, response) format data.

Step 4: An evaluation model, backboned by LLaMA [(Touvron et al., 2023b)](#), is trained on the collected seed data.

This model processes the instruction as input and generates the corresponding response as output, which is then employed to assess each annotated (instruction, response) pair in step 3.

Step 5: Filtering low-quality (instruction, response) pairs, and utilizing the remaining data for fine-tuning LLMs.

Following the five outlined steps, [Li et al. (2023g)](#) generates 502K pieces of synthetic data.

The LLaMA model [(Touvron et al., 2023b)](#), fine-tuned with this annotated dataset, surpasses all other LLaMA-based models on the Alpaca leaderboard without depending on distillation data, showcasing a highly efficient self-improvement process.

## Instruction Tuned LLMs

In this section, we detail widely-used LLM models in the community that are trained through instruction tuning.

## InstructonGPT

InstructGPT (176B) [(Ouyang et al., 2022)](#b106) is initialized with GPT-3 (176B) [(Brown et al., 2020b)](#) and then fine-tuned on human instructions. The fine-tuning procedure is composed of the following three steps: (1) supervised fine-tuning (SFT) on the human-filtered instruction dataset, which is collected from Playground API history records;

(2) training a reward model to predict human preferences based on an annotated dataset, which is constructed though human labors by sampling multiple responses for one instruction and rank them from the best to the worst; (3) further optimizing the model from Step 1 with new instructions and the trained reward model in step (2). Parameters are updated using the proximal policy optimization (PPO) [(Schulman et al., 2017)](#b121) method, a policy gradient reinforcement learning method. Steps (2) and (3) are alternated multiple times until the model performance does not significantly improve.

Overall, InstructGPT outperforms GPT-3. For automatic evaluations, InstructGPT outperforms GPT-3 by 10% on the TruthfulQA [(Lin et al., 2021)](#b88) dataset in terms of truthfulness and by 7% on the RealToxicityPrompts (Gehman et al., 2020) in terms of toxicity.

On NLP datasets (i.e., WSC), InstructGPT achieves comparable performance to GPT-3. For human evaluations, regarding four different aspects, including following correct instructions, following explicit constraints, fewer hallucinations, and generating appropriate responses, InstructGPT outperforms GPT-3 +10%, +20%, -20%, and +10%, respectively.

## BLOOMZ

BLOOMZ (176B) [(Muennighoff et al., 2022)](#b101) is initialized with BLOOM (176B) (Scao et al., Instruction fine-tuned LLMs # Params Base Model Fine-tuning Trainset Self-build Dataset Name Size Instruct-GPT (Ouyang et al., 2022) 176B GPT-3 (Brown et al., 2020b) Yes --BLOOMZ (Muennighoff et al., 2022)[foot_1](#foot_1) 176B BLOOM (Scao et al., 2022) No xP3 -FLAN-T5 (Chung et al., 2022)[foot_2](#foot_2) 11B T5 (Raffel et al., 2019) No FLAN 2021 -Alpaca (Taori et al., 2023a)[foot_3](#foot_3) 7B LLaMA (Touvron et al., 2023a) Yes -52K Vicuna (Chiang et al., 2023)[foot_4](#foot_4) 13B LLaMA (Touvron et al., 2023a) Yes -70K GPT-4-LLM (Peng et al., 2023)[foot_5](#foot_5) 7B LLaMA (Touvron et al., 2023a) Yes -52K Claude (Bai et al., 2022b) --Yes --WizardLM (Xu et al., 2023a)[foot_6](#foot_6) 7B LLaMA (Touvron et al., 2023a) Yes Evol-Instruct 70K ChatGLM2 (Du et al., 2022)[foot_7](#foot_7) 6B GLM (Du et al., 2022) Yes -1.1 Tokens LIMA (Zhou et al., 2023a) 65B LLaMA (Touvron et al., 2023a) Yes -1K OPT-IML (Iyer et al., 2022)[foot_8](#foot_8) 175B OPT (Zhang et al., 2022a) No --Dolly 2.0 (Conover et al., 2023a)[foot_9](#foot_9) 12B Pythia (Biderman et al., 2023) No -15K Falcon-Instruct (Almazrouei et al., 2023a)[foot_10](#foot_10) 40B Falcon (Almazrouei et al., 2023b) No --Guanaco (JosephusCheung, 2021)[foot_11](#foot_11) 7B LLaMA (Touvron et al., 2023a) Yes -586K Minotaur (Collective, 2023)[foot_12](#foot_12) 15B Starcoder Plus (Li et al., 2023f) No --Nous-Hermes (NousResearch, 2023)[foot_13](#foot_13) 13B LLaMA (Touvron et al., 2023a) No -300K+ TÜLU (Wang et al., 2023d)[foot_14](#foot_14) 6.7B OPT (Zhang et al., 2022a) No Mixed -YuLan-Chat (YuLan-Chat-Team, 2023)[foot_15](#foot_15) 13B LLaMA (Touvron et al., 2023a) Yes -250K MOSS (Tianxiang and Xipeng, 2023)[foot_16](#foot_16) 16B -Yes --Airoboros (Durbin, 2023)[foot_17](#foot_17) 13B LLaMA (Touvron et al., 2023a) Yes --UltraLM (Ding et al., 2023a)[foot_18](#foot_18) 13B LLaMA (Touvron et al., 2023a) Yes --

Table 3: An overview of LLMs tuned on IT datasets.

2022), and then fine-tuned on the instruction dataset xP3 [(Muennighoff et al., 2022)](#b101), a collection of human-instruction datasets in 46 languages, coming from two sources: (1) P3, which is a collection of (English instruction, English response) pairs; and (2) an (English instruction, Multilingual response) set which is transformed from multilingual NLP datasets (e.g., Chinese benchmarks) by filling task templates with predefined English instructions.

For automatic evaluation, BLOOMZ performs better than BLOOM in the zero-shot setting by +10.4%, 20.5%, and 9.8% on coreference resolution, sentence completion and natural language inference datasets, respectively. For the HumanEval benchmark [(Chen et al., 2021b)](#), BLOOMZ outperforms BLOOM by 10% in terms of the Pass@100 metric. For generative tasks, BLOOMZ receives +9% BLEU improvement compared to BLOOM on the lm-evaluation-harness benchmark[foot_19](#foot_19) .

## Flan-T5

Flan-T5 (11B) is is a large language model initialized with T5 (11B) [(Raffel et al., 2019)](#b114), and then fine-tuned on the FLAN dataset [(Longpre et al., 2023)](#b96). The FLAN dataset is a collection of (instruction, pairs) pairs, constructed from 62 datasets of 12 NLP tasks (e.g., natural language inference, commonsense reasoning, paraphrase generation) by filling templates with various instructions under a unified task formalization.

During fine-tuning, FLAN-T5 adapts the JAXbased T5X framework and selects the best model evaluated on the held-out tasks every 2k step. Compared with T5's pre-training stage, fine-tuning costs 0.2% computational resources (approximately 128 TPU v4 chips for 37 hours).

For evaluation, FLAN-T5 (11B) outperforms T5 (11B), and achieves comparable results to larger models, including PaLM (60B) [(Chowdhery et al., 2022)](#b28) in the few-shot setting. FLAN-T5 outperforms T5 by +18.9%, +12.3%, +4.1%, +5.8%, +2.1%, and +8% on MMLU [(Hendrycks et al., 2020b)](#), BBH [(Suzgun et al., 2022b)](#), TyDiQA [(Clark et al., 2020)](#b30), MGSM [(Shi et al., 2022)](#b122), open-ended generation, and RealToxicityPrompts (Gehman et al., 2020), respectively. In few-shot settings, FLAN-T5 outperforms PaLM +1.4% and +1.2% on the BBH and TyDiQA datasets.

## Alpaca

Alpaca (7B) [(Taori et al., 2023a](#)) is a language model trained by fine-tuning LLaMA (7B) [(Touvron et al., 2023a)](#) on the constructed instruction dataset generated by InstructGPT (175B, text-davinci-003) [(Ouyang et al., 2022)](#b106). The fine-tuning process takes around 3 hours on an 8-card 80GB A100 device with mixed precision training and fully shared data parallelism.

Alpaca (7B) achieves comparable performances to InstructGPT (175B,text-davinci-003) in terms of human evaluation.

Specifically, Alpaca outperforms InstructGPT on the self-instruct dataset, garnering 90 instances of victories compared to 89 instances.

## Vicuna

Vicuna (13B) [(Chiang et al., 2023](#b26)) is a language model trained by fine-tuning LLaMA (13B) [(Touvron et al., 2023a)](#) on the conversational dataset generated by ChatGPT[foot_20](#foot_20) .

The authors gathered user-shared ChatGPT conversations from ShareGPT.com[foot_21](#foot_21) , and got 70K conversation records after filtering out low-quality samples. LLaMA (13B) was fine-tuned on the constructed conversation dataset using a modified loss function tailored to multi-turn conversations. To better understand long context across multipleturn dialog, the authors expanded the max context length from 512 to 2048. For training, the authors adopted the gradient checkpointing and flash attention [(Dao et al., 2022)](#b36) techniques to reduce the GPU memory cost in the fine-tuning process. The fine-tuning process takes 24 hours on an 8 × 80GB A100 device with fully shared data parallelism.

The authors built a test set used exclusively to measure chatbots' performances. They collected a test set composed by 8 question categories, such as Fermi problems, role play scenarios, coding/math tasks, etc, and then asked [GPT-4 (OpenAI, 2023)](#) to rate models' responses considering helpfulness, relevance, accuracy, and detail. On the constructed test set, Vicuna (13B) outperforms Alpaca (13B) [(Taori et al., 2023a)](#) and LLaMA (13B) in 90% of the test questions, and generates equal or better rating responses compared to ChatGPT in 45% of the questions.

## GPT-4-LLM

GPT-4-LLM (7B) [(Peng et al., 2023](#b109)) is a language model trained by fine-tuning LLaMA (7B) [(Touvron et al., 2023a)](#) on the GPT-4 (OpenAI, 2023) generated instruction dataset. GPT-4-LLM is initialized with LLaMA, then fine-tuned in the following two steps: (1) supervised finetuning on the constructed instruction dataset. The authors used the instructions from Alpaca [(Taori et al., 2023a)](#), and then collected responses using GPT-4. LLaMA is fine-tuned on the GPT-4 generated dataset. The fine-tuning process takes approximately three hours on an 8*80GB A100 machine with mixed precision and fully shared data parallelism. (2) optimizing the step-1 model using the proximal policy optimization (PPO) [(Schulman et al., 2017)](#b121) method, the authors first built a comparison dataset by collecting responses from GPT-4, InstructGPT [(Ouyang et al., 2022)](#b106), and OPT-IML [(Iyer et al., 2022)](#b64) to a collection of instructions and then asked GPT-4 to rate each response from 1 to 10. Using the ratings, a reward model is trained based on OPT [(Zhang et al., 2022a)](#). The fine-tuned model from Step 1 is optimized by using the reward model to compute the policy gradient.

For evaluations, GPT-4-LLM (7B) outperforms not only the baseline model Alpaca (7B), but also larger models including Alpaca (13B) and LLAMA (13B). For automated evaluation, GPT-4-LLM (7B) outperforms Alpaca by 0.2, 0.5, and 0.7 on User-Oriented-Instructions-252 [(Wang et al., 2022c)](#), Vicuna-Instructions [(Chiang et al., 2023)](#b26), and Unnatural Instructions [(Honovich et al., 2022)](#b59) datasets, respectively. For human evaluation, regarding aspects including helpfulness, honesty, and harmlessness, GPT-4-LLM outperforms Alpaca by 11.7, 20.9, and 28.6 respectively.

## Claude

Claude[foot_22](#foot_22) is a language model trained by fine-tuning the pre-trained language model on an instruction dataset, aiming to generate helpful and harmless responses. The fine-tuning process consists of two stages: (1) supervised fine-tuning on the instruction dataset. The authors created an instruction dataset by collecting 52K different instructions, paired with responses generated by GPT-4. The finetuning process takes approximately eight hours on an 8-card 80GB A100 machine with mixed precision and fully shared data parallelism. ( [2](#)) optimizing the step-1 model with the proximal policy optimization [(Schulman et al., 2017)](#b121) method. The authors first built a comparison dataset by collecting responses from multiple large language models (e.g., [GPT-3 (Brown et al., 2020b)](#)) to the given collection of instructions and then asking [GPT-4 (OpenAI, 2023)](#) to rate each response. Using the ratings, a reward model is trained. Then, the fine-tuned model from Step 1 is optimized using the reward model with the proximal policy optimization method.

Claude generates more helpful and harmless responses compared to the backbone model. For automatic evaluations, Claude outperforms GPT-3 by 7% on the RealToxicityPrompts [(Gehman et al., 2020)](#b49) in terms of toxicity.

For human evaluations, regarding four different aspects, including following correct instructions, following explicit constraints, fewer hallucinations, and generating appropriate responses, Claude outperforms GPT-3 [(Brown et al., 2020b](#)) +10%, +20%, -20%, and +10%. respectively.

## WizardLM

WizardLM (7B) [(Xu et al., 2023a](#)) is a language model trained by fine-tuning LLaMA (7B) [(Touvron et al., 2023a)](#) on the instruction dataset Evol-Instruct generated by ChatGPT (details see Section 3.2). It is fine-tuned on a subset (with 70K) of Evol-Instruct to enable a fair comparison with Vicuna [(Chiang et al., 2023)](#b26). The fine-tuning process takes approximately 70 hours on 3 epochs based on an 8 V100 GPU with the Deepspeed Zero-3 [(Rasley et al., 2020)](#b116) technique. During inference, the max generation length is 2048.

To evaluate LLMs' performances on complex instructions, the authors collected 218 humangenerated instructions from real scenarios (e.g., open-source projects, platforms, and forums), called Evol-Instruct testset.

Evaluations are conducted on the Evol-Instruct testset and Vicuna's testset. For human evaluation, WizardLM outperforms Alpaca (7B) [(Taori et al., 2023a)](#) and Vicuna (7B) by a large margins, and generates equal or better responses on 67% test samples compared to ChatGPT. Automatic evaluation is conducted by asking GPT-4 to rate LLMs' reponses. Specifically, WizardLM gains performance boosts compared to Alpaca by +6.2%, +5.3% on the Evol-Instruct testset and Vicuna's test sets. WizardLM achieves outperforms Vicuna by +5.8 on the Evol-Instruct testset and +1.7% on the Vicuna's test set.

## ChatGLM2

ChatGLM2 (6B) [(Du et al., 2022](#b41)) is a language model trained by fine-tuning GLM (6B) [(Du et al., 2022)](#b41) on a bilingual dataset that contains both English and Chinese instructions The bilingual instruction dataset contains 1.4T tokens, with a 1:1 ratio of Chinese to English. Instructions in the dataset are sampled from the question-answering and dialogue completion tasks.

ChatGLM is initialized with GLM, then trained by the three-step fine-tuning strategy, which is akin to InstructGPT [(Ouyang et al., 2022)](#b106). To better model contextual information across multi-turn conversations, the authors expanded the maximum context length from 1024 to 32K. To reduce GPU memory cost in the fine-tuning stage, the authors employed multi-query attention and causal mask strategies. During inference, ChatGLM2 requires 13GB GPU memory with FP16 and supports conversations up to 8K in length with 6GB GPU memory using the INT4 model quantization technique.

Evaluations are conducted on four English and Chinese benchmarks, including MMLU (English) [(Hendrycks et al., 2020b)](#), C-Eval (Chinese) [(Huang et al., 2023)](#), GSM8K (Math) [(Cobbe et al., 2021)](#b31), and BBH (English) [(Suzgun et al., 2022b)](#). ChatGLM2 (6B) outperforms GLM (6B) and the baseline model ChatGLM (6B) on all benchmarks. Specifically, ChatGLM2 outperforms GLM by +3.1 on MMLU, +5.0 on C-Eval, +8.6 on GSM8K, and +2.2 on BBH. ChatGLM2 achieves better performances than ChatGLM by +2.1, +1.2, +0.4, +0.8 on MMLU, C-Eval, GSM8K and BBH, respectively. 4.10 LIMA LIMA (65B) [(Zhou et al., 2023a)](#) is a large language model trained by fine-tuning LLaMA (65B) [(Touvron et al., 2023a](#)) on an instruction dataset, which is constructed based on the proposed superficial alignment hypothesis.

The superficial alignment hypothesis refers to the idea that the knowledge and capabilities of a model are almost acquired in the pre-training stage, while the alignment training (e.g., instruction tuning) teaches models to generate responses under user-preferred formalizations. Based on the superficial alignment hypothesis, the authors claimed that large language models can generate user-satisfied responses by fine-tuning it on a small fraction of instruction data. Therefore, the authors built instruction train/valid/test sets to verify this hypothesis.

Evaluations are conducted on the constructed test set. For human evaluations, LIMA outperforms InstructGPT and Alpaca by 17% and 19%, respectively.

Additionally, LIMA achieves comparable results to BARD[foot_23](#foot_23) , Cladue[foot_24](#foot_24) , and GPT-4. For automatic evaluation, which is conducted by asking GPT-4 to rate responses and a higher rate score denotes better performance, LIMA outperforms InstructGPT and Alpaca by 20% and 36%, respectively, achieving comparable results to BARD, while underperforming Claude and GPT-4. Experimental results verify the proposed superficial alignment hypothesis.

## Others

OPT-IML (175B) [(Iyer et al., 2022)](#b64) is a large language model trained by fine-tuning the OPT (175B) [(Zhang et al., 2022a)](#) model on the constructed Instruction Meta-Learning (IML) dataset, which consists of over 1500 NLP tasks from 8 publicly available benchmarks such as PromptSource [(Bach et al., 2022)](#b5), FLAN [(Longpre et al., 2023)](#b96), and Super-NaturalInstructions [(Wang et al., 2022e)](#). After fine-tuning, OPT-IML outperforms OPT across all benchmarks. Dolly 2.0 (12B) [(Conover et al., 2023a](#)) is initialized with the pre-trained language model Pythia (12B) [(Biderman et al., 2023)](#b13), and fine-tuned on the instruction dataset databricksdolly-15k[foot_25](#foot_25) , which contains 7 categories of NLP tasks such as text classification and information extraction. After fine-tuning, Dolly 2.0 (12B) outperforms Pythia (12B) on the EleutherAI LLM Evaluation Harness benchmark [(Gao et al., 2021)](#) by a large margin, and achieves comparable performances to GPT-NEOX (20B) [(Black et al., 2022)](#b14), which has two times more parameters compared to Dolly 2.0 (12B).

Falcon-Instruct (40B) [(Almazrouei et al., 2023a)](#) is a large language model trained by finetuning Falcon (40B) [(Almazrouei et al., 2023b)](#) on an English dialogue dataset, which contains 150 million tokens from the Baize dataset [(Xu et al., 2023c)](#), with an additional 5% of the data from the RefinedWeb dataset [(Penedo et al., 2023)](#b108). To reduce memory usage, the authors employed flash attention [(Dao et al., 2022)](#b36) and multi-query techniques. For evaluation, Falcon-Instruct (40B) achieved better performance on the Open LLM Leaderboard [(Beeching et al., 2023)](#b12)[foot_26](#foot_26) compared to the baseline model Falcon (40B), and outperforms the Guanaco (65B), which has more model parameters.

Guanaco (7B) (JosephusCheung, 2021) is a multi-turn dialog language model trained by finetuning LLaMA (7B) [(Touvron et al., 2023a)](#) on the constructed multilingual dialogue dataset. The multilingual dialogue dataset comes from two sources: Alpaca [(Taori et al., 2023a)](#), which contains 52K English instruction data pairs; and a multilingual (e.g., Simplified Chinese, Traditional Chinese, Japanese, German) dialogue data, which contains 534K+ multi-turn conversations. After fine-tuning, Guanaco is to generate role-specific responses and continuous responses on a given topic in multi-turn conversations.

Minotaur (15B) is a large language model trained by fine-tuning the Starcoder Plus (15B) [(Li et al., 2023f)](#) on open-source instruction datasets including WizardLM [(Xu et al., 2023a)](#) and GPTeacher-General-Instruct[foot_27](#foot_27) .

For model inference, Minotaur supports a maximum context length of 18K tokens.

Nous-Herme (13B) is a large language model trained by fine-tuning LLaMA (13B) [(Touvron et al., 2023a](#)) on an instruction dataset, which contains over 300k instructions, sampled from GPTeacher[foot_28](#foot_28) , CodeAlpaca [(Chaudhary, 2023)](#b19), GPT-4-LLM [(Peng et al., 2023)](#b109), Unnatural Instructions [(Honovich et al., 2022)](#b59), and BiologyPhysicsChemistry subsets in the Camel-AI [(Li et al., 2023c)](#). Responses are generated by GPT-4. For evaluations, Nous-Herme (13B) achieves comparable performances to GPT-3.5turbo on multiple tasks like ARC challenge [(Clark et al., 2018)](#b30) and BoolQ [(Clark et al., 2019)](#b29).

TÜLU (6.7B) [(Wang et al., 2023d)](#) is a large language model trained by fine-tuning OPT (6.7B) [(Zhang et al., 2022a](#)) on a mixed instruction dataset, which contains FLAN V2 [(Longpre et al., 2023)](#b96), CoT [(Wei et al., 2022)](#b151), Dolly [(Conover et al., 2023a)](#), Open Assistant-1[foot_29](#foot_29) , GPT4-Alpaca[foot_30](#foot_30) , Code-Alpaca [(Chaudhary, 2023)](#b19), and ShareGPT[foot_31](#foot_31) . After fine-tuning, TÜLU (6.7B) reaches on average 83% of ChatGPT's performance and 68% of GPT-4's performance.

YuLan-Chat (13B) (YuLan-Chat-Team, 2023) is a language model trained by fine-tuning LLaMA (13B) [(Touvron et al., 2023a](#)) on a constructed bilingual dataset, which contains 250,000 Chinese-English instruction pairs. After fine-tuning, YuLan-Chat-13B achieves comparable results to the state-of-the-art open-source model ChatGLM (6B) [(Du et al., 2022)](#b41), and outperforms Vicuna (13B) [(Chiang et al., 2023)](#b26) on the English BBH3K (BBH3K is a subset of BBH benchmark [(Srivastava et al., 2022a](#))) dataset.

## MOSS (16B)

[foot_32](#foot_32) is a bilingual dialogue language model, which aims to engage in multi-turn conversations and utilize various plugins, trained by fine-tuning on dialogue instructions. After finetuning, MOSS outperforms the backbone model and generates responses that better align with human preferences. Airoboros (13B) 17 is a large language model trained by fine-tuning LLAMA (13B) [(Touvron et al., 2023a](#)) on the Self-instruct dataset [(Wang et al., 2022c)](#). After fine-tuning, Airoboros significantly outperforms LLAMA (13B) [(Touvron et al., 2023a](#)) on all benchmarks and achieves highly comparable results to models fine-tuned specifically for certain benchmarks.

UltraLM (13B) [(Ding et al., 2023a)](#) is a large language model trained by fine-tuning LLAMA (13B) [(Touvron et al., 2023a)](#). For evaluation, UltraLM (13B) outperforms Dolly (12B) [(Conover et al., 2023a)](#) and achieves the winning rate up to 98%. Additionally, it surpasses the previous best open-source models (i.e., Vicuna [(Chiang et al., 2023)](#b26) and WizardLM [(Xu et al., 2023a](#))) with winning rates of 9% and 28%, respectively.

5 Multi-modality Instruction Tuning 5.1 Multi-modality Datasets MUL-TIINSTRUCT [(Xu et al., 2022](#b161)) is a multimodal instruction tuning dataset consisting of 62 diverse multimodal tasks in a unified seqto-seq format. This dataset covers 10 broad categories and its tasks are derived from 21 existing open-sourced datasets. Each task is equipped with 5 expert-written instructions. For the existing tasks, the authors use the input/output pairs from their available open-source datasets to create instances. While for each new task, the authors create 5k to 5M instances by extracting the necessary information from instances of existing tasks or reformulating them.

The MUL-TIINSTRUCT dataset has demonstrated its efficiency in enhancing various transfer learning technique.

For example, fine-tuning the OFA model (930M) [(Wang et al., 2022a)](#) using various transfer learning strategies such as Mixed Instruction Tuning and Sequential Instruction Tuning on MUL-TIINSTRUCT improve the zeroshot performance across all unseen tasks. On commonsense VQA task, OFA fine-tuned on MUL-TIINSTRUCT achieves 50.60 on RougeL and 31.17 on accuracy, while original OFA achieves 14.97 on RougeL and 0.40 on accuracy.

## PMC-VQA (Zhang et al., 2023c) is a largescale medical visual question-answering dataset that comprises 227k image-question pairs of 149k

Multi-modality Instruction Fine-tuning Dataset Modalities # Tasks Modality Pair # Instance MUL-TIINSTRUCT (Xu et al., 2022) 1 Image-Text 5k to 5M per task 62 PMC-VQA (Zhang et al., 2023c) 2 Image-Text 227k 2 LAMM (Yin et al., 2023) 3 Image-Text 186k 9 Point Cloud-Text 10k 3 Vision-Flan (Xu et al., 2024) 4 Multi-Pairs Over 1M 200+ ALLAVA (Chen et al., 2024a) 5 Image-Text 1.4M 2 ShareGPT4V (Chen et al., 2023a) 6 Image-Text 1.2M 2 1 [https://github.com/VT-NLP/MultiInstruct](https://github.com/VT-NLP/MultiInstruct) 2 [https://github.com/xiaoman-zhang/PMC-VQA](https://github.com/xiaoman-zhang/PMC-VQA) 3 [https://github.com/OpenLAMM/LAMM](https://github.com/OpenLAMM/LAMM) 4 [https://vision-flan.github.io/](https://vision-flan.github.io/) 5 [https://github.com/FreedomIntelligence/ALLaVA](https://github.com/FreedomIntelligence/ALLaVA) 6 [https://sharegpt4v.github.io/](https://sharegpt4v.github.io/) Table 4: An overview of multi-modality instruction fine-tuning datasets. images, covering various modalities or diseases. The dataset can be used for both open-ended and multiple-choice tasks. The pipeline for generating the PMC-VQA dataset involves collecting imagecaption pairs from the PMC-OA (Lin et al., 2023b) dataset, using ChatGPT to generate question-answer pairs, and manually verifying a subset of the dataset for quality. The authors propose a generative-based model MedVInT for medical visual understanding by aligning visual information with a large language model. MedVInT pretrained on PMC-VQA achieves stateof-the-art performance and outperforms existing models on VQA-RAD (Lau et al., 2018) and SLAKE (Liu et al., 2021a) benchmarks, with 81.6% accuracy on VQA-RAD and 88.0% accuracy on SLAKE. LAMM (Yin et al., 2023) is a comprehensive multi-modal instruction tuning dataset for 2D image and 3D point cloud understanding. LAMM contains 186K language-image instructionresponse pairs, and 10K language-point cloud instruction-response pairs. The authors collect images and point clouds from publicly available datasets and use the GPT-API and self-instruction methods to generate instructions and responses based on the original labels from these datasets. LAMM-Dataset includes data pairs for commonsense knowledge question answering by incorporating a hierarchical knowledge graph label system from the Bamboo (Zhang et al., 2022b) dataset and the corresponding Wikipedia description. The authors also propose the LAMM-Benchmark, which evaluates existing multi-modal language models (MLLM) on various computer vision tasks. It includes 9 common image tasks and 3 common point cloud tasks, and LAMM-Framework, a primary MLLM training framework that differentiates the encoder, projector, and LLM finetuning blocks for different modalities to avoid modality conflicts.

Vision-Flan [(Xu et al., 2024)](#b160) is the largest public-available human-annotated visual instruction tuning dataset that consists of 1,664,261 instances and 200+ diverse vision-language tasks derived from 101 open-source computer vision datasets. Each task is accompanied by expertly written instructions and meticulously crafted templates for inputs and outputs. The dataset covers a broad spectrum of tasks, including image captioning, visual question-answering, and visual comprehension. Designed to enhance research and application in vision-language model domains, Vision-Flan aims to expand the horizons of interaction and comprehension between visual and linguistic modalities. It provides researchers and developers with a valuable resource to push the envelope of vision-language models and to innovate algorithms across a diverse array of fields.

ALLaVA [(Chen et al., 2024a)](#) represents an opensource, extensive dataset tailored for fine-tuning visual question-answering models, featuring 1.4M entries that include detailed captions, intricate instructions, and comprehensive answers produced by GPT-4V [(Yang et al., 2023b)](#). To craft highquality captions and visual question-answers, [Chen et al. (2024a)](#) introduced a method to distill both a caption and a QA pair for an image in a single interaction. This process involves initially presenting GPT-4V [(Yang et al., 2023b](#)) with an image, followed by prompting it to generate both a detailed caption and a visual question-answer pair. This approach of incorporating additional visual data enables the model to develop a more nuanced understanding of both the visual and textual elements, enhancing its capacity to deliver precise and contextually appropriate answers. Furthermore, this method has the potential to reduce the occurrence of hallucinations by providing the model with more contextual information (visual data).

ShareGPT4V [(Chen et al., 2023a](#)) is a collection of highly descriptive image-text pairs, consisting of two components: 100K captions generated by GPT4-Vision [(Yang et al., 2023b)](#) from a variety of images, and 1.2M captions developed using their pre-trained model, which was trained on the initial set of 100K high-quality captions. These captions comprehensively cover aspects such as global knowledge, object attributes, spatial relationships, and aesthetic evaluations. Utilizing this dataset, the ShareGPT4V-7B model, once fine-tuned, surpasses competing 7B-scale LMMs across all 11 benchmark tests. Notably, it secures a remarkable cumulative score of 1943.8 on the MME benchmark, outperforming the second-place Qwen-VL-Chat-7B [(Bai et al., 2023)](#b6) model, which was trained with 1.4 billion samples, by 95.6 points.

## Multi-modality Instruction Tuning Models

InstructPix2Pix (983M) [(Brooks et al., 2022)](#b15) is a conditional diffusion model trained by fine-tuning Stable Diffusion (983M) [(Rombach et al., 2022)](#b117) on a constructed multi-modal dataset that contains more than 450K text editing instructions and corresponding images before and after the edit. The authors combine the abilities of two large-scale pretrained models, a language model GPT-3 [(Brown et al., 2020b)](#) and a text-to-image model Stable Diffusion [(Rombach et al., 2022)](#b117), to generate the the training dataset. GPT-3 is fine-tuned to generate text edits based on image prompts, while Stable Diffusion is used to convert the generated text edits into actual image edits. InstructPix2Pix is then trained on this generated dataset using a latent diffusion objective. Figure [7](#fig_8) shows the process of generating image editing dataset and training the diffusion model on that dataset. The authors compares the proposed method qualitatively with previous works such as SDEdit [(Meng et al., 2022)](#b98)  LLaVA (13B) [(Liu et al., 2023b)](#) is a large multimodal model developed by connecting the visual encoder of CLIP (400M) [(Radford et al., 2021)](#b111) with the language decoder LLaMA (7B) [(Touvron et al., 2023a)](#). LLaVA is fine-tuned using the generated instructional vision-language dataset consisted of 158K unique language-image instruction-following samples. The data collection process involved creating conversation, detailed description, and complex reasoning prompts. GPT-4 is used to convert image-text pairs into appropriate instruction-following format for this dataset. Visual features such as captions and bounding boxes were used to encode images. LLaVA yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.

Video-LLaMA [(Zhang et al., 2023b](#)) is a multimodal framework that enhances large language models with the ability to understand both visual and auditory content in videos. The architecture of Video-LLaMA consists of two branche encoders: the Vision-Language (VL) Branch and the Audio-Language (AL) Branch, and a language decoder (Vicuna (7B/13B) [(Chiang et al., 2023)](#b26), LLaMA (7B) [(Touvron et al., 2023a)](#), etc.). The VL Branch includes a frozen pre-trained image encoder (pre-trained vision component of BLIP-2 [(Li et al., 2023d)](#), which includes a ViT-G/14 and a pre-trained Q-former), a position embedding layer, a video Q-former and a linear layer. The AL Branch includes a pre-

Multi-modality Instruction # Params Modality Base Model Fine-tuning Trainset Fine-tuned LLMs Model Name # Params Self-build Size InstructPix2Pix (Brooks et al., 2022)[foot_33](#foot_33) 983M I/T Stable Diffusion 983M Yes 450K LLaVA (Liu et al., 2023b)[foot_34](#foot_34) 13B I/T CLIP (Radford et al., 2021) 400M Yes 158K LLaMA (Touvron et al., 2023a) 7B LLaMA (Touvron et al., 2023a) 7B Video-LLaMA (Zhang et al., 2023b)[foot_35](#foot_35) -I/T/V/A BLIP-2 (Li et al., 2023d) -No -ImageBind (Girdhar et al., 2023) -Vicuna (Chiang et al., 2023) 7B/13B InstructBLIP (1.2B) (Dai et al., 2023)[foot_36](#foot_36) -I/T/V BLIP-2 (Li et al., 2023d) -No -Otter (Li et al., 2023b)[foot_37](#foot_37) -I/T/V OpenFlamingo (Awadalla et al., 2023) 9B Yes 2.8M MultiModal-GPT (Gong et al., 2023)[foot_38](#foot_38) -I/T/V OpenFlamingo (Awadalla et al., 2023) 9B No -

Table 5: An overview of multi-modality instruction fine-tuned LLMs. I/T/V/A stand for Image/Text/Video/Audio trained audio encoder (ImageBind [(Girdhar et al., 2023)](#b50)) and an Audio Q-former. Figure [8](#fig_9) shows the overall architecture of Video-LLaMA with Vision-Language Branch and Audio-Language Branch. The VL Branch is trained on the Webvid-2M [(Bain et al., 2021)](#b9) video caption dataset with a video-to-text generation task, and fine-tuned on the instruction tuning data from [MiniGPT-4 (Zhu et al., 2023b)](#), LLaVA [(Liu et al., 2023b)](#) and VideoChat [(Li et al., 2023e)](#).

The AL Branch is trained on video/image instrucaption data to connect the output of ImageBind to language decoder. After finetuning, Video-LLaMA can perceive and comprehend video content, demonstrating its ability to integrate auditory and visual information, understand static images, recognize common-knowledge concepts, and capture temporal dynamics in videos.

InstructBLIP (1.2B) [(Dai et al., 2023](#b35)) is a vision-language instruction tuning framework initialized with a pre-trained BLIP-2 [(Li et al., 2023d)](#)) model consisting of an image encoder, an LLM (FlanT5 (3B/11B) [(Chung et al., 2022)](#) or Vicuna (7B/13B) [(Chiang et al., 2023)](#b26)), and a Query Transformer (Q-Former) to bridge the two. As shown in Figure [9](#fig_10), the Q-Former extracts instruction-aware visual features from the output embeddings of the frozen image encoder, and feeds the visual features as soft prompt input to the frozen LLM. The authors evaluate the proposed InstructBLIP model on a variety of visionlanguage tasks, including image classification, image captioning, image question answering, and visual reasoning. They use 26 publicly available datasets, dividing them into 13 held-in and 13 held-out datasets for training and evaluation. The authors demonstrate that InstructBLIP achieves state-of-the-art zero-shot performance on a wide range of vision-language tasks. InstructBLIP yields an average relative improvement of 15.0% when compared to BLIP-2, smallest InstructBLIP (4B) outperforms Flamingo (80B) [(Alayrac et al., 2022)](#b1) on all six shared evaluation datasets with an average relative improvement of 24.8%.

Otter [(Li et al., 2023b](#)) is a multi-modal model trained by fine-tuning OpenFlamingo (9B) [(Awadalla et al., 2023)](#), with the language and vision encoders frozen and only fine-tuning the Perceiver resampler module, cross-attention layers, and input/output embeddings. The authors organize diverse multi-modal tasks covering 11 categories and build multi-modal in-context instruction tuning datasets MIMIC-IT of 2.8M multimodal instruction-response pairs, which consists of imageinstruction-answer triplets, where the instructionanswer is tailored to the image. Each data sample also includes context, which contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet. Otter demonstrates the ability to follow user instructions more accurately and provide more detailed descriptions of images compared to OpenFlamingo [(Awadalla et al., 2023)](#).

MultiModal-GPT [(Gong et al., 2023](#b51)) is a multimodal instruction tuning model that is capable of following diverse instructions, generating detailed captions, counting specific objects, and addressing general inquiries. MultiModal-GPT is trained by fine-tuning OpenFlamingo (9B) [(Awadalla et al., 2023)](#) on various created visual instruction data with open datasets, including VQA, Image Captioning, Visual Reasoning, Text OCR, and Visual Dialogue. The experiments demonstrate the proficiency of MultiModal-GPT in maintaining continuous dialogues with humans.

## Domain-specific Instruction Tuning

In this section, we describe instruction tuning in different domains and applications.

## Dialogue

InstructDial [(Gupta et al., 2022)](#b55) is an instruction tuning framework designed for dialogue. It contains a collection of 48 dialogue tasks in a consistent text-to-text format created from 59 dialogue datasets. Each task instance includes a task description, instance inputs, constraints, instructions, and output. To ensure adherence to instructions, the framework introduces two metatasks: (1) an instruction selection task, where the model selects the instruction corresponding to a given input-output pair; and (2) an instruction binary task, where the model predicts "yes" or "no" if an instruction leads to a given output from an input. Two base models T0-3B [(Sanh et al., 2021)](#b119) (3B parameters version of T5 [(Lester et al., 2021)](#b72)) and BART0 [(Lin et al., 2022](#)) (406M parameters based on Bart-large [(Lewis et al., 2019)](#b73)) are finetuned on the tasks from InstructDial. InstructDial achieves impressive results on unseen dialogue datasets and tasks, including dialogue evaluation and intent detection. Moreover, it delivers even better results when applied to a few-shot setting.

## Intent Classification and Slot Tagging

LINGUIST [(Rosenbaum et al., 2022)](#b118) finetunes AlexaTM 5B (Soltan et al., 2022), a 5-billionparameter multilingual model, on the instruction dataset for intent classification and slot tagging tasks. Each instruction consists of five blocks: (i) the language of the generated output, (ii) intention, (iii) slot types and values to include in the output (e.g., the number 3 in [3, snow] corresponds the slot type, and snow is the value used for that slot), (iv) a mapping from slot type labels to numbers, and (v) up to 10 examples to instruct the format of the outputs. LINGUIST shows significant improvements over state-of-the-art approaches in a 10-shot novel intent setting using the SNIPS dataset [(Coucke et al., 2018)](#b34). In the zero-shot crosslingual setting of the mATIS++ dataset [(Xu et al., 2020)](#b159), LINGUIST surpasses a strong baseline of Machine Translation with Slot Alignment across 6 languages while maintaining intent classification performance.

## Information Extraction

InstructUIE [(Wang et al., 2023c](#)) is a unified information extraction (IE) framework based on instruction tuning, which transforms IE tasks to the seq2seq format and solves them by fine-

Domain Type Domain-specific Instruction Base Model Trainset Size Fine-tuned LLMs Model Name # Params Dialogue InstructDial (Gupta et al., 2022)[foot_39](#foot_39) T0 (Sanh et al., 2021) 3B -Classification LINGUIST (Rosenbaum et al., 2022) AlexaTM (Soltan et al., 2022) 5B 13K Information extraction InstructUIE (Wang et al., 2023c)[foot_40](#foot_40) FlanT5 (Chung et al., 2022) 11B 1.0M Sentiment analysis IT-MTL (Varia et al., 2022)[foot_41](#foot_41) T5 (Raffel et al., 2019) 220M -Writing Writing-Alpaca-7B (Zhang et al., 2023d)[foot_42](#foot_42) LLaMA (Touvron et al., 2023a) 7B -CoEdIT (Raheja et al., 2023)[foot_43](#foot_43) FlanT5 (Chung et al., 2022) 11B CoPoet (Chakrabarty et al., 2022)[foot_44](#foot_44) T5 (Raffel et al., 2019) 11B Medical Radiology-GPT (Liu et al., 2023c)[foot_45](#foot_45) Alpaca (Taori et al., 2023a) 7B 122K ChatDoctor (Li et al., 2023j)[foot_46](#foot_46) LLaMA (Touvron et al., 2023a) 7B 100K ChatGLM-Med (Wang et al., 2023a)[foot_47](#foot_47) ChatGLM (Du et al., 2022) 6B -Arithmetic Goat (Liu and Low, 2023)[foot_48](#foot_48) LLaMA (Touvron et al., 2023a) 7B 1.0M Code WizardCoder (Luo et al., 2023)[foot_49](#foot_49) StarCoder (Li et al., 2023f) 15B 78K

Table 6: An overview of domain-specific instruction fine-tuned LLMs.  The instruction dataset comprises approximately 82K <instruction: source, target> pairs. As shown in Figure [11](#fig_12), the model takes instructions from the user specifying the characteristics of the desired text, such as "Make the sentence simpler", and outputs the edited text. CoEdIT achieves state-of-the-art performance on several text editing tasks, including grammatical error correction, text simplification, iterative text editing, and three stylistic editing tasks: formality style transfer, neutralization, and paraphrasing. Furthermore, it can generalize well to new, adjacent tasks not seen during fine-tuning.

## CoPoet

( [Chakrabarty et al., 2022](#b18)) is a collaborative poetry writing tool that utilizes a large language model (e.g. T5-3B, T5-11B and T0-3B models) trained on a diverse collection of instructions for poetry writing. Each sample in the instruction dataset includes an <instruction, poem_line> pair. There are three major types of instructions: Continuation, Lexical Constraints, and Rhetorical Techniques. The CoPoet is guided by user instructions that specify desired attributes of the poetry, such as writing a sentence about "love" or ending a sentence with "fly." Not only is the system competitive with publicly available LLMs trained on instructions, such as InstructGPT [(Ouyang et al., 2022)](#b106), but it is also capable of satisfying unseen compositional instructions.

## Medical

Radiology-GPT [(Liu et al., 2023c](#)) is a finetuned Alpaca-7B [(Taori et al., 2023a)](#) model for radiology, which utilizes an instruction tuning approach on an extensive dataset of radiology domain knowledge. Radiology reports usually include two corresponding sections: "Findings" and "Impression". The "Findings" section contains detailed observations from the radiology images, while the "Impression" section summarizes the interpretations drawn from those observations. Radiology-GPT provides a brief instruction to the "Findings" text: "Derive the impression from findings in the radiology report". The "Impression" text from the same report serves as the target output. In comparison to general language models such as StableLM (Islamovic), Dolly [(Conover et al., 2023a)](#), and LLaMA [(Touvron et al., 2023a)](#), Radiology-GPT demonstrates significant adaptability in radiological diagnosis, research, and communication.

ChatDoctor [(Li et al., 2023j)](#) is based on the fine-tuned LLaMa-7B [(Touvron et al., 2023a)](#) model, utilizing the alpaca instruction dataset [(Taori et al., 2023a)](#) and the HealthCareMagic100k patient-doctor dialogue dataset. And prompt templates are designed for retrieving external knowledge databases, such as the Disease Database and Wikipedia retrieval, during doctor-patient conversations to obtain more accurate outputs from the model. The ChatDoctor significantly improves the model's ability to comprehend patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, the accuracy of its responses is substantially improved.

ChatGLM-Med [(Wang et al., 2023a](#)) is finetuned on the Chinese medical instruction dataset based on the ChatGLM-6B [(Du et al., 2022)](#b41) model. The instruction dataset comprises medically relevant question and answer pairs, created using the GPT 3.5 API and the Medical Knowledge Graph.

This model improves the questionanswering performance of ChatGLM [(Du et al., 2022)](#b41) in the medical field.

## Arithmetic

Goat [(Liu and Low, 2023](#b94)) is a fine-tuned LLaMA-7B [(Touvron et al., 2023a](#)) model based on instructions, which aims to solve arithmetic problems. It expresses arithmetic problems in the form of natural language question answering, such as "What is 8914/64?", by generating hundreds of instruction templates using [ChatGPT (OpenAI, 2022)](#). The model applies various techniques to enhance its adaptability to diverse question formats, such as randomly removing spaces between numbers and symbols in the arithmetic expression and replacing "*" with "x" or "times". The Goat model achieves state-of-the-art performance on the BIG-bench [(Srivastava et al., 2022a)](#) arithmetic subtask. In particular, zero-shot Goat-7B matches or exceeds the accuracy achieved by the few-shot PaLM-540B [(Chowdhery et al., 2022)](#b28). [(Luo et al., 2023)](#) utilizes StarCoder 15B [(Li et al., 2023f)](#) as the foundation with complex instruction tuning, by adapting the Evol-Instruct method [(Xu et al., 2023a)](#) to the domain of code. The training dataset is produced through iterative application of the Evol-Instruct technique on the Code Alpaca dataset [(Taori et al., 2023b)](#), which includes the following attributes for each sample: instruction, input, and expected output. For instance, when the instruction is "Amend the following SQL query to select distinct elements", the input is the SQL query, and the expected output is the generated answer. The WizardCoder outperforms all other open-source Code LLMs and even surpasses the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+.

## Code

## WizardCoder

## Efficient Tuning Techniques

Efficient fine-tuning techniques aim at adapting LLMs to downstream tasks by optimizing a small fraction of parameters in multiple ways, i.e., addition-based, specification-based, and reparameterization-based. Addition-based methods introduce extra trainable parameters or modules not present in the original model. Representative methods include adapter tuning [(Houlsby et al., 2019)](#b60) and prompt-based tuning [(Schick and Schütze, 2021)](#b120). Specification-based methods specify certain inherent model parameters to be tuned while freezing others. For example, BitFit [(Zaken et al., 2022)](#b171) tunes the bias terms of the pre-trained model. Reparameterization methods transform model weights into more parameter-efficient forms for tuning. The key hypothesis is that model adaptation is low-rank, so weights can be reparameterized into lowrank factors or a low-dimensional subspace (e.g., [LoRA (Hu et al., 2021)](#)). Intrinsic prompt tuning finds a low-dimensional subspace shared by tuning prompts across diverse tasks.

## LoRA

Low-Rank Adaptation (LoRA) [(Hu et al., 2021)](#b61) enables efficient adaptation of LLMs using lowrank updates. LoRA use DeepSpeed [(Rasley et al., 2020)](#b116) as the training backbone. The key insight of LoRA is that the actual change in LLMs' weights required for new task adaptation lies in a lowdimensional subspace. Specifically, for a pretrained weight matrix W 0 , the authors model the adapted weight matrix as W 0 + ∆W , where ∆W is a low rank update. ∆W is parameterized as ∆W = BA, where A and B are much smaller trainable matrices. The rank r of ∆W is chosen to be much smaller than the dimensions of W 0 . The intuition is that instead of directly training all of W 0 , the authors train low-dimensional A and B, which indirectly trains W 0 in a low-rank subspace of directions that matter for the downstream task. This results in far fewer trainable parameters compared to full finetuning. For GPT-3, LoRA reduces the number of trainable parameters by 10,000x and memory usage by 3x compared to full fine-tuning.

## HINT

HINT [(Ivison et al., 2022)](#b63) combines the generalization benefits of instruction tuning with efficient on-demand fine-tuning, avoiding repeatedly processing lengthy instructions. The essence of HINT lies in hypernetworks, which generate parameter-efficient modules for LLMs adaptation based on natural language instructions and few-shot examples. The adopted hypernetwork converts instructions and few-shot examples into a encoded instruction and generates adapter and prefix parameters using a pretrained text encoder and cross-attention based parameter generator. Then, the generated adapters and prefixes are inserted into the backbone model as efficient tuning modules. At inference, the hypernetwork performs inference only once per task to generate adapted modules. The benefits are that HINT can incorporate long instructions and additional fewshots without increasing compute, unlike regular fine-tuning or input concatenation methods.

## Qlora

QLORA [(Dettmers et al., 2023)](#b37) includes optimal quantization and memory optimization, aiming at providing efficient and effective LLMs finetuning. QLORA includes 4-bit NormalFloat (NF4) Quantization, which is a quantization scheme optimized for the typical normal distribution of LLM weights. By quantizing based on the quantiles of a normal distribution, NF4 provides better performance than standard 4-bit integer or float quantization. To further reduce memory, the quantization constants are themselves quantized to 8 bits. This second level of quantization saves an additional 0.37 bits per parameter on average. QLORA leverages NVIDIA's unified memory feature to page optimizer states to CPU RAM when GPU memory is exceeded. avoiding out-of-memory during training. QLORA enables training a 65B parameter LLM on a single 48GB GPU with no degradation compared to full 16bit finetuning. QLORA works by freezing the 4-bit quantized base LLM, then backpropagating through it into a small set of 16-bit low-rank adapter weights which are learned.

## LOMO

LOw-Memory Optimization (LOMO) [(Lv et al., 2023)](#) enables full parameter fine-tuning of LLMs using limited computational resources through a fusion of gradient computation and update. The essence is to fuse gradient computation and parameter update into one step during backpropagation, thereby avoiding storage of full gradient tensors. Firstly, theoretical analysis is provided in LOMO on why SGD can work well for fine-tuning large pre-trained models despite its challenges on smaller models. In addition, LOMO updates each parameter tensor immediately after computing its gradient in backpropagation.

Storing the gradient of one parameter at a time reduces gradient memory to O(1). LOMO employs gradient value clipping, separate gradient norm computation pass and dynamic loss scaling to stabilize training. The integration of activation checkpointing and ZeRO optimization methods saves memory.

## Delta-tuning

Delta-tuning [(Ding et al., 2023b)](#) provides optimization and optimal control perspectives for theoretical analyzation. Intuitively, delta-tuning performs subspace optimization by restricting tuning to a low-dimensional manifold. The tuned parameters act as optimal controllers guiding model behavior on downstream tasks.

## Evaluation, Analysis and Criticism

## Close-ended Evaluations

It is widely accepted among researchers that general-purpose models must demonstrate proficiency in certain core tasks before they can effectively generalize to meet diverse real-world needs. Close-ended evaluations help achieve this objective, often involving multiple-choice questions to assess the performance of LLMs. Below are 6 widely used close-ended evaluations:

(1) MMLU. Massive Multitask Language Understanding (MMLU) [(Hendrycks et al., 2020a)](#) consists of 14079 questions covering 57 tasks including elementary mathematics, US history, computer science, law, and more. The wide range of subjects and complex questions make MMLU suitable for testing the model's language comprehension and decision-making capabilities.

(2) MATH and (3) GSM8K. MATH [(Hendrycks et al., 2021)](#b58) and GSM8K [(Cobbe et al., 2021)](#b31) are two distinct mathematical datasets utilized for evaluating different aspects of model capabilities. The MATH [(Hendrycks et al., 2021)](#b58) dataset comprises 12,500 complex competition-level mathematics problems, primarily designed to assess the ability of models to tackle challenging and advanced mathematical questions typically encountered at the college level. Conversely, the GSM8K [(Cobbe et al., 2021)](#b31) dataset contains 8,500 high-quality elementary school math problems, aimed at testing the basic mathematical reasoning abilities of models.

(4) BBH. BBH, short for BIG-Bench Hard [(Suzgun et al., 2022a)](#), is a subset of the BIG-Bench [(Srivastava et al., 2022b)](#) dataset comprising 23 challenging tasks. These tasks were selected because they consistently proved too difficult for current large language models to handle effectively. Requiring complex, multi-step reasoning, the BBH dataset is primarily utilized to assess the general reasoning capabilities of models, testing their ability to navigate and solve intricate problems.

(5) HumanEval (Coding). HumanEval [(Chen et al., 2021a)](#) consists of 164 programming problems, including language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions. The primary purpose of this dataset is to assess the ability of models to generate correct programs based on provided docstrings.

(6) IFEval. IFEval [(Zhou et al., 2023b)](#) consists of 500 prompts, each containing specific instructions like "write an article with more than 800 words" or "enclose your response in double quotation marks." This dataset is used to test the ability of large language models to accurately follow given instructions.

## HELM Evaluation

HELM [(Liang et al., 2022)](#b84) is a holistic evaluation of Language Models (LMs) to improve the transparency of language models, providing a more comprehensive understanding of the capabilities, risks, and limitations of language models. Specifically, differing from other evaluation methods, HELM holds that a holistic evaluation of language models should focus on the following three factors:

(1) Broad coverage. During the development, language models can be adapted to various NLP tasks (e.g., sequence labeling and question answering), thus, the evaluation of language models needs to be carried out in a wide range of scenarios. To involve all potential scenarios, HELM proposed a top-down taxonomy, which begins by compiling all existing tasks in a major NLP conference (ACL2022) into a task space and dividing each task into the form of scenarios (e.g., languages) and metrics (e.g., accuracy). Then when facing a specific task, the taxonomy would select one or more scenarios and metrics in the task space to cover it. By analyzing the structure of each task, HELM clarifies the evaluation content (task scenarios and metrics) and improves the scenario coverage of language models from 17.9% to 96.0%.

(2) Multi-metric measurement. In order to enable human to weigh language models from different perspectives, HELM proposes multimetric measurement. HELM has covered 16 different scenarios and 7 metrics. To ensure the results of intensive multi-metric measurement, HELM measured 98 of 112 possible core scenarios (87.5%).

(3) Standardization. The increase in the scale and training complexity of language models has seriously hindered human's understanding of the structure of each language model. To establish a unified understanding of existing language models, HELM benchmarks 30 well-known language models, covering such institutions as Google (UL2 [(Tay et al., 2022)](#b133)), OpenAI (GPT-3 [(Brown et al., 2020b)](#)), and EleutherAI (GPT-NeoX [(Black et al., 2022)](#b14)). Interestingly, HELM pointed out that LMs such as T5 [(Raffel et al., 2019)](#b114) and Anthropic-LMv4-s3 [(Bai et al., 2022a)](#) had not been directly compared in the initial work, while LLMs such as GPT-3 and YaLM were still different from their corresponding reports after multiple evaluations.

## LLM As a Judge

LLM as a judge refers to a set of methods that utilize powerful LLMs, particularly [GPT-4 (OpenAI, 2023)](#), to evaluate the outputs of other LLMs. There are three primary reasons for this approach: (1) Efficiency -Manually reviewing numerous LLM outputs can be labor-intensive, whereas GPT-4 can evaluate large-scale responses quickly, saving both time and effort; (2) Reliable Benchmark -As one of the most advanced models available, GPT-4 provides a dependable benchmark, allowing researchers to compare the performance of different LLMs against a high standard; and (3) Enhanced Capability -With improved comprehension and reasoning over previous models, GPT-4 is better suited to analyze subtle aspects of language generation and handle complex outputs from other LLMs. In the following, we detail 4 commonly accepted judge benchmarks:

(1) AlpacaEval. AlpacaEval [(Li et al., 2023h](#)) is an automated evaluation metric leveraging LLMs, consisting of 805 instructions selected to reflect typical user interactions from the Alpaca web demo[foot_50](#foot_50) . Specifically, for each instruction, both a baseline model b (currently GPT-4 turbo (OpenAI, 2023)) and the model under evaluation m generate responses. A GPT-4 turbo-based evaluator then conducts a head-to-head comparison of these responses, determining the probability of favoring the evaluated model. The win rate is calculated as the expected probability that the evaluator prefers the evaluated model's response across the 805 instructions, serving as a key metric for assessing the performance of the evaluated LM chatbot.

(2) Length-Controlled AlpacaEval. Length-Controlled AlpacaEval [(Dubois et al., 2024)](#b42) is a variation of the AlpacaEval [(Li et al., 2023h)](#) evaluation metric, designed to minimize length bias, as the original AlpacaEval tends to favor models that produce longer responses. To achieve this goal, [Dubois et al. (2024)](#b42) first fit a generalized linear model to predict the annotator's (GPT-4's) preference based on three factors: (1) the instruction, (2) the model used, and (3) the length difference between the baseline and the model's output. Then, by conditioning the length difference to 0, [Dubois et al. (2024)](#b42) can obtain the lengthcontrolled preference. This idea, which predicts the outcome while conditioning on the length difference (mediator), is a common technique in statistical inference, and by introducing it, Length-Controlled AlpacaEval increases the Spearman correlation with LMSYS' Chatbot Arena from 0.94 to 0.98.

(3) MT-Bench. Currently, close-ended evaluations only measure LLMs' core capability on a confined set of tasks, such as MMLU [(Hendrycks et al., 2020a)](#) for multi-choice decisions, without adequately assessing its alignment with human preference in open-ended tasks, such as the ability to adhere to instructions in multi-turn dialogues accurately. To alleviate this issue, [Zheng et al. (2023)](#b180) introduced MT-Bench, which comprises 80 high-quality multi-turn questions designed to assess LLMs' capability in multi-turn conversations and instruction-following, with evaluations conducted using GPT-4. MT-Bench is meticulously crafted to cover eight common tasks: writing, roleplay, extraction, reasoning, math, coding, knowledge I (STEM), and knowledge II (humanities/social sciences). For alignment, GPT-4 achieves over 80% agreement, comparable to the level of agreement among humans, making it a more reliable choice for a public benchmark.

(4) WildBench. Although the above evaluations are effective, they have notable limitations in task composition and skill coverage. For example, MT-Bench [(Hendrycks et al., 2020a)](#) includes only 80 test instructions, while AlpacaEval [(Li et al., 2023h)](#) features many straightforward tasks, such as "What is the capital of Australia?" To address this issue, [Lin et al. (2024)](#) introduced WildBench, comprising 1,024 test instructions carefully curated from extensive human-chatbot conversation logs. WildBench draws directly from real-world user interactions, featuring numerous challenging tasks, such as coding and math problem-solving. These tasks frequently demand critical thinking, making WildBench significantly more difficult than other benchmarks. WildBench utilizes two metrics: WB-Reward for pairwise comparisons and WB-Score for individual assessments. Both metrics show strong alignment with human evaluations, with Pearson correlations of 0.98 for WB-Reward and 0.95 for WB-Score when compared to the humanvoted ratings.

8.4 Low-resource Instruction Tuning [Gupta et al. (2023)](#) attempts to estimate the minimal downstream training data required by SFT models to match the SOTA supervised models over various tasks. [Gupta et al. (2023)](#) conducted experiments on 119 tasks from Super Natural Instructions (SuperNI) in both single-task learning (STL) and multi-task learning (MTL) settings. The results indicate that in the STL setting, SFT models with only 25% of downstream training data outperform the SOTA models on those tasks, while in the MTL setting, just 6% of downstream training data can lead SFT models to achieve the SOTA performance. These findings suggest that instruction tuning can effectively assist a model in quickly learning a task even with limited data.

However, due to resource limitations, [Gupta et al. (2023)](#) did not conduct experiments on LLMs, like T5-11B. So, to gain a more comprehensive understanding of the SFT models, further investigation using larger language models and datasets is necessary.

## Smaller Instruction Dataset

SFT requires a substantial amount of specialized instruction data for training. [Zhou et al. (2023a)](#) hypothesized that the pre-trained LLM only has to learn the style or format to interact with users and proposed LIMA that achieves strong performance by fine-tuning an LLM on only 1,000 carefully selected training examples.

Specifically, LIMA first manually curates 1,000 demonstrations with high-quality prompts and responses. Then the 1,000 demonstrations are used to fine-tune the pre-trained 65B-parameter LLaMa [(Touvron et al., 2023b)](#). By comparison, across more than 300 challenging tasks, LIMA outperfrms GPT-davinci003 [(Brown et al., 2020b)](#), which was fine-tuned on 5,200 examples by human feedback tuning. Moreover, with only half amount of demonstrations, LIMA achieves equivalent results to [GPT-4 (OpenAI, 2023)](#), Claude [(Bai et al., 2022b)](#), and Bard[foot_51](#foot_51) . Above all, LIMA demonstrated that LLMs' powerful knowledge and capabilities can be exposed to users with only a few carefully curated instructions to fine-tune.

## Evaluating Instruction Tuning Datasets

The performance of SFT model highly depends on the SFT datasets. However, there lacks of evaluations for these SFT datasets from open-ended and subjective aspects.

To address this issue, [Wang et al. (2023d)](#) performs dataset evaluation by fine-tuning the LLaMa model [(Touvron et al., 2023b)](#) on various of open SFT datasets and measure different finetuned models through both automatic and human evaluations. An additional model is trained on the combination of SFT datasets. For the results, [Wang et al. (2023d)](#) showed that there is not a single best SFT dataset across all tasks, while by manually combining datasets it can achieve the best overall performance. Besides, [Wang et al. (2023d)](#) pointed out that though SFT can bring large benefits on LLMs at all sizes, smaller models and models with a high base quality benefit most from SFT. For human evaluations, [Wang et al. (2023d)](#) a larger model is more likely to gain a higher acceptability score.

## Superficial Alignment

Despite the impressive improvements in the performance of instruction tuning, there lacks clarity about the specific knowledge that models acquire through instruction tuning, raising questions about: Does instruction tuning just learn Pattern Copying? or How exactly does the alignment tuning transform a base LLM?

To answer these questions, [Kung and Peng (2023)](#b68) delves into the analysis of how models make use of instructions during SFT by comparing the tuning when provided with altered instructions versus the original instructions.

Specifically, [Kung and Peng (2023)](#b68) creates simplified task definitions that remove all semantic components, leaving only the output information. In addition, [Kung and Peng (2023)](#b68) also incorporates delusive examples that contain incorrect input-output mapping. Surprisingly, the experiments show that models trained on these simplified task definitions or delusive examples can achieve comparable performance to the ones trained on the original instructions and examples. Moreover, the paper also introduces a baseline for the classification task with zero-shot, which achieves similar performance to SFT in lowresource settings.

Similar to the findings of [Kung and Peng (2023)](#b68), several subsequent studies [(Zhou et al., 2023a;](#)[Lin et al., 2023a)](#) reached the same conclusion: the observed performance improvements in current SFT models are often due to superficial alignment. This means the models excel at recognizing superficial alignment, such as mastering output formats and making educated guesses, rather than truly understanding and learning the underlying tasks.

## Proprietary LLMs Imitation

LLMs imitation is an approach that collects outputs from a stronger model, such as a proprietary system like ChatGPT, and uses these outputs to fine-tune an open-source LLM. Through this way, an opensource LLM may get competitive capabilities with any proprietary model. [Gudibande et al. (2023)](#b52) conducted several experiments to critically analyze the efficacy of model imitation. Specifically, [Gudibande et al. (2023)](#b52) first collected datasets from outputs of ChatGPT over broad tasks. Then these datasets were used to fine-tune a range of models covering sizes from 1.5B to 13B, base models GPT-2 and LLaMA, and data amounts from 0.3M tokens to 150M tokens.

For evaluations, Gudibande et al. ( [2023](#)) demonstrated that on tasks with supported datasets, imitation models are far better than before, and their outputs appear similar to ChatGPT's. While on tasks without imitation datasets, imitation models do not have improvement or even decline in accuracy.

Thus, [Gudibande et al. (2023)](#b52) pointed out that it's the phenomenon that imitation models are adept at mimicking ChatGPT's style (e.g., being fluent, confident and well-structured) that makes researchers have the illusion about general abilities of imitation models. So, [Gudibande et al. (2023)](#b52) suggested that instead of imitating proprietary models, researchers had better focus on improving the quality of base models and instruction examples.

## Conclusion

This work surveys recent advances in the fast growing field of instruction tuning, which can also be referred to as supervised fine-tuning (SFT). We make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, SFT's applications to different modalities, domains and application. We also review analysis on SFT models to discover both their advantages and potential pitfalls. We hope this work will act as a stimulus to motivate further endeavors to address the deficiencies of current SFT models.

![Figure 1: General pipeline of instruction tuning.]()

![Input: Sentence: During breakfast one morning, he seemed lost in thought and ignored his food. -Expected Output: How long was he lost in thoughts? Instance (b) An example of INSTANCES in Natural Instruction dataset.]()

![Figure 2: The figure is adapted from Mishra et al. (2021).]()

![Input: "Context: … 'That's fantastic, I'm glad we came to something we both agree with.' Utterance: 'Me too. I hope you have a wonderful camping trip.'" • Output: "Yes" • Explanation: "The participant engages in small talk when wishing their opponent to have a wonderful trip." • Input: "Context: … 'Sounds good, I need food the most, what is your most needed item?!' Utterance: 'My item is food too'." • Output: "Yes" • Explanation: "The utterance only takes the negotiation forward and there is no side talk. Hence, the correct answer is 'No'." Definition "... Given an utterance and recent dialogue context containing past 3 utterances (wherever available), output 'Yes' if the utterance contains the small-talk strategy, otherwise output 'No'. Small-talk is a cooperative negotiation strategy. It is used for discussing topics apart from the negotiation, to build a rapport with the opponent." Task Instruction • Input: "Context: … 'I am excited to spend time with everyone from camp!' Utterance: 'That's awesome! I really love being out here with my son. Do you think you could spare some food?' " • Expected Output: An example of INSTRUCTIONS in Super-Natural Instruction dataset. Input: What kind of, no hold up, what describes the proportionality of acceleration to force and mass? Output: ["What describes the proportionality of acceleration to force and mass?"] Instance (b) An example of INSTANCES in Super-Natural Instruction dataset.]()

![Figure 3: The figure is adapted from Wang et al. (2022e).]()

![Figure 4: The figure is copied from Köpf et al. (2023).]()

![Figure 5: General pipeline of distillation for synthetic data generation. The figure is adapted from Taori et al. (2023a).]()

![Figure 6: General pipeline of self-improvement for synthetic data generation. The figure is adapted from Wang et al. (2022c).]()

![Figure 7: Image editing dataset generation and diffusion model training. The figure is copied from Brooks et al. (2022).]()

![Figure 8: Overall architecture of Video-LLaMA. The figure is copied from Zhang et al. (2023b).]()

![Figure 9: Overall architecture of InstructBLIP. The figure is copied from Dai et al. (2023).]()

![Figure 10: The overview framework of InstructUIE. The figure is copied from Wang et al. (2023c).]()

![Figure 11: The overview framework of COEDIT. The figure is copied from Raheja et al. (2023).]()

https://huggingface.co/datasets/RyokoAI/ShareGPT52K

https://huggingface.co/bigscience/bloomz

https://huggingface.co/google/flan-t5-xxl

https://github.com/tatsu-lab/stanford_alpaca

https://github.com/lm-sys/FastChat

https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM

https://github.com/nlpxucan/WizardLM

https://github.com/THUDM/ChatGLM2-6B

https://huggingface.co/facebook/opt-iml-30b

https://github.com/databrickslabs/dolly

https://huggingface.co/tiiuae/falcon-40b-instruct

https://huggingface.co/JosephusCheung/Guanaco

https://huggingface.co/openaccess-ai-collective/minotaur-15b

https://huggingface.co/NousResearch/Nous-Hermes-13b

https://github.com/allenai/open-instruct

https://github.com/RUC-GSAI/YuLan-Chat

https://github.com/OpenLMLab/MOSS

https://github.com/jondurbin/airoboros

https://github.com/thunlp/UltraChat

https://github.com/EleutherAI/lm-evaluation-harness

https://openai.com/blog/chatgpt

https://sharegpt.com/

https://www.anthropic.com/index/introducing-claude

https://bard.google.com/

https://www.anthropic.com/index/introducing-claude

https://huggingface.co/datasets/databricks/databricksdolly-15k

https://huggingface.co/spaces/HuggingFaceH4 /open_llm_leaderboard

https://github.com/teknium1/GPTeacher

https://github.com/teknium1/GPTeacher

https://huggingface.co/datasets/OpenAssistant/oasst1

https://huggingface.co/datasets/vicgalle/alpaca-gpt4

https://sharegpt.com/

https://txsun1997.github.io/blogs/moss.html

https://github.com/timothybrooks/instruct-pix2pix

https://github.com/haotian-liu/LLaVA

https://github.com/DAMO-NLP-SG/Video-LLaMA

https://github.com/salesforce/LAVIS/tree/main/projects/instructblip

https://github.com/Luodian/Otter

https://github.com/open-mmlab/Multimodal-GPT

https://github.com/prakharguptaz/Instructdial

https://github.com/BeyonderXX/InstructUIE

https://github.com/amazon-science/instruction-tuning-for-absa

https://github.com/facebookresearch/EditEval

https://github.com/vipulraheja/coedit

https://github.com/vishakhpk/creative-instructions

https://huggingface.co/spaces/allen-eric/radiology-gpt

https://github.com/Kent0n-Li/ChatDoctor

https://github.com/SCIR-HI/Med-ChatGLM

https://github.com/liutiedong/goat

https://github.com/nlpxucan/WizardLM

https://crfm.stanford.edu/2023/03/13/ alpaca.html

Bard, designed by Google, is an interface to generative AI platform, and the link is: https://ai.google/static/documents/google-about-bard.pdf

