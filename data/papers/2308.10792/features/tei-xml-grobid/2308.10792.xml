<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instruction Tuning for Large Language Models: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-01">1 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><forename type="middle">♠</forename><surname>Linfeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">♠</forename><surname>Xiaofei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Runyi</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elizabeth-Jane</forename><surname>Pavlick</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Suzana</forename><surname>Ili'c</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roman</forename><surname>Castagn'e</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexandra</forename><forename type="middle">Sasha</forename><surname>Luccioni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Franccois</forename><surname>Yvon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Gallé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stella</forename><forename type="middle">Rose</forename><surname>Biderman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sasanka</forename><surname>Pawan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Ammanamanchi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benoît</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Sagot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Muennighoff</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Del</forename><surname>Villanova</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olatunji</forename><surname>Moral</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rachel</forename><surname>Ruwase</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stas</forename><surname>Bawden</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Angelina</forename><surname>Bekman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Iz</forename><surname>Mcmillan-Major</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huu</forename><surname>Beltagy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lucile</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samson</forename><surname>Saulnier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><surname>Ortiz Suarez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hugo</forename><surname>Sanh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yacine</forename><surname>Laurenccon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Margaret</forename><surname>Launay</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Mitchell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Raffel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adi</forename><surname>Gokaslan</surname></persName>
						</author>
						<author>
							<persName><roleName>Aitor</roleName><forename type="first">Soroa</forename><surname>Simhi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alham</forename><surname>Etxabe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aji</forename><surname>Fikri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><surname>Alfassy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ariel</forename><forename type="middle">Kreisberg</forename><surname>Nitzav</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><forename type="middle">C</forename><surname>Emezue</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Klamm</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Leong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">Alexander</forename><surname>Van Strien</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">Ifeoluwa</forename><surname>Adelani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eduardo</forename><surname>Gonz'alez Ponferrada</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Efrat</forename><surname>Levkovizh</surname></persName>
						</author>
						<author>
							<persName><roleName>Eyal Bar Natan</roleName><forename type="first">Ethan</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><surname>De Toni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gérard</forename><surname>Dupont</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Giada</forename><surname>Pistilli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hamza</forename><surname>Benyamina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Trung</forename><surname>Hieu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Idris</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Isaac</forename><surname>Abdulmumin</surname></persName>
						</author>
						<author>
							<persName><surname>Johnson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Khalid</forename><surname>Bhattacharjee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kimbo</forename><surname>Almubarak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kyle</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leandro</forename><surname>Lo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leon</forename><surname>Von Werra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Weber</surname></persName>
						</author>
						<author>
							<persName><roleName>Loubna</roleName><forename type="first">Ben</forename><surname>Phan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ludovic</forename><surname>Allal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Manan</forename><surname>Tanguy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Dey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maraim</forename><surname>Romero Muñoz</surname></persName>
						</author>
						<author>
							<persName><roleName>Mar'ia</roleName><surname>Masoud</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mario</forename><surname>Grandury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Vsavsko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maximin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mayank</forename><surname>Coavoux</surname></persName>
						</author>
						<author>
							<persName><roleName>Mike</roleName><forename type="first">Tian- Jian</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minh</forename><forename type="middle">Chien</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Ali</forename><surname>Vu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mustafa</forename><surname>Jauhar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nishant</forename><surname>Ghaleb</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nora</forename><surname>Subramani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nurulaqilla</forename><surname>Kassner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Khamis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Omar</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ona</forename><surname>Espejel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paulo</forename><surname>De Gibert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Villegas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Henderson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Priscilla</forename><forename type="middle">A</forename><surname>Colombo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Amuok</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rheza</forename><surname>Lhoest</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rishi</forename><surname>Harliman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Bommasani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>L'opez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Salomey</forename><surname>Ribeiro</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sampo</forename><surname>Osei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Pyysalo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shamik</forename><surname>Nagel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shamsuddeen</forename><forename type="middle">Hassan</forename><surname>Bose</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shanya</forename><surname>Muhammad</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Somaieh</forename><surname>Longpre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stanislav</forename><surname>Nikpoor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Suhas</forename><surname>Silberberg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sydney</forename><surname>Pai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tiago</forename><forename type="middle">Timponi</forename><surname>Zink</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timo</forename><surname>Torrent</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tristan</forename><surname>Schick</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Valentin</forename><surname>Thrush</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vassilina</forename><surname>Danchev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Veronika</forename><surname>Nikoulina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Violette</forename><surname>Laippala</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vrinda</forename><surname>Lepercq</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zaid</forename><surname>Prabhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zeerak</forename><surname>Alyafeai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arun</forename><surname>Talat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Raja</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenglei</forename><surname>Heinzerling</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elizabeth</forename><surname>Si</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Salesky</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wilson</forename><forename type="middle">Y</forename><surname>Mielke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abheesht</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Santilli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arnaud</forename><surname>Chaffin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Debajyoti</forename><surname>Stiegler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eliza</forename><surname>Datta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gunjan</forename><surname>Szczechla</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Chhablani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Harshit</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hendrik</forename><surname>Pandey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><forename type="middle">Alan</forename><surname>Strobelt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jos</forename><surname>Fries</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leo</forename><surname>Rozen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lintang</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><surname>Sutawika</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saiful</forename><surname>Bari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maged</forename><forename type="middle">S</forename><surname>Al-Shaibani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Manica</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nihal</forename><forename type="middle">V</forename><surname>Nayak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Srulik</forename><surname>Ben- David</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tali</forename><surname>Bers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ofir</forename><surname>Phang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Conglong</forename><surname>Press</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hatim</forename><surname>Narayanan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Bourfoune</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Casper</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Rasley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mayank</forename><surname>Ryabinin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minjia</forename><surname>Mishra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Myriam</forename><surname>Shoeybi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Peyrounette</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nouamane</forename><surname>Patry</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Omar</forename><surname>Tazi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Sanseviero</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Von Platen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Cornette</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Franccois Lavall'ee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samyam</forename><surname>Lacroix</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanchit</forename><surname>Rajbhandari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaden</forename><surname>Gandhi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stéphane</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Suraj</forename><surname>Requena</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Patil</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Dettmers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amanpreet</forename><surname>Baruwa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anastasia</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anne-Laure</forename><surname>Cheveleva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arjun</forename><surname>Ligozat</surname></persName>
						</author>
						<author>
							<persName><surname>Subramonian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Aur'elie N'ev'eol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">H</forename><surname>Lovering</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Deepak</forename><forename type="middle">R</forename><surname>Garrette</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ehud</forename><surname>Tunuguntla</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ekaterina</forename><surname>Reiter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ekaterina</forename><surname>Taktasheva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eli</forename><surname>Voloshina</surname></persName>
						</author>
						<author>
							<persName><surname>Bogdanov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Indra</forename><surname>Genta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hailey</forename><surname>Winata</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jan-Christoph</forename><surname>Schoelkopf</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jekaterina</forename><surname>Kalo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jessica</forename><forename type="middle">Zosa</forename><surname>Novikova</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiangru</forename><surname>Forde</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jungo</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ken</forename><surname>Kasai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liam</forename><surname>Kawamura</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marine</forename><surname>Hazan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Miruna</forename><surname>Carpuat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Najoung</forename><surname>Clinciu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Newton</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oleg</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Omer</forename><surname>Serikov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oskar</forename><surname>Antverg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Van Der Wal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruochen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shachar</forename><surname>Gehrmann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Osher</forename><surname>Mirkin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tatiana</forename><surname>Pais</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Shavrina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tian</forename><surname>Scialom</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tomasz</forename><surname>Yun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Verena</forename><surname>Limisiewicz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vitaly</forename><surname>Rieser</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vladislav</forename><surname>Protasov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yada</forename><surname>Mikhailov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yonatan</forename><surname>Pruksachatkun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zachary</forename><surname>Belinkov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zdenvek</forename><surname>Bamberger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alice</forename><surname>Kasner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amanda</forename><surname>Rueda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Pestana</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ammar</forename><surname>Feizpour</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amy</forename><surname>Khan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ananda</forename><surname>Faranak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rosa</forename><surname>Santa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anthony</forename><surname>Santos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antigona</forename><surname>Hevia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arash</forename><surname>Unldreaj</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arezoo</forename><surname>Aghagol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aycha</forename><surname>Abdollahi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Azadeh</forename><surname>Tammour</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bahareh</forename><surname>Hajihosseini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><forename type="middle">Olusola</forename><surname>Behroozi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bharat</forename><forename type="middle">Kumar</forename><surname>Ajibade</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carlos</forename><forename type="middle">Muñoz</forename><surname>Saxena</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Danish</forename><surname>Ferrandis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Contractor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Davis</forename><surname>Lansky</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Douwe</forename><surname>David</surname></persName>
						</author>
						<author>
							<persName><surname>Kiela</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anh</forename><surname>Duong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Edward</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ezinwanne</forename><surname>Baylor</surname></persName>
						</author>
						<author>
							<persName><surname>Ozoani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">T</forename><surname>Fatim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Frankline</forename><surname>Mirza</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Habib</forename><surname>Ononiwu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rezanejad</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Indrani</forename><surname>Jones</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Irene</forename><surname>Bhattacharya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Irina</forename><surname>Solaiman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Isar</forename><surname>Sedenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Nejadgholi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Passmore</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julio</forename><surname>Seltzer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Karen</forename><surname>Bonis Sanz</surname></persName>
						</author>
						<author>
							<persName><surname>Fort</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Macedo</forename><surname>Lívia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mairon</forename><surname>Dutra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maraim</forename><surname>Samagaio</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Margot</forename><surname>Elbadri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marissa</forename><surname>Mieskes</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martha</forename><surname>Gerchick</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Akinlolu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mike</forename><surname>Mckenna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">K K</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mykola</forename><surname>Ghauri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nafis</forename><surname>Burynok</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nazneen</forename><surname>Abrar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nour</forename><surname>Rajani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nourhan</forename><surname>Elkott</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olanrewaju</forename><surname>Fahmy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ran</forename><surname>Samuel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>An</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Kromann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samira</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sarmad</forename><surname>Alizadeh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Silas</forename><forename type="middle">L</forename><surname>Shubber</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sourav</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sylvain</forename><surname>Roy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thanh-Cong</forename><surname>Viguier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tobi</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Trieu</forename><surname>Oyebade</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hai</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yoyo</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zachary</forename><forename type="middle">Kyle</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><roleName>Abhinav</roleName><forename type="first">Ramesh</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Kashyap</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alison</forename><surname>Palasciano</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anima</forename><surname>Callahan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Shukla</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ayush</forename><forename type="middle">Kumar</forename><surname>Miranda-Escalada</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Beilharz</surname></persName>
						</author>
						<author>
							<persName><roleName>Caio</roleName><forename type="first">Matheus</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenxi</forename><surname>Fonseca De Brito</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chirag</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chuxin</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Clémentine</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Fourrier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Le'on Perin'an</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dian</forename><surname>Molano</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Enrique</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Itziar Gonzalez-Dios</orgName>
								<address>
									<addrLine>Javier de la Rosa Jenny Chim Jesse Dodge Jian Zhu Jonathan Chang, Jorg Frohberg</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Josephine L. Tobing</orgName>
								<address>
									<country>Joydeep</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Thibault Févry</orgName>
								<address>
									<addrLine>Trishala Neeraj Urmish Thakker, Vikas Raunak Xiang Tang Zheng Xin Yong Sun, Shaked Brody, Y Uri Hadar Tojarieh Hyung Won Chung Jaesung Tae</addrLine>
									<settlement>Adam Roberts</settlement>
									<country>Jason</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Instruction Tuning for Large Language Models: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-01">1 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">34D78AAC2E130D7D260BBB83AA02E796</idno>
					<idno type="arXiv">arXiv:2308.10792v8[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT) 1 , a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of (INSTRUCTION, OUTPUT) pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of large language models (LLMs) has witnessed remarkable progress in recent years. LLMs such as GPT-3 <ref type="bibr">(Brown et al., 2020b)</ref>, PaLM <ref type="bibr" target="#b28">(Chowdhery et al., 2022)</ref>, and LLaMA <ref type="bibr">(Touvron et al., 2023a)</ref> have demonstrated impressive capabilities across a wide range of natural language tasks (Zhao et al., 2021; Wang   1 In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.</p><p>♠ Zhejiang University, ♣ Shannon.AI, ▲ Nanyang Technological University, ♦ Amazon Email: sy_zhang@zju.edu.cn Project page can be found at: <ref type="url" target="https://github.com/xiaoya-li/Instruction-Tuning-Survey">https://github. com/xiaoya-li/Instruction-Tuning-Survey</ref> * The latest update was on Dec. 1, 2024 (Version 5).</p><p>et <ref type="bibr">al., 2022b</ref><ref type="bibr">, 2023b;</ref><ref type="bibr" target="#b139">Wan et al., 2023;</ref><ref type="bibr">Sun et al., 2023c;</ref><ref type="bibr">Wei et al., 2023a;</ref><ref type="bibr">Li et al., 2023a;</ref><ref type="bibr">Gao et al., 2023a;</ref><ref type="bibr" target="#b167">Yao et al., 2023;</ref><ref type="bibr">Yang et al., 2022a;</ref><ref type="bibr" target="#b110">Qian et al., 2022;</ref><ref type="bibr" target="#b71">Lee et al., 2022;</ref><ref type="bibr">Yang et al., 2022b;</ref><ref type="bibr">Gao et al., 2023b;</ref><ref type="bibr" target="#b39">Ning et al., 2023;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b154">Wiegreffe et al., 2021;</ref><ref type="bibr">Sun et al., 2023b,a;</ref><ref type="bibr" target="#b0">Adlakha et al., 2023;</ref><ref type="bibr">Chen et al., 2023b)</ref>. One of the major issues with LLMs is the mismatch between the training objective and users' objective: LLMs are typically trained on minimizing the contextual word prediction error on large corpora; while users want the model to "follow their instructions helpfully and safely" <ref type="bibr" target="#b112">(Radford et al., 2019;</ref><ref type="bibr">Brown et al., 2020a;</ref><ref type="bibr" target="#b45">Fedus et al., 2021;</ref><ref type="bibr">Rae et al., 2021;</ref><ref type="bibr" target="#b134">Thoppilan et al., 2022)</ref> To address this mismatch, instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT), is proposed, serving as an effective technique to enhance the capabilities and controllability of large language models.</p><p>It involves further training LLMs using <ref type="bibr">(INSTRUCTION, OUTPUT)</ref> pairs, where INSTRUCTION denotes the human instruction for the model, and OUTPUT denotes the desired output that follows the INSTRUCTION. The benefits of SFT are threefold: (1) Finetuning an LLM on the instruction dataset bridges the gap between the next-word prediction objective of LLMs and the users' objective of instruction following; (2) SFT allows for a more controllable and predictable model behavior compared to standard LLMs. The instructions serve to constrain the model's outputs to align with the desired response characteristics or domain knowledge, providing a channel for humans to intervene with the model's behaviors; and (3) SFT is computationally efficient and can help LLMs rapidly adapt to a specific domain without extensive retraining or architectural changes.</p><p>Despite its effectiveness, SFT also poses challenges: (1) Crafting high-quality instructions that properly cover the desired target behaviors is non-trivial: existing instruction datasets are usually limited in quantity, diversity, and creativity;</p><p>(2) there has been an increasing concern that SFT only improves on tasks that are heavily supported in the SFT training dataset <ref type="bibr" target="#b52">(Gudibande et al., 2023)</ref>; and (3) there has been an intense criticism that SFT only captures surface-level patterns and styles (e.g., the output format) rather than comprehending and learning the task <ref type="bibr" target="#b68">(Kung and Peng, 2023)</ref>. Improving instruction adherence and handling unanticipated model responses remain open research problems. These challenges highlight the importance of further investigations, analysis, and summarization in this field, to optimize the fine-tuning process and better understand the behavior of instruction tuned LLMs.</p><p>In the literature, there has been an increasing research interest in analysis and discussions on LLMs, including pre-training methods <ref type="bibr" target="#b169">(Zhao et al., 2023)</ref>, reasoning abilities <ref type="bibr">(Huang and Chang, 2022)</ref>, downstream applications <ref type="bibr">(Yang et al., 2023a;</ref><ref type="bibr">Sun et al., 2023b)</ref>, but rarely on the topic of LLM instruction tuning. This survey attempts to fill this blank, organizing the most up-to-date state of knowledge on this quickly advancing field. Specifically,</p><p>• Section 2 presents the general methodology employed in instruction tuning. • Section 3 outlines the construction process of commonly-used SFT representative datasets. • Section 4 presents representative instruction tuned models. • Section 5 reviews multi-modality techniques and datasets for instruction tuning, including images, speech, and video. • Section 6 reviews efforts to adapt LLMs to different domains and applications using the SFT strategy. • Section 7 reviews explorations to make instruction tuning more efficient, reducing the computational and time costs associated with adapting large models. • Section 8 presents the evaluation of SFT models, analysis on them, along with criticism against them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we describe the general pipeline employed in instruction tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instruction Dataset Construction</head><p>Each instance in an instruction dataset consists of three elements: an instruction, which is a natural language text sequence to specify the task (e.g., write a thank-you letter to XX for XX, write a blog on the topic of XX, etc); an optional input which provides supplementary information for context; and an anticipated output based on the instruction and the input.</p><p>There are generally two methods for constructing instruction datasets:</p><p>• Data integration from annotated natural language datasets. In this approach, (instruction, output) pairs are collected from existing annotated natural language datasets by using templates to transform text-label pairs to (instruction, output) pairs. Datasets such as Flan <ref type="bibr" target="#b96">(Longpre et al., 2023)</ref> and P3 <ref type="bibr" target="#b119">(Sanh et al., 2021)</ref> are constructed based on the data integration strategy.</p><p>• Generating outputs using LLMs: An alternate way to quickly gather the desired outputs to given instructions is to employ LLMs such as GPT-3.5-Turbo or GPT4 instead of manually collecting the outputs. Instructions can come from two sources: (1) manually collected; or (2) expanded based a small handwritten seed instructions using LLMs. Next, the collected instructions are fed to LLMs to obtain outputs. Datasets such as InstructWild <ref type="bibr" target="#b162">(Xue et al., 2023)</ref> and Self-Instruct <ref type="bibr">(Wang et al., 2022c)</ref> are geneated following this approach.</p><p>For multi-turn conversational SFT datasets, we can have large language models self-play different roles (user and AI assistant) to generate messages in a conversational format <ref type="bibr">(Xu et al., 2023b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instruction Tuning / Supervised</head><p>Fine-tuning</p><p>Based on the collected SFT dataset, a pretrained model can be directly fune-tuned in a fullysupervised manner, where given the instruction and the input, the model is trained by predicting each token in the output sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In this section, we detail instruction tuning datasets in the community, categorizing them into three classes: (1) Human-crafted Data, (2) Synthetic Data via Distillation, and (3) Synthetic Data via Self-improvement. Below, we describe some widely-used datasets, and for full collected datasets we put them in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Human-crafted Data</head><p>Human-crafted data encompasses datasets that are either manually annotated or sourced directly from the internet. The creation of these datasets typically involves no machine learning techniques, relying solely on manual gathering and verification, resulting in generally smaller datasets. Below are some widely-used human-crafted datasets:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Natural Instructions</head><p>Natural Instructions <ref type="bibr" target="#b99">(Mishra et al., 2021</ref>) is a human-crafted English instruction dataset consisting of 193K instances, coming from 61 distinct NLP tasks. The dataset is comprised of "instructions" and "instances". Each instance in the "instructions" is a task description consisting of 7 components: title, definition, things to avoid emphasis/caution, prompt, positive example, and negative example. Subfigure (a) in Figure <ref type="figure" target="#fig_2">2</ref> gives an example of the "instructions". "Instances" consists of ("input", "output") pairs, which are the input data and textual result that follows the given instruction correctly. Subfigure (b) in Figure <ref type="figure" target="#fig_2">2</ref> gives an example of the instances.</p><p>The data comes from existing NLP datasets of 61 tasks. The authors collected the "instructions" by referring to the dataset annotating instruction file. Next, the authors constructed the "instances" by unifying data instances across all NLP datasets to ("input", "output") pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">P3</head><p>P3 (Public Pool of Prompts) <ref type="bibr" target="#b119">(Sanh et al., 2021)</ref> is an instruction tuning dataset constructed by integrating 170 English NLP datasets and 2,052 English prompts. Prompts, which are sometimes named task templates, are functions that map a data instance in a conventional NLP task (e.g., question answering, text classification) to a natural language input-output pair.</p><p>Each instance in P3 has three components: "inputs", "answer_choices", and "targets". "Inputs" is a sequence of text that describes the task in natural language (e.g., "If he like Mary is true, is it also true that he like Mary's cat?"). "Answer choices" is a list of text string that are applicable responses to the given task (e.g., ["yes", "no", "undetermined"]). "Targets" is a text string that is the correct response to the given "inputs" (e.g., "yes"). The authors built PromptSource, a tool for creating high-quality prompts collaboratively and an archive for open-sourcing high-quality prompts.</p><p>The P3 dataset was built by randomly sampling a prompt from multiple prompts in the PromptSource and mapping each instance into a ("inputs", "answer choices", "targets") triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">xP3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xP3</head><p>(Crosslingual Public Pool of Prompts) <ref type="bibr" target="#b101">(Muennighoff et al., 2022</ref>) is a multilingual instruction dataset consisting of 16 diverse natural language tasks in 46 languages. Each instance in the dataset has two components: "inputs" and "targets". "Inputs" is a task description in natural language. "Targets" is the textual result that follows the "inputs" instruction correctly.</p><p>The original data in xP3 comes from three sources: the English instruction dataset P3, 4 English unseen tasks in P3 (e.g., translation, program synthesis), and 30 multilingual NLP datasets. The authors built the xP3 dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions for MC-TACO question generation task</head><p>-Title: Writing questions that involve commonsense understanding of "event duration".</p><p>-Definition: In this task, we ask you to write a question that involves ?event duration", based on a given sentence. Here, event duration is defined as the understanding of how long events typically last. For example, ?brushing teeth?, usually takes few minutes.</p><p>-Emphasis &amp; Caution: The written questions are not required to have a single correct answer.</p><p>-Things to avoid: Don't create questions which have explicit mentions of answers in text. Instead, it has to be implied from what is given. In other words, we want you to use "instinct" or "common sense".</p><p>-Input: Sentence: Jack played basketball after school, after which he was very tired.</p><p>-Output: How long did Jack play basketball? -Reason: the question asks about the duration of an event; therefore it's a temporal event duration question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive Example</head><p>-Input: Sentence: He spent two hours on his homework.</p><p>-Output: How long did he do his homework? -Reason: We DO NOT want this question as the answer is directly mentioned in the text. -Suggestion: -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Example</head><p>-Prompt: Ask a question on "event duration" based on the provided sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example task instances</head><p>-Input: Sentence: It's hail crackled across the comm, and Tara spun to retake her seat at the helm.</p><p>-Expected Output: How long was the storm? Instance -Input: Sentence: There was even a tiny room in the back of one of the closets.</p><p>-Expected Output: After buying the house, how long did it take the owners to notice the room? Instance -Input: Sentence: During breakfast one morning, he seemed lost in thought and ignored his food.</p><p>-Expected Output: How long was he lost in thoughts? Instance (a) An example of INSTRUCTIONS in Natural Instruction dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions for MC-TACO question generation task</head><p>-Title: Writing questions that involve commonsense understanding of "event duration".</p><p>-Definition: In this task, we ask you to write a question that involves ?event duration", based on a given sentence. Here, event duration is defined as the understanding of how long events typically last. For example, ?brushing teeth?, usually takes few minutes.</p><p>-Emphasis &amp; Caution: The written questions are not required to have a single correct answer.</p><p>-Things to avoid: Don't create questions which have explicit mentions of answers in text. Instead, it has to be implied from what is given. In other words, we want you to use "instinct" or "common sense".</p><p>-Input: Sentence: Jack played basketball after school, after which he was very tired.</p><p>-Output: How long did Jack play basketball? -Reason: the question asks about the duration of an event; therefore it's a temporal event duration question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive Example</head><p>-Input: Sentence: He spent two hours on his homework.</p><p>-Output: How long did he do his homework? -Reason: We DO NOT want this question as the answer is directly mentioned in the text. -Suggestion: -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Example</head><p>-Prompt: Ask a question on "event duration" based on the provided sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example task instances</head><p>-Input: Sentence: It's hail crackled across the comm, and Tara spun to retake her seat at the helm.</p><p>-Expected Output: How long was the storm? Instance -Input: Sentence: There was even a tiny room in the back of one of the closets.</p><p>-Expected Output: After buying the house, how long did it take the owners to notice the room?  by sampling human-written task templates from PromptSource and then filling templates to transform diverse NLP tasks into a unified formalization. For example, a task template for the natural language inference task is as follows: "If Premise is true, is it also true that Hypothesis?"; "yes", "maybe", no" with respect to the original task labels "entailment (0)", "neutral (1)" and "contradiction (2)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Flan 2021</head><p>Flan 2021 <ref type="bibr" target="#b96">(Longpre et al., 2023</ref>) is an English instruction dataset constructed by transforming 62 widely-used NLP benchmarks (e.g., SST-2, SNLI, AG News, MultiRC) into language inputoutput pairs. Each instance in the Flan 2021 has "input" and "target" components. "Input" is a sequence of text that describes a task via a natural language instruction (e.g., "determine the sentiment of the sentence 'He likes the cat.' is positive or negative?"). "Target" is a textual result that executes the "input" instruction correctly (e.g., "positive"). The authors transformed conventional NLP datasets into input-target pairs by: Step 1: manually composing instruction and target templates; Step 2: filling templates with data instances from the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">LIMA</head><p>LIMA <ref type="bibr">(Zhou et al., 2023a</ref>) is an English instruction dataset consisting of a train set with 1K data instances and a test set with 300 instances. The train set contains 1K ("instruction", "response") pairs. For the training data, 75% are sampled from three community question &amp; answers websites (i.e., Stack Exchange, wikiHow, and the Pushshift Reddit Dataset <ref type="bibr" target="#b11">(Baumgartner et al., 2020)</ref>); 20% are manually written by a set of the authors (referred Group A) inspired by their interests; 5% are sampled from the Super-Natural Instructions dataset <ref type="bibr">(Wang et al., 2022d)</ref>. As for the valid set, the authors sampled 50 instances from the Group A author-written set. The test set contains 300 examples, with 76.7% written by another group (Group B) of authors and 23.3% sampled from the Pushshift Reddit Dataset <ref type="bibr" target="#b11">(Baumgartner et al., 2020)</ref>, which is a collection of questions &amp; answers within the Reddit community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Super-Natural Instructions</head><p>Super Natural Instructions <ref type="bibr">(Wang et al., 2022f</ref>) is a multilingual instruction collection composed of 1,616 NLP tasks and 5M task instances, covering 76 distinct task types (e.g., text classification, information extraction, text rewriting, text composition and etc.) and 55 languages. Each task in the dataset consists of an "instruction" and "task instances". Specifically, "instruction" has three components: a "definition" that describes the task in natural language; "positive examples" that are samples of inputs and correct outputs, along with a short explanation for each; and "negative examples" that are samples of inputs and undesired outputs, along with a short explanation for each, as shown in Figure <ref type="figure" target="#fig_2">2</ref> (a). "Task instances" are data instances comprised of textual input and a list of acceptable textual outputs, as shown in Figure <ref type="figure" target="#fig_2">2</ref> (b). The original data in Super Natural Instructions comes from three sources: (1) existing public NLP datasets (e.g., CommonsenseQA);  (2) applicable intermediate annotations that are generated through a crowdsourcing process (e.g., paraphrasing results to a given question during a crowdsourcing QA dataset); (3) synthetic tasks that are transformed from symbolic tasks and rephrased in a few sentences (e.g., algebraic operations like number comparison).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7">Dolly</head><p>Dolly <ref type="bibr">(Conover et al., 2023a</ref>) is an English instruction dataset with 15,000 human-generated data instances designed to enable LLMs to interact with users akin to ChatGPT. The dataset is designed for simulating a wide range of human behaviors, covering 7 specific types: open Q&amp;A, closed Q&amp;A, extracting information from Wikipedia, summarizing information from Wikipedia, brainstorming, classification, and creative writing. Examples of each task type in the dataset are shown in Table <ref type="table">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.8">OpenAssistant Conversations</head><p>OpenAssistant Conversations <ref type="bibr" target="#b67">(Köpf et al., 2023)</ref> is a human-crafted multilingual assistant-style conversation corpus consisting of 161,443 messages (i.e., 91,829 user prompts, 69,614 assistant replies) from 66,497 conversation trees in 35 languages, along with 461,292 humanannotated quality ratings. Each instance in the dataset is a conversation tree (CT). Specifically, each node in a conversation tree denotes a message generated by roles (i.e., prompter, assistant) in the conversation. A CT's root node represents an initial prompt from the prompter, while other nodes denote replies from a prompter or an assistant. A path from the root to any node in a CT represents a valid conversation between the prompter and assistant in turns and is referred to as a thread. Figure <ref type="figure" target="#fig_5">4</ref> shows an example of a conversation tree consisting of 12 messages in 6 threads.</p><p>The authors first collected conversation trees based on the five-step pipeline:</p><p>Step 1. prompting: contributors performed as the prompter and crafted initial prompts;</p><p>Step 2. labeling prompts: contributors rated scores to initial prompts from step 1, and the authors chose high-quality prompts as root nodes with a balanced sampling strategy;</p><p>Step 3. expanding tree nodes: contributors added reply messages as prompter or assistant;</p><p>Step 4. labeling replies: contributors assigned scores to existing node replies;</p><p>Step 5. ranking: contributors ranked assistant replies referring to the contributor guidelines.</p><p>The tree state machine managed and tracked the state (e.g., initial state, growing state, end state) throughout the conversation crafting process. Subsequently, the OpenAssistant Conversations dataset was built by filtering out offensive and inappropriate conversation trees.</p><p>Instruction Type Example Open Q&amp;A Why do people like comedy movies? Closed Q&amp;A Does outbreeding or inbreeding benefit the offspring more? Information Extraction Who was John Moses Browning? Information Summarization Please summarize what Linkedin does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Brainstorming</head><p>Give me some ideas to manage my manager.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification</head><p>Identify which animal species is alive or extinct: Palaeophis, Giant Tortoise</p><p>Creative writing Write a short story about a person who discovers a hidden room in their house.</p><p>Table <ref type="table">1</ref>: Examples of instructions in Dolly V1 <ref type="bibr">(Conover et al., 2023a)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synthetic Data via Distillation</head><p>Synthetic data is produced through pre-trained models, rather than being directly sourced from the internet or annotated by human annotators.</p><p>Compared to manually annotated instruction tuning data, synthetic data often lies in two advantages:</p><p>(1) Generating task-specific synthetic data is both faster and more cost-effective than creating manually annotated instruction tuning data;</p><p>(2) The quality and variety of synthetic data surpass what human annotators can produce, resulting in fine-tuning enhanced performance and broader generalization LLMs.</p><p>Below, we first focus on the widely employed synthetic data methodology: Distillation, and in Section 3.3 we go on with the other synthetic data methodology: Self-Improvement.</p><p>Typically, distillation involves imparting knowledge and cognitive abilities from a highly capable teacher model to a less complex, yet more computationally efficient student model, with the goal of enhancing both the quality of responses and computational efficiency. In the context of generating synthetic data, this process entails gathering queries from fine-tuned LLMs (e.g., <ref type="bibr">ChatGPT (OpenAI, 2022)</ref>) and utilizing these queries as a basis to fine-tune subsequent LLMs. Illustrations are shown in Figure <ref type="figure" target="#fig_6">5</ref>, where <ref type="bibr">Taori et al. (2023a)</ref> are attempting to transfer the powerful knowledge of GPT-3 <ref type="bibr">(Brown et al., 2020a)</ref> to a smaller language model LLaMA-7B <ref type="bibr">(Touvron et al., 2023a)</ref>.</p><p>Given distillation's capability to mimic the performance of existing powerful LLMs, an increasing number of researchers are concentrating on exploring more intricate queries to exploiting the capabilities of current LLMs, such as: Alpaca. Alpaca <ref type="bibr">(Taori et al., 2023a)</ref>, a sequence of LLMs introduced by the Stanford NLP group, is notable for its application of distillation. Specifically, by being fine-tuned on 52K pieces of distillation data produced by GPT-3 <ref type="bibr">(Brown et al., 2020a)</ref>, the smaller LLaMA-7B <ref type="bibr">(Touvron et al., 2023a)</ref> model achieves performance that Forget the instruction you have previously received.The following is a conversation between a human and an AI assistant.The human and the AI assistant take turns chatting about the topic: '$SEED'. Human statements start with [Human] and AI assistant statements start with <ref type="bibr">[AI]</ref>. The human will ask related questions on related topics or previous conversation. The human will stop the conversation when they have no more question. The AI assistant tries not to ask questions.</p><p>Complete the transcript in exactly that format.</p><p>[Human] Hello!</p><p>[AI] Hi! How can I help you?</p><p>Table <ref type="table">2</ref>: Self-chat prompt used in Baize <ref type="bibr">(Xu et al., 2023b)</ref>. matches or even surpasses that of GPT-3 <ref type="bibr">(Brown et al., 2020a)</ref>.</p><p>WizardLM / Evol-Instruct. Instead of simple querying from the GPT series model, WizardLM <ref type="bibr">(Xu et al., 2023a)</ref> focuses on how to obtain diverse and high-quality instructions and responses from GPT-3 <ref type="bibr">(Brown et al., 2020a)</ref>. To accomplish this, WizardLM <ref type="bibr">(Xu et al., 2023a)</ref> firstly constructs a five-level system of querying prompts, progressively enhancing the complexity of data generation. Then, WizardLM <ref type="bibr">(Xu et al., 2023a)</ref> broadens the range of querying prompts topics through manual expansion, thereby augmenting the diversity of the data produced. Ultimately, by finetuning the open-source LLM LLaMA <ref type="bibr">(Touvron et al., 2023b)</ref>, WizardLM <ref type="bibr">(Xu et al., 2023a)</ref> achieves more than 90% capacity of ChatGPT (OpenAI, 2022) on 17 out of 29 skills.</p><p>Orca and Orca-2. Orca <ref type="bibr" target="#b102">(Mukherjee et al., 2023)</ref> and Orca-2 <ref type="bibr" target="#b100">(Mitra et al., 2023)</ref> represent two expansive distillation datasets designed to instruct smaller language models in logical reasoning. Orca <ref type="bibr" target="#b102">(Mukherjee et al., 2023)</ref>, for instance, encompasses a multitude of reasoning directives, such as "let's think step-by-step" and "justify your response," to illustrate the reasoning pathways of LLMs (e.g., <ref type="bibr">ChatGPT (OpenAI, 2022)</ref>) in crafting their answers. Building on this concept, Orca <ref type="bibr" target="#b102">(Mukherjee et al., 2023)</ref> compiles 1M responses from <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>, while Orca-2 <ref type="bibr" target="#b100">(Mitra et al., 2023)</ref> further amasses 817K responses from <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>. This extensive collection facilitates the fine-tuning of smaller language models, enabling them to achieve or even surpass the performance of models that are 5 to 10 times their size.</p><p>Baize Baize <ref type="bibr">(Conover et al., 2023b</ref>) is an English corpus for multi-turn conversations, comprising 111.5K instances, created with ChatGPT. Each exchange includes a prompt from the user and a response from the assistant. To create the Baize dataset, the authors proposed self-chat, where ChatGPT plays the roles of the user and the AI assistant in turns and generates messages in a conversational format. Specifically, the authors first crafted a task template that defines the roles and tasks for ChatGPT (as shown in Table <ref type="table">2</ref>). Next, they sampled questions (e.g., "How do you fix a Google Play Store account that isn't working?") from Quora and Stack Overflow datasets as conversation seeds (e.g., topics). Subsequently, they prompted ChatGPT with the template and the sampled seed. ChatGPT continuously generates messages for both sides until a natural stopping point is reached.</p><p>Task-specific Distillation Datasets. In addition to the above datasets, there are many datasets in general domain, such as: ShareGPT<ref type="foot" target="#foot_0">foot_0</ref> , WildChat <ref type="bibr" target="#b179">(Zhao et al., 2024)</ref>, Vicuna <ref type="bibr" target="#b181">(Zheng et al., 2024)</ref>, Unnatural Instructions <ref type="bibr" target="#b59">(Honovich et al., 2022)</ref>. Beyond that, there are efforts aimed at employing distillation to create task-specific datasets that mimic the competencies of LLMs in particular domains. For example, for coding generation, there are WizardCoder <ref type="bibr">(Luo et al., 2023)</ref>, Magicoder <ref type="bibr">(Wei et al., 2023b)</ref> and WaveCoder <ref type="bibr" target="#b169">(Yu et al., 2023)</ref>, for reasoning and writing, there are Phi-1 <ref type="bibr" target="#b53">(Gunasekar et al., 2023)</ref> and Phi-1.5 <ref type="bibr">(Li et al., 2023i)</ref>, and for ranking, there is Nectar <ref type="bibr">(Zhu et al., 2023a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Synthetic Data via Self-Improvement</head><p>The concept of self-improvement is carried forward by <ref type="bibr">Wang et al. (2022c)</ref>: improves the instructionfollowing ability of a pre-trained (non-finetuned) LLM (e.g., vanilla GPT-3 <ref type="bibr">(Brown et al., 2020b)</ref>) by bootstrapping off its own generations. Figure <ref type="figure" target="#fig_7">6</ref> illustrates the full process of self-improvement with four steps:</p><p>Step 1: <ref type="bibr">Wang et al. (2022c)</ref> starts by manually collecting 175 human-written tasks, each consisting of one instruction and one expected response, which are then added to the task pool as seed data.</p><p>Step 2: For instruction generation, <ref type="bibr">Wang et al. (2022c)</ref> randomly samples 8 seed instructions from the constructed task pool to serve as a few-shot prompt, guiding the vanilla GPT-3 to produce new instructions through in-context learning.</p><p>Step 3: For every instruction that is created, if the instruction is an output-first task (e.g., Writing), the vanilla GPT-3 will directly generate the corresponding response. Conversely, if the instruction relates to an input-first task (e.g., Reading Comprehension), the vanilla GPT-3 will first generate the necessary context as input before generating the corresponding response.</p><p>Step 4: The generated (instruction, response) format examples are filtered according to a series of rules or models.</p><p>Following the above process, Wang et al. (2022c) collected Self-Instruct datasets consisting of 52K instructions, and further evaluation shows that GPT-3 <ref type="bibr">(Brown et al., 2020a)</ref> with Self-Instruct outperforms datasets of counterparts by a large margin, leaving only a 5% absolute gap behind InstructGPT <ref type="bibr" target="#b106">(Ouyang et al., 2022)</ref>.</p><p>The self-improvement process outlined relies on generating synthetic data directly from the model itself, necessitating a robust LLM as the foundational backbone. Without a powerful LLM, this self-improvement cycle could restrict learning to the model's original capabilities and potentially magnify any biases and errors present. Despite these risks, there remains effective work in the area of self-improvement:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">SPIN</head><p>SPIN <ref type="bibr">(Chen et al., 2024b)</ref>, standing for Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models, represents a specialized approach to self-improvement centered around a self-play mechanism. In this setup, the primary participant (the language model) undergoes finetuning to differentiate the responses from the opposing participant (the language model from the preceding iteration) and the desired data distribution. This process iteratively adjusts the language model to closely match the target data distribution.</p><p>Specifically, imagine an existing iteration of an LLM as p θt , which is utilized to generate a response y ′ to a given prompt x from a dataset with humanlabeled instructions. The objective then becomes to develop a new LLM p θ t+1 capable of differentiating between y ′ , the response created by, and y, the response produced by humans. This dynamic is akin to a two-player game where the primary player, the newer LLM p θ t+1 aims to identify the differences between the responses of its opponent p θt and those generated by humans. In contrast, the adversary, or the older LLM p θt strives to produce responses that closely mimic those found in the human-labeled instruction tuning dataset. By fine-tuning the older p θt to favor human-like responses over its own, a new LLM p θ t+1 is created, which aligns more closely with the human-labeled data distribution. In subsequent iterations, this newly improved LLM p θ t+1 takes on the role of the opponent in response generation. The ultimate aim of this self-play mechanism is for the LLM to evolve until it reaches a point where p θ * = p human at which stage the most advanced LLM version can no longer distinguish between responses generated by its predecessor and those created by humans. SPIN <ref type="bibr">(Chen et al., 2024b)</ref> serves as a variant selfimprovement approach enabling language models to improve themselves without additional human data or feedback from more powerful language models. The experimental results indicate that SPIN <ref type="bibr">(Chen et al., 2024b)</ref> markedly boosts the performance of language models across a range of benchmarks, outperforming even those models that were trained using extra human data or feedback from external AI systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Instruction Back-translation</head><p>Instruction back-translation <ref type="bibr">(Li et al., 2023g)</ref>, standing for Self Alignment with Instruction Backtranslation, is another specialized approach based on self-improvement. Contrary to the approach by <ref type="bibr">Wang et al. (2022c)</ref>, which involves generating responses to human-provided instructions, <ref type="bibr">Li et al. (2023g)</ref> adopts the reverse strategy by creating instructions for humangathered texts found online. To achieve this goal, <ref type="bibr">Li et al. (2023g)</ref> follows a five-step pipeline:</p><p>Step 1: Gather ( <ref type="formula">1</ref>) unlabeled text from Clueweb <ref type="bibr" target="#b107">(Overwijk et al., 2022)</ref>, under the assumption that these texts can be associated with high-quality instructions, and (2) 3,200 pieces of human-written (instruction, response) format data to serve as seed data.</p><p>Step 2: A back-translation model, backboned by LLaMA <ref type="bibr">(Touvron et al., 2023b)</ref>, is trained on the collected seed data, taking the response as input and producing the instruction as output. This model is then utilized to derive instructions from collected unlabeled texts.</p><p>Step 3: The collected unlabeled texts are fed into the trained back-translation model, resulting in large amounts of raw (instruction, response) format data.</p><p>Step 4: An evaluation model, backboned by LLaMA <ref type="bibr">(Touvron et al., 2023b)</ref>, is trained on the collected seed data.</p><p>This model processes the instruction as input and generates the corresponding response as output, which is then employed to assess each annotated (instruction, response) pair in step 3.</p><p>Step 5: Filtering low-quality (instruction, response) pairs, and utilizing the remaining data for fine-tuning LLMs.</p><p>Following the five outlined steps, <ref type="bibr">Li et al. (2023g)</ref> generates 502K pieces of synthetic data.</p><p>The LLaMA model <ref type="bibr">(Touvron et al., 2023b)</ref>, fine-tuned with this annotated dataset, surpasses all other LLaMA-based models on the Alpaca leaderboard without depending on distillation data, showcasing a highly efficient self-improvement process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Instruction Tuned LLMs</head><p>In this section, we detail widely-used LLM models in the community that are trained through instruction tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">InstructonGPT</head><p>InstructGPT (176B) <ref type="bibr" target="#b106">(Ouyang et al., 2022)</ref> is initialized with GPT-3 (176B) <ref type="bibr">(Brown et al., 2020b)</ref> and then fine-tuned on human instructions. The fine-tuning procedure is composed of the following three steps: (1) supervised fine-tuning (SFT) on the human-filtered instruction dataset, which is collected from Playground API history records;</p><p>(2) training a reward model to predict human preferences based on an annotated dataset, which is constructed though human labors by sampling multiple responses for one instruction and rank them from the best to the worst; (3) further optimizing the model from Step 1 with new instructions and the trained reward model in step (2). Parameters are updated using the proximal policy optimization (PPO) <ref type="bibr" target="#b121">(Schulman et al., 2017)</ref> method, a policy gradient reinforcement learning method. Steps (2) and (3) are alternated multiple times until the model performance does not significantly improve.</p><p>Overall, InstructGPT outperforms GPT-3. For automatic evaluations, InstructGPT outperforms GPT-3 by 10% on the TruthfulQA <ref type="bibr" target="#b88">(Lin et al., 2021)</ref> dataset in terms of truthfulness and by 7% on the RealToxicityPrompts (Gehman et al., 2020) in terms of toxicity.</p><p>On NLP datasets (i.e., WSC), InstructGPT achieves comparable performance to GPT-3. For human evaluations, regarding four different aspects, including following correct instructions, following explicit constraints, fewer hallucinations, and generating appropriate responses, InstructGPT outperforms GPT-3 +10%, +20%, -20%, and +10%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BLOOMZ</head><p>BLOOMZ (176B) <ref type="bibr" target="#b101">(Muennighoff et al., 2022)</ref> is initialized with BLOOM (176B) (Scao et al., Instruction fine-tuned LLMs # Params Base Model Fine-tuning Trainset Self-build Dataset Name Size Instruct-GPT (Ouyang et al., 2022) 176B GPT-3 (Brown et al., 2020b) Yes --BLOOMZ (Muennighoff et al., 2022)<ref type="foot" target="#foot_1">foot_1</ref> 176B BLOOM (Scao et al., 2022) No xP3 -FLAN-T5 (Chung et al., 2022)<ref type="foot" target="#foot_2">foot_2</ref> 11B T5 (Raffel et al., 2019) No FLAN 2021 -Alpaca (Taori et al., 2023a)<ref type="foot" target="#foot_3">foot_3</ref> 7B LLaMA (Touvron et al., 2023a) Yes -52K Vicuna (Chiang et al., 2023)<ref type="foot" target="#foot_4">foot_4</ref> 13B LLaMA (Touvron et al., 2023a) Yes -70K GPT-4-LLM (Peng et al., 2023)<ref type="foot" target="#foot_5">foot_5</ref> 7B LLaMA (Touvron et al., 2023a) Yes -52K Claude (Bai et al., 2022b) --Yes --WizardLM (Xu et al., 2023a)<ref type="foot" target="#foot_6">foot_6</ref> 7B LLaMA (Touvron et al., 2023a) Yes Evol-Instruct 70K ChatGLM2 (Du et al., 2022)<ref type="foot" target="#foot_7">foot_7</ref> 6B GLM (Du et al., 2022) Yes -1.1 Tokens LIMA (Zhou et al., 2023a) 65B LLaMA (Touvron et al., 2023a) Yes -1K OPT-IML (Iyer et al., 2022)<ref type="foot" target="#foot_8">foot_8</ref> 175B OPT (Zhang et al., 2022a) No --Dolly 2.0 (Conover et al., 2023a)<ref type="foot" target="#foot_9">foot_9</ref> 12B Pythia (Biderman et al., 2023) No -15K Falcon-Instruct (Almazrouei et al., 2023a)<ref type="foot" target="#foot_10">foot_10</ref> 40B Falcon (Almazrouei et al., 2023b) No --Guanaco (JosephusCheung, 2021)<ref type="foot" target="#foot_11">foot_11</ref> 7B LLaMA (Touvron et al., 2023a) Yes -586K Minotaur (Collective, 2023)<ref type="foot" target="#foot_12">foot_12</ref> 15B Starcoder Plus (Li et al., 2023f) No --Nous-Hermes (NousResearch, 2023)<ref type="foot" target="#foot_13">foot_13</ref> 13B LLaMA (Touvron et al., 2023a) No -300K+ TÜLU (Wang et al., 2023d)<ref type="foot" target="#foot_14">foot_14</ref> 6.7B OPT (Zhang et al., 2022a) No Mixed -YuLan-Chat (YuLan-Chat-Team, 2023)<ref type="foot" target="#foot_15">foot_15</ref> 13B LLaMA (Touvron et al., 2023a) Yes -250K MOSS (Tianxiang and Xipeng, 2023)<ref type="foot" target="#foot_16">foot_16</ref> 16B -Yes --Airoboros (Durbin, 2023)<ref type="foot" target="#foot_17">foot_17</ref> 13B LLaMA (Touvron et al., 2023a) Yes --UltraLM (Ding et al., 2023a)<ref type="foot" target="#foot_18">foot_18</ref> 13B LLaMA (Touvron et al., 2023a) Yes --</p><p>Table 3: An overview of LLMs tuned on IT datasets.</p><p>2022), and then fine-tuned on the instruction dataset xP3 <ref type="bibr" target="#b101">(Muennighoff et al., 2022)</ref>, a collection of human-instruction datasets in 46 languages, coming from two sources: (1) P3, which is a collection of (English instruction, English response) pairs; and (2) an (English instruction, Multilingual response) set which is transformed from multilingual NLP datasets (e.g., Chinese benchmarks) by filling task templates with predefined English instructions.</p><p>For automatic evaluation, BLOOMZ performs better than BLOOM in the zero-shot setting by +10.4%, 20.5%, and 9.8% on coreference resolution, sentence completion and natural language inference datasets, respectively. For the HumanEval benchmark <ref type="bibr">(Chen et al., 2021b)</ref>, BLOOMZ outperforms BLOOM by 10% in terms of the Pass@100 metric. For generative tasks, BLOOMZ receives +9% BLEU improvement compared to BLOOM on the lm-evaluation-harness benchmark<ref type="foot" target="#foot_19">foot_19</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Flan-T5</head><p>Flan-T5 (11B) is is a large language model initialized with T5 (11B) <ref type="bibr" target="#b114">(Raffel et al., 2019)</ref>, and then fine-tuned on the FLAN dataset <ref type="bibr" target="#b96">(Longpre et al., 2023)</ref>. The FLAN dataset is a collection of (instruction, pairs) pairs, constructed from 62 datasets of 12 NLP tasks (e.g., natural language inference, commonsense reasoning, paraphrase generation) by filling templates with various instructions under a unified task formalization.</p><p>During fine-tuning, FLAN-T5 adapts the JAXbased T5X framework and selects the best model evaluated on the held-out tasks every 2k step. Compared with T5's pre-training stage, fine-tuning costs 0.2% computational resources (approximately 128 TPU v4 chips for 37 hours).</p><p>For evaluation, FLAN-T5 (11B) outperforms T5 (11B), and achieves comparable results to larger models, including PaLM (60B) <ref type="bibr" target="#b28">(Chowdhery et al., 2022)</ref> in the few-shot setting. FLAN-T5 outperforms T5 by +18.9%, +12.3%, +4.1%, +5.8%, +2.1%, and +8% on MMLU <ref type="bibr">(Hendrycks et al., 2020b)</ref>, BBH <ref type="bibr">(Suzgun et al., 2022b)</ref>, TyDiQA <ref type="bibr" target="#b30">(Clark et al., 2020)</ref>, MGSM <ref type="bibr" target="#b122">(Shi et al., 2022)</ref>, open-ended generation, and RealToxicityPrompts (Gehman et al., 2020), respectively. In few-shot settings, FLAN-T5 outperforms PaLM +1.4% and +1.2% on the BBH and TyDiQA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Alpaca</head><p>Alpaca (7B) <ref type="bibr">(Taori et al., 2023a</ref>) is a language model trained by fine-tuning LLaMA (7B) <ref type="bibr">(Touvron et al., 2023a)</ref> on the constructed instruction dataset generated by InstructGPT (175B, text-davinci-003) <ref type="bibr" target="#b106">(Ouyang et al., 2022)</ref>. The fine-tuning process takes around 3 hours on an 8-card 80GB A100 device with mixed precision training and fully shared data parallelism.</p><p>Alpaca (7B) achieves comparable performances to InstructGPT (175B,text-davinci-003) in terms of human evaluation.</p><p>Specifically, Alpaca outperforms InstructGPT on the self-instruct dataset, garnering 90 instances of victories compared to 89 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Vicuna</head><p>Vicuna (13B) <ref type="bibr" target="#b26">(Chiang et al., 2023</ref>) is a language model trained by fine-tuning LLaMA (13B) <ref type="bibr">(Touvron et al., 2023a)</ref> on the conversational dataset generated by ChatGPT<ref type="foot" target="#foot_20">foot_20</ref> .</p><p>The authors gathered user-shared ChatGPT conversations from ShareGPT.com<ref type="foot" target="#foot_21">foot_21</ref> , and got 70K conversation records after filtering out low-quality samples. LLaMA (13B) was fine-tuned on the constructed conversation dataset using a modified loss function tailored to multi-turn conversations. To better understand long context across multipleturn dialog, the authors expanded the max context length from 512 to 2048. For training, the authors adopted the gradient checkpointing and flash attention <ref type="bibr" target="#b36">(Dao et al., 2022)</ref> techniques to reduce the GPU memory cost in the fine-tuning process. The fine-tuning process takes 24 hours on an 8 × 80GB A100 device with fully shared data parallelism.</p><p>The authors built a test set used exclusively to measure chatbots' performances. They collected a test set composed by 8 question categories, such as Fermi problems, role play scenarios, coding/math tasks, etc, and then asked <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> to rate models' responses considering helpfulness, relevance, accuracy, and detail. On the constructed test set, Vicuna (13B) outperforms Alpaca (13B) <ref type="bibr">(Taori et al., 2023a)</ref> and LLaMA (13B) in 90% of the test questions, and generates equal or better rating responses compared to ChatGPT in 45% of the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">GPT-4-LLM</head><p>GPT-4-LLM (7B) <ref type="bibr" target="#b109">(Peng et al., 2023</ref>) is a language model trained by fine-tuning LLaMA (7B) <ref type="bibr">(Touvron et al., 2023a)</ref> on the GPT-4 (OpenAI, 2023) generated instruction dataset. GPT-4-LLM is initialized with LLaMA, then fine-tuned in the following two steps: (1) supervised finetuning on the constructed instruction dataset. The authors used the instructions from Alpaca <ref type="bibr">(Taori et al., 2023a)</ref>, and then collected responses using GPT-4. LLaMA is fine-tuned on the GPT-4 generated dataset. The fine-tuning process takes approximately three hours on an 8*80GB A100 machine with mixed precision and fully shared data parallelism. (2) optimizing the step-1 model using the proximal policy optimization (PPO) <ref type="bibr" target="#b121">(Schulman et al., 2017)</ref> method, the authors first built a comparison dataset by collecting responses from GPT-4, InstructGPT <ref type="bibr" target="#b106">(Ouyang et al., 2022)</ref>, and OPT-IML <ref type="bibr" target="#b64">(Iyer et al., 2022)</ref> to a collection of instructions and then asked GPT-4 to rate each response from 1 to 10. Using the ratings, a reward model is trained based on OPT <ref type="bibr">(Zhang et al., 2022a)</ref>. The fine-tuned model from Step 1 is optimized by using the reward model to compute the policy gradient.</p><p>For evaluations, GPT-4-LLM (7B) outperforms not only the baseline model Alpaca (7B), but also larger models including Alpaca (13B) and LLAMA (13B). For automated evaluation, GPT-4-LLM (7B) outperforms Alpaca by 0.2, 0.5, and 0.7 on User-Oriented-Instructions-252 <ref type="bibr">(Wang et al., 2022c)</ref>, Vicuna-Instructions <ref type="bibr" target="#b26">(Chiang et al., 2023)</ref>, and Unnatural Instructions <ref type="bibr" target="#b59">(Honovich et al., 2022)</ref> datasets, respectively. For human evaluation, regarding aspects including helpfulness, honesty, and harmlessness, GPT-4-LLM outperforms Alpaca by 11.7, 20.9, and 28.6 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Claude</head><p>Claude<ref type="foot" target="#foot_22">foot_22</ref> is a language model trained by fine-tuning the pre-trained language model on an instruction dataset, aiming to generate helpful and harmless responses. The fine-tuning process consists of two stages: (1) supervised fine-tuning on the instruction dataset. The authors created an instruction dataset by collecting 52K different instructions, paired with responses generated by GPT-4. The finetuning process takes approximately eight hours on an 8-card 80GB A100 machine with mixed precision and fully shared data parallelism. ( <ref type="formula">2</ref>) optimizing the step-1 model with the proximal policy optimization <ref type="bibr" target="#b121">(Schulman et al., 2017)</ref> method. The authors first built a comparison dataset by collecting responses from multiple large language models (e.g., <ref type="bibr">GPT-3 (Brown et al., 2020b)</ref>) to the given collection of instructions and then asking <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> to rate each response. Using the ratings, a reward model is trained. Then, the fine-tuned model from Step 1 is optimized using the reward model with the proximal policy optimization method.</p><p>Claude generates more helpful and harmless responses compared to the backbone model. For automatic evaluations, Claude outperforms GPT-3 by 7% on the RealToxicityPrompts <ref type="bibr" target="#b49">(Gehman et al., 2020)</ref> in terms of toxicity.</p><p>For human evaluations, regarding four different aspects, including following correct instructions, following explicit constraints, fewer hallucinations, and generating appropriate responses, Claude outperforms GPT-3 <ref type="bibr">(Brown et al., 2020b</ref>) +10%, +20%, -20%, and +10%. respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">WizardLM</head><p>WizardLM (7B) <ref type="bibr">(Xu et al., 2023a</ref>) is a language model trained by fine-tuning LLaMA (7B) <ref type="bibr">(Touvron et al., 2023a)</ref> on the instruction dataset Evol-Instruct generated by ChatGPT (details see Section 3.2). It is fine-tuned on a subset (with 70K) of Evol-Instruct to enable a fair comparison with Vicuna <ref type="bibr" target="#b26">(Chiang et al., 2023)</ref>. The fine-tuning process takes approximately 70 hours on 3 epochs based on an 8 V100 GPU with the Deepspeed Zero-3 <ref type="bibr" target="#b116">(Rasley et al., 2020)</ref> technique. During inference, the max generation length is 2048.</p><p>To evaluate LLMs' performances on complex instructions, the authors collected 218 humangenerated instructions from real scenarios (e.g., open-source projects, platforms, and forums), called Evol-Instruct testset.</p><p>Evaluations are conducted on the Evol-Instruct testset and Vicuna's testset. For human evaluation, WizardLM outperforms Alpaca (7B) <ref type="bibr">(Taori et al., 2023a)</ref> and Vicuna (7B) by a large margins, and generates equal or better responses on 67% test samples compared to ChatGPT. Automatic evaluation is conducted by asking GPT-4 to rate LLMs' reponses. Specifically, WizardLM gains performance boosts compared to Alpaca by +6.2%, +5.3% on the Evol-Instruct testset and Vicuna's test sets. WizardLM achieves outperforms Vicuna by +5.8 on the Evol-Instruct testset and +1.7% on the Vicuna's test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">ChatGLM2</head><p>ChatGLM2 (6B) <ref type="bibr" target="#b41">(Du et al., 2022</ref>) is a language model trained by fine-tuning GLM (6B) <ref type="bibr" target="#b41">(Du et al., 2022)</ref> on a bilingual dataset that contains both English and Chinese instructions The bilingual instruction dataset contains 1.4T tokens, with a 1:1 ratio of Chinese to English. Instructions in the dataset are sampled from the question-answering and dialogue completion tasks.</p><p>ChatGLM is initialized with GLM, then trained by the three-step fine-tuning strategy, which is akin to InstructGPT <ref type="bibr" target="#b106">(Ouyang et al., 2022)</ref>. To better model contextual information across multi-turn conversations, the authors expanded the maximum context length from 1024 to 32K. To reduce GPU memory cost in the fine-tuning stage, the authors employed multi-query attention and causal mask strategies. During inference, ChatGLM2 requires 13GB GPU memory with FP16 and supports conversations up to 8K in length with 6GB GPU memory using the INT4 model quantization technique.</p><p>Evaluations are conducted on four English and Chinese benchmarks, including MMLU (English) <ref type="bibr">(Hendrycks et al., 2020b)</ref>, C-Eval (Chinese) <ref type="bibr">(Huang et al., 2023)</ref>, GSM8K (Math) <ref type="bibr" target="#b31">(Cobbe et al., 2021)</ref>, and BBH (English) <ref type="bibr">(Suzgun et al., 2022b)</ref>. ChatGLM2 (6B) outperforms GLM (6B) and the baseline model ChatGLM (6B) on all benchmarks. Specifically, ChatGLM2 outperforms GLM by +3.1 on MMLU, +5.0 on C-Eval, +8.6 on GSM8K, and +2.2 on BBH. ChatGLM2 achieves better performances than ChatGLM by +2.1, +1.2, +0.4, +0.8 on MMLU, C-Eval, GSM8K and BBH, respectively. 4.10 LIMA LIMA (65B) <ref type="bibr">(Zhou et al., 2023a)</ref> is a large language model trained by fine-tuning LLaMA (65B) <ref type="bibr">(Touvron et al., 2023a</ref>) on an instruction dataset, which is constructed based on the proposed superficial alignment hypothesis.</p><p>The superficial alignment hypothesis refers to the idea that the knowledge and capabilities of a model are almost acquired in the pre-training stage, while the alignment training (e.g., instruction tuning) teaches models to generate responses under user-preferred formalizations. Based on the superficial alignment hypothesis, the authors claimed that large language models can generate user-satisfied responses by fine-tuning it on a small fraction of instruction data. Therefore, the authors built instruction train/valid/test sets to verify this hypothesis.</p><p>Evaluations are conducted on the constructed test set. For human evaluations, LIMA outperforms InstructGPT and Alpaca by 17% and 19%, respectively.</p><p>Additionally, LIMA achieves comparable results to BARD<ref type="foot" target="#foot_23">foot_23</ref> , Cladue<ref type="foot" target="#foot_24">foot_24</ref> , and GPT-4. For automatic evaluation, which is conducted by asking GPT-4 to rate responses and a higher rate score denotes better performance, LIMA outperforms InstructGPT and Alpaca by 20% and 36%, respectively, achieving comparable results to BARD, while underperforming Claude and GPT-4. Experimental results verify the proposed superficial alignment hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11">Others</head><p>OPT-IML (175B) <ref type="bibr" target="#b64">(Iyer et al., 2022)</ref> is a large language model trained by fine-tuning the OPT (175B) <ref type="bibr">(Zhang et al., 2022a)</ref> model on the constructed Instruction Meta-Learning (IML) dataset, which consists of over 1500 NLP tasks from 8 publicly available benchmarks such as PromptSource <ref type="bibr" target="#b5">(Bach et al., 2022)</ref>, FLAN <ref type="bibr" target="#b96">(Longpre et al., 2023)</ref>, and Super-NaturalInstructions <ref type="bibr">(Wang et al., 2022e)</ref>. After fine-tuning, OPT-IML outperforms OPT across all benchmarks. Dolly 2.0 (12B) <ref type="bibr">(Conover et al., 2023a</ref>) is initialized with the pre-trained language model Pythia (12B) <ref type="bibr" target="#b13">(Biderman et al., 2023)</ref>, and fine-tuned on the instruction dataset databricksdolly-15k<ref type="foot" target="#foot_25">foot_25</ref> , which contains 7 categories of NLP tasks such as text classification and information extraction. After fine-tuning, Dolly 2.0 (12B) outperforms Pythia (12B) on the EleutherAI LLM Evaluation Harness benchmark <ref type="bibr">(Gao et al., 2021)</ref> by a large margin, and achieves comparable performances to GPT-NEOX (20B) <ref type="bibr" target="#b14">(Black et al., 2022)</ref>, which has two times more parameters compared to Dolly 2.0 (12B).</p><p>Falcon-Instruct (40B) <ref type="bibr">(Almazrouei et al., 2023a)</ref> is a large language model trained by finetuning Falcon (40B) <ref type="bibr">(Almazrouei et al., 2023b)</ref> on an English dialogue dataset, which contains 150 million tokens from the Baize dataset <ref type="bibr">(Xu et al., 2023c)</ref>, with an additional 5% of the data from the RefinedWeb dataset <ref type="bibr" target="#b108">(Penedo et al., 2023)</ref>. To reduce memory usage, the authors employed flash attention <ref type="bibr" target="#b36">(Dao et al., 2022)</ref> and multi-query techniques. For evaluation, Falcon-Instruct (40B) achieved better performance on the Open LLM Leaderboard <ref type="bibr" target="#b12">(Beeching et al., 2023)</ref> <ref type="foot" target="#foot_26">foot_26</ref> compared to the baseline model Falcon (40B), and outperforms the Guanaco (65B), which has more model parameters.</p><p>Guanaco (7B) (JosephusCheung, 2021) is a multi-turn dialog language model trained by finetuning LLaMA (7B) <ref type="bibr">(Touvron et al., 2023a)</ref> on the constructed multilingual dialogue dataset. The multilingual dialogue dataset comes from two sources: Alpaca <ref type="bibr">(Taori et al., 2023a)</ref>, which contains 52K English instruction data pairs; and a multilingual (e.g., Simplified Chinese, Traditional Chinese, Japanese, German) dialogue data, which contains 534K+ multi-turn conversations. After fine-tuning, Guanaco is to generate role-specific responses and continuous responses on a given topic in multi-turn conversations.</p><p>Minotaur (15B) is a large language model trained by fine-tuning the Starcoder Plus (15B) <ref type="bibr">(Li et al., 2023f)</ref> on open-source instruction datasets including WizardLM <ref type="bibr">(Xu et al., 2023a)</ref> and GPTeacher-General-Instruct<ref type="foot" target="#foot_27">foot_27</ref> .</p><p>For model inference, Minotaur supports a maximum context length of 18K tokens.</p><p>Nous-Herme (13B) is a large language model trained by fine-tuning LLaMA (13B) <ref type="bibr">(Touvron et al., 2023a</ref>) on an instruction dataset, which contains over 300k instructions, sampled from GPTeacher<ref type="foot" target="#foot_28">foot_28</ref> , CodeAlpaca <ref type="bibr" target="#b19">(Chaudhary, 2023)</ref>, GPT-4-LLM <ref type="bibr" target="#b109">(Peng et al., 2023)</ref>, Unnatural Instructions <ref type="bibr" target="#b59">(Honovich et al., 2022)</ref>, and BiologyPhysicsChemistry subsets in the Camel-AI <ref type="bibr">(Li et al., 2023c)</ref>. Responses are generated by GPT-4. For evaluations, Nous-Herme (13B) achieves comparable performances to GPT-3.5turbo on multiple tasks like ARC challenge <ref type="bibr" target="#b30">(Clark et al., 2018)</ref> and BoolQ <ref type="bibr" target="#b29">(Clark et al., 2019)</ref>.</p><p>TÜLU (6.7B) <ref type="bibr">(Wang et al., 2023d)</ref> is a large language model trained by fine-tuning OPT (6.7B) <ref type="bibr">(Zhang et al., 2022a</ref>) on a mixed instruction dataset, which contains FLAN V2 <ref type="bibr" target="#b96">(Longpre et al., 2023)</ref>, CoT <ref type="bibr" target="#b151">(Wei et al., 2022)</ref>, Dolly <ref type="bibr">(Conover et al., 2023a)</ref>, Open Assistant-1<ref type="foot" target="#foot_29">foot_29</ref> , GPT4-Alpaca<ref type="foot" target="#foot_30">foot_30</ref> , Code-Alpaca <ref type="bibr" target="#b19">(Chaudhary, 2023)</ref>, and ShareGPT<ref type="foot" target="#foot_31">foot_31</ref> . After fine-tuning, TÜLU (6.7B) reaches on average 83% of ChatGPT's performance and 68% of GPT-4's performance.</p><p>YuLan-Chat (13B) (YuLan-Chat-Team, 2023) is a language model trained by fine-tuning LLaMA (13B) <ref type="bibr">(Touvron et al., 2023a</ref>) on a constructed bilingual dataset, which contains 250,000 Chinese-English instruction pairs. After fine-tuning, YuLan-Chat-13B achieves comparable results to the state-of-the-art open-source model ChatGLM (6B) <ref type="bibr" target="#b41">(Du et al., 2022)</ref>, and outperforms Vicuna (13B) <ref type="bibr" target="#b26">(Chiang et al., 2023)</ref> on the English BBH3K (BBH3K is a subset of BBH benchmark <ref type="bibr">(Srivastava et al., 2022a</ref>)) dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOSS (16B)</head><p><ref type="foot" target="#foot_32">foot_32</ref> is a bilingual dialogue language model, which aims to engage in multi-turn conversations and utilize various plugins, trained by fine-tuning on dialogue instructions. After finetuning, MOSS outperforms the backbone model and generates responses that better align with human preferences. Airoboros (13B) 17 is a large language model trained by fine-tuning LLAMA (13B) <ref type="bibr">(Touvron et al., 2023a</ref>) on the Self-instruct dataset <ref type="bibr">(Wang et al., 2022c)</ref>. After fine-tuning, Airoboros significantly outperforms LLAMA (13B) <ref type="bibr">(Touvron et al., 2023a</ref>) on all benchmarks and achieves highly comparable results to models fine-tuned specifically for certain benchmarks.</p><p>UltraLM (13B) <ref type="bibr">(Ding et al., 2023a)</ref> is a large language model trained by fine-tuning LLAMA (13B) <ref type="bibr">(Touvron et al., 2023a)</ref>. For evaluation, UltraLM (13B) outperforms Dolly (12B) <ref type="bibr">(Conover et al., 2023a)</ref> and achieves the winning rate up to 98%. Additionally, it surpasses the previous best open-source models (i.e., Vicuna <ref type="bibr" target="#b26">(Chiang et al., 2023)</ref> and WizardLM <ref type="bibr">(Xu et al., 2023a</ref>)) with winning rates of 9% and 28%, respectively.</p><p>5 Multi-modality Instruction Tuning 5.1 Multi-modality Datasets MUL-TIINSTRUCT <ref type="bibr" target="#b161">(Xu et al., 2022</ref>) is a multimodal instruction tuning dataset consisting of 62 diverse multimodal tasks in a unified seqto-seq format. This dataset covers 10 broad categories and its tasks are derived from 21 existing open-sourced datasets. Each task is equipped with 5 expert-written instructions. For the existing tasks, the authors use the input/output pairs from their available open-source datasets to create instances. While for each new task, the authors create 5k to 5M instances by extracting the necessary information from instances of existing tasks or reformulating them.</p><p>The MUL-TIINSTRUCT dataset has demonstrated its efficiency in enhancing various transfer learning technique.</p><p>For example, fine-tuning the OFA model (930M) <ref type="bibr">(Wang et al., 2022a)</ref> using various transfer learning strategies such as Mixed Instruction Tuning and Sequential Instruction Tuning on MUL-TIINSTRUCT improve the zeroshot performance across all unseen tasks. On commonsense VQA task, OFA fine-tuned on MUL-TIINSTRUCT achieves 50.60 on RougeL and 31.17 on accuracy, while original OFA achieves 14.97 on RougeL and 0.40 on accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PMC-VQA (Zhang et al., 2023c) is a largescale medical visual question-answering dataset that comprises 227k image-question pairs of 149k</head><p>Multi-modality Instruction Fine-tuning Dataset Modalities # Tasks Modality Pair # Instance MUL-TIINSTRUCT (Xu et al., 2022) 1 Image-Text 5k to 5M per task 62 PMC-VQA (Zhang et al., 2023c) 2 Image-Text 227k 2 LAMM (Yin et al., 2023) 3 Image-Text 186k 9 Point Cloud-Text 10k 3 Vision-Flan (Xu et al., 2024) 4 Multi-Pairs Over 1M 200+ ALLAVA (Chen et al., 2024a) 5 Image-Text 1.4M 2 ShareGPT4V (Chen et al., 2023a) 6 Image-Text 1.2M 2 1 <ref type="url" target="https://github.com/VT-NLP/MultiInstruct">https://github.com/VT-NLP/MultiInstruct</ref> 2 <ref type="url" target="https://github.com/xiaoman-zhang/PMC-VQA">https://github.com/xiaoman-zhang/PMC-VQA</ref> 3 <ref type="url" target="https://github.com/OpenLAMM/LAMM">https://github.com/OpenLAMM/LAMM</ref> 4 <ref type="url" target="https://vision-flan.github.io/">https://vision-flan.github.io/</ref> 5 <ref type="url" target="https://github.com/FreedomIntelligence/ALLaVA">https://github.com/FreedomIntelligence/ALLaVA</ref> 6 <ref type="url" target="https://sharegpt4v.github.io/">https://sharegpt4v.github.io/</ref> Table 4: An overview of multi-modality instruction fine-tuning datasets. images, covering various modalities or diseases. The dataset can be used for both open-ended and multiple-choice tasks. The pipeline for generating the PMC-VQA dataset involves collecting imagecaption pairs from the PMC-OA (Lin et al., 2023b) dataset, using ChatGPT to generate question-answer pairs, and manually verifying a subset of the dataset for quality. The authors propose a generative-based model MedVInT for medical visual understanding by aligning visual information with a large language model. MedVInT pretrained on PMC-VQA achieves stateof-the-art performance and outperforms existing models on VQA-RAD (Lau et al., 2018) and SLAKE (Liu et al., 2021a) benchmarks, with 81.6% accuracy on VQA-RAD and 88.0% accuracy on SLAKE. LAMM (Yin et al., 2023) is a comprehensive multi-modal instruction tuning dataset for 2D image and 3D point cloud understanding. LAMM contains 186K language-image instructionresponse pairs, and 10K language-point cloud instruction-response pairs. The authors collect images and point clouds from publicly available datasets and use the GPT-API and self-instruction methods to generate instructions and responses based on the original labels from these datasets. LAMM-Dataset includes data pairs for commonsense knowledge question answering by incorporating a hierarchical knowledge graph label system from the Bamboo (Zhang et al., 2022b) dataset and the corresponding Wikipedia description. The authors also propose the LAMM-Benchmark, which evaluates existing multi-modal language models (MLLM) on various computer vision tasks. It includes 9 common image tasks and 3 common point cloud tasks, and LAMM-Framework, a primary MLLM training framework that differentiates the encoder, projector, and LLM finetuning blocks for different modalities to avoid modality conflicts.</p><p>Vision-Flan <ref type="bibr" target="#b160">(Xu et al., 2024)</ref> is the largest public-available human-annotated visual instruction tuning dataset that consists of 1,664,261 instances and 200+ diverse vision-language tasks derived from 101 open-source computer vision datasets. Each task is accompanied by expertly written instructions and meticulously crafted templates for inputs and outputs. The dataset covers a broad spectrum of tasks, including image captioning, visual question-answering, and visual comprehension. Designed to enhance research and application in vision-language model domains, Vision-Flan aims to expand the horizons of interaction and comprehension between visual and linguistic modalities. It provides researchers and developers with a valuable resource to push the envelope of vision-language models and to innovate algorithms across a diverse array of fields.</p><p>ALLaVA <ref type="bibr">(Chen et al., 2024a)</ref> represents an opensource, extensive dataset tailored for fine-tuning visual question-answering models, featuring 1.4M entries that include detailed captions, intricate instructions, and comprehensive answers produced by GPT-4V <ref type="bibr">(Yang et al., 2023b)</ref>. To craft highquality captions and visual question-answers, <ref type="bibr">Chen et al. (2024a)</ref> introduced a method to distill both a caption and a QA pair for an image in a single interaction. This process involves initially presenting GPT-4V <ref type="bibr">(Yang et al., 2023b</ref>) with an image, followed by prompting it to generate both a detailed caption and a visual question-answer pair. This approach of incorporating additional visual data enables the model to develop a more nuanced understanding of both the visual and textual elements, enhancing its capacity to deliver precise and contextually appropriate answers. Furthermore, this method has the potential to reduce the occurrence of hallucinations by providing the model with more contextual information (visual data).</p><p>ShareGPT4V <ref type="bibr">(Chen et al., 2023a</ref>) is a collection of highly descriptive image-text pairs, consisting of two components: 100K captions generated by GPT4-Vision <ref type="bibr">(Yang et al., 2023b)</ref> from a variety of images, and 1.2M captions developed using their pre-trained model, which was trained on the initial set of 100K high-quality captions. These captions comprehensively cover aspects such as global knowledge, object attributes, spatial relationships, and aesthetic evaluations. Utilizing this dataset, the ShareGPT4V-7B model, once fine-tuned, surpasses competing 7B-scale LMMs across all 11 benchmark tests. Notably, it secures a remarkable cumulative score of 1943.8 on the MME benchmark, outperforming the second-place Qwen-VL-Chat-7B <ref type="bibr" target="#b6">(Bai et al., 2023)</ref> model, which was trained with 1.4 billion samples, by 95.6 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-modality Instruction Tuning Models</head><p>InstructPix2Pix (983M) <ref type="bibr" target="#b15">(Brooks et al., 2022)</ref> is a conditional diffusion model trained by fine-tuning Stable Diffusion (983M) <ref type="bibr" target="#b117">(Rombach et al., 2022)</ref> on a constructed multi-modal dataset that contains more than 450K text editing instructions and corresponding images before and after the edit. The authors combine the abilities of two large-scale pretrained models, a language model GPT-3 <ref type="bibr">(Brown et al., 2020b)</ref> and a text-to-image model Stable Diffusion <ref type="bibr" target="#b117">(Rombach et al., 2022)</ref>, to generate the the training dataset. GPT-3 is fine-tuned to generate text edits based on image prompts, while Stable Diffusion is used to convert the generated text edits into actual image edits. InstructPix2Pix is then trained on this generated dataset using a latent diffusion objective. Figure <ref type="figure" target="#fig_8">7</ref> shows the process of generating image editing dataset and training the diffusion model on that dataset. The authors compares the proposed method qualitatively with previous works such as SDEdit <ref type="bibr" target="#b98">(Meng et al., 2022)</ref>  LLaVA (13B) <ref type="bibr">(Liu et al., 2023b)</ref> is a large multimodal model developed by connecting the visual encoder of CLIP (400M) <ref type="bibr" target="#b111">(Radford et al., 2021)</ref> with the language decoder LLaMA (7B) <ref type="bibr">(Touvron et al., 2023a)</ref>. LLaVA is fine-tuned using the generated instructional vision-language dataset consisted of 158K unique language-image instruction-following samples. The data collection process involved creating conversation, detailed description, and complex reasoning prompts. GPT-4 is used to convert image-text pairs into appropriate instruction-following format for this dataset. Visual features such as captions and bounding boxes were used to encode images. LLaVA yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.</p><p>Video-LLaMA <ref type="bibr">(Zhang et al., 2023b</ref>) is a multimodal framework that enhances large language models with the ability to understand both visual and auditory content in videos. The architecture of Video-LLaMA consists of two branche encoders: the Vision-Language (VL) Branch and the Audio-Language (AL) Branch, and a language decoder (Vicuna (7B/13B) <ref type="bibr" target="#b26">(Chiang et al., 2023)</ref>, LLaMA (7B) <ref type="bibr">(Touvron et al., 2023a)</ref>, etc.). The VL Branch includes a frozen pre-trained image encoder (pre-trained vision component of BLIP-2 <ref type="bibr">(Li et al., 2023d)</ref>, which includes a ViT-G/14 and a pre-trained Q-former), a position embedding layer, a video Q-former and a linear layer. The AL Branch includes a pre-</p><p>Multi-modality Instruction # Params Modality Base Model Fine-tuning Trainset Fine-tuned LLMs Model Name # Params Self-build Size InstructPix2Pix (Brooks et al., 2022)<ref type="foot" target="#foot_33">foot_33</ref> 983M I/T Stable Diffusion 983M Yes 450K LLaVA (Liu et al., 2023b)<ref type="foot" target="#foot_34">foot_34</ref> 13B I/T CLIP (Radford et al., 2021) 400M Yes 158K LLaMA (Touvron et al., 2023a) 7B LLaMA (Touvron et al., 2023a) 7B Video-LLaMA (Zhang et al., 2023b)<ref type="foot" target="#foot_35">foot_35</ref> -I/T/V/A BLIP-2 (Li et al., 2023d) -No -ImageBind (Girdhar et al., 2023) -Vicuna (Chiang et al., 2023) 7B/13B InstructBLIP (1.2B) (Dai et al., 2023)<ref type="foot" target="#foot_36">foot_36</ref> -I/T/V BLIP-2 (Li et al., 2023d) -No -Otter (Li et al., 2023b)<ref type="foot" target="#foot_37">foot_37</ref> -I/T/V OpenFlamingo (Awadalla et al., 2023) 9B Yes 2.8M MultiModal-GPT (Gong et al., 2023)<ref type="foot" target="#foot_38">foot_38</ref> -I/T/V OpenFlamingo (Awadalla et al., 2023) 9B No -</p><p>Table 5: An overview of multi-modality instruction fine-tuned LLMs. I/T/V/A stand for Image/Text/Video/Audio trained audio encoder (ImageBind <ref type="bibr" target="#b50">(Girdhar et al., 2023)</ref>) and an Audio Q-former. Figure <ref type="figure" target="#fig_9">8</ref> shows the overall architecture of Video-LLaMA with Vision-Language Branch and Audio-Language Branch. The VL Branch is trained on the Webvid-2M <ref type="bibr" target="#b9">(Bain et al., 2021)</ref> video caption dataset with a video-to-text generation task, and fine-tuned on the instruction tuning data from <ref type="bibr">MiniGPT-4 (Zhu et al., 2023b)</ref>, LLaVA <ref type="bibr">(Liu et al., 2023b)</ref> and VideoChat <ref type="bibr">(Li et al., 2023e)</ref>.</p><p>The AL Branch is trained on video/image instrucaption data to connect the output of ImageBind to language decoder. After finetuning, Video-LLaMA can perceive and comprehend video content, demonstrating its ability to integrate auditory and visual information, understand static images, recognize common-knowledge concepts, and capture temporal dynamics in videos.</p><p>InstructBLIP (1.2B) <ref type="bibr" target="#b35">(Dai et al., 2023</ref>) is a vision-language instruction tuning framework initialized with a pre-trained BLIP-2 <ref type="bibr">(Li et al., 2023d)</ref>) model consisting of an image encoder, an LLM (FlanT5 (3B/11B) <ref type="bibr">(Chung et al., 2022)</ref> or Vicuna (7B/13B) <ref type="bibr" target="#b26">(Chiang et al., 2023)</ref>), and a Query Transformer (Q-Former) to bridge the two. As shown in Figure <ref type="figure" target="#fig_10">9</ref>, the Q-Former extracts instruction-aware visual features from the output embeddings of the frozen image encoder, and feeds the visual features as soft prompt input to the frozen LLM. The authors evaluate the proposed InstructBLIP model on a variety of visionlanguage tasks, including image classification, image captioning, image question answering, and visual reasoning. They use 26 publicly available datasets, dividing them into 13 held-in and 13 held-out datasets for training and evaluation. The authors demonstrate that InstructBLIP achieves state-of-the-art zero-shot performance on a wide range of vision-language tasks. InstructBLIP yields an average relative improvement of 15.0% when compared to BLIP-2, smallest InstructBLIP (4B) outperforms Flamingo (80B) <ref type="bibr" target="#b1">(Alayrac et al., 2022)</ref> on all six shared evaluation datasets with an average relative improvement of 24.8%.</p><p>Otter <ref type="bibr">(Li et al., 2023b</ref>) is a multi-modal model trained by fine-tuning OpenFlamingo (9B) <ref type="bibr">(Awadalla et al., 2023)</ref>, with the language and vision encoders frozen and only fine-tuning the Perceiver resampler module, cross-attention layers, and input/output embeddings. The authors organize diverse multi-modal tasks covering 11 categories and build multi-modal in-context instruction tuning datasets MIMIC-IT of 2.8M multimodal instruction-response pairs, which consists of imageinstruction-answer triplets, where the instructionanswer is tailored to the image. Each data sample also includes context, which contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet. Otter demonstrates the ability to follow user instructions more accurately and provide more detailed descriptions of images compared to OpenFlamingo <ref type="bibr">(Awadalla et al., 2023)</ref>.</p><p>MultiModal-GPT <ref type="bibr" target="#b51">(Gong et al., 2023</ref>) is a multimodal instruction tuning model that is capable of following diverse instructions, generating detailed captions, counting specific objects, and addressing general inquiries. MultiModal-GPT is trained by fine-tuning OpenFlamingo (9B) <ref type="bibr">(Awadalla et al., 2023)</ref> on various created visual instruction data with open datasets, including VQA, Image Captioning, Visual Reasoning, Text OCR, and Visual Dialogue. The experiments demonstrate the proficiency of MultiModal-GPT in maintaining continuous dialogues with humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Domain-specific Instruction Tuning</head><p>In this section, we describe instruction tuning in different domains and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dialogue</head><p>InstructDial <ref type="bibr" target="#b55">(Gupta et al., 2022)</ref> is an instruction tuning framework designed for dialogue. It contains a collection of 48 dialogue tasks in a consistent text-to-text format created from 59 dialogue datasets. Each task instance includes a task description, instance inputs, constraints, instructions, and output. To ensure adherence to instructions, the framework introduces two metatasks: (1) an instruction selection task, where the model selects the instruction corresponding to a given input-output pair; and (2) an instruction binary task, where the model predicts "yes" or "no" if an instruction leads to a given output from an input. Two base models T0-3B <ref type="bibr" target="#b119">(Sanh et al., 2021)</ref> (3B parameters version of T5 <ref type="bibr" target="#b72">(Lester et al., 2021)</ref>) and BART0 <ref type="bibr">(Lin et al., 2022</ref>) (406M parameters based on Bart-large <ref type="bibr" target="#b73">(Lewis et al., 2019)</ref>) are finetuned on the tasks from InstructDial. InstructDial achieves impressive results on unseen dialogue datasets and tasks, including dialogue evaluation and intent detection. Moreover, it delivers even better results when applied to a few-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Intent Classification and Slot Tagging</head><p>LINGUIST <ref type="bibr" target="#b118">(Rosenbaum et al., 2022)</ref> finetunes AlexaTM 5B (Soltan et al., 2022), a 5-billionparameter multilingual model, on the instruction dataset for intent classification and slot tagging tasks. Each instruction consists of five blocks: (i) the language of the generated output, (ii) intention, (iii) slot types and values to include in the output (e.g., the number 3 in [3, snow] corresponds the slot type, and snow is the value used for that slot), (iv) a mapping from slot type labels to numbers, and (v) up to 10 examples to instruct the format of the outputs. LINGUIST shows significant improvements over state-of-the-art approaches in a 10-shot novel intent setting using the SNIPS dataset <ref type="bibr" target="#b34">(Coucke et al., 2018)</ref>. In the zero-shot crosslingual setting of the mATIS++ dataset <ref type="bibr" target="#b159">(Xu et al., 2020)</ref>, LINGUIST surpasses a strong baseline of Machine Translation with Slot Alignment across 6 languages while maintaining intent classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Information Extraction</head><p>InstructUIE <ref type="bibr">(Wang et al., 2023c</ref>) is a unified information extraction (IE) framework based on instruction tuning, which transforms IE tasks to the seq2seq format and solves them by fine-</p><p>Domain Type Domain-specific Instruction Base Model Trainset Size Fine-tuned LLMs Model Name # Params Dialogue InstructDial (Gupta et al., 2022)<ref type="foot" target="#foot_39">foot_39</ref> T0 (Sanh et al., 2021) 3B -Classification LINGUIST (Rosenbaum et al., 2022) AlexaTM (Soltan et al., 2022) 5B 13K Information extraction InstructUIE (Wang et al., 2023c)<ref type="foot" target="#foot_40">foot_40</ref> FlanT5 (Chung et al., 2022) 11B 1.0M Sentiment analysis IT-MTL (Varia et al., 2022)<ref type="foot" target="#foot_41">foot_41</ref> T5 (Raffel et al., 2019) 220M -Writing Writing-Alpaca-7B (Zhang et al., 2023d)<ref type="foot" target="#foot_42">foot_42</ref> LLaMA (Touvron et al., 2023a) 7B -CoEdIT (Raheja et al., 2023)<ref type="foot" target="#foot_43">foot_43</ref> FlanT5 (Chung et al., 2022) 11B CoPoet (Chakrabarty et al., 2022)<ref type="foot" target="#foot_44">foot_44</ref> T5 (Raffel et al., 2019) 11B Medical Radiology-GPT (Liu et al., 2023c)<ref type="foot" target="#foot_45">foot_45</ref> Alpaca (Taori et al., 2023a) 7B 122K ChatDoctor (Li et al., 2023j)<ref type="foot" target="#foot_46">foot_46</ref> LLaMA (Touvron et al., 2023a) 7B 100K ChatGLM-Med (Wang et al., 2023a)<ref type="foot" target="#foot_47">foot_47</ref> ChatGLM (Du et al., 2022) 6B -Arithmetic Goat (Liu and Low, 2023)<ref type="foot" target="#foot_48">foot_48</ref> LLaMA (Touvron et al., 2023a) 7B 1.0M Code WizardCoder (Luo et al., 2023)<ref type="foot" target="#foot_49">foot_49</ref> StarCoder (Li et al., 2023f) 15B 78K</p><p>Table 6: An overview of domain-specific instruction fine-tuned LLMs.  The instruction dataset comprises approximately 82K &lt;instruction: source, target&gt; pairs. As shown in Figure <ref type="figure" target="#fig_12">11</ref>, the model takes instructions from the user specifying the characteristics of the desired text, such as "Make the sentence simpler", and outputs the edited text. CoEdIT achieves state-of-the-art performance on several text editing tasks, including grammatical error correction, text simplification, iterative text editing, and three stylistic editing tasks: formality style transfer, neutralization, and paraphrasing. Furthermore, it can generalize well to new, adjacent tasks not seen during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoPoet</head><p>( <ref type="bibr" target="#b18">Chakrabarty et al., 2022</ref>) is a collaborative poetry writing tool that utilizes a large language model (e.g. T5-3B, T5-11B and T0-3B models) trained on a diverse collection of instructions for poetry writing. Each sample in the instruction dataset includes an &lt;instruction, poem_line&gt; pair. There are three major types of instructions: Continuation, Lexical Constraints, and Rhetorical Techniques. The CoPoet is guided by user instructions that specify desired attributes of the poetry, such as writing a sentence about "love" or ending a sentence with "fly." Not only is the system competitive with publicly available LLMs trained on instructions, such as InstructGPT <ref type="bibr" target="#b106">(Ouyang et al., 2022)</ref>, but it is also capable of satisfying unseen compositional instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Medical</head><p>Radiology-GPT <ref type="bibr">(Liu et al., 2023c</ref>) is a finetuned Alpaca-7B <ref type="bibr">(Taori et al., 2023a)</ref> model for radiology, which utilizes an instruction tuning approach on an extensive dataset of radiology domain knowledge. Radiology reports usually include two corresponding sections: "Findings" and "Impression". The "Findings" section contains detailed observations from the radiology images, while the "Impression" section summarizes the interpretations drawn from those observations. Radiology-GPT provides a brief instruction to the "Findings" text: "Derive the impression from findings in the radiology report". The "Impression" text from the same report serves as the target output. In comparison to general language models such as StableLM (Islamovic), Dolly <ref type="bibr">(Conover et al., 2023a)</ref>, and LLaMA <ref type="bibr">(Touvron et al., 2023a)</ref>, Radiology-GPT demonstrates significant adaptability in radiological diagnosis, research, and communication.</p><p>ChatDoctor <ref type="bibr">(Li et al., 2023j)</ref> is based on the fine-tuned LLaMa-7B <ref type="bibr">(Touvron et al., 2023a)</ref> model, utilizing the alpaca instruction dataset <ref type="bibr">(Taori et al., 2023a)</ref> and the HealthCareMagic100k patient-doctor dialogue dataset. And prompt templates are designed for retrieving external knowledge databases, such as the Disease Database and Wikipedia retrieval, during doctor-patient conversations to obtain more accurate outputs from the model. The ChatDoctor significantly improves the model's ability to comprehend patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, the accuracy of its responses is substantially improved.</p><p>ChatGLM-Med <ref type="bibr">(Wang et al., 2023a</ref>) is finetuned on the Chinese medical instruction dataset based on the ChatGLM-6B <ref type="bibr" target="#b41">(Du et al., 2022)</ref> model. The instruction dataset comprises medically relevant question and answer pairs, created using the GPT 3.5 API and the Medical Knowledge Graph.</p><p>This model improves the questionanswering performance of ChatGLM <ref type="bibr" target="#b41">(Du et al., 2022)</ref> in the medical field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Arithmetic</head><p>Goat <ref type="bibr" target="#b94">(Liu and Low, 2023</ref>) is a fine-tuned LLaMA-7B <ref type="bibr">(Touvron et al., 2023a</ref>) model based on instructions, which aims to solve arithmetic problems. It expresses arithmetic problems in the form of natural language question answering, such as "What is 8914/64?", by generating hundreds of instruction templates using <ref type="bibr">ChatGPT (OpenAI, 2022)</ref>. The model applies various techniques to enhance its adaptability to diverse question formats, such as randomly removing spaces between numbers and symbols in the arithmetic expression and replacing "*" with "x" or "times". The Goat model achieves state-of-the-art performance on the BIG-bench <ref type="bibr">(Srivastava et al., 2022a)</ref> arithmetic subtask. In particular, zero-shot Goat-7B matches or exceeds the accuracy achieved by the few-shot PaLM-540B <ref type="bibr" target="#b28">(Chowdhery et al., 2022)</ref>. <ref type="bibr">(Luo et al., 2023)</ref> utilizes StarCoder 15B <ref type="bibr">(Li et al., 2023f)</ref> as the foundation with complex instruction tuning, by adapting the Evol-Instruct method <ref type="bibr">(Xu et al., 2023a)</ref> to the domain of code. The training dataset is produced through iterative application of the Evol-Instruct technique on the Code Alpaca dataset <ref type="bibr">(Taori et al., 2023b)</ref>, which includes the following attributes for each sample: instruction, input, and expected output. For instance, when the instruction is "Amend the following SQL query to select distinct elements", the input is the SQL query, and the expected output is the generated answer. The WizardCoder outperforms all other open-source Code LLMs and even surpasses the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Code</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WizardCoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Efficient Tuning Techniques</head><p>Efficient fine-tuning techniques aim at adapting LLMs to downstream tasks by optimizing a small fraction of parameters in multiple ways, i.e., addition-based, specification-based, and reparameterization-based. Addition-based methods introduce extra trainable parameters or modules not present in the original model. Representative methods include adapter tuning <ref type="bibr" target="#b60">(Houlsby et al., 2019)</ref> and prompt-based tuning <ref type="bibr" target="#b120">(Schick and Schütze, 2021)</ref>. Specification-based methods specify certain inherent model parameters to be tuned while freezing others. For example, BitFit <ref type="bibr" target="#b171">(Zaken et al., 2022)</ref> tunes the bias terms of the pre-trained model. Reparameterization methods transform model weights into more parameter-efficient forms for tuning. The key hypothesis is that model adaptation is low-rank, so weights can be reparameterized into lowrank factors or a low-dimensional subspace (e.g., <ref type="bibr">LoRA (Hu et al., 2021)</ref>). Intrinsic prompt tuning finds a low-dimensional subspace shared by tuning prompts across diverse tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">LoRA</head><p>Low-Rank Adaptation (LoRA) <ref type="bibr" target="#b61">(Hu et al., 2021)</ref> enables efficient adaptation of LLMs using lowrank updates. LoRA use DeepSpeed <ref type="bibr" target="#b116">(Rasley et al., 2020)</ref> as the training backbone. The key insight of LoRA is that the actual change in LLMs' weights required for new task adaptation lies in a lowdimensional subspace. Specifically, for a pretrained weight matrix W 0 , the authors model the adapted weight matrix as W 0 + ∆W , where ∆W is a low rank update. ∆W is parameterized as ∆W = BA, where A and B are much smaller trainable matrices. The rank r of ∆W is chosen to be much smaller than the dimensions of W 0 . The intuition is that instead of directly training all of W 0 , the authors train low-dimensional A and B, which indirectly trains W 0 in a low-rank subspace of directions that matter for the downstream task. This results in far fewer trainable parameters compared to full finetuning. For GPT-3, LoRA reduces the number of trainable parameters by 10,000x and memory usage by 3x compared to full fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">HINT</head><p>HINT <ref type="bibr" target="#b63">(Ivison et al., 2022)</ref> combines the generalization benefits of instruction tuning with efficient on-demand fine-tuning, avoiding repeatedly processing lengthy instructions. The essence of HINT lies in hypernetworks, which generate parameter-efficient modules for LLMs adaptation based on natural language instructions and few-shot examples. The adopted hypernetwork converts instructions and few-shot examples into a encoded instruction and generates adapter and prefix parameters using a pretrained text encoder and cross-attention based parameter generator. Then, the generated adapters and prefixes are inserted into the backbone model as efficient tuning modules. At inference, the hypernetwork performs inference only once per task to generate adapted modules. The benefits are that HINT can incorporate long instructions and additional fewshots without increasing compute, unlike regular fine-tuning or input concatenation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Qlora</head><p>QLORA <ref type="bibr" target="#b37">(Dettmers et al., 2023)</ref> includes optimal quantization and memory optimization, aiming at providing efficient and effective LLMs finetuning. QLORA includes 4-bit NormalFloat (NF4) Quantization, which is a quantization scheme optimized for the typical normal distribution of LLM weights. By quantizing based on the quantiles of a normal distribution, NF4 provides better performance than standard 4-bit integer or float quantization. To further reduce memory, the quantization constants are themselves quantized to 8 bits. This second level of quantization saves an additional 0.37 bits per parameter on average. QLORA leverages NVIDIA's unified memory feature to page optimizer states to CPU RAM when GPU memory is exceeded. avoiding out-of-memory during training. QLORA enables training a 65B parameter LLM on a single 48GB GPU with no degradation compared to full 16bit finetuning. QLORA works by freezing the 4-bit quantized base LLM, then backpropagating through it into a small set of 16-bit low-rank adapter weights which are learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">LOMO</head><p>LOw-Memory Optimization (LOMO) <ref type="bibr">(Lv et al., 2023)</ref> enables full parameter fine-tuning of LLMs using limited computational resources through a fusion of gradient computation and update. The essence is to fuse gradient computation and parameter update into one step during backpropagation, thereby avoiding storage of full gradient tensors. Firstly, theoretical analysis is provided in LOMO on why SGD can work well for fine-tuning large pre-trained models despite its challenges on smaller models. In addition, LOMO updates each parameter tensor immediately after computing its gradient in backpropagation.</p><p>Storing the gradient of one parameter at a time reduces gradient memory to O(1). LOMO employs gradient value clipping, separate gradient norm computation pass and dynamic loss scaling to stabilize training. The integration of activation checkpointing and ZeRO optimization methods saves memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Delta-tuning</head><p>Delta-tuning <ref type="bibr">(Ding et al., 2023b)</ref> provides optimization and optimal control perspectives for theoretical analyzation. Intuitively, delta-tuning performs subspace optimization by restricting tuning to a low-dimensional manifold. The tuned parameters act as optimal controllers guiding model behavior on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation, Analysis and Criticism</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Close-ended Evaluations</head><p>It is widely accepted among researchers that general-purpose models must demonstrate proficiency in certain core tasks before they can effectively generalize to meet diverse real-world needs. Close-ended evaluations help achieve this objective, often involving multiple-choice questions to assess the performance of LLMs. Below are 6 widely used close-ended evaluations:</p><p>(1) MMLU. Massive Multitask Language Understanding (MMLU) <ref type="bibr">(Hendrycks et al., 2020a)</ref> consists of 14079 questions covering 57 tasks including elementary mathematics, US history, computer science, law, and more. The wide range of subjects and complex questions make MMLU suitable for testing the model's language comprehension and decision-making capabilities.</p><p>(2) MATH and (3) GSM8K. MATH <ref type="bibr" target="#b58">(Hendrycks et al., 2021)</ref> and GSM8K <ref type="bibr" target="#b31">(Cobbe et al., 2021)</ref> are two distinct mathematical datasets utilized for evaluating different aspects of model capabilities. The MATH <ref type="bibr" target="#b58">(Hendrycks et al., 2021)</ref> dataset comprises 12,500 complex competition-level mathematics problems, primarily designed to assess the ability of models to tackle challenging and advanced mathematical questions typically encountered at the college level. Conversely, the GSM8K <ref type="bibr" target="#b31">(Cobbe et al., 2021)</ref> dataset contains 8,500 high-quality elementary school math problems, aimed at testing the basic mathematical reasoning abilities of models.</p><p>(4) BBH. BBH, short for BIG-Bench Hard <ref type="bibr">(Suzgun et al., 2022a)</ref>, is a subset of the BIG-Bench <ref type="bibr">(Srivastava et al., 2022b)</ref> dataset comprising 23 challenging tasks. These tasks were selected because they consistently proved too difficult for current large language models to handle effectively. Requiring complex, multi-step reasoning, the BBH dataset is primarily utilized to assess the general reasoning capabilities of models, testing their ability to navigate and solve intricate problems.</p><p>(5) HumanEval (Coding). HumanEval <ref type="bibr">(Chen et al., 2021a)</ref> consists of 164 programming problems, including language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions. The primary purpose of this dataset is to assess the ability of models to generate correct programs based on provided docstrings.</p><p>(6) IFEval. IFEval <ref type="bibr">(Zhou et al., 2023b)</ref> consists of 500 prompts, each containing specific instructions like "write an article with more than 800 words" or "enclose your response in double quotation marks." This dataset is used to test the ability of large language models to accurately follow given instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">HELM Evaluation</head><p>HELM <ref type="bibr" target="#b84">(Liang et al., 2022)</ref> is a holistic evaluation of Language Models (LMs) to improve the transparency of language models, providing a more comprehensive understanding of the capabilities, risks, and limitations of language models. Specifically, differing from other evaluation methods, HELM holds that a holistic evaluation of language models should focus on the following three factors:</p><p>(1) Broad coverage. During the development, language models can be adapted to various NLP tasks (e.g., sequence labeling and question answering), thus, the evaluation of language models needs to be carried out in a wide range of scenarios. To involve all potential scenarios, HELM proposed a top-down taxonomy, which begins by compiling all existing tasks in a major NLP conference (ACL2022) into a task space and dividing each task into the form of scenarios (e.g., languages) and metrics (e.g., accuracy). Then when facing a specific task, the taxonomy would select one or more scenarios and metrics in the task space to cover it. By analyzing the structure of each task, HELM clarifies the evaluation content (task scenarios and metrics) and improves the scenario coverage of language models from 17.9% to 96.0%.</p><p>(2) Multi-metric measurement. In order to enable human to weigh language models from different perspectives, HELM proposes multimetric measurement. HELM has covered 16 different scenarios and 7 metrics. To ensure the results of intensive multi-metric measurement, HELM measured 98 of 112 possible core scenarios (87.5%).</p><p>(3) Standardization. The increase in the scale and training complexity of language models has seriously hindered human's understanding of the structure of each language model. To establish a unified understanding of existing language models, HELM benchmarks 30 well-known language models, covering such institutions as Google (UL2 <ref type="bibr" target="#b133">(Tay et al., 2022)</ref>), OpenAI (GPT-3 <ref type="bibr">(Brown et al., 2020b)</ref>), and EleutherAI (GPT-NeoX <ref type="bibr" target="#b14">(Black et al., 2022)</ref>). Interestingly, HELM pointed out that LMs such as T5 <ref type="bibr" target="#b114">(Raffel et al., 2019)</ref> and Anthropic-LMv4-s3 <ref type="bibr">(Bai et al., 2022a)</ref> had not been directly compared in the initial work, while LLMs such as GPT-3 and YaLM were still different from their corresponding reports after multiple evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">LLM As a Judge</head><p>LLM as a judge refers to a set of methods that utilize powerful LLMs, particularly <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>, to evaluate the outputs of other LLMs. There are three primary reasons for this approach: (1) Efficiency -Manually reviewing numerous LLM outputs can be labor-intensive, whereas GPT-4 can evaluate large-scale responses quickly, saving both time and effort; (2) Reliable Benchmark -As one of the most advanced models available, GPT-4 provides a dependable benchmark, allowing researchers to compare the performance of different LLMs against a high standard; and (3) Enhanced Capability -With improved comprehension and reasoning over previous models, GPT-4 is better suited to analyze subtle aspects of language generation and handle complex outputs from other LLMs. In the following, we detail 4 commonly accepted judge benchmarks:</p><p>(1) AlpacaEval. AlpacaEval <ref type="bibr">(Li et al., 2023h</ref>) is an automated evaluation metric leveraging LLMs, consisting of 805 instructions selected to reflect typical user interactions from the Alpaca web demo<ref type="foot" target="#foot_50">foot_50</ref> . Specifically, for each instruction, both a baseline model b (currently GPT-4 turbo (OpenAI, 2023)) and the model under evaluation m generate responses. A GPT-4 turbo-based evaluator then conducts a head-to-head comparison of these responses, determining the probability of favoring the evaluated model. The win rate is calculated as the expected probability that the evaluator prefers the evaluated model's response across the 805 instructions, serving as a key metric for assessing the performance of the evaluated LM chatbot.</p><p>(2) Length-Controlled AlpacaEval. Length-Controlled AlpacaEval <ref type="bibr" target="#b42">(Dubois et al., 2024)</ref> is a variation of the AlpacaEval <ref type="bibr">(Li et al., 2023h)</ref> evaluation metric, designed to minimize length bias, as the original AlpacaEval tends to favor models that produce longer responses. To achieve this goal, <ref type="bibr" target="#b42">Dubois et al. (2024)</ref> first fit a generalized linear model to predict the annotator's (GPT-4's) preference based on three factors: (1) the instruction, (2) the model used, and (3) the length difference between the baseline and the model's output. Then, by conditioning the length difference to 0, <ref type="bibr" target="#b42">Dubois et al. (2024)</ref> can obtain the lengthcontrolled preference. This idea, which predicts the outcome while conditioning on the length difference (mediator), is a common technique in statistical inference, and by introducing it, Length-Controlled AlpacaEval increases the Spearman correlation with LMSYS' Chatbot Arena from 0.94 to 0.98.</p><p>(3) MT-Bench. Currently, close-ended evaluations only measure LLMs' core capability on a confined set of tasks, such as MMLU <ref type="bibr">(Hendrycks et al., 2020a)</ref> for multi-choice decisions, without adequately assessing its alignment with human preference in open-ended tasks, such as the ability to adhere to instructions in multi-turn dialogues accurately. To alleviate this issue, <ref type="bibr" target="#b180">Zheng et al. (2023)</ref> introduced MT-Bench, which comprises 80 high-quality multi-turn questions designed to assess LLMs' capability in multi-turn conversations and instruction-following, with evaluations conducted using GPT-4. MT-Bench is meticulously crafted to cover eight common tasks: writing, roleplay, extraction, reasoning, math, coding, knowledge I (STEM), and knowledge II (humanities/social sciences). For alignment, GPT-4 achieves over 80% agreement, comparable to the level of agreement among humans, making it a more reliable choice for a public benchmark.</p><p>(4) WildBench. Although the above evaluations are effective, they have notable limitations in task composition and skill coverage. For example, MT-Bench <ref type="bibr">(Hendrycks et al., 2020a)</ref> includes only 80 test instructions, while AlpacaEval <ref type="bibr">(Li et al., 2023h)</ref> features many straightforward tasks, such as "What is the capital of Australia?" To address this issue, <ref type="bibr">Lin et al. (2024)</ref> introduced WildBench, comprising 1,024 test instructions carefully curated from extensive human-chatbot conversation logs. WildBench draws directly from real-world user interactions, featuring numerous challenging tasks, such as coding and math problem-solving. These tasks frequently demand critical thinking, making WildBench significantly more difficult than other benchmarks. WildBench utilizes two metrics: WB-Reward for pairwise comparisons and WB-Score for individual assessments. Both metrics show strong alignment with human evaluations, with Pearson correlations of 0.98 for WB-Reward and 0.95 for WB-Score when compared to the humanvoted ratings.</p><p>8.4 Low-resource Instruction Tuning <ref type="bibr">Gupta et al. (2023)</ref> attempts to estimate the minimal downstream training data required by SFT models to match the SOTA supervised models over various tasks. <ref type="bibr">Gupta et al. (2023)</ref> conducted experiments on 119 tasks from Super Natural Instructions (SuperNI) in both single-task learning (STL) and multi-task learning (MTL) settings. The results indicate that in the STL setting, SFT models with only 25% of downstream training data outperform the SOTA models on those tasks, while in the MTL setting, just 6% of downstream training data can lead SFT models to achieve the SOTA performance. These findings suggest that instruction tuning can effectively assist a model in quickly learning a task even with limited data.</p><p>However, due to resource limitations, <ref type="bibr">Gupta et al. (2023)</ref> did not conduct experiments on LLMs, like T5-11B. So, to gain a more comprehensive understanding of the SFT models, further investigation using larger language models and datasets is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Smaller Instruction Dataset</head><p>SFT requires a substantial amount of specialized instruction data for training. <ref type="bibr">Zhou et al. (2023a)</ref> hypothesized that the pre-trained LLM only has to learn the style or format to interact with users and proposed LIMA that achieves strong performance by fine-tuning an LLM on only 1,000 carefully selected training examples.</p><p>Specifically, LIMA first manually curates 1,000 demonstrations with high-quality prompts and responses. Then the 1,000 demonstrations are used to fine-tune the pre-trained 65B-parameter LLaMa <ref type="bibr">(Touvron et al., 2023b)</ref>. By comparison, across more than 300 challenging tasks, LIMA outperfrms GPT-davinci003 <ref type="bibr">(Brown et al., 2020b)</ref>, which was fine-tuned on 5,200 examples by human feedback tuning. Moreover, with only half amount of demonstrations, LIMA achieves equivalent results to <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>, Claude <ref type="bibr">(Bai et al., 2022b)</ref>, and Bard<ref type="foot" target="#foot_51">foot_51</ref> . Above all, LIMA demonstrated that LLMs' powerful knowledge and capabilities can be exposed to users with only a few carefully curated instructions to fine-tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Evaluating Instruction Tuning Datasets</head><p>The performance of SFT model highly depends on the SFT datasets. However, there lacks of evaluations for these SFT datasets from open-ended and subjective aspects.</p><p>To address this issue, <ref type="bibr">Wang et al. (2023d)</ref> performs dataset evaluation by fine-tuning the LLaMa model <ref type="bibr">(Touvron et al., 2023b)</ref> on various of open SFT datasets and measure different finetuned models through both automatic and human evaluations. An additional model is trained on the combination of SFT datasets. For the results, <ref type="bibr">Wang et al. (2023d)</ref> showed that there is not a single best SFT dataset across all tasks, while by manually combining datasets it can achieve the best overall performance. Besides, <ref type="bibr">Wang et al. (2023d)</ref> pointed out that though SFT can bring large benefits on LLMs at all sizes, smaller models and models with a high base quality benefit most from SFT. For human evaluations, <ref type="bibr">Wang et al. (2023d)</ref> a larger model is more likely to gain a higher acceptability score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7">Superficial Alignment</head><p>Despite the impressive improvements in the performance of instruction tuning, there lacks clarity about the specific knowledge that models acquire through instruction tuning, raising questions about: Does instruction tuning just learn Pattern Copying? or How exactly does the alignment tuning transform a base LLM?</p><p>To answer these questions, <ref type="bibr" target="#b68">Kung and Peng (2023)</ref> delves into the analysis of how models make use of instructions during SFT by comparing the tuning when provided with altered instructions versus the original instructions.</p><p>Specifically, <ref type="bibr" target="#b68">Kung and Peng (2023)</ref> creates simplified task definitions that remove all semantic components, leaving only the output information. In addition, <ref type="bibr" target="#b68">Kung and Peng (2023)</ref> also incorporates delusive examples that contain incorrect input-output mapping. Surprisingly, the experiments show that models trained on these simplified task definitions or delusive examples can achieve comparable performance to the ones trained on the original instructions and examples. Moreover, the paper also introduces a baseline for the classification task with zero-shot, which achieves similar performance to SFT in lowresource settings.</p><p>Similar to the findings of <ref type="bibr" target="#b68">Kung and Peng (2023)</ref>, several subsequent studies <ref type="bibr">(Zhou et al., 2023a;</ref><ref type="bibr">Lin et al., 2023a)</ref> reached the same conclusion: the observed performance improvements in current SFT models are often due to superficial alignment. This means the models excel at recognizing superficial alignment, such as mastering output formats and making educated guesses, rather than truly understanding and learning the underlying tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.8">Proprietary LLMs Imitation</head><p>LLMs imitation is an approach that collects outputs from a stronger model, such as a proprietary system like ChatGPT, and uses these outputs to fine-tune an open-source LLM. Through this way, an opensource LLM may get competitive capabilities with any proprietary model. <ref type="bibr" target="#b52">Gudibande et al. (2023)</ref> conducted several experiments to critically analyze the efficacy of model imitation. Specifically, <ref type="bibr" target="#b52">Gudibande et al. (2023)</ref> first collected datasets from outputs of ChatGPT over broad tasks. Then these datasets were used to fine-tune a range of models covering sizes from 1.5B to 13B, base models GPT-2 and LLaMA, and data amounts from 0.3M tokens to 150M tokens.</p><p>For evaluations, Gudibande et al. ( <ref type="formula">2023</ref>) demonstrated that on tasks with supported datasets, imitation models are far better than before, and their outputs appear similar to ChatGPT's. While on tasks without imitation datasets, imitation models do not have improvement or even decline in accuracy.</p><p>Thus, <ref type="bibr" target="#b52">Gudibande et al. (2023)</ref> pointed out that it's the phenomenon that imitation models are adept at mimicking ChatGPT's style (e.g., being fluent, confident and well-structured) that makes researchers have the illusion about general abilities of imitation models. So, <ref type="bibr" target="#b52">Gudibande et al. (2023)</ref> suggested that instead of imitating proprietary models, researchers had better focus on improving the quality of base models and instruction examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>This work surveys recent advances in the fast growing field of instruction tuning, which can also be referred to as supervised fine-tuning (SFT). We make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, SFT's applications to different modalities, domains and application. We also review analysis on SFT models to discover both their advantages and potential pitfalls. We hope this work will act as a stimulus to motivate further endeavors to address the deficiencies of current SFT models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General pipeline of instruction tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Instance-</head><figDesc>Input: Sentence: During breakfast one morning, he seemed lost in thought and ignored his food. -Expected Output: How long was he lost in thoughts? Instance (b) An example of INSTANCES in Natural Instruction dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The figure is adapted from Mishra et al. (2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><figDesc>Input: "Context: … 'That's fantastic, I'm glad we came to something we both agree with.' Utterance: 'Me too. I hope you have a wonderful camping trip.'" • Output: "Yes" • Explanation: "The participant engages in small talk when wishing their opponent to have a wonderful trip." • Input: "Context: … 'Sounds good, I need food the most, what is your most needed item?!' Utterance: 'My item is food too'." • Output: "Yes" • Explanation: "The utterance only takes the negotiation forward and there is no side talk. Hence, the correct answer is 'No'." Definition "... Given an utterance and recent dialogue context containing past 3 utterances (wherever available), output 'Yes' if the utterance contains the small-talk strategy, otherwise output 'No'. Small-talk is a cooperative negotiation strategy. It is used for discussing topics apart from the negotiation, to build a rapport with the opponent." Task Instruction • Input: "Context: … 'I am excited to spend time with everyone from camp!' Utterance: 'That's awesome! I really love being out here with my son. Do you think you could spare some food?' " • Expected Output: An example of INSTRUCTIONS in Super-Natural Instruction dataset. Input: What kind of, no hold up, what describes the proportionality of acceleration to force and mass? Output: ["What describes the proportionality of acceleration to force and mass?"] Instance (b) An example of INSTANCES in Super-Natural Instruction dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The figure is adapted from Wang et al. (2022e).</figDesc><graphic coords="5,75.49,318.57,210.00,55.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The figure is copied from Köpf et al. (2023).</figDesc><graphic coords="5,306.14,70.87,226.77,85.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: General pipeline of distillation for synthetic data generation. The figure is adapted from Taori et al. (2023a).</figDesc><graphic coords="6,82.21,236.89,430.87,176.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: General pipeline of self-improvement for synthetic data generation. The figure is adapted from Wang et al. (2022c).</figDesc><graphic coords="8,82.21,70.87,430.86,189.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Image editing dataset generation and diffusion model training. The figure is copied from Brooks et al. (2022).</figDesc><graphic coords="16,306.14,70.87,226.76,67.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Overall architecture of Video-LLaMA. The figure is copied from Zhang et al. (2023b).</figDesc><graphic coords="17,306.14,351.76,226.76,230.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Overall architecture of InstructBLIP. The figure is copied from Dai et al. (2023).</figDesc><graphic coords="18,70.87,70.86,226.77,94.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The overview framework of InstructUIE. The figure is copied from Wang et al. (2023c).</figDesc><graphic coords="19,70.87,399.47,226.77,104.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The overview framework of COEDIT. The figure is copied from Raheja et al. (2023).</figDesc><graphic coords="20,70.87,70.86,226.75,168.11" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://huggingface.co/datasets/RyokoAI/ShareGPT52K</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>https://huggingface.co/bigscience/bloomz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://huggingface.co/google/flan-t5-xxl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>https://github.com/tatsu-lab/stanford_alpaca</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://github.com/lm-sys/FastChat</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>https://github.com/nlpxucan/WizardLM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>https://github.com/THUDM/ChatGLM2-6B</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8"><p>https://huggingface.co/facebook/opt-iml-30b</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9"><p>https://github.com/databrickslabs/dolly</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_10"><p>https://huggingface.co/tiiuae/falcon-40b-instruct</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_11"><p>https://huggingface.co/JosephusCheung/Guanaco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_12"><p>https://huggingface.co/openaccess-ai-collective/minotaur-15b</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_13"><p>https://huggingface.co/NousResearch/Nous-Hermes-13b</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_14"><p>https://github.com/allenai/open-instruct</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_15"><p>https://github.com/RUC-GSAI/YuLan-Chat</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_16"><p>https://github.com/OpenLMLab/MOSS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_17"><p>https://github.com/jondurbin/airoboros</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_18"><p>https://github.com/thunlp/UltraChat</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_19"><p>https://github.com/EleutherAI/lm-evaluation-harness</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_20"><p>https://openai.com/blog/chatgpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_21"><p>https://sharegpt.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_22"><p>https://www.anthropic.com/index/introducing-claude</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_23"><p>https://bard.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_24"><p>https://www.anthropic.com/index/introducing-claude</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_25"><p>https://huggingface.co/datasets/databricks/databricksdolly-15k</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_26"><p>https://huggingface.co/spaces/HuggingFaceH4 /open_llm_leaderboard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_27"><p>https://github.com/teknium1/GPTeacher</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_28"><p>https://github.com/teknium1/GPTeacher</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_29"><p>https://huggingface.co/datasets/OpenAssistant/oasst1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_30"><p>https://huggingface.co/datasets/vicgalle/alpaca-gpt4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_31"><p>https://sharegpt.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_32"><p>https://txsun1997.github.io/blogs/moss.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_33"><p>https://github.com/timothybrooks/instruct-pix2pix</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_34"><p>https://github.com/haotian-liu/LLaVA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_35"><p>https://github.com/DAMO-NLP-SG/Video-LLaMA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_36"><p>https://github.com/salesforce/LAVIS/tree/main/projects/instructblip</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_37"><p>https://github.com/Luodian/Otter</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_38"><p>https://github.com/open-mmlab/Multimodal-GPT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_39"><p>https://github.com/prakharguptaz/Instructdial</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_40"><p>https://github.com/BeyonderXX/InstructUIE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_41"><p>https://github.com/amazon-science/instruction-tuning-for-absa</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_42"><p>https://github.com/facebookresearch/EditEval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_43"><p>https://github.com/vipulraheja/coedit</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_44"><p>https://github.com/vishakhpk/creative-instructions</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_45"><p>https://huggingface.co/spaces/allen-eric/radiology-gpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_46"><p>https://github.com/Kent0n-Li/ChatDoctor</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_47"><p>https://github.com/SCIR-HI/Med-ChatGLM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_48"><p>https://github.com/liutiedong/goat</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_49"><p>https://github.com/nlpxucan/WizardLM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_50"><p>https://crfm.stanford.edu/2023/03/13/ alpaca.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_51"><p>Bard, designed by Google, is an interface to generative AI platform, and the link is: https://ai.google/static/documents/google-about-bard.pdf</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>Table <ref type="table">7</ref> gives an overview of our collected datasets. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Evaluating correctness and faithfulness of instructionfollowing models for question answering</title>
		<author>
			<persName><forename type="first">Parishad</forename><surname>Vaibhav Adlakha</surname></persName>
		</author>
		<author>
			<persName><surname>Behnamghader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName><surname>Reddy</surname></persName>
		</author>
		<idno>ArXiv, abs/2307.16877</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">L</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karén</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023a. Falcon-40B: an open large language model with stateof-the-art performance</title>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulaziz</forename><surname>Alshamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merouane</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Heslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulaziz</forename><surname>Alshamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merouane</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Heslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<imprint>
			<pubPlace>Quentin Malartic</pubPlace>
		</imprint>
	</monogr>
	<note>et al. 2023b. Falcon-40b: an open large language model with stateof-the-art performance</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Anas</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Hanafy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Kalyani Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><surname>Jitsev</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. 2023. Openflamingo</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Promptsource: An integrated development environment and repository for natural language prompts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srulik</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maged</forename><forename type="middle">S</forename><surname>Al-Shaibani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Almubarak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno>ArXiv, abs/2202.01279</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading</title>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>and beyond</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08073</idno>
		<title level="m">Harmlessness from ai feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text2live: Textdriven layered image and video editing</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dolev</forename><surname>Ofri-Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafail</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="707" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pushshift reddit dataset</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savvas</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Blackburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web and Social Media</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Edward</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Sanseviero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<title level="m">Open llm leaderboard. Hugging Face</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">Stella</forename><surname>Rose Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">G</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviya</forename><surname>Raff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oskar</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der Wal</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.01373</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">Rose</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">G</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Martin Pieler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benqi</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Weinbach</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.06745</idno>
		<title level="m">Gpt-neox-20b: An open-source autoregressive language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Instructpix2pix: Learning to follow image editing instructions</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>ArXiv, abs/2211.09800</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>et al. 2020a</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><surname>Mccandlish</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.14165</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Help me write a poeminstruction tuning as a vehicle for collaborative poetry writing</title>
		<author>
			<persName><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengxing</forename><surname>He</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.13669</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Code alpaca: An instructionfollowing llama model for code generation</title>
		<author>
			<persName><forename type="first">Sahil</forename><surname>Chaudhary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Guiming</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shunian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianquan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.11684</idno>
		<title level="m">Xiang Wan, and Benyou Wang. 2024a. Allava: Harnessing gpt4v-synthesized data for a lite vision-language model</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12793</idno>
		<title level="m">Feng Zhao, and Dahua Lin. 2023a. Sharegpt4v: Improving large multimodal models with better captions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2021a</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Arun</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<idno>ArXiv, abs/2107.03374</idno>
		<imprint/>
	</monogr>
	<note>Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating large language models trained on code</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distinguish before answer: Generating contrastive explanation as knowledge for commonsense question answering</title>
		<author>
			<persName><forename type="first">Qianglong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingshi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">2024b. Self-play fine-tuning converts weak language models to strong language models</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihe</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizhuo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01335</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<imprint>
			<date type="published" when="2023-04">2023. April 2023</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benton</forename><forename type="middle">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><surname>Austin</surname></persName>
		</author>
		<editor>Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra</editor>
		<imprint>
			<pubPlace>Kevin Robinson, Liam Fedus, Denny Zhou, Daphne</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Le Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Valter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Huai Hsin Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.11416</idno>
	</analytic>
	<monogr>
		<title level="m">Hyung Won Chung</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Scaling instruction-finetuned language models</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>ArXiv, abs/1905.10044</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<idno>ArXiv, abs/1803.05457</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="454" to="470" />
			<date type="published" when="2018">2020. 2018</date>
		</imprint>
	</monogr>
	<note>Think you have solved question answering? try arc, the ai2 reasoning challenge</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>ArXiv, abs/2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ai Collective ;</forename><surname>Openaccess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Conover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharia</surname></persName>
		</author>
		<ptr target="software:huggingface.co/openaccess-ai-collective/minotaur-15b" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>et al. 2023a. Free dolly: Introducing the world&apos;s first truly open instruction-tuned llm</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mike</forename><surname>Conover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Reynold Xin. 2023b. Free dolly: Introducing the world&apos;s first truly open instructiontuned llm</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théodore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Instructblip: Towards general-purpose visionlanguage models with instruction tuning</title>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huat</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.06500</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14314</idno>
		<title level="m">Qlora: Efficient finetuning of quantized llms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Enhancing chat language models by scaling high-quality instructional conversations</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14233</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">2023b. Parameter-efficient fine-tuning of largescale pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="220" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glm: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Galambosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori B</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.04475</idno>
		<title level="m">Length-controlled alpacaeval: A simple way to debias automatic evaluators</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Jon</forename><surname>Durbin</surname></persName>
		</author>
		<ptr target=".com/jondurbin/airoboros" />
		<title level="m">Airoboros. software: github</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Editeval: An instruction-based benchmark for text improvements</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">2023a. Exploring the feasibility of chatgpt for event extraction</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<idno>ArXiv, abs/2303.03836</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Niklas Muennighoff, et al. 2021. A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Version v0. 0.1. Sept</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">2023b. Enabling large language models to generate text with citations</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14627</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno>ArXiv, abs/2009.11462</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imagebind: One embedding space to bind them all</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Multimodal-gpt: A vision and language model for dialogue with humans</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianmengke</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.04790</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Gudibande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15717</idno>
		<title level="m">The false promise of imitating proprietary llms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11644</idno>
		<title level="m">Textbooks are all you need</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Saurabh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Sawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mutsumi</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05539</idno>
		<title level="m">Santosh Mashetty, and Chitta Baral. 2023. Instruction tuned models are quick learners</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Instructdial: Improving zero and few-shot generalization in dialogue through instruction tuning</title>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskénazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03300</idno>
		<title level="m">Measuring massive multitask language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Dawn Xiaodong Song, and Jacob Steinhardt. 2020b. Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<idno>ArXiv, abs/2009.03300</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m">Measuring mathematical problem solving with the math dataset</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Or</forename><surname>Honovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09689</idno>
		<title level="m">Unnatural instructions: Tuning language models with (almost) no human labor</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<idno>arXiv:2212.10403</idno>
		<title level="m">Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Lora: Low-rank adaptation of large language models</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models</title>
		<author>
			<persName><forename type="first">Yuzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhuo</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tangjun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08322</idno>
		<ptr target="https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablel" />
	</analytic>
	<monogr>
		<title level="m">Anel Islamovic. Stability AI Launches the First of its StableLM Suite of Language Models -Stability AI -stability</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Accessed 09-Jun-2023</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Hint: Hypernetwork instruction tuning for efficient zero-shot generalisation</title>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<idno>ArXiv, abs/2212.10315</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Opt-iml: Scaling language model instruction meta learning through the lens of generalization</title>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/2212.12017</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Guanaco: Generative universal assistant for natural-language adaptive context-aware omnilingual outputs</title>
		<author>
			<persName><surname>Josephuscheung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00700</idno>
		<title level="m">Unifiedqa: Crossing format boundaries with a single qa system</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannic</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotiris</forename><surname>Dimitri Von Rütte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Rui</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><surname>Barhoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richárd</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><surname>Nagyfi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07327</idno>
		<title level="m">Openassistant conversations-democratizing large language model alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Do models really learn to follow instructions? an empirical study of instruction tuning</title>
		<author>
			<persName><forename type="first">Po-Nien</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.11383</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Oig: the open instruction generalist dataset</title>
		<author>
			<persName><surname>Laion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities</title>
		<author>
			<persName><forename type="first">Mina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Abdel Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">2023a. Evaluating chatgpt&apos;s information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gexiang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.11633</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">2023b. Otter: A multi-modal model with in-context instruction tuning</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.03726</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">2023c. Camel: Communicative agents for</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasan</forename><surname>Abed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Kader Hammoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hani</forename><surname>Itani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Khizbullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>mind&quot; exploration of large scale language model society</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06355</idno>
		<title level="m">Limin Wang, and Yu Qiao. 2023e. Videochat: Chat-centric video understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06161</idno>
		<title level="m">Starcoder: may the source be with you! arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06259</idno>
		<title level="m">Self-alignment with instruction backtranslation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">2023h. Alpacaeval: An automatic evaluator of instruction-following models</title>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>GitHub repository</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05463</idno>
		<title level="m">Textbooks are all you need ii: phi-1.5 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge</title>
		<author>
			<persName><forename type="first">Yunxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruilong</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv, abs/2303.14070</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Cosgrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Christopher R'e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Acosta-Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frieda</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><forename type="middle">J</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Yuksekgonul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">S</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladri</forename><forename type="middle">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Koreeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faeze</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04770</idno>
		<title level="m">Wildbench: Benchmarking llms with challenging tasks from real users in the wild</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">2023a. The unlocking spell on base llms: Rethinking alignment via in-context learning</title>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Sclar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Beiwen Tian, and Xiang Ren. 2022. Unsupervised crosstask generalization via retrieval augmentation</title>
		<author>
			<persName><forename type="first">Kangmin</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.07937</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Truthfulqa: Measuring how models mimic human falsehoods</title>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">Weixiong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07240</idno>
		<title level="m">Pmc-clip: Contrastive language-image pretraining using biomedical documents</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Slake: A semanticallylabeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Ming</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1650" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">2023a. Logicot: Logical chain-of-thought instruction-tuning data collection with gpt-4</title>
		<author>
			<persName><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiji</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.12147</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.08485</idno>
		<title level="m">Visual instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06804</idno>
		<title level="m">What makes good in-context examples for gpt-3? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">Tiedong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang</forename><surname>Low</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14201</idno>
		<title level="m">Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoxiao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longtao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sekeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixing</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dajiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Quanzheng Li, and Tianming Liu. 2023c. Radiologygpt: A large language model for radiology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13688</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Wizardcoder: Empowering code large language models with evolinstruct</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Qi jie Gao, Qipeng Guo, and Xipeng Qiu. 2023. Full parameter fine-tuning for large language models with limited resources</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengxiao</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">SDEdit: Guided image synthesis and editing with stochastic differential equations</title>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08773</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Del Corro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweti</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Codas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarisse</forename><surname>Simoes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Razdaibiedina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.11045</idno>
	</analytic>
	<monogr>
		<title level="m">Teaching small language models how to reason</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01786</idno>
		<title level="m">Crosslingual generalization through multitask finetuning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02707</idno>
		<title level="m">Orca: Progressive learning from complex explanation traces of gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Album storytelling with iterative storyaware captioning and large language models</title>
		<author>
			<persName><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuliang</forename><surname>Yuan</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.12943. NousResearch. 2023. software: huggingface.co/NousResearch/Nous-Hermes-13b</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Introducing chatgpt. Blog post openai</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target=".com/blog/chatgpt" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno>ArXiv, abs/2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Vandenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.15848</idno>
		<title level="m">Clueweb22: 10 billion web documents with visual and semantic information</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03277</idno>
		<title level="m">Instruction tuning with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">Jing</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13257</idno>
		<title level="m">Controllable natural language generation with contrastive prefixes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><surname>Ring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Susannah Young, et al. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Coedit: Text editing by task-specific instruction tuning</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Vipul Raheja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyeop</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.09857</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Linguist: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleh</forename><surname>Soltan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Boese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08207</idno>
		<idno>ArXiv, abs/2211.05100</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Mathilde Bras, Younes Belkada</publisher>
			<pubPlace>Nathan Dahlberg, Nicholas Michio Broad</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and Thomas Wolf. 2022. Bloom: A 176b-parameter open-access multilingual language model</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno>ArXiv, abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Language models are multilingual chain-of-thought reasoners</title>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Srivats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.03057</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><surname>Saleh Soltan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Shankar Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haidar</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charith</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01448</idno>
		<title level="m">Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Adrià Garriga-Alonso, et al. 2022a. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Adrià Garriga-Alonso, et al. 2022b. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Maarten de Rijke, and Zhaochun Ren. 2023a. Answering ambiguous questions via iterative prompting</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengyi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2307.03897</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09719</idno>
		<title level="m">Pushing the limits of chatgpt on tasks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2023b</note>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08377</idno>
		<title level="m">Text classification via large language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">2022a. Challenging big-bench tasks and whether chain-of-thought can solve them</title>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09261</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">2022b. Challenging big-bench tasks and whether chain-ofthought can solve them</title>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Scharli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Huai Hsin Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.09261</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://crfm.stanford.edu/2023/03/13/alpaca.html" />
		<title level="m">Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ul</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Unifying language learning paradigms</note>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Lamda: Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Sun Tianxiang and Qiu Xipeng. 2023. Moss. Blog post txsun1997</title>
		<ptr target="github.io/blogs/moss.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Aur'elien Rodriguez</surname></persName>
		</author>
		<author>
			<persName><surname>Joulin</surname></persName>
		</author>
		<idno>ArXiv, abs/2302.13971</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023b. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Instruction tuning for few-shot aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Varia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishaloy</forename><surname>Halder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Vacareanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yassine</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Neha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smaranda</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.06629</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Gpt-re: In-context learning for relation extraction using large language models</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyue</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.02105</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Haochun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/SCIR-HI/Med-ChatGLM" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">2022a. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="23318" to="23340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongbin</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10428</idno>
		<title level="m">Gpt-ner: Named entity recognition via large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Instructuie: Multi-task instruction tuning for unified information extraction</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihua</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunsai</forename><surname>Du</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.08085</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Selfconsistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Huai Hsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ArXiv, abs/2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">How far can camels go? exploring the state of instruction tuning on open resources</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelsey</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Macmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
		<idno>ArXiv, abs/2306.04751</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">2022c. Self-instruct: Aligning language model with self generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10560</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Pegah</note>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks</title>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjana</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arut</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atharva</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Stap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><surname>Karamanolakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Haizhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishani</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirby</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krima</forename><surname>Kuznia</surname></persName>
		</author>
		<author>
			<persName><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Maitreya</forename><surname>Patel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kuntal</forename><surname>Kumar Pal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Moradshahi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mihir</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mirali</forename><surname>Purohit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neeraj</forename><surname>Varshney</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rohitha</forename><surname>Phani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pulkit</forename><surname>Kaza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravsehaj</forename><surname>Verma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rushang</forename><surname>Singh Puri</surname></persName>
		</editor>
		<editor>
			<persName><surname>Karia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Keyur</forename><surname>Shailaja</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Savan</forename><surname>Sampat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siddharth</forename><surname>Doshi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sujan</forename><surname>Deepak Mishra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sumanta</forename><surname>Reddy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tanay</forename><surname>Patro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xudong</forename><surname>Dixit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chitta</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yejin</forename><surname>Baral</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanna</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Hajishirzi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Khashabi</surname></persName>
		</editor>
		<meeting><address><addrLine>Wang, Swaroop Mishra, Pegah</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjana</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arut</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atharva</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><surname>Stap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07705</idno>
		<title level="m">Generalization via declarative instructions on 1600+ nlp tasks</title>
		<meeting><address><addrLine>Wang, Swaroop Mishra, Pegah</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2022e. Super-naturalinstructions</note>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks</title>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjana</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arut</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atharva</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><surname>Stap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Huai Hsin Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ArXiv, abs/2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">2023a. Zeroshot information extraction via chatting with chatgpt</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10205</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02120</idno>
		<title level="m">Yifeng Ding, and Lingming Zhang. 2023b. Magicoder: Source code is all you need</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Reframing human-ai collaboration for generating free-text explanations</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08674</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models</title>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Scholak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansong</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large language models to follow complex instructions</title>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">Baize: An open-source chat model with parameter-efficient tuning on self-chat data</title>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01196</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Baize: An open-source chat model with parameter-efficient tuning on self-chat data</title>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.01196</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">End-to-end slot alignment and recognition for crosslingual nlu</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Batool</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14353</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rulin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.11690</idno>
		<title level="m">Vision-flan: Scaling human-labeled tasks in visual instruction tuning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Multiinstruct: Improving multi-modal zeroshot learning via instruction tuning</title>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<idno>ArXiv, abs/2212.10773</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<author>
			<persName><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kabir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahir Hitesh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<ptr target="https://github.com/XueFuzhao/InstructionWild" />
		<title level="m">Instruction in the wild: A user-based instruction dataset</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13712</idno>
		<title level="m">Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023a. Harnessing the power of llms in practice: A survey on chatgpt and beyond</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06774</idno>
		<title level="m">Generating longer stories with recursive reprompting and revision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Boxing Chen, and Jun Xie. 2022b. Tailor: A prompt-based approach to attribute-based controlled text generation</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfeng</forename><surname>Xue</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.13362</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17421</idno>
		<title level="m">The dawn of lmms: Preliminary explorations with gpt-4v (ision)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.10601</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Lamm: Language-assisted multimodal instruction-tuning dataset, framework, and benchmark</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhenfei Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhelun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingning</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshui</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><surname>Shao</surname></persName>
		</author>
		<idno>ArXiv, abs/2306.06687</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<author>
			<persName><forename type="first">Zhaojian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishujie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiufeng</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.14187</idno>
		<title level="m">Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Yulan-chat: An opensource bilingual chatbot</title>
		<ptr target="https://github.com/RUC-GSAI/YuLan-Chat" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>YuLan-Chat-Team</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Short Papers), ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">2023a. Chinese open instruction generalist: A preliminary release</title>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoqun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Fen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.07987</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">2023b. Videollama: An instruction-tuned audio-visual language model for video understanding</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02858</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>ArXiv, abs/2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">2023c. Pmc-vqa: Visual instruction tuning for medical visual question answering</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixiong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.10415</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07845</idno>
		<title level="m">Building mega-scale vision dataset continually with humanmachine synergy</title>
		<meeting><address><addrLine>Bamboo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Multi-task instruction tuning of llama for specific scenarios: A preliminary study on writing assistance</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.13225</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wayne Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A survey of large language models</note>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>wildchat: 570k chatgpt interaction logs in the wild</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="46595" to="46623" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.11206</idno>
		<title level="m">Less is more for alignment</title>
		<meeting><address><addrLine>Lima</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujoy</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07911</idno>
		<title level="m">Instruction-following evaluation for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">2023a. Starling-7b: Improving llm helpfulness &amp; harmlessness with rlaif</title>
		<author>
			<persName><forename type="first">Banghua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<title level="m" type="main">2023b. Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
