<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TOWARDS VARIATIONAL FLOW MATCHING ON GENERAL GEOMETRIES</title>
				<funder>
					<orgName type="full">Finnish Center for Artificial Intelligence</orgName>
					<orgName type="abbreviated">FCAI</orgName>
				</funder>
				<funder ref="#_hjTNhuD">
					<orgName type="full">Bosch Center for Artificial Intelligence</orgName>
				</funder>
				<funder ref="#_MA4Gkr8">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-18">18 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Olga</forename><surname>Zaghen</surname></persName>
							<email>o.zaghen@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alison</forename><surname>Pouplin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TOWARDS VARIATIONAL FLOW MATCHING ON GENERAL GEOMETRIES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-18">18 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">D327555E02910EB2FDABDF47D968D8AA</idno>
					<idno type="arXiv">arXiv:2502.12981v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an extension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian distributions for generative modeling on structured manifolds. We derive a variational objective for probability flows on manifolds with closed-form geodesics, making RG-VFM comparable -though fundamentally different to Riemannian Flow Matching (RFM) in this geometric setting. Experiments on a checkerboard dataset wrapped on the sphere demonstrate that RG-VFM captures geometric structure more effectively than Euclidean VFM and baseline methods, establishing it as a robust framework for manifold-aware generative modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative modeling has become a fundamental task in machine learning, with different frameworks achieving remarkable success across various data modalities <ref type="bibr" target="#b10">(Ramesh et al., 2022;</ref><ref type="bibr" target="#b11">Rombach et al., 2022)</ref>. While diffusion models have shown exceptional effectiveness <ref type="bibr" target="#b7">(Ho et al., 2020;</ref><ref type="bibr" target="#b13">Song et al., 2020)</ref>, they rely on constrained probability paths requiring specialized techniques. Continuous normalizing flows (CNFs) (Chen et al., 2018) offer greater flexibility (Song et al., 2021), but remain computationally expensive due to solving high-dimensional ODEs during training and sampling <ref type="bibr" target="#b0">(Ben-Hamu et al., 2022;</ref><ref type="bibr" target="#b12">Rozen et al., 2021;</ref><ref type="bibr" target="#b6">Grathwohl et al., 2019)</ref>. Flow Matching (FM) <ref type="bibr" target="#b8">(Lipman et al., 2023)</ref> addresses these challenges by expressing the transport field through conditional fields, enabling simulation-free learning of ODEs through assumed dynamics towards specific endpoints.</p><p>Recent developments have extended flow matching in two key directions: Variational Flow Matching (VFM) <ref type="bibr" target="#b3">(Eijkelboom et al., 2024)</ref> reframes the problem as posterior inference over trajectories, providing a probabilistic perspective with flexible modeling choices, while other work has demonstrated flow matching's potential on geometric structures <ref type="bibr" target="#b1">(Chen &amp; Lipman, 2024)</ref>. This geometric extension is particularly relevant for data on non-Euclidean spaces, where underlying geometry crucially influences probability paths.</p><p>In this paper, we develop Riemannian Gaussian VFM, bridging these directions by extending VFM to Riemannian manifolds with closed-form metrics. We derive a variational objective for Gaussian distributions in these spaces, establishing a geometric extension of VFM, comparable yet fundamentally different from RFM. Through experiments on a spherical checkerboard dataset, we demonstrate that Riemannian VFM effectively leverages manifold structure, outperforming geometry-unaware methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Flow Matching. Modern generative modeling interprets sampling from a target distribution p 1 as transporting a base distribution p 0 by learning dynamics. Typically, p 0 is a standard Gaussian, and the transformation follows a time-dependent mapping φ t : r0, 1s ˆRD Ñ R D where φ 0 is the identity and φ 1 pushes p 0 onto p 1 , e.g. normalizing flows (Chen et al., 2018)  use an ODE governed by some time-dependent velocity field u t . Though likelihood training is possible through the change of variables formula, solving an ODE during training is expensive. Flow Matching (FM) (Lipman</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">et al., 2023)</ref> <p>bypasses this by directly learning the velocity field:</p><formula xml:id="formula_0">L FM pθq " E t,x " ||u t pxq ´vθ t pxq|| 2 ‰ .<label>(1)</label></formula><p>This is made computationally feasible by reformulating u t with a conditional velocity field (i.e. assumed dynamics towards a given x 1 ), giving rise to Conditional Flow Matching (CFM):</p><formula xml:id="formula_1">L CFM pθq " E t,x1,x " ||u t px | x 1 q ´vθ t pxq|| 2 ‰ . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Minimizing eq. ( <ref type="formula" target="#formula_1">2</ref>) provides an unbiased estimate of ∇ θ L FM , allowing efficient per-sample training.</p><p>Riemannian Flow Matching. Riemannian Flow Matching (RFM) (Chen &amp; Lipman, 2024) extends FM to Riemannian manifolds. Given a smooth Riemannian manifold M with closed-form geodesics and metric g, RFM learns a vector field v t :</p><formula xml:id="formula_3">L RFM pθq " E t,x1,x " › › v θ t pxq ´log x px 1 q{p1 ´tq › › 2 g ı ,<label>(3)</label></formula><p>with log x px 1 q denoting the Riemannian log map, which returns the initial velocity vector of the geodesic connecting x to x 1 (more details on Riemannian manifolds are in appendix A.1). Unlike Euclidean Flow Matching, RFM respects the curvature and geodesics of the underlying space M.</p><p>Through geodesic or spectral distances, it enables simulation-free training when manifold operations are available, and can utilize approximate distances when closed-form geodesics are intractable, maintaining theoretical guarantees while enabling efficient generative modeling.</p><p>Variational Flow Matching. Variational Flow Matching (VFM) <ref type="bibr" target="#b3">(Eijkelboom et al., 2024)</ref> reformulates FM by introducing a variational distribution q θ t px 1 | xq to approximate the unknown posterior p t px 1 | xq, where the learned velocity v θ t is expressed as the expectation of the condition velocity under this variational approximation over trajectories. Then, the VFM objective is to minimize the KL divergence between joint distributions, i.e.:</p><formula xml:id="formula_4">L VFM pθq " E t " KL `pt px 1 , xq || q θ t px 1 , xq ˘‰ " ´Et,x1,x " log q θ t px 1 | xq ‰ `const.<label>(4)</label></formula><p>When u t px | x 1 q is linear in x 1 -e.g. a straight-line interpolation -the expectation depends only on marginal distributions, implying this objective reduces to a series of D univariate tasks:</p><formula xml:id="formula_5">L VFM pθq " ´Et,x1,x « D ÿ d"1 log q θ t px d 1 | xq ff , e.g. L VFM pθq " E t,x1,x " }µ θ t pxq ´x1 } 2 ‰ ,<label>(5)</label></formula><p>if q θ t is Gaussian, relating VFM directly back to FM (see <ref type="bibr" target="#b3">Eijkelboom et al. (2024)</ref> for details). A key feature of VFM is its flexibility in choosing q θ t , as different choices of q θ t allow adaptation to various geometries and data types, improving efficiency and expressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RIEMANNIAN GAUSSIAN VARIATIONAL FLOW MATCHING</head><p>The geometric generalization of the VFM framework stems from the observation that learning the posterior probability p t px 1 | xq implicitly encodes the geometry of the distribution's support. For example, in CatFlow <ref type="bibr" target="#b3">(Eijkelboom et al., 2024)</ref>, defining q θ t px 1 | xq as a categorical distribution ensures that the velocities point towards the probability simplex. This raises the question of whether other geometric information about the support of p 1 can be similarly encoded in q θ t px 1 | xq. To investigate this, we consider the case where p t px 1 | xq is defined as a Gaussian distribution, but with its support on the manifold M :" supppp 1 q instead of the ambient Euclidean space. In this setting, the Riemannian Gaussian distribution naturally arises as a generalization of the Gaussian to a Riemannian manifold. We will refer to CFM and RFM as vanilla models, minimizing velocity differences, while VFM and the proposed model are variational, matching endpoints in their loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MODEL DEFINITION</head><p>Riemannian Gaussian. Let M be a Riemannian manifold with metric g. The Riemannian Gaussian (RG) distribution <ref type="bibr" target="#b9">(Pennec, 2006)</ref> is defined as where z, µ P M (with µ as the mean), σ ą 0 is a scale parameter, and dist g pz, µq denotes the geodesic distance determined by g. The constant C depends on both z and µ, and it normalizes the distribution over M. A more detailed geometric explanation can be found in Appendix A.2.</p><formula xml:id="formula_6">N Riem pz | σ, µq " 1 C exp ˆ´dist g pz, µq 2 2σ 2 ˙,<label>(6)</label></formula><p>RG-VFM objective. We define the Riemannian Gaussian-VFM (RG-VFM) objective by using the Riemannian Gaussian as our variational approximation, i.e.</p><formula xml:id="formula_7">L RG-VFM pθq " ´Et,x1,x " log N Riem px 1 | µ θ t pxq, σ t pxqq ‰ .<label>(7)</label></formula><p>In the Gaussian VFM case, this setting reduces to a straightforward MSE optimization, so it is natural to wonder whether a similar simplification holds here. In fact, such a simplification exists under two assumptions: (1) the manifold is homogeneous -that is, any point can be transformed into any other by a distance-preserving symmetry (a formal definition is provided in appendix A.1); and</p><p>(2) we have access to a closed-form expression for its geodesics. Formally, the following holds (see appendix B.1 for details): Proposition 3.1. Let M be a homogeneous manifold with closed-form geodesics. Then, the VFM objective reduces to</p><formula xml:id="formula_8">L RG-VFM pθq " E t,x1,x " || log x1 pµ θ t pxqq|| 2 g ‰ . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>Minimizing this loss is equivalent to computing the Fréchet mean of the distribution, that is, compute µ ‹ " arg min µ θ PM ş M dist g px 1 , µ θ q 2 dx 1 , i.e. the point µ θ that minimizes the expected squared geodesic distance to the data points <ref type="bibr" target="#b4">(Fréchet, 1948)</ref>. Intuitively, this can be viewed as a generalization of the mean squared error from the Euclidean setting to a Riemannian framework.</p><p>The RG-VFM objective (eq. 8) minimizes the geodesic distance on M between target and predicted endpoints, so it only needs to learn the local geometry of M around p 1 . In contrast, vanilla RFM requires that supppp 0 q lie entirely on M because its vector fields depend on the manifold's intrinsic geometry; off-manifold points can lead to unstable estimates of tangents and vector fields.</p><p>For this study, we consider two versions: (1) RG-VFM-R 3 , where p 0 is Euclidean, M Ĺ supppp 0 q, and we use linear interpolation for conditional velocities, and (2) RG-VFM-M, where supppp 0 q " M, we use geodesic interpolation, and define velocities on the tangent spaces to M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HOW DOES RG-VFM FIT IN THE EXISTING FLOW MATCHING FRAMEWORK?</head><p>Figure <ref type="figure" target="#fig_0">1</ref> (left) illustrates how RG-VFM fits within the framework of related FM models. In VFM, a probabilistic generalization of CFM is obtained by making the posterior distribution explicit and customizable, obtaining standard CFM under the choice of a specific Gaussian (see <ref type="bibr" target="#b3">Eijkelboom et al. (2024)</ref>). In contrast, RFM serves as a geometric generalization of CFM, where the model and its objective take into account the intrinsic properties and metric of the underlying Riemannian manifold. The same happens for the variational models: VFM with a Gaussian posterior is a particular instance of RG-VFM when the geometry is Euclidean. In Euclidean space, } log x1 pµ θ t pxqq} 2 g reduces to }µ θ t pxq ´x1 } 2 2 , thereby recovering the VFM objective. A further comparison can be made between the simplified version of RFM and RG-VFM-M, where M is a homogeneous manifold with closed-form geodesics. The variational model (RG-VFM) is not a direct generalization of vanilla RFM because, unlike in Euclidean space, tangent spaces at different points on a manifold do not coincide. This difference is reflected in the models' outputs (fig. <ref type="figure" target="#fig_0">1</ref>): vanilla models predict velocity fields, which are integrated as ODEs to construct flows, whereas variational models predict endpoint distributions, ideally aligning with the target distribution p 1 .</p><p>In Euclidean space, the difference between two vectors starting at x and pointing to different endpoints is simply the vector between those endpoints, leading to identical L 2 terms in the objectives, i.e. }µ θ t pxq ´x1 } 2 2 for VFM and }u t px | x 1 q ´vθ t pxq} 2 for CFM. However, since T x M ‰ T x1 M in general, in their geometric counterparts this equivalence no longer holds: indeed, the difference vector in the RFM objective, v θ t pxq ´log x px1q p1´tq , is in T x M, while log x1 pµ θ t pxqq is in T x1 M. This fundamental distinction separates RG-VFM from RFM. More details are in appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Inspired by the planar checkerboard benchmark in generative modeling <ref type="bibr" target="#b5">(Grathwohl et al., 2018)</ref>, we introduce a spherical checkerboard distribution as our target p 1 , whose support is S 2 Ă R 3 . The noisy distribution p 0 is defined differently for each model: for CFM, VFM, and RG-VFM-R 3 , p 0 is the uniform distribution on r´1, 1s 3 Ă R 3 , while for RG-VFM-M and RFM, it is the uniform distribution on S 2 . We conduct two sets of experiments: we (1) compare Euclidean to geometric models in capturing the correct geometry -assessed by the norms of generated samples (ideally unit norm, since the points should lie on the sphere) -and (2) evaluate vanilla versus variational models in reproducing the target distribution. Figure <ref type="figure" target="#fig_1">2</ref> displays the generated distributions alongside the ground truth. All the experimental details and results are provided in appendix C.</p><p>Comparing the Euclidean and geometric models, the norm statistics reveals key differences. The Euclidean models (CFM and VFM) show slight norm deviation from the unit sphere (CFM: mean = 1.00, std = 0.094; VFM: mean = 1.00, std = 0.021), while the geometric models maintain a nearperfect unit norm. The geometric extensions effectively capture the underlying geometry well.</p><p>Comparing vanilla and variational models, we observe that vanilla models produce more blurred distributions, whereas variational models better patches contrasts. As shown in fig. <ref type="figure" target="#fig_1">2</ref>, RG-VFM-R 3 and RG-VFM-M exhibit the best visual performance, with minimal differences. This is likely because RG-VFM's loss functions is both geometrically informed and focused on minimizing the distance between predictions and distribution endpoints rather than matching intermediate flow velocities. More interestingly, standard VFM also demonstrates strong performance, which we speculate is due to its emphasis on endpoint minimization. In essence, emphasizing endpoint accuracy enables variational models to capture the fine details of the target distribution's shape, and the additional geometric awareness of RG-VFM further enhances the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduced Riemannian Gaussian Variational Flow Matching (RG-VFM), extending VFM to structured manifolds through Riemannian Gaussian distributions. Our approach unifies RFM and VFM under a common probabilistic framework, with experiments on spherical data demonstrating superior performance over Euclidean-based methods. These results establish RG-VFM as a promising framework for modeling distributions on complex geometries, with applications in molecular modeling and physics-informed generative tasks.</p><p>Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415-1428, 2021.</p><p>A GEOMETRIC BACKGROUND</p><p>A.1 RIEMANNIAN MANIFOLDS</p><p>In this section, we provide a concise review of Riemannian manifolds in the setting of complete, connected, and smooth manifolds. Let M be such a manifold endowed with a Riemannian metric g. At each point x P M, the tangent space is denoted by T x M, and g induces an inner product on each T x M, which we write as xu, vy g for u, v P T x M. By collecting all tangent spaces across M, we obtain the tangent bundle:</p><formula xml:id="formula_10">T M " ď xPM txu ˆTx M.</formula><p>We will also consider time-dependent vector fields on M. Specifically, let tu t u tPr0,1s be a family of smooth vector fields, with each u t mapping a point x P M to an element u t pxq P T x M. The operator div g pu t q denotes the Riemannian divergence of u t with respect to the spatial variable x.</p><p>Furthermore, the volume form on M induced by g is denoted by dM z . For any real-valued function f : M Ñ R, we write ż M f pxq dM z to denote its integral over M.</p><p>Homogeneous Manifold. A Riemannian manifold M is homogeneous if its isometry group acts transitively on M, i.e., for any two points x, y P M, there exists an isometry f : M Ñ M such that f pxq " y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 RIEMANNIAN GAUSSIAN DISTRIBUTIONS</head><p>We describe the construction of the Riemannian Gaussian (RG) distribution, which generalizes the familiar Gaussian distribution to the setting of a Riemannian manifold. The definition is inspired from <ref type="bibr" target="#b9">Pennec (2006)</ref>.</p><p>Riemannian Gaussian. Let M be a Riemannian manifold endowed with the metric tensor g. The RG distribution is defined by</p><formula xml:id="formula_11">N Riem pz | σ, µq " 1 C exp ´´dist g pz, µq 2 2σ 2 ¯,</formula><p>where z P M is a point on the manifold, µ P M plays the role of the mean, and σ ą 0 is a scale parameter controlling the spread of the distribution. Here, dist g pz, µq denotes the geodesic distance between z and µ as determined by the metric g, and C is a normalization constant chosen so that the total probability integrates to 1 over M:</p><formula xml:id="formula_12">C " ż M exp ´´dist g pz, µq 2 2σ 2 ¯dM z .</formula><p>The measure dM z represents the Riemannian volume element, which in local coordinates takes the form dM z " a det gpzq dz, with dz being the standard Lebesgue measure in the coordinate chart and gpzq is the Riemannian metric tensor at the point z. This formulation ensures that the probability density is adapted to the geometric structure of the manifold.</p><p>Observation. In the special case where M " R d and the metric is Euclidean (i.e., gpzq " I), the geodesic distance reduces to the usual Euclidean distance, and the RG distribution becomes the standard multivariate Gaussian with covariance matrix σ 2 I. On more general manifolds, however, the curvature and topology are taken into account through the geodesic distance and the volume element, leading to a natural extension of the Gaussian concept. This construction can be applied to spaces such as hyperbolic manifolds, where one can define the distribution in the tangent space at a point µ and then use the exponential map to project it onto the manifold.</p><p>Comparison to vMF. A closely related distribution is the von Mises-Fisher (vMF) distribution, which is traditionally defined on the sphere S n by vMFpz | µ, κq9 exp `κ xz, µy ˘, with µ P S n and x¨, ¨y denoting the standard dot product. The vMF distribution is based on the notion of directional data and an inner product structure that measures alignment. In contrast, the RG distribution is inherently tied to the Riemannian metric, making it applicable to a much wider class of manifolds. Generalizing the idea behind the vMF distribution to other geometries often requires embedding the manifold into a larger ambient space and defining a suitable bilinear form (such as the Minkowski inner product in hyperbolic geometry). In this sense, the RG approach offers a more natural and geometrically intrinsic formulation.</p><p>In summary, the Riemannian Gaussian distribution is defined in terms of the geodesic distance and the corresponding volume element, and it adapts to the underlying geometry of any Riemannian manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B R-VFM AND LINK WITH RFM</head><p>B.1 DETAILED DERIVATION OF RG-VFM OBJECTIVE Proposition 3.1. Let M be a homogeneous manifold with closed-form geodesics. Then, the VFM objective reduces to</p><formula xml:id="formula_13">L RG-VFM pθq " E t,x1,x " || log x1 pµ θ t pxqq|| 2 g ‰ . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>Proof. The objective of VFM is defined as</p><formula xml:id="formula_15">L VFM pθq " ´Et,x1,x " log q θ t px 1 |xq ‰ .</formula><p>We define the objective function of RG-VFM by setting the posterior probability as the Riemannian Gaussian, i.e.,</p><formula xml:id="formula_16">q θ t px 1 |xq " N Riem px 1 | µ θ t pxq, σpxqq, so that L RG-VFM pθq " ´Et,x1,x " log N Riem px 1 | µ θ t pxq, σpxqq ‰ .</formula><p>More explicitly, we have</p><formula xml:id="formula_17">L RG-VFM pθq " ´Et,x1,x " log q θ t px 1 |xq ‰ " ´Et,x1,x " log N Riem px 1 | µ θ t pxq, σpxqq ‰ " ´Et,x1,x " log ˆ1 Cpµ θ t pxqq exp ˆ´dist g px 1 , µ θ t pxqq 2 2σpxq 2 ˙˙ȷ " ´Et,x1,x " log ˆ1 Cpµ θ t pxqq ˙´dist g px 1 , µ θ t pxqq 2 2σpxq 2 ȷ " ´Et,x1,x " log ˆ1 Cpµ θ t pxqq ˙ȷ `Et,x1,x " dist g px 1 , µ θ t pxqq 2 2σpxq 2 ȷ ,</formula><p>where dist g pq denotes the geodesic distance induced by the Riemannian metric g.</p><p>Without any regularity assumptions on M, no further simplification is possible. However, under the following assumptions the objective becomes more tractable:</p><p>1. Homogeneity: If the manifold pM, gq is homogeneous, the normalization constant</p><formula xml:id="formula_18">C " ż M exp ˆ´dist g pz, µq 2 2σ 2 ˙dM z</formula><p>is independent of the mean µ. Hence, defining</p><formula xml:id="formula_19">K :" ´Et,x1,x " log ˆ1 Cpµ θ t pxqq ˙ȷ ,</formula><p>which is constant with respect to θ, we obtain</p><formula xml:id="formula_20">L RG-VFM pθq " K `Et,x1,x " dist g px 1 , µ θ t pxqq 2 2σpxq 2 ȷ .</formula><p>Since K is a constant that is independent of the model's parameters θ, the minimization objective becomes</p><formula xml:id="formula_21">L RG-VFM pθq " E t,x1,x " dist g px 1 , µ θ t pxqq 2 2σpxq 2 ȷ .</formula><p>2. Closed-form Geodesics: If the geometry allows closed-form expressions for geodesics, namely</p><formula xml:id="formula_22">γptq " exp x ´t ¨log x pyq ¯,</formula><p>then the geodesic distance between two points is given by: dist g pz, µq " } log z pµq} g .</p><p>In this setting, we can write dist g px 1 , µ θ t pxqq 2 " } log x1 pµ θ t pxqq} 2 g , so that the objective becomes</p><formula xml:id="formula_23">L RG-VFM pθq " ´Et,x1,x " log ˆ1 Cpµ θ t pxqq ˙ȷ `Et,x1,x " 1 2σpxq 2 } log x1 pµ θ t pxqq} 2 g ȷ .</formula><p>3. Combined Assumptions: If both conditions hold, the objective simplifies to</p><formula xml:id="formula_24">L RG-VFM pθq " E t,x1,x " 1 2σpxq 2 } log x1 pµ θ t pxqq} 2 g ȷ .</formula><p>If we further assume that σpxq is constant, this reduces to</p><formula xml:id="formula_25">L RG-VFM pθq " E t,x1,x " } log x1 pµ θ t pxqq} 2 g ‰ .</formula><p>Examples of simple geometries. A homogeneous manifold does not necessarily imply that geodesics admit closed-form expressions. Conversely, the simple geometries with closed-form geodesics considered in the RFM setting-such as hyperspheres S n , hyperbolic spaces H n , flat tori T n " r0, 2πs n , and the space of SPD matrices S d with the affine-invariant metric-are homogeneous. Thus, when restricting to these geometries for comparison with RFM, we are in the combined case.</p><p>Euclidean space. In the Euclidean case (which also falls into the combined case), the objective simplifies further to L RG-VFM pθq " E t,x1,x " }µ θ t pxq ´x1 } 2 ‰ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 RG-VFM VS RFM ON HOMOGENEOUS SPACES WITH CLOSED-FORM GEODESICS</head><p>The objective of RG-VFM is defined as</p><formula xml:id="formula_26">L RG-VFM pθq " E t,x1,x " } log x1 pµ θ t pxqq} 2 g ‰ ,</formula><p>while the objective of RFM, in the case of closed-form geodesics, is given by</p><formula xml:id="formula_27">L RFM pθq " E t,x1,x " › › v θ t pxq ´log x px 1 q{p1 ´tq › › 2 g ı ,</formula><p>with g being the metric tensor at x " p t px|x 1 q.</p><p>Ignoring multiplicative constants that depend only on t and x, comparing the two losses reduces to comparing the quantities } log x1 pµ θ t pxqq} 2 g and }v θ t pxq ´log x px 1 q} 2 g .</p><p>Euclidean space. In Euclidean space R n , the tangent space at each point is naturally identified with R n . In this setting, log x1 pµ θ t pxqq " µ θ t pxq ´x1 and log x px 1 q " x 1 ´x.</p><p>Notice that µ θ t pxq ´x1 " µ θ t pxq ´x `x ´x1 " pµ θ t pxq ´xq ´px 1 ´xq " pµ θ t pxq ´xq ´log x px 1 q.</p><p>If we define (ignoring multiplicative constants such as 1{p1 ´tq) v θ t pxq " log x pµ θ t pxqq " µ θ t pxq ´x, then it follows that log x1 pµ θ t pxqq " log x pµ θ t pxqq ´log x px 1 q, implying } log x1 pµ θ t pxqq} 2 g " }v θ t pxq ´log x px 1 q} 2 g . Thus, L RG-VFM pθq and L RFM pθq are equivalent up to an additive constant. This result is consistent with the known equivalence between L VFM pθq and L CFM pθq.</p><p>General geometries. In non-Euclidean spaces, however, the quantities } log x1 pµ θ t pxqq} 2 g and }v θ t pxq ´log x px 1 q} 2 g are not necessarily equal. This is because log x1 pµ θ t pxqq is a vector in T x1 M, while log x pµ θ t pxqq ĺog x px 1 q lies in T x M, and in general T x1 M ‰ T x M. Establishing a relation between these vectors is not straightforward and can be illustrated by comparing the law of cosines in Euclidean, hyperbolic spaces, and on hyperspheres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTS</head><p>In this section, we present further results from the experiments described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 EXPERIMENTAL SETUP</head><p>In all experiments, the target distribution p 1 is the spherical checkerboard, so its support is S 2 . The noisy distribution p 0 varies by model: for CFM, VFM, and RG-VFM-R 3 , p 0 is the uniform distribution on r´1, 1s 3 Ă R 3 , while for RG-VFM-M and RFM, p 0 is the uniform distribution on S 2 . In every case, we train a five-layer MLP with 64/128 hidden features for 3000 epochs, that we use to generate 5000 samples using a Runge-Kutta ODE solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 RESULTS</head><p>Figure <ref type="figure" target="#fig_3">3</ref> illustrates the generative flow trajectories over time, from the initial distribution p 0 to the generated distribution at t " 1.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> displays the generated distributions unwrapped onto a flat surface for easier visualization and comparison. These results visually confirm the observations presented in Section 4.</p><p>Finally, Figure <ref type="figure" target="#fig_7">5</ref> shows histograms of the norm values of the generated samples. As discussed in Section 4, this metric differentiates the Euclidean models (CFM and VFM) from the others. Ideally, points should have a norm of 1, since they lie on the sphere. However, because the Euclidean models lack explicit geometric information, their points deviate slightly from a norm of 1-with CFM exhibiting a larger divergence. In contrast, the geometric models consistently generate points that lie almost exactly on the sphere.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Overview of the main Flow Matching models relevant to our proposed approach, illustrating their relationships. Right: Visualization of each model's predictions (color-coded to match the corresponding model on the left) for a target distribution p 1 supported on the sphere S 1 .</figDesc><graphic coords="3,127.80,81.86,356.40,117.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of the spherical checkerboard distribution generated with CFM, VFM, RFM and our methods RG-VFM-R 3 and RG-VFM-M.</figDesc><graphic coords="4,108.00,81.86,396.00,73.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>(a) Model: CFM; supppp0q :" R 3 , p0: uniform distribution on r´1, 1s 3 Ă R 3 . (b) Model: VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (c) Model: RG-VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (d) Model: RFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .(e) Model: RG-VFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Flow trajectories of 5,000 samples, initially drawn from the noisy distribution p 0 at t " 0, evolving to reach their final configuration by t " 1.</figDesc><graphic coords="11,70.99,538.63,470.02,75.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>(a) Model: CFM; supppp0q :" R 3 , p0: uniform distribution on r´1, 1s 3 Ă R 3 . (b) Model: VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (c) Model: RG-VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (d) Model: RFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .(e) Model: RG-VFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sample distributions generated by different models (representing the flow configuration at t " 1) unwrapped from S 2 to R 2 for improved visualization. The true checkerboard distribution is shown in gray in the background.</figDesc><graphic coords="12,210.96,474.50,190.08,142.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>(a) Model: CFM; supppp0q :" R 3 , p0: uniform distribution on r´1, 1s 3 Ă R 3 . (b) Model: VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (c) Model: RG-VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (d) Model: RFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .(e) Model: RG-VFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Histogram of the norm values of the 5000 samples describing the generated distribution.</figDesc><graphic coords="13,210.96,485.46,190.08,142.56" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">ACKNOWLEDGMENTS</head><p>This project was funded by the <rs type="projectName">CaLiForNIA</rs> project (<rs type="projectName">Marie Skłodowska-Curie Actions Doctoral Network 2022</rs>), by the <rs type="funder">Bosch Center for Artificial Intelligence</rs> and by the <rs type="funder">Finnish Center for Artificial Intelligence (FCAI)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_MA4Gkr8">
					<orgName type="project" subtype="full">CaLiForNIA</orgName>
				</org>
				<org type="funded-project" xml:id="_hjTNhuD">
					<orgName type="project" subtype="full">Marie Skłodowska-Curie Actions Doctoral Network 2022</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching normalizing flows and probability paths on manifolds</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flow matching on general geometries</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Variational flow matching for graph generation</title>
		<author>
			<persName><forename type="first">Floor</forename><surname>Eijkelboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Bartosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Andersson Naesseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04843</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Les éléments aléatoires de nature quelconque dans un espace distancié</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Fréchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales de l&apos;institut Henri Poincaré</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="215" to="310" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable reversible generative models with free-form continuous dynamics</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flow matching for generative modeling</title>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="127" to="154" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moser flow: Divergencebased generative modeling on manifolds</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17669" to="17680" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
