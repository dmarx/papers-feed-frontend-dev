# TOWARDS VARIATIONAL FLOW MATCHING ON GENERAL GEOMETRIES

## Abstract

## 

We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an extension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian distributions for generative modeling on structured manifolds. We derive a variational objective for probability flows on manifolds with closed-form geodesics, making RG-VFM comparable -though fundamentally different to Riemannian Flow Matching (RFM) in this geometric setting. Experiments on a checkerboard dataset wrapped on the sphere demonstrate that RG-VFM captures geometric structure more effectively than Euclidean VFM and baseline methods, establishing it as a robust framework for manifold-aware generative modeling.

## INTRODUCTION

Generative modeling has become a fundamental task in machine learning, with different frameworks achieving remarkable success across various data modalities [(Ramesh et al., 2022;](#b10)[Rombach et al., 2022)](#b11). While diffusion models have shown exceptional effectiveness [(Ho et al., 2020;](#b7)[Song et al., 2020)](#b13), they rely on constrained probability paths requiring specialized techniques. Continuous normalizing flows (CNFs) (Chen et al., 2018) offer greater flexibility (Song et al., 2021), but remain computationally expensive due to solving high-dimensional ODEs during training and sampling [(Ben-Hamu et al., 2022;](#b0)[Rozen et al., 2021;](#b12)[Grathwohl et al., 2019)](#b6). Flow Matching (FM) [(Lipman et al., 2023)](#b8) addresses these challenges by expressing the transport field through conditional fields, enabling simulation-free learning of ODEs through assumed dynamics towards specific endpoints.

Recent developments have extended flow matching in two key directions: Variational Flow Matching (VFM) [(Eijkelboom et al., 2024)](#b3) reframes the problem as posterior inference over trajectories, providing a probabilistic perspective with flexible modeling choices, while other work has demonstrated flow matching's potential on geometric structures [(Chen & Lipman, 2024)](#b1). This geometric extension is particularly relevant for data on non-Euclidean spaces, where underlying geometry crucially influences probability paths.

In this paper, we develop Riemannian Gaussian VFM, bridging these directions by extending VFM to Riemannian manifolds with closed-form metrics. We derive a variational objective for Gaussian distributions in these spaces, establishing a geometric extension of VFM, comparable yet fundamentally different from RFM. Through experiments on a spherical checkerboard dataset, we demonstrate that Riemannian VFM effectively leverages manifold structure, outperforming geometry-unaware methods.

## BACKGROUND

Flow Matching. Modern generative modeling interprets sampling from a target distribution p 1 as transporting a base distribution p 0 by learning dynamics. Typically, p 0 is a standard Gaussian, and the transformation follows a time-dependent mapping φ t : r0, 1s ˆRD Ñ R D where φ 0 is the identity and φ 1 pushes p 0 onto p 1 , e.g. normalizing flows (Chen et al., 2018)  use an ODE governed by some time-dependent velocity field u t . Though likelihood training is possible through the change of variables formula, solving an ODE during training is expensive. Flow Matching (FM) (Lipman

## 

[et al., 2023)](#)bypasses this by directly learning the velocity field:

$L FM pθq " E t,x " ||u t pxq ´vθ t pxq|| 2 ‰ .(1)$This is made computationally feasible by reformulating u t with a conditional velocity field (i.e. assumed dynamics towards a given x 1 ), giving rise to Conditional Flow Matching (CFM):

$L CFM pθq " E t,x1,x " ||u t px | x 1 q ´vθ t pxq|| 2 ‰ . (2$$)$Minimizing eq. ( [2](#formula_1)) provides an unbiased estimate of ∇ θ L FM , allowing efficient per-sample training.

Riemannian Flow Matching. Riemannian Flow Matching (RFM) (Chen & Lipman, 2024) extends FM to Riemannian manifolds. Given a smooth Riemannian manifold M with closed-form geodesics and metric g, RFM learns a vector field v t :

$L RFM pθq " E t,x1,x " › › v θ t pxq ´log x px 1 q{p1 ´tq › › 2 g ı ,(3)$with log x px 1 q denoting the Riemannian log map, which returns the initial velocity vector of the geodesic connecting x to x 1 (more details on Riemannian manifolds are in appendix A.1). Unlike Euclidean Flow Matching, RFM respects the curvature and geodesics of the underlying space M.

Through geodesic or spectral distances, it enables simulation-free training when manifold operations are available, and can utilize approximate distances when closed-form geodesics are intractable, maintaining theoretical guarantees while enabling efficient generative modeling.

Variational Flow Matching. Variational Flow Matching (VFM) [(Eijkelboom et al., 2024)](#b3) reformulates FM by introducing a variational distribution q θ t px 1 | xq to approximate the unknown posterior p t px 1 | xq, where the learned velocity v θ t is expressed as the expectation of the condition velocity under this variational approximation over trajectories. Then, the VFM objective is to minimize the KL divergence between joint distributions, i.e.:

$L VFM pθq " E t " KL `pt px 1 , xq || q θ t px 1 , xq ˘‰ " ´Et,x1,x " log q θ t px 1 | xq ‰ `const.(4)$When u t px | x 1 q is linear in x 1 -e.g. a straight-line interpolation -the expectation depends only on marginal distributions, implying this objective reduces to a series of D univariate tasks:

$L VFM pθq " ´Et,x1,x « D ÿ d"1 log q θ t px d 1 | xq ff , e.g. L VFM pθq " E t,x1,x " }µ θ t pxq ´x1 } 2 ‰ ,(5)$if q θ t is Gaussian, relating VFM directly back to FM (see [Eijkelboom et al. (2024)](#b3) for details). A key feature of VFM is its flexibility in choosing q θ t , as different choices of q θ t allow adaptation to various geometries and data types, improving efficiency and expressiveness.

## RIEMANNIAN GAUSSIAN VARIATIONAL FLOW MATCHING

The geometric generalization of the VFM framework stems from the observation that learning the posterior probability p t px 1 | xq implicitly encodes the geometry of the distribution's support. For example, in CatFlow [(Eijkelboom et al., 2024)](#b3), defining q θ t px 1 | xq as a categorical distribution ensures that the velocities point towards the probability simplex. This raises the question of whether other geometric information about the support of p 1 can be similarly encoded in q θ t px 1 | xq. To investigate this, we consider the case where p t px 1 | xq is defined as a Gaussian distribution, but with its support on the manifold M :" supppp 1 q instead of the ambient Euclidean space. In this setting, the Riemannian Gaussian distribution naturally arises as a generalization of the Gaussian to a Riemannian manifold. We will refer to CFM and RFM as vanilla models, minimizing velocity differences, while VFM and the proposed model are variational, matching endpoints in their loss.

## MODEL DEFINITION

Riemannian Gaussian. Let M be a Riemannian manifold with metric g. The Riemannian Gaussian (RG) distribution [(Pennec, 2006)](#b9) is defined as where z, µ P M (with µ as the mean), σ ą 0 is a scale parameter, and dist g pz, µq denotes the geodesic distance determined by g. The constant C depends on both z and µ, and it normalizes the distribution over M. A more detailed geometric explanation can be found in Appendix A.2.

$N Riem pz | σ, µq " 1 C exp ˆ´dist g pz, µq 2 2σ 2 ˙,(6)$RG-VFM objective. We define the Riemannian Gaussian-VFM (RG-VFM) objective by using the Riemannian Gaussian as our variational approximation, i.e.

$L RG-VFM pθq " ´Et,x1,x " log N Riem px 1 | µ θ t pxq, σ t pxqq ‰ .(7)$In the Gaussian VFM case, this setting reduces to a straightforward MSE optimization, so it is natural to wonder whether a similar simplification holds here. In fact, such a simplification exists under two assumptions: (1) the manifold is homogeneous -that is, any point can be transformed into any other by a distance-preserving symmetry (a formal definition is provided in appendix A.1); and

(2) we have access to a closed-form expression for its geodesics. Formally, the following holds (see appendix B.1 for details): Proposition 3.1. Let M be a homogeneous manifold with closed-form geodesics. Then, the VFM objective reduces to

$L RG-VFM pθq " E t,x1,x " || log x1 pµ θ t pxqq|| 2 g ‰ . (8$$)$Minimizing this loss is equivalent to computing the Fréchet mean of the distribution, that is, compute µ ‹ " arg min µ θ PM ş M dist g px 1 , µ θ q 2 dx 1 , i.e. the point µ θ that minimizes the expected squared geodesic distance to the data points [(Fréchet, 1948)](#b4). Intuitively, this can be viewed as a generalization of the mean squared error from the Euclidean setting to a Riemannian framework.

The RG-VFM objective (eq. 8) minimizes the geodesic distance on M between target and predicted endpoints, so it only needs to learn the local geometry of M around p 1 . In contrast, vanilla RFM requires that supppp 0 q lie entirely on M because its vector fields depend on the manifold's intrinsic geometry; off-manifold points can lead to unstable estimates of tangents and vector fields.

For this study, we consider two versions: (1) RG-VFM-R 3 , where p 0 is Euclidean, M Ĺ supppp 0 q, and we use linear interpolation for conditional velocities, and (2) RG-VFM-M, where supppp 0 q " M, we use geodesic interpolation, and define velocities on the tangent spaces to M.

## HOW DOES RG-VFM FIT IN THE EXISTING FLOW MATCHING FRAMEWORK?

Figure [1](#fig_0) (left) illustrates how RG-VFM fits within the framework of related FM models. In VFM, a probabilistic generalization of CFM is obtained by making the posterior distribution explicit and customizable, obtaining standard CFM under the choice of a specific Gaussian (see [Eijkelboom et al. (2024)](#b3)). In contrast, RFM serves as a geometric generalization of CFM, where the model and its objective take into account the intrinsic properties and metric of the underlying Riemannian manifold. The same happens for the variational models: VFM with a Gaussian posterior is a particular instance of RG-VFM when the geometry is Euclidean. In Euclidean space, } log x1 pµ θ t pxqq} 2 g reduces to }µ θ t pxq ´x1 } 2 2 , thereby recovering the VFM objective. A further comparison can be made between the simplified version of RFM and RG-VFM-M, where M is a homogeneous manifold with closed-form geodesics. The variational model (RG-VFM) is not a direct generalization of vanilla RFM because, unlike in Euclidean space, tangent spaces at different points on a manifold do not coincide. This difference is reflected in the models' outputs (fig. [1](#fig_0)): vanilla models predict velocity fields, which are integrated as ODEs to construct flows, whereas variational models predict endpoint distributions, ideally aligning with the target distribution p 1 .

In Euclidean space, the difference between two vectors starting at x and pointing to different endpoints is simply the vector between those endpoints, leading to identical L 2 terms in the objectives, i.e. }µ θ t pxq ´x1 } 2 2 for VFM and }u t px | x 1 q ´vθ t pxq} 2 for CFM. However, since T x M ‰ T x1 M in general, in their geometric counterparts this equivalence no longer holds: indeed, the difference vector in the RFM objective, v θ t pxq ´log x px1q p1´tq , is in T x M, while log x1 pµ θ t pxqq is in T x1 M. This fundamental distinction separates RG-VFM from RFM. More details are in appendix B.2.

## EXPERIMENTS

Inspired by the planar checkerboard benchmark in generative modeling [(Grathwohl et al., 2018)](#b5), we introduce a spherical checkerboard distribution as our target p 1 , whose support is S 2 Ă R 3 . The noisy distribution p 0 is defined differently for each model: for CFM, VFM, and RG-VFM-R 3 , p 0 is the uniform distribution on r´1, 1s 3 Ă R 3 , while for RG-VFM-M and RFM, it is the uniform distribution on S 2 . We conduct two sets of experiments: we (1) compare Euclidean to geometric models in capturing the correct geometry -assessed by the norms of generated samples (ideally unit norm, since the points should lie on the sphere) -and (2) evaluate vanilla versus variational models in reproducing the target distribution. Figure [2](#fig_1) displays the generated distributions alongside the ground truth. All the experimental details and results are provided in appendix C.

Comparing the Euclidean and geometric models, the norm statistics reveals key differences. The Euclidean models (CFM and VFM) show slight norm deviation from the unit sphere (CFM: mean = 1.00, std = 0.094; VFM: mean = 1.00, std = 0.021), while the geometric models maintain a nearperfect unit norm. The geometric extensions effectively capture the underlying geometry well.

Comparing vanilla and variational models, we observe that vanilla models produce more blurred distributions, whereas variational models better patches contrasts. As shown in fig. [2](#fig_1), RG-VFM-R 3 and RG-VFM-M exhibit the best visual performance, with minimal differences. This is likely because RG-VFM's loss functions is both geometrically informed and focused on minimizing the distance between predictions and distribution endpoints rather than matching intermediate flow velocities. More interestingly, standard VFM also demonstrates strong performance, which we speculate is due to its emphasis on endpoint minimization. In essence, emphasizing endpoint accuracy enables variational models to capture the fine details of the target distribution's shape, and the additional geometric awareness of RG-VFM further enhances the result.

## CONCLUSION

We introduced Riemannian Gaussian Variational Flow Matching (RG-VFM), extending VFM to structured manifolds through Riemannian Gaussian distributions. Our approach unifies RFM and VFM under a common probabilistic framework, with experiments on spherical data demonstrating superior performance over Euclidean-based methods. These results establish RG-VFM as a promising framework for modeling distributions on complex geometries, with applications in molecular modeling and physics-informed generative tasks.

Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415-1428, 2021.

A GEOMETRIC BACKGROUND

A.1 RIEMANNIAN MANIFOLDS

In this section, we provide a concise review of Riemannian manifolds in the setting of complete, connected, and smooth manifolds. Let M be such a manifold endowed with a Riemannian metric g. At each point x P M, the tangent space is denoted by T x M, and g induces an inner product on each T x M, which we write as xu, vy g for u, v P T x M. By collecting all tangent spaces across M, we obtain the tangent bundle:

$T M " ď xPM txu ˆTx M.$We will also consider time-dependent vector fields on M. Specifically, let tu t u tPr0,1s be a family of smooth vector fields, with each u t mapping a point x P M to an element u t pxq P T x M. The operator div g pu t q denotes the Riemannian divergence of u t with respect to the spatial variable x.

Furthermore, the volume form on M induced by g is denoted by dM z . For any real-valued function f : M Ñ R, we write ż M f pxq dM z to denote its integral over M.

Homogeneous Manifold. A Riemannian manifold M is homogeneous if its isometry group acts transitively on M, i.e., for any two points x, y P M, there exists an isometry f : M Ñ M such that f pxq " y.

## A.2 RIEMANNIAN GAUSSIAN DISTRIBUTIONS

We describe the construction of the Riemannian Gaussian (RG) distribution, which generalizes the familiar Gaussian distribution to the setting of a Riemannian manifold. The definition is inspired from [Pennec (2006)](#b9).

Riemannian Gaussian. Let M be a Riemannian manifold endowed with the metric tensor g. The RG distribution is defined by

$N Riem pz | σ, µq " 1 C exp ´´dist g pz, µq 2 2σ 2 ¯,$where z P M is a point on the manifold, µ P M plays the role of the mean, and σ ą 0 is a scale parameter controlling the spread of the distribution. Here, dist g pz, µq denotes the geodesic distance between z and µ as determined by the metric g, and C is a normalization constant chosen so that the total probability integrates to 1 over M:

$C " ż M exp ´´dist g pz, µq 2 2σ 2 ¯dM z .$The measure dM z represents the Riemannian volume element, which in local coordinates takes the form dM z " a det gpzq dz, with dz being the standard Lebesgue measure in the coordinate chart and gpzq is the Riemannian metric tensor at the point z. This formulation ensures that the probability density is adapted to the geometric structure of the manifold.

Observation. In the special case where M " R d and the metric is Euclidean (i.e., gpzq " I), the geodesic distance reduces to the usual Euclidean distance, and the RG distribution becomes the standard multivariate Gaussian with covariance matrix σ 2 I. On more general manifolds, however, the curvature and topology are taken into account through the geodesic distance and the volume element, leading to a natural extension of the Gaussian concept. This construction can be applied to spaces such as hyperbolic manifolds, where one can define the distribution in the tangent space at a point µ and then use the exponential map to project it onto the manifold.

Comparison to vMF. A closely related distribution is the von Mises-Fisher (vMF) distribution, which is traditionally defined on the sphere S n by vMFpz | µ, κq9 exp `κ xz, µy ˘, with µ P S n and x¨, ¨y denoting the standard dot product. The vMF distribution is based on the notion of directional data and an inner product structure that measures alignment. In contrast, the RG distribution is inherently tied to the Riemannian metric, making it applicable to a much wider class of manifolds. Generalizing the idea behind the vMF distribution to other geometries often requires embedding the manifold into a larger ambient space and defining a suitable bilinear form (such as the Minkowski inner product in hyperbolic geometry). In this sense, the RG approach offers a more natural and geometrically intrinsic formulation.

In summary, the Riemannian Gaussian distribution is defined in terms of the geodesic distance and the corresponding volume element, and it adapts to the underlying geometry of any Riemannian manifold.

## B R-VFM AND LINK WITH RFM

B.1 DETAILED DERIVATION OF RG-VFM OBJECTIVE Proposition 3.1. Let M be a homogeneous manifold with closed-form geodesics. Then, the VFM objective reduces to

$L RG-VFM pθq " E t,x1,x " || log x1 pµ θ t pxqq|| 2 g ‰ . (8$$)$Proof. The objective of VFM is defined as

$L VFM pθq " ´Et,x1,x " log q θ t px 1 |xq ‰ .$We define the objective function of RG-VFM by setting the posterior probability as the Riemannian Gaussian, i.e.,

$q θ t px 1 |xq " N Riem px 1 | µ θ t pxq, σpxqq, so that L RG-VFM pθq " ´Et,x1,x " log N Riem px 1 | µ θ t pxq, σpxqq ‰ .$More explicitly, we have

$L RG-VFM pθq " ´Et,x1,x " log q θ t px 1 |xq ‰ " ´Et,x1,x " log N Riem px 1 | µ θ t pxq, σpxqq ‰ " ´Et,x1,x " log ˆ1 Cpµ θ t pxqq exp ˆ´dist g px 1 , µ θ t pxqq 2 2σpxq 2 ˙˙ȷ " ´Et,x1,x " log ˆ1 Cpµ θ t pxqq ˙´dist g px 1 , µ θ t pxqq 2 2σpxq 2 ȷ " ´Et,x1,x " log ˆ1 Cpµ θ t pxqq ˙ȷ `Et,x1,x " dist g px 1 , µ θ t pxqq 2 2σpxq 2 ȷ ,$where dist g pq denotes the geodesic distance induced by the Riemannian metric g.

Without any regularity assumptions on M, no further simplification is possible. However, under the following assumptions the objective becomes more tractable:

1. Homogeneity: If the manifold pM, gq is homogeneous, the normalization constant

$C " ż M exp ˆ´dist g pz, µq 2 2σ 2 ˙dM z$is independent of the mean µ. Hence, defining

$K :" ´Et,x1,x " log ˆ1 Cpµ θ t pxqq ˙ȷ ,$which is constant with respect to θ, we obtain

$L RG-VFM pθq " K `Et,x1,x " dist g px 1 , µ θ t pxqq 2 2σpxq 2 ȷ .$Since K is a constant that is independent of the model's parameters θ, the minimization objective becomes

$L RG-VFM pθq " E t,x1,x " dist g px 1 , µ θ t pxqq 2 2σpxq 2 ȷ .$2. Closed-form Geodesics: If the geometry allows closed-form expressions for geodesics, namely

$γptq " exp x ´t ¨log x pyq ¯,$then the geodesic distance between two points is given by: dist g pz, µq " } log z pµq} g .

In this setting, we can write dist g px 1 , µ θ t pxqq 2 " } log x1 pµ θ t pxqq} 2 g , so that the objective becomes

$L RG-VFM pθq " ´Et,x1,x " log ˆ1 Cpµ θ t pxqq ˙ȷ `Et,x1,x " 1 2σpxq 2 } log x1 pµ θ t pxqq} 2 g ȷ .$3. Combined Assumptions: If both conditions hold, the objective simplifies to

$L RG-VFM pθq " E t,x1,x " 1 2σpxq 2 } log x1 pµ θ t pxqq} 2 g ȷ .$If we further assume that σpxq is constant, this reduces to

$L RG-VFM pθq " E t,x1,x " } log x1 pµ θ t pxqq} 2 g ‰ .$Examples of simple geometries. A homogeneous manifold does not necessarily imply that geodesics admit closed-form expressions. Conversely, the simple geometries with closed-form geodesics considered in the RFM setting-such as hyperspheres S n , hyperbolic spaces H n , flat tori T n " r0, 2πs n , and the space of SPD matrices S d with the affine-invariant metric-are homogeneous. Thus, when restricting to these geometries for comparison with RFM, we are in the combined case.

Euclidean space. In the Euclidean case (which also falls into the combined case), the objective simplifies further to L RG-VFM pθq " E t,x1,x " }µ θ t pxq ´x1 } 2 ‰ .

## B.2 RG-VFM VS RFM ON HOMOGENEOUS SPACES WITH CLOSED-FORM GEODESICS

The objective of RG-VFM is defined as

$L RG-VFM pθq " E t,x1,x " } log x1 pµ θ t pxqq} 2 g ‰ ,$while the objective of RFM, in the case of closed-form geodesics, is given by

$L RFM pθq " E t,x1,x " › › v θ t pxq ´log x px 1 q{p1 ´tq › › 2 g ı ,$with g being the metric tensor at x " p t px|x 1 q.

Ignoring multiplicative constants that depend only on t and x, comparing the two losses reduces to comparing the quantities } log x1 pµ θ t pxqq} 2 g and }v θ t pxq ´log x px 1 q} 2 g .

Euclidean space. In Euclidean space R n , the tangent space at each point is naturally identified with R n . In this setting, log x1 pµ θ t pxqq " µ θ t pxq ´x1 and log x px 1 q " x 1 ´x.

Notice that µ θ t pxq ´x1 " µ θ t pxq ´x `x ´x1 " pµ θ t pxq ´xq ´px 1 ´xq " pµ θ t pxq ´xq ´log x px 1 q.

If we define (ignoring multiplicative constants such as 1{p1 ´tq) v θ t pxq " log x pµ θ t pxqq " µ θ t pxq ´x, then it follows that log x1 pµ θ t pxqq " log x pµ θ t pxqq ´log x px 1 q, implying } log x1 pµ θ t pxqq} 2 g " }v θ t pxq ´log x px 1 q} 2 g . Thus, L RG-VFM pθq and L RFM pθq are equivalent up to an additive constant. This result is consistent with the known equivalence between L VFM pθq and L CFM pθq.

General geometries. In non-Euclidean spaces, however, the quantities } log x1 pµ θ t pxqq} 2 g and }v θ t pxq ´log x px 1 q} 2 g are not necessarily equal. This is because log x1 pµ θ t pxqq is a vector in T x1 M, while log x pµ θ t pxqq ĺog x px 1 q lies in T x M, and in general T x1 M ‰ T x M. Establishing a relation between these vectors is not straightforward and can be illustrated by comparing the law of cosines in Euclidean, hyperbolic spaces, and on hyperspheres.

## C EXPERIMENTS

In this section, we present further results from the experiments described in Section 4.

## C.1 EXPERIMENTAL SETUP

In all experiments, the target distribution p 1 is the spherical checkerboard, so its support is S 2 . The noisy distribution p 0 varies by model: for CFM, VFM, and RG-VFM-R 3 , p 0 is the uniform distribution on r´1, 1s 3 Ă R 3 , while for RG-VFM-M and RFM, p 0 is the uniform distribution on S 2 . In every case, we train a five-layer MLP with 64/128 hidden features for 3000 epochs, that we use to generate 5000 samples using a Runge-Kutta ODE solver.

## C.2 RESULTS

Figure [3](#fig_3) illustrates the generative flow trajectories over time, from the initial distribution p 0 to the generated distribution at t " 1.

Figure [4](#fig_5) displays the generated distributions unwrapped onto a flat surface for easier visualization and comparison. These results visually confirm the observations presented in Section 4.

Finally, Figure [5](#fig_7) shows histograms of the norm values of the generated samples. As discussed in Section 4, this metric differentiates the Euclidean models (CFM and VFM) from the others. Ideally, points should have a norm of 1, since they lie on the sphere. However, because the Euclidean models lack explicit geometric information, their points deviate slightly from a norm of 1-with CFM exhibiting a larger divergence. In contrast, the geometric models consistently generate points that lie almost exactly on the sphere.      

![Figure 1: Left: Overview of the main Flow Matching models relevant to our proposed approach, illustrating their relationships. Right: Visualization of each model's predictions (color-coded to match the corresponding model on the left) for a target distribution p 1 supported on the sphere S 1 .]()

![Figure 2: Comparison of the spherical checkerboard distribution generated with CFM, VFM, RFM and our methods RG-VFM-R 3 and RG-VFM-M.]()

![(a) Model: CFM; supppp0q :" R 3 , p0: uniform distribution on r´1, 1s 3 Ă R 3 . (b) Model: VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (c) Model: RG-VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (d) Model: RFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .(e) Model: RG-VFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .]()

![Figure3: Flow trajectories of 5,000 samples, initially drawn from the noisy distribution p 0 at t " 0, evolving to reach their final configuration by t " 1.]()

![(a) Model: CFM; supppp0q :" R 3 , p0: uniform distribution on r´1, 1s 3 Ă R 3 . (b) Model: VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (c) Model: RG-VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (d) Model: RFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .(e) Model: RG-VFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .]()

![Figure 4: Sample distributions generated by different models (representing the flow configuration at t " 1) unwrapped from S 2 to R 2 for improved visualization. The true checkerboard distribution is shown in gray in the background.]()

![(a) Model: CFM; supppp0q :" R 3 , p0: uniform distribution on r´1, 1s 3 Ă R 3 . (b) Model: VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (c) Model: RG-VFM; supppp0q :" R 3 ; p0: uniform distribution on r´1, 1s 3 Ă R 3 . (d) Model: RFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .(e) Model: RG-VFM; supppp0q :" S 2 ; p0: uniform distribution on S 2 .]()

![Figure 5: Histogram of the norm values of the 5000 samples describing the generated distribution.]()

