<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-10">10 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junwei</forename><surname>Luo</surname></persName>
							<email>luojunwei@whu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xue</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Wu</surname></persName>
							<email>kangwu@whu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Liang</surname></persName>
							<email>leywar.liang@antgroup.com</email>
						</author>
						<author>
							<persName><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
							<email>jingdongchen.cjd@antgroup.com</email>
						</author>
						<author>
							<persName><forename type="first">Yansheng</forename><surname>Li</surname></persName>
							<email>yansheng.li@whu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wuhan</forename><surname>Universtiy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ant</forename><surname>Group</surname></persName>
						</author>
						<title level="a" type="main">When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-10">10 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">AF62B2289E59242702A97DA9D23F8DAB</idno>
					<idno type="arXiv">arXiv:2503.07588v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-03-18T18:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Benefiting from the rapid advancement of Large Language Models (LLMs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b66">67]</ref>, Large Visual-Language Models (LVLMs) have demonstrated strong capabilities in * indicates interns at Ant Group. ‡ Corresponding author. perceiving and understanding visual information in textbased multimodal interactions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b56">57]</ref>. Currently, LVLMs have been extensively studied and applied across various fields like remote sensing (RS) intelligent interpretation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>Advances in satellite imaging technology allow for the acquisition of large remote sensing images (RSIs) that cover extensive ground areas and contain detailed land use information. Therefore, natural language-based intelligent analysis of large RSIs is valuable for applications such as com-plex scene understanding <ref type="bibr" target="#b32">[33]</ref>, urban planning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b55">56]</ref>, infrastructure development <ref type="bibr" target="#b61">[62]</ref>, and monitoring <ref type="bibr" target="#b84">[85]</ref>.</p><p>Some RS LVLMs handle high-resolution input through position embedding interpolation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b77">78]</ref>. Additionally, various dynamic high-resolution strategies designed for general LVLMs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b71">72]</ref> can be adapted to RS LVLMs, such as EarthDial <ref type="bibr" target="#b52">[53]</ref> (based on InternVL1.5 <ref type="bibr" target="#b7">[8]</ref>) and GeoPixel <ref type="bibr" target="#b48">[49]</ref> (using InternLM-XComposer2.5 <ref type="bibr" target="#b75">[76]</ref>). However, these approaches for high-resolution image processing are limited when handling large RSIs exceeding 10,000×10,000 pixels. As depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, these methods typically employ limited pre-defined grid partitions, leading to detail loss when large RSIs are excessively downsampled. Conversely, unlimited grids suffer from prohibitive time and memory costs (e.g., over 5 million vision tokens for a large RSI in LLaVA-1.5 <ref type="bibr" target="#b34">[35]</ref>). This necessitates balancing resolution and computational efficiency when processing large RSI.</p><p>To overcome the above limitations of existing strategies in handling large RSIs, we propose a text-guided token pruning strategy that consists of two key components: a Region Focus Module (RFM) and a Dynamic Image Pyramid (DIP). The RFM identifies text-relevant key vision tokens by leveraging the capability distilled from the LLM component. Based on the RFM's output, we select critical image tiles and conduct token pruning in a coarse-to-fine manner within the DIP. This enables the LVLM to focus and zoom in key areas, avoiding to process all visual tokens, thereby facilitating flexible and efficient inference.</p><p>Moreover, benchmarks for LVLMs in understanding large RSIs remain insufficiently developed. Although MME-Realworld <ref type="bibr" target="#b82">[83]</ref> includes large RSIs with high-quality manually annotated questions, its RS subset suffers from limited question diversity and image sizes. To more comprehensively characterize the challenges of large RSI perception, we construct a new benchmark called LRS-VQA (Large Remote Sensing image Visual Question Answering). LRS-VQA features larger size images with lengths up to 27,328 pixels and 8 distinct question types. Our key contributions are as follows:</p><p>• We introduce a Region Focus Module (RFM) that distills the ability from LLM part of LVLM to efficiently identify text-related key vision tokens. • Based on RFM and DIP, we propose an efficient coarseto-fine inference strategy for large RSIs, which achieves a balance between resolution and efficiency through key tile selection and vision token pruning. • We construct a new benchmark named LRS-VQA, featuring larger image sizes and more diverse question types than existing benchmarks, to reflect the challenges of large RSIs perception. • The proposed method is architecture-agnostic and shows performance improvements and efficiency gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">LVLMs for High-Resolution Comprehension</head><p>Numerous LVLMs designed for high-resolution image or video perception have recently emerged. They can be primarily categorized into the following paradigms: Methods that carefully design image cropping and padding strategy. Methods like <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b81">82]</ref> traverse pre-defined partition grids and obtain the most suitable grid, to divide the high-resolution image into image tiles. These methods encounter limitations when processing large RSIs as in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Methods based on multiple visual branches. These approach <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref> employ additional high-resolution visual encoders like SAM <ref type="bibr" target="#b21">[22]</ref> encoder or ConvNext <ref type="bibr" target="#b38">[39]</ref> to process images with higher input resolution.</p><p>Visual chain-of-thoughts methods. These approaches utilize a search-and-focus strategy <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b60">61]</ref> or external RAG pipeline <ref type="bibr" target="#b83">[84]</ref> to identify text-related key image regions. However, such pipelines require multiple LVLM inferences, making them more complex and cumbersome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Vision Token Reduction in LVLMs</head><p>Visual token pruning for transformers has been a classic research topic, it aims to accelerate computation by dynamically reducing less important tokens. It has been extensively studied in both natural language processing <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b70">71]</ref> and computer vision domains <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>Recently, a variety of token pruning methods have been proposed for LVLMs, which can be categorized into three main types: token pruning conducted in the vision encoder <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref>, token pruning performed in the LLM component <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b79">80]</ref>, and collaborative approaches that integrate both <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b76">77]</ref>.</p><p>Although vision-part pruning methods efficiently reduce vision tokens, when processing large images, it's highly time-consuming to traverse pre-defined grids and handle numerous image tiles with the vision encoder. Moreover, the absence of language guidance makes it hard to identify foreground regions in complex RSIs. The LLM-part pruning methods require feeding all vision tokens into the LLM. Therefore, long visual sequences from large images may exceed LLM length limits and incur high computational costs for attention score ranking.</p><p>Among the collaborative approaches, the methods most closely related to ours are LLaVA-Mini <ref type="bibr" target="#b76">[77]</ref> and FlexAttention <ref type="bibr" target="#b26">[27]</ref>. The former employs modality pre-fusion to compress visual information; however, its LLM-like pre-fusion module still struggles with handling extremely long vision sequences. The latter integrates text-guided high-resolution features within the LLM component. Nevertheless, the allowable image size for its high-resolution view remains limited, and it can only index high-resolution features once. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>In this section, we present the foundational concepts of text-image attention within the LLM component of highresolution LVLMs. The proposed RFM leverages this mechanism to guide the text-related key image tile selection and the vision token pruning.</p><p>Mainstream modular LVLMs include three components <ref type="bibr" target="#b40">[41]</ref>: a pre-trained vision encoder, a feature projector, and a pre-trained decoder-only LLM. Conventional highresolution strategies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> process the original image I img into a thumbnail view I thumb and a set of image tiles I tiles . Then the vision encoder and the projector transforms I thumb and I tiles into vision token embeddings T vis = [T lr vis , T hr vis ], which contain low-resolution tokens T lr vis from the I thumb and higher-resolution T hr vis from the I tiles . T vis are concatenated with text token embeddings T txt from the tokenized text instruction, forming a multimodal token sequence T = [T vis , T txt ]. The LVLM processes T via transformer-based decoder layers with causal self-attention, where the attention score A can be represented as:</p><formula xml:id="formula_0">A = Attention(Q, K) = Softmax ( QK ⊺ √ d ) ,<label>(1)</label></formula><p>where A ∈ R n×n is the self-attention map, Q, K ∈ R n×d are the query and key matrices derived from the input embeddings through linear transformations, n is the input sequence length, and d is the feature dimension. As A determines the weight each token assigns to the former tokens in the sequence, by analyzing the attention from text tokens to vision tokens, we can identify text-related important vision tokens. Some studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b72">73]</ref> observe that in the deep layers of LLM part in LVLMs, attention is predominantly allocated to text-related vision tokens compared to other vision tokens. This insight inspired us to design a coarse-to-fine mechanism to focus on key image regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method and Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Method</head><p>To achieve efficient high-resolution image understanding, we propose a method that combines text-guided key region localization with a multi-level dynamic image pyramid (DIP), as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Initially, the DIP is constructed based on the input large image. At the low-resolution DIP level, the Region Focus Module (RFM) provides attention distribution for the initial vision tokens. These result guides the retrieval of corresponding image tiles from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">The Construction of Dynamic Image Pyramid</head><p>Given an original large RSI I img , we construct an imagelevel pyramid with a variable number of layers to generate a series of image tiles at different ground sample distances (GSDs), enabling coarse-to-fine inference. First, we iteratively downsampling I img by a factor of 2, generating a series of images {I 1 init , I 2 init , . . . , I P init } until the shorter side reaches a pre-defined minimum length. Let a basic image tile size be B × B (e.g., 336 × 336 for CLIP-L14), for the p-th scaled image I p init , we calculate the number of image tiles along the height and width as follows:</p><formula xml:id="formula_1">N p h = ⌈H p /B⌉, N p w = ⌈W p /B⌉,<label>(2)</label></formula><p>where H p , W p are the height and width of I p init , respectively. For proper tiling, we compute a scale factor r p as:</p><formula xml:id="formula_2">r p = min ( N p h × B H p , N p w × B W p ) .<label>(3)</label></formula><p>The image I p init is then resized to I p using r p , followed by padding to preserve the original aspect ratio. After resizing, I p is partitioned into non-overlap tiles I p tiles . Tiles from all scaled images are combined in reverse order, and integrated with the thumbnail view I thumb to form the final pyramid: I DIP = {I thumb , I 1 tiles , I 2 tiles , . . . , I P tiles }, which consists of P + 1 layers with progressively increasing resolutions. During training, we select the thumbnail I thumb and I 1 tiles as visual inputs for computational efficiency. During inference, all pyramid layers become accessible to enable multi-scale inference, as in the left part of Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Attention Distillation for Key Region Focus</head><p>In this section, we introduce the core idea and details of the Region Focus Module (RFM), a lightweight module designed to replicate the text-guided region localization capability inherent in LVLM's LLM part. RFM can be seamlessly integrated during the supervised fine-tuning (SFT) stage of LVLM, and generates attention scores for vision tokens during inference.</p><p>Prior findings <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b72">73]</ref> demonstrate that the deep layers in the LLM part of LVLM could accurately localize textrelated key regions in the image through cross-model attention. Inspired by this observation, our core idea is to distill this text-guided key region focus capability from the LLM, as shown in the left part of Fig. <ref type="figure" target="#fig_2">3</ref>. After distillation, the trained RFM can help to select text-related key image tiles and prune vision tokens before the LLM, reducing computational overhead from the image encoding stage.</p><p>Specifically, assuming the LLM has M layers l 1 , l 2 , . . . , l M , the proposed RFM adopts the same architecture with R layers r 1 , r 2 , . . . , r R , where R&lt;M . Each RFM layer r i is initialized from a selected LLM layer l mi , where {m 1 , m 2 , . . . , m R } forms a sparse subset of LLM layer indices. For layer-wise distillation, we select K pairs (K&lt;R) from RFM and LLM as student-teacher pairs, represented as {(r k , l m k ) | k = 1, 2, . . . , K}. This strategy avoids rigidly aligning each r i with l mi , which would neglect the non-teacher layers in the LLM, resulting in inconsistency between adjacent RFM layers when training.</p><p>As depicted in the right part of Fig. <ref type="figure" target="#fig_2">3</ref>, the concatenated multimodal tokens T = [T vis , T txt ] are fed into both the RFM and the LLM during training. For the k-th studentteacher layer pair (r k , l m k ), we extract the attention scores between the last text token and all vision tokens from the multi-head self attention (MHSA). For the h-th head, the self-attention is represented as A k,h stu , A k,h tea ∈ R j×nv , where j is the number of dialogue turns for multi-turn instruction and n v is the length of vision tokens. We apply the Kullback-Leibler (KL) divergence loss for distillation:</p><formula xml:id="formula_3">L h kl = K ∑ k=1 [D KL (A k,h tea ∥ A k,h stu ) + λ hr D KL (A k,h,hr tea ∥ A k,h,hr stu )] ,<label>(4)</label></formula><p>where A k,h,hr tea , A k,h,hr stu denote attention corresponding to T hr vis , and λ hr is the weighting factor. Additionally, we enforce stronger alignment constraints to the attention of T hr vis by using an additional Mean Squared Error (MSE) loss:</p><formula xml:id="formula_4">L h mse = K ∑ k=1 MSE (A k,h,hr stu , A k,h,hr tea ) .<label>(5)</label></formula><p>The total distillation loss is computed as:</p><formula xml:id="formula_5">L distill = 1 KH H ∑ h=1 (λ mse L h mse + λ kl L h kl ) , (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where K is the number of selected student-teacher layer pairs, H is the number of attention heads, and λ mse , λ kl are hyperparameters. For the flash-attention <ref type="bibr" target="#b10">[11]</ref>, we employ a specialized value matrix to extract attention maps like <ref type="bibr" target="#b79">[80]</ref> to ensure compatibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Text-Guided Token Pruning with Pyramid</head><p>Given the extensive irrelevant background content in large RSIs, the core of our strategy lies in iteratively selecting text-related key image tiles and performing token pruning, thereby achieving a coarse-to-fine inference, as illustrated in the below part of Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, we integrate DIP and RFM for inference, as detailed in Alg. 1. For simplicity, the tokens from the system prompt are omitted. We initialize the vision input using I thumb and I p tiles to generate the initial T vis = [T lr vis , T p,hr vis ], where p = 1, then the concatenated tokens T are fed into the RFM. From the last layer of the RFM, we compute the average attention scores from all heads in the MHSA, and extract the attention scores A K,hr stu of T p,hr vis . By selecting the top-λ proportion from A K,hr stu , we identify the indices of key tokens and map their coordinates to image tile-level coordinates, to select key image tiles I p+1 key from I p+1 tiles . Based Algorithm 1 Coarse-to-Fine Token Pruning with DIP Require: {I 1 tiles , I 2 tiles , . . . , I P tiles } in I DIP , K-layer RFM, initial multimodal tokens T , vision encoder and projector V(⋅), token saving ratio α, tile number threshold N max Ensure: Retained tokens T retain vis 1: Definition: Top-α(X) selects elements in X with values in the top α proportion. 2: for p = 1 to P do Normalize:</p><p>5:</p><p>Identify key positions: P K,hr key ← Top-α(A</p><p>K,hr stu ) 6: if p &lt; P then 7: Map coordinates and select key tiles: I p+1 key ← {t ∈ I p+1 tiles | mapped from P K,hr key } 8: if |I p+1 key | &gt; N max then 9: Prune tokens: T retain vis ← {t ∈ T p,hr vis | P K,hr key } 10: break 11: else 12: Encode selected tiles: T p+1,hr vis ← V(I p+1 key ) 13: Update T ← [T lr vis , T p+1,hr vis , T txt ] 14: end if 15: else 16: Prune tokens at final layer: T retain vis ← {t ∈ T p,hr vis | P K,hr key } 17:</p><p>end if 18: end for 19: return T retain vis on the number of I p+1 key , we determine whether to prune the T p,hr vis directly or replace it with vision tokens T p+1,hr vis from higher-resolution tiles. If T p+1,hr vis is obtained, the tile selection process is repeated recursively until the last layer of DIP is reached. This approach enables LVLM to focus only on processing a few high-resolution image tiles in a coarseto-fine manner, thereby reducing computational complexity while preserving critical text-related image details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The Construction of LRS-VQA Benchmark</head><p>Considering the limitations of existing benchmarks for evaluating LVLMs' perception of RSIs in terms of question diversity and image size, we propose a new benchmark for more comprehensively evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Annotation Pipeline</head><p>The annotation pipeline of LRS-VQA is illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>. We collect 3 publicly remote sensing datasets: FAIR1M-1.0 <ref type="bibr" target="#b53">[54]</ref>, GLH-Bridge <ref type="bibr" target="#b30">[31]</ref>, and STAR <ref type="bibr" target="#b32">[33]</ref>, and identify all unique targets based on the object detection labels to generate unique references from them (e.g., "the top-most ship" and "the largest dock"). Subsequently, we crop the region with the unique target from the original image, and feed it, along with a prompt containing the unique reference, into  GPT-4V <ref type="bibr" target="#b46">[47]</ref> to generate question-answer pairs about the target's color, shape, status, etc. Details of the overall construction process are provided in the Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Visual-Centric Quality Check and Refinement</head><p>To ensure the quality of the generated question-answer pairs, we employ powerful Qwen2-VL <ref type="bibr" target="#b56">[57]</ref> for quality assessment. Qwen2-VL supports manual adjustment of the maximum pixel resolution, allowing us to evaluate whether increased resolution improves accuracy while keeping the LLM unchanged. Using this approach, we filter out question types where accuracy can not improve with resolution. Through iterative filtering, manual review, and prompt refinement, we obtain the LRS-VQA dataset, which could effectively assess the large RSI perception capability of LVLMs, as shown in Fig.</p><p>5. LRS-VQA contains eight question-answer types: count, color, category, shape, status, reasoning, rural/urban classification, and target background. It features greater diversity in question types and larger image sizes than MME-Realworld-RS as in Tab. 1, highlighting the complexities of large RSI perception. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Details</head><p>Data. During the pre-training (PT) phase, we utilized the same 558K data used in LLaVA-1.5 <ref type="bibr" target="#b35">[36]</ref>. For the SFT phase, we collect 484K samples, including 300K sampled from LLaVA-1.5-665K, 146K sampled from RSVQA-HR <ref type="bibr" target="#b39">[40]</ref>, and 38K template-based samples from 3 RS datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref> using their labels. Note some images in this 38k data are from the same source as LRS-VQA, but there is no overlap or duplication between them. Details can be found in the Appendix A.2.1. Benchmarks and evaluation metrics: i) MME-RealWorld-RS: the RS part of MME-Realworld <ref type="bibr" target="#b82">[83]</ref>, containing 1,298 RSIs with expert-annotated questions in three types: color, count, and position. We follow the official evaluation script but modify the prompt by removing "The best answer is:" to address Vicuna-1.5-based models' tendency to respond with only option A. ii) LRS-VQA: it consists of 3 parts: LRS-FAIR, LRS-Bridge, and LRS-STAR, containing 2,272, 1,062, and 3,999 QA pairs, respectively.</p><p>For the short open-ended format, we adopt a structured evaluation metric following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b62">63]</ref>, using WordNet <ref type="bibr" target="#b44">[45]</ref>, with a semantic similarity threshold of 0.8.</p><p>Experimental settings. We employ existing LVLMs' official weights or api for evaluation. For comparison, Table 4. Comparison of computational efficiency with different token reduction methods based on LLaVA-Next-Qwen2. We assume inferring a 4000×4000 pixel image (anyres-p144) and report the theoretical TFLOPs of vision tokens after the vision projector.</p><p>respectively. The RFM-LLM layer pairs are <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref> with distillation applied to the first and last pairs. More experimental details are in the Appendix A.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Leaderboard and Comparison</head><p>Leaderboard on LRS-VQA. We evaluate a total of 11 open-source LVLMs, including methods for high-resolution <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b81">82]</ref>, chain-of-thought reasoning <ref type="bibr" target="#b60">[61]</ref>, multiple visual encoders <ref type="bibr" target="#b29">[30]</ref>, and RS LVLMs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref>. We also Comparison with token reduction methods. As shown in Tab. 3 and Tab. 4, we reproduce 4 plug-and-play methods for comparison on MME-RealWorld-RS. For Prune-Merge++ <ref type="bibr" target="#b49">[50]</ref> and our RFM-based pruning, the retain ratio is 25%. For VisionZip <ref type="bibr" target="#b68">[69]</ref>, the number of retained tokens is 64. For PDrop <ref type="bibr" target="#b63">[64]</ref> under Qwen2, pruning layers are set to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>. Additionally, we employ 3-layer DIP, supporting up to 2,016×2,016 pixels, which improves accuracy by 1.46% and boosts FPS by 39% compared to other methods. For Tab. 4, we consistently use a 4-layer DIP. Since our method dynamically selects image tiles, we calculate the average number of tiles selected from similar size images in the dataset. Above results indicate that existing methods often overlook the high cost of processing many text-irrelevant image tiles in large images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Results</head><p>We conducted ablation studies on the MME-RealWorld-RS, from the following aspects to perform an in-depth analysis:</p><p>Different ratios for RFM-based pruning. As shown in Tab. 5, a key finding is that in large RSIs, the complex background and the small foreground regions enable high pruning rates to deliver performance benefits.</p><p>Fixed DIP layers for inference. As shown in Tab. 6, we fix the number of DIP layers reached during inference. If an image's DIP is shallower than the specified number, the last layer is used. Results show that forcing traversal to higher resolutions can degrade performance since not all questions require image details. Therefore, our strategy dynamically selects the termination DIP layer based on the input text, balancing accuracy and efficiency.</p><p>Distillation settings for the RFM module. We explore the impact of different RFM-LLM layer-pairs in Tab. 7. Results show that introducing too many layers may hinder effective distillation and increase training and inference time, while too few layers may prevent RFM from effectively learning text-aware localization ability.</p><p>Visualization results. In Fig. <ref type="figure" target="#fig_6">6</ref>, we visualize the attention maps generated by the trained RFM for vision tokens from initial input image tiles. The results demonstrate that text references to targets effectively guide the RFM's attention output, supporting our coarse-to-fine inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper presents a text-guided token pruning method tailored for efficiently processing large remote sensing images (RSIs). To address the limitations of existing high-resolution approaches when handling large RSIs, our method integrates the Region Focus Module (RFM) and Dynamic Image Pyramid (DIP) to focus on text-relevant key vision tokens while reducing computational overhead. Furthermore, we introduce a new benchmark, LRS-VQA, with 7,333 QA pairs covering 8 question types, and features larger image sizes and a diverse range of question types.</p><p>Future work. Vision-language understanding of large RSIs remains challenging. Exploring alternative perspectives, such as long-context transfer from LLMs to LVLMs, to seek solutions is also meaningful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. High-resolution strategy comparison for modular LVLMs. (a) and (b) show existing grid-based cropping methods face challenges when processing large RSIs. (c) The proposed dynamic pyramid-based token pruning strategy can dynamically select image tiles of key regions related to the input text, balancing image detail and computational cost.</figDesc><graphic coords="1,317.25,301.44,236.25,205.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The pipeline of the proposed method. The entire process iterates in a coarse-to-fine manner, dynamically retrieving highresolution features from the next DIP level (leftward orange arrow) or performing token pruning at the current level (rightward orange arrow) based on the output of the RFM module at each iteration. During training, the RFM distillation text-related attention from the LLM; during inference, RFM generates the attention scores for the input vision tokens. GSD means ground sample distance.</figDesc><graphic coords="3,58.50,72.00,495.00,282.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The proposed RFM and attention distillation strategy. The left part indicates our core idea: distill accurate text-related key region localization ability from the LLM part of the LVLM. The right part shows the distillation details. We only select specific layer pairs for distillation to avoid hidden state discontinuities. "sys token" represents the tokens from the system prompt.higher-resolution DIP levels or trigger token pruning at the current level. This iterative process could continue through the pyramid until reaching the original resolution. Below, we introduce our approach in three components: DIP construction in Sec. 4.1.1, RFM and attention distillation in Sec. 4.1.2, and their integration in Sec. 4.1.3.</figDesc><graphic coords="4,58.50,72.00,495.00,219.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>Compute attention: A K,hr stu ← RFM(T )[T p,hr vis ]4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The construction pipeline of the proposed LRS-VQA dataset. The visual prompt (red box) is inspired by SoM [68].</figDesc><graphic coords="6,58.50,157.40,236.25,262.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The accuracy trends of Qwen2-VL across varying input maximum pixels. This demonstrates that accuracy on both the manually annotated MME-RealWorld-RS and our proposed LRS-VQA exhibit a positive correlation with resolution improvement, proving the effectiveness of LRS-VQA in evaluating LVLM's high-resolution RSI perception capabilities.</figDesc><graphic coords="6,317.25,157.39,236.24,151.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Text-related visual attention localization in the last layer if trained RFM under LLaVA-Next-Qwen2.</figDesc><graphic coords="8,317.25,72.00,236.24,155.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of existing benchmarks for evaluating LVLMs' perception capabilities in large RSIs.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Image Size Image Num Ques. Cate. Ques. Num</cell><cell>QS Format</cell></row><row><cell cols="2">MME-RealWorld-RS [83] 689-11500</cell><cell>1298</cell><cell>3</cell><cell>3738</cell><cell>Single-choice</cell></row><row><cell>LRS-VQA (Ours)</cell><cell>1024-27328</cell><cell>1657</cell><cell>8</cell><cell>7333</cell><cell>Open-end</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Leaderboard and performance comparison on LRS-VQA and MME-RealWorld-RS. "MME-RW-RS" indicates the MME-RealWorld-RS. "Data" refers to the total instruction data used during PT and SFT. "pX" indicates the max number is X for the pre-defined grids. "method*" indicates we reproduce the SFT stage of existing methods using the 484k instruction data.For our method, the minimum length for DIP is 1,008 pixels. For Vicuna-1.5 and Qwen2, the N max is set to 40 and 80, respectively. The vision token saving ratio α is 0.25, λ hr , λ mse , λ kl are 2.0, 1.0, 1.0,</figDesc><table><row><cell>Leaderboard</cell><cell>Data</cell><cell>Vis. Encoder</cell><cell>LLM</cell><cell>Max Size</cell><cell cols="2">MME-RW-RS</cell><cell>LRS-FAIR</cell><cell>LRS-Bridge</cell><cell cols="2">LRS-STAR</cell><cell>Avg. Acc</cell></row><row><cell>Qwen2-VL [57]</cell><cell>-</cell><cell>QwenViT</cell><cell>Qwen2-7B</cell><cell>3,333×3,333</cell><cell cols="2">49.73</cell><cell cols="2">23.80 38.12</cell><cell cols="2">27.87 34.88</cell></row><row><cell>LLaVA-OV [25]</cell><cell>4.8M</cell><cell>SigLip</cell><cell>Qwen2-7B</cell><cell>2,304×2,304</cell><cell cols="2">53.53</cell><cell cols="2">20.61 35.11</cell><cell cols="2">26.08 33.83</cell></row><row><cell>IXC-2.5 [76]</cell><cell>-</cell><cell>CLIP-L14</cell><cell cols="2">InternLM2-7B 4,096×4,096</cell><cell cols="2">36.12</cell><cell cols="2">25.25 38.41</cell><cell cols="2">27.30 31.77</cell></row><row><cell cols="5">LLaVA-UHD-v2 [81] 1.42M CLIP-L14+JBU Vicuna-1.5-7B 672×1,008</cell><cell cols="2">40.77</cell><cell cols="2">22.82 32.57</cell><cell cols="2">26.08 30.56</cell></row><row><cell cols="2">LLaVA-FlexAttn [27] 1.22M</cell><cell>CLIP-L14</cell><cell cols="2">Vicuna-1.5-7B 1,008×1,008</cell><cell cols="2">37.75</cell><cell cols="2">19.57 29.99</cell><cell cols="2">22.76 27.52</cell></row><row><cell>SEAL [61]</cell><cell>0.95M</cell><cell>CLIP-L14</cell><cell>Vicuna-1.5-7B</cell><cell>-</cell><cell cols="2">30.55</cell><cell cols="2">21.29 34.75</cell><cell cols="2">21.29 26.97</cell></row><row><cell>MGM-HD [30]</cell><cell>3.0M</cell><cell>Mixed</cell><cell cols="2">Vicuna-1.5-7B 1,536×1,536</cell><cell cols="2">31.51</cell><cell cols="2">17.90 35.92</cell><cell cols="2">20.13 26.36</cell></row><row><cell>SliME [82]</cell><cell>2.0M</cell><cell>CLIP-L14</cell><cell cols="2">Vicuna-1.5-7B 672×1,008</cell><cell cols="2">28.54</cell><cell cols="2">17.11 32.09</cell><cell cols="2">22.99 25.18</cell></row><row><cell>LLaVA-1.5 [35]</cell><cell>1.22M</cell><cell>CLIP-L14</cell><cell>Vicuna-1.5-7B</cell><cell>336×336</cell><cell cols="2">26.38</cell><cell cols="2">18.76 30.70</cell><cell cols="2">22.63 24.62</cell></row><row><cell>RSUniVLM [37]</cell><cell>1.2M</cell><cell>SigLip</cell><cell>Qwen2-0.5B</cell><cell>336×336</cell><cell cols="2">25.39</cell><cell cols="2">21.02 32.61</cell><cell cols="2">24.72 25.93</cell></row><row><cell>Geochat [24]</cell><cell>1.53M</cell><cell>CLIP-L14</cell><cell>Vicuna-1.5-7B</cell><cell>504×504</cell><cell cols="2">28.62</cell><cell cols="2">20.18 24.54</cell><cell cols="2">13.75 21.77</cell></row><row><cell>GPT-4o [2]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">28.92</cell><cell cols="2">22.15 31.84</cell><cell cols="2">27.40 27.58</cell></row><row><cell>GPT-4o-mini [2]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">6.69</cell><cell cols="2">18.67 31.99</cell><cell cols="2">25.85 20.80</cell></row><row><cell>Claude-3.5-Sonnet [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">25.74</cell><cell cols="2">12.95 26.69</cell><cell cols="2">13.29 19.67</cell></row><row><cell>Comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LLaVA-1.5*</cell><cell></cell><cell>CLIP-L14</cell><cell></cell><cell>336×336</cell><cell cols="2">34.40</cell><cell cols="2">18.24 32.28</cell><cell cols="2">24.17 27.30</cell></row><row><cell>LLaVA-1.5-p4*</cell><cell></cell><cell>CLIP-L14</cell><cell></cell><cell>672×672</cell><cell cols="2">38.20</cell><cell cols="2">19.18 35.43</cell><cell cols="2">26.50 29.83</cell></row><row><cell>Geochat* SLiME*</cell><cell>1.04M</cell><cell>CLIP-L14 CLIP-L14</cell><cell>Vicuna-1.5-7B</cell><cell>504×504 672×1,008</cell><cell cols="2">33.68 34.56</cell><cell cols="2">21.51 35.97 21.98 33.20</cell><cell cols="2">25.86 29.25 25.10 28.71</cell></row><row><cell>LLaVA-UHD-v2*</cell><cell></cell><cell>CLIP-L14+JBU</cell><cell></cell><cell>672×1,008</cell><cell cols="2">38.55</cell><cell cols="2">20.77 36.57</cell><cell cols="2">25.79 30.42</cell></row><row><cell>Ours (LLaVA-1.5)</cell><cell></cell><cell>CLIP-L14</cell><cell></cell><cell>Dynamic</cell><cell cols="2">39.04</cell><cell cols="2">22.97 36.89</cell><cell cols="2">27.48 31.59</cell></row><row><cell>LLaVA-Next-p4*</cell><cell></cell><cell></cell><cell></cell><cell>672×672</cell><cell cols="2">39.12</cell><cell cols="2">21.06 36.27</cell><cell cols="2">25.80 30.56</cell></row><row><cell>LLaVA-Next-p9* LLaVA-Next-p25*</cell><cell>1.04M</cell><cell>CLIP-L14</cell><cell>Qwen2-7B</cell><cell>1,008×1,008 1,680×1,680</cell><cell cols="2">40.35 39.65</cell><cell cols="2">21.14 37.25 20.99 36.38</cell><cell cols="2">26.10 31.21 26.18 30.80</cell></row><row><cell>Ours (LLaVA-Next)</cell><cell></cell><cell></cell><cell></cell><cell>Dynamic</cell><cell cols="2">41.89</cell><cell cols="2">21.85 38.24</cell><cell cols="2">26.67 32.16</cell></row><row><cell cols="6">Setting anyres-p25 w/ PruMerge++ 43.27 28.38 Color Count Position Acc FPS 41.56 31.05 46.14 39.65 0.188 32.78 34.86 0.152 w/ VisionZip 42.71 24.55 34.55 33.98 0.183 w/ FastV 39.52 30.10 43.99 37.93 0.192 w/ PDrop 41.00 30.54 47.77 39.85 0.184 w/ prune (Ours) 43.98 30.42 49.16 41.28 0.165 DIP-3layer w/ prune (Ours) 44.08 30.94 49.26 41.31 0.267 Table 3. Comparison with different token reduction methods based on LLaVA-Next-Qwen2. To enable a fair comparison, we adopt the "anyres-p25" setting, where the maximum number of pre-defined grids is 25 (1,680×1,680 pixels). FPS is computed based on the inference time for 800 images from MME-Realworld-RS. we reproduce other methods using the same 484k SFT data. Since the LLaVA-NeXT's training code is not open-source, we integrate the anyres strategy and Qwen2 into the LLaVA-1.5 framework, which serves as the codebase for our approach. All the experiments are conducted on 4 NVIDIA A100 80GB GPUs. Setting anyres-p144 w/ PruMerge++ w/ VisionZip w/ FastV w/ PDrop w/ prune (Ours) DIP-4layer w/ prune (Ours)</cell><cell cols="2">Vis. Tokens (Total) 83520 83520 83520 83520 83520 83520 55296</cell><cell cols="2">Vis. Tokens (to LLM) 21312 5328 9280 21312 21312 5760 2376</cell><cell>TFLOPs (B) 243.37 43.75 83.56 109.21 101.62 82.56 36.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>25% 42.47 31.08 47.41 40.40 50% 42.39 31.24 48.45 40.77 75% 43.98 30.42 49.16 41.28 90% 41.51 29.77 46.70 39.41 Ablation study on different prune ratios with LLaVA-Next-Qwen2, when pruning vision tokens based on RFM results without DIP. This table demonstrates that pruning irrelevant tokens in high-resolution RSIs can improve performance.</figDesc><table><row><cell>Method</cell><cell>Drop Ratio</cell><cell cols="3">Color Count Pos</cell><cell>Acc</cell></row><row><cell>anyres-p25</cell><cell cols="5">0% 41.56 31.05 46.14 39.65</cell></row><row><cell cols="2">w/ prune (Ours)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Max Size</cell><cell cols="2">Color Count</cell><cell>Pos</cell><cell>Acc</cell><cell>FPS</cell></row><row><cell cols="2">1,008×1,008 42.87</cell><cell>29.85</cell><cell cols="3">47.89 40.29 0.271</cell></row><row><cell cols="2">2,016×2,016 44.08</cell><cell>30.94</cell><cell cols="3">49.26 41.31 0.267</cell></row><row><cell cols="2">4,032×4,032 44.86</cell><cell>29.85</cell><cell cols="3">47.18 40.72 0.221</cell></row><row><cell cols="2">8,064×8,064 43.98</cell><cell>30.42</cell><cell cols="3">45.11 39.91 0.204</cell></row><row><cell>Dynamic</cell><cell>44.70</cell><cell>31.00</cell><cell cols="3">49.72 41.89 0.238</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on fixing the number of DIP layers reached in inference with LLaVA-Next-Qwen2. Rows 1-4 correspond to fixing the number of DIP layers being 2-5, respectively.</figDesc><table><row><cell>RFM Layers</cell><cell>LLM Layers</cell><cell>Color Count Pos</cell><cell>Acc</cell></row><row><cell>3</cell><cell>[1,8,14]</cell><cell cols="2">41.35 28.30 47.89 39.27</cell></row><row><cell>4</cell><cell>[1,5,11,14]</cell><cell cols="2">44.70 31.00 49.72 41.89</cell></row><row><cell>5</cell><cell cols="3">[1,5,10,12,14] 29.36 48.13 40.58</cell></row><row><cell>6</cell><cell cols="3">[1,3,6,9,12,14] 42.31 31.08 47.18 40.26</cell></row><row><cell cols="4">Table 7. Ablation study on different RFM-LLM layer-pair config-</cell></row><row><cell cols="4">urations in MME-RealWorld-RS, with LLaVA-Next-Qwen2.</cell></row><row><cell cols="4">evaluate 3 closed-source MLLMs. Results in Tab. 2 indicate</cell></row><row><cell cols="4">that LRS-VQA is more challenging than MME-RealWorld-</cell></row><row><cell cols="4">RS on average, reflecting the complexities of large RSIs.</cell></row><row><cell cols="4">Comparison with high-resolution methods. As shown</cell></row><row><cell cols="4">in Tab. 2, we select 5 different high resolution strategies,</cell></row><row><cell cols="4">performing SFT on the same dataset for comparison. Our</cell></row><row><cell cols="3">method performs better across all 4 datasets.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lama Ahmad</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal ; Ilge Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
	</analytic>
	<monogr>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Open</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">Hello gpt-4o. 2024. 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Claude 3.5 sonnet model card addendum</title>
		<author>
			<persName><surname>Ai Anthropic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hired: Attention-guided token dropping for efficient inference of high-resolution vision-language models</title>
		<author>
			<persName><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibn</forename><surname>Arif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyi</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">S</forename><surname>Nikolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Vandierendonck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepu</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12966</idno>
		<title level="m">Qwen-vl: A frontier large vision-language model with versatile abilities</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangwei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erfei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinguo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.05271</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglong</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kongzhi</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Geobench-vlm: Benchmarking vision-language models for geospatial tasks</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Sohail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Akhtar Munir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syed</forename><surname>Roshaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Kuckreja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.19325</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16344" to="16359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Object detection in aerial images: A large-scale benchmark and challenges. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="7778" to="7796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Heatvit: Hardware-efficient adaptive token pruning for vision transformers</title>
		<author>
			<persName><forename type="first">Peiyan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyue</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglun</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenman</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="442" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Llava-uhd: an lmm perceiving any aspect ratio and highresolution images</title>
		<author>
			<persName><forename type="first">Zonghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zanlin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rethinking token reduction in mllms: Towards a unified paradigm for training-free acceleration</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengxiang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siteng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.17686</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Runhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinpeng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08706</idno>
		<title level="m">Hires-llava: Restoring fragmentation input in high-resolution large vision-language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">What&apos;s in the image? a deep-dive into the vision of vision language models</title>
		<author>
			<persName><forename type="first">Omri</forename><surname>Kaduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.17491</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learned token pruning for transformers</title>
		<author>
			<persName><forename type="first">Sehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Thorsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Hassoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="784" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Segment anything</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4015" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spvit: Enabling faster vision transformers via latency-aware soft token pruning</title>
		<author>
			<persName><forename type="first">Zhenglun</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="620" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geochat: Grounded large vision-language model for remote sensing</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Kuckreja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Sohail Danish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03326</idno>
		<title level="m">Llava-onevision: Easy visual task transfer</title>
		<imprint>
			<date type="published" when="2007">2024. 1, 2, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International conference on machine learning</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="19730" to="19742" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flexattention for efficient high-resolution vision-language models</title>
		<author>
			<persName><forename type="first">Junyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A simple aerial detection baseline of multimodal language models</title>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.09720</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Wentong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.02392</idno>
		<title level="m">Tokenpacker: Efficient visual projector for multimodal llm</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuechen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.18814</idno>
		<title level="m">Mini-gemini: Mining the potential of multi-modality vision language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to holistically detect bridges from large-size vhr remote sensing imagery</title>
		<author>
			<persName><forename type="first">Yansheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Llama-vid: An image is worth 2 tokens in large language models</title>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Star: A first-ever dataset and a large-scale benchmark for scene graph generation in large-size satellite imagery</title>
		<author>
			<persName><forename type="first">Yansheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingzhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Junchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monkey: Image resolution and text label are important things for large multi-modal models</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="26763" to="26773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03744</idno>
		<title level="m">Improved baselines with visual instruction tuning</title>
		<imprint>
			<date type="published" when="2007">2023. 1, 2, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rsunivlm: A unified vision language model for remote sensing via granularity-oriented mixture of experts</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouhui</forename><surname>Lian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.05679</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Textmonkey: An ocr-free large multimodal model for understanding document</title>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04473</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rsvqa: Visual question answering for remote sensing data</title>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Lobry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="8555" to="8566" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training</title>
		<author>
			<persName><forename type="first">Gen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaokai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.08202</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Feast your eyes: Mixture-ofresolution adaptation for multimodal large language models</title>
		<author>
			<persName><forename type="first">Gen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.03003</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Skysensegpt: A fine-grained instruction tuning dataset and model for remote sensing visionlanguage understanding</title>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingzhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangwei</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihua</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10100</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adavit: Adaptive vision transformers for efficient image recognition</title>
		<author>
			<persName><forename type="first">Lingchen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bor-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12309" to="12318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lhrs-bot: Empowering remote sensing with vgi-enhanced large multimodal language model</title>
		<author>
			<persName><forename type="first">Dilxat</forename><surname>Muhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfeng</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="440" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Gpt-4v(ision) system card</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="13937" to="13949" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Akashah</forename><surname>Shabbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zumri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Fahad S Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.13925</idno>
		<title level="m">Geopixel: Pixel grounding large multimodal model in remote sensing</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Yuzhang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.15388</idno>
		<title level="m">Llava-prumerge: Adaptive token reduction for efficient large multimodal models</title>
		<imprint>
			<date type="published" when="2008">2024. 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengju</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuofan</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Letian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8612" to="8642" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration</title>
		<author>
			<persName><forename type="first">Haozhan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.16044</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Dudhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiyam</forename><surname>Debary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustansar</forename><surname>Fiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Akhtar Munir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Sohail Danish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levente</forename><forename type="middle">J</forename><surname>Campbell D Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><surname>Shahbaz Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.15190</idno>
		<title level="m">Turning multi-sensory earth observations to interactive dialogues</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fair1m: A benchmark dataset for finegrained object recognition in high-resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingchao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="116" to="130" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cambrian-1: A fully open, vision-centric exploration of multimodal llms</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellis</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adithya</forename><surname>Jairam Vedagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iyer</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Charitha Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Middepogu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziteng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87310" to="87356" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Earthvqa: Towards queryable earth via relational reasoning-based remote sensing visual question answering</title>
		<author>
			<persName><forename type="first">Junjue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5481" to="5489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Qwen2-vl: Enhancing vision-language model&apos;s perception of the world at any resolution</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12191</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Divide, conquer and combine: A training-free framework for high-resolution image perception in multimodal large language models</title>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiabin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Longllava: Scaling multi-modal llms to 1000 images efficiently via a hybrid architecture</title>
		<author>
			<persName><forename type="first">Xidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingjie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.02889</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint token pruning and squeezing towards more aggressive compression of vision transformers</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2092" to="2101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">V*: Guided visual search as a core mechanism in multimodal llms</title>
		<author>
			<persName><forename type="first">Penghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Foundation models for remote sensing and earth observation: A survey</title>
		<author>
			<persName><forename type="first">Aoran</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Yokoya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.16602</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Next-qa: Next phase of question-answering to explaining temporal actions</title>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9777" to="9786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qidong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.17247</idno>
		<title level="m">Accelerating your large vision-language models via pyramid visual redundancy reduction</title>
		<imprint>
			<date type="published" when="2008">2024. 2, 4, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Xizhe</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoting</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.16583</idno>
		<title level="m">Reo-vlm: Transforming vlm to meet regression challenges in earth observation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.09564</idno>
		<title level="m">Tg-llava: Text guided llava via learnable latent embeddings</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfeng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.15115</idno>
		<title level="m">Qwen2.5 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Senqiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Visionzip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.04467</idno>
		<title level="m">Longer is better but not necessary in vision language models</title>
		<imprint>
			<date type="published" when="2008">2024. 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Linli</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhuai</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><surname>Deco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20985</idno>
		<title level="m">Decoupling token compression from semantic abstraction in multimodal large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11618</idno>
		<title level="m">Tr-bert: Dynamic token reduction for accelerating bert inference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model</title>
		<author>
			<persName><forename type="first">Jiabo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Fit and prune: Fast and training-free visual token pruning for multi-modal large language models</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.10197</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Voco-llama: Towards vision compression with large language models</title>
		<author>
			<persName><forename type="first">Xubing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoke</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.12275</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Skyeyegpt: Unifying remote sensing vision-language tasks via instruction tuning with large language model</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linke</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.03320</idno>
		<title level="m">Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Llava-mini: Efficient image and video large multimodal models with one vision token</title>
		<author>
			<persName><forename type="first">Shaolei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.03895</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Earthmarker: A visual prompting multimodal large language model for remote sensing</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaoxin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuerui</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Earthgpt: A universal multi-modal large language model for multi-sensor image comprehension in remote sensing domain</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaoxin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuerui</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.16822</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Sparsevlm: Visual token sparsification for efficient vision-language model inference</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Kai</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Gudovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoyuki</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohei</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.04417</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Llava-uhd v2: an mllm integrating highresolution feature pyramid via hierarchical window transformer</title>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuesong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.13871</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Beyond llava-hd: Diving into high-resolution large multimodal models</title>
		<author>
			<persName><forename type="first">Yi-Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.08487</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Yi-Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.13257</idno>
		<title level="m">Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Enhancing ultra high resolution remote sensing imagery analysis with imagerag</title>
		<author>
			<persName><forename type="first">Zilun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongheng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.07688</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Towards vision-language geo-foundation models: A survey</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09385</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
