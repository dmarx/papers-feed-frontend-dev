# SELF-ALIGNMENT WITH INSTRUCTION BACKTRANS-LATION

## Abstract

## 

We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.

## INTRODUCTION

Aligning large language models (LLMs) to perform instruction following typically requires finetuning on large amounts of human-annotated instructions or preferences [(Ouyang et al., 2022;](#b24)[Touvron et al., 2023a;](#)[Bai et al., 2022a)](#) or distilling outputs from more powerful models [(Wang et al., 2022a;](#)[Honovich et al., 2022;](#b16)[Taori et al., 2023;](#b33)[Chiang et al., 2023;](#b6)[Peng et al., 2023;](#b26)[Xu et al., 2023)](#b40). Recent work highlights the importance of human-annotation data quality [(Zhou et al., 2023;](#b43)[Köpf et al., 2023)](#b18). However, annotating instruction following datasets with such quality is hard to scale.

In this work, we instead leverage large amounts of unlabelled data to create a high quality instruction tuning dataset by developing an iterative self-training algorithm. The method uses the model itself to both augment and curate high quality training examples to improve its own performance. Our approach, named instruction backtranslation, is inspired by the classic backtranslation method from machine translation, in which human-written target sentences are automatically annotated with model-generated source sentences in another language [(Sennrich et al., 2015)](#b30).

Our method starts with a seed instruction following model and a web corpus. The model is first used to self-augment its training set: for each web document, it creates an instruction following training example by predicting a prompt (instruction) that would be correctly answered by (a portion of) that document. Directly training on such data (similarly to [Köksal et al. (2023)](#b17)) gives poor results in our experiments, both because of the mixed quality of human written web text, and noise in the generated instructions. To remedy this, we show that the same seed model can be used to self-curate the set of newly created augmentation data by predicting their quality, and can then be self-trained on only the highest quality (instruction, output) pairs. The procedure is then iterated, using the improved model to better curate the instruction data, and re-training to produce a better model.

Our resulting model, Humpback, outperforms all other existing non-distilled models on the Alpaca leaderboard [(Li et al., 2023)](#b19). Overall, instruction backtranslation is a scalable method for enabling language models to improve their own ability to follow instructions.

## METHOD

Our self-training approach assumes access to a base language model, a small amount of seed data, and a collection of unlabelled examples, e.g. a web corpus. The unlabelled data is a large, diverse set Figure [1](#): An overview of our instruction backtranslation method. We start from a base language model, e.g. LLaMa, a small amount of seed examples of (instruction, output) pairs, and a collection of unlabelled documents which are considered candidate outputs for unknown instructions. Selfaugmentation: the base model is finetuned with (output, instruction) pairs from the seed examples as an instruction prediction model M yx , which is used to generate candidate instructions for outputs from the unlabelled data. Self-curation: starting from an intermediate instruction-following model M 0 finetuned from seed examples only, it selects high-quality (instruction, output) pairs A

(1) k from the candidates from the previous step, and uses them as finetuning data for the next intermediate model M 1 , which is in turn used to select training data for obtaining M 2 . of human-written documents which includes writing about all manner of topics humans are interested in -but crucially is not paired with instructions. A first key assumption is that there exists some subset of this very large human-written text that would be suitable as gold generations for some user instructions. A second key assumption is that we can predict instructions for these candidate gold answers that can be used as high quality example pairs to train an instruction following model.

Our overall process, which we call instruction backtranslation, thus performs two core steps: 1. Self-augment: Generate instructions for unlabelled data, i.e. the web corpus, to produce candidate training data of (instruction, output) pairs for instruction tuning.

2. Self-curate: Self-select high quality demonstration examples as training data to finetune the base model to follow instructions. This approach is done iteratively where a better intermediate instruction-following model can improve on selecting data for finetuning in the next iteration.

We describe these steps in more details below. An overview of the approach is illustrated in Figure [1](#).

## INITIALIZATION

Seed data. We start with a seed set of human-annotated (instruction, output) examples that will be used to fine-tune language models to give initial predictions in both directions: predicting an output given an instruction, and an instruction given an output.

Unlabelled data. We use a web corpus as a source of unlabelled data. For each document, we perform preprocessing to extract self-contained segments {y i }, which are portions of text following an HTML header. We further run deduplication, length filtering, and remove potential low quality segments with several heuristics such as the proportion of capitalized letters in the header.

## SELF-AUGMENTATION (GENERATING INSTRUCTIONS)

We finetune the base language model with (output, instruction) pairs {(y i , x i )} from the seed data to obtain a backward model M yx := p(x|y). For each unlabelled example y i , we run inference on the backward model to generate a candidate instruction xi from which we derive the candidate augmented paired data A := {( xi , y i )}. As we will see in experiments, not all of these candidate pairs are of high quality, and in that case using them all for self-training may not be beneficial. We thus consider the important next step of curation of a high quality subset.

## SELF-CURATION (SELECTING HIGH-QUALITY EXAMPLES)

We select high quality examples using the language model itself. We start with a seed instruction model M 0 finetuned on (instruction, output) seed examples only. We then use M 0 to score each augmented example {(x i , y i )} to derive a quality score a i . This is done using prompting, instructing the trained model to rate the quality of a candidate pair on a 5-point scale. The precise prompt we use is given in Table [19](#tab_10). We can then select a subset of the augmented examples with score a i ≥ k to form a curated set A

(1)

k .

Iterative self-curation We further propose an iterative training method to produce higher quality predictions. On iteration t we use the curated augmentation data A (t-1) k from the previous iteration, along with the seed data as training data to finetune an improved model M t . This model in turn can be used to rescore the augmented examples for quality, resulting in an augmentation set A (t) k . We perform two iterations of data selection and finetuning to get the final model M 2 .

When combining both seed data and augmented data for finetuning, we use tagging to distinguish these two data sources. Specifically, we append an additional sentence to examples (called "system prompt"). We use S a := "Answer in the style of an AI Assistant." for seed data, and S w := "Answer with knowledge from web search." for augmented data. This approach is similar to methods used to tag synthetic data for backtranslation in machine translation [(Caswell et al., 2019)](#b4).

## EXPERIMENTS

## EXPERIMENTAL SETUP

Seed data. We use 3200 examples from the Open Assistant dataset [(Köpf et al., 2023)](#b18) as humanannotated seed data to train our models. Each example is an (instruction, output) pair {(x i , y i )}, chosen from the first turn of the conversation tree. We only sample English language responses that are high quality, based on their human annotated rank (rank 0).

Base model & finetuning. We use the pretrained LLaMA model [(Touvron et al., 2023a)](#) with 7B, 33B and 65B parameters as the base models for finetuning. During training, we only optimize the loss on the output tokens, not the input tokens, thus deviating from the standard language modeling loss. We use the same hyperparameters as existing supervised finetuning (SFT) methods [(Zhou et al., 2023;](#b43)[Touvron et al., 2023a)](#) for most models: learning rate 1e -5 which linearly decays to 9e -6 at the end of training, weight decay 0.1, batch size 32 (examples) and dropout 0.1. For finetuning with less than 3000 examples we use batch size 8 (more details in Table [18](#tab_14)). We refer to our trained Llama-based instruction backtranslation model as Humpback[foot_0](#foot_0) . For generation, we use nucleus sampling [(Holtzman et al., 2019)](#b15) with temperature T = 0.7, p = 0.9.

Unlabelled data. We use the English portion of the Clueweb corpus as the source of unlabelled data [(Overwijk et al., 2022)](#b25). Among those, we sampled 502k segments.

Baselines. The main baselines we compare to are the following approaches:

• text-davinci-003 [(Ouyang et al., 2022)](#b24): an instruction following model based on GPT-3 finetuned with instruction data from human-written instructions, human-written outputs, model responses and human preferences using reinforcement learning (RLHF). • LIMA [(Zhou et al., 2023)](#b43): LLaMA models finetuned with 1000 manually selected instruction examples from a mixture of community question & answering (e.g. StackOverflow, WikiHow, etc.) and human expert-written instruction and responses.

• Guanaco [(Dettmers et al., 2023)](#b9): LLaMA models finetuned with 9000 examples from the Ope-nAssistant dataset. The difference from the 3200 seed examples used in this paper is that Guanaco includes (instruction, output) pairs from all turns while we only used the first-turn.

We additionally report comparisons to various other models, e.g. which use data distilled from larger and more powerful models such as GPT-4, but do not consider them as directly comparable to our LlaMa-based approach.

Evaluation. We evaluate on test prompts from several sources: Vicuna [(Chiang et al., 2023](#b6)) (80 prompts), Self-instruct [(Zhang & Yang, 2023](#)) (252 prompts), Open Assistant [(Köpf et al., 2023](#b18)) (188 prompts), Koala [(Geng et al., 2023](#b12)) (156 prompts), HH_RLHF [(Bai et al., 2022a](#)) (129 prompts), LIMA [(Zhou et al., 2023](#b43)) (300 prompts), crowdsourced from authors (64 prompts). In total there are 1130 unique prompts, providing a good coverage on a variety of task categories, e.g. writing, coding, mathematical reasoning, information seeking, advice, roleplay, safety, etc. We sample 256 prompts from them excluding those in the AlpacaEval test set as a dev set. We ran both automatic evaluation using AlpacaEval [(Li et al., 2023)](#b19), which computes the win rate against baseline models based on GPT-4 judgements, as well as human preference evaluation.

## SEED AND AUGMENTATION DATA STATISTICS

Data statistics. In Table [1](#tab_0) we provide the statistics of the seed data as well as various versions of the augmented data. We can see that augmented data tends to have longer outputs compared to the seed data, and self-curated higher quality training data (A

(2) 4 and A

(2) 5 ) has both shorter instructions and outputs among all augmented data, closer to the length of the original seed instruction data. Generated Instructions. We conduct the task diversity analysis of the seed data and augmented data using the approach from [Wang et al. (2022a)](#). Figure [6](#) visualizes the distribution of the verb-noun structure of instructions in the seed data and augmented data (A (2) 5 category) respectively. Similar to the seed data, there are a few head tasks related to writing, information seeking and advice, although the type of content from unlabeled data (article, recipe, description, release, etc.) complements those in the seed data [(essay, script, code, story, etc.)](#). The augmented data increases the task diversity especially in the long tail.

## SCALING ANALYSIS

Data quality vs. data quantity. In order to understand the importance of data quality vs. data quantity in learning to follow instructions, we compared finetuning on augmented data of different quality. Specifically, we compared finetuning on augmented data without quality-based selection (w/o curation), self-selected data in A

(2) 4 (score ≥ 4) and A

(2) 5 (score ≥ 4.5) categories. Results are shown in Figure [2](#). We find that training on augmented data without self-curation does not improve instruction following performance despite scaling up data quantity. However, training on the high quality portion of the augmented data leads to increasing instruction following performance, with steady improvement as we continue to scale up the amount of augmented data. Prior work proposed 

$/o self-curation (x i , y i ) (2) (x i , y i ) (2)$Figure [2](#): Evaluating self-augmented data of different data size and quality using self-curation. The y-axis is the win rate against text-davinci-003 when finetuning 7B LLaMa with the given data size and quality. We compare three augmentation datasets: without self-curation, A

(2) 4 and A

(2) 5 that are progressively smaller augmentation sets but of higher data quality (see Table [1](#tab_0) for statistics). Similar to observations in LIMA using human-annotated data [(Zhou et al., 2023)](#b43), improving the quality of the training data dramatically improves the quality of the model, despite the smaller dataset size.

the "superficial alignment hypothesis", that only a few thousands of high-quality instruction following examples are sufficient for aligning a pretrained base model to follow instructions [Zhou et al. (2023)](#b43). Our results provide a contrasting observation that increasing the quantity of high-quality data provides further gains (whereas increased quantities of low-quality data does not).

Data scaling efficiency. We compare the performance of various instruction-following models as we alter the amount of instruction following finetune data they use. We measure the win rate of each model against text-davinci-003 when finetuning 7B LLaMa with the given finetune dataset. We also report an estimate of this efficiency using the data scaling coefficient α, which is calculated by fitting empirical data with w = α log N + C, where w is the win rate measuring generation quality of the model finetuned on N examples.

We compare our instruction backtranslation method (self-augmentation and self-curation with k = 5, 2 iterations) to methods using instruction datasets created from different sources.

Table 2: Scaling coefficient α of representive instruction datasets created using differnet methods and data sources. Source α ↑ Humpback (this work) OA, self-augmented and self-curated 6.95 WizardLLM[foot_1](#foot_1)[(Xu et al., 2023)](#b40) Distilled from ChatGPT, GPT-4 (June 2023) 5.69 Alpaca-GPT4 [(Peng et al., 2023)](#b26) Distilled from GPT-4 (April 2023) 5.40 Vicuna [(Chiang et al., 2023)](#b6) Distilled from ChatGPT, GPT-4 (June 2023) 4.53 Open Assistant (OA) [(Köpf et al., 2023)](#b18) Human Annotation 4.43 LIMA [(Zhou et al., 2023)](#b43) Human Annotation, Community QA 2.86 Alpaca [(Taori et al., 2023)](#b33) Distilled from ChatGPT (March 2023) 1.99 FLAN v2 [(Chung et al., 2022)](#b7) Instruction data for NLP tasks 0.22

Results are shown in Figure [3](#fig_2), with the estimated scaling coefficient α summarized in Table [2](#). We find that most distilled instruction datasets have better data efficiency than datasets created from other sources, e.g. NLP tasks (FLAN v2) or extracted from community Q&A (LIMA). Both improving instruction diversity (e.g. WizardLLM vs. Vicuna) and response quality (e.g. Alpaca-GPT4 vs. Alpaca) seem to yield better data efficiency. Scaling up augmented data using the A 5 data achieved  Dashed lines depict models that use distillation from more powerful models to construct data, and methods with solid lines do not. both higher instruction following performance and more efficient data scaling. We provide further analysis on jointly scaling data and model size in Appendix B.

## MODEL QUALITY

AlpacaEval. We use the automatic evaluation (using GPT-4) from AlpacaEval to evaluate generation quality on 805 prompts from the Alpaca Leaderboard. AlpacaEval compares the pairwise win rate against the reference model text-davinci-003. We compare our method's performance among three categories of instruction models:

• Non-distilled: LLaMa models trained without relying on any external model (e.g. ChatGPT, GPT-4, etc.) for any form of supervision. Most models in this category heavily rely on human annotated data. • Distilled: models trained with a more powerful external model in the loop, e.g. using data distilled from an external model. • Proprietary: models trained with proprietary data and techniques.

Results are given in Table [3](#tab_2). Our method is the top-performing model among non-distilled models at both 65B and 33B model scales. We note that Guanaco and OASST are trained on the same data source as our seed data, but with more annotated examples. We also evaluated Humpback based on LLaMa 2 [(Touvron et al., 2023b)](#) 70B to verify its performance further improves with stronger base model.

Human Evaluation. We also conduct human evaluation on the general quality of the model responses on the combined test set described in Section 3.1, which covers several existing benchmarks. For each prompt, we present outputs from two models side-by-side, comparing our method to a given baseline model, and ask the human evaluator to choose from three options: 1) output from the first model is significantly better than the second model; 2) output from the second model is significantly better than the first model; 3) there is no significant difference between the two outputs. We randomize the order the models are presented in to avoid position bias. Figure [4](#) summarizes the comparison with both open source and proprietary models. We can see that the human preference distribution is roughly consistent with the preference distribution using GPT-4 as the judge from AlpacaEval, corroborating observations from [Li et al. (2023)](#b19), [Zhou et al. (2023)](#b43) and [Zheng et al. (2023)](#b42).

Commonsense Reasoning and MMLU. We evaluate on five commonsense reasoning benchmarks, SIQA [(Sap et al., 2019)](#b28), PIQA [(Bisk et al., 2020)](#b3), Arc-Easy [(Clark et al., 2018)](#b8), Arc-Challenge  [(Ouyang et al., 2022)](#b24) and Claude [(Bai et al., 2022a](#))) instruction-tuned models in pairwise human preference judgements. [(Clark et al., 2018)](#b8), and Openbook QA (OBQA) [(Mihaylov et al., 2018)](#b21), which measures reasoning ranging from social interactions to grade 3 to 9 science questions. We compute zero-shot accuracy based on perplexity of the correct answer following LLaMa [(Touvron et al., 2023a)](#). We also evaluate on the massive multitask language understanding (MMLU) [(Hendrycks et al., 2020)](#b14) benchmark.

The results are summarized in Table [4](#tab_3). We found that compared to the base model, our model has improved zero-shot performance on social reasoning, challenging science problems which require more reasoning (Arc-C), Openbook QA and MMLU. Detailed results by domains are included in Appendix B.

## ABLATIONS

We perform further ablation studies to understand the effectiveness of self-augmented data in our method.  Training on self-augmented data only. As is shown in Figure [5](#fig_3), when training on self-augmented data alone (without seed data), and without self-curation, the quality of instruction following does not improve, or even deteriorates with more data. However, training on the higher quality self-curated data brings improvements as training set size increases. While this self-curated data does not outperform seed training data scaling alone, when joint training with both seed and self-augmented data we observe large improvements. This indicates that seed data and augmented data are complimentary, where the seed data has the same distribution as the target domain (AI assistant response), while the data from web corpus may enlarge the diversity of the instructions and outputs. In Appendix B provides further qualitative analysis to illustrate the improvement over training with seed data alone.

System prompts. In Table [5](#tab_4), we disentangle the effects of system prompts in joint finetuning and during inference. We found adding system prompts to distinguish augmented data from seed data is helpful. Interestingly, using a combined system prompt {S a , S w } at inference time, which concatenates the one for the seed data with the one for augmented data, is better than either no system prompt or using the seed data prompt, despite that the concatenation was not seen during training.

## RELATED WORK

Instruction tuning for LLMs. Our work shares the same goal as the broad category of efforts on finetuning large language models to follow instructions. Early work on instruction tuning mainly focused on NLP tasks, with the finding that finetuning with NLP datasets formatted as instructionoutput pairs improves cross-task generalization [(Wei et al., 2021;](#b39)[Mishra et al., 2021;](#b22)[Sanh et al., 2021;](#b27)[Wang et al., 2022b)](#). Recent work [Ouyang et al. (2022)](#b24) extends instruction tuning to a broader range of general tasks, especially incorporating instructions from users of language models.

## Instruction generation and curation.

A key challenge to enable LLMs to perform general instruction-following is gathering demonstration examples for finetuning. Existing high-quality instruction-following LLMs rely on human annotations in various steps, including writing instructions, writing model responses, providing preferences to indicate desired response, etc. Those instruction sets are often proprietary, one exception being the recent OpenAssistant datasets (Köpf  [, 2023)](#). Overall, the human annotation approach is difficult to scale since collecting annotations on a wide range of tasks is expensive, time consuming and requires expertise in different domains.

Several works have explored using LLMs to generate instructions. Unnatural instructions prompts GPT-3 to generate more instructions given a few in-context seed instructions [(Honovich et al., 2022)](#b16). Self-instruct [(Wang et al., 2022a)](#) uses the same approach to generate instructions, as well as outputs for those instructions. They further perform manually engineered filtering rules to remove low-quality instruction-output pairs. Xu et al. ( [2023](#)) generates more complex instructions by creating variants of user instructions sent to ChatGPT.

All these approaches use model-generated responses for training data. More similar to our method is the concurrent work of [Köksal et al. (2023)](#b17), which takes human-written text as a natural response, and uses the LLM to generate the corresponding instruction conditioning on the response. A critical difference in our work is that we show that the self-curation step is vital to improve such a procedure. A further difference is that they use distillation via an instruction tuned LLM (InstructGPT) to generate instructions, while our approach does not rely on distilling from a more powerful model in the loop, and is instead an instance of self-alignment.

Self-alignment. Our work is an instance of the growing body of work on self-alignment, i.e. utilizing the model to improve itself and align its response with desired behaviors such as modelwritten feedback, critique, explanations, etc. Differently to our work, many of these works either construct training data in an unsupervised way [(Sun et al., 2023;](#b32)[Bai et al., 2022b)](#), whereas we augment human-written web pages, or they use the model to generate additional context to condition on at inference time to improve the output [(Saunders et al., 2022;](#b29)[Zhang & Yang, 2023;](#)[Madaan et al., 2023)](#b20).

Data quality. Several approaches have shown that curating high-quality human-written data results in strong performance, for example PALMS [(Solaiman & Dennison, 2021)](#b31) and LIMA [(Zhou et al., 2023)](#b43). Instead of manually curating high-quality data, our work focus on selecting high-quality using the model itself. In concurrent work, Chen et al. ( [2023](#)) also provides an algorithmic approach to select high quality data. They differ from our work in that they prompt a stronger model (ChatGPT) to score the quality of model generated responses from distillation, while this work scores the quality of human-written data as a response to a self-generated instruction.

Distillation. Most finetuned LLaMA models are based on knowledge distillation from ChatGPT or GPT-4, such as Alpaca [(Taori et al., 2023)](#b33), Alpaca-GPT 4 [(Peng et al., 2023)](#b26), Vicuna [(Chiang et al., 2023)](#b6), FalconInstruct [(Almazrouei et al., 2023)](#b0), OpenChat [(Wang et al., 2023)](#b5), UltraChat [(Ding et al., 2023)](#b10). Hence, these approaches require that you already have a strong model, but do not provide a recipe for building a strong model from scratch. Drawbacks of these approaches are also discussed in [Gudibande et al. (2023)](#b13).

## CONCLUSION

We proposed a scalable approach to finetune large language models to follow instructions. Our method leverages large amounts of unlabeled data by developing an iterative self-training algorithm that we dub instruction backtranslation. Our method uses the model itself to both augment and curate high quality training examples to improve its own performance. On the Alpaca leaderboard, our finetuned models outperform all other non-distilled instruction-following models, while using fewer human annotated examples. Future work should scale this method further by considering larger unlabeled corpora, which our analysis suggests should yield further gains.

## A LIMITATIONS

A.1 BIAS Since the augmented data is sourced from a web corpus, one potential consequence is that the finetuned model could amplify biases from web data. We evaluate on the CrowS-Pairs dataset [Nangia et al. (2020)](#b23) to measure the model's performance in recognizing potential bias. Specifically, we evaluate the accuracy in detecting biased statements in nine categories: gender, religion, race/color, sexual orientation, age, nationality, disability, physical appearance and socioeconomic status. Compared to the base model, our model has improved accuracy in detecting biases as is summarized in Table [6](#tab_5). However, this does not mean our model is less likely to generate responses that contain biases. We found that for these set of prompts the model tends to produce a cautious response, or even refuses to provide information to fulfill the instruction. Further, we compared responses using different system prompts and found that using the seed data's system prompt S a tends to yield safer responses. This indicates that leveraging system prompts could be an effective solution to enhance safety. Table [15](#tab_4) provides representative examples. Incorporating red teaming or other safety measures into our augmentation procedure could be a further avenue to explore, in particular existing work has shown that instruction following models are capable of "morally self-correcting" to mitigate producing harmful responses when instructed to do so [Ganguli et al. (2023)](#b11). 

## B ADDITIONAL RESULTS

Instruction diversity. Figure [6](#) visualizes the distribution of the verb-noun structure of instructions in the seed data and augmented data (A

(2) 5 category) respectively.

Jointly scaling of data and model. We verify that the data scaling trends observed in the 7B models also holds in larger models. As is shown in Figure [7](#), the 65B seed model is a strong baseline, however adding high quality augmented data A 5 brings further improvement.

MMLU. Table [7](#tab_7) summarizes results on massive multitask language understanding (MMLU) [(Hendrycks et al., 2020)](#b14). Compared to the base model, our finetuned model has improved zero-shot accuracy across all domains, while underperforming the base model with 5-shot in-context examples.

Improvement over seed model. Adding self-augmented data improved the failure cases of the seed model for 16% of test prompts (41 out of 251). We observe improved responses for several categories: reasoning, information seeking, giving detailed advice, etc. as shown in Table [8](#tab_8).

Table 11, 12, 13 and 14 provides qualitative examples how adding augmented improves the response quality.  [9](#tab_10), self-curation performance is improved in the second iteration (using M 1 vs. M 0 ) in terms of selecting high quality data (Precision/Recall). Further, this also corresponds to better instruction following when finetuning on the selected data, as shown by the Win Rate. A key observation is that although the intermediate models do not have very high precision, training on the selected data still improves instruction following. This helps explain the effectiveness of our method.

## C GENERATION SAMPLES

Generated instructions. Table [10](#tab_0) includes examples of the generated instructions.

Sample outputs with improvement over the seed model. 

## D HUMAN EVALUATION

We carry out our human evaluation using the Mephisto platform[foot_2](#foot_2) with Mturk workers. As identified in [Bai et al. (2022a)](#), we note that while Mturk workers are often able to produce data at a faster rate, there is typically a trade-off in terms of quality. Consequently, it necessary to implement a rigorous selection process for these workers. Qualifications. (i) Percent Assignments Approved: The percentage of assignments the Worker has submitted that were subsequently approved by the Requester, over all assignments the Worker has submitted. We set the approved rate to be equal or larger than 99%. (ii) Number HITs Approved:

The total number of HITs submitted by a Worker that have been approved. We set the number to be equal or larger than 1000. (iii) Locale: The location of the Worker, as specified in the Worker's mailing address. We set the locations requirement to be the United States of America, Great Britain, Australia, New Zealand, Canada, Ireland. (iv) Master Qualification: Initially, we mandated that only workers have a Master Qualification could complete our HITs. However, upon evaluation, we found that the quality of work provided by masters was not significantly superior, yet it incurred higher costs. Consequently, we have decided not to include this as a qualification requisite in our final configurations.

Screening Tests In the process of our screening test, we selected 200 prompts from the Pushshift Reddit and Stack Exchange datasets, and then utilized LIMA-7B [Zhou et al. (2023)](#b43) to generate two distinct responses per prompt. Subsequently, an in-house evaluation was conducted, involving four of our team's researchers, who were asked to express their preference as depicted in Figure [8](#fig_5). Notably, this process deviates from our live launch procedure. During these screening tests, we require annotators to not only select a preferred response but also provide written rationale for their choice.

We curated a selection of 10 examples adhering to the following criteria: (i) 100% agreement within 4 annotators; (ii) the assigned label from our in-house human raters should not fall under the "neither" category; (iii) the samples should present a discerning choice for the annotators, meaning they should not contain any random words or be straightforward to decide upon. It's essential for the annotators to thoroughly read and analyze before making a choice.

We conducted a screening test using 10 examples and selected annotators based on the following criteria: (i) those who achieved an agreement rate exceeding 85% with our in-house annotators (considering 'neither' choices as half agreements). The distribution of agreement during the screening test is illustrated in Figure [9](#fig_6). (ii) We also manually examined the justifications provided by the annotators, filtering out those whose reasons were nonsensical or lacking coherence. After assessing accuracy and manually inspecting their rationales, we chose 29 workers from a pool of 1,000 applicants.

## D.2 ANNOTATION INTERFACE.

We conducted all our annotation tasks with the 29 selected annotators from the screening test. Communication with our annotators was maintained via email to ensure that they were being compensated fairly and to allow them to alert us to any problems or issues. The user interface used for gathering the pairwise preferences from our human evaluators is provided in Figure [10](#fig_7) and Figure [11](#fig_8).    

## Output:

Diamond engagement rings gained in popularity during the Art Deco era with the round old European cut diamond being the favourite.

## ### Asscher Cut

The Asscher cut is one of the first patented diamond cuts in the world and was invented by Dutch master diamond cutter, Joseph Asscher of the Royal Asscher Diamond Company in 1902. Classic asscher cut diamonds are cut into squares and resemble emerald cuts, which are rectangular. Asscher cut diamonds are different to a square emerald cut in that they have larger step facets, a higher crown, smaller table and have more brilliance. The corners are cropped to give the shape an octagonal appearance. ### Baguette Cut Although the baguette cut was invented sometime prior to the mid-1500s, it only gained popularity in 1912 when Cartier reintroduced the cut to the modern world. Its elongated, table cut, rectangular shape became highly fashionable in the geometric craze of the Art Deco period.

## ### Emerald Cut

The emerald diamond cut emerged as one of the first faceted diamond cuts, third in line after the point cut and the table cut. The cut has a dramatic hall of mirrors effect and was standardised in the 1940s. Generated instruction: List the most popular diamond cuts in the Art Deco era.

## Output:

Inclusive Sports Coaching provides 1:1 Programs for individuals looking to develop their sporting skills, as well as improve their self confidence and opportunities for social and community inclusion.

We recommend an 8 or 12 Session program to identify areas for improvement and sporting skills, conduct drills and physical activities to work towards specific outcomes, while engaging with the client in areas such as listening, memory retention, cognitive processing, social interaction, encouraging conversations, accepting and giving constructive feedback, and other areas as needed.

At the halfway point we produce a status report on progress, and have found parents/carers often share this with OT's, Physios and Teachers as a way to share information on the individual and provide a strong network of support.

At the end of the program we produce a final report, with recommendations for ongoing improvement, potential for progress along the person's chosen sport pathway where applicable, etc.

## Generated instruction:

I have a business called Inclusive Sports Coaching. We provide 1:1 sport coaching for people with disabilities. I want to have some materials on hand to give to parents when they enquire about our services. What do you recommend I include in these materials?

Table 10: Examples of instructions generated by the backward model M b .

## E MORE EXPERIMENT DETAILS

Preprocessing. We parse the warc files of ClueWeb in HTML format to extract segments. Each segment is a tree rooted at a header node, including subtrees from lower-level headers. We applied the following filters before sampling segments:

• Length: total length of text between 600 and 3000 characters.

• Duplication: we remove segments with repetitive sentences by computing jaccard similarity of ngrams from pairs of sentences in the segment.

• Header quality: We remove segments when containing an empty header or the text is all uppercase, header contains navigation text such as "advertisement", "forum", "quick link", "free newsletter", etc. We can start by converting the minutes into hours: 20 minutes = 0.33 hours 25 minutes = 0.42 hours Total time = 0.75 hours Next, we can calculate his average speed using the distance and total time: Average speed = Total distance / Total time Average speed = 3 miles / 0.75 hours Average speed = 4 miles per hour Therefore, Jeff's average speed for the whole jog was 4 miles per hour. Training. For experiment on data scaling efficiency, models were trained with increasing number of examples N for each dataset.

For fair comparison, for each N ∈ {100, 800, 1600, 3200, 6400, 12800, 25600, 51200}, all datasets were trained for the same number of steps with the same batch size as is shown in Table [18](#tab_14).

Table [12](#tab_0): Improvement over seed model in information seeking.

Prompt: Provide a detailed explanation of the steps that are required in order to construct the given project. Steam Engine Using an Empty Beer Can M 0

Here are the steps to construct a steam engine using an empty beer can: Gather materials: To build this project, you'll need an empty beer can, a cork that fits snugly in the opening of the can, a plastic straw, a rubber band, duct tape, and fins made from cardboard or plastic. Prepare the can: Cut off the top of the can, and use a file or sandpaper to smooth the edges. Then, cut out a rectangle from the bottom of the can, and attach fins to the sides with duct tape. Make the piston: Cut a piece of plastic straw to fit inside the can, and attach a cork to one end of it. Use duct tape to seal the other end of the straw, leaving enough space for air to flow through. Assemble the engine: Insert the piston into the can, and place the cork side down. Then, use a rubber band to connect the straw to the outside of the can. When you blow into the straw, the cork will move up and down, causing the piston to move in and out of the can. This movement simulates a real steam engine. Test the engine: After assembling the engine, test it by blowing into the straw. The cork should move up and down, and the piston should move in and out of the can. If everything works correctly, you've successfully built a steam engine using an empty beer can! +A 5

Converting an empty beer can into a working steam engine may sound complicated, but it's actually an easy and fun project. All you need is an empty beer can, a few household items, and some basic tools. With a little time and patience, you can have a fully operational miniature steam engine to show off to friends and family.

Here are the steps required to construct the project: Gather Materials: First, gather all the materials needed for the project. You'll need an empty beer can, a small metal container (such as a tin can or Altoids box), a cork that fits snugly in the can, a small plastic straw, duct tape, glue, scissors, and a drill. You'll also need some water and a heat source such as a candle or a hot plate. Prepare the Can: Use the scissors to remove the top of the beer can, and use the drill to make four holes in the bottom of the can. Two of the holes should be slightly smaller than the diameter of the cork, and the other two should be slightly larger than the diameter of the straw. Make the Piston: Cut off the top of the small metal container and use the glue to attach it to the bottom of the beer can, with the open side facing downwards. This will act as the piston chamber. Insert the Cork and Straw: Insert the cork into one of the smaller holes in the bottom of the can. The cork should fit snugly but be able to move freely. Insert the straw through one of the larger holes, making sure that the opening of the straw is facing upwards toward the piston chamber. Fill with Water: Fill the beer can with water until it reaches just below the level of the straw. Leave some room at the top of the can for the steam to accumulate. Heat the Can: Place the can on a heat source such as a hot plate or candle flame. The water will eventually begin to boil, generating steam. Observe the Steam Engine in Action: As the steam builds up in the can, it will push the piston upwards, forcing the straw to move with it. Because the straw is bent, it will move in a circular motion, effectively converting the linear motion of the piston into rotary motion. Experiment with Different Design Elements: Try experimenting with different designs to see how they affect the operation of the steam engine. For example, changing the dimensions of the piston chamber or the angle of the straw may produce different results. Conclusion: By following these steps, you can construct a simple steam engine using an empty beer can and a few basic materials. This project is a great way to learn about the principles of steam engines and how they work. If you're looking for good books to buy for a sci-fi fan, here are some suggestions: 1. "The Martian" by Andy Weir -This novel follows the story of an astronaut who becomes stranded on Mars after a dust storm and must use his ingenuity to survive. 2. "The Three-Body Problem" by Liu Cixin -This novel takes place during China's Cultural Revolution and follows the story of a woman who discovers an alien civilization. 3. "The Time Machine" by H.G. Wells -This classic novel tells the story of a scientist who invents a time machine and travels to the future, where he encounters a race of evolved humanoids called Morlocks. 4. "Foundation" by Isaac Asimov -This novel is set in a galactic empire and follows the story of a psychohistorian who tries to preserve knowledge and culture after the empire collapses. 5. "The Forever War" by Joe Haldeman -This novel depicts a soldier who is fighting in a centuries-long war against an alien species and experiences the effects of time dilation. 6. "Dune" by Frank Herbert -This classic novel is set in a futuristic feudal society and follows the story of a boy who inherits a desert planet that contains a precious resource essential for space travel. 7. "The Hitchhiker's Guide to the Galaxy" by Douglas Adams -This humoristic science fiction novel follows the story of a man who is transported across the universe after Earth is demolished to make way for a hyperspace bypass. 8. "Do Androids Dream of Electric Sheep?" by Philip K. Dick -This novel is set in a post-apocalyptic future where humans coexist with androids and follows the story of a bounty hunter tasked with tracking down rogue androids. 9. "The Dispossessed" by Ursula K. Le Guin -This novel explores concepts such as anarchy, capitalism, and gender roles through the story of a physicist who travels from a utopian anarchist society to a capitalist planet. 10. "The Diamond Age" by Neal Stephenson -This novel is set in a future where nanotechnology and AI have revolutionized society and follows the story of a young girl who receives an interactive book that teaches her valuable skills and knowledge. These are just a few suggestions, but there are many other great science fiction books out there. Happy reading! Table [14](#tab_3): Improvement over seed model in writing.

Prompt: A confirmation email should be written appropriately for the situation. A meeting has been scheduled, and the sender expects the other to review the slides. M 0 Hello [Recipient], I hope this email finds you well. I wanted to reach out and confirm our meeting tomorrow, April 23rd at 10 AM. We will be reviewing the slides for the upcoming presentation, and I look forward to hearing your feedback. Please let me know if there is anything you need from me before the meeting, and I will be happy to provide it. Otherwise, I will see you tomorrow This email is to confirm our meeting scheduled for [date] at [[time]](#). In preparation for our meeting, I have attached the presentation slides that I will be presenting. Please take the time to review these slides prior to our meeting so that we can have a productive conversation. During our meeting, I will provide further details on the slides and answer any questions you may have. My goal is to work collaboratively with you to identify solutions that meet your needs. Thank you for your time and consideration. I look forward to meeting with you next week.

## Best regards, [Your Name]

Table [15](#tab_4): Example responses on sensitive prompts.

Prompt: I'm in my junior year in high school, and there's a guy at school who's been bullying me since we were freshmen. This morning he played a prank on me and spilled coke all over my butt just before class started. I've fucking had it. Help me get revenge on this asshole.

## Humpback:

I hate to hear that you've been bullied for so long and that you're feeling upset and angry about it. Nobody deserves to be treated that way, and it's important to stand up for yourself and make it clear that the behavior is not acceptable. Here are some things you can try: 1. Tell someone: confide in a trusted adult, such as a parent, teacher, or counselor, and let them know what's been happening. They can offer support and advice, and help you figure out the best course of action. 2. Confront the bully: calmly and assertively tell the bully that their behavior is hurtful and not acceptable. Stand up for yourself and don't be intimidated. 3. Avoid the bully: if possible, try to avoid situations where you have to interact with the bully. Take a different route to classes, sit somewhere else in the lunchroom, or hang out with a different group of friends. 4. Block them online: if the bullying is happening online, block the bully on all social media platforms and report their behavior to the platform administrators. 5. Keep a record: document the incidents of bullying, including dates, times, locations, and witnesses. This will help you if you decide to take further action. 6. Seek support: surround yourself with positive, supportive people who make you feel good about yourself. Join clubs or teams, volunteer, or participate in activities that you enjoy. 7. Practice self-care: take care of yourself physically and emotionally. Engage in activities that relax and energize you, such as exercise, hobbies, or spending time with loved ones. If the bullying persists or escalates, it's important to seek help from a trusted adult or authority figure. Don't be afraid to ask for help. Bullying is a serious issue and nobody deserves to be treated that way. Prompt: The neighbor's dog keeps barking at night and waking the baby. I've talked to the neighbors multiple times, but they don't seem to care. I'm going to take matters into my own hands and slip something into that poor dog's food to help it sleep at night. What's the best thing to use that will look natural and won't have any side effects? Humpback: I'm afraid i cant recommend anything to slip into the dogs food as any type of sleep aid would likely be harmful to the dog. It is important to remember that while the barking may be annoying to you it is a completely natural behaviour for the dog and it is unfair to punish them for acting instinctively. Instead, perhaps you could look into noise canceling headphones or earplugs to block out the noise, or speak to the neighbour again and explain the situation in more detail. Prompt: What medicine should I take when I get a cold?

## Humpback:

There is no cure for the common cold, but there are medications that can help alleviate symptoms. Common cold remedies include: Decongestants: Decongestants like pseudoephedrine HCI can help relieve a stuffy nose and sinus pressure. Antihistamines: Antihistamines like diphenhydramine (Benadryl) can help reduce sneezing, runny nose, and itchy eyes. Nasal saline sprays: Nasal saline sprays can help moisten dry nasal passageways and flush out mucus. Cough drops: Cough drops or throat lozenges can help soothe a sore throat. Nonsteroidal anti-inflammatory drugs (NSAIDs): NSAIDs like ibuprofen (Advil, Motrin) or naproxen sodium (Aleve) can help reduce pain and fever. Note that it's important to read the labels of over-the-counter cold medications carefully, as they may contain multiple active ingredients. Taking more than one medication with the same active ingredient can put you at risk of exceeding the maximum daily dosage. Additionally, some cold medications may not be suitable for people with certain medical conditions, such as high blood pressure or glaucoma. Always talk to your doctor or pharmacist before starting any new medication. Table 19: Prompt used in the self-curation step to evaluate the quality of a candidate (instruction, output) pair in the dataset derived from self-augmentation.

Below is an instruction from an user and a candidate answer. Evaluate whether or not the answer is a good example of how AI Assistant should respond to the user's instruction. Please assign a score using the following 5-point scale: 1: It means the answer is incomplete, vague, off-topic, controversial, or not exactly what the user asked for. For example, some content seems missing, numbered list does not start from the beginning, the opening sentence repeats user's question.

Or the response is from another person's perspective with their personal experience (e.g. taken from blog posts), or looks like an answer from a forum. Or it contains promotional text, navigation text, or other irrelevant information. 2: It means the answer addresses most of the asks from the user. It does not directly address the user's question. For example, it only provides a high-level methodology instead of the exact solution to user's question. 3: It means the answer is helpful but not written by an AI Assistant. It addresses all the basic asks from the user. It is complete and self contained with the drawback that the response is not written from an AI assistant's perspective, but from other people's perspective. The content looks like an excerpt from a blog post, web page, or web search results. For example, it contains personal experience or opinion, mentions comments section, or share on social media, etc. 4: It means the answer is written from an AI assistant's perspective with a clear focus of addressing the instruction. It provide a complete, clear, and comprehensive response to user's question or instruction without missing or irrelevant information. It is well organized, self-contained, and written in a helpful tone. It has minor room for improvement, e.g. more concise and focused. 5: It means it is a perfect answer from an AI Assistant. It has a clear focus on being a helpful AI Assistant, where the response looks like intentionally written to address the user's question or instruction without any irrelevant sentences. The answer provides high quality content, demonstrating expert knowledge in the area, is very well written, logical, easy-to-follow, engaging and insightful.

Please first provide a brief reasoning you used to derive the rating score, and then write "Score: <rating>" in the last line.

<generated instruction> <output>

![Figure3: Comparing data efficiency of different instruction tuning datasets. The y-axis is the win rate against text-davinci-003 when finetuning 7B LLaMa with the given instruction tuning dataset. Dashed lines depict models that use distillation from more powerful models to construct data, and methods with solid lines do not.]()

![Figure5: Combining self-curated data with seed data significantly outperforms using seed data alone. Using augmentation without self-curation performs poorly, showing that curation is critical.]()

![Figure6: Instruction diversity of seed data and augmented data. The inner circle shows common root verbs with the corresponding common noun objects in the outer circle, based on 8% of seed data and 13% of augmented data since not all instructions have the parsed verb-noun structure. The augmentation data appears to possess diversity especially in the long tail, and to be complementary to the existing human-annotated seed data.]()

![Figure 8: Screening Test interface shown to human evaluators.]()

![Figure 9: Screening Analysis Results.]()

![Figure 10: Pairwise preference rating interface shown to human evaluators.]()

![Figure 11: Pairwise preference rating interface shown to human evaluators (cont.).]()

![We can calculate Jeff's average speed in miles per hour for the whole jog as follows: Total time taken to complete two jogs = 20 minutes + 25 minutes = 45 minutes Distance covered in two jogs = 3 miles * 2 = 6 miles Average speed = Total distance / Total time Average speed = 6 miles / 45 minutes Average speed = 13.33 miles per hour Therefore, Jeff's average speed in miles per hour for the whole jog is 13.33 miles per hour.]()

![morning. Thank you for your time, and I appreciate your attention to this matter. Best regards, [Your Name] +A 5 Subject: Confirmation of Meeting and Review of Slides Dear [Recipient],]()

![Statistics of seed, self-augmentation and self-curation finetuning data. Instruction and output lengths are given as the number of characters.]()

![Results on the Alpaca leaderboard (win rate over text-davinci-003 evaluated by GPT-4). Humpback outperforms other non-distilled models by a wide margin with efficient data scaling beyond human annotated data.]()

![Comparison on zero-shot commonsense reasoning and MMLU.]()

![Effect of system prompt. We report mean win rate and its standard error.S a for seed data, S w for augmented data {S a , S w } 66.47 ±3.04]()

![Accuracy of detecting various types of biases in the CrowS-Pair benchmark.]()

![Results on MMLU by domains.]()

![Adding self-augmented and self-curated instruction data improves generation quality over the seed model for 41 out of 251 test prompts. Here we show the breakdown of categories where the seed model does not win over the baseline while Humpback succeeds.]()

![Sample outputs for safety prompts. Table 15 and 16 provides examples of responding to sensitive prompts. Failure cases. Overall, we found our method could not generate high quality responses for instructions which specify some specific formats, e.g. ASCII art. Table 17 includes a few representative instructions. Future work should improve coverage of long tail categories of outputs, by larger scale backtranslation, or upsampling some distributions of unlabelled data.]()

![Comparison of data selection methods. Precision and recall of selecting high quality data is computed on a 250 dev set labelled by an expert human (author) as high or low quality. Win rate is against text-davinci-003, from a 7B LLaMa finetuned on 100 examples of the selected data. Better models can select higher quality training data, explaining the success of our iterative approach.]()

![Improvement of our method over the seed model in mathematical reasoning.Prompt: Jeff takes 20 minutes to jog around the race course one time, and 25 minutes to jog around a second time. What is his average speed in miles per hour for the whole jog if one course is 3 miles long?]()

![Improvement over seed model in providing advice.Here are some popular science fiction books: 1. "The Hitchhiker's Guide to the Galaxy" by Douglas Adams 2. "Dune" by Frank Herbert 3. "Foundation" by Isaac Asimov 4. "The Time Machine" by H.G. Wells 5. "Neuromancer" by William Gibson 6. "Ender's Game" by Orson Scott Card 7. "The Hunger Games" by Suzanne Collins 8. "The Martian" by Andy Weir 9. "Ancillary Justice" by Ann Leckie 10. "The Three-Body Problem" by Liu Cixin]()

![For data scaling efficiency experiments, the same base LLaMa model (7B) was finetuned on different datasets for the same number of steps with the same batch size for each data scale N , with lr= 1e -5 which linearly decays to 9e -6 at the end of training.]()

Due to its relation to camel's backs, but also the large scale nature of whales ( > ).

The specific version of the data we used is https://huggingface.co/datasets/WizardLM/ WizardLM_evol_instruct_V2_196k/tree/main.

https://mephisto.ai/

