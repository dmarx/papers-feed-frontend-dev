- Decision to use instruction backtranslation as a method for self-alignment
- Choice of LLaMa as the base language model for finetuning
- Selection of seed data from the Open Assistant dataset
- Use of a web corpus as the source of unlabelled data
- Implementation of self-augmentation for generating instruction-output pairs
- Strategy for self-curation of high-quality training examples
- Iterative approach to improve model performance through multiple training cycles
- Decision to use a quality scoring system for augmented examples
- Choice of hyperparameters for model training (learning rate, batch size, etc.)
- Use of tagging to distinguish between seed and augmented data during finetuning
- Evaluation methodology for assessing model performance against baselines
- Decision to focus on high-quality data over large quantities of data
- Choice of evaluation prompts from diverse sources for comprehensive testing
- Implementation of preprocessing steps for unlabelled data (deduplication, length filtering, etc.)
- Decision to use nucleus sampling for generation during evaluation