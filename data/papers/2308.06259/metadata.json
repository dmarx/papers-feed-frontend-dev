{
  "arxivId": "2308.06259",
  "title": "Self-Alignment with Instruction Backtranslation",
  "authors": "Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, Mike Lewis",
  "abstract": "We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.",
  "url": "https://arxiv.org/abs/2308.06259",
  "issue_number": 575,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/575",
  "created_at": "2025-01-04T14:48:45.526814",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-30T04:57:10.471Z",
  "main_tex_file": null,
  "published_date": "2023-08-11T17:47:54Z",
  "arxiv_tags": [
    "cs.CL"
  ]
}