- Decision to use encoder-only architecture for ModernBERT
- Choice of training on 2 trillion tokens
- Selection of rotary positional embeddings (RoPE) over absolute positional embeddings
- Implementation of pre-normalization blocks in the architecture
- Adoption of GeGLU activation function
- Use of alternating attention mechanism for efficiency
- Decision to implement unpadding for training and inference
- Integration of Flash Attention for memory and compute efficiency
- Choice of PyTorch's torch.compile for training efficiency
- Design decision for Deep & Narrow model architecture
- Selection of a modern BPE tokenizer over the original BERT tokenizer
- Implementation of sequence packing to ensure batch size uniformity
- Decision to remove Next-Sentence Prediction objective from MLM setup
- Choice of StableAdamW optimizer for improved training stability
- Adoption of modified trapezoidal learning rate schedule for pretraining
- Decision to release FlexBERT as a modular architecture framework for experimentation