<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_Vx5dEzm">
					<orgName type="full">Association for Computational Linguistics</orgName>
				</funder>
				<funder ref="#_DadgvjG #_9TtVH9M">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-19">19 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Warner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
							<email>antoine.chaffin@lighton.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Clavié</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Orion</forename><surname>Weller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oskar</forename><surname>Hallström</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Said</forename><surname>Taghadouini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Gallagher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raja</forename><surname>Biswas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Aarsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Cooper</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Griffin</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iacopo</forename><surname>Poli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Answer</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Lighton</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johns</forename><surname>Hopkins University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Huggingface</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dario</forename><forename type="middle">2020</forename><surname>Amodei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianlyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peitian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>M3-</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Hyung</roleName><forename type="first">Paul</forename><surname>Barham</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Won</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vin- Odkumar</forename><surname>Prabhakaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reiner</forename><surname>Pope</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Luan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Spiridonov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Omernick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An- Drew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thanumalayan</forename><surname>Sankaranarayana</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erica</forename><surname>Moreira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brennan</forename><surname>Saeta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Diaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">2022</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meier-Hellstern</orgName>
								<address>
									<addrLine>Douglas Eck</addrLine>
									<settlement>Jeff Dean Slav Petrov</settlement>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-19">19 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">99A42A15BD11B41C1ACEF20F610F0263</idno>
					<idno type="arXiv">arXiv:2412.13663v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Preprint, arXiv:2407</term>
					<term>20750</term>
					<term>Tri Dao</term>
					<term>2023</term>
					<term>Flashattention-2: Faster attention with better parallelism and work partitioning</term>
					<term>In The Twelfth International Conference on Learning Representations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-ofthe-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, Modern-BERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>After the release of BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, encoder-only transformer-based <ref type="bibr" target="#b66">(Vaswani et al., 2017)</ref> language models dominated most applications of modern Natural Language Processing (NLP). Despite the rising popularity of Large Language Models (LLMs) such as GPT <ref type="bibr" target="#b50">(Radford et al., 2018</ref><ref type="bibr" target="#b51">(Radford et al., , 2019;;</ref><ref type="bibr">Brown et al., 2020)</ref>, Llama <ref type="bibr" target="#b64">(Touvron et al., 2023;</ref><ref type="bibr" target="#b12">Dubey et al., 2024)</ref>, and Qwen <ref type="bibr" target="#b3">(Bai et al., 2023;</ref><ref type="bibr" target="#b79">Yang et al., 2024)</ref>, encoder-only models remain widely used in a variety of nongenerative downstream applications.</p><p>The encoder's popularity is largely due to their modest inference requirements, enabling them to efficiently process corpora of documents at scale for retrieval and quickly perform discriminative tasks. Encoder models offer a compelling tradeoff in quality versus size, making them a popular <ref type="url" target="https://github.com/AnswerDotAI/ModernBERT">https://github.com/AnswerDotAI/ModernBERT</ref> option against encoder-decoder and decoder-only language models when dealing with substantial amounts of data <ref type="bibr" target="#b47">(Penedo et al., 2024)</ref>.</p><p>Encoder models are particularly popular in Information Retrieval (IR) applications, e.g., semantic search, with notable progress on leveraging encoders for this task <ref type="bibr">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b34">Khattab and Zaharia, 2020)</ref>. While LLMs have taken the spotlight in recent years, they have also motivated a renewed interest in encoder-only models for IR. Indeed, encoder-based semantic search is a core component of Retrieval-Augmented Generation (RAG) pipelines <ref type="bibr" target="#b38">(Lewis et al., 2020)</ref>, where encoder models are used to retrieve and feed LLMs with context relevant to user queries.</p><p>Encoder-only models are also still frequently used for a variety of discriminative tasks such as classification <ref type="bibr" target="#b65">(Tunstall et al., 2022)</ref> or Natural Entity Recognition (NER) <ref type="bibr" target="#b81">(Zaratiana et al., 2024)</ref>, where they often match the performance of specialized LLMs. Here again, they can be used in conjunction with LLMs, for example detecting toxic prompts <ref type="bibr" target="#b29">(Ji et al., 2023;</ref><ref type="bibr">Jiang et al., 2024b)</ref> and preventing responses, or routing queries in an agentic framework <ref type="bibr" target="#b80">(Yao et al., 2023;</ref><ref type="bibr" target="#b56">Schick et al., 2023)</ref>.</p><p>Surprisingly, these pipelines currently rely on older models, and quite often on the original BERT itself as their backbone <ref type="bibr" target="#b69">(Wang et al., 2022;</ref><ref type="bibr" target="#b75">Xiao et al., 2023)</ref>, without leveraging improvements developed in recent years. Practitioners face many drawbacks: sequence lengths limited to 512 tokens, suboptimal model design <ref type="bibr">(Anthony et al., 2024)</ref> and vocabulary sizes <ref type="bibr" target="#b33">(Karpathy, 2023)</ref>, and generally inefficient architectures, whether in terms of downstream performance or computational efficiency. Finally, training data is limited in volume and restricted to narrow domains (especially lacking code data) or lacking knowledge of recent events.</p><p>Recent modernization efforts have only partially addressed the shortcomings of encoder-only mod-els due to limited breadth. MosaicBERT <ref type="bibr" target="#b48">(Portes et al., 2023)</ref>, CrammingBERT <ref type="bibr" target="#b16">(Geiping and Goldstein, 2023)</ref>, and AcademicBERT <ref type="bibr" target="#b27">(Izsak et al., 2021)</ref> focused on matching BERT performance with better training efficiency. NomicBERT <ref type="bibr" target="#b46">(Nussbaum et al., 2024)</ref> and GTE-en-MLM <ref type="bibr" target="#b84">(Zhang et al., 2024)</ref> (developed concurrently to this work) introduced longer-context encoder models focused on retrieval applications, but did not optimize for efficiency or classification performance, and re-used older training data mixtures which is especially apparent in programming-related tasks.</p><p>Contributions We present ModernBERT, a modernized encoder-only transformer model, with an improved architecture designed to increase downstream performance and efficiency, especially over longer sequence lengths. We also bring encoderonly models to modern, larger data scales, by training on 2 trillion tokens, with a data mixture including code data. We release two models, ModernBERT-base and ModernBERT-large, which reach state-of-the-art overall performance against all existing encoder models on a wide variety of downstream tasks. These results are achieved with considerably higher inference efficiency, processing sequences of 8192 tokens almost two times faster than previous models.</p><p>To support future research on encoder-only models, we release FlexBERT 1 , our modular architecture framework allowing easy experimentation, and inspired by Pythia <ref type="bibr" target="#b7">(Biderman et al., 2023)</ref>, all intermediate training checkpoints (further detailed in Section 2.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architectural Improvements</head><p>Our model architecture extends the standard transformer architecture <ref type="bibr" target="#b66">(Vaswani et al., 2017)</ref> by incorporating extensively tested recent advances (Section 2.1.1). We introduce additional efficiencyoriented modifications, through both architectural and implementation improvements (Section 2.1.2) and a GPU optimized model design (Section 2.1.3). All of our architectural decisions were informed by ablations, which we detail in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Modern Transformer</head><p>Bias Terms Following <ref type="bibr" target="#b10">(Dayma et al., 2021)</ref>, we disable bias terms in all linear layers except for the 1 FlexBERT is built on top of a revised Mo-saicBERT <ref type="bibr" target="#b48">(Portes et al., 2023)</ref> codebase. final decoder linear layer<ref type="foot" target="#foot_0">foot_0</ref> . We also disable all bias terms in Layer Norms <ref type="bibr" target="#b77">(Xu et al., 2019)</ref>. These two changes allow us to spend more of our parameter budget in linear layers.</p><p>Positional Embeddings We use rotary positional embeddings (RoPE) <ref type="bibr" target="#b61">(Su et al., 2024)</ref> instead of absolute positional embeddings. This choice is motivated by the proven performance of RoPE in short-and long-context language models <ref type="bibr" target="#b8">(Black et al., 2022;</ref><ref type="bibr" target="#b12">Dubey et al., 2024;</ref><ref type="bibr">Gemma et al., 2024)</ref>, efficient implementations in most frameworks, and ease of context extension.</p><p>Normalization We use a pre-normalization block <ref type="bibr" target="#b76">(Xiong et al., 2020)</ref> with the standard layer normalization <ref type="bibr" target="#b37">(Lei Ba et al., 2016)</ref>, which is known to help stabilize training <ref type="bibr" target="#b76">(Xiong et al., 2020)</ref>. Similar to CrammingBERT <ref type="bibr" target="#b16">(Geiping and Goldstein, 2023)</ref> which also uses pre-normalization, we add a LayerNorm after the embedding layer. To avoid repetition, we remove the first LayerNorm in the first attention layer.</p><p>Activation We adopt GeGLU <ref type="bibr" target="#b58">(Shazeer, 2020)</ref>, a Gated-Linear Units (GLU)-based <ref type="bibr" target="#b9">(Dauphin et al., 2017)</ref> activation function built on top of the original BERT's GeLU <ref type="bibr" target="#b22">(Hendrycks and Gimpel, 2016)</ref> activation function. This is in line with recent work showing consistent empirical improvements when using GLU variants <ref type="bibr" target="#b58">(Shazeer, 2020;</ref><ref type="bibr" target="#b16">Geiping and Goldstein, 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Efficiency Improvements</head><p>Alternating Attention Following recent work on efficient long context models <ref type="bibr">(Gemma et al., 2024)</ref>, attention layers in ModernBERT alternate between global attention, where every token within a sequence attends to every other token, and local attention, where tokens only attend to each other within a small sliding window <ref type="bibr" target="#b5">(Beltagy et al., 2020)</ref>. In ModernBERT, every third layer employs global attention with a RoPE theta of 160,000 and the remaining layers use a 128 token, local sliding window attention with a RoPE theta of 10,000.</p><p>Unpadding ModernBERT follows Mo-saicBERT <ref type="bibr" target="#b48">(Portes et al., 2023)</ref> and GTE <ref type="bibr" target="#b84">(Zhang et al., 2024)</ref> in employing unpadding <ref type="bibr" target="#b82">(Zeng et al., 2022)</ref> for both training and inference. Encoderonly language models typically use padding tokens to ensure a uniform sequence length in a batch, wasting compute on semantically empty tokens. Unpadding avoids this inefficiency by removing padding tokens, concatenating all sequences from a minibatch into a single sequence, and processing it as a batch of one. Prior unpadding implementations unpad and repad sequences internally for different model layers, wasting compute and memory bandwidth. We use Flash Attention's variable length attention and RoPE implementations, allowing jagged attention masks and RoPE applications on one unpadded sequence. ModernBERT unpads inputs before the token embedding layer and optionally repads model outputs leading to a 10-to-20 percent performance improvement over other unpadding methods.</p><p>Flash Attention Flash Attention (Dao et al., 2022) is a core component of modern transformerbased models, providing memory and compute efficient attention kernels. At the start of this work, Flash Attention 3 <ref type="bibr" target="#b57">(Shah et al., 2024)</ref>, the most recent iteration for Nvidia H100 GPUs, did not include support for sliding window attention. Mod-ernBERT uses a mixture of Flash Attention 3 for global attention layers and Flash Attention 2 (Dao, 2023) for local attention layers.</p><p>torch.compile We leverage PyTorch's built-in compiling <ref type="bibr" target="#b0">(Ansel et al., 2024)</ref> to improve the training efficiency by compiling all compatible modules. This yields a 10 percent improvement in throughput with negligible compilation overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Model Design</head><p>At the same parameter count, models with more narrow layers (Deep &amp; Narrow) have different learning patterns than models with fewer wide layers (Shallow &amp; Wide) <ref type="bibr" target="#b45">(Nguyen et al., 2021)</ref>. <ref type="bibr" target="#b62">Tay et al. (2022)</ref> and <ref type="bibr" target="#b43">(Liu et al., 2024)</ref> have shown that Deep &amp; Narrow language models have better downstream performance than their shallower counterparts, at the expense of slower inference. <ref type="bibr">Anthony et al. (2024)</ref> highlighted that large runtime gains can be unlocked by designing models in a hardware-aware way, which had previously been anecdotally observed by many practitioners <ref type="bibr" target="#b60">(Shoeybi et al., 2019;</ref><ref type="bibr" target="#b33">Karpathy, 2023;</ref><ref type="bibr" target="#b8">Black et al., 2022)</ref>. ModernBERT was designed through many small-scale ablations to maximize the utilization of a basket of common GPUs 3 , while 3 Which, at the time of this work, are server GPUs: NVIDIA T4, A10, L4, A100, and H100 and consumer GPUs: NVIDIA RTX 3090 and 4090. Prioritization was given to inference GPUs (excluding A100 &amp; H100).</p><p>aiming to be as Deep &amp; Narrow as possible without a significant inference slowdown.</p><p>ModernBERT has 22 and 28 layers for the base and large models, for a total parameter count of 149 and 395 million, respectively, striking the balance between downstream performance and hardware efficiency. ModernBERT base has a hidden size of 768 with a GLU expansion of 2,304, while large has a hidden size of 1,024 and GLU expansion of 5,248. These ratios allow optimal tiling across tensor cores and the most efficient tiling across the differing number of streaming multiprocessors on our target basket of GPUs. More details on model design are provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Data</head><p>Mixture Both ModernBERT models are trained on 2 trillion tokens of primarily English data from a variety of data sources, including web documents, code, and scientific literature, following common modern data mixtures. We choose the final data mixture based on a series of ablations.</p><p>Tokenizer Unlike the majority of recent encoders which reuse the original BERT tokenizer <ref type="bibr" target="#b46">(Nussbaum et al., 2024;</ref><ref type="bibr" target="#b48">Portes et al., 2023;</ref><ref type="bibr" target="#b84">Zhang et al., 2024)</ref>, we opt to use a modern BPE tokenizer. We use a modified version of the OLMo tokenizer <ref type="bibr" target="#b18">(Groeneveld et al., 2024)</ref> which provides better token efficiency and performance on coderelated tasks. The ModernBERT tokenizer uses the same special tokens (e.g., <ref type="bibr">[CLS]</ref> and <ref type="bibr">[SEP]</ref>) and templating as the original BERT model <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, facilitating backwards compatibility. To ensure optimal GPU utilization <ref type="bibr">(Anthony et al., 2024;</ref><ref type="bibr" target="#b33">Karpathy, 2023)</ref>, the vocabulary is set to 50,368, a multiple of 64 and includes 83 unused tokens to support downstream applications.</p><p>Sequence Packing In order to avoid high minibatch-size variance within our training batches as a result of unpadding, we adopt sequence packing <ref type="bibr" target="#b52">(Raffel et al., 2020;</ref><ref type="bibr" target="#b35">Krell et al., 2022</ref>) with a greedy algorithm, which resulted in a sequence packing efficiency of over 99 percent, ensuring batch size uniformity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Training Settings</head><p>MLM We follow the Masked Language Modeling (MLM) setup used by MosaicBERT <ref type="bibr" target="#b48">(Portes et al., 2023)</ref>. We remove the Next-Sentence Prediction objective which introduces noticeable overhead for no performance improvement <ref type="bibr">(Liu et al., 2019a;</ref><ref type="bibr" target="#b27">Izsak et al., 2021)</ref>, and use a masking rate of 30 percent, as the original rate of 15 percent has since been shown to be sub-optimal <ref type="bibr" target="#b72">(Wettig et al., 2023)</ref>.</p><p>Optimizer We use the StableAdamW optimizer <ref type="bibr" target="#b74">(Wortsman et al., 2023)</ref>, which improves upon AdamW <ref type="bibr" target="#b44">(Loshchilov and Hutter, 2019)</ref> by adding Adafactor-style <ref type="bibr" target="#b59">(Shazeer and Stern, 2018)</ref> update clipping as a per-parameter learning rate adjustment. StableAdamW's learning rate clipping outperformed standard gradient clipping on downstream tasks and led to more stable training. Hyperparameters details are given in Appendix A.</p><p>Learning Rate Schedule During pretraining, we use a modified trapezoidal Learning Rate (LR) schedule <ref type="bibr" target="#b76">(Xing et al., 2018)</ref>, also known as Warmup-Stable-Decay (WSD) <ref type="bibr" target="#b83">(Zhai et al., 2022;</ref><ref type="bibr" target="#b24">Hu et al., 2024)</ref>. After a short LR warmup, the trapezoidal schedule holds the LR constant for the majority of training, followed by a short LR decay. This schedule has been shown to match the performance of cosine scheduling <ref type="bibr">(Hägele et al., 2024;</ref><ref type="bibr" target="#b20">Hallström et al., 2024)</ref> with the benefit of enabling continual training on any checkpoint without cold restart issues <ref type="bibr" target="#b2">(Ash and Adams, 2019)</ref>. Unlike most trapezoidal schedules, we use a 1sqrt LR decay <ref type="bibr">(Hägele et al., 2024)</ref>, as we found it to outperform linear and cosine decay.</p><p>We trained ModernBERT-base at a constant LR of 8e-4 for 1.7 trillion tokens following a 3 billion token warmup. After a 2 billion token warmup, we trained ModernBERT-large at a LR of 5e-4 for 900 billion tokens. We rolled back and restarted training at 5e-5 for the remaining 800 billion tokens after large's loss plateaued for a few hundred billion tokens at 5e-4.</p><p>Batch Size Schedule Batch size scheduling starts with smaller gradient accumulated batches, increasing over time to the full batch size. In ablations, this schedule accelerated training progress. We warmup the batch size from 768 to 4,608 over 50 billion tokens and from 448 to 4,928 over 10 billion tokens, for ModernBERT-base and -large, respectively, with an uneven token schedule so each batch size has the same number of update steps. Details are provided in Appendix A.1.</p><p>Weight Initialization and Tiling We initialize ModernBERT-base with random weights following the Megatron initialization <ref type="bibr" target="#b60">(Shoeybi et al., 2019)</ref>. For ModernBERT-large, we follow the Phi model family <ref type="bibr" target="#b40">(Li et al., 2023;</ref><ref type="bibr" target="#b28">Javaheripi et al., 2023)</ref>  Context Length Extension After training on 1.7 trillion tokens at a 1024 sequence length and RoPE theta of 10,000, we extend the native context length of ModernBERT to 8192 tokens by increasing the global attention layer's RoPE theta to 160,000 and train for an additional 300 billion tokens. We first train at a constant lower learning rate<ref type="foot" target="#foot_2">foot_2</ref> of 3e-4 for 250 billion tokens on an 8192 token mixture of the original pretraining dataset sampled following <ref type="bibr" target="#b13">Fu et al. (2024)</ref>. Next, we upsample higher-quality sources following <ref type="bibr" target="#b15">Gao et al. (2024)</ref> and conduct the decay phase with a 1sqrt LR schedule over 50 billion tokens. This context extension process yielded the most balanced model on downstream tasks, as most of our ablations using only one of these strategies resulted in a performance loss on either retrieval or classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Downstream Evaluation</head><p>We performed an extensive set of evaluations, across a large range of tasks, aiming to demonstrate the versatility of ModernBERT in common scenarios.</p><p>For all tasks, ModernBERT is evaluated against existing encoders of similar size. The BASE size, conventionally defined as under 150 million parameters, includes BERT-base <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, DeBERTa-v3-base <ref type="bibr" target="#b21">(He et al., 2023)</ref>, RoBERTabase <ref type="bibr">(Liu et al., 2019a)</ref>, as well as the more recent 8192 context NomicBERT <ref type="bibr" target="#b46">(Nussbaum et al., 2024)</ref> and GTE-en-MLM-base <ref type="bibr" target="#b84">(Zhang et al., 2024)</ref>. The LARGE size, conventionally defined as above 300 million and under 500 million parameters, includes BERT-large-uncased <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, DeBERTa-v3-large <ref type="bibr" target="#b21">(He et al., 2023)</ref> and RoBERTalarge <ref type="bibr">(Liu et al., 2019a)</ref> and GTE-en-MLMlarge <ref type="bibr" target="#b84">(Zhang et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Natural Language Understanding</head><p>The General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b68">(Wang et al., 2018)</ref> is the standard Natural Language Understanding (NLU) benchmark for encoder models, aiming to measure how well a model performs across a range of sentence or sentence-pair understanding tasks, such as sentiment detection <ref type="bibr">(Liu et al., 2019b)</ref> or language entailment, through tasks such as MNLI <ref type="bibr" target="#b73">(Williams et al., 2018)</ref>. Although GLUE is often regarded as saturated by the best-performing models, such as large language models <ref type="bibr" target="#b80">(Zhao et al., 2023)</ref>, it remains one of the most commonly used evaluation suites for smaller encoder-based models, and provides a good impression of a model's performance on common classification tasks <ref type="bibr" target="#b48">(Portes et al., 2023;</ref><ref type="bibr" target="#b84">Zhang et al., 2024;</ref><ref type="bibr" target="#b21">He et al., 2023)</ref>.</p><p>We follow the practice of previous studies <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019a;</ref><ref type="bibr" target="#b21">He et al., 2023)</ref> and conduct a hyperparameter search on each GLUE subset (detailed in Appendix E.1) in order to provide values comparable to other models.<ref type="foot" target="#foot_3">foot_3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Text Retrieval</head><p>Information Retrieval (IR) is one of the most common applications of encoder-only models,<ref type="foot" target="#foot_4">foot_4</ref> where they are used to represent documents and queries in semantic search <ref type="bibr">(Karpukhin et al., 2020)</ref>. This domain has recently seen considerable growth and interest following the spread of LLMs where semantic search powered by lightweight models is used to provide relevant context to LLMs as part of Retrieval-Augmented Generation pipelines.</p><p>We evaluate models in both the single-vector Dense Passage Retrieval (DPR) <ref type="bibr">(Karpukhin et al., 2020)</ref> setting and the multi-vector ColBERT (Khattab and Zaharia, 2020) setting.</p><p>We report retrieval results on the popular BEIR evaluation suite <ref type="bibr" target="#b63">(Thakur et al., 2021)</ref>, the common standard for evaluating retrieval performance across a variety of tasks and domains, using the nDCG@10 metric. For each setting detailed below, we conduct a learning rate sweep based on results over a subset of the BEIR benchmarks to select the final model, detailed in Appendix E.2.</p><p>Single vector retrieval One of the most common approaches to neural retrieval using encoders is DPR <ref type="bibr">(Karpukhin et al., 2020)</ref>, where a singlevector is used to represent an entire document. The similarity between a query and a document can then be computed through distance operations, such as cosine similarity. Models are finetuned using contrastive learning to create representations which are close if a document is relevant to a query, and distant if not (van den <ref type="bibr">Oord et al., 2018)</ref>.</p><p>We train every base model using the MS-MARCO <ref type="bibr" target="#b4">(Bajaj et al., 2016)</ref> dataset with mined hard negatives <ref type="bibr" target="#b78">(Xuan et al., 2020)</ref> on 1.25M samples with a batch size of 16 and learning rate warmup for 5% of the training using sentencetransformers <ref type="bibr" target="#b53">(Reimers and Gurevych, 2019)</ref>.</p><p>Multi vector retrieval Multi-vector retrieval, championed by ColBERT <ref type="bibr" target="#b34">(Khattab and Zaharia, 2020)</ref>, seeks to mitigate lost information from compressing an entire sequence into a single vector. In multi-vector retrieval, each document is represented by all of its individual token vectors, and the similarity between a query and a document is computed using the MaxSim<ref type="foot" target="#foot_5">foot_5</ref> operator.</p><p>We adopt the training setup of JaCol-BERTv2.5 <ref type="bibr">(Clavié, 2024)</ref>, an update on the ColBERTv2 <ref type="bibr" target="#b55">(Santhanam et al., 2022)</ref> training procedure, with a batch size of 16 and a 5% learning rate warmup. We train all models by distilling the knowledge of a teacher model by using the KL-Divergence between the normalized teacher and student scores. Models are trained on 810k samples from MS-Marco <ref type="bibr" target="#b4">(Bajaj et al., 2016)</ref> and teacher scores from BGE-M3 (Chen et al., 2024), using the PyLate library (Chaffin and Sourty, 2024).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Long-Context Text Retrieval</head><p>With a native 8192 context length, ModernBERT improves long-context performance over most existing encoders. However, there are relatively few standardized long-context benchmarks for encoder-only models, and most benchmarks, such as Needle-in-a-haystack <ref type="bibr" target="#b32">(Kamradt, 2023)</ref> and RULER <ref type="bibr" target="#b23">(Hsieh et al., 2024)</ref> are geared towards generative tasks. Given this limitation, we demonstrate improved long-context performance on the English subset of MLDR (Chen et al., 2024), a long-context retrieval benchmark comprised of over 200,000 long documents. We evaluate three settings:</p><p>Single Vector -Out-Of-Domain Models are trained on short-context MS-MARCO as described above, and is evaluated on long context MLDR without any further fine-tuning. Single Vector -In Domain Models trained on MS-MARCO are further fine-tuned on longcontext MLDR training set before being evaluated.</p><p>Multi-Vector -Out-Of-Domain Due to its token-level MaxSim mechanism, ColBERT models are able to generalize to long-context without any specific training <ref type="bibr" target="#b6">(Bergum, 2024)</ref>. We directly evaluate the best checkpoints from Section 3.1.2 without any further fine-tuning on MLDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Code Retrieval</head><p>Fueled by increasingly good code completion models <ref type="bibr">(Jiang et al., 2024a)</ref>, downstream applications have quickly grown in popularity following the emergence of code assistants. 10 Encoder-only models are used to process and retrieve large quantities of code-related information under resource constraints, increasing the importance of measuring and improving code capabilities of encoder models <ref type="bibr">(Li et al., 2024)</ref>. Unlike most previous encoders which were largely trained only on textual data <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019a;</ref><ref type="bibr" target="#b48">Portes et al., 2023;</ref><ref type="bibr" target="#b84">Zhang et al., 2024;</ref><ref type="bibr" target="#b46">Nussbaum et al., 2024)</ref>, ModernBERT is pre-trained on code and uses a code-aware tokenizer 11 .</p><p>To measure programming-related performance, we evaluate all models on CodeSearchNet <ref type="bibr" target="#b25">(Husain et al., 2019)</ref>, a code-to-text benchmark where the model must identify relevant docstring or comments for code blocks, and StackOverflow-QA (Li   10 Spearheaded by GitHub Copilot in 2021 11 Avoiding issues such as the ones seen in T5 <ref type="bibr" target="#b52">(Raffel et al., 2020)</ref>, whose vocabulary did not include curly braces. <ref type="bibr">et al., 2024)</ref>, where the model must identify relevant responses to StackOverflow questions, in a "hybrid" setting where documents contain both text and code. The latter benchmark also leverages longcontext capabilities, as its queries and documents respectively contain 1,400 and 1,200 words on average, leading to average token counts of over 2000.</p><p>We evaluate these benchmarks using the CoIR (CodeIR) framework <ref type="bibr">(Li et al., 2024)</ref>, as singlevector retrieval tasks. All models are trained by re-using the best hyper-parameters identified in Section 3.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Downstream Results and Discussion</head><p>Aggregated results for all evaluations are presented in Table <ref type="table" target="#tab_0">1</ref>. For BEIR and GLUE, the two common evaluation suites, we follow existing practice in reporting the average results. Detailed results are provided in Appendix E.</p><p>In terms of downstream performance, Modern-BERT is the strongest overall model at both the BASE and LARGE model sizes. ModernBERT represents a Pareto improvement on all tasks over the original BERT and RoBERTA models, with better performance on every evaluation category.</p><p>Short-Context Retrieval On BEIR, both variants of ModernBERT outperform existing encoders in both the DPR and ColBERT settings, including the recent GTE-en-MLM and NomicBERT models designed to serve as better backbones for retrieval <ref type="bibr" target="#b84">(Zhang et al., 2024;</ref><ref type="bibr" target="#b46">Nussbaum et al., 2024)</ref> ModernBERT-large increases its lead despite having comparatively fewer parameters at 395M to GTE-en-MLM-large's 435M.</p><p>Long-Context Retrieval -Single Vector In the DPR setting, ModernBERT achieves impressive performance on MLDR, a long-context text retrieval task. However, these results also highlight an interesting phenomenon: without long-context finetuning ModernBERT outperforms both shortercontext models and the long-context NomicBERT but performs noticeably worse than GTE-en-MLM. The performance gap narrows considerably when evaluated in-domain, with both models performing similarly. This suggests that ModernBERT can effectively process long context sequences as a dense encoder but may require more adapted tuning. We plan to explore multiple potential explanations for this phenomenon in future work, including the impact of local attention or GTE-en-MLM having spent a larger part of its pretraining compute budget on longer sequence lengths <ref type="bibr" target="#b84">(Zhang et al., 2024)</ref>.</p><p>Long-Context Retrieval -Multi-Vector In the ColBERT setting, long-context models (GTEen-MLM, NomicBERT, and ModernBERT) all outperform short-context models by at least 40 NDCG@10 points without requiring any specific finetuning. These results confirm the findings of Bergum (2024), who showed that ColBERT models are particularly well-suited to long-context retrieval tasks. Among the long-context models, Modern-BERT outperforms other long-context models, with at least a 9 NDCG@10 point lead on both model sizes. We theorize that these sizable gains could be explained by our long pretraining ensuring few, if any, tokens are under-trained, as well as a potentially synergistic effect of local attention with ColBERT-style retrieval, but leave further exploration of this phenomenon to future work.</p><p>Natural Language Understanding Both Mod-ernBERT models demonstrate exceptional NLU results, as measured by GLUE. ModernBERTbase surpasses all existing base models, including DeBERTaV3-base, becoming the first MLMtrained model to do so. This is surprising, as DeBERTaV3 was trained with the Replaced-Token-Detection objective, which was previously thought to yield stronger downstream NLU performance <ref type="bibr">(Clark et al., 2020;</ref><ref type="bibr" target="#b21">He et al., 2023)</ref>. ModernBERT-large is the second-best large encoder on GLUE, almost matching DeBERTaV3large with one-tenth fewer parameters while processing tokens in half the time (see Section 4).</p><p>Code On programming tasks, in both code-totext (CodeSearchNet) and longer-context hybrid settings (StackQA), ModernBERT outperforms all other models. This result was expected, as it is the only evaluated encoder to be trained on a data mixture including programming data. These results, combined with ModernBERT's strong showings on other tasks, indicates that ModernBERT has improved understanding of code at no detriment to its ability to process natural text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Setting</head><p>To measure inference efficiency across multiple sequence lengths, we create 4 synthetic sets of 8192 documents<ref type="foot" target="#foot_6">foot_6</ref> . The first two document sets are fixed-length: in fixed short-context, all documents contain 512 tokens and in fixed long-context all documents contain 8192 tokens<ref type="foot" target="#foot_7">foot_7</ref> . To account for the impact of unpadding, we also create two varying-length document sets, where the number of tokens in each set are defined by a normal distribution centered on half the maximum sequence length, 256 and 4096 tokens, respectively. Full data statistics are provided in Appendix F.</p><p>We then evaluate all models based on the number of tokens they can process per second, averaged over ten runs. All efficiency evaluations are ran on a single NVIDIA RTX 4090, one of the target GPUs of ModernBERT outlined in Section 2.1.3 We evaluate the GTE-en-MLM models under two settings: out-of-the box, and with the use of the xformers <ref type="bibr" target="#b36">(Lefaudeux et al., 2022)</ref> library, which enables efficiency enhancements such as unpadding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>All tokens-per-second efficiency results are presented in Table <ref type="table" target="#tab_1">2</ref>, with absolute run-times provided in Appendix F. ModernBERT stands out as the most efficient model overall. On short context, it processes fixed-length 512 token inputs faster than all other recent encoders, although slower than the original BERT and RoBERTa models<ref type="foot" target="#foot_8">foot_8</ref> . On longcontext, ModernBERT is faster than all competing encoders, processing documents 2.65 and 3 times faster than the next-fastest encoder at the BASE and LARGE sizes, respectively. ModernBERT-large's processing speed at length 8192 (46,801 tokens per second) is closer to that of GTE-en-MLM base (47,507 tokens per second) than it is to GTE-en-MLM-large (16,532 tokens per second).</p><p>On variable-length inputs, both GTE-en-MLM and ModernBERT models are considerably faster than all other models, largely due to unpadding. However, ModernBERT remains noticeably more efficient than GTE-en-MLM, processing 14.5-30.9 percent more tokens per second at low context lengths and 98.8-118.8 percent more at longer context lengths, thanks to its use of local attention.</p><p>ModernBERT is the overall most memory efficient model on both model sizes. ModernBERTbase is able to process batch sizes twice as large as every other model on both input lengths. ModernBERT-large is slightly less memory efficient than the original BERT-large on short-context inputs, but can process batches at least 60 percent bigger than every other large model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present ModernBERT, an open family of encoder-only models which set a new state of the art over existing encoder models on a wide range of classification and retrieval tasks. We show that encoders benefit from both recent pretraining data scales and architecture improvements from autoregressive LLMs.</p><p>ModernBERT has a native sequence length of 8,192 tokens and incorporates recent architecture improvements, such as GeGLU layers, RoPE positional embeddings, and alternating local-global attention. ModernBERT is the first open model to feature entire model unpadding and is the first encoder designed in a hardware-aware way to maximize inference efficiency.</p><p>ModernBERT pushes the encoder state of the art forward across a wide range of benchmarks. On GLUE, ModernBERT-base is the first encoder to beat DeBERTaV3-base since its release in 2021. ModernBERT is in a class of its own in code and ColBERT-style long-context retrieval benchmarks, scoring at least 6.85 and 9.1 percentage points higher than the closest model, respectively, while remaining state-of-the-art on short-context retrieval in both single and multi-vector settings.</p><p>At the same time, ModernBERT processes short context inputs twice as fast as DeBERTaV3 and long-context inputs two times faster than the next fastest model with best-in-class memory efficiency.</p><p>ModernBERT is a generational leap over the original encoder models, with notable performance improvements over BERT and RoBERTa on both classification and retrieval tasks. ModernBERT is one of the few encoders to support long-context and programming applications, while simultaneously setting a new record in encoder inference efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Language This study focuses exclusively on the English language, and trains on a very large number of tokens. As such, a major limitation of our work is that it is not directly applicable to other languages, and potentially even less-so to lower resources languages.</p><p>Biases Our model is trained largely on web data, as a result, all of its representations are subject to the biases present in such data.</p><p>Harmful Content Generation The MLM objective gives the model some ability to generate text by suggesting a given token to replace the [MASK] token <ref type="bibr" target="#b54">(Samuel, 2024)</ref>, which could result in the generation of harmful content. However, Modern-BERT is not, primarily, a generative model, and as such, has not been trained to and therefore cannot generate longer sequences of text. As a result, it is considerably less likely to be at risk of generating harmful content of any kind.</p><p>MLM-only objective Given the strong results of DeBERTav3 on classification tasks but weak ones on retrieval, it seems that a training leveraging both MLM and RTD might be better suited to achieve best results on classification. Extending our work to RTD is thus a promising line of research.</p><p>Scaling Besides the architectural modifications, a key aspect of our studies is data scaling. However, other scaling axes, notably in terms of model parameters are left unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Contribution Statement</head><p>BW, AC, and BC jointly led the project and contributed to all parts of it. BW worked on all aspects of the project and contributed to all major decisions. He led model design, model training, implemented the majority of the model architecture, and assisted with data selection, elevations, and paper writing. AC co-initiated the project and worked on all aspects of it, including project coordination. Notably, he contributed to monitoring training runs and coled ablations, final evaluations and paper writing. BC initiated the project and worked on all aspects of it. He contributed to model design and co-led final evaluations, led paper writing, and contributed to the context extension data processing. OW led and conducted the majority of the data selection, processing, and discussion, for all stages of training. He also contributed valuable inputs throughout all stages of the project. OH and ST contributed to a majority of the stages of the project, in particular model architecture and training, with both discussions, implementations and paper writing. Other contributions include pretraining monitoring, final traditional evaluations, and ablations. ST specifically worked on adapting the RoPE kernel for unpadded sequences and running the final GLUE benchmarks. OH additionally conducted a thorough investigation into complex issues that arose during training. RB contributed greatly to the initial evaluation work, focusing on ablations and in-training evals. AG and FL contributed to training efficiency, especially in implementing sequence packing. AG and GA contributed to model evaluations, especially in long context evaluations. TA contributed to discussions throughout the project and assisted in integrating the original research implementation with open source software. NC contributed to context extension data mixtures, and provided insight into model training and on improving the quality of code data. IP and JH provided guidance and support throughout the project, especially on key decisions. survey of large language models. arXiv preprint arXiv:2303.18223.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Settings</head><p>Detailed training settings can be found in Table <ref type="table">3</ref>.</p><p>During training we used MNLI as a live evaluation, along with validation loss and token accuracy metrics on a 500 million randomly sampled sequences from the source datasets.</p><p>We use Composer (Mosaic ML Team, 2021) as our training framework and optimī <ref type="bibr" target="#b70">(Warner, 2023)</ref> for our optimizer implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Batch Size Schedule</head><p>Batch size warmup is a common-knowledge trick to speed up model training when working with medium to large batch sizes. Instead of "wasting" a full batch on updating the suboptimal initial weight distribution, we update the model weights on a gradually increasing batch size. Batch size warmup is usually longer than learning rate warmup, and can be thought of as providing a higher initial learning rate with a mini-learning rate decay to the defined learning rate schedule. We warmup Mod-ernBERT's batch size from 768 to 4,608 over 50 billion tokens and from 448 to 4,928 over 10 billion tokens, for -base and -large, respectively, with an uneven token schedule so each batch size has the same number of update steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Weight Tiling</head><p>Following the Phi family of models <ref type="bibr" target="#b40">(Li et al., 2023;</ref><ref type="bibr" target="#b28">Javaheripi et al., 2023)</ref>, we initialized ModernBERT-large directly from ModernBERTbase's pretraining weights using center tiling and Gopher layer scaling <ref type="bibr">(Rae et al., 2022)</ref>. Since Base's weight matrices are smaller than Large's, we centered Base' weights, accounting for each token embedding and attention head, then filled rest the of the weights using wraparound. Like Phi, we tested center initialization with random edge values and tiling from an edge, but both of these underperformed center tiling with wraparound. This weight initialization strategy greatly accelerates ModernBERT-large's initial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Weight Decay</head><p>We did not apply weight decay to the bias terms or normalization layers. Instead of PyTorch-style decoupled weight decay, we applied fully decoupled weight decay following <ref type="bibr" target="#b44">Loshchilov and Hutter (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Final Checkpoints</head><p>Inspired by recent work showing that checkpoint averaging yields stronger final models <ref type="bibr" target="#b12">(Dubey et al., 2024;</ref><ref type="bibr">Clavié, 2024)</ref>, we selected our final checkpoints by experimenting with various averaging methods and evaluating them on a subset of evaluation tasks. In no cases did Exponential Moving Average during annealing, as used by <ref type="bibr" target="#b12">Dubey et al. (2024)</ref>, result in stronger performance. ModernBERT-base is the result of averaging the 3 best performing annealing checkpoints with the final one. Averaging did not yield successful results on the large size, ModernBERT-Large model is the best performing annealing checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Design</head><p>From <ref type="bibr">Anthony et al. (2024)</ref>, in addition to setting attention heads as multiples of 64 and setting the embedding matrix as a power of 2 or multiple of 64, there are three model design choices to maximize performance (assuming float16 or bfloat16 computation):</p><p>• Tensor Core Requirement: Weight matrix dimensions should be divisible by 64</p><p>• Tile Quantization: Weight matrix is divisible into 128 × 256 blocks.</p><p>• Wave Quantization: Number of blocks is divisible by the number of streaming multiprocessors (SM).</p><p>Given that we wanted to target good performance across multiple GPUs with a wide variety of SM counts, wave quantization is an impossible ask. So we selected a basket of GPUs (NVIDIA T4, A10, L4, RTX 3090, RTX 4090, A100, and H100) and calculated the approximate SM utilization for each by dividing the modulus blocks by the number of SMs. This appeared to be a decent performance heuristic in our spot checking. We then designed our models to maximize performance on the basket of GPUs, putting more weight on inference GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Log</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Sampling Issue</head><p>Our first pretraining run of ModernBERT-base ended in disaster as the loss exhibited a slow seesaw pattern before slowly diverging. Despite using PyTorch's distributed random sampler, training metrics suggested that the model was training ModernBERT-base showed a continuous, but diminishing, improvement on training loss, validation metrics, and live evaluations through the entire 1.719 trillion token training phase. This highlights one of the risks of training with a constant learning rate, other learning rate schedules can mitigate selecting a too high learning rate (or too small batch size) by lowering the learning rate throughout training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Architecture ablations</head><p>To select the updates to add in the ModernBERT architecture, we performed different ablations, except where stated, most ablations where ran at the 8-20 billion token scale:</p><p>• We compared two GLU layers, GeGLU and SwiGLU. We find close to no difference between the two and choose to use GeGLU layers.</p><p>• Using different percentage of the head dimension for the RoPE dimension (50, 75, 100). Lower percentages gave slightly better results. However, the observed difference was minimal. As the ablations were conducted at a considerably smaller scale than the final training, we choose to err on the side of caution and opt to keep the dimension at 100 % to avoid potentially hindering the capabilities of the fully trained models.</p><p>• Both LayerNorm and RMSNorm yielded very similar results. While RMSNorm is theoretically faster, at the time this work was conducted, PyTorch did not have a native RMSNorm implementation, leading to eagermode RMSNorm being the default implementation used for many users. To ensure Modern-BERT has the highest possible out-of-the-box efficiency, we choose to use LayerNorm in the final models.</p><p>• We investigated using parallel attention to compute the MLP and attention matrices at the same time, which has been shown to increase processing speeds for larger model sizes <ref type="bibr">(Chowdhery et al., 2023)</ref>. However, for models within our targe sizes and pre-training sequence length, the speed-up we observed was minimal while we encountered significant degradation in downstream performance.</p><p>As such, we do not use parallel attention. It is however possible that larger encoders and/or larger sequence lengths might see a different trade-off.</p><p>• We explored the use of alternating global/local attention, with global attention every 3 layers and local attention over a 128 token sliding window otherwise. This setup yielded identical downstream performance when compared to the use of global attention in every layer, even at 100 billion tokens, while resulting in major speedups.</p><p>• We experimented with multiple tokenizers, before selecting our final one, based on a modified OLMo <ref type="bibr" target="#b18">(Groeneveld et al., 2024)</ref> tokenizer, which performed the best out of the recent tokenizers evaluated. Tokenizers from the BERT and RoBERTa generation of encoder models had competitive downstream performance on MNLI, but we theorized that their lack of recent training data and lack of code support would hinder downstream applications. Interestingly, we observed significant downstream performance degradation when using the Llama 2 <ref type="bibr" target="#b64">(Touvron et al., 2023</ref>) tokenizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Extended results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Full GLUE results</head><p>The results for all the models each GLUE subsets are presented in Table <ref type="table" target="#tab_2">5</ref>. The values for prior models are extracted from the literature. As mentioned in Section 3.1.1, we follow standard practice <ref type="bibr">(Liu et al., 2019a;</ref><ref type="bibr" target="#b48">Portes et al., 2023;</ref><ref type="bibr" target="#b21">He et al., 2023)</ref> and conduct an hyperparameter search on each subset. More specifically, we perform a sweep over learning rates in [1e-5, 3e-5, 5e-5, 8e-5], weight decay in [1e-6, 5e-6, 8e-6, 1e-5], and number of epochs in [1, 2, 3] for tasks in <ref type="bibr">MNLI,</ref><ref type="bibr">and RTE,</ref><ref type="bibr">and [2,</ref><ref type="bibr">5,</ref><ref type="bibr">10]</ref> for tasks in QNLI, QQP, CoLA, MRPC, and STS-B. The final values are detailed in Table <ref type="table">6</ref>. Early stopping is used for all the fine-tuning runs which reduces the overall fine-tuning time considerably. RTE MRPC and STS-B checkpoints are trained starting from the MNLI checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Full BEIR results</head><p>In the main body, we only report the average score over the 15 very diverse datasets of BEIR. We report the results on every subsets for both CoLA 8e-5 1e-6 5 3e-5 8e-6 5 MNLI 5e-5 5e-6 1 3e-5 1e-5 1 MRPC 5e-5 5e-6 10 8e-5 5e-6 2 QNLI 8e-5 5e-6 2 3e-5 5e-6 2 QQP 5e-5 5e-6 10 5e-5 8e-6 2 RTE 5e-5 1e-5 3 5e-5 8e-6 3 SST-2 8e-5 1e-5 2 1e-5 1e-6 3 STSB 8e-5 5e-6 10 8e-5 1e-5 10</p><p>Table 6: Fine-tuning hyperparameters for ModernBERT on GLUE tasks. LR: Learning Rate, WD: Weight Decay, Ep: Epochs.</p><p>single and multi-vector retrieval in Table <ref type="table">7</ref> and Table <ref type="table">8</ref> respectively. For both settings and for every model, we perform a sweep for learning rates in <ref type="bibr">[1e-5, 2e-5, 3e-5, 5e-5, 8e-5, 1e-4]</ref> and choose the model obtaining the best average result over a subset of datasets composed of NFCorpus, SciFact, TREC-Covid and FiQA as the final model. Best learning rates for every setting are reported in Table <ref type="table">9</ref>. Although ModernBERT showcase strong results across the board, it should be noted that an important factor in its performance is TREC-COVID <ref type="bibr" target="#b67">(Voorhees et al., 2021)</ref>, potentially showcasing the benefits of ModernBERT being trained with a more recent knowledge cutoff than most existing encoders. However, NomicBERT and GTE have also been trained on updated data, so the cutoff cannot be the only factor affecting the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Efficiency</head><p>Full statistics of the synthetic datasets used to evaluate the efficiency of the models in Section 4 are given in Table <ref type="table" target="#tab_0">10</ref>. The detailed runtimes, alongside with the maximum batch size for every model is detailed in Table <ref type="table" target="#tab_0">11</ref>.</p><p>The high maximum batch-size achieved by Mod-ernBERT models, considerably higher than any other models, highlight the strong memory efficiency of the model at both sizes. Inversely, it is worth noting that while DeBERTaV3 has competitive GLUE performance, it stands out as particularly inefficient, both in its memory use and processing speed. Indeed, on both model sizes, De-BERTaV3's memory use is 5-to-7 times higher than ModernBERT's, and it processes inputs, two times slower even in the most favorable scenario where all sequences are at the maximum possible length, thus negating any advantage from unpadding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>4 and 4 As detailed in their 2023 NeurIPS presentation. initialize -large's weights from ModernBERT-base. In ablation runs, this consistently matched Phi's improved training results and greatly speed up the initial loss decrease of our model training 5 . Details are provided in Appendix A.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results for all models across an overview of all tasks. CSN refers to CodeSearchNet and SQA to StackQA. MLDR ID refers to in-domain (fine-tuned on the training set) evaluation, and MLDR OOD to out-of-domain.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>IR (DPR)</cell><cell></cell><cell cols="2">IR (ColBERT)</cell><cell>NLU</cell><cell>Code</cell></row><row><cell></cell><cell>Model</cell><cell cols="7">BEIR MLDR OOD MLDR ID BEIR MLDR OOD GLUE CSN SQA</cell></row><row><cell></cell><cell>BERT</cell><cell>38.9</cell><cell>23.9</cell><cell>32.2</cell><cell>49.0</cell><cell>28.1</cell><cell>84.7</cell><cell>41.2 59.5</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>37.7</cell><cell>22.9</cell><cell>32.8</cell><cell>48.7</cell><cell>28.2</cell><cell>86.4</cell><cell>44.3 59.6</cell></row><row><cell>Base</cell><cell>DeBERTaV3 NomicBERT</cell><cell>20.2 41.0</cell><cell>5.4 26.7</cell><cell>13.4 30.3</cell><cell>47.1 49.9</cell><cell>21.9 61.3</cell><cell>88.1 84.0</cell><cell>17.5 18.6 41.6 61.4</cell></row><row><cell></cell><cell cols="2">GTE-en-MLM 41.4</cell><cell>34.3</cell><cell>44.4</cell><cell>48.2</cell><cell>69.3</cell><cell>85.6</cell><cell>44.9 71.4</cell></row><row><cell></cell><cell>ModernBERT</cell><cell>41.6</cell><cell>27.4</cell><cell>44.0</cell><cell>51.3</cell><cell>80.2</cell><cell>88.4</cell><cell>56.4 73.6</cell></row><row><cell></cell><cell>BERT</cell><cell>38.9</cell><cell>23.3</cell><cell>31.7</cell><cell>49.5</cell><cell>28.5</cell><cell>85.2</cell><cell>41.6 60.8</cell></row><row><cell>Large</cell><cell cols="2">RoBERTa DeBERTaV3 GTE-en-MLM 42.5 41.4 25.6</cell><cell>22.6 7.1 36.4</cell><cell>36.1 19.2 48.9</cell><cell>49.8 46.7 50.7</cell><cell>28.8 23.0 71.3</cell><cell>88.9 91.4 87.6</cell><cell>47.3 68.1 21.2 19.7 40.5 66.9</cell></row><row><cell></cell><cell>ModernBERT</cell><cell>44.0</cell><cell>34.3</cell><cell>48.6</cell><cell>52.4</cell><cell>80.4</cell><cell>90.4</cell><cell>59.5 83.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Memory (max batch size, BS) and Inference (in thousands of tokens per second) efficiency results on an NVIDIA RTX 4090, averaged over 10 runs. Dashes indicate unsupported configurations.</figDesc><table><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>GLUE<ref type="bibr" target="#b68">(Wang et al., 2018)</ref> dev set scores. α taken from Table8of(Liu et al., 2019a), β taken from TableS3of<ref type="bibr" target="#b48">(Portes et al., 2023)</ref>, γ from Table2of<ref type="bibr" target="#b46">(Nussbaum et al., 2024</ref>), δ from Table21of<ref type="bibr" target="#b84">(Zhang et al., 2024)</ref>, ϵ from Table2of<ref type="bibr" target="#b49">(Qiang et al., 2024</ref>) and ζ from Table3of<ref type="bibr" target="#b21">(He et al., 2023)</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Single Sentence Paraphrase and Similarity Natural Language Inference</cell></row><row><cell></cell><cell>Model</cell><cell>Params</cell><cell cols="8">Seq. CoLA SST-2 MRPC STS-B QQP MNLI QNLI</cell><cell>RTE</cell></row><row><cell></cell><cell>BERT β</cell><cell>110M</cell><cell>512</cell><cell>59.0</cell><cell>93.1</cell><cell>89.5</cell><cell>89.4</cell><cell>91.4</cell><cell>85.4</cell><cell>91.6</cell><cell>78.2</cell></row><row><cell></cell><cell>RoBERTa α</cell><cell>125M</cell><cell>512</cell><cell>63.6</cell><cell>94.8</cell><cell>90.2</cell><cell>91.2</cell><cell>91.9</cell><cell>87.6</cell><cell>92.8</cell><cell>78.7</cell></row><row><cell>Base</cell><cell>DeBERTav3 ϵ MosaicBERT-128 β NomicBERT-2048 γ</cell><cell cols="2">183M 137M 137M 2048 512 128</cell><cell>69.2 58.2 50.0</cell><cell>95.6 93.5 93.0</cell><cell>89.5 89.0 88.0</cell><cell>91.6 90.3 90.0</cell><cell>92.4 92.0 92.0</cell><cell>90.0 85.6 86.0</cell><cell>94.0 91.4 92.0</cell><cell>83.8 83.0 82.0</cell></row><row><cell></cell><cell>GTE-en-MLM δ</cell><cell cols="2">137M 8192</cell><cell>57.0</cell><cell>93.4</cell><cell>92.1</cell><cell>90.2</cell><cell>88.8</cell><cell>86.7</cell><cell>91.9</cell><cell>84.8</cell></row><row><cell></cell><cell>ModernBERT</cell><cell cols="2">149M 8192</cell><cell>65.1</cell><cell>96.0</cell><cell>92.2</cell><cell>91.8</cell><cell>92.1</cell><cell>89.1</cell><cell>93.9</cell><cell>87.4</cell></row><row><cell></cell><cell>BERT β</cell><cell>330M</cell><cell>512</cell><cell>56.2</cell><cell>93.3</cell><cell>87.8</cell><cell>90.6</cell><cell>90.9</cell><cell>86.3</cell><cell>92.8</cell><cell>83.8</cell></row><row><cell>Large</cell><cell>RoBERTa α DeBERTav3 ζ GTE-en-MLM δ</cell><cell cols="2">355M 434M 434M 8192 512 512</cell><cell>68.0 75.3 60.4</cell><cell>96.4 96.9 95.1</cell><cell>90.9 92.2 93.5</cell><cell>92.4 93.0 91.4</cell><cell>92.2 93.3 89.2</cell><cell>90.2 91.8 89.2</cell><cell>94.7 96.0 93.9</cell><cell>86.6 92.7 88.1</cell></row><row><cell></cell><cell>ModernBERT</cell><cell cols="2">395M 8192</cell><cell>71.4</cell><cell>97.1</cell><cell>91.7</cell><cell>92.8</cell><cell>92.7</cell><cell>90.8</cell><cell>95.2</cell><cell>92.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base</cell><cell></cell><cell></cell><cell>Large</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Task</cell><cell>LR</cell><cell cols="2">WD Ep</cell><cell>LR</cell><cell cols="2">WD Ep</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>While many efficient BERT training recipes disable the bias term in the decoder, e.g.<ref type="bibr" target="#b16">Geiping and Goldstein (2023)</ref>, we hypothesized a decoder bias might help alleviate weight tying's negative effects<ref type="bibr" target="#b14">(Gao et al., 2019;</ref><ref type="bibr" target="#b71">Welch et al., 2020)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>This initialization reduced the amount of batch size and LR warmup needed for ModernBERT-large</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>We only lowered the LR for ModernBERT-base, as large already decreased LR during the 1024 token training phase.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>As<ref type="bibr" target="#b84">(Zhang et al., 2024)</ref> do not explicitly mention a parameter sweep, we initially ran the same hyperparameter sweep as we did for ModernBERT, but observed inconsistencies in the results. To avoid under-representing GTE-en-MLM's capabilities, we choose to use their reported GLUE results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>At the time of this paper's writing, over half of the 100 most downloaded models on the HuggingFace Model Hub were encoder-based retrieval models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>The sum for every query token of its similarity with the most similar document token</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6"><p>Many common benchmarks are biased towards low and uniform sequence lengths, which is unrepresentative of many real-world situations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7"><p>512 being the maximum length of most existing encoders, while 8192 is the maximum length of all long-context ones.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8"><p>This is partially due to the relatively low parameter count of BERT and RoBERTa compared to more recent encoders.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>The authors would like to acknowledge &amp; thank the many people who assisted, supported, or offered insights useful for the completion of this project.</p><p>We are particularly thankful for the one-off implementation or evaluation work conducted by <rs type="person">Jack Cook</rs>, <rs type="person">Mark Tenenholtz</rs>, <rs type="person">Johno Whitaker</rs>, and <rs type="person">Wayde Gilliam</rs>. We also extend similar thanks to <rs type="person">Zach Nussbaum</rs> for assisting in resolving issues we encountered with NomicBERT during evaluation.</p><p>We would like to acknowledge <rs type="person">Enrico Shippole</rs>, <rs type="person">Daniel Han</rs>, <rs type="person">Colin Raffel</rs>, <rs type="person">Pierre-Carl Langlais</rs>, <rs type="person">Omar Khattab</rs>, <rs type="person">Urchade Zaratiana</rs>, <rs type="person">Aurélien Lac</rs>, <rs type="person">Amélie Chatelain</rs>, and <rs type="person">Raphaël Sourty</rs>, for their helpful contributions to discussions.</p><p>We also thank <rs type="institution">Weights&amp;Biases</rs> for providing free access to their platform, in particular <rs type="person">Morgan McGuire</rs> and <rs type="person">Thomas Capelle</rs> for their support.</p><p>We thank <rs type="person">HuggingFace's Arthur Zucker</rs>, Cyril</p><p>Vallez, and <rs type="person">Pedro Cuenca</rs> for assisting with dayone HuggingFace support. Finally, we acknowledge <rs type="institution">Orange Business Cloud Avenue as compute</rs> provider and their hardware support throughout the project and thank <rs type="institution">LightOn</rs> for sponsoring the compute.</p></div>
			</div>
			<div type="funding">
<div><p>embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through selfknowledge distillation. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August <rs type="grantNumber">11-16</rs>, <rs type="grantNumber">2024</rs>, pages <rs type="grantNumber">2318-2335</rs>. <rs type="funder">Association for Computational Linguistics</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DadgvjG">
					<idno type="grant-number">11-16</idno>
				</org>
				<org type="funding" xml:id="_9TtVH9M">
					<idno type="grant-number">2024</idno>
				</org>
				<org type="funding" xml:id="_Vx5dEzm">
					<idno type="grant-number">2318-2335</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> 15 <p>, we determined that the PyTorch random sampler returns sequentially biased samples when the number of samples is somewhere between 500 million and 1 billion samples 16 . We resolved this issue by replacing the PyTorch sam- 15 We found a comment and GitHub issue about this in the Olmo codebase after resolving the issue ourselves. 16 We did not conduct a rigorous statistical analysis to determine exactly when this happens.</p><p>pler with NumPy's PCG64DXSM random sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Large Rollback</head><p>We rolled back and restarted ModernBERT-large training at a lower learning rate of 5e-5 and lower weight decay of 1e-6 for the last 800 billion tokens. Prior to restarting training, large's training loss, validation metrics, and live evaluations on MNLI had plateaued for a few hundred billion tokens at the higher 5e-4 learning rate. In contrast, Table <ref type="table">8</ref>: BEIR <ref type="bibr" target="#b63">(Thakur et al., 2021)</ref> nDCG@10 scores for multi-vector retrieval models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Single-vector (DPR) Multi-vector (ColBERT)  <ref type="bibr">8,192 8,192 8,192 8,192</ref> Table <ref type="table">10</ref>: Token statistics for the synthetic datasets used in efficiency evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Licensing</head><p>We </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Voznesensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="929" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Aamir Shafi, Hari Subramoni, and Dhabaleswar Panda. 2024. The case for co-designing model architectures with hardware</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.14489</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the difficulty of warm-starting neural network training</title>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">T</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno>CoRR, abs/1910.08475</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m">Ms marco: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Announcing vespa longcontext ColBERT</title>
		<author>
			<persName><forename type="first">Jo</forename><surname>Kristian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bergum</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Vespa Blog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">Gregory</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gpt-neox-20b: An open-source autoregressive language model</title>
		<author>
			<persName><forename type="first">Sidney</forename><surname>Pmlr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><surname>Phang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BigScience Episode# 5-Workshop on Challenges &amp; Perspectives in Creating Large Language Models</title>
		<meeting>BigScience Episode# 5-Workshop on Challenges &amp; Perspectives in Creating Large Language Models</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16344" to="16359" />
		</imprint>
	</monogr>
	<note>. memory-efficient exact attention with io-awareness</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Boris</forename><surname>Dayma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Cuenca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Saifullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanishq</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phúc</forename><surname>Lê Khȃc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Melas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritobrata</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5146400</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Dall•e mini</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Data engineering for scaling language models to 128k context</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10171</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Representation degeneration problem in training natural language generation models</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.12009</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">How to train long-context language models (effectively)</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.02660</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cramming: Training a language model on a single GPU in one day</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-29">2023. 23-29 July 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="11117" to="11143" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Team</forename><surname>Gemma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léonard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00118</idno>
		<title level="m">Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical size</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
		<title level="m">Accelerating the science of language models</title>
		<meeting><address><addrLine>Olmo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Loubna Ben Allal, Leandro von Werra, and Martin Jaggi. 2024. Scaling laws and compute-optimal training beyond fixed training durations</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Bakouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atli</forename><surname>Kosson</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2405.18392</idno>
		<idno>CoRR, abs/2405.18392</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Passing the torch: Training a mamba model for smooth handover</title>
		<author>
			<persName><forename type="first">Oskar</forename><surname>Hallström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Said</forename><surname>Taghadouini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Thiriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2023-05-01">2023. May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ruler: What&apos;s the real context size of your long-context language models</title>
		<author>
			<persName><forename type="first">Cheng-Ping</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kriman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Rekesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.06654</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Minicpm: Unveiling the potential of small language models with scalable training strategies</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuge</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoqun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Leng Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2404.06395</idno>
		<idno>CoRR, abs/2404.06395</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. 2024. Scaling laws and compute-optimal training beyond fixed training durations</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Bakouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atli</forename><surname>Kosson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18392</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How to train BERT with an academic budget</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Berchansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.831</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10644" to="10652" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Phi-2: The surprising power of small language models</title>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marah</forename><surname>Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microsoft Research Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Beavertails: Towards improved safety alignment of llm via a humanpreference dataset</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mickel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04657</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">2024a. A survey on large language models for code generation</title>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.00515</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">2024b. Wildteaming at scale: From in-thewild jailbreaks to (adversarially) safer language models</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavel</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faeze</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niloofar</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.18510</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Needle In A Haystack -pressure testing LLMs</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Kamradt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Github</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The most dramatic optimization to nanogpt so far ( 25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64)</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Vladimir Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2020.EMNLP-MAIN.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16">2023. 2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
	<note>Dense passage retrieval for open-domain question answering Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over BERT</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-07-25">2020. July 25-30, 2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
	<note>SIGIR 2020, Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Michael Krell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kosec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02027</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Caggiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Naren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Tintore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Sizov</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/xformers" />
		<title level="m">xformers: A modular and hackable transformer modelling library</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">1607</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuicai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Quan Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.02883</idno>
		<title level="m">Yasheng Wang, and Ruiming Tang. 2024. Coir: A comprehensive benchmark for code information retrieval models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05463</idno>
	</analytic>
	<monogr>
		<title level="m">Textbooks are all you need ii: phi-1.5 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mobilellm: Optimizing sub-billion parameter language models for on-device use cases</title>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14905</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://github.com/mosaicml/composer/" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. The Mosaic ML Team. 2021. composer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth</title>
		<author>
			<persName><forename type="first">Thao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Nomic embed: Training a reproducible long context text embedder</title>
		<author>
			<persName><forename type="first">Zach</forename><surname>Nussbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">X</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Duderstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mulyar</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2402.01613</idno>
		<idno>CoRR, abs/2402.01613</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17557</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mosaicbert: A bidirectional encoder optimized for fast pretraining</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Portes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Havens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Venigalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Sardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Khudia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. 2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Bilora: A bi-level optimization framework for overfitting-resilient low-rank adaptation of large pretrained models</title>
		<author>
			<persName><forename type="first">Rushi</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2403.13037</idno>
		<idno>CoRR, abs/2403.13037</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenAI Tech Report</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskeve</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Openai</forename><surname>Blog</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">W</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><surname>Toyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
	</analytic>
	<monogr>
		<title level="j">Cyprien de Masson d&apos;Autume</title>
		<editor>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iason</forename><surname>Gabriel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">William</forename><surname>Isaac</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><surname>Lockhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kareem</forename><surname>Ayoub</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeff</forename><surname>Stanway</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lorrayne</forename><surname>Bennett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Diego de Las Casas</publisher>
			<pubPlace>Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling language models: Methods, analysis &amp; insights from training gopher</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Berts are generative in-context learners</title>
		<author>
			<persName><forename type="first">David</forename><surname>Samuel</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2406.04823</idno>
		<idno>CoRR, abs/2406.04823</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Colbertv2: Effective and efficient retrieval via lightweight late interaction</title>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.NAACL-MAIN.272</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07-10">2022. July 10-15, 2022</date>
			<biblScope unit="page" from="3715" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Toolformer: Language models can teach themselves to use tools</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dessì</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. 2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Flashattention-3: Fast and accurate attention with asynchrony and low-precision</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Bikshandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08608</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4596" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Pmlr</forename><forename type="middle">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.NEUCOM.2023.127063</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scale efficiently: Insights from pretraining and finetuning transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models</title>
		<author>
			<persName><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021</title>
		<meeting>the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021</meeting>
		<imprint>
			<date type="published" when="2021-12">2021. December 2021</date>
		</imprint>
	</monogr>
	<note>Abhishek Srivastava, and Iryna Gurevych virtual</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Unso</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Korat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Pereg</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2209.11055</idno>
		<idno>CoRR, abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2022. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Efficient few-shot learning without prompts Aäron van den Oord, Yazhe Li, and Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Trec-covid: constructing a pandemic information retrieval test collection</title>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tasmeer</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>William R Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirk</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Forum</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binxing</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.03533</idno>
		<title level="m">Text embeddings by weaklysupervised contrastive pre-training</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">optimī: Fast, modern, memory efficient, and low precision pytorch optimizers</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Warner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Improving low compute language modeling with in-domain embedding initialisation</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14109</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08005</idno>
		<title level="m">Should you mask 15% in masked language modeling? Preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Stable and low-precision training for large-scale vision-language models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13013</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">C-pack: Packaged resources to advance general chinese embedding</title>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peitian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Tsirigotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08770</idno>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2020. 18 July 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>A walk with sgd Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Understanding and improving layer normalization. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Hard negative examples are hard, but useful</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23">2020. August 23-28, 2020</date>
			<biblScope unit="page" from="126" to="142" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV 16</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10671</idno>
		<title level="m">Qwen2 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2023-05-01">2023. May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Gliner: Generalist model for named entity recognition using bidirectional transformer</title>
		<author>
			<persName><forename type="first">Urchade</forename><surname>Zaratiana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadi</forename><surname>Tomeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Holat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Charnois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5364" to="5376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Jinle</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.08124</idno>
		<title level="m">Boosting distributed training performance of the unpadded bert model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01179</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06-18">2022. June 18-24, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="1204" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">mgte: Generalized longcontext text representation and reranking models for multilingual text retrieval</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 -Industry Track</title>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 -Industry Track<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-11-12">2024. November 12-16, 2024</date>
			<biblScope unit="page" from="1393" to="1412" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. 2023. A</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m">Model NFCorpus SciFact TREC-Covid FiQA ArguAna Climate-FEVER DBPedia FEVER HotpotQA MSMARCO NQ Quora SciDocs Touche2020 CQADupstack Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><surname>Beir (thakur</surname></persName>
		</author>
		<title level="m">nDCG@10 scores for single-vector retrieval models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m">Model NFCorpus SciFact TREC-Covid FiQA ArguAna Climate-FEVER DBPedia FEVER HotpotQA MSMARCO NQ Quora SciDocs Touche2020 CQADupstack Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Base BERT</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
