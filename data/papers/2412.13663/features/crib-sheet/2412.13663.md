- **Model Overview**: ModernBERT is an encoder-only transformer model that improves upon BERT, achieving state-of-the-art performance on various classification and retrieval tasks.
  
- **Training Data**: Trained on 2 trillion tokens, including diverse sources such as web documents, code, and scientific literature.

- **Sequence Length**: Supports a native sequence length of 8192 tokens, significantly higher than the original BERT's 512 tokens.

- **Architectural Enhancements**:
  - **Bias Terms**: Bias terms disabled in all linear layers except the final decoder layer.
  - **Positional Embeddings**: Utilizes rotary positional embeddings (RoPE) for improved performance in both short and long contexts.
  - **Normalization**: Implements pre-normalization blocks to stabilize training.
  - **Activation Function**: Adopts GeGLU activation function for better performance.

- **Efficiency Improvements**:
  - **Alternating Attention**: Combines global and local attention mechanisms, with every third layer using global attention.
  - **Unpadding**: Removes padding tokens during training and inference to enhance computational efficiency.
  - **Flash Attention**: Utilizes Flash Attention for memory and compute-efficient attention kernels.
  - **torch.compile**: Leverages PyTorch's compilation for improved training efficiency.

- **Model Design**:
  - **Layer Configuration**: ModernBERT-base has 22 layers, while ModernBERT-large has 28 layers, with parameter counts of 149 million and 395 million, respectively.
  - **Hidden Sizes**: Base model has a hidden size of 768; large model has a hidden size of 1024.

- **Training Settings**:
  - **Masked Language Modeling (MLM)**: Follows a masking rate of 30%, removing the Next-Sentence Prediction objective.
  - **Optimizer**: Uses StableAdamW optimizer for improved stability during training.
  - **Learning Rate Schedule**: Implements a modified trapezoidal learning rate schedule for effective training.

- **Performance Metrics**: ModernBERT demonstrates significant improvements in inference speed and memory efficiency compared to previous models, processing sequences almost twice as fast.

- **Release Information**: Two models released: ModernBERT-base and ModernBERT-large, with a focus on modular architecture for future research (FlexBERT).

- **Key Contributions**: Represents a major Pareto improvement over older encoder models, addressing limitations in sequence length, training data volume, and computational efficiency.