# Efficient Operator Learning in Frequency Domain

## Abstract

## 

Spectral analysis provides one of the most effective paradigms for informationpreserving dimensionality reduction, as simple descriptions of naturally occurring signals are often obtained via few terms of periodic basis functions. In this work, we study deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time: frequencydomain models (FDMs). Existing FDMs are based on complex-valued transforms i.e. Fourier Transforms (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: transform once (T1). To enable efficient, direct learning in the frequency domain we derive a variance preserving weight initialization scheme and investigate methods for frequency selection in reduced-order FDMs. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of 3 x to 10 x that increase with data resolution and model size. We perform extensive experiments on learning the solution operator of spatio-temporal dynamics, including incompressible Navier-Stokes, turbulent flows around airfoils and highresolution video of smoke. T1 models improve on the test performance of FDMs while requiring significantly less computation (5 hours instead of 32 for our largescale experiment), with over 20% reduction in predictive error across tasks.

## Introduction

Nature uses only the longest threads to weave her patterns, so that each small piece of her fabric reveals the organization of the entire tapestry. [(Feynman, 1965)](#b9) Naturally occurring signals are often sparse when projected on periodic basis functions [(Strang, 1999)](#b43). Central to recently-introduced instances of frequency-domain neural operators [(Li et al., 2020;](#b23)[Tran et al., 2021)](#b46), which we refer to as frequency-domain models (FDMs), is the idea of learning to modify specific frequency components of inputs to obtain a desired output in data space. With a hierarchical structure that blends learned transformations on frequency domain coefficients with regular convolutions, FDMs are able to effectively approximate global, long-range dependencies in higher resolution signals without requiring prohibitively deep architectures.

2 Related Work and Background

## Learning and Frequency Domain: A Short History

Links between frequency-domain signal processing and neural network architectures have been explored for decades, starting with the original CNN designs [(Fukushima and Miyake, 1982)](#b10). [Mathieu et al. (2013)](#b29); [Rippel et al. (2015)](#b38) proposed replacing convolutions in pixel space with element-wise multiplications in Fourier domain. In the context of learning to solve partial differential equations (PDEs), Fourier Neural Operators (FNOs) [(Li et al., 2020)](#b23) popularized the state-of-the-art FDM layer structure: forward transform → learned layer → inverse transform. Similar architectures had been previously proposed for generic image classification tasks in [(Pratt et al., 2017;](#b36)[Chi et al., 2020)](#b4). Modifications to the basic FNO recipe are provided in [(Tran et al., 2021;](#b46)[Guibas et al., 2021;](#b12)[Wen et al., 2022)](#b50). A frequency domain representation of convolutional weights has also been used for model compression [(Chen et al., 2016)](#b3). Fourier features of input domains and periodic activation functions play important roles in deep implicit representations [(Sitzmann et al., 2020;](#b40)[Dupont et al., 2021;](#b6)[Poli et al., 2022)](#b35) and general-purpose models [(Jaegle et al., 2021)](#b18).

## Learning to Solve Differential Equations

A variety of deep learning approaches have been developed to solve differential equations: neural operators and physics-informed networks [(Long et al., 2018;](#b25)[Raissi et al., 2019;](#b37)[Lu et al., 2019;](#b26)[Karniadakis et al., 2021)](#b20), specialized architectures [(Wang et al., 2020;](#b49)[Lienen and Günnemann, 2022)](#b24), hybrid neural-numerical methods [(Poli et al., 2020;](#b34)[Kochkov et al., 2021;](#)[Mathiesen et al., 2022;](#b28)[Berto et al., 2022)](#b1), and FDMs [(Li et al., 2020;](#b23)[Tran et al., 2021)](#b46), the focus of this work.

## Frequency-Domain Models

Let D n (n-space) to be the set of real-valued discrete signals[foot_1](#foot_1) of resolution N . Our objective is to develop efficient neural networks to process discrete signals x ∈ D n , x 0 , x 1 , . . . , x N -1 , x n ∈ R.

We define a layer of FDMs mapping x to an output signal ŷ ∈ D n as the structured operator:

$X = T (x) Forward Transform X = f θ (X)$Learned Map

$x = T -1 ( X) Inverse Transform ŷ = x + g(x)$Residual

$x ŷ T X f θ X T -1 x g + (1$$)$where T is an orthogonal (possibly complex) linear operator. We denote the T -transformed n-space with D k (k-space) so that T : D n → D k . Typically, we assume T to be a Fourier-type transform[foot_2](#foot_2)[(Oppenheim, 1999, Chapter 8](#)) so that the k-space corresponds to the frequency domain and its elements form the spectrum of the input signal x.

The learned parametric map f θ : D k → D k is the stem of a FDM layer: it maps the k-space into itself and is typically chosen to be rank-deficient in the linear case, e.g. f θ (X) = S m A(θ)S m X, A(θ) ∈ C m×m (m ≤ N ). The matrix S m ∈ R n×m selects m desired elements of X, setting the rest to zero. In the case of frequency domain transforms, this allows (1) to preserve or modify only specific frequencies of the input signal x.

Residual connections or residual convolutions g [(Li et al., 2020;](#b23)[Wen et al., 2022)](#b50) are optionally added to reintroduce frequency components filtered by S m . A FDM mixes global transformations applied to coefficients of the chosen transform to local transformations g i.e. convolutions with finite kernel sizes. To ensure that such models can approximate generic nonlinear functions, nonlinear activations are introduced after each inverse transform.

Fourier Neural Operators Layers of the form (1) appear in recent FDMs such as Fourier Neural Operators (FNOs) [(Li et al., 2020)](#b23) and variants [(Tran et al., 2021;](#b46)[Guibas et al., 2021;](#b12)[Wen et al., 2022)](#b50).

In example, an FNO is recovered from (1) by letting T be a Discrete Fourier Transform (DFT)

$x = T -1 • f θ • T (x) = W * S m A(θ)S m W x$where W ∈ C N ×N is the standard N -dimensional DFT matrix and W * its conjugate transpose. The Discrete Fourier Transforms (DFTs) is a natural choice of T as it can be computed in O(N log N ) via Fast Fourier Transform (FFT) algorithms [(Oppenheim, 1999, Chapter 9.](#)2).

We identify two major limitations of FDMs in the form (1); each layer performs T and T -1 and DFTs are complex-valued, resulting in overheads and a restriction of the design space for f θ (X).

With T1, we aim to develop an FDM that does not require more than a single T , while preserving or improving on predictive accuracy. Ideally, the transform in T1 should be (1) real-valued, to avoid restrictions in the design space of the architecture and thus retain compatibility with existing pretrained models, (2) universal, to allow the representation of target signals, and (3) approximately sparse or structured, to allow dimensionality reduction.

$ŷ x X X x T T -1 f θ T T -1 g x x X X S m A(θ)S m W W * W W *$Commutative diagrams for FDM layers (1) and linear FNOs (frequency domain part).

3 Transform Once: The T1 Recipe

With T1, we introduce major modifications to the way FDMs are designed and optimized. In particular, T1 is defined, inferred and trained directly in the frequency domain with only a single direct transform required to process data. Hence follows the name: transform once (T1).

Direct learning in the frequency domain Consider two signals x ∈ D n , y ∈ D n and suppose there exists a function ϕ : D n → D n mapping x to y, i.e. y = ϕ(x). Then, there must also exist another function ψ : D k → D k that relates the spectra of the two signals, i.e. Y = ψ(X) being X = T (x) and Y = T (y). In particular,

$ϕ(x) = T -1 • ψ • T (x) ⇔ T • ϕ(x) = ψ • T (x)$It follows that, from a learning perspective, we can aim to approximate ψ directly in the k-space rather than ϕ in the n-space. To do so, we define a learnable parametric function f θ : D k → D k and train it to minimize the approximation error J θ of the output signal spectrum Y in the k-space. Given a distribution p(x) of input signals, T1 is characterized by the following nonlinear program

$min θ E x,y T (y) -Ŷ subject to Ŷ = f θ • T (x) x ∼ p(x) y = ϕ(x) x y X Y Ŷ T T f θ J θ (2)$If T is a DFT, the above turns out to be a close approximation (or equivalent, depending on the function class of f θ ) to the minimization of yŷ in n-space by the Parseval-Plancherel identity. Theorem 3.1 (Parseval-Plancherel Identity [(Stein and Shakarchi, 2011, pp. 223)](#) ). Let T be the normalized DFT. Given a signal v ∈ D n and its transform

$V = T (v), it holds v = V .$This result also applies to any other norm-preserving transform T , e.g. a normalized type-II DCT [(Oppenheim, 1999, pp. 679)](#). For the linear transforms considered in this work, T (x) = W x, W ∈ C N ×N , condition for Th. 3.1 to hold is W to be orthonormal, i.e. W * W = I. Note that T1 retains, in principle, the same universal approximation properties of FNOs [(Kovachki et al., 2021)](#b22) as f θ is allowed to operate on the entirety of the input spectrum. Given enough capacity, f θ can arbitrarily approximate ψ, implicitly reconstructing

$ϕ via T -1 • f θ • T .$Speedup measurements We provide a concrete example of the effect of pruning redundant transforms on computational costs. We measure wall-clock inference time speedups of depth d T1 T1(x)

$:= f d • • • • • f 2 • f 1 • T (x)$over an equivalent depth d FNO with layers (1). The only difference concerns the application of transforms between layers. Fig. [3](#fig_1).1 provides the speedups on two-dimensional signals: on the left, we fix model depth d = 6 and investigate the scaling in signal width (i.e. number of channels) and signal resolution. On the right, we fix signal width to be 32 and visualize the interaction of model depth and signal resolution. For common experimental settings e.g. resolutions of 64 or 128, 6 layers and width 32, T1 is at least 10 x faster than other FDMs. It will later be shown ( § 4) that T1 also preserves or improves on predictive accuracy of other FDMs across tasks. The speedup for a given configuration (point on the plane) is shown as background color gradient.

The improvement grows with signal width, resolution and model depth.

When T1 is not preceded by online preprocessing steps for inputs x, such as other neural networks or randomized data augmentations, the transform on T (x) can be done once on the dataset, amortizing the cost over training epochs, and increasing the speed of T1 further.

Choosing the right transform The transform T in T1 is chosen to be in the class of Discrete Cosine Transforms (DCTs) [(Ahmed et al., 1974;](#b0)[Strang, 1999)](#b43), in particular the normalized DCT-II, which can also be computed O(N log N ) via FFTs [(Makhoul, 1980)](#b27). DCTs provide effective representations of smooth continuous signals [(Trefethen, 2019)](#b47) and are the backbone of modern lossy compression methods for digital images.

Although other transforms are available, we empirically observe DCT-based T1 to perform best in our experiments. This phenomenon can be explained by the sparsity and energy distribution properties of the transformed spaces, an intrinsic property of the specific dataset and chosen transform. This is in line with results of classic signal processing and compression literature. Particularly, DCT features are known to have a higher energy compaction than their DFT counterparts in a variety of domains, from natural images [(Yaroslavsky, 2014)](#b51) to audio signals [(Soon et al., 1998)](#b41). Energy compaction is often the decisive factor in choosing a transform for downstream tasks.

Letting T be a real-valued transform in T1 architectures preserves compatibility between f θ and existing architectures e.g., models pre-trained on natural image datasets.

## Reduced-Order T1 Model and Irreducible Loss Bound

We seek to leverage structure induced in D k by T . To this end we allow T1, similarly to (1), to modify specific elements of X and consequently trasform only certain frequency components of x (and y).

The reduced-order T1 model is designed to operate only on m < N elements (selected by S m ∈ R N ×m ) of the input k-space, i.e. on a reduced k-space D m ≡ R m of lower dimension. Thus, we can employ a smaller neural network γ θ : D m → D m for mapping S m X to the corresponding m elements S m Y of the output k-space. Thus, training involves a truncated objective that compares predictions with elements in the output signal spectrum also selected by S m :

$γ θ x X S m X Ŷ y Y S m Y J θ min θ E x,y S m • T (y) -Ŷ subject to Ŷ = γ θ • S m • T (x) x ∼ p(x) y = ϕ(x)(3)$How to choose modes in reduced-order FDMs We now detail some intuitions and heuristic to choose which modes k 0 , . . . , k m-1 should be kept to maximize the information content in the truncated spectrum. For this reason, we evaluate the irreducible loss arising from discarding some Nm modes. We recall that the (reduced) k-space training objective J θ (X, Y ) reads as

$J θ (X, Y ) = S m Y -Ŷ = m l=1 |Y k l -γ θ,k l • S m (X)| ,$since only the first m predicted output modes Ŷk1 , . . . , Ŷkm can be compared to Y k . We then consider the total loss L θ of the approximation task, including the Nm elements of the output k-space discarded by our model, i.e.

$L θ (X, Y ) = Y -S m Ŷ = m-1 l=0 |Y k l -γ θ,k l • S m (X)| J θ (X,Y ) + N -1 k=m |Y k -0| Ro(Y )$.

It follows that the overall loss L θ is higher than T1's training objective J θ , i.e.

$L θ = J θ + R o > J θ ,$whilst R o represents the irreducible residual loss due to truncation of the predictions Ŷk .

Optimal mode selection in auto-encoding T1 In case Y = X, i.e. the reduced-order T1 is tasked with reconstructing the input spectrum, the optimal modes minimizing the irreducible loss are the ones with highest magnitude. This can be formalized as follows.

Proposition 3.1 (Top-m modes minimize the irreducible loss). Let Y = X (reconstruction task).

Then the choice k

$0 , . . . , k m-1 = top k (m) |X k | minimizes the irreducible loss term R o .$This means that if the spectrum of X is monotonically decreasing in magnitude, then low-pass filtering is the optimal mode selection. Corollary 3.1 (Low pass filtering is optimal for monotonic spectrum).

$If |X k | is monotonically decreasing in k, then the choice k 0 , . . . , k m-1 = 0, . . . , m -1 minimizes the residual R o .$However, spectra in practical datasets are commonly non-monotonic e.g., the spectrum of solutions of chaotic or turbulent systems [(Dumont and Brumer, 1988](#b5)). We show an example in Fig. [3](#fig_1).2.

## Mode selection criteria in general tasks

When Y = X and the task is a general prediction task, the simple top m analysis is not optimal. Nonetheless, given a dataset of input-output signals it is still possible to perform an a priori analysis on R o to inform the choice of the best modes to keep.

Often, we empirically observe the irreducible error R o for reduced-order T1 to be smaller than for non-reduced-order FDMs i.e R o <

$K-1 k=m Y k -T k (ŷ)$with layers of type (1) 5 . We also note that the reachable component J θ of the objective cannot always be minimized to zero regardless of the approximation power of γ θ . For each k < m, S m discards Nm frequency 5 See Fig. [4](#fig_3).1 and Appendix B for experimental evidence in support of this phenomenon. components of the input signal which, if different than zero, likely contain the necessary information to approximate ψ k (X) exactly. Specifically, the irreducible lower bound on J θ should depend on "how much" the output's m frequency components depend on the discarded N -m input's elements.

A rough quantification of such bound can be obtained by inspecting the mismatch between the gradients of ψ kγ θ,k • S m with respect to X. In particular, it holds

$N -1 j=0 ∂ψ k (X) ∂X j - ∂γ θ,k (S m X) ∂X j = m-1 j=0 ∂ψ k (X) ∂X j - ∂γ θ,k (S m X) ∂X j + N -1 j=m ∂ψ k (X) ∂X j ,$Unless ∂ Xj ψ k (X) = 0 holds for all j = m, . . . , N -1 and k = 0, 1, . . . , N -1 i.e. no dependency of the ground truth map in k-space on the truncated elements, there will be an irreducible overall gradient mismatch and thus a nonzero J θ .

## Weight Initialization for Reduced-Order FDMs

FDMs [(Li et al., 2020;](#b23)[Tran et al., 2021;](#b46)[Wen et al., 2022)](#b50) opt for a standard Xavier-like [(Glorot and Bengio, 2010)](#b11) initialization distribution that takes into account the input channels c to a layer i.e. N (0, 1 c ). However, well-known variance preserving properties of Xavier schemes do not hold for FDM layers truncating Nm elements of the k-space. Notably, Xavier schemes do not scale the variance of the weight initialization distribution based on the number of elements m kept after truncation of the spectrum performed by f θ , leading to the collapse of the outputs to zero.

To avoid this issue in T1 and other FDMs, we develop a simple variance-preserving (vp) that introduces a variance scaling factor based on m and the class of transform.

$Theorem 3.2 (Variance Preserving (vp) Initialization). Let x = W * S m AS m W x be a k-space reduced-order layer and W is a normalized DCT-II transform. If x ∈ R N is a random vector with E[x] = 0, V[x] = σ 2 I. Then, A ij ∼ N 0, N m 2 ⇒ V[x] = V[x].$We report the proof in Appendix A, including some considerations for specific forms of f θ . Corollary 3.2 (vp initialization for DFTs). Under the assumptions of Theorem 3.2, if W is a normalized DFT matrix we have

$Re(A ij ), Im(A ij ) ∼ N (0, N 2m 2 ) ⇒ V[x] = V[x].$The collapse phenomenon is empirically shown in Fig. [3](#fig_1).3 for m = 24, comparing a single layer of FNO and FFNO (with Xavier initialization) with FNO equipped with the proposed vp scheme. Under the assumptions of Corollary 3.2, we sample A and compute empirical variances of x = W * S m A(θ)S m W x for several finite batches of input signals x. We repeat the experiment for signals of different lengths N . The vp scheme preserves unitary variances whereas the other layers concentrate output variances towards zero at a rate that grows with Nm. When the learned frequency-domain transformation f θ is obtained, instead of the single low-rank linear layer f θ = A(θ)S m X, as the composition of several layers, preserving variances can be achieved by applying the vp scheme only to the first layer. For some variants of FDMs e.g. FNO that truncate the spectrum at each layer, vp initialization should instead be applied to all.

$0.5 1 1.5 2 2.5 σ 2 Ours (DFT) 0 0.5 1 1.5 σ 2 Variances of x FNO (DFT) 0 2 4 6 σ 2 FFNO (DFT)$
## Experiments

We validate T1 on learning to approximate solution operators of dynamical systems from images.

• In § 4.1, we apply T1 on the standard task of learning solution operators for incompressible Navier-Stokes, comparing against other FDMs. In § 4.1.1 we perform a series of ablation experiments on each ingredient of the T1 recipe, including weight initialization and architecture. In § 4.1.2 we provide scaling laws. • In § 4.2 we deal with fluid-solid interaction dynamics in the form of higher resolution images (128). We consider turbulent flows around varying airfoil geometries, benchmarking against current SOTA [(Thuerey et al., 2020)](#b45). • In § 4.3 we show how the computational efficiency of T1 allows learning on unwieldy data without downsampling or building low-resolution meshes. We consider learning on high-resolution video (600 × 1062) capturing the turbulent dynamics of smoke [(Eckert et al., 2019)](#b7).

Configuration and model details are reported in the supplementary material. The code is available at [https://github.com/DiffEqML/kairos](https://github.com/DiffEqML/kairos). Weights & Biases (wandb) [(Biewald, 2020](#b2)) logs of results are provided.

## Incompressible Navier-Stokes

We show that T1 matches or outperforms SOTA FDMs with less computation on the standard incompressible Navier-Stokes benchmark. Losses are reported in n-space (signal space) for comparison.

Setup We consider two-dimensional Navier-Stokes equations for incompressible fluid in vorticity form as described in [(Li et al., 2020)](#b23). Given a dataset of initial conditions, we train all models to approximate the solution operator at time 50 seconds for high viscosity (ν = 1e -3 ) and at time 15 for lower viscosity (ν = 1e -4 ). As a metric, we report normalized mean squared error (N-MSE).

Both initial condition as well as solution are provided as images of resolution 64.

We include as baseline established FDMs, such as Fourier Neural Operators (FNOs) [(Li et al., 2020)](#b23) and Factorized Fourier Neural Operators (FFNOs) [(Tran et al., 2021)](#b46). We indicate with the suffix vp models that employ the proposed variance preserving initialization scheme. All models truncate to m = 24, except FFNOs to m = 32. 

## Results

We perform 20 training runs for each model and report mean and standard deviation in

Table 4.1. T1 reduces solution error w.r.t FNOs by over 20% and FFNOs by over 40%. A single forward pass of T1 models is on average 2x faster than FNO and 10x than FFNOs. We note that FFNOs Method Param. (M) Size (MB) Step (ms) high ν low ν FFNO (Tran et al., 2021) 8.9 35 294 0.997±0.003 1.016±0.010 FNO (Li et al., 2020) 14.2 56 31 0.379±0.006 0.328±0.004 FNOvp 14.2 56 32 0.351±0.003 0.315±0.006 T1+vp 10.2 40 19 0.257±0.007 0.240±0.004 Table 4.1: Benchmarks on incompressible Navier-Stokes. Direct long-range prediction errors (N-MSE) in n-space (signal space) of different models.

are designed to share parameters between layers and thus require deeper architectures -and slower, due to more transforms. In particular, training time (500 epochs) for T1 is cut to 20 minutes down from 40 of FNOs, matching the model speedup. Finally, we report an improvement in performance for FNOs with parameters initialized following our proposed scheme (FNOvp). Fig. [4](#fig_3).1 provides sample predictions in n-space (left) to contextualize the task, in addition to prediction errors in frequency domain (right). Despite being a reduced order model with m = 24, T1+vp produces smaller errors on truncated k-space elements (k > m) compared to FNOvp and FFNO.

4.1.1 Ablations on weight scheme and architecture Method high ν low ν T1 0.491 0.449 T1vp 0.304 0.280 T1+ 0.295 0.260 T1+vp 0.257 0.240 Table 4.2: Ablation on the effect of the proposed weight initialization scheme and T1 architecture.

We repeat the previous experiment and report prediction errors for four variants of T1: same architecture and weight initialization scheme as FNOs (T1), T1 with our proposed vp scheme (T1vp), a reduced-order variant with k-space model f θ defined as a UNet architecture (T1+), and T1+ with variance preserving scheme (T1+vp). The results in Table [4](#).2 provide empirical evidence in support of the vp scheme and its synergistic effect with the proposed architecture. In particular, combining vp scheme and UNet structure in frequency domain reduces error by half compared to the naive T1 approach. We verify whether the reduction in predictive error of T1 over neural operator baselines is preserved as the size of training dataset grows. We perform 10 training runs on the Navier-Stokes ν = 1e -4 experiment, each time with a larger dataset size, and report the scaling laws in Fig. [4](#fig_3).2. With additional data, the gaps in test errors narrow slightly, with noticeable improvements obtained by applying the vp scheme to both FNO and T1+.

## Scaling laws

## Flow Around Airfoils

We investigate the performance of T1 in predicting steady-state solutions of flow around airfoils.

Setup We use data introduced in [(Thuerey et al., 2020)](#b45) in the form of 10000 training pairs of initial conditions, specifying freestream velocities and the airfoil mask, with the target steady-state velocity and pressure fields. This task introduces additional complexity in the form of higher resolution input images (128) and a full k-space due to the discontinuity in the field produced by the mask.

We compare a SOTA UNet architecture (DFPNet) introduced by [(Thuerey et al., 2020)](#b45) to FNOs and T1 with vp initialization schemes. We perform a search on the most representative hyperparameters (detailed in the Appendix). Averages for 5 runs are reported in

Table 4.3. Method N-MSE Time (hrs) DFPNet 0.023 1.3 FNO 0.020 6.0 T1+vp 0.024 1.3 Table 4.3: Test N-MSE and total training time on the flow around airfoil task.

Results All models are able to accurately predict steady-state solutions for different airfoils with small normalized errors. Test N-MSE is comparable as all models are within a single standard deviation. Training of T1 is as fast as DFPNets [(Thuerey et al., 2020)](#b45) and as accurate as FNOs, as evidence of the applicability of T1 to tasks with signals that are not band-limited (in this case due to the airfoil mask).

## Turbulent Smoke

We investigate the performance of T1 in predicting iterative rollouts from high-resolution video of real rising smoke plumes. Method N-MSE Time (hrs) FNO 0.232 32.4 T1 0.239 8.1 T1+ 0.256 4.7 T1+vp 0.228 4.7 Table 4.4: Test 10-steps rollout n-space prediction errors (N-MSE) and total training time on the ScalarFlow dataset. Results Fig. 4.3 provides a sample rollout of different model predictions in k-space (DCT-II). T1+vp accumulates smaller errors over the rollout and is less prone to generating non-physical artifacts by performing prediction only on a subset of the k-space (Table 4.4). Notably, T1 and T1+ are 4× to 7× faster, providing a reduction in training time from 32.4 hours to 4.7. Appendix B includes additional visualizations, including averaged prediction errors on k-space.

## Conclusion

We present a streamlined class of frequency domain models (FDM): Transform Once (T1). T1 models are optimized directly in frequency domain, after a single transform, and achieve similar or improved predictive performance at a fraction of the computational cost (3x to 10x speedups across tasks). Further, a simple truncation-aware weight initialization scheme is introduced and shown to improve the performance of T1 and existing FDMs.

A Proof of Theorem 3.2

A.1 Preliminary Lemmas Lemma A.1 (Propagation of Uncertainty under DFT/DCT).

$Let X = W x with x ∈ R N and W ∈ C N ×N . Then Σ X = W Σ x W * Proof. Σ X = E [(W x -E[W x]) ∧ (W x -E[W x])] = E [W (x -E[x]) ∧ W (x -E[x])] = E W (x -E[x])(x -E[x]) W * = W E W (x -E[x])(x -E[x]) W * = W Σ x W * Lemma A.2 (Propagation of Total Variance under DFT/DCT). Let X = W x with x ∈ R N and W ∈ C N ×N . Then V[X] = V[x]$Proof. Recalling that the total variance of a random variable is equal to the trace of its covariance matrix, i.e.

$V[x] = tr(Σ x ), V[X] = tr(Σ X ) then tr(Σ x ) = tr(Σ X ) ⇔ V[X] = V[x] Recalling Lemma A.1 yields V[X] = V[x] ⇔ tr(Σ x ) = tr(W Σ x W * ) ⇔ tr(Σ x ) -tr(W Σ x W * ) = 0 ⇔ tr(Σ x ) -tr(Σ x W * W ) = 0$Since the DCT/DFT matrix is orthonormal, i.e. W * = W -1 we have that tr(Σ x W * W ) = tr(Σ x ), proving the result.

$Lemma A.3 (Gaussian initialization in rank-deficient linear layers). Let X = S m AS m X with X ∈ R N , A ∈ C m×m and S m ∈ C m×N , S m = 1 • • • 0 0 • • • 0 . . . . . . . . . . . . . . . . . . 0 • • • 1 0 • • • 0            m m N -m . If E[X k ] = 0, V[X k ] = σ 2 for all k the following hold: i. for k ≥ m E[ Xk ] = 0, V[ Xk ] = 0 ii. for k < m and Re(A ij ), Im(A ij ) ∼ N (0, σ 2 A ) E[ Xk ] = 0, V[ Xk ] = 2mσ 2 σ 2 A iii. for k < m and Re(A ij ) ∼ N (0, σ 2 A ), Im(A ij ) = 0 E[ Xk ] = 0, V[ Xk ] = mσ 2 σ 2 A Proof. Let M = S m AS m . It holds, M = A × × × ∈ C N ×N$where"×" are blocks of complex zeros. By expanding component-wise the layer computation, i.e.

$Xk = N -1 j=0 M kj X j , it holds that for k < m Xk = m-1 j=0 A kj X j ,$while Xk = 0 for k ≥ m. Hence i. follows naturally from the latter and we focus on proving ii. and iii.

Case ii. The probability distribution of Xk is a sum of product distributions involving independent random variables A kj and X j . The first central moment is readily obtained

$E[ Xk ] = m-1 t=0 E[A kj ]E[X j ] = 0 since both E[X k ] = 0 and ∀ k, j < m : E[A kj ] = 0. V[ Xk ]$can be then obtained by computing the variance of the product of two random variables, i.e.

$V[ Xk ] = m-1 j=0 V[A kj ] + $ $ $ $ E[A kj ] 2 )(V[X j ] + ¨Ë [X j ] 2 ) - @ @ @ @ @ @ @ E[A kj ] 2 E[X j ] 2 = m-1 j=0 V[A kj ]V[X j ] = m-1 j=0 σ 2 V[A kj ] = σ 2 m-1 j=0 (V[Re(A kj )] + V[Im(A kj )]) = σ 2 m-1 j=0 2σ 2 A = 2mσ 2 σ 2 A Case iii.$Similarly to the previous case we get

$V[ Xk ] = σ 2 m-1 j=0 V[Re(A kj )] + $ $ $ $ $ V[Im(A kj )] = σ 2 m-1 j=0 σ 2 A = mσ 2 σ 2 A A.2 Proof of Main Result$Proof. According to Lemma A.2, the total variance is preserved under the normalized DCT. Therefore, with X = W x and X = W x we have

$V[X] = V[x], V[ X] = V[x].$Using X = S m AS m X, we can find the condition under which the variance is preserved by the map x → x:

$V[x] = V[x] ⇔ N -1 n=0 V[x n ] = N -1 n=0 V[x n ] ⇔ N -1 k=0 V[ Xk ] = N -1 k=0 V[X k ] ⇔ m-1 k=0 mσ 2 σ 2 A = N -1 k=0 σ 2 Lemma A.3 ⇔ m 2 σ 2 σ 2 A = N σ 2 ⇔ σ 2 A = N m 2$Hence, initializing A by sampling its entries from a normal distribution with zero mean and variance N/m 2 is sufficient for preserving the variance under the reduced-order FDM layer, i.e.

$A ij ∼ N 0, N m 2 ⇒ V[x] = V[x],$proving the result.

Corollary 3.2 (vp initialization for DFTs). Under the assumptions of Theorem 3.2, if W is a normalized DFT matrix we have

$Re(A ij ), Im(A ij ) ∼ N (0, N 2m 2 ) ⇒ V[x] = V[x].$Proof. The proof follows directly from the one of Theorem 3.2 using the fact that since the DFT's k-space is complex (D k ≡ C N ) as W ∈ C N ×N , the weights are typically chosen complex, i.e.

A ∈ C m×m . Therefore, in this case V[ X] = 2mσ 2 σ 2 A according to Lemma A.3.

Corollary A.1 ((vp) initialization with diagonal layers). Under the assumptions of Theorem 3.2, if A is diagonal s.t ∀i = j :

$A ij = 0, we have A ii ∼ N 0, N m ⇒ V[x] = V[x].$Proof. The proof follows directly from Lemma A.3

$V[ Xk ] = m-1 j=0 V[A kj ]V[X j ] = V[A kk ]V[X k ] = σ 2 V[Re(A kk )] + $ $ $ $ $ $ V[Im(A kk )] = σ 2 σ 2 A leading to the condition V[x] = V[x] ⇔ m-1 k=0 σ 2 σ 2 A = N -1 k=0 σ 2 ⇔ mσ 2 σ 2 A = N σ 2 ⇔ σ 2 A = N m$The layer structure treated by A.1 is common among many FDMs, e.g. FNOs in [(Li et al., 2020)](#b23).

## B Additional Details

Broader impact FDMs are widely used in the context of learning to predict the evolution of dynamical systems. The model class presented in this work, T1, provides an accessible way to train and evaluate large-scale FDMs, reducing memory overhead and overall training times. When predicting the solution of e.g. a partial differential equation (PDE), care should be taken especially when the prediction is used to inform downstream decision making, as many systems are optimally predictable only for a certain time scale [(Strogatz, 2018, pp. 366](#)). We anticipate a potential positive environmental impact from the adoption of T1 as a replacement for the largest FDMs currently in use.

## Experimental setup

Experiments have been performed on an NVIDIA© DGX workstation equipped with a 128 threads AMD© EPYC 7742 CPU, 512GB of RAM and four NVIDIA© A100 GPUs. The main software implementation has been done within the PyTorch [(Paszke et al., 2017)](#b31) ecosystem building upon the pytorch-lightning [(Falcon et al., 2019)](#b8) framework.

## Common experimental settings B.1 Incompressible Navier-Stokes

Dataset We use data generated in [(Li et al., 2020)](#b23) in the form of pairs of initial conditions and solutions of the incompressible Navier-Stokes equations in vorticity form solved with a pseudospectral method. The dataset[foot_3](#foot_3) is comprised rollouts of solutions as images of resolution 64.

## Models and training

The training configuration is shared by all models:

datamodule: ntrain: 1000 ntest: 200 batch_size: 64 history_size: 1 train: optimizer: type: AdamW learning_rate: 1e-3 weight_decay: 1e-4 scheduler: type: Step step_size: 100 gamma: 0.5 scheduler_interval: epoch loss_fn: RelativeL2Loss

For the high viscosity (1e -3 ) setting, the models are trained to predict the solution at time T = 50 seconds directly, without producing rollouts and supervising the model with solutions at times between 0 and 50. Crucially, this ensures that the task is much more challenging than that of [(Li et al., 2020)](#b23), where for a single training sample the entire rollout is used as supervision. For the low viscosity setting (1e -4 ), target times are T = 15 seconds.

Model configurations are given below: FNO: modes: 24 nlayers: 6 width: 32 T1: modes: 24 nlayers: 6 width: 48 FFNO: modes: 32 nlayers: 10 width: 82

where each layer in a model shares the same structure. In FNOs and FFNOs, we employ a regular FDM layer following [(Li et al., 2020;](#b23)[Tran et al., 2021)](#b46) with k-space convolutions and residual connections given by n-space layers (pointwise convolutions for FNOs, dense for FFNOs). T1 uses a similar layer without n-space residual paths. The differences in number of layers and width have been introduced to keep parameter counts comparable. At a given channel width, FNOs require the largest number of parameters due to k-space convolutions on complex numbers given by the DFT coefficients. Although FFNOs [(Tran et al., 2021)](#b46) are most parameter efficient due to parameter sharing, we found them unable to tackle the task and produce high-quality predictions. T1+ employs a UNet on the patch constructed by the elements of the k-space kept, and shares its structure with T1 otherwise. The vp parameter initialization scheme in T1 is applied only to the first layer performing the truncation in k-space, not to the following layers which use standard Kaiming initialization [He et al. (2015)](#b15). In FNOvp the scheme is applied to all layers. Hyperparameter tuning We start with the basic model structure of FNOs as detailed [(Li et al., 2020)](#b23) and perform a basic hyperparameter search on a small slice of the training set, with the goal of ensuring proper convergence of a model. We did not find the number of layers to have a significant impact on convergence. Width plays an important role and is best kept above 24.

Scaling laws We use the same settings as the main experiment, repeating separate training runs for the low viscosity setting. In particular, we increase the dataset size for each set of runs by a factor of 2: 1024, 2048, 4096, 8192. The total number of epochs is kept fixed, so that more iterations are performed for larger datasets. The same test set of size 200 is used in all cases. 

## B.2 Flow Around Airfoils

Dataset We use a slice of the dataset introduced by [Thuerey et al. (2020)](#b45) in the form of 11000 training pairs of initial conditions and solutions. The solutions are obtained via OpenFOAM [(Jasak et al., 2007](#b19)) SIMPLE, a steady-state solver for incompressible and turbulent flows. In particular, the initial conditions are specified as freestream velocities over the domain [(two-directional components)](#), in addition to a specification of the airfoil in point cloud format. Delaunay triangulation is used for mesh generation.

After simulation, data is provided as initial condition and steady-state solution pairs. The initial condition is a three channel 128 × 128 image: two channels for freestream velocities and one for the airfoil mask. The solution is a three channel 128 × 128 image: a velocity field and a scalar pressure field. All data is normalized using training set statistics.

## Models and training

Training configuration is given as datamodule: ntrain: 8000 nval: 2000 ntest: 1000 batch_size: 64 train: optimizer: type: AdamW learning_rate: 1e-3 weight_decay: 1e-4 scheduler: type: Step step_size: 100 gamma: 0.6 scheduler_interval: epoch loss_fn: RelativeL2Loss

The baseline UNet matches the architecture of [(Thuerey et al., 2020) (DFPNet)](#). The FNO architecture is comprised of a standard stack of FDM layer as discussed in B.1. The k-space UNet in T1+ has the same structure as a DFPNet. Hyperparameter tuning This is an example of a dataset where the k-space is full due to discontinuity in the solution given by the airfoil mask.

We use the training and validation sets to inspect the k-space and set m to 100 for the irreducible loss term to be sufficiently small as shown in Fig. [B](#).5. We swept over m for FNOs and found larger than 24 to perform worse, likely due to k-space convolution being sufficient to capture higher frequency components. We observe DFPNets with larger channel exponents perform worse due to overfitting.

Further comments A sample of predictions is given in Fig. [B.3. Fig. B.4](#) shows the n-space and corresponding DCT k-space of a data point. As can be observed, the k-space is structured but full due to the discontinuity caused by the airfoil mask. Fig. [B](#).5 shows the approximation error on solution fields due to truncation in k-space at different m. In this task, the DCT is more efficient, given a budget of modes to keep, as it yields lower errors. This error provides a theoretical lower On the x-axis, the normalized cost for a number of modes m: for DCTs, since the k-space is real, truncation at m modes requires m 2 floats, for real FFTs with complex k-space and conjugacy the cost in floats is 4m 2 . The vertical line indicates the budget used for T1 used in this task (m = 100), while the horizontal line is the test N-MSE achieved.

## B.3 Turbulent Smoke

Dataset We employ for this experiment the ScalarFlow dataset introduced in [(Eckert et al., 2019)](#b7) which is available online under the Creative Commons license CC-BY-NC-SA 4.0[foot_4](#foot_4) . [Eckert et al. (2019)](#b7) created an environment for controlling the release of smoke plumes: a fog machine generated fog inside of a container; the fog was then heated up by a heating cable and a valve controlled its release. Data was captured via multiple calibrated cameras in high resolution at 60 fps (frames per second) for 150 frames. The dataset contains 3D reconstructions of the smoke plumes and 2D input and rendered images: input images are used by [Eckert et al. (2019)](#b7) to solve an optimization problem in which the goal is to generate a 3D reconstruction that minimizes the difference between input and rendered images. 2D input images are obtained directly from raw data on which only post-processing is applied by [(Eckert et al., 2019)](#b7) in the form of gray scaling and denoising: these are saved in compressed numpy [(Harris et al., 2020)](#b14) arrays named imgsTarget_000xxx.npz. Each resulting frame comprises 5 different camera views 600 × 1062 in size. Since we want to use T1 on high-resolution experimental data, we directly utilize the central camera view of these input images in our learning task without any further downsampling or data processing. Similarly to [(Lienen and Günnemann, 2022)](#b24), we divide the 104 recordings into the first 64 for training and use the remaining 20 for validation and 20 for testing.

Data is normalized to the [0, 1] range based on training dataset statistics.

Hyperparameter selection and tuning We performed a search on the most representative hyperparameters. One of the most important hyperparameters to choose from is the number of DCT modes to keep, i.e. first m elements in k-space. We note that for simplicity as well as for compatibility with the UNet inside of T1+, we consider a square mode pruning, i.e. we keep the same  Models and training All models share the configuration for training: datamodule: ntrain: 64 nval: 20 ntest: 20 batch_size: 1 history_size: 1 target_steps_train: 3 target_steps_val_test: 10 train: optimizer: type: AdamW learning_rate: 1e-3 weight_decay: 1e-4 scheduler: type: CosineAnnealingWarmRestarts T_0: 32 step_size: 1 scheduler_interval: step loss_fn: RelativeL2Loss

Where we used the implementation in PyTorch of the cosine annealing schedule with warm restarts[foot_5](#foot_5) . The FNO architecture comprises a standard stack of FDM layers as discussed in B.1. The k-space UNet in T1+ (and in its vp variant) has the same structure as a DFPNet.

modes: 48 nlayers: 4 width: 48 modes: 512 nlayers: 4 width: 8 modes: 224 nlayers: 1 width: 4 channel_exponent: 7

where we note that all models employ GeLU [(Hendrycks and Gimpel, 2016)](#b16) activation functions between inner layers. [B](#).1 provides a larger version of the table in the main text, including 1-step mean absolute errors (MAE). We note that while FNO produces smaller errors in one-step predictions, it quickly accumulates larger errors in extrapolation. Fig. [B](#).9 shows mean errors in k-space of FNO vs T1 and T1+. T1 models demonstrate smaller overall errors and lower maxima compared to the FNO. Table B.1: Full benchmark on the ScalarFlow dataset over 5 runs with different random seeds. N-MSE refers to 10-step test rollouts. T1+vp generates more stable rollouts while requiring a fraction of FNO's training time. Method Param (M) Size (MB) Time (hrs) N-MSE (×10 -1 ) FNO 84.9 339 32.4 2.32 ± 0.02 T1 83.9 335 8.1 2.39 ± 0.02 T1+ 67.8 271 4.7 2.56 ± 0.16 T1+vp 67.8 271 4.7 2.28 ± 0.09

## Analysis of results Table

![Figure 3.1: Speedup in a forward pass of T1 over FNOs sharing the same transform T (DFT) on two-dimensional signals of increasing resolution.The speedup for a given configuration (point on the plane) is shown as background color gradient. The improvement grows with signal width, resolution and model depth.]()

![Figure 3.2: Reconstructions after low-pass filtering (first m modes) [Bottom] or top-m selection [Top] of ERA5 (Hersbach et al., 2020) climate data. The non-monotonic structure of the spectrum implies more accurate reconstructions can be obtained with top-m selection.]()

![Figure 3.3: Output variance histogram in layer outputs x = W * m S m A(θ)SmWN , for a finite sample of inputs x and a single sample of θ. Color indicates signal resolution.]()

![Figure 4.1: [Left] Direct predictions at T = 50s on high viscosity Navier-Stokes. [Right] Ground-truth spectrum and absolute errors in k-space (DCT-II). Despite predicting only the first m = 24 elements, reducedorder T1 models produce smaller errors even in other regions of the k-space.]()

![Figure 4.2: Scaling laws for N-MSE.]()

![Figure 4.3: [Left] 10-step rollout predictions on ScalarFlow. FNOs produce high-frequency, non-physical artifacts and accumulate error more rapidly in time compared to T1 models [Right] Log-absolute values of predictions in k-space (DCT-II). Although T1 is limited to m = 512 and T1+ to m = 224 k-space elements, the predictions are overall more physically accurate in n-space.Setup We use the ScalarFlow dataset introduced in(Eckert et al., 2019) consisting of 104 sequences of 150 frames each collected from video recordings of rising hot smoke plumes. The dataset consists of raw video data at high-resolution (600 × 1062) collected at 60 fps. This task scales up complexity by involving real-world high-definition data, capturing highly-turbulent dynamics. We perform rollouts iteratively based on previous predictions: all models are trained on 3-step rollouts and evaluated over 10-steps extrapolation to test their generalization in time. We compare FNOs against T1, T1+ and T1+vp of similar model sizes after performing a search on most representative hyperparameters (Appendix B).]()

![Figure B.1: Incompressible Navier-Stokes: metrics vs number of DCT modes (i.e. m elements) kept (i.e. not pruned).]()

![Figure B.2: Initial conditions, ground truth solutions at time T = 50 seconds, and models predictions for incompressible Navier-Stokes in vorticity form (high viscosity of 1e -3 ). T1 reduces solution error w.r.t FNOs by over 20% and FFNOs by over 40%. A single forward pass of T1 models is on average 2× faster than FNO and 10× than FFNOs.]()

![comments Additional predictions are provided in Fig. B.2. Fig. B.1 shows the approximation error on the Navier-Stokes solutions due to truncation at different number of k-space elements m.]()

![Figure B.3: Ground truth solutions and predictions with different airfoil designs and angles of attack of the flow. The background color is the scalar pressure value while the vector field represents the velocity field: arrow colors indicate its "strength" i.e. 2-norm.]()

![Figure B.6: ScalarFlow dataset: reconstruction error versus number of kept DCT modes.]()

![Figure B.8: Reconstruction errors in pixel space of low-pass filtering of the lowest m frequency modes vs top k (m) selection on a single frame of ScalarFlow.]()

![Figure B.9: Mean log-absolute values of predictions in k-space (DCT-II) of a 20-elements batch in the test dataset. Although T1 is limited to m = 512 and T1+ to m = 224 k-space elements (visible as square "shadows" in the error plots), its predictions are overall more physically accurate in n-space.]()

Existing methods to overcome this limitation avoid the frequency domain of inputs, instead introducing an intermediate patch embedding step[(Guibas et al., 2021;](#b12)[Pathak et al., 2022)](#b32).

For clarity of exposition, models and algorithms proposed in the paper are introduced without loss of generality for one-dimensional scalar signals (i.e. Dn ≡ R n ).

e.g. discrete Fourier transform (DFT), discrete cosine transform (DCT), etc.

Data can be downloaded here: Google Drive link. High viscosity: NavierStokes_V1e-3_N5000_T50, Low viscosity: NavierStokes_V1e-4_N10000_T30.

ScalarFlow dataset download: https://ge.in.tum.de/publications/2019-scalarflow-eckert/

We used the scheduler torch.optim.lr_scheduler.CosineAnnealingWarmRestarts with the number of iterations for the first restart T _0 = 32. All other hyperparameters are the same as in the reference implementation.

