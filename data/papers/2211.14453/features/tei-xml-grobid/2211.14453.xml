<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Operator Learning in Frequency Domain</title>
				<funder>
					<orgName type="full">DOE</orgName>
				</funder>
				<funder ref="#_YQwbP6d">
					<orgName type="full">AFOSR</orgName>
				</funder>
				<funder ref="#_qbKqzHd">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_BpDfRG5">
					<orgName type="full">JSPS Kakenhi</orgName>
				</funder>
				<funder>
					<orgName type="full">ONR</orgName>
				</funder>
				<funder ref="#_J3caV5H">
					<orgName type="full">ARO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-11-26">26 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
							<email>poli@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University DiffeqML</orgName>
								<orgName type="institution" key="instit2">KAIST DiffeqML</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University CZ Biohub</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University DiffeqML</orgName>
								<orgName type="institution" key="instit2">KAIST DiffeqML</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University CZ Biohub</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mila</forename><surname>Diffeqml</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University DiffeqML</orgName>
								<orgName type="institution" key="instit2">KAIST DiffeqML</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University CZ Biohub</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Berto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University DiffeqML</orgName>
								<orgName type="institution" key="instit2">KAIST DiffeqML</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University CZ Biohub</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinykoo</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University DiffeqML</orgName>
								<orgName type="institution" key="instit2">KAIST DiffeqML</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University CZ Biohub</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University DiffeqML</orgName>
								<orgName type="institution" key="instit2">KAIST DiffeqML</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University CZ Biohub</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University DiffeqML</orgName>
								<orgName type="institution" key="instit2">KAIST DiffeqML</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University CZ Biohub</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University DiffeqML</orgName>
								<orgName type="institution" key="instit2">KAIST DiffeqML</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University CZ Biohub</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Operator Learning in Frequency Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-26">26 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">2C0F725A1D71A9EAD63693FEEACFFDA9</idno>
					<idno type="arXiv">arXiv:2211.14453v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spectral analysis provides one of the most effective paradigms for informationpreserving dimensionality reduction, as simple descriptions of naturally occurring signals are often obtained via few terms of periodic basis functions. In this work, we study deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time: frequencydomain models (FDMs). Existing FDMs are based on complex-valued transforms i.e. Fourier Transforms (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: transform once (T1). To enable efficient, direct learning in the frequency domain we derive a variance preserving weight initialization scheme and investigate methods for frequency selection in reduced-order FDMs. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of 3 x to 10 x that increase with data resolution and model size. We perform extensive experiments on learning the solution operator of spatio-temporal dynamics, including incompressible Navier-Stokes, turbulent flows around airfoils and highresolution video of smoke. T1 models improve on the test performance of FDMs while requiring significantly less computation (5 hours instead of 32 for our largescale experiment), with over 20% reduction in predictive error across tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nature uses only the longest threads to weave her patterns, so that each small piece of her fabric reveals the organization of the entire tapestry. <ref type="bibr" target="#b9">(Feynman, 1965)</ref> Naturally occurring signals are often sparse when projected on periodic basis functions <ref type="bibr" target="#b43">(Strang, 1999)</ref>. Central to recently-introduced instances of frequency-domain neural operators <ref type="bibr" target="#b23">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Tran et al., 2021)</ref>, which we refer to as frequency-domain models (FDMs), is the idea of learning to modify specific frequency components of inputs to obtain a desired output in data space. With a hierarchical structure that blends learned transformations on frequency domain coefficients with regular convolutions, FDMs are able to effectively approximate global, long-range dependencies in higher resolution signals without requiring prohibitively deep architectures.</p><p>2 Related Work and Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning and Frequency Domain: A Short History</head><p>Links between frequency-domain signal processing and neural network architectures have been explored for decades, starting with the original CNN designs <ref type="bibr" target="#b10">(Fukushima and Miyake, 1982)</ref>. <ref type="bibr" target="#b29">Mathieu et al. (2013)</ref>; <ref type="bibr" target="#b38">Rippel et al. (2015)</ref> proposed replacing convolutions in pixel space with element-wise multiplications in Fourier domain. In the context of learning to solve partial differential equations (PDEs), Fourier Neural Operators (FNOs) <ref type="bibr" target="#b23">(Li et al., 2020)</ref> popularized the state-of-the-art FDM layer structure: forward transform → learned layer → inverse transform. Similar architectures had been previously proposed for generic image classification tasks in <ref type="bibr" target="#b36">(Pratt et al., 2017;</ref><ref type="bibr" target="#b4">Chi et al., 2020)</ref>. Modifications to the basic FNO recipe are provided in <ref type="bibr" target="#b46">(Tran et al., 2021;</ref><ref type="bibr" target="#b12">Guibas et al., 2021;</ref><ref type="bibr" target="#b50">Wen et al., 2022)</ref>. A frequency domain representation of convolutional weights has also been used for model compression <ref type="bibr" target="#b3">(Chen et al., 2016)</ref>. Fourier features of input domains and periodic activation functions play important roles in deep implicit representations <ref type="bibr" target="#b40">(Sitzmann et al., 2020;</ref><ref type="bibr" target="#b6">Dupont et al., 2021;</ref><ref type="bibr" target="#b35">Poli et al., 2022)</ref> and general-purpose models <ref type="bibr" target="#b18">(Jaegle et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning to Solve Differential Equations</head><p>A variety of deep learning approaches have been developed to solve differential equations: neural operators and physics-informed networks <ref type="bibr" target="#b25">(Long et al., 2018;</ref><ref type="bibr" target="#b37">Raissi et al., 2019;</ref><ref type="bibr" target="#b26">Lu et al., 2019;</ref><ref type="bibr" target="#b20">Karniadakis et al., 2021)</ref>, specialized architectures <ref type="bibr" target="#b49">(Wang et al., 2020;</ref><ref type="bibr" target="#b24">Lienen and Günnemann, 2022)</ref>, hybrid neural-numerical methods <ref type="bibr" target="#b34">(Poli et al., 2020;</ref><ref type="bibr">Kochkov et al., 2021;</ref><ref type="bibr" target="#b28">Mathiesen et al., 2022;</ref><ref type="bibr" target="#b1">Berto et al., 2022)</ref>, and FDMs <ref type="bibr" target="#b23">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Tran et al., 2021)</ref>, the focus of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Frequency-Domain Models</head><p>Let D n (n-space) to be the set of real-valued discrete signals<ref type="foot" target="#foot_1">foot_1</ref> of resolution N . Our objective is to develop efficient neural networks to process discrete signals x ∈ D n , x 0 , x 1 , . . . , x N -1 , x n ∈ R.</p><p>We define a layer of FDMs mapping x to an output signal ŷ ∈ D n as the structured operator:</p><formula xml:id="formula_0">X = T (x) Forward Transform X = f θ (X)</formula><p>Learned Map</p><formula xml:id="formula_1">x = T -1 ( X) Inverse Transform ŷ = x + g(x)</formula><p>Residual</p><formula xml:id="formula_2">x ŷ T X f θ X T -1 x g + (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where T is an orthogonal (possibly complex) linear operator. We denote the T -transformed n-space with D k (k-space) so that T : D n → D k . Typically, we assume T to be a Fourier-type transform<ref type="foot" target="#foot_2">foot_2</ref>  <ref type="bibr">(Oppenheim, 1999, Chapter 8</ref>) so that the k-space corresponds to the frequency domain and its elements form the spectrum of the input signal x.</p><p>The learned parametric map f θ : D k → D k is the stem of a FDM layer: it maps the k-space into itself and is typically chosen to be rank-deficient in the linear case, e.g. f θ (X) = S m A(θ)S m X, A(θ) ∈ C m×m (m ≤ N ). The matrix S m ∈ R n×m selects m desired elements of X, setting the rest to zero. In the case of frequency domain transforms, this allows (1) to preserve or modify only specific frequencies of the input signal x.</p><p>Residual connections or residual convolutions g <ref type="bibr" target="#b23">(Li et al., 2020;</ref><ref type="bibr" target="#b50">Wen et al., 2022)</ref> are optionally added to reintroduce frequency components filtered by S m . A FDM mixes global transformations applied to coefficients of the chosen transform to local transformations g i.e. convolutions with finite kernel sizes. To ensure that such models can approximate generic nonlinear functions, nonlinear activations are introduced after each inverse transform.</p><p>Fourier Neural Operators Layers of the form (1) appear in recent FDMs such as Fourier Neural Operators (FNOs) <ref type="bibr" target="#b23">(Li et al., 2020)</ref> and variants <ref type="bibr" target="#b46">(Tran et al., 2021;</ref><ref type="bibr" target="#b12">Guibas et al., 2021;</ref><ref type="bibr" target="#b50">Wen et al., 2022)</ref>.</p><p>In example, an FNO is recovered from (1) by letting T be a Discrete Fourier Transform (DFT)</p><formula xml:id="formula_4">x = T -1 • f θ • T (x) = W * S m A(θ)S m W x</formula><p>where W ∈ C N ×N is the standard N -dimensional DFT matrix and W * its conjugate transpose. The Discrete Fourier Transforms (DFTs) is a natural choice of T as it can be computed in O(N log N ) via Fast Fourier Transform (FFT) algorithms <ref type="bibr">(Oppenheim, 1999, Chapter 9.</ref>2).</p><p>We identify two major limitations of FDMs in the form (1); each layer performs T and T -1 and DFTs are complex-valued, resulting in overheads and a restriction of the design space for f θ (X).</p><p>With T1, we aim to develop an FDM that does not require more than a single T , while preserving or improving on predictive accuracy. Ideally, the transform in T1 should be (1) real-valued, to avoid restrictions in the design space of the architecture and thus retain compatibility with existing pretrained models, (2) universal, to allow the representation of target signals, and (3) approximately sparse or structured, to allow dimensionality reduction.</p><formula xml:id="formula_5">ŷ x X X x T T -1 f θ T T -1 g x x X X S m A(θ)S m W W * W W *</formula><p>Commutative diagrams for FDM layers (1) and linear FNOs (frequency domain part).</p><p>3 Transform Once: The T1 Recipe</p><p>With T1, we introduce major modifications to the way FDMs are designed and optimized. In particular, T1 is defined, inferred and trained directly in the frequency domain with only a single direct transform required to process data. Hence follows the name: transform once (T1).</p><p>Direct learning in the frequency domain Consider two signals x ∈ D n , y ∈ D n and suppose there exists a function ϕ : D n → D n mapping x to y, i.e. y = ϕ(x). Then, there must also exist another function ψ : D k → D k that relates the spectra of the two signals, i.e. Y = ψ(X) being X = T (x) and Y = T (y). In particular,</p><formula xml:id="formula_6">ϕ(x) = T -1 • ψ • T (x) ⇔ T • ϕ(x) = ψ • T (x)</formula><p>It follows that, from a learning perspective, we can aim to approximate ψ directly in the k-space rather than ϕ in the n-space. To do so, we define a learnable parametric function f θ : D k → D k and train it to minimize the approximation error J θ of the output signal spectrum Y in the k-space. Given a distribution p(x) of input signals, T1 is characterized by the following nonlinear program</p><formula xml:id="formula_7">min θ E x,y T (y) -Ŷ subject to Ŷ = f θ • T (x) x ∼ p(x) y = ϕ(x) x y X Y Ŷ T T f θ J θ (2)</formula><p>If T is a DFT, the above turns out to be a close approximation (or equivalent, depending on the function class of f θ ) to the minimization of yŷ in n-space by the Parseval-Plancherel identity. Theorem 3.1 (Parseval-Plancherel Identity <ref type="bibr">(Stein and Shakarchi, 2011, pp. 223)</ref> ). Let T be the normalized DFT. Given a signal v ∈ D n and its transform</p><formula xml:id="formula_8">V = T (v), it holds v = V .</formula><p>This result also applies to any other norm-preserving transform T , e.g. a normalized type-II DCT <ref type="bibr">(Oppenheim, 1999, pp. 679)</ref>. For the linear transforms considered in this work, T (x) = W x, W ∈ C N ×N , condition for Th. 3.1 to hold is W to be orthonormal, i.e. W * W = I. Note that T1 retains, in principle, the same universal approximation properties of FNOs <ref type="bibr" target="#b22">(Kovachki et al., 2021)</ref> as f θ is allowed to operate on the entirety of the input spectrum. Given enough capacity, f θ can arbitrarily approximate ψ, implicitly reconstructing</p><formula xml:id="formula_9">ϕ via T -1 • f θ • T .</formula><p>Speedup measurements We provide a concrete example of the effect of pruning redundant transforms on computational costs. We measure wall-clock inference time speedups of depth d T1 T1(x)</p><formula xml:id="formula_10">:= f d • • • • • f 2 • f 1 • T (x)</formula><p>over an equivalent depth d FNO with layers (1). The only difference concerns the application of transforms between layers. Fig. <ref type="figure" target="#fig_1">3</ref>.1 provides the speedups on two-dimensional signals: on the left, we fix model depth d = 6 and investigate the scaling in signal width (i.e. number of channels) and signal resolution. On the right, we fix signal width to be 32 and visualize the interaction of model depth and signal resolution. For common experimental settings e.g. resolutions of 64 or 128, 6 layers and width 32, T1 is at least 10 x faster than other FDMs. It will later be shown ( § 4) that T1 also preserves or improves on predictive accuracy of other FDMs across tasks. The speedup for a given configuration (point on the plane) is shown as background color gradient.</p><p>The improvement grows with signal width, resolution and model depth.</p><p>When T1 is not preceded by online preprocessing steps for inputs x, such as other neural networks or randomized data augmentations, the transform on T (x) can be done once on the dataset, amortizing the cost over training epochs, and increasing the speed of T1 further.</p><p>Choosing the right transform The transform T in T1 is chosen to be in the class of Discrete Cosine Transforms (DCTs) <ref type="bibr" target="#b0">(Ahmed et al., 1974;</ref><ref type="bibr" target="#b43">Strang, 1999)</ref>, in particular the normalized DCT-II, which can also be computed O(N log N ) via FFTs <ref type="bibr" target="#b27">(Makhoul, 1980)</ref>. DCTs provide effective representations of smooth continuous signals <ref type="bibr" target="#b47">(Trefethen, 2019)</ref> and are the backbone of modern lossy compression methods for digital images.</p><p>Although other transforms are available, we empirically observe DCT-based T1 to perform best in our experiments. This phenomenon can be explained by the sparsity and energy distribution properties of the transformed spaces, an intrinsic property of the specific dataset and chosen transform. This is in line with results of classic signal processing and compression literature. Particularly, DCT features are known to have a higher energy compaction than their DFT counterparts in a variety of domains, from natural images <ref type="bibr" target="#b51">(Yaroslavsky, 2014)</ref> to audio signals <ref type="bibr" target="#b41">(Soon et al., 1998)</ref>. Energy compaction is often the decisive factor in choosing a transform for downstream tasks.</p><p>Letting T be a real-valued transform in T1 architectures preserves compatibility between f θ and existing architectures e.g., models pre-trained on natural image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reduced-Order T1 Model and Irreducible Loss Bound</head><p>We seek to leverage structure induced in D k by T . To this end we allow T1, similarly to (1), to modify specific elements of X and consequently trasform only certain frequency components of x (and y).</p><p>The reduced-order T1 model is designed to operate only on m &lt; N elements (selected by S m ∈ R N ×m ) of the input k-space, i.e. on a reduced k-space D m ≡ R m of lower dimension. Thus, we can employ a smaller neural network γ θ : D m → D m for mapping S m X to the corresponding m elements S m Y of the output k-space. Thus, training involves a truncated objective that compares predictions with elements in the output signal spectrum also selected by S m :</p><formula xml:id="formula_11">γ θ x X S m X Ŷ y Y S m Y J θ min θ E x,y S m • T (y) -Ŷ subject to Ŷ = γ θ • S m • T (x) x ∼ p(x) y = ϕ(x)<label>(3)</label></formula><p>How to choose modes in reduced-order FDMs We now detail some intuitions and heuristic to choose which modes k 0 , . . . , k m-1 should be kept to maximize the information content in the truncated spectrum. For this reason, we evaluate the irreducible loss arising from discarding some Nm modes. We recall that the (reduced) k-space training objective J θ (X, Y ) reads as</p><formula xml:id="formula_12">J θ (X, Y ) = S m Y -Ŷ = m l=1 |Y k l -γ θ,k l • S m (X)| ,</formula><p>since only the first m predicted output modes Ŷk1 , . . . , Ŷkm can be compared to Y k . We then consider the total loss L θ of the approximation task, including the Nm elements of the output k-space discarded by our model, i.e.</p><formula xml:id="formula_13">L θ (X, Y ) = Y -S m Ŷ = m-1 l=0 |Y k l -γ θ,k l • S m (X)| J θ (X,Y ) + N -1 k=m |Y k -0| Ro(Y )</formula><p>.</p><p>It follows that the overall loss L θ is higher than T1's training objective J θ , i.e.</p><formula xml:id="formula_14">L θ = J θ + R o &gt; J θ ,</formula><p>whilst R o represents the irreducible residual loss due to truncation of the predictions Ŷk .</p><p>Optimal mode selection in auto-encoding T1 In case Y = X, i.e. the reduced-order T1 is tasked with reconstructing the input spectrum, the optimal modes minimizing the irreducible loss are the ones with highest magnitude. This can be formalized as follows.</p><p>Proposition 3.1 (Top-m modes minimize the irreducible loss). Let Y = X (reconstruction task).</p><p>Then the choice k</p><formula xml:id="formula_15">0 , . . . , k m-1 = top k (m) |X k | minimizes the irreducible loss term R o .</formula><p>This means that if the spectrum of X is monotonically decreasing in magnitude, then low-pass filtering is the optimal mode selection. Corollary 3.1 (Low pass filtering is optimal for monotonic spectrum).</p><formula xml:id="formula_16">If |X k | is monotonically decreasing in k, then the choice k 0 , . . . , k m-1 = 0, . . . , m -1 minimizes the residual R o .</formula><p>However, spectra in practical datasets are commonly non-monotonic e.g., the spectrum of solutions of chaotic or turbulent systems <ref type="bibr" target="#b5">(Dumont and Brumer, 1988</ref>). We show an example in Fig. <ref type="figure" target="#fig_1">3</ref>.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mode selection criteria in general tasks</head><p>When Y = X and the task is a general prediction task, the simple top m analysis is not optimal. Nonetheless, given a dataset of input-output signals it is still possible to perform an a priori analysis on R o to inform the choice of the best modes to keep.</p><p>Often, we empirically observe the irreducible error R o for reduced-order T1 to be smaller than for non-reduced-order FDMs i.e R o &lt;</p><formula xml:id="formula_17">K-1 k=m Y k -T k (ŷ)</formula><p>with layers of type (1) 5 . We also note that the reachable component J θ of the objective cannot always be minimized to zero regardless of the approximation power of γ θ . For each k &lt; m, S m discards Nm frequency 5 See Fig. <ref type="figure" target="#fig_3">4</ref>.1 and Appendix B for experimental evidence in support of this phenomenon. components of the input signal which, if different than zero, likely contain the necessary information to approximate ψ k (X) exactly. Specifically, the irreducible lower bound on J θ should depend on "how much" the output's m frequency components depend on the discarded N -m input's elements.</p><p>A rough quantification of such bound can be obtained by inspecting the mismatch between the gradients of ψ kγ θ,k • S m with respect to X. In particular, it holds</p><formula xml:id="formula_18">N -1 j=0 ∂ψ k (X) ∂X j - ∂γ θ,k (S m X) ∂X j = m-1 j=0 ∂ψ k (X) ∂X j - ∂γ θ,k (S m X) ∂X j + N -1 j=m ∂ψ k (X) ∂X j ,</formula><p>Unless ∂ Xj ψ k (X) = 0 holds for all j = m, . . . , N -1 and k = 0, 1, . . . , N -1 i.e. no dependency of the ground truth map in k-space on the truncated elements, there will be an irreducible overall gradient mismatch and thus a nonzero J θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weight Initialization for Reduced-Order FDMs</head><p>FDMs <ref type="bibr" target="#b23">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Tran et al., 2021;</ref><ref type="bibr" target="#b50">Wen et al., 2022)</ref> opt for a standard Xavier-like <ref type="bibr" target="#b11">(Glorot and Bengio, 2010)</ref> initialization distribution that takes into account the input channels c to a layer i.e. N (0, 1 c ). However, well-known variance preserving properties of Xavier schemes do not hold for FDM layers truncating Nm elements of the k-space. Notably, Xavier schemes do not scale the variance of the weight initialization distribution based on the number of elements m kept after truncation of the spectrum performed by f θ , leading to the collapse of the outputs to zero.</p><p>To avoid this issue in T1 and other FDMs, we develop a simple variance-preserving (vp) that introduces a variance scaling factor based on m and the class of transform.</p><formula xml:id="formula_19">Theorem 3.2 (Variance Preserving (vp) Initialization). Let x = W * S m AS m W x be a k-space reduced-order layer and W is a normalized DCT-II transform. If x ∈ R N is a random vector with E[x] = 0, V[x] = σ 2 I. Then, A ij ∼ N 0, N m 2 ⇒ V[x] = V[x].</formula><p>We report the proof in Appendix A, including some considerations for specific forms of f θ . Corollary 3.2 (vp initialization for DFTs). Under the assumptions of Theorem 3.2, if W is a normalized DFT matrix we have</p><formula xml:id="formula_20">Re(A ij ), Im(A ij ) ∼ N (0, N 2m 2 ) ⇒ V[x] = V[x].</formula><p>The collapse phenomenon is empirically shown in Fig. <ref type="figure" target="#fig_1">3</ref>.3 for m = 24, comparing a single layer of FNO and FFNO (with Xavier initialization) with FNO equipped with the proposed vp scheme. Under the assumptions of Corollary 3.2, we sample A and compute empirical variances of x = W * S m A(θ)S m W x for several finite batches of input signals x. We repeat the experiment for signals of different lengths N . The vp scheme preserves unitary variances whereas the other layers concentrate output variances towards zero at a rate that grows with Nm. When the learned frequency-domain transformation f θ is obtained, instead of the single low-rank linear layer f θ = A(θ)S m X, as the composition of several layers, preserving variances can be achieved by applying the vp scheme only to the first layer. For some variants of FDMs e.g. FNO that truncate the spectrum at each layer, vp initialization should instead be applied to all.</p><formula xml:id="formula_21">0.5 1 1.5 2 2.5 σ 2 Ours (DFT) 0 0.5 1 1.5 σ 2 Variances of x FNO (DFT) 0 2 4 6 σ 2 FFNO (DFT)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We validate T1 on learning to approximate solution operators of dynamical systems from images.</p><p>• In § 4.1, we apply T1 on the standard task of learning solution operators for incompressible Navier-Stokes, comparing against other FDMs. In § 4.1.1 we perform a series of ablation experiments on each ingredient of the T1 recipe, including weight initialization and architecture. In § 4.1.2 we provide scaling laws. • In § 4.2 we deal with fluid-solid interaction dynamics in the form of higher resolution images (128). We consider turbulent flows around varying airfoil geometries, benchmarking against current SOTA <ref type="bibr" target="#b45">(Thuerey et al., 2020)</ref>. • In § 4.3 we show how the computational efficiency of T1 allows learning on unwieldy data without downsampling or building low-resolution meshes. We consider learning on high-resolution video (600 × 1062) capturing the turbulent dynamics of smoke <ref type="bibr" target="#b7">(Eckert et al., 2019)</ref>.</p><p>Configuration and model details are reported in the supplementary material. The code is available at <ref type="url" target="https://github.com/DiffEqML/kairos">https://github.com/DiffEqML/kairos</ref>. Weights &amp; Biases (wandb) <ref type="bibr" target="#b2">(Biewald, 2020</ref>) logs of results are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Incompressible Navier-Stokes</head><p>We show that T1 matches or outperforms SOTA FDMs with less computation on the standard incompressible Navier-Stokes benchmark. Losses are reported in n-space (signal space) for comparison.</p><p>Setup We consider two-dimensional Navier-Stokes equations for incompressible fluid in vorticity form as described in <ref type="bibr" target="#b23">(Li et al., 2020)</ref>. Given a dataset of initial conditions, we train all models to approximate the solution operator at time 50 seconds for high viscosity (ν = 1e -3 ) and at time 15 for lower viscosity (ν = 1e -4 ). As a metric, we report normalized mean squared error (N-MSE).</p><p>Both initial condition as well as solution are provided as images of resolution 64.</p><p>We include as baseline established FDMs, such as Fourier Neural Operators (FNOs) <ref type="bibr" target="#b23">(Li et al., 2020)</ref> and Factorized Fourier Neural Operators (FFNOs) <ref type="bibr" target="#b46">(Tran et al., 2021)</ref>. We indicate with the suffix vp models that employ the proposed variance preserving initialization scheme. All models truncate to m = 24, except FFNOs to m = 32. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We perform 20 training runs for each model and report mean and standard deviation in</p><p>Table 4.1. T1 reduces solution error w.r.t FNOs by over 20% and FFNOs by over 40%. A single forward pass of T1 models is on average 2x faster than FNO and 10x than FFNOs. We note that FFNOs Method Param. (M) Size (MB) Step (ms) high ν low ν FFNO (Tran et al., 2021) 8.9 35 294 0.997±0.003 1.016±0.010 FNO (Li et al., 2020) 14.2 56 31 0.379±0.006 0.328±0.004 FNOvp 14.2 56 32 0.351±0.003 0.315±0.006 T1+vp 10.2 40 19 0.257±0.007 0.240±0.004 Table 4.1: Benchmarks on incompressible Navier-Stokes. Direct long-range prediction errors (N-MSE) in n-space (signal space) of different models.</p><p>are designed to share parameters between layers and thus require deeper architectures -and slower, due to more transforms. In particular, training time (500 epochs) for T1 is cut to 20 minutes down from 40 of FNOs, matching the model speedup. Finally, we report an improvement in performance for FNOs with parameters initialized following our proposed scheme (FNOvp). Fig. <ref type="figure" target="#fig_3">4</ref>.1 provides sample predictions in n-space (left) to contextualize the task, in addition to prediction errors in frequency domain (right). Despite being a reduced order model with m = 24, T1+vp produces smaller errors on truncated k-space elements (k &gt; m) compared to FNOvp and FFNO.</p><p>4.1.1 Ablations on weight scheme and architecture Method high ν low ν T1 0.491 0.449 T1vp 0.304 0.280 T1+ 0.295 0.260 T1+vp 0.257 0.240 Table 4.2: Ablation on the effect of the proposed weight initialization scheme and T1 architecture.</p><p>We repeat the previous experiment and report prediction errors for four variants of T1: same architecture and weight initialization scheme as FNOs (T1), T1 with our proposed vp scheme (T1vp), a reduced-order variant with k-space model f θ defined as a UNet architecture (T1+), and T1+ with variance preserving scheme (T1+vp). The results in Table <ref type="table">4</ref>.2 provide empirical evidence in support of the vp scheme and its synergistic effect with the proposed architecture. In particular, combining vp scheme and UNet structure in frequency domain reduces error by half compared to the naive T1 approach. We verify whether the reduction in predictive error of T1 over neural operator baselines is preserved as the size of training dataset grows. We perform 10 training runs on the Navier-Stokes ν = 1e -4 experiment, each time with a larger dataset size, and report the scaling laws in Fig. <ref type="figure" target="#fig_3">4</ref>.2. With additional data, the gaps in test errors narrow slightly, with noticeable improvements obtained by applying the vp scheme to both FNO and T1+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Scaling laws</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Flow Around Airfoils</head><p>We investigate the performance of T1 in predicting steady-state solutions of flow around airfoils.</p><p>Setup We use data introduced in <ref type="bibr" target="#b45">(Thuerey et al., 2020)</ref> in the form of 10000 training pairs of initial conditions, specifying freestream velocities and the airfoil mask, with the target steady-state velocity and pressure fields. This task introduces additional complexity in the form of higher resolution input images (128) and a full k-space due to the discontinuity in the field produced by the mask.</p><p>We compare a SOTA UNet architecture (DFPNet) introduced by <ref type="bibr" target="#b45">(Thuerey et al., 2020)</ref> to FNOs and T1 with vp initialization schemes. We perform a search on the most representative hyperparameters (detailed in the Appendix). Averages for 5 runs are reported in</p><p>Table 4.3. Method N-MSE Time (hrs) DFPNet 0.023 1.3 FNO 0.020 6.0 T1+vp 0.024 1.3 Table 4.3: Test N-MSE and total training time on the flow around airfoil task.</p><p>Results All models are able to accurately predict steady-state solutions for different airfoils with small normalized errors. Test N-MSE is comparable as all models are within a single standard deviation. Training of T1 is as fast as DFPNets <ref type="bibr" target="#b45">(Thuerey et al., 2020)</ref> and as accurate as FNOs, as evidence of the applicability of T1 to tasks with signals that are not band-limited (in this case due to the airfoil mask).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Turbulent Smoke</head><p>We investigate the performance of T1 in predicting iterative rollouts from high-resolution video of real rising smoke plumes. Method N-MSE Time (hrs) FNO 0.232 32.4 T1 0.239 8.1 T1+ 0.256 4.7 T1+vp 0.228 4.7 Table 4.4: Test 10-steps rollout n-space prediction errors (N-MSE) and total training time on the ScalarFlow dataset. Results Fig. 4.3 provides a sample rollout of different model predictions in k-space (DCT-II). T1+vp accumulates smaller errors over the rollout and is less prone to generating non-physical artifacts by performing prediction only on a subset of the k-space (Table 4.4). Notably, T1 and T1+ are 4× to 7× faster, providing a reduction in training time from 32.4 hours to 4.7. Appendix B includes additional visualizations, including averaged prediction errors on k-space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a streamlined class of frequency domain models (FDM): Transform Once (T1). T1 models are optimized directly in frequency domain, after a single transform, and achieve similar or improved predictive performance at a fraction of the computational cost (3x to 10x speedups across tasks). Further, a simple truncation-aware weight initialization scheme is introduced and shown to improve the performance of T1 and existing FDMs.</p><p>A Proof of Theorem 3.2</p><p>A.1 Preliminary Lemmas Lemma A.1 (Propagation of Uncertainty under DFT/DCT).</p><formula xml:id="formula_22">Let X = W x with x ∈ R N and W ∈ C N ×N . Then Σ X = W Σ x W * Proof. Σ X = E [(W x -E[W x]) ∧ (W x -E[W x])] = E [W (x -E[x]) ∧ W (x -E[x])] = E W (x -E[x])(x -E[x]) W * = W E W (x -E[x])(x -E[x]) W * = W Σ x W * Lemma A.2 (Propagation of Total Variance under DFT/DCT). Let X = W x with x ∈ R N and W ∈ C N ×N . Then V[X] = V[x]</formula><p>Proof. Recalling that the total variance of a random variable is equal to the trace of its covariance matrix, i.e.</p><formula xml:id="formula_23">V[x] = tr(Σ x ), V[X] = tr(Σ X ) then tr(Σ x ) = tr(Σ X ) ⇔ V[X] = V[x] Recalling Lemma A.1 yields V[X] = V[x] ⇔ tr(Σ x ) = tr(W Σ x W * ) ⇔ tr(Σ x ) -tr(W Σ x W * ) = 0 ⇔ tr(Σ x ) -tr(Σ x W * W ) = 0</formula><p>Since the DCT/DFT matrix is orthonormal, i.e. W * = W -1 we have that tr(Σ x W * W ) = tr(Σ x ), proving the result.</p><formula xml:id="formula_24">Lemma A.3 (Gaussian initialization in rank-deficient linear layers). Let X = S m AS m X with X ∈ R N , A ∈ C m×m and S m ∈ C m×N , S m = 1 • • • 0 0 • • • 0 . . . . . . . . . . . . . . . . . . 0 • • • 1 0 • • • 0            m m N -m . If E[X k ] = 0, V[X k ] = σ 2 for all k the following hold: i. for k ≥ m E[ Xk ] = 0, V[ Xk ] = 0 ii. for k &lt; m and Re(A ij ), Im(A ij ) ∼ N (0, σ 2 A ) E[ Xk ] = 0, V[ Xk ] = 2mσ 2 σ 2 A iii. for k &lt; m and Re(A ij ) ∼ N (0, σ 2 A ), Im(A ij ) = 0 E[ Xk ] = 0, V[ Xk ] = mσ 2 σ 2 A Proof. Let M = S m AS m . It holds, M = A × × × ∈ C N ×N</formula><p>where"×" are blocks of complex zeros. By expanding component-wise the layer computation, i.e.</p><formula xml:id="formula_25">Xk = N -1 j=0 M kj X j , it holds that for k &lt; m Xk = m-1 j=0 A kj X j ,</formula><p>while Xk = 0 for k ≥ m. Hence i. follows naturally from the latter and we focus on proving ii. and iii.</p><p>Case ii. The probability distribution of Xk is a sum of product distributions involving independent random variables A kj and X j . The first central moment is readily obtained</p><formula xml:id="formula_26">E[ Xk ] = m-1 t=0 E[A kj ]E[X j ] = 0 since both E[X k ] = 0 and ∀ k, j &lt; m : E[A kj ] = 0. V[ Xk ]</formula><p>can be then obtained by computing the variance of the product of two random variables, i.e.</p><formula xml:id="formula_27">V[ Xk ] = m-1 j=0 V[A kj ] + $ $ $ $ E[A kj ] 2 )(V[X j ] + ¨Ë [X j ] 2 ) - @ @ @ @ @ @ @ E[A kj ] 2 E[X j ] 2 = m-1 j=0 V[A kj ]V[X j ] = m-1 j=0 σ 2 V[A kj ] = σ 2 m-1 j=0 (V[Re(A kj )] + V[Im(A kj )]) = σ 2 m-1 j=0 2σ 2 A = 2mσ 2 σ 2 A Case iii.</formula><p>Similarly to the previous case we get</p><formula xml:id="formula_28">V[ Xk ] = σ 2 m-1 j=0 V[Re(A kj )] + $ $ $ $ $ V[Im(A kj )] = σ 2 m-1 j=0 σ 2 A = mσ 2 σ 2 A A.2 Proof of Main Result</formula><p>Proof. According to Lemma A.2, the total variance is preserved under the normalized DCT. Therefore, with X = W x and X = W x we have</p><formula xml:id="formula_29">V[X] = V[x], V[ X] = V[x].</formula><p>Using X = S m AS m X, we can find the condition under which the variance is preserved by the map x → x:</p><formula xml:id="formula_30">V[x] = V[x] ⇔ N -1 n=0 V[x n ] = N -1 n=0 V[x n ] ⇔ N -1 k=0 V[ Xk ] = N -1 k=0 V[X k ] ⇔ m-1 k=0 mσ 2 σ 2 A = N -1 k=0 σ 2 Lemma A.3 ⇔ m 2 σ 2 σ 2 A = N σ 2 ⇔ σ 2 A = N m 2</formula><p>Hence, initializing A by sampling its entries from a normal distribution with zero mean and variance N/m 2 is sufficient for preserving the variance under the reduced-order FDM layer, i.e.</p><formula xml:id="formula_31">A ij ∼ N 0, N m 2 ⇒ V[x] = V[x],</formula><p>proving the result.</p><p>Corollary 3.2 (vp initialization for DFTs). Under the assumptions of Theorem 3.2, if W is a normalized DFT matrix we have</p><formula xml:id="formula_32">Re(A ij ), Im(A ij ) ∼ N (0, N 2m 2 ) ⇒ V[x] = V[x].</formula><p>Proof. The proof follows directly from the one of Theorem 3.2 using the fact that since the DFT's k-space is complex (D k ≡ C N ) as W ∈ C N ×N , the weights are typically chosen complex, i.e.</p><p>A ∈ C m×m . Therefore, in this case V[ X] = 2mσ 2 σ 2 A according to Lemma A.3.</p><p>Corollary A.1 ((vp) initialization with diagonal layers). Under the assumptions of Theorem 3.2, if A is diagonal s.t ∀i = j :</p><formula xml:id="formula_33">A ij = 0, we have A ii ∼ N 0, N m ⇒ V[x] = V[x].</formula><p>Proof. The proof follows directly from Lemma A.3</p><formula xml:id="formula_34">V[ Xk ] = m-1 j=0 V[A kj ]V[X j ] = V[A kk ]V[X k ] = σ 2 V[Re(A kk )] + $ $ $ $ $ $ V[Im(A kk )] = σ 2 σ 2 A leading to the condition V[x] = V[x] ⇔ m-1 k=0 σ 2 σ 2 A = N -1 k=0 σ 2 ⇔ mσ 2 σ 2 A = N σ 2 ⇔ σ 2 A = N m</formula><p>The layer structure treated by A.1 is common among many FDMs, e.g. FNOs in <ref type="bibr" target="#b23">(Li et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Details</head><p>Broader impact FDMs are widely used in the context of learning to predict the evolution of dynamical systems. The model class presented in this work, T1, provides an accessible way to train and evaluate large-scale FDMs, reducing memory overhead and overall training times. When predicting the solution of e.g. a partial differential equation (PDE), care should be taken especially when the prediction is used to inform downstream decision making, as many systems are optimally predictable only for a certain time scale <ref type="bibr">(Strogatz, 2018, pp. 366</ref>). We anticipate a potential positive environmental impact from the adoption of T1 as a replacement for the largest FDMs currently in use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup</head><p>Experiments have been performed on an NVIDIA© DGX workstation equipped with a 128 threads AMD© EPYC 7742 CPU, 512GB of RAM and four NVIDIA© A100 GPUs. The main software implementation has been done within the PyTorch <ref type="bibr" target="#b31">(Paszke et al., 2017)</ref> ecosystem building upon the pytorch-lightning <ref type="bibr" target="#b8">(Falcon et al., 2019)</ref> framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common experimental settings B.1 Incompressible Navier-Stokes</head><p>Dataset We use data generated in <ref type="bibr" target="#b23">(Li et al., 2020)</ref> in the form of pairs of initial conditions and solutions of the incompressible Navier-Stokes equations in vorticity form solved with a pseudospectral method. The dataset<ref type="foot" target="#foot_3">foot_3</ref> is comprised rollouts of solutions as images of resolution 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and training</head><p>The training configuration is shared by all models:</p><p>datamodule: ntrain: 1000 ntest: 200 batch_size: 64 history_size: 1 train: optimizer: type: AdamW learning_rate: 1e-3 weight_decay: 1e-4 scheduler: type: Step step_size: 100 gamma: 0.5 scheduler_interval: epoch loss_fn: RelativeL2Loss</p><p>For the high viscosity (1e -3 ) setting, the models are trained to predict the solution at time T = 50 seconds directly, without producing rollouts and supervising the model with solutions at times between 0 and 50. Crucially, this ensures that the task is much more challenging than that of <ref type="bibr" target="#b23">(Li et al., 2020)</ref>, where for a single training sample the entire rollout is used as supervision. For the low viscosity setting (1e -4 ), target times are T = 15 seconds.</p><p>Model configurations are given below: FNO: modes: 24 nlayers: 6 width: 32 T1: modes: 24 nlayers: 6 width: 48 FFNO: modes: 32 nlayers: 10 width: 82</p><p>where each layer in a model shares the same structure. In FNOs and FFNOs, we employ a regular FDM layer following <ref type="bibr" target="#b23">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Tran et al., 2021)</ref> with k-space convolutions and residual connections given by n-space layers (pointwise convolutions for FNOs, dense for FFNOs). T1 uses a similar layer without n-space residual paths. The differences in number of layers and width have been introduced to keep parameter counts comparable. At a given channel width, FNOs require the largest number of parameters due to k-space convolutions on complex numbers given by the DFT coefficients. Although FFNOs <ref type="bibr" target="#b46">(Tran et al., 2021)</ref> are most parameter efficient due to parameter sharing, we found them unable to tackle the task and produce high-quality predictions. T1+ employs a UNet on the patch constructed by the elements of the k-space kept, and shares its structure with T1 otherwise. The vp parameter initialization scheme in T1 is applied only to the first layer performing the truncation in k-space, not to the following layers which use standard Kaiming initialization <ref type="bibr" target="#b15">He et al. (2015)</ref>. In FNOvp the scheme is applied to all layers. Hyperparameter tuning We start with the basic model structure of FNOs as detailed <ref type="bibr" target="#b23">(Li et al., 2020)</ref> and perform a basic hyperparameter search on a small slice of the training set, with the goal of ensuring proper convergence of a model. We did not find the number of layers to have a significant impact on convergence. Width plays an important role and is best kept above 24.</p><p>Scaling laws We use the same settings as the main experiment, repeating separate training runs for the low viscosity setting. In particular, we increase the dataset size for each set of runs by a factor of 2: 1024, 2048, 4096, 8192. The total number of epochs is kept fixed, so that more iterations are performed for larger datasets. The same test set of size 200 is used in all cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Flow Around Airfoils</head><p>Dataset We use a slice of the dataset introduced by <ref type="bibr" target="#b45">Thuerey et al. (2020)</ref> in the form of 11000 training pairs of initial conditions and solutions. The solutions are obtained via OpenFOAM <ref type="bibr" target="#b19">(Jasak et al., 2007</ref>) SIMPLE, a steady-state solver for incompressible and turbulent flows. In particular, the initial conditions are specified as freestream velocities over the domain <ref type="bibr">(two-directional components)</ref>, in addition to a specification of the airfoil in point cloud format. Delaunay triangulation is used for mesh generation.</p><p>After simulation, data is provided as initial condition and steady-state solution pairs. The initial condition is a three channel 128 × 128 image: two channels for freestream velocities and one for the airfoil mask. The solution is a three channel 128 × 128 image: a velocity field and a scalar pressure field. All data is normalized using training set statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and training</head><p>Training configuration is given as datamodule: ntrain: 8000 nval: 2000 ntest: 1000 batch_size: 64 train: optimizer: type: AdamW learning_rate: 1e-3 weight_decay: 1e-4 scheduler: type: Step step_size: 100 gamma: 0.6 scheduler_interval: epoch loss_fn: RelativeL2Loss</p><p>The baseline UNet matches the architecture of <ref type="bibr">(Thuerey et al., 2020) (DFPNet)</ref>. The FNO architecture is comprised of a standard stack of FDM layer as discussed in B.1. The k-space UNet in T1+ has the same structure as a DFPNet. Hyperparameter tuning This is an example of a dataset where the k-space is full due to discontinuity in the solution given by the airfoil mask.</p><p>We use the training and validation sets to inspect the k-space and set m to 100 for the irreducible loss term to be sufficiently small as shown in Fig. <ref type="figure">B</ref>.5. We swept over m for FNOs and found larger than 24 to perform worse, likely due to k-space convolution being sufficient to capture higher frequency components. We observe DFPNets with larger channel exponents perform worse due to overfitting.</p><p>Further comments A sample of predictions is given in Fig. <ref type="bibr">B.3. Fig. B.4</ref> shows the n-space and corresponding DCT k-space of a data point. As can be observed, the k-space is structured but full due to the discontinuity caused by the airfoil mask. Fig. <ref type="figure">B</ref>.5 shows the approximation error on solution fields due to truncation in k-space at different m. In this task, the DCT is more efficient, given a budget of modes to keep, as it yields lower errors. This error provides a theoretical lower On the x-axis, the normalized cost for a number of modes m: for DCTs, since the k-space is real, truncation at m modes requires m 2 floats, for real FFTs with complex k-space and conjugacy the cost in floats is 4m 2 . The vertical line indicates the budget used for T1 used in this task (m = 100), while the horizontal line is the test N-MSE achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Turbulent Smoke</head><p>Dataset We employ for this experiment the ScalarFlow dataset introduced in <ref type="bibr" target="#b7">(Eckert et al., 2019)</ref> which is available online under the Creative Commons license CC-BY-NC-SA 4.0<ref type="foot" target="#foot_4">foot_4</ref> . <ref type="bibr" target="#b7">Eckert et al. (2019)</ref> created an environment for controlling the release of smoke plumes: a fog machine generated fog inside of a container; the fog was then heated up by a heating cable and a valve controlled its release. Data was captured via multiple calibrated cameras in high resolution at 60 fps (frames per second) for 150 frames. The dataset contains 3D reconstructions of the smoke plumes and 2D input and rendered images: input images are used by <ref type="bibr" target="#b7">Eckert et al. (2019)</ref> to solve an optimization problem in which the goal is to generate a 3D reconstruction that minimizes the difference between input and rendered images. 2D input images are obtained directly from raw data on which only post-processing is applied by <ref type="bibr" target="#b7">(Eckert et al., 2019)</ref> in the form of gray scaling and denoising: these are saved in compressed numpy <ref type="bibr" target="#b14">(Harris et al., 2020)</ref> arrays named imgsTarget_000xxx.npz. Each resulting frame comprises 5 different camera views 600 × 1062 in size. Since we want to use T1 on high-resolution experimental data, we directly utilize the central camera view of these input images in our learning task without any further downsampling or data processing. Similarly to <ref type="bibr" target="#b24">(Lienen and Günnemann, 2022)</ref>, we divide the 104 recordings into the first 64 for training and use the remaining 20 for validation and 20 for testing.</p><p>Data is normalized to the [0, 1] range based on training dataset statistics.</p><p>Hyperparameter selection and tuning We performed a search on the most representative hyperparameters. One of the most important hyperparameters to choose from is the number of DCT modes to keep, i.e. first m elements in k-space. We note that for simplicity as well as for compatibility with the UNet inside of T1+, we consider a square mode pruning, i.e. we keep the same  Models and training All models share the configuration for training: datamodule: ntrain: 64 nval: 20 ntest: 20 batch_size: 1 history_size: 1 target_steps_train: 3 target_steps_val_test: 10 train: optimizer: type: AdamW learning_rate: 1e-3 weight_decay: 1e-4 scheduler: type: CosineAnnealingWarmRestarts T_0: 32 step_size: 1 scheduler_interval: step loss_fn: RelativeL2Loss</p><p>Where we used the implementation in PyTorch of the cosine annealing schedule with warm restarts<ref type="foot" target="#foot_5">foot_5</ref> . The FNO architecture comprises a standard stack of FDM layers as discussed in B.1. The k-space UNet in T1+ (and in its vp variant) has the same structure as a DFPNet.</p><p>modes: 48 nlayers: 4 width: 48 modes: 512 nlayers: 4 width: 8 modes: 224 nlayers: 1 width: 4 channel_exponent: 7</p><p>where we note that all models employ GeLU <ref type="bibr" target="#b16">(Hendrycks and Gimpel, 2016)</ref> activation functions between inner layers. <ref type="table">B</ref>.1 provides a larger version of the table in the main text, including 1-step mean absolute errors (MAE). We note that while FNO produces smaller errors in one-step predictions, it quickly accumulates larger errors in extrapolation. Fig. <ref type="figure">B</ref>.9 shows mean errors in k-space of FNO vs T1 and T1+. T1 models demonstrate smaller overall errors and lower maxima compared to the FNO. Table B.1: Full benchmark on the ScalarFlow dataset over 5 runs with different random seeds. N-MSE refers to 10-step test rollouts. T1+vp generates more stable rollouts while requiring a fraction of FNO's training time. Method Param (M) Size (MB) Time (hrs) N-MSE (×10 -1 ) FNO 84.9 339 32.4 2.32 ± 0.02 T1 83.9 335 8.1 2.39 ± 0.02 T1+ 67.8 271 4.7 2.56 ± 0.16 T1+vp 67.8 271 4.7 2.28 ± 0.09</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of results Table</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 . 1 :</head><label>31</label><figDesc>Figure 3.1: Speedup in a forward pass of T1 over FNOs sharing the same transform T (DFT) on two-dimensional signals of increasing resolution.The speedup for a given configuration (point on the plane) is shown as background color gradient. The improvement grows with signal width, resolution and model depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure 3.2: Reconstructions after low-pass filtering (first m modes) [Bottom] or top-m selection [Top] of ERA5 (Hersbach et al., 2020) climate data. The non-monotonic structure of the spectrum implies more accurate reconstructions can be obtained with top-m selection.</figDesc><graphic coords="6,123.05,639.49,102.13,51.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3.3: Output variance histogram in layer outputs x = W * m S m A(θ)SmWN , for a finite sample of inputs x and a single sample of θ. Color indicates signal resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4.1: [Left] Direct predictions at T = 50s on high viscosity Navier-Stokes. [Right] Ground-truth spectrum and absolute errors in k-space (DCT-II). Despite predicting only the first m = 24 elements, reducedorder T1 models produce smaller errors even in other regions of the k-space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: Scaling laws for N-MSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Figure 4.3: [Left] 10-step rollout predictions on ScalarFlow. FNOs produce high-frequency, non-physical artifacts and accumulate error more rapidly in time compared to T1 models [Right] Log-absolute values of predictions in k-space (DCT-II). Although T1 is limited to m = 512 and T1+ to m = 224 k-space elements, the predictions are overall more physically accurate in n-space.Setup We use the ScalarFlow dataset introduced in<ref type="bibr" target="#b7">(Eckert et al., 2019)</ref> consisting of 104 sequences of 150 frames each collected from video recordings of rising hot smoke plumes. The dataset consists of raw video data at high-resolution (600 × 1062) collected at 60 fps. This task scales up complexity by involving real-world high-definition data, capturing highly-turbulent dynamics. We perform rollouts iteratively based on previous predictions: all models are trained on 3-step rollouts and evaluated over 10-steps extrapolation to test their generalization in time. We compare FNOs against T1, T1+ and T1+vp of similar model sizes after performing a search on most representative hyperparameters (Appendix B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Figure B.1: Incompressible Navier-Stokes: metrics vs number of DCT modes (i.e. m elements) kept (i.e. not pruned).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>Figure B.2: Initial conditions, ground truth solutions at time T = 50 seconds, and models predictions for incompressible Navier-Stokes in vorticity form (high viscosity of 1e -3 ). T1 reduces solution error w.r.t FNOs by over 20% and FFNOs by over 40%. A single forward pass of T1 models is on average 2× faster than FNO and 10× than FFNOs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Further</head><figDesc>comments Additional predictions are provided in Fig. B.2. Fig. B.1 shows the approximation error on the Navier-Stokes solutions due to truncation at different number of k-space elements m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>Figure B.3: Ground truth solutions and predictions with different airfoil designs and angles of attack of the flow. The background color is the scalar pressure value while the vector field represents the velocity field: arrow colors indicate its "strength" i.e. 2-norm.</figDesc><graphic coords="21,117.91,277.59,62.01,62.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>Figure B.6: ScalarFlow dataset: reconstruction error versus number of kept DCT modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure B. 8 :</head><label>8</label><figDesc>Figure B.8: Reconstruction errors in pixel space of low-pass filtering of the lowest m frequency modes vs top k (m) selection on a single frame of ScalarFlow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>Figure B.9: Mean log-absolute values of predictions in k-space (DCT-II) of a 20-elements batch in the test dataset. Although T1 is limited to m = 512 and T1+ to m = 224 k-space elements (visible as square "shadows" in the error plots), its predictions are overall more physically accurate in n-space.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Existing methods to overcome this limitation avoid the frequency domain of inputs, instead introducing an intermediate patch embedding step<ref type="bibr" target="#b12">(Guibas et al., 2021;</ref><ref type="bibr" target="#b32">Pathak et al., 2022)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>For clarity of exposition, models and algorithms proposed in the paper are introduced without loss of generality for one-dimensional scalar signals (i.e. Dn ≡ R n ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>e.g. discrete Fourier transform (DFT), discrete cosine transform (DCT), etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Data can be downloaded here: Google Drive link. High viscosity: NavierStokes_V1e-3_N5000_T50, Low viscosity: NavierStokes_V1e-4_N10000_T30.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>ScalarFlow dataset download: https://ge.in.tum.de/publications/2019-scalarflow-eckert/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>We used the scheduler torch.optim.lr_scheduler.CosineAnnealingWarmRestarts with the number of iterations for the first restart T _0 = 32. All other hyperparameters are the same as in the reference implementation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported by <rs type="funder">NSF</rs> (<rs type="grantNumber">1651565</rs>), <rs type="funder">AFOSR</rs> (<rs type="grantNumber">FA95501910024</rs>), <rs type="funder">ARO</rs> (<rs type="grantNumber">W911NF-21-1-0125</rs>), <rs type="funder">ONR</rs>, <rs type="funder">DOE</rs>, <rs type="person">CZ Biohub</rs>, <rs type="person">Sloan Fellowship</rs> and <rs type="funder">JSPS Kakenhi</rs> (<rs type="grantNumber">21J14546</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qbKqzHd">
					<idno type="grant-number">1651565</idno>
				</org>
				<org type="funding" xml:id="_YQwbP6d">
					<idno type="grant-number">FA95501910024</idno>
				</org>
				<org type="funding" xml:id="_J3caV5H">
					<idno type="grant-number">W911NF-21-1-0125</idno>
				</org>
				<org type="funding" xml:id="_BpDfRG5">
					<idno type="grant-number">21J14546</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transform Once Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table of Contents</head><p>Notation We report here a reference for notation used in main text and supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head><p>Set of reals</p><p>Expected value of random variable x</p><p>Variance of random variable x Σ x</p><p>Covariance matrix of random variable x tr Trace operator for square matrices.  first m elements) .</p><p>[Bottom] Error between the ground truth frame y and its inverse transformation after mode pruning from k-space back to n-space. As expected, the first few k-space elements are crucial to minimizing reconstruction errors, with higher frequency components contributing minimally.</p><p>We also experiment with different iterative rollout update strategies as in <ref type="bibr" target="#b33">(Pfaff et al., 2020)</ref>. We consider the time step ∆t to be unitary, i.e. ∆t = 1, given that the training frames are sampled consistently at 60 fps. We call 0-order integration an update of the type: x t+1 = h θ (x t ; x t-1 , . . . , x t-H ) in which h θ denotes a learned model which takes as inputs the current state x t and optionally a history of size H of past states x t-1 , . . . , x t-H and directly predicts the next state x t+1 . A 1-order integrator performs the following update:</p><p>, in which the model predicts the state update, i.e. the velocity, similarly to an Euler step. A 2-order integrator, also known as basic Störmer-Verlet <ref type="bibr" target="#b48">(Verlet, 1967</ref>) can be written as following:</p><p>•); the model h θ predicts the acceleration of the system. We empirically found the zero-order integration to be more prone to generating artifacts with slower convergence, which may be because the model has to directly predict the next step with no "help" from the current step information. We found models trained with first-order integrators to have lower predictive errors than those trained with second-order ones, and we thus use it in all the experiments. As for the history size, we selected H = 1 since it provided noticeable benefits compared to H = 0, in which the model has no way of knowing previous states and thus inferring velocities. Larger history sizes did not seem to provide any improvements and only made the models larger as also noted in <ref type="bibr" target="#b33">(Pfaff et al., 2020)</ref>.</p><p>Mode selection We further show in Fig. <ref type="figure">B</ref>.8 the effect of simple low-pass filtering of lowest m frequency modes and top k (m) mode selection in pixel space reconstruction (as a fraction of total pixes, i.e., 600 × 1062). The latter achieves better reconstruction results with the same number of parameters.</p><p>Equation ( <ref type="formula">6</ref>) becomes 2 sin(z)y = 2 sin(z) + sin(z + z)sin(zz) + sin(2z + z)sin(2zz) + sin(3z + z)sin(3zz)</p><p>where terms on the right-hand side cancel out pairwise 9 . After cleanup, we are left with the following 2 sin(z)y = sin(z) + sin((N -1)z) + sin(N z).</p><p>By substituting back z = 2πk N we obtain</p><p>where we used the trigonometric identity sin(-α) =sin(α). After dividing by the factor 2 sin 2πk N , we readily obtain the result y = 0.</p><p>Proof. We recall the following trigonometric identity</p><p>Let us substitute z = 2πk N for simplicity. We can thus rewrite the finite series as follows</p><p>9 Alternatively, we could think about the finite cosine series itself as the summation of N cosine terms on a circle with terms from 0 up to N -1 -scaled by k, which does not affect the result. The cosine terms then cancel out in a pair-wise fashion (or in triplets, depending on even or odd N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Statistics Under Fourier Transform</head><p>There are various ways to show how probability measures and moments propagated under frequency domain transforms. We showcase two additional proof methods based on change of variables or explicit computation for simple input distributions.</p><p>Lemma C.3 (Central moment preservation under unitary linear operators). Let x ∼ p x (x), x ∈ C and let T be a unitary linear operator. With X = T (x), it holds p X (X) = p x (T -1 (X))</p><p>Proof. The result follows immediately from the change of variables formula</p><p>Lemma C.4 (Variance preservation under unitary linear operators). Let x ∈ R N be a random vector with</p><p>Proof. Let x be real-valued input and distributed according to p Re(x) = N (0, σ 2 I) p Im(x) = δ(0).</p><p>Consider a single element of X</p><p>For clarity, we will treat the real part Re(X k ) first.</p><p>We observe that (a) the first central moment is preserved and (b) the variance term can be simplified as</p><p>where the variance again simplifies to</p><p>A similar argument can be developed using basic properties of circular-symmetry of complex Normals.</p><p>It is critical that the normalization factor 1 √ N be included in W in order to preserve the variance of V[X]. Indeed, normalization factors used in different conventions lead to different results</p><p>As N can easily be in the order of hundreds or thousands for generic signals, explosion of variance can be an issue if the orthogonalization factor 1 √ N is not applied to W .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discrete cosine transform</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="93" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural solvers for fast and accurate numerical optimal control</title>
		<author>
			<persName><forename type="first">F</forename><surname>Berto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
		<ptr target="https://www.wandb.com/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Software available from wandb.com</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compressing convolutional neural networks in the frequency domain</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1475" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast fourier convolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4479" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Characteristics of power spectra for regular and chaotic systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1481" to="1496" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goliński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03123</idno>
		<title level="m">Coin: Compression with implicit neural representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalarflow: a large-scale volumetric data set of realworld scalar transport flows for computer animation and machine learning</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Um</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pytorch lightning. GitHub</title>
		<author>
			<persName><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Character of Physical Law, with new foreword</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feynman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Competition and cooperation in neural nets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13587</idno>
		<title level="m">Adaptive fourier neural operators: Efficient token mixers for transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiwavelet-based operator learning for differential equations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bogdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Array programming with numpy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The era5 global reanalysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hersbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berrisford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hirahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horányi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muñoz-Sabater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schepers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">730</biblScope>
			<biblScope unit="page" from="1999" to="2049" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Perceiver io: A general architecture for structured inputs &amp; outputs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14795</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Openfoam: A c++ library for complex physics simulations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jasak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jemcov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tukovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on coupled methods in numerical dynamics</title>
		<imprint>
			<publisher>IUC Dubrovnik Croatia</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1000</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Physics-informed machine learning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Physics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="422" to="440" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Machine learningaccelerated computational fluid dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kochkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On universal approximation and error bounds for fourier neural operators</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2021">2021</date>
			<publisher>Art-No</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08895</idno>
		<title level="m">Fourier neural operator for parametric partial differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning the dynamics of physical systems from sparse observations with finite element networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lienen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08852</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PDE-net: Learning PDEs from data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v80/long18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03193</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A fast cosine transform in one and two dimensions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hyperverlet: A symplectic hypersolver for hamiltonian systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mathiesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5851</idno>
		<title level="m">Fast training of convolutional networks through ffts</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Discrete-time signal processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11214</idno>
		<title level="m">A global data-driven high-resolution weather model using adaptive fourier neural operators</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning mesh-based simulation with graph networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03409</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hypersolvers: Toward fast continuousdepth models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21105" to="21117" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07673</idno>
		<title level="m">Self-similarity priors: Neural collages as differentiable fractal representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fcnn: Fourier convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="786" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational physics</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spectral representations for convolutional neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7462" to="7473" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Noisy speech enhancement using discrete cosine transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Soon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Yeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fourier analysis: an introduction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shakarchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The discrete cosine transform</title>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="147" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
		<title level="m">Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning methods for reynolds-averaged navier-stokes simulations of airfoil flows</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weißenow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prantl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIAA Journal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13802</idno>
		<title level="m">Factorized fourier neural operators</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Approximation Theory and Approximation Practice, Extended Edition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Trefethen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Computer&quot; experiments&quot; on classical fluids. i. thermodynamical properties of lennardjones molecules</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards physics-informed deep learning for turbulent flow prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1457" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">U-fno-an enhanced fourier neural operator-based deep-learning model for multiphase flow</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Water Resources</title>
		<imprint>
			<biblScope unit="page">104180</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast transforms in image processing: compression, restoration, and resampling</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Yaroslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Electrical Engineering</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
