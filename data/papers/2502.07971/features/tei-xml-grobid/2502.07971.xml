<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RETREEVER: TREE-BASED COARSE-TO-FINE REPRESENTATIONS FOR RETRIEVAL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-11">11 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shubham</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ServiceNow Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Université Laval 4 Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<addrLine>2 6 5 11 12 23 24 25 26</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zichao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ServiceNow Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ServiceNow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cem</forename><surname>Subakan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université Laval 4 Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<addrLine>2 6 5 11 12 23 24 25 26</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ServiceNow Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ServiceNow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ServiceNow Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RETREEVER: TREE-BASED COARSE-TO-FINE REPRESENTATIONS FOR RETRIEVAL</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-11">11 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">D9E086625E342B49A4F66B3CC3E25E1E</idno>
					<idno type="arXiv">arXiv:2502.07971v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>WWE</term>
					<term>Defeat</term>
					<term>Championship</term>
					<term>Pre-show Drama</term>
					<term>Episode</term>
					<term>Season</term>
					<term>NBC</term>
					<term>Fox</term>
					<term>CBS WWE</term>
					<term>Basketball</term>
					<term>Television</term>
					<term>Animation Manga</term>
					<term>novel</term>
					<term>poem</term>
					<term>magazine Manga Comics TV fictional characters</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document retrieval is a core component of question-answering systems, as it enables conditioning answer generation on new and large-scale corpora. While effective, the standard practice of encoding documents into high-dimensional embeddings for similarity search entails large memory and compute footprints, and also makes it hard to inspect the inner workings of the system. In this paper, we propose a tree-based method for organizing and representing reference documents at various granular levels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus content and retrieval operations. Our method, called RETREEVER, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance. Our evaluations show that RETREEVER generally preserves full representation accuracy. Its hierarchical structure further provides strong coarse representations and enhances transparency by indirectly learning meaningful semantic groupings. Among hierarchical retrieval methods, RETREEVER achieves the best retrieval accuracy at the lowest latency, proving that this family of techniques can be viable in practical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information retrieval enables us to efficiently search for relevant information across millions of documents. With techniques like retrieval-augmented generation, document retrievers have become a key component in transforming large language models (LLMs) into tailored experts without the need for exhaustive fine-tuning <ref type="bibr" target="#b19">[Lewis et al., 2020</ref><ref type="bibr" target="#b12">, Izacard and Grave, 2021</ref><ref type="bibr" target="#b10">, Gao et al., 2023]</ref>. They enable LLM-generated content to be grounded in retrieved knowledge, thereby alleviating hallucinations <ref type="bibr" target="#b34">[Shuster et al., 2021]</ref>. Moreover, retrieval allows LLMs to scale to massive datasets without increasing their internal parameters.</p><p>A popular approach for document retrieval is to represent documents as high-dimensional embeddings and use nearestneighbor techniques to find relevant documents for a given query embedding <ref type="bibr" target="#b13">[Karpukhin et al., 2020]</ref>. While effective, the high dimensionality of these document embeddings results in a large memory footprint and heavy computation at inference time. The documents are also stored without any human-readable structure or understandable grouping, making it difficult to derive insights from the corpus or verify the retrieval process. In this paper, we propose RETREEVER, a tree-based method where documents are organized and represented at various granular levels, offering coarse-to-fine representations, learned entirely end-to-end by optimizing for retrieval performance as the learning objective. These representations offer the flexibility to balance cost and utility. Instead of training and storing a separate model for each desired representation size, a full-capacity tree can be learned, allowing excerpts to be encoded at any equal or lower level based on computational constraints during inference.</p><p>RETREEVER takes inspiration from the hierarchical retrieval literature, where tree organizers are built and navigated via calls to an LLM by cross-encoding queries and contexts <ref type="bibr" target="#b6">[Chen et al., 2023</ref><ref type="bibr" target="#b33">, Sarthi et al., 2024</ref><ref type="bibr" target="#b9">, Edge et al., 2024]</ref>. Such retrievers have the advantage of preserving the existing organization of the data, such as document divisions and entity relationships, and grouping documents semantically into a readable tree organization. However, their reliance on LLMs make them slow and expensive to train and run, hence impractical even for medium-size text corpora. In ; then their encodings are each given as input to the split nodes of the tree (here of depth D = 3) which output the probability of an embedding being routed left or right; all these scores are finally combined to output an assignment embedding of length the number of leaves, whose elements correspond to the probability of an excerpt reaching a certain leaf. At inference, the leaf assignments are used as fine representations, while assignments at intermediate levels as coarse representations, as they also provide valid distributions.</p><p>contrast, RETREEVER is designed to operate entirely on embeddings, eliminating the need for LLM calls during both construction and navigation.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a schematic of our approach. RETREEVER first converts reference document snippets into a embedding using a standard encoder, such as BGE <ref type="bibr" target="#b41">Xiao et al. [2024]</ref>, <ref type="bibr">BERT Devlin et al. [2019]</ref>, <ref type="bibr">ModernBERT Warner et al. [2024]</ref>, or LongFormer <ref type="bibr" target="#b2">Beltagy et al. [2020]</ref>. It then uses these representations to learn end-to-end a binary tree structure by optimizing directly the retrieval objective, making use of the query-context supervision available from most datasets. This is achieved by learning the parameters of a routing function at each internal node of the tree, such that positive query-context pairs are routed similarly through it.</p><p>RETREEVER is attractive not only for computational reasons but also for transparency. The learned hierarchical structure naturally provides an organization of the documents, which allows us to probe the tree to gain insights into the corpus content and retrieval operations. Practitioners can inspect different levels of the tree to identify key thematic clusters influencing retrieval or understand why certain branches lead to specific documents. While this is not a full causal explanation of query-context matching, it does ease the inspection of the model's inner workings. In Section 5, we analyze nodes at various levels of the hierarchy and show that documents in these nodes have thematic overlaps even though the tree is solely optimized for query-document similarity without any clustering objective. Our contributions are as follows:</p><p>1. We propose RETREEVER, a method for training coarse-to-fine representations, each corresponding to a level of a tree that is optimized for retrieval; unlike existing hierarchical retrievers, RETREEVER scales to large corpora and does not rely on LLM calls.</p><p>2. We evaluate RETREEVER's retrieval efficiency and accuracy, as compared to a variety of encoding and tree-based methods, and show that it preserves the accuracy of full representations, while offering strong coarse representations.</p><p>3. We illustrate how RETREEVER can be used to inspect the content of the corpus by leveraging its hierarchical structure, and show that it implicitly organizes documents into semantic groupings, which is a key step for making retrieval transparent and interpretable.</p><p>Figure <ref type="figure">2</ref>: Visualization of the topics (in bold) and keywords extracted from the contexts assigned to one subtree (in green) rooted at node 5 of a RETREEVER tree of depth 10 learned on NQ. For compactness, we represent only a subset of the nodes and paths, and stop at depth 5. Topics are locally coherent along a path, which indicates that RETREEVER naturally groups contexts semantically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Bi-encoders Retrieval models typically rely on sparse or dense vector embeddings to select the most relevant documents for a query, reducing the problem to a nearest-neighbor search. Dense retrieval models <ref type="bibr" target="#b13">Karpukhin et al. [2020]</ref>, <ref type="bibr" target="#b14">Khattab and Zaharia [2020]</ref> rely on encoders, neural networks designed to embed the semantic meaning of text into dense vector representations. State-of-the-art sentence embedding models often leverage bidirectional encoders, such as <ref type="bibr">BERT Devlin et al. [2019]</ref> and <ref type="bibr">BGE Xiao et al. [2024]</ref>), while others are built upon pretrained large language models, such as LLM2Vec BehnamGhader et al. <ref type="bibr">[2024]</ref>). Some encoders, such as BGE-large <ref type="bibr" target="#b41">Xiao et al. [2024]</ref>, are specifically designed for retrieval, offering a sentence embedding framework optimized for such tasks. Other notable examples are: DPR <ref type="bibr" target="#b13">Karpukhin et al. [2020]</ref>, which finetunes a BERT <ref type="bibr" target="#b7">Devlin et al. [2019]</ref> dual encoder to separately encode queries and documents into a single dense vector by contrastive learning; ColBERT <ref type="bibr" target="#b14">Khattab and Zaharia [2020]</ref>, <ref type="bibr" target="#b32">Santhanam et al. [2021]</ref>, which finetunes BERT to encode queries and documents into multiple dense vector representations and by matching individual token embeddings between queries and documents. Unlike sparse retrieval methods, such as BM25 or TF-IDF <ref type="bibr" target="#b31">Salton and Buckley [1988]</ref>, these dense representations are less interpretable, expensive to compute (generally because of attention operations) and take up a large amount of storage space. They do however perform better on many downstream tasks such as QA, specially for complex queries.</p><p>Hierarchical retrieval Hierarchical retrieval approaches aim to balance test-time efficiency and accuracy by organizing the retrieval process into multiple stages, typically involving coarse-to-fine filtering of candidate documents. Many hierarchical methods face challenges related to computational cost and performance trade-offs, and scale poorly with the size of the corpus. MemWalker <ref type="bibr" target="#b6">Chen et al. [2023]</ref> addresses LLM context limitations by structuring long texts into a hierarchical memory tree. It first segments the text into chunks, summarizing each into a node, which are then recursively merged into higher-level summaries until forming a root node. At query time, the LLM navigates this tree interactively, using iterative prompting to locate the most relevant segments, MemWalker <ref type="bibr" target="#b6">Chen et al. [2023]</ref> reframes retrieval in the context of addressing LLM context window limitations. It first segments long contexts into smaller chunks, summarizing each into hierarchical nodes that form a memory tree. At query time, the LLM traverses this tree using iterative prompting, efficiently locating the most relevant segments without exceeding its context limit.</p><p>Raptor <ref type="bibr" target="#b33">Sarthi et al. [2024]</ref> uses a clustering algorithm to group similar documents and then applies an LLM to recursively summarize and re-embed chunks of text, constructing a tree with differing levels of summarization from the bottom up, resulting in a structured, multi-layered tree representation of the original documents. GraphRAG <ref type="bibr" target="#b9">Edge et al. [2024]</ref> uses an LLM to build a graph-based text index by first deriving an entity knowledge graph from source documents and then pregenerating community summaries for related entities. For a given question, partial responses are generated from each community summary and combined into a final summarized response. While hierarchical retrieval methods improve response quality using LLMs and provides inspectable structures, they incur high computational costs and slow processing times, especially with large datasets. This trade-off makes them less suitable for real-time or resource-limited scenarios, emphasizing the need for more efficient solutions. RETREEVER overcomes these limitations by constructing and navigating a tree with no LLM calls.</p><p>Coarse-to-fine representations As compute and running time generally scale with the representation size, an effective solution to limit retrieval costs is through dimensionality reduction <ref type="bibr" target="#b37">[Van Der Maaten et al., 2009]</ref>. When the computational budget is not known in advance, the standard solution is to train multiple models or low-dimensional adapters not to incur into accuracy degradation, as one would by applying post-hoc compression techniques. However, this solution requires higher training and memory costs than learning and storing a single model. Single-model alternatives have been recently developed <ref type="bibr" target="#b21">[Yu et al., 2018</ref><ref type="bibr" target="#b5">, Cai et al., 2019</ref><ref type="bibr" target="#b17">, Kusupati et al., 2022]</ref>. In particular, <ref type="bibr" target="#b17">[Kusupati et al., 2022]</ref> propose Matryoshka Representation Learning (MRL), a simple framework that learns a nested representation. MRL boils down to learning an adaptive capacity embedding, ensuring that any first m-dimensions vector is as accurate as an independently trained m-dimensional one. This approach improves retrieval efficiency and flexibility, making it well-suited for diverse and evolving retrieval scenarios. Similarly, RETREEVER benefits from such advantages by training a nested representation, where each level input-to-node assignment corresponds to an embedding. Its structure and the learning of assignments additionally provide an organization of the documents into semantic groupings, allowing practitioners to inspect the corpus content and the inner workings of the retrieval system.</p><p>Differentiable trees and hierarchical indexes Because of their non-differentiable form, tree structures have been traditionally optimized by greedy algorithms, specialized for a particular objective, e.g. for classification, regression, or hierarchical clustering <ref type="bibr" target="#b28">[Quinlan, 1986</ref><ref type="bibr" target="#b16">, Krishnamurthy et al., 2012</ref><ref type="bibr" target="#b29">, Quinlan, 2014</ref><ref type="bibr" target="#b4">, Breiman, 2017</ref><ref type="bibr" target="#b26">, Moseley and Wang, 2023]</ref>. Recent literature has focused on differentiable formulations to take advantage of continuous optimization techniques for training trees on large datasets and for arbitrary objectives <ref type="bibr" target="#b11">[Irsoy et al., 2012</ref><ref type="bibr">, Yang et al., 2018a</ref><ref type="bibr" target="#b23">, Monath et al., 2019</ref><ref type="bibr" target="#b36">, Tanno et al., 2019</ref><ref type="bibr" target="#b45">, Zantedeschi et al., 2021</ref><ref type="bibr" target="#b22">, Marton et al., 2024]</ref>. We leverage this literature and extend it to learning binary trees that are optimal for the retrieval objective (via contrastive learning), and with complex split and propagation functions. Finally, trees and graphs have been used in the retrieval literature as indices for storing and quickly retrieving document embeddings via approximate similarity search, and not designed for being inspected <ref type="bibr" target="#b3">[Bernhardsson, 2017</ref><ref type="bibr" target="#b21">, Malkov and Yashunin, 2018</ref><ref type="bibr" target="#b8">, Douze et al., 2024]</ref>. RETREEVER does not belong to this line of works, as it is an embedding extractor that learns an inspectable tree to organize and represent documents at different granularity and into semantic groupings.</p><p>3 Proposed Method RETREEVER consists of (1) a frozen encoder E that returns embeddings for a given chunk of text, and (2) a learnable binary tree T that organizes encoded pieces of text into a hierarchy and routes queries to their relevant contexts (see Figure <ref type="figure" target="#fig_0">1</ref>). In this section, we describe how the tree hierarchy is learned by continuous optimization and how search is performed at test time. In what follows, we use the term context to refer to the text segments organized by RETREEVER.</p><p>Tree Formulation A perfect binary tree T is a binary tree where all levels are completely filled. The nodes t ∈ T of the tree satisfy the following two properties: (i) each node has either no children or exactly two, one left child and one right child; (ii) each node, except the unique root, has exactly one parent. There exist two types of tree nodes: branching (or split) nodes T B , which route their inputs to their left or right child, and leaf nodes (or simply leaves) T L , which have no children and constitute the terminal state of the tree T := T B ∪ T L .</p><p>Given positive query-context pairs {(q i , c i ) ∈ P} n i=1 where q i , c i are embedding pairs generated by the encoder E, our goal is to learn a perfect binary tree T of chosen depth D that assigns q i and c i to the same leaf node t l ∈ T L . Such leaf assignments denoted T (x i ) are obtained by routing a given input x i ∈ X (e.g., x i := q i the query embedding) sequentially through branching nodes until it reaches a specific leaf node in the tree. Specifically, each branching node t ∈ T B is parametrized by a split function s θt that routes an input to its left child node if a certain condition is met, or to its right one otherwise. We denote by z i ∈ {0, 1} |T | the route taken by the input, where z i,t equals 1 if the input has reached node t, and 0 otherwise. However, hard assignments would result in piece-wise constant functions with null gradients, making back-propagation ineffective. Therefore, we make use of probabilistic relaxations, inspired by works such as <ref type="bibr" target="#b11">Irsoy et al. [2012]</ref>, to make the tree learning problem differentiable, hence to allow the learning of the split parameters Θ := {θ t } T B t=1 jointly by gradient descent and so that they are optimal for retrieval. Such manipulations effectively relax the hard routes into soft ones z i ∈ [0, 1] |T | , where each element z i,t can now be interpreted as the probability of the input traversing node t (see Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Choice of Split Function A split function s θt : X → [0, 1] can be implemented in several ways, as long as it outputs a routing score, that determines the probability of routing an input left or right. Its simplest form is a linear projection, such as the one used in <ref type="bibr" target="#b45">Zantedeschi et al. [2021]</ref>: given a split node t, its left child t left and right child t right , the split function is defined as the dot product s θt (x i ) = θ t x T (interpreting θ t ∈ X as a hyper-plane). We experiment also with more complex functions, such as neural networks (see Appendix A.1).</p><p>In particular, we propose a cross-attention-based function <ref type="bibr" target="#b38">[Vaswani, 2017]</ref>, where learned embeddings e t ∈ R ne×demb for node t selectively attend to the input text x i ∈ R n d ×demb , which consist of n d embedded tokens (as extracted by the encoder). Recall that an attention mechanism is computed as: Attention</p><formula xml:id="formula_0">(Q, K, V) = softmax QK ⊤ √ d k V, where d k is</formula><p>the dimension of the Q and K matrices<ref type="foot" target="#foot_0">foot_0</ref> . We apply linear projections on e t to obtain the queries Q and on x i to obtain the keys K and values V, respectively. The projection matrices for the queries, keys, and values are shared across the entire tree.</p><p>The transformed node embeddings for each node are then combined via a node-specific aggregation function to obtain the final split node score. Various choices for this aggregation function are explored in Appendix A.2. The crossattention split function is generally more expressive than a linear projection (as our ablation shows in Appendix A.1) . Indeed, the node embeddings and projection matrices act as different memories that store information from past query and context embeddings, useful for scoring the current inputs. No matter the design of split function, the routing probabilities are finally computed as z tleft = σ(s θt (x i )) and z tright = 1 -σ(s θt (x i )), with σ(x) := 1 1+e -x the sigmoid function, to ensure a valid probability distribution.</p><p>Tree Propagation Note that any split function defined above outputs a score that is independent of the scores from the other nodes. However, the probability of reaching a node should be constrained by the probability of having reached its parent, the parent of its parent, and so on, to avoid degenerate trees where a descendant has higher probability of being reached than its ancestors. The simplest way of enforcing such tree constraints is by propagating the scores of the ancestors down to the node of interest by multiplying the probabilities along the path. We refer to this type of tree propagation as product propagation. Given a node t and its ancestors A t (the split nodes along the path from the root to t), the probability of reaching t is T (x i ) t = z t a∈At z a . Notice that the product propagation naturally enforces that the sum of node probabilities at any depth is always constant and equal to 1. Alternatively, one can learn how to propagate probability scores through the tree. We refer to this type of tree propagation as learned propagation and describe it in Appendix A.3. In either tree propagation, evaluating each split function sequentially, in order of depth, would unnecessarily slow down training and inference, in particular for trees of large depth. Equivalently, in our implementation we leverage parallel computing and the independence of split functions to evaluate all split functions in parallel, regardless of their depth, and then apply the selected tree propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training by Contrastive Learning</head><p>The task now is to learn end-to-end the parameters Θ (that include those of the split and propagation functions), so that any query is routed optimally to the leaf that contains its corresponding ground-truth context. Such an optimal assignment is achieved when positive query-context pairs are independently routed similarly through the tree, leading to similar leaf assignments. However, optimizing solely for perfect positive assignments is likely to lead to the trivial solution where all contexts and queries are routed to the same leaf, resulting in a collapsed tree with a single active path. To avoid such a solution, we define an objective that additionally encourages maximal use of the tree, by spreading unrelated contexts and queries across different tree leaves. Building on the contrastive learning literature <ref type="bibr" target="#b27">Oord et al. [2018]</ref>, <ref type="bibr" target="#b30">Radford et al. [2021]</ref>, we optimize the following Softmax-based InfoNCE loss,</p><formula xml:id="formula_1">- 1 2|P| |P| i=1 log e sim(qi,ci)</formula><p>|P| j=1 e sim(qi,cj )</p><p>+ log e sim(ci,qi)</p><p>|P| j=1 e sim(ci,qj )</p><p>(1)</p><p>where P is the set of query-context pairs, and we take the similarity sim :</p><formula xml:id="formula_2">[0, 1] |T L | × [0, 1] |T L | → R to be the negative Total Variation Distance (nTVD) between two leaf assignments: sim(a, b) = -1 2 |T L | l=1 |a l -b l |, where a = (a 1 , a 2 , . . . , a |T L | ) and b = (b 1 , b 2 , . . . , b |T L |</formula><p>) are the leaf assignment distributions, and |T L | is the number of leaf nodes in the tree. In Eq.( <ref type="formula">1</ref>), the left term encourages any query to have a leaf assignment similar to the assignment of its ground-truth context and different from any other context in the batch. Conversely, the second term encourages contexts to be routed similarly as their positive queries and differently from their negative ones.</p><p>Notice that learning node assignment probabilities, as opposed to unconstrained features (e.g., in MRL <ref type="bibr" target="#b17">[Kusupati et al., 2022]</ref>), naturally provides a hierarchical and soft clustering of the documents which can be inspected by the user. We show in Section 5 that learned clusterings capture semantic groupings, where assigned documents share topics and keywords, even though RETREEVER does not directly optimize for semantic coherence.</p><p>Coarse-to-Fine Representations Because of the hierarchical structure of the tree and its constraints, optimizing assignments at the leaf level, as expressed in Eq.( <ref type="formula">1</ref>), implicitly optimizes the assignments at intermediate levels, making them suitable coarse representations. To boost the retrieval performance of such coarse representations, we devise an optimization scheme where at each iteration a random level of the tree is selected and the constrastive loss is optimized w.r.t. its intermediate assignments. We call this scheme stochastic, as opposed to constant for when we optimize exclusively at the leaf level.</p><p>Indexing and Search Once the tree is trained, we index new content by encoding each context excerpt with the encoder E and then routing it through the tree T to obtain assignments for the chosen tree level. Such assignments can then be interpreted as a new representation of an excerpt, where each dimension represents the alignment between the excerpt and a learned semantic group. To retrieve related contexts for a query, we build a vector index based on these level assignments, process the query, and return its nearest neighbors based on the nTVD metric used at training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Retrieval Experiments</head><p>In this section, we assess the retrieval performance of RETREEVER at different representation levels and as compared to flat and hierarchical retrieval methods. Our results indicate that using the learned node assignments for retrieval (1) generally preserves the representation power of the encoder at the finer level, (2) results in more expressive embeddings at the coarsest levels and (3) strikes the best latency-accuracy trade-off among hierarchical retrieval methods.</p><p>Metrics and datasets To evaluate retrieval results, we measure the following standard metrics: RECALL@K, which assesses the proportion of ground-truth documents that are present within the top-k results returned by the retriever; Normalized Discounted Cumulative Gain at rank k (NDCG@K) which accounts for ground-truth ranking, as relevant documents appearing earlier are considered more valuable. We use four open-domain question-answering datasets for our experiments: NATURAL QUESTIONS (NQ) <ref type="bibr" target="#b18">[Kwiatkowski et al., 2019]</ref>, HOTPOTQA <ref type="bibr" target="#b43">[Yang et al., 2018b]</ref>, TOPIOCQA <ref type="bibr" target="#b0">[Adlakha et al., 2022]</ref>, and REPLIQA <ref type="bibr">Monteiro et al. [2024b]</ref>. A sample from our datasets consists of a query (natural-language question) and one ground-truth context. We follow the pre-processing done in <ref type="bibr">Monteiro et al. [2024a]</ref>, with the difference that for HOTPOTQA we concatenate all relevant contexts and discard irrelevant ones to obtain context. We make use of a validation set for model selection and hyperparameter tuning. For REPLIQA, we use the first split REPLIQA 0 for training and validation (10% of the split) and REPLIQA 1 for testing. For datasets with a validation set but not a test partition, we use the validation set for testing and create a validation set by holding out 10% of randomly selected training samples. Then, for training RETREEVER and the baselines we make use of the training query-context pairs, and for testing, we build the index with all the contexts from training, validation and test splits, if not specified otherwise. Baselines, Resources and Implementation Details We compare the retrieval results of RETREEVER with the following baselines:</p><p>BGE: we fine-tune BAAI/bge-large-en-v1.5 <ref type="bibr" target="#b40">Xiao et al. [2023]</ref> on each dataset by training a linear fully connected layer (of equal number of input and output units) using (1) as loss and the cosine similarity as similarity score. For the MRL version, we still use a linear adapter but modify the loss to incorporate the Matryoshka Representations-based training approach <ref type="bibr" target="#b17">[Kusupati et al., 2022]</ref> on top of the contrastive loss. This ensures that the learned representations can also be interpreted as a coarse-to-fine hierarchy, making the comparison more aligned. We refer to this version as BGE-MRL.</p><p>Hierarchical clustering: we perform a top-down hierarchical k-means (HIER-KMEANS) or GMM (HIER-GMM) clustering, where documents are iteratively partitioned into two clusters at each level of the hierarchy based on the cosine similarity of their embeddings as extracted by an encoder. This recursive process continues until the hierarchy reaches the same depth of RETREEVER. A similar procedure is applied for building the index: contexts are greedily routed through the learned tree and assigned to the leaf they reach. Further, we report results for two inference methods for retrieval. We apply tree search for both HIER-KMEANS and HIER-GMM, where instead of greedily traversing the tree (which would give poor performance), we compute a global search to find the leaf with the highest score: all split-node clustering models are evaluated, then their scores are propagated and aggregated top-down. Finally, the contexts assigned to the selected leaf are re-ranked based on the cosine similarity between query and context embeddings. Alternatively, we interpret the probability distribution of queries or contexts over all leaf nodes in HIER-GMM as a tree assignment representation, indexing and searching in the same manner as RETREEVER. This allows us to assess whether RETREEVER's tree-based representation power stems from our proposed method or if it naturally emerges from probabilistic hierarchical clustering.</p><p>RAPTOR <ref type="bibr" target="#b33">Sarthi et al. [2024]</ref>: we evaluate RAPTOR that recursively builds the tree bottom-up via LLM calls as described in the original paper. For a fair assessment of its retrieval accuracy, we then apply a similar tree search as for hierarchical clustering, by scoring nodes using the cosine similarity between query and summary embeddings. The contexts assigned to the selected leaf are then reranked again via the cosine similarity. Note that this is the only method we fit on the test corpus, as it cannot be applied to unseen documents and it does not scale to our training sets.</p><p>For all competing methods, including RETREEVER, we use the same BGE-Large model BAAI/bge-large-en-v1.5, which has a representation size of 1024. We truncate the input to 512 tokens. We train our model and BGE on a single NVIDIA H100 GPU with a batch size of 64, using AdamW <ref type="bibr" target="#b20">Loshchilov [2017]</ref> with a learning rate of 0.0004 for 200K steps and a 10K-step linear warmup. We set RETREEVER's depth to 10, which gives representations of size up to 1024.</p><p>For the split function, we utilize the cross-attention split and the tree-based score aggregation (see Appendix A.2 for definitions). The attention mechanism in our split function employs 8 heads, each with a dimensionality of 64, resulting in a total hidden dimension of 512. To propagate probability scores through the tree, we utilize a small 2-layer MLP with ReLU activation and dropout <ref type="bibr" target="#b35">Srivastava et al. [2014]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with Representation Learning Methods</head><p>For these experiments, all methods are evaluated using the flat index by <ref type="bibr" target="#b8">Douze et al. [2024]</ref>, with exact search for retrieving the top-k contexts and their rankings. These rankings are determined based on either the cosine similarity (BGE) or the nTVD (HIER-GMM and RETREEVER) between query and context representations. As nearest neighbor search dominates the inference running times, memory and latency of all methods scale linearly with the embedding dimensionality, hence their inference costs are similar for the same representation size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Representations</head><p>In Table <ref type="table" target="#tab_0">1</ref> we report recall@10 and NDCG@10 for all representation learning methods, using their full representations (at leaf level for RETREEVER and HIER-GMM). We first observe that BGE and RETREEVER have generally similar performance on all datasets, which suggests that learning a tree retains the encoder's representational power. To assess whether this is a general property of models learning soft clustering, we report the retrieval performance of HIER-GMM, using its node assignments as representations. The results indicate that the extracted representations are clearly not suited for retrieval purposes, stressing the need to directly optimize the retrieval objective and not a proxy one, as we do.</p><p>Adding constraints to the learning problem typically results in lower performance as compared to an unconstrained problem. To quantify the performance degradation due to our tree constraints, we also report the performance of probabilistic assignments as extracted by a flat model, that we name RETREEVER-NO TREE. The only difference with RETREEVER is that the tree structure and propagation are removed from the model. When lifting the tree constraints, we indeed observe a performance improvement on 5 out of 8 cases. However, such improvement comes at the price of losing the coarse-to-fine benefits of a tree structure.</p><p>Tables 2-3: Comparison of tree-based methods on two datasets by retrieval latency (Lat., in ms) and NDCG@10. RAPTOR has the advantage of having been fit on the test corpus and runs Out-Of-Time (24h limit) on the all-contexts setting.  We finally report results for the models trained with specialized procedures that encourage good coarse representations: BGE learned via the MRL (BGE-MRL) training objective and RETREEVER learned with the stochastic depth scheduler (RETREEVER-Stochastic). We remark a consistent deterioration of the learned full representations across methods and datasets (expect for BGE-MRL's NDCG@10 on REPLIQA), a phenomenon that we further analyze below.</p><p>Coarse-to-Fine Representations In Figure <ref type="figure" target="#fig_1">3</ref> we plot retrieval performance as a function of the representation size, to study the cost-utility trade-off. As coarse embeddings, we use the node assignments at level h for RETREEVER, with h ∈ {1, . . . , 10}, which gives representations of size 2 h . For BGE, we use the first 2 h dimensions of its output embeddings, as indicated in the MRL framework and in order to compare performance at the same representation size. We confirm the previous observation that coarse-to-fine learning techniques (MRL and Stochastic) generally improve the retrieval performance of coarser embeddings, although at the expense of the quality of the finer ones. Furthermore RETREEVER consistently provides the best results at the coarsest levels (for representation sizes up to 32), while being competitive or better than BGE-MRL for finer levels on NQ and HOTPOTQA. We observe the greatest performance degradation for RETREEVER's full representations on REPLIQA, where MRL surprisingly boosts BGE performance for any dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Tree-based Methods</head><p>In Tables <ref type="table" target="#tab_1">2</ref> and <ref type="table" target="#tab_2">3</ref> we compare retrieval performance and inference time of tree-based methods. For these experiments, we report results both using the full pool of contexts and only the test-split ones, as it would be extremely expensive to build and slow to run RAPTOR on the entire context corpus. Remarkably, RETREEVER achieves the best retrieval accuracy while being several order faster to run than other hierarchical retrieval methods. Its low latency is due to the use of parallel computing for traversing it. Unlike RETREEVER, the other methods need to evaluate the split nodes sequentially to obtain a leaf assignment, which significantly slows down inference. The second best performing method is HIER-KMEANS, although the gap with our model and its latency increase when testing with all contexts, which hints to poor scalability. Finally, RAPTOR fares the worst, despite its use of LLM calls for building the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Inspecting the Tree</head><p>By learning node assignment probabilities, RETREEVER has the appealing side-effect of providing an organization of reference documents, which serves as an interface for inspecting the model and analyzing the contexts. In this section, we probe RETREEVER's organization via several tools and report evidence that semantic groupings naturally emerge when optimizing for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tree Congruence</head><p>We first verify whether RETREEVER learns representations that are congruent with its tree structure,despite being trained solely on query-context alignment labels.</p><p>We begin by investigating whether the node embeddings of the cross-attention splits align with the tree structure, i.e., whether their similarity inversely correlates with their distance in the tree. In Figure <ref type="figure" target="#fig_3">4</ref>(left) we plot the average cosine similarity between the node embeddings of a node and its descendants as a function of their tree distance (expressed as their hop distance). Cosine similarity clearly decreases as the distance increases, demonstrating that the learned embeddings reflect the tree structure. See Appendix B.4 for an extended analysis.</p><p>Next, we assess whether semantically similar contexts are assigned to nearby nodes. To do so, we measure the average cosine similarity between all pairs of context embeddings (extracted with BGE-large) grouped by the depth of their  lowest common ancestor (LCA) -the lowest node in the tree that has both contexts as a descendant. The hypothesis is that contexts routed to different nodes in earlier layers (whose LCA is shallow and closer to the root) should be thematically more distinct than those split later in the tree. As shown in Figure <ref type="figure" target="#fig_3">4</ref>(right), the average context similarity increases with deeper LCAs, indicating that RETREEVER has learned a semantically coherent organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Topics and Keywords Coherence</head><p>To further inspect the learned structure in a human-understandable way, we resort to topic models for extracting and analyzing the distribution of topics and keywords of the contexts assigned to nodes across the tree. To better understand how RETREEVER organizes contexts, we present a visualization of the RETREEVER tree trained on NQ, highlighting the topics and keywords associated with several nodes (Figure <ref type="figure">2</ref>). For each node t, we collect all contexts assigned to the leaves of the subtree rooted at t and extract topics and keywords using the method from <ref type="bibr" target="#b15">Kilgarriff [2009]</ref>. A quick inspection of these keywords reveals the hierarchical structure reflects the semantic relationships of the contexts. For example, the node whose contexts are on the topic of media (node 5) has child nodes focused on publishing and TV. Furthermore, the path from node 5 to node 54 illustrates a consistent refinement of topics and keywords, progressing from media down to Television seasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Future Work</head><p>In this work, we introduced a retrieval system that generates tree-based representations optimized for retrieval. Our approach provides flexible control over the trade-off between representation size, inference latency, and retrieval accuracy. Unlike black-box models, RETREEVER allows us to inspect the model by examining how information is traversed and stored at different levels. A natural extension of this work is to learn the tree structure including the width and depth adaptively, or pruning the tree dynamically based on retrieval needs. Another direction is to develop tools for analyzing the hierarchical structure, such as a summary decoder that explains what a node represents based on its learned embeddings. An important challenge is adapting a learned tree to new tasks or datasets-whether certain subtrees can be updated or new ones grown while keeping the rest of the tree unchanged, or if full retraining is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. We however believe that contributing towards transparent and verifiable retrievers should benefit society in the long term, and is essential in fostering trust and accountability of retrieval systems. We compare the effect of different split functions on retrieval performance across various datasets in Figure <ref type="figure" target="#fig_5">6</ref>. Notably, the cross-attention split function achieves the best retrieval metrics across all datasets, while the MLP split function worse than the linear one. This surprising result can be explained by highlighting that both cross-attention and linear splits learn virtual embeddings (the node embeddings in the cross attention and the hyper-plane in the linear one) and compare them with the input. Further investigation would be required to confirm this speculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Cross-attention score aggregation</head><p>The cross-attention split function, described in Appendix A.1.3, uses node embeddings to compute a score for each node. This function consists of two components:</p><p>1. The cross-attention mechanism, which processes the input by attending to node embeddings as described above.</p><p>2. The node scorer, which outputs a single score for each node based on the output of the attention mechanism.</p><p>Here, we discuss different choices for the node scorer.</p><p>For a given sample, the output y of the cross-attention has the shape [n t , n e , d k ], where n t is the number of tree nodes, n e is the number of embeddings per node, and d k is the dimension of the attention output projection. Since we need a single score per tree node, y must be reduced to the shape [n t ]. This transformation can be performed in multiple ways, as detailed below:</p><p>• Mean then Linear: In this approach, we first aggregate all embeddings for a given node by performing mean pooling over the n e dimension, resulting in a single embedding per node. A shared linear layer is then applied to map this embedding into a score, producing a final score for each node.</p><p>• Per-Node Linear Map, then Mean of Scores: Instead of using a shared transformation, this method learns a separate linear function for each node to map its n e embeddings into individual scores. The final score for the node is then computed as the mean of these scores. This approach allows each node to focus on different aspects of its embeddings, leading to more flexible representations. This scoring is illustrated in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>• Incorporating Tree Structure: This method extends the previous approach by modifying the computed node scores to account for the hierarchical structure. Specifically, in addition to the score obtained per node, we refine these scores using a small MLP trained per node. This MLP takes as input the scores of the node and all of its ancestors, and outputs a modified score, allowing the scoring mechanism to incorporate hierarchical information learned by the tree.</p><p>In Figure <ref type="figure">7</ref>, we compare how these different scoring methods affect the retrieval performance of RETREEVER. We observe that tree-based methods consistently perform the best across most datasets, highlighting the importance of incorporating hierarchical structure into the scoring mechanism. In all the experiments reported in this paper, we make use of this aggregation method. A.3 Tree propagation.</p><p>Note that any split function defined above outputs a score that is independent of the scores from the other nodes. However, the probability of reaching a node should be constrained by the probability of having reached its parent, the parent of its parent, and so on. The simplest way of enforcing such tree constraints is by propagating the scores of the ancestors down to the node of interest by multiplying the probabilities along the path. We refer to this type of tree propagation as product propagation. Consider a node t, its left and right children t left and t right , and its ancestors A t (the split nodes along the path from the root to t). The probabilities of routing an input left or right based on node t's split score are:</p><formula xml:id="formula_3">z tleft (x i ) = σ(s θt (x i )) z tright (x i ) = 1 -σ(s θt (x i ))</formula><p>where σ() is the sigmoid function. Applying the sigmoid in this way ensures that the output probabilities of a split node always form a valid distribution (non-negative densities that sum to 1). Then, we constrain the probability of reaching node t left (and similarly node t right ) is defined as: Figure <ref type="figure">7</ref>: Effect of using various node score aggregation methods over different datasets. We label the "representation level" on x-axis in these plots -which is the depth of the tree form which these representations are taken. Hence a level k represents embeddings of size 2 k .</p><formula xml:id="formula_4">T (x i ) tleft = z tleft z t a∈At z a .<label>(7)</label></formula><p>Notice that the product propagation naturally enforces that the sum of node probabilities at any depth is always constant and equal to 1 (in particular, the sum of leaf assignments t∈T L T (x i ) t = 1).</p><p>Alternatively, one can learn how to propagate probability scores through the tree. We refer to this type of tree propagation as learned propagation. This enables a more expressive way of assigning probabilities to the leaf nodes compared to simply taking the product of probabilities along the path. More precisely, the probability of a leaf can be defined as the output of a leaf-specific learnable function that takes as input the routing probabilities of all its ancestors. The leaf scores are then normalized across all leaves to sum to 1, forming a valid probability distribution. This ensures that the probabilities assigned to a leaf are a meaningful and learnable function of the routing probabilities of the nodes along its path. Then, the probabilities of internal nodes can be computed in a bottom-up fashion. Specifically, the probability of reaching an internal node t is computed as the sum of the probabilities of its two children. This process starts from the second-to-last level and proceeds upward toward the root. Since the leaf probabilities form a valid distribution, this approach ensures that the probability distribution at any tree depth is also valid.</p><p>In Figure <ref type="figure" target="#fig_7">8</ref>, we compare the learned propagation mechanism with the product propagation mechanism described above. We find that the learned propagation method always yields better metrics at the last level. We use this tree propagation method in our main model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Evaluations</head><p>We report additional metrics and evaluations for better understanding RETREEVER's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Extended Evaluation of Representation Learning Methods</head><p>In Figures 9 to 11 we report NDCG@k and recall@k for k ∈ {1, 10, 100}. As noted elsewhere in our work, we observe here that models trained to optimize only the last layer perform best when evaluated at that level. However, introducing stochastic depth training to RETREEVER or training BGE with the MRL loss leads to significantly better representations at lower levels.</p><p>Among the two models trained to optimize all layers -RETREEVER-Stochastic-Depth and BGE-MRL -we find that the former consistently outperforms BGE-MRL at lower levels. Without exception, RETREEVER trained with stochastic depth provides the most effective representations at lower layers across all datasets on both Recall@k and NDCG@k metrics for all k values. Moreover, on datasets like NQ, RETREEVER-Stochastic-Depth outperforms other methods across nearly all representation levels and for all recall and NDCG metrics.</p><p>We conclude that RETREEVER trained with stochastic depth offers an effective balance between representation length, query latency, and retrieval performance across a variety of datasets. In our main results, we use bge-large as the base bi-encoder model. Here, we experiment with alternative choices for the base encoder. The results across various datasets are shown in Figure <ref type="figure" target="#fig_0">12</ref>. We evaluate three models: BERT, BGE-Large (marked as bge in the plots), and BGEM3. Our findings indicate that bge consistently performs best across all datasets, while BERT yields the lowest performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Depth of Trained Tree</head><p>We experiment with training trees of varying depths using stochastic depth training to ensure meaningful representations at every level of the tree. The results are shown in Figure <ref type="figure" target="#fig_1">13</ref>. We observe that increasing tree depth does not improve retrieval performance at intermediate levels. However, deeper trees yield better representations at the final level. Additionally, stochastic depth training leads to a trade-off: while it enhances intermediate representations, it results in slightly suboptimal performance at the deepest levels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Depth scheduler</head><p>Training a binary tree-based model provides the opportunity to fetch text representations not only from the leaf nodes but also from any intermediate level of the tree. This enables the retrieval of smaller representations which help speed up inference time. While any trained Retreever tree allows extracting intermediate representations, we can further explicitly optimize these intermediate layers during training by incorporating them into the loss function. We achieve this through depth schedulers, which regulate how different tree depths contribute to training.</p><p>We experiment with the following types of depth schedulers:</p><p>• Constant Depth Training: This is the standard training procedure for Retreever, where the loss function is computed only at the final layer of the tree. This approach trains representations at the deepest level but does not explicitly optimize for intermediate layers.</p><p>• Adaptive Depth Increase To enforce an inductive bias that encourages meaningful intermediate representations, we explicitly train on shallower layers as well. One way to achieve this is by gradually growing the tree depth during training. We experiment with two such depth increase schedules: linear and exponential.</p><p>• Stochastic Depth Training Unlike adaptive depth increase, which gradually expands the tree depth, stochastic depth training continuously trains all tree levels throughout the entire training process. In each training batch, we randomly select a tree depth, and during that batch, we train the tree truncated at that depth only. This ensures that all levels of the tree are trained throughout the training regime. Since training deeper layers is inherently harder than training shallower ones, we bias the depth selection probability to sample higher depths more frequently, ensuring sufficient training at deeper levels.</p><p>• Matryoshka Embedding Style Training: Inspired by Matryoshka Representations Learning <ref type="bibr" target="#b17">[Kusupati et al., 2022]</ref>, we adapt their training strategy to Retreever. Here, instead of computing a contrastive loss only at the final layer, we sum the contrastive losses from all layers, thus ensuring that all tree levels are explicitly trained during each training batch.</p><p>Figure <ref type="figure" target="#fig_3">14</ref> presents the results of this ablation study. We make the following observations: • The constant scheduler consistently achieves the best performance at the highest tree level, whereas other schedulers outperform it at lower levels. This is expected, as the constant scheduler does not explicitly optimize for intermediate layers.</p><p>• All alternative schedulers lag behind the constant scheduler in at least one dataset/metric combination. This is also expected, as introducing inductive biases to train lower levels can result in a suboptimal final layer compared to a model trained explicitly for the last level. • The exponential depth increase scheduler proves to be highly disruptive to model learning. In contrast, a gradual linear depth increase closely matches the performance of the constant variant at the highest level while providing better intermediate-level representations.</p><p>• Both Stochastic Depth Training and MRL consistently underperform compared to the constant scheduler at the highest level but significantly outperform it at lower levels. This occurs because both methods train all intermediate layers throughout the training regime, creating a tradeoff between optimizing only for the last level versus learning useful representations at all depths. • We observe that Stochastic Depth Training likely acts as a regularizer. This effect is particularly noticeable on the TOPIOCQA dataset, which is known to overfit. The fact that Stochastic Depth Training outperforms the constant scheduler at the highest level suggests that the added regularization helps in training a more robust RETREEVER. • While MRL and Stochastic Depth Training generally perform similarly, we note that Stochastic Depth Training achieves better results in certain cases, particularly in the REPLIQA and TOPIOCQA datasets.</p><p>Based on these findings, we use Stochastic Depth Training as the default schedule in our main model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.4 Cross-Attention Split Function Hyper Parameters</head><p>Here, we experiment with different hyperparameter choices for the cross-attention split function. Figure <ref type="figure" target="#fig_5">16</ref> analyzes the effect of the number of embeddings per node, while Figure <ref type="figure" target="#fig_4">15</ref> examines the impact of the number of attention heads.</p><p>We observe that using 8 heads yields the best performance across most datasets, which is the value we adopt for our main experiments. However, for TOPIOCQA, a much smaller value (such as 2) performs best. This aligns with our expectation, as we find this dataset to be more prone to overfitting. Figure <ref type="figure" target="#fig_3">14</ref>: Effect of using various depth schedulers with RETREEVER over different datasets. We label the "representation level" on x-axis in these plots -which is the depth of the tree form which these representations are taken. Hence a level k represents embeddings of size 2 k .</p><p>size grows. This trend is expected, as larger representations require more computation and memory access during retrieval. We observe, that the inference latency is linearly related to the size of the representation obtained from RETREEVER. We also note from the figure that its possible to obtain a significant decrease in query latency without sacrificing on retrieval accuracy by much.</p><p>Figure <ref type="figure" target="#fig_8">19</ref> further illustrates the trade-off between retrieval effectiveness, measured by NDCG@10, and query latency.</p><p>While larger representations significantly increase model latency, they offer only marginal improvements in retrieval performance. This provides a way to balance query latency and retrieval performance by selecting an appropriate embedding size from RETREEVER to achieve the desired trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Analyzing Node Embeddings</head><p>As observed earlier in Appendix A.1, the cross-attention split function is the most expressive among the different split functions we experimented with. We believe that learning dedicated node embeddings is crucial for effective retrieval, as they capture the semantic structure of the documents the model was trained on, allowing the model to route new text accordingly.</p><p>To analyze the structure of these embeddings, we conduct two experiments that examine how node embeddings relate to the hierarchical organization of the retrieval tree.</p><p>• Pairwise Cosine Similarity Between All Nodes: First, we analyze the overall structure of learned node embeddings by computing the pairwise cosine similarity between all tree nodes. Specifically, we measure the</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RETREEVER's traversal. At training, a positive query-context pair is first encoded by the frozen E (Bi-Encoder); then their encodings are each given as input to the split nodes of the tree (here of depth D = 3) which output the probability of an embedding being routed left or right; all these scores are finally combined to output an assignment embedding of length the number of leaves, whose elements correspond to the probability of an excerpt reaching a certain leaf. At inference, the leaf assignments are used as fine representations, while assignments at intermediate levels as coarse representations, as they also provide valid distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NDCG@10 (y-axis) as a function of the representation size (x-axis) for all four datasets. Metrics for other values of top-k are reported in the Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (left) Cosine similarity between node embeddings decreases with their pairwise distance in the tree. (right) Pairwise cosine similarity between context embeddings increases with the depth of LCA. The red lines indicate the average pairwise cosine similarity: between node embedding pairs on the left and between randomly selected context pairs on the right.</figDesc><graphic coords="9,343.44,77.52,196.56,148.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: RETREEVER's cross attention split function with node scoring done by a per node linear map followed by a mean of scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Effect of using various split functions on different datasets. The x-axis correspond to the representation level, i.e. the depth of the tree from which these representations are taken. Hence, a level k corresponds to embeddings of size 2 k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Effect of different tree propagation methods across datasets. The x-axis represents the "representation level," which corresponds to the tree depth from which embeddings are extracted. A level k indicates embeddings of size 2 k . prod prop refers to probability product-based propagation, as explained in Appendix A.3. learned prop represents a learned propagation method where node probabilities are determined by a node-specific network based on ancestor scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: NDCG@1 and recall@1 as a function of the representation size for four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: NDCG@10 and recall@10 as a function of the representation size for four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: NDCG@100 and recall@100 as a function of the representation size for four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Retrieval performance with full representations. Best results per metric and dataset are in bold. For HIER-GMM's results, the index is built exclusively with the test contexts.</figDesc><table><row><cell>MODEL</cell><cell>NQ</cell><cell></cell><cell cols="2">HOTPOTQA</cell><cell cols="2">TOPIOCQA</cell><cell cols="2">REPLIQA</cell></row><row><cell></cell><cell>RECALL@10</cell><cell>NDCG@10</cell><cell>RECALL@10</cell><cell>NDCG@10</cell><cell>RECALL@10</cell><cell>NDCG@10</cell><cell>RECALL@10</cell><cell>NDCG@10</cell></row><row><cell>BGE</cell><cell>0.7353</cell><cell>0.5139</cell><cell>0.9635</cell><cell>0.8940</cell><cell>0.3119</cell><cell>0.2169</cell><cell>0.9000</cell><cell>0.7123</cell></row><row><cell>HIER-GMM  *</cell><cell>0.0098</cell><cell>0.0048</cell><cell>0.0088</cell><cell>0.0044</cell><cell>0.0180</cell><cell>0.0084</cell><cell>0.0108</cell><cell>0.0050</cell></row><row><cell>RETREEVER</cell><cell>0.7824</cell><cell>0.5496</cell><cell>0.9185</cell><cell>0.8451</cell><cell>0.2804</cell><cell>0.1735</cell><cell>0.8940</cell><cell>0.7957</cell></row><row><cell>RETREEVER-NO TREE</cell><cell>0.7790</cell><cell>0.5443</cell><cell>0.9645</cell><cell>0.8946</cell><cell>0.3258</cell><cell>0.2247</cell><cell>0.9171</cell><cell>0.7854</cell></row><row><cell>BGE-MRL</cell><cell>0.6702</cell><cell>0.4561</cell><cell>0.9201</cell><cell>0.8194</cell><cell>0.3062</cell><cell>0.2125</cell><cell>0.8972</cell><cell>0.7750</cell></row><row><cell>RETREEVER-STOCHASTIC</cell><cell>0.7411</cell><cell>0.5185</cell><cell>0.8884</cell><cell>0.7973</cell><cell>0.2629</cell><cell>0.1703</cell><cell>0.8231</cell><cell>0.6974</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>NQ dataset</figDesc><table><row><cell>METHOD</cell><cell cols="2">TEST CONTEXTS</cell><cell cols="2">ALL CONTEXTS</cell></row><row><cell></cell><cell cols="4">LAT. NDCG@10 LAT. NDCG@10</cell></row><row><cell>HIER-KMEANS</cell><cell>293</cell><cell>0.7188</cell><cell>2910</cell><cell>0.3334</cell></row><row><cell>HIER-GMM</cell><cell>1531</cell><cell>0.1637</cell><cell>1536</cell><cell>0.0030</cell></row><row><cell>RAPTOR  *</cell><cell>266</cell><cell>0.1618</cell><cell>OOT</cell><cell>OOT</cell></row><row><cell>RETREEVER</cell><cell>20</cell><cell>0.7599</cell><cell>64</cell><cell>0.5496</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>REPLIQA dataset</figDesc><table><row><cell>METHOD</cell><cell cols="2">TEST CONTEXTS</cell><cell cols="2">ALL CONTEXTS</cell></row><row><cell></cell><cell cols="4">LAT. NDCG@10 LAT. NDCG@10</cell></row><row><cell>HIER-KMEANS</cell><cell>196</cell><cell>0.8254</cell><cell>2770</cell><cell>0.7348</cell></row><row><cell>HIER-GMM</cell><cell>1310</cell><cell>0.1004</cell><cell>1516</cell><cell>0.0851</cell></row><row><cell>RAPTOR  *</cell><cell>474</cell><cell>0.0678</cell><cell>OOT</cell><cell>OOT</cell></row><row><cell>RETREEVER</cell><cell>24</cell><cell>0.8329</cell><cell>33</cell><cell>0.7957</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This formulation describes a single-head attention mechanism, which can be naturally extended to multi-head attention by using multiple independent sets of projection matrices for queries, keys, and values, and then concatenating the outputs from all heads.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RETREEVER's Additional Design Options</head><p>RETREEVER can be instantiated in many different ways. In this section we report a selection of the design choices we explored in our experiments and that are implemented in our codebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Split Functions</head><p>A split function s θt : X → [0, 1] determines the routing probability of an input x i ∈ X through node t. The primary requirement for a split function is that it outputs a scalar ∈ R, based on which we compute the probabilities of routing the input to node t's left or right children t left and t right (see <ref type="bibr">Appendix A.3)</ref>. Below, we describe different types of split functions that can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Linear Split Function</head><p>The simplest form of a split function is a linear projection, similar to the approach used in <ref type="bibr" target="#b45">Zantedeschi et al. [2021]</ref>. For a given split node t, with left and right children t left and t right , the split function is defined as:</p><p>where θ t ∈ X represents a learnable hyperplane. Each split node learns a separate hyperplane, and there are no shared parameters across the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 MLP Split Function</head><p>A more expressive alternative is to use a learnable neural network, modeled as a Multi-Layer Perceptron (MLP). This MLP S Θ : X → R |T B | maps an input x i to a routing probability for each branching node in the tree, where T B is the set of branching nodes. The MLP consists of multiple layers with nonlinear activations, such as ReLU, and incorporates dropout for regularization. Unlike the linear split function, which maintains separate parameters per node, the MLP split function shares parameters across different nodes while still allowing for node-specific learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Cross-Attention Split Function</head><p>While the linear and MLP split functions operate on dense passage embeddings for the entire document, the crossattention split function allows us to leverage token-level embeddings for more expressive routing. This method utilizes a cross-attention mechanism between learnable node embeddings and text tokens to determine the routing probabilities at each split node.</p><p>Let the input text x i ∈ R n d ×demb consist of n d embedded tokens, encoded by the encoder E. Instead of a simple projection, we introduce learnable node embeddings e t ∈ R ne×d ′ emb for each node t. These node embeddings interact with the text tokens via a cross-attention mechanism, where the node embeddings serve as queries, and the text tokens provide keys and values.</p><p>We define the attention mechanism as follows:</p><p>where:</p><p>Here,</p><p>, and W v ∈ R d k ×demb are learnable projection matrices shared across the tree, while d k represents the dimension of the projected queries and keys. This formulation describes a single-head attention mechanism but can be naturally extended to multi-head attention by introducing independent projection matrices for each head and concatenating the resulting outputs.</p><p>The transformed node embeddings are then aggregated via a node-specific function to produce the final routing score. This aggregation is discussed in detail in Appendix A.2, and shown in Figure <ref type="figure">5</ref>. This mechanism significantly increases the expressivity of the split function compared to a simple linear projection. The node embeddings and projection Figure <ref type="figure">12</ref>: Effect of using various encoders as base encoders for RETREEVER over different datasets. We label the "representation level" on x-axis in these plots -which is the depth of the tree form which these representations are taken. Hence a level k represents embeddings of size 2 k .</p><p>Additionally, we did not observe a significant impact from varying the number of node embeddings across most datasets, except for TOPIOCQA, where using 3 embeddings per node led to the best results. For our main experiments, we chose to use a single embedding per node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.5 Regularization</head><p>We explore additional regularization techniques in the following ways:</p><p>• Dropout on Node Scores: We apply dropout to node scores before performing tree propagation to compute the final node probabilities. Figure <ref type="figure">17</ref> illustrates the effect of this regularization. We observe that it improves performance across all datasets, with the most significant impact on TOPIOCQA. Consequently, we incorporate this dropout in our main model.</p><p>• L1 Regularization: We experiment with adding an L1 regularization term to the loss function to encourage sparser query and context embeddings. However, we do not observe a significant impact on any dataset. As a result, we do not include this regularization in our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Retreever Latency vs Representation Size</head><p>We analyze the relationship between embedding size obtained from RETREEVER and its impact on inference latency and retrieval performance. As shown in Figure <ref type="figure">19</ref>, the seconds per query (sec/query) increase as the representation average similarity between node pairs as a function of their distance in the tree, where distance is defined as the length of the shortest path connecting two nodes. Our hypothesis is that nodes closer together in the tree should have more similar embeddings, while nodes farther apart should be more distinct. This is exactly what we observe in Figure <ref type="figure">20</ref>, where similarity decreases with increasing node distance, confirming that the learned embeddings reflect the hierarchical organization of the tree.</p><p>• Cosine Similarity Between Nodes and Their Descendants: In addition to pairwise similarity across all nodes, we specifically examine how embedding similarity evolves between a node and its direct descendants.</p><p>Since each node's embedding is expected to encode thematic information from past inputs, we hypothesize that embeddings should become progressively less similar as we move deeper in the tree. To test this, we compute the cosine similarity between a node and its descendants at varying depths, grouping values based on their ancestor-descendant distance. As shown in Figure <ref type="figure">20</ref>, similarity decreases with increasing distance, reinforcing the idea that the model learns hierarchical representations that respect the tree structure.</p><p>Together, these analyses provide strong evidence that the learned node embeddings preserve and make use of the hierarchical organization of the retrieval tree. Figure <ref type="figure">17</ref>: Effect of adding dropout on the node scores before applying tree based propagation. We label the "representation level" on x-axis in these plots -which is the depth of the tree form which these representations are taken. Hence a level k represents embeddings of size 2 k . Figure <ref type="figure">19</ref>: Trade-off between retrieval speed and effectiveness in Retreever. The left plot shows a linear dependence between the representation size from RETREEVER and the query latency, while the right plot demonstrates that a speedup in query inference with only a slight reduction in retrieval performance.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topiocqa: Open-domain conversational question answering with topic switching</title>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Vaibhav Adlakha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="468" to="483" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Harm de Vries, and Siva Reddy</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LLM2vec: Large language models are secretly powerful text encoders</title>
		<author>
			<persName><forename type="first">Parishad</forename><surname>Behnamghader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Adlakha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=IW1PR7vEBf" />
	</analytic>
	<monogr>
		<title level="m">First Conference on Language Modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.05150" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Annoy: approximate nearest neighbors in c++/python optimized for memory usage and loading/saving to disk</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Bernhardsson</surname></persName>
		</author>
		<ptr target="https://github.com/spotify/annoy" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="0" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Walking down the memory maze: Beyond context limit through interactive reading</title>
		<author>
			<persName><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05029</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandr</forename><surname>Guzhva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gergely</forename><surname>Szilvasy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The faiss library</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">From local to global: A graph rag approach to query-focused summarization</title>
		<author>
			<persName><forename type="first">Darren</forename><surname>Edge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ha</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newman</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Mody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Truitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Larson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.16130" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for large language models: A survey</title>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangxiang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10997</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Soft decision trees</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olcay</forename><surname>Taner Yıldız</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethem</forename><surname>Alpaydın</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on pattern recognition (ICPR2012)</title>
		<meeting>the 21st international conference on pattern recognition (ICPR2012)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1819" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.74</idno>
		<ptr target="https://aclanthology.org/2021.eacl-main.74" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<editor>
			<persName><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jorg</forename><surname>Tiedemann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</editor>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">April 2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.550" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<editor>
			<persName><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple maths for keywords</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Corpus Linguistics Conference 2009 (CL2009)</title>
		<meeting>the Corpus Linguistics Conference 2009 (CL2009)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient active algorithms for hierarchical clustering</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matryoshka representation learning</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gantavya</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Rege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wallingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Howard-Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="30233" to="30249" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Loshchilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradtree: Learning axis-aligned decision trees with gradient descent</title>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lüdtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bartelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="14323" to="14331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient-based hierarchical clustering using continuous representations of trees in hyperbolic space</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Monath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th acm sigkdd international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th acm sigkdd international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xc-cache: Cross-attending to cached context for efficient llm inference</title>
		<author>
			<persName><forename type="first">João</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Étienne</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-André</forename><surname>Noël</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RepliQA: A question-answering dataset for benchmarking LLMs on unseen reference content</title>
		<author>
			<persName><forename type="first">Joao</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Andre</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Étienne</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=4diKTLmg2y" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximation bounds for hierarchical clustering: Average linkage, bisecting k-means, and local search</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">C4. 5: programs for machine learning</title>
		<author>
			<persName><forename type="first">Quinlan</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="DOI">10.1016/0306-4573(88)90021-0</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/0306457388900210" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<idno type="ISSN">0306-4573</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Colbertv2: Effective and efficient retrieval via lightweight late interaction</title>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01488</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Parth</forename><surname>Sarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubh</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.18059</idno>
		<title level="m">Raptor: Recursive abstractive processing for tree-organized retrieval</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Retrieval augmentation reduces hallucination in conversation</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.320</idno>
		<ptr target="https://aclanthology.org/2021.findings-emnlp.320" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<editor>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Scott</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">-Tau</forename><surname>Yih</surname></persName>
		</editor>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="3784" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive neural trees</title>
		<author>
			<persName><forename type="first">Ryutaro</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Nori</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6166" to="6175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dimensionality reduction: a comparative</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaap</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="66" to="71" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Clavié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orion</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oskar</forename><surname>Hallström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Said</forename><surname>Taghadouini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Aarsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Griffin</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iacopo</forename><surname>Poli</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.13663" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">C-pack: Packaged resources to advance general chinese embedding</title>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peitian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">C-pack: Packed resources for general chinese embeddings</title>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peitian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<idno type="DOI">10.1145/3626772.3657878</idno>
		<ptr target="https://doi.org/10.1145/3626772.3657878" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;24</title>
		<meeting>the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;24<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">9798400704314</biblScope>
			<biblScope unit="page" from="641" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Garcia Morillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06988</idno>
		<title level="m">Deep neural decision trees</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<title level="m">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08928</idno>
		<title level="m">Slimmable neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning binary decision trees by argmin differentiation</title>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/zantedeschi21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
