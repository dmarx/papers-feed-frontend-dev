# RETREEVER: TREE-BASED COARSE-TO-FINE REPRESENTATIONS FOR RETRIEVAL

## Abstract

## 

Document retrieval is a core component of question-answering systems, as it enables conditioning answer generation on new and large-scale corpora. While effective, the standard practice of encoding documents into high-dimensional embeddings for similarity search entails large memory and compute footprints, and also makes it hard to inspect the inner workings of the system. In this paper, we propose a tree-based method for organizing and representing reference documents at various granular levels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus content and retrieval operations. Our method, called RETREEVER, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance. Our evaluations show that RETREEVER generally preserves full representation accuracy. Its hierarchical structure further provides strong coarse representations and enhances transparency by indirectly learning meaningful semantic groupings. Among hierarchical retrieval methods, RETREEVER achieves the best retrieval accuracy at the lowest latency, proving that this family of techniques can be viable in practical applications.

## Introduction

Information retrieval enables us to efficiently search for relevant information across millions of documents. With techniques like retrieval-augmented generation, document retrievers have become a key component in transforming large language models (LLMs) into tailored experts without the need for exhaustive fine-tuning [[Lewis et al., 2020](#b19)[, Izacard and Grave, 2021](#b12)[, Gao et al., 2023]](#b10). They enable LLM-generated content to be grounded in retrieved knowledge, thereby alleviating hallucinations [[Shuster et al., 2021]](#b34). Moreover, retrieval allows LLMs to scale to massive datasets without increasing their internal parameters.

A popular approach for document retrieval is to represent documents as high-dimensional embeddings and use nearestneighbor techniques to find relevant documents for a given query embedding [[Karpukhin et al., 2020]](#b13). While effective, the high dimensionality of these document embeddings results in a large memory footprint and heavy computation at inference time. The documents are also stored without any human-readable structure or understandable grouping, making it difficult to derive insights from the corpus or verify the retrieval process. In this paper, we propose RETREEVER, a tree-based method where documents are organized and represented at various granular levels, offering coarse-to-fine representations, learned entirely end-to-end by optimizing for retrieval performance as the learning objective. These representations offer the flexibility to balance cost and utility. Instead of training and storing a separate model for each desired representation size, a full-capacity tree can be learned, allowing excerpts to be encoded at any equal or lower level based on computational constraints during inference.

RETREEVER takes inspiration from the hierarchical retrieval literature, where tree organizers are built and navigated via calls to an LLM by cross-encoding queries and contexts [[Chen et al., 2023](#b6)[, Sarthi et al., 2024](#b33)[, Edge et al., 2024]](#b9). Such retrievers have the advantage of preserving the existing organization of the data, such as document divisions and entity relationships, and grouping documents semantically into a readable tree organization. However, their reliance on LLMs make them slow and expensive to train and run, hence impractical even for medium-size text corpora. In ; then their encodings are each given as input to the split nodes of the tree (here of depth D = 3) which output the probability of an embedding being routed left or right; all these scores are finally combined to output an assignment embedding of length the number of leaves, whose elements correspond to the probability of an excerpt reaching a certain leaf. At inference, the leaf assignments are used as fine representations, while assignments at intermediate levels as coarse representations, as they also provide valid distributions.

contrast, RETREEVER is designed to operate entirely on embeddings, eliminating the need for LLM calls during both construction and navigation.

Figure [1](#fig_0) shows a schematic of our approach. RETREEVER first converts reference document snippets into a embedding using a standard encoder, such as BGE [Xiao et al. [2024]](#b41), [BERT Devlin et al. [2019]](#), [ModernBERT Warner et al. [2024]](#), or LongFormer [Beltagy et al. [2020]](#b2). It then uses these representations to learn end-to-end a binary tree structure by optimizing directly the retrieval objective, making use of the query-context supervision available from most datasets. This is achieved by learning the parameters of a routing function at each internal node of the tree, such that positive query-context pairs are routed similarly through it.

RETREEVER is attractive not only for computational reasons but also for transparency. The learned hierarchical structure naturally provides an organization of the documents, which allows us to probe the tree to gain insights into the corpus content and retrieval operations. Practitioners can inspect different levels of the tree to identify key thematic clusters influencing retrieval or understand why certain branches lead to specific documents. While this is not a full causal explanation of query-context matching, it does ease the inspection of the model's inner workings. In Section 5, we analyze nodes at various levels of the hierarchy and show that documents in these nodes have thematic overlaps even though the tree is solely optimized for query-document similarity without any clustering objective. Our contributions are as follows:

1. We propose RETREEVER, a method for training coarse-to-fine representations, each corresponding to a level of a tree that is optimized for retrieval; unlike existing hierarchical retrievers, RETREEVER scales to large corpora and does not rely on LLM calls.

2. We evaluate RETREEVER's retrieval efficiency and accuracy, as compared to a variety of encoding and tree-based methods, and show that it preserves the accuracy of full representations, while offering strong coarse representations.

3. We illustrate how RETREEVER can be used to inspect the content of the corpus by leveraging its hierarchical structure, and show that it implicitly organizes documents into semantic groupings, which is a key step for making retrieval transparent and interpretable.

Figure [2](#): Visualization of the topics (in bold) and keywords extracted from the contexts assigned to one subtree (in green) rooted at node 5 of a RETREEVER tree of depth 10 learned on NQ. For compactness, we represent only a subset of the nodes and paths, and stop at depth 5. Topics are locally coherent along a path, which indicates that RETREEVER naturally groups contexts semantically.

## Related Work

Bi-encoders Retrieval models typically rely on sparse or dense vector embeddings to select the most relevant documents for a query, reducing the problem to a nearest-neighbor search. Dense retrieval models [Karpukhin et al. [2020]](#b13), [Khattab and Zaharia [2020]](#b14) rely on encoders, neural networks designed to embed the semantic meaning of text into dense vector representations. State-of-the-art sentence embedding models often leverage bidirectional encoders, such as [BERT Devlin et al. [2019]](#) and [BGE Xiao et al. [2024]](#)), while others are built upon pretrained large language models, such as LLM2Vec BehnamGhader et al. [[2024]](#)). Some encoders, such as BGE-large [Xiao et al. [2024]](#b41), are specifically designed for retrieval, offering a sentence embedding framework optimized for such tasks. Other notable examples are: DPR [Karpukhin et al. [2020]](#b13), which finetunes a BERT [Devlin et al. [2019]](#b7) dual encoder to separately encode queries and documents into a single dense vector by contrastive learning; ColBERT [Khattab and Zaharia [2020]](#b14), [Santhanam et al. [2021]](#b32), which finetunes BERT to encode queries and documents into multiple dense vector representations and by matching individual token embeddings between queries and documents. Unlike sparse retrieval methods, such as BM25 or TF-IDF [Salton and Buckley [1988]](#b31), these dense representations are less interpretable, expensive to compute (generally because of attention operations) and take up a large amount of storage space. They do however perform better on many downstream tasks such as QA, specially for complex queries.

Hierarchical retrieval Hierarchical retrieval approaches aim to balance test-time efficiency and accuracy by organizing the retrieval process into multiple stages, typically involving coarse-to-fine filtering of candidate documents. Many hierarchical methods face challenges related to computational cost and performance trade-offs, and scale poorly with the size of the corpus. MemWalker [Chen et al. [2023]](#b6) addresses LLM context limitations by structuring long texts into a hierarchical memory tree. It first segments the text into chunks, summarizing each into a node, which are then recursively merged into higher-level summaries until forming a root node. At query time, the LLM navigates this tree interactively, using iterative prompting to locate the most relevant segments, MemWalker [Chen et al. [2023]](#b6) reframes retrieval in the context of addressing LLM context window limitations. It first segments long contexts into smaller chunks, summarizing each into hierarchical nodes that form a memory tree. At query time, the LLM traverses this tree using iterative prompting, efficiently locating the most relevant segments without exceeding its context limit.

Raptor [Sarthi et al. [2024]](#b33) uses a clustering algorithm to group similar documents and then applies an LLM to recursively summarize and re-embed chunks of text, constructing a tree with differing levels of summarization from the bottom up, resulting in a structured, multi-layered tree representation of the original documents. GraphRAG [Edge et al. [2024]](#b9) uses an LLM to build a graph-based text index by first deriving an entity knowledge graph from source documents and then pregenerating community summaries for related entities. For a given question, partial responses are generated from each community summary and combined into a final summarized response. While hierarchical retrieval methods improve response quality using LLMs and provides inspectable structures, they incur high computational costs and slow processing times, especially with large datasets. This trade-off makes them less suitable for real-time or resource-limited scenarios, emphasizing the need for more efficient solutions. RETREEVER overcomes these limitations by constructing and navigating a tree with no LLM calls.

Coarse-to-fine representations As compute and running time generally scale with the representation size, an effective solution to limit retrieval costs is through dimensionality reduction [[Van Der Maaten et al., 2009]](#b37). When the computational budget is not known in advance, the standard solution is to train multiple models or low-dimensional adapters not to incur into accuracy degradation, as one would by applying post-hoc compression techniques. However, this solution requires higher training and memory costs than learning and storing a single model. Single-model alternatives have been recently developed [[Yu et al., 2018](#b21)[, Cai et al., 2019](#b5)[, Kusupati et al., 2022]](#b17). In particular, [[Kusupati et al., 2022]](#b17) propose Matryoshka Representation Learning (MRL), a simple framework that learns a nested representation. MRL boils down to learning an adaptive capacity embedding, ensuring that any first m-dimensions vector is as accurate as an independently trained m-dimensional one. This approach improves retrieval efficiency and flexibility, making it well-suited for diverse and evolving retrieval scenarios. Similarly, RETREEVER benefits from such advantages by training a nested representation, where each level input-to-node assignment corresponds to an embedding. Its structure and the learning of assignments additionally provide an organization of the documents into semantic groupings, allowing practitioners to inspect the corpus content and the inner workings of the retrieval system.

Differentiable trees and hierarchical indexes Because of their non-differentiable form, tree structures have been traditionally optimized by greedy algorithms, specialized for a particular objective, e.g. for classification, regression, or hierarchical clustering [[Quinlan, 1986](#b28)[, Krishnamurthy et al., 2012](#b16)[, Quinlan, 2014](#b29)[, Breiman, 2017](#b4)[, Moseley and Wang, 2023]](#b26). Recent literature has focused on differentiable formulations to take advantage of continuous optimization techniques for training trees on large datasets and for arbitrary objectives [[Irsoy et al., 2012](#b11)[, Yang et al., 2018a](#)[, Monath et al., 2019](#b23)[, Tanno et al., 2019](#b36)[, Zantedeschi et al., 2021](#b45)[, Marton et al., 2024]](#b22). We leverage this literature and extend it to learning binary trees that are optimal for the retrieval objective (via contrastive learning), and with complex split and propagation functions. Finally, trees and graphs have been used in the retrieval literature as indices for storing and quickly retrieving document embeddings via approximate similarity search, and not designed for being inspected [[Bernhardsson, 2017](#b3)[, Malkov and Yashunin, 2018](#b21)[, Douze et al., 2024]](#b8). RETREEVER does not belong to this line of works, as it is an embedding extractor that learns an inspectable tree to organize and represent documents at different granularity and into semantic groupings.

3 Proposed Method RETREEVER consists of (1) a frozen encoder E that returns embeddings for a given chunk of text, and (2) a learnable binary tree T that organizes encoded pieces of text into a hierarchy and routes queries to their relevant contexts (see Figure [1](#fig_0)). In this section, we describe how the tree hierarchy is learned by continuous optimization and how search is performed at test time. In what follows, we use the term context to refer to the text segments organized by RETREEVER.

Tree Formulation A perfect binary tree T is a binary tree where all levels are completely filled. The nodes t ∈ T of the tree satisfy the following two properties: (i) each node has either no children or exactly two, one left child and one right child; (ii) each node, except the unique root, has exactly one parent. There exist two types of tree nodes: branching (or split) nodes T B , which route their inputs to their left or right child, and leaf nodes (or simply leaves) T L , which have no children and constitute the terminal state of the tree T := T B ∪ T L .

Given positive query-context pairs {(q i , c i ) ∈ P} n i=1 where q i , c i are embedding pairs generated by the encoder E, our goal is to learn a perfect binary tree T of chosen depth D that assigns q i and c i to the same leaf node t l ∈ T L . Such leaf assignments denoted T (x i ) are obtained by routing a given input x i ∈ X (e.g., x i := q i the query embedding) sequentially through branching nodes until it reaches a specific leaf node in the tree. Specifically, each branching node t ∈ T B is parametrized by a split function s θt that routes an input to its left child node if a certain condition is met, or to its right one otherwise. We denote by z i ∈ {0, 1} |T | the route taken by the input, where z i,t equals 1 if the input has reached node t, and 0 otherwise. However, hard assignments would result in piece-wise constant functions with null gradients, making back-propagation ineffective. Therefore, we make use of probabilistic relaxations, inspired by works such as [Irsoy et al. [2012]](#b11), to make the tree learning problem differentiable, hence to allow the learning of the split parameters Θ := {θ t } T B t=1 jointly by gradient descent and so that they are optimal for retrieval. Such manipulations effectively relax the hard routes into soft ones z i ∈ [0, 1] |T | , where each element z i,t can now be interpreted as the probability of the input traversing node t (see Figure [1](#fig_0)).

Choice of Split Function A split function s θt : X → [0, 1] can be implemented in several ways, as long as it outputs a routing score, that determines the probability of routing an input left or right. Its simplest form is a linear projection, such as the one used in [Zantedeschi et al. [2021]](#b45): given a split node t, its left child t left and right child t right , the split function is defined as the dot product s θt (x i ) = θ t x T (interpreting θ t ∈ X as a hyper-plane). We experiment also with more complex functions, such as neural networks (see Appendix A.1).

In particular, we propose a cross-attention-based function [[Vaswani, 2017]](#b38), where learned embeddings e t ∈ R ne×demb for node t selectively attend to the input text x i ∈ R n d ×demb , which consist of n d embedded tokens (as extracted by the encoder). Recall that an attention mechanism is computed as: Attention

$(Q, K, V) = softmax QK ⊤ √ d k V, where d k is$the dimension of the Q and K matrices[foot_0](#foot_0) . We apply linear projections on e t to obtain the queries Q and on x i to obtain the keys K and values V, respectively. The projection matrices for the queries, keys, and values are shared across the entire tree.

The transformed node embeddings for each node are then combined via a node-specific aggregation function to obtain the final split node score. Various choices for this aggregation function are explored in Appendix A.2. The crossattention split function is generally more expressive than a linear projection (as our ablation shows in Appendix A.1) . Indeed, the node embeddings and projection matrices act as different memories that store information from past query and context embeddings, useful for scoring the current inputs. No matter the design of split function, the routing probabilities are finally computed as z tleft = σ(s θt (x i )) and z tright = 1 -σ(s θt (x i )), with σ(x) := 1 1+e -x the sigmoid function, to ensure a valid probability distribution.

Tree Propagation Note that any split function defined above outputs a score that is independent of the scores from the other nodes. However, the probability of reaching a node should be constrained by the probability of having reached its parent, the parent of its parent, and so on, to avoid degenerate trees where a descendant has higher probability of being reached than its ancestors. The simplest way of enforcing such tree constraints is by propagating the scores of the ancestors down to the node of interest by multiplying the probabilities along the path. We refer to this type of tree propagation as product propagation. Given a node t and its ancestors A t (the split nodes along the path from the root to t), the probability of reaching t is T (x i ) t = z t a∈At z a . Notice that the product propagation naturally enforces that the sum of node probabilities at any depth is always constant and equal to 1. Alternatively, one can learn how to propagate probability scores through the tree. We refer to this type of tree propagation as learned propagation and describe it in Appendix A.3. In either tree propagation, evaluating each split function sequentially, in order of depth, would unnecessarily slow down training and inference, in particular for trees of large depth. Equivalently, in our implementation we leverage parallel computing and the independence of split functions to evaluate all split functions in parallel, regardless of their depth, and then apply the selected tree propagation.

## Training by Contrastive Learning

The task now is to learn end-to-end the parameters Θ (that include those of the split and propagation functions), so that any query is routed optimally to the leaf that contains its corresponding ground-truth context. Such an optimal assignment is achieved when positive query-context pairs are independently routed similarly through the tree, leading to similar leaf assignments. However, optimizing solely for perfect positive assignments is likely to lead to the trivial solution where all contexts and queries are routed to the same leaf, resulting in a collapsed tree with a single active path. To avoid such a solution, we define an objective that additionally encourages maximal use of the tree, by spreading unrelated contexts and queries across different tree leaves. Building on the contrastive learning literature [Oord et al. [2018]](#b27), [Radford et al. [2021]](#b30), we optimize the following Softmax-based InfoNCE loss,

$- 1 2|P| |P| i=1 log e sim(qi,ci)$|P| j=1 e sim(qi,cj )

+ log e sim(ci,qi)

|P| j=1 e sim(ci,qj )

(1)

where P is the set of query-context pairs, and we take the similarity sim :

$[0, 1] |T L | × [0, 1] |T L | → R to be the negative Total Variation Distance (nTVD) between two leaf assignments: sim(a, b) = -1 2 |T L | l=1 |a l -b l |, where a = (a 1 , a 2 , . . . , a |T L | ) and b = (b 1 , b 2 , . . . , b |T L |$) are the leaf assignment distributions, and |T L | is the number of leaf nodes in the tree. In Eq.( [1](#)), the left term encourages any query to have a leaf assignment similar to the assignment of its ground-truth context and different from any other context in the batch. Conversely, the second term encourages contexts to be routed similarly as their positive queries and differently from their negative ones.

Notice that learning node assignment probabilities, as opposed to unconstrained features (e.g., in MRL [[Kusupati et al., 2022]](#b17)), naturally provides a hierarchical and soft clustering of the documents which can be inspected by the user. We show in Section 5 that learned clusterings capture semantic groupings, where assigned documents share topics and keywords, even though RETREEVER does not directly optimize for semantic coherence.

Coarse-to-Fine Representations Because of the hierarchical structure of the tree and its constraints, optimizing assignments at the leaf level, as expressed in Eq.( [1](#)), implicitly optimizes the assignments at intermediate levels, making them suitable coarse representations. To boost the retrieval performance of such coarse representations, we devise an optimization scheme where at each iteration a random level of the tree is selected and the constrastive loss is optimized w.r.t. its intermediate assignments. We call this scheme stochastic, as opposed to constant for when we optimize exclusively at the leaf level.

Indexing and Search Once the tree is trained, we index new content by encoding each context excerpt with the encoder E and then routing it through the tree T to obtain assignments for the chosen tree level. Such assignments can then be interpreted as a new representation of an excerpt, where each dimension represents the alignment between the excerpt and a learned semantic group. To retrieve related contexts for a query, we build a vector index based on these level assignments, process the query, and return its nearest neighbors based on the nTVD metric used at training.

## Retrieval Experiments

In this section, we assess the retrieval performance of RETREEVER at different representation levels and as compared to flat and hierarchical retrieval methods. Our results indicate that using the learned node assignments for retrieval (1) generally preserves the representation power of the encoder at the finer level, (2) results in more expressive embeddings at the coarsest levels and (3) strikes the best latency-accuracy trade-off among hierarchical retrieval methods.

Metrics and datasets To evaluate retrieval results, we measure the following standard metrics: RECALL@K, which assesses the proportion of ground-truth documents that are present within the top-k results returned by the retriever; Normalized Discounted Cumulative Gain at rank k (NDCG@K) which accounts for ground-truth ranking, as relevant documents appearing earlier are considered more valuable. We use four open-domain question-answering datasets for our experiments: NATURAL QUESTIONS (NQ) [[Kwiatkowski et al., 2019]](#b18), HOTPOTQA [[Yang et al., 2018b]](#b43), TOPIOCQA [[Adlakha et al., 2022]](#b0), and REPLIQA [Monteiro et al. [2024b]](#). A sample from our datasets consists of a query (natural-language question) and one ground-truth context. We follow the pre-processing done in [Monteiro et al. [2024a]](#), with the difference that for HOTPOTQA we concatenate all relevant contexts and discard irrelevant ones to obtain context. We make use of a validation set for model selection and hyperparameter tuning. For REPLIQA, we use the first split REPLIQA 0 for training and validation (10% of the split) and REPLIQA 1 for testing. For datasets with a validation set but not a test partition, we use the validation set for testing and create a validation set by holding out 10% of randomly selected training samples. Then, for training RETREEVER and the baselines we make use of the training query-context pairs, and for testing, we build the index with all the contexts from training, validation and test splits, if not specified otherwise. Baselines, Resources and Implementation Details We compare the retrieval results of RETREEVER with the following baselines:

BGE: we fine-tune BAAI/bge-large-en-v1.5 [Xiao et al. [2023]](#b40) on each dataset by training a linear fully connected layer (of equal number of input and output units) using (1) as loss and the cosine similarity as similarity score. For the MRL version, we still use a linear adapter but modify the loss to incorporate the Matryoshka Representations-based training approach [[Kusupati et al., 2022]](#b17) on top of the contrastive loss. This ensures that the learned representations can also be interpreted as a coarse-to-fine hierarchy, making the comparison more aligned. We refer to this version as BGE-MRL.

Hierarchical clustering: we perform a top-down hierarchical k-means (HIER-KMEANS) or GMM (HIER-GMM) clustering, where documents are iteratively partitioned into two clusters at each level of the hierarchy based on the cosine similarity of their embeddings as extracted by an encoder. This recursive process continues until the hierarchy reaches the same depth of RETREEVER. A similar procedure is applied for building the index: contexts are greedily routed through the learned tree and assigned to the leaf they reach. Further, we report results for two inference methods for retrieval. We apply tree search for both HIER-KMEANS and HIER-GMM, where instead of greedily traversing the tree (which would give poor performance), we compute a global search to find the leaf with the highest score: all split-node clustering models are evaluated, then their scores are propagated and aggregated top-down. Finally, the contexts assigned to the selected leaf are re-ranked based on the cosine similarity between query and context embeddings. Alternatively, we interpret the probability distribution of queries or contexts over all leaf nodes in HIER-GMM as a tree assignment representation, indexing and searching in the same manner as RETREEVER. This allows us to assess whether RETREEVER's tree-based representation power stems from our proposed method or if it naturally emerges from probabilistic hierarchical clustering.

RAPTOR [Sarthi et al. [2024]](#b33): we evaluate RAPTOR that recursively builds the tree bottom-up via LLM calls as described in the original paper. For a fair assessment of its retrieval accuracy, we then apply a similar tree search as for hierarchical clustering, by scoring nodes using the cosine similarity between query and summary embeddings. The contexts assigned to the selected leaf are then reranked again via the cosine similarity. Note that this is the only method we fit on the test corpus, as it cannot be applied to unseen documents and it does not scale to our training sets.

For all competing methods, including RETREEVER, we use the same BGE-Large model BAAI/bge-large-en-v1.5, which has a representation size of 1024. We truncate the input to 512 tokens. We train our model and BGE on a single NVIDIA H100 GPU with a batch size of 64, using AdamW [Loshchilov [2017]](#b20) with a learning rate of 0.0004 for 200K steps and a 10K-step linear warmup. We set RETREEVER's depth to 10, which gives representations of size up to 1024.

For the split function, we utilize the cross-attention split and the tree-based score aggregation (see Appendix A.2 for definitions). The attention mechanism in our split function employs 8 heads, each with a dimensionality of 64, resulting in a total hidden dimension of 512. To propagate probability scores through the tree, we utilize a small 2-layer MLP with ReLU activation and dropout [Srivastava et al. [2014]](#b35).

## Comparison with Representation Learning Methods

For these experiments, all methods are evaluated using the flat index by [Douze et al. [2024]](#b8), with exact search for retrieving the top-k contexts and their rankings. These rankings are determined based on either the cosine similarity (BGE) or the nTVD (HIER-GMM and RETREEVER) between query and context representations. As nearest neighbor search dominates the inference running times, memory and latency of all methods scale linearly with the embedding dimensionality, hence their inference costs are similar for the same representation size.

## Full Representations

In Table [1](#tab_0) we report recall@10 and NDCG@10 for all representation learning methods, using their full representations (at leaf level for RETREEVER and HIER-GMM). We first observe that BGE and RETREEVER have generally similar performance on all datasets, which suggests that learning a tree retains the encoder's representational power. To assess whether this is a general property of models learning soft clustering, we report the retrieval performance of HIER-GMM, using its node assignments as representations. The results indicate that the extracted representations are clearly not suited for retrieval purposes, stressing the need to directly optimize the retrieval objective and not a proxy one, as we do.

Adding constraints to the learning problem typically results in lower performance as compared to an unconstrained problem. To quantify the performance degradation due to our tree constraints, we also report the performance of probabilistic assignments as extracted by a flat model, that we name RETREEVER-NO TREE. The only difference with RETREEVER is that the tree structure and propagation are removed from the model. When lifting the tree constraints, we indeed observe a performance improvement on 5 out of 8 cases. However, such improvement comes at the price of losing the coarse-to-fine benefits of a tree structure.

Tables 2-3: Comparison of tree-based methods on two datasets by retrieval latency (Lat., in ms) and NDCG@10. RAPTOR has the advantage of having been fit on the test corpus and runs Out-Of-Time (24h limit) on the all-contexts setting.  We finally report results for the models trained with specialized procedures that encourage good coarse representations: BGE learned via the MRL (BGE-MRL) training objective and RETREEVER learned with the stochastic depth scheduler (RETREEVER-Stochastic). We remark a consistent deterioration of the learned full representations across methods and datasets (expect for BGE-MRL's NDCG@10 on REPLIQA), a phenomenon that we further analyze below.

Coarse-to-Fine Representations In Figure [3](#fig_1) we plot retrieval performance as a function of the representation size, to study the cost-utility trade-off. As coarse embeddings, we use the node assignments at level h for RETREEVER, with h ∈ {1, . . . , 10}, which gives representations of size 2 h . For BGE, we use the first 2 h dimensions of its output embeddings, as indicated in the MRL framework and in order to compare performance at the same representation size. We confirm the previous observation that coarse-to-fine learning techniques (MRL and Stochastic) generally improve the retrieval performance of coarser embeddings, although at the expense of the quality of the finer ones. Furthermore RETREEVER consistently provides the best results at the coarsest levels (for representation sizes up to 32), while being competitive or better than BGE-MRL for finer levels on NQ and HOTPOTQA. We observe the greatest performance degradation for RETREEVER's full representations on REPLIQA, where MRL surprisingly boosts BGE performance for any dimensionality.

## Comparison with Tree-based Methods

In Tables [2](#tab_1) and [3](#tab_2) we compare retrieval performance and inference time of tree-based methods. For these experiments, we report results both using the full pool of contexts and only the test-split ones, as it would be extremely expensive to build and slow to run RAPTOR on the entire context corpus. Remarkably, RETREEVER achieves the best retrieval accuracy while being several order faster to run than other hierarchical retrieval methods. Its low latency is due to the use of parallel computing for traversing it. Unlike RETREEVER, the other methods need to evaluate the split nodes sequentially to obtain a leaf assignment, which significantly slows down inference. The second best performing method is HIER-KMEANS, although the gap with our model and its latency increase when testing with all contexts, which hints to poor scalability. Finally, RAPTOR fares the worst, despite its use of LLM calls for building the tree.

## Inspecting the Tree

By learning node assignment probabilities, RETREEVER has the appealing side-effect of providing an organization of reference documents, which serves as an interface for inspecting the model and analyzing the contexts. In this section, we probe RETREEVER's organization via several tools and report evidence that semantic groupings naturally emerge when optimizing for retrieval.

## Tree Congruence

We first verify whether RETREEVER learns representations that are congruent with its tree structure,despite being trained solely on query-context alignment labels.

We begin by investigating whether the node embeddings of the cross-attention splits align with the tree structure, i.e., whether their similarity inversely correlates with their distance in the tree. In Figure [4](#fig_3)(left) we plot the average cosine similarity between the node embeddings of a node and its descendants as a function of their tree distance (expressed as their hop distance). Cosine similarity clearly decreases as the distance increases, demonstrating that the learned embeddings reflect the tree structure. See Appendix B.4 for an extended analysis.

Next, we assess whether semantically similar contexts are assigned to nearby nodes. To do so, we measure the average cosine similarity between all pairs of context embeddings (extracted with BGE-large) grouped by the depth of their  lowest common ancestor (LCA) -the lowest node in the tree that has both contexts as a descendant. The hypothesis is that contexts routed to different nodes in earlier layers (whose LCA is shallow and closer to the root) should be thematically more distinct than those split later in the tree. As shown in Figure [4](#fig_3)(right), the average context similarity increases with deeper LCAs, indicating that RETREEVER has learned a semantically coherent organization.

## Topics and Keywords Coherence

To further inspect the learned structure in a human-understandable way, we resort to topic models for extracting and analyzing the distribution of topics and keywords of the contexts assigned to nodes across the tree. To better understand how RETREEVER organizes contexts, we present a visualization of the RETREEVER tree trained on NQ, highlighting the topics and keywords associated with several nodes (Figure [2](#)). For each node t, we collect all contexts assigned to the leaves of the subtree rooted at t and extract topics and keywords using the method from [Kilgarriff [2009]](#b15). A quick inspection of these keywords reveals the hierarchical structure reflects the semantic relationships of the contexts. For example, the node whose contexts are on the topic of media (node 5) has child nodes focused on publishing and TV. Furthermore, the path from node 5 to node 54 illustrates a consistent refinement of topics and keywords, progressing from media down to Television seasons.

## Discussion and Future Work

In this work, we introduced a retrieval system that generates tree-based representations optimized for retrieval. Our approach provides flexible control over the trade-off between representation size, inference latency, and retrieval accuracy. Unlike black-box models, RETREEVER allows us to inspect the model by examining how information is traversed and stored at different levels. A natural extension of this work is to learn the tree structure including the width and depth adaptively, or pruning the tree dynamically based on retrieval needs. Another direction is to develop tools for analyzing the hierarchical structure, such as a summary decoder that explains what a node represents based on its learned embeddings. An important challenge is adapting a learned tree to new tasks or datasets-whether certain subtrees can be updated or new ones grown while keeping the rest of the tree unchanged, or if full retraining is necessary.

## Impact Statement

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. We however believe that contributing towards transparent and verifiable retrievers should benefit society in the long term, and is essential in fostering trust and accountability of retrieval systems. We compare the effect of different split functions on retrieval performance across various datasets in Figure [6](#fig_5). Notably, the cross-attention split function achieves the best retrieval metrics across all datasets, while the MLP split function worse than the linear one. This surprising result can be explained by highlighting that both cross-attention and linear splits learn virtual embeddings (the node embeddings in the cross attention and the hyper-plane in the linear one) and compare them with the input. Further investigation would be required to confirm this speculation.

## A.2 Cross-attention score aggregation

The cross-attention split function, described in Appendix A.1.3, uses node embeddings to compute a score for each node. This function consists of two components:

1. The cross-attention mechanism, which processes the input by attending to node embeddings as described above.

2. The node scorer, which outputs a single score for each node based on the output of the attention mechanism.

Here, we discuss different choices for the node scorer.

For a given sample, the output y of the cross-attention has the shape [n t , n e , d k ], where n t is the number of tree nodes, n e is the number of embeddings per node, and d k is the dimension of the attention output projection. Since we need a single score per tree node, y must be reduced to the shape [n t ]. This transformation can be performed in multiple ways, as detailed below:

• Mean then Linear: In this approach, we first aggregate all embeddings for a given node by performing mean pooling over the n e dimension, resulting in a single embedding per node. A shared linear layer is then applied to map this embedding into a score, producing a final score for each node.

• Per-Node Linear Map, then Mean of Scores: Instead of using a shared transformation, this method learns a separate linear function for each node to map its n e embeddings into individual scores. The final score for the node is then computed as the mean of these scores. This approach allows each node to focus on different aspects of its embeddings, leading to more flexible representations. This scoring is illustrated in Figure [5](#fig_4).

• Incorporating Tree Structure: This method extends the previous approach by modifying the computed node scores to account for the hierarchical structure. Specifically, in addition to the score obtained per node, we refine these scores using a small MLP trained per node. This MLP takes as input the scores of the node and all of its ancestors, and outputs a modified score, allowing the scoring mechanism to incorporate hierarchical information learned by the tree.

In Figure [7](#), we compare how these different scoring methods affect the retrieval performance of RETREEVER. We observe that tree-based methods consistently perform the best across most datasets, highlighting the importance of incorporating hierarchical structure into the scoring mechanism. In all the experiments reported in this paper, we make use of this aggregation method. A.3 Tree propagation.

Note that any split function defined above outputs a score that is independent of the scores from the other nodes. However, the probability of reaching a node should be constrained by the probability of having reached its parent, the parent of its parent, and so on. The simplest way of enforcing such tree constraints is by propagating the scores of the ancestors down to the node of interest by multiplying the probabilities along the path. We refer to this type of tree propagation as product propagation. Consider a node t, its left and right children t left and t right , and its ancestors A t (the split nodes along the path from the root to t). The probabilities of routing an input left or right based on node t's split score are:

$z tleft (x i ) = σ(s θt (x i )) z tright (x i ) = 1 -σ(s θt (x i ))$where σ() is the sigmoid function. Applying the sigmoid in this way ensures that the output probabilities of a split node always form a valid distribution (non-negative densities that sum to 1). Then, we constrain the probability of reaching node t left (and similarly node t right ) is defined as: Figure [7](#): Effect of using various node score aggregation methods over different datasets. We label the "representation level" on x-axis in these plots -which is the depth of the tree form which these representations are taken. Hence a level k represents embeddings of size 2 k .

$T (x i ) tleft = z tleft z t a∈At z a .(7)$Notice that the product propagation naturally enforces that the sum of node probabilities at any depth is always constant and equal to 1 (in particular, the sum of leaf assignments t∈T L T (x i ) t = 1).

Alternatively, one can learn how to propagate probability scores through the tree. We refer to this type of tree propagation as learned propagation. This enables a more expressive way of assigning probabilities to the leaf nodes compared to simply taking the product of probabilities along the path. More precisely, the probability of a leaf can be defined as the output of a leaf-specific learnable function that takes as input the routing probabilities of all its ancestors. The leaf scores are then normalized across all leaves to sum to 1, forming a valid probability distribution. This ensures that the probabilities assigned to a leaf are a meaningful and learnable function of the routing probabilities of the nodes along its path. Then, the probabilities of internal nodes can be computed in a bottom-up fashion. Specifically, the probability of reaching an internal node t is computed as the sum of the probabilities of its two children. This process starts from the second-to-last level and proceeds upward toward the root. Since the leaf probabilities form a valid distribution, this approach ensures that the probability distribution at any tree depth is also valid.

In Figure [8](#fig_7), we compare the learned propagation mechanism with the product propagation mechanism described above. We find that the learned propagation method always yields better metrics at the last level. We use this tree propagation method in our main model.

## B Additional Evaluations

We report additional metrics and evaluations for better understanding RETREEVER's performance.

## B.1 Extended Evaluation of Representation Learning Methods

In Figures 9 to 11 we report NDCG@k and recall@k for k ∈ {1, 10, 100}. As noted elsewhere in our work, we observe here that models trained to optimize only the last layer perform best when evaluated at that level. However, introducing stochastic depth training to RETREEVER or training BGE with the MRL loss leads to significantly better representations at lower levels.

Among the two models trained to optimize all layers -RETREEVER-Stochastic-Depth and BGE-MRL -we find that the former consistently outperforms BGE-MRL at lower levels. Without exception, RETREEVER trained with stochastic depth provides the most effective representations at lower layers across all datasets on both Recall@k and NDCG@k metrics for all k values. Moreover, on datasets like NQ, RETREEVER-Stochastic-Depth outperforms other methods across nearly all representation levels and for all recall and NDCG metrics.

We conclude that RETREEVER trained with stochastic depth offers an effective balance between representation length, query latency, and retrieval performance across a variety of datasets. In our main results, we use bge-large as the base bi-encoder model. Here, we experiment with alternative choices for the base encoder. The results across various datasets are shown in Figure [12](#fig_0). We evaluate three models: BERT, BGE-Large (marked as bge in the plots), and BGEM3. Our findings indicate that bge consistently performs best across all datasets, while BERT yields the lowest performance.

## B.2.2 Depth of Trained Tree

We experiment with training trees of varying depths using stochastic depth training to ensure meaningful representations at every level of the tree. The results are shown in Figure [13](#fig_1). We observe that increasing tree depth does not improve retrieval performance at intermediate levels. However, deeper trees yield better representations at the final level. Additionally, stochastic depth training leads to a trade-off: while it enhances intermediate representations, it results in slightly suboptimal performance at the deepest levels. 

## B.2.3 Depth scheduler

Training a binary tree-based model provides the opportunity to fetch text representations not only from the leaf nodes but also from any intermediate level of the tree. This enables the retrieval of smaller representations which help speed up inference time. While any trained Retreever tree allows extracting intermediate representations, we can further explicitly optimize these intermediate layers during training by incorporating them into the loss function. We achieve this through depth schedulers, which regulate how different tree depths contribute to training.

We experiment with the following types of depth schedulers:

• Constant Depth Training: This is the standard training procedure for Retreever, where the loss function is computed only at the final layer of the tree. This approach trains representations at the deepest level but does not explicitly optimize for intermediate layers.

• Adaptive Depth Increase To enforce an inductive bias that encourages meaningful intermediate representations, we explicitly train on shallower layers as well. One way to achieve this is by gradually growing the tree depth during training. We experiment with two such depth increase schedules: linear and exponential.

• Stochastic Depth Training Unlike adaptive depth increase, which gradually expands the tree depth, stochastic depth training continuously trains all tree levels throughout the entire training process. In each training batch, we randomly select a tree depth, and during that batch, we train the tree truncated at that depth only. This ensures that all levels of the tree are trained throughout the training regime. Since training deeper layers is inherently harder than training shallower ones, we bias the depth selection probability to sample higher depths more frequently, ensuring sufficient training at deeper levels.

• Matryoshka Embedding Style Training: Inspired by Matryoshka Representations Learning [[Kusupati et al., 2022]](#b17), we adapt their training strategy to Retreever. Here, instead of computing a contrastive loss only at the final layer, we sum the contrastive losses from all layers, thus ensuring that all tree levels are explicitly trained during each training batch.

Figure [14](#fig_3) presents the results of this ablation study. We make the following observations: • The constant scheduler consistently achieves the best performance at the highest tree level, whereas other schedulers outperform it at lower levels. This is expected, as the constant scheduler does not explicitly optimize for intermediate layers.

• All alternative schedulers lag behind the constant scheduler in at least one dataset/metric combination. This is also expected, as introducing inductive biases to train lower levels can result in a suboptimal final layer compared to a model trained explicitly for the last level. • The exponential depth increase scheduler proves to be highly disruptive to model learning. In contrast, a gradual linear depth increase closely matches the performance of the constant variant at the highest level while providing better intermediate-level representations.

• Both Stochastic Depth Training and MRL consistently underperform compared to the constant scheduler at the highest level but significantly outperform it at lower levels. This occurs because both methods train all intermediate layers throughout the training regime, creating a tradeoff between optimizing only for the last level versus learning useful representations at all depths. • We observe that Stochastic Depth Training likely acts as a regularizer. This effect is particularly noticeable on the TOPIOCQA dataset, which is known to overfit. The fact that Stochastic Depth Training outperforms the constant scheduler at the highest level suggests that the added regularization helps in training a more robust RETREEVER. • While MRL and Stochastic Depth Training generally perform similarly, we note that Stochastic Depth Training achieves better results in certain cases, particularly in the REPLIQA and TOPIOCQA datasets.

Based on these findings, we use Stochastic Depth Training as the default schedule in our main model.

## B.2.4 Cross-Attention Split Function Hyper Parameters

Here, we experiment with different hyperparameter choices for the cross-attention split function. Figure [16](#fig_5) analyzes the effect of the number of embeddings per node, while Figure [15](#fig_4) examines the impact of the number of attention heads.

We observe that using 8 heads yields the best performance across most datasets, which is the value we adopt for our main experiments. However, for TOPIOCQA, a much smaller value (such as 2) performs best. This aligns with our expectation, as we find this dataset to be more prone to overfitting. Figure [14](#fig_3): Effect of using various depth schedulers with RETREEVER over different datasets. We label the "representation level" on x-axis in these plots -which is the depth of the tree form which these representations are taken. Hence a level k represents embeddings of size 2 k .

size grows. This trend is expected, as larger representations require more computation and memory access during retrieval. We observe, that the inference latency is linearly related to the size of the representation obtained from RETREEVER. We also note from the figure that its possible to obtain a significant decrease in query latency without sacrificing on retrieval accuracy by much.

Figure [19](#fig_8) further illustrates the trade-off between retrieval effectiveness, measured by NDCG@10, and query latency.

While larger representations significantly increase model latency, they offer only marginal improvements in retrieval performance. This provides a way to balance query latency and retrieval performance by selecting an appropriate embedding size from RETREEVER to achieve the desired trade-off.

## B.4 Analyzing Node Embeddings

As observed earlier in Appendix A.1, the cross-attention split function is the most expressive among the different split functions we experimented with. We believe that learning dedicated node embeddings is crucial for effective retrieval, as they capture the semantic structure of the documents the model was trained on, allowing the model to route new text accordingly.

To analyze the structure of these embeddings, we conduct two experiments that examine how node embeddings relate to the hierarchical organization of the retrieval tree.

• Pairwise Cosine Similarity Between All Nodes: First, we analyze the overall structure of learned node embeddings by computing the pairwise cosine similarity between all tree nodes. Specifically, we measure the

![Figure 1: RETREEVER's traversal. At training, a positive query-context pair is first encoded by the frozen E (Bi-Encoder); then their encodings are each given as input to the split nodes of the tree (here of depth D = 3) which output the probability of an embedding being routed left or right; all these scores are finally combined to output an assignment embedding of length the number of leaves, whose elements correspond to the probability of an excerpt reaching a certain leaf. At inference, the leaf assignments are used as fine representations, while assignments at intermediate levels as coarse representations, as they also provide valid distributions.]()

![Figure 3: NDCG@10 (y-axis) as a function of the representation size (x-axis) for all four datasets. Metrics for other values of top-k are reported in the Appendix B.]()

![Figure 4: (left) Cosine similarity between node embeddings decreases with their pairwise distance in the tree. (right) Pairwise cosine similarity between context embeddings increases with the depth of LCA. The red lines indicate the average pairwise cosine similarity: between node embedding pairs on the left and between randomly selected context pairs on the right.]()

![Figure 5: RETREEVER's cross attention split function with node scoring done by a per node linear map followed by a mean of scores.]()

![Figure6: Effect of using various split functions on different datasets. The x-axis correspond to the representation level, i.e. the depth of the tree from which these representations are taken. Hence, a level k corresponds to embeddings of size 2 k .]()

![Figure8: Effect of different tree propagation methods across datasets. The x-axis represents the "representation level," which corresponds to the tree depth from which embeddings are extracted. A level k indicates embeddings of size 2 k . prod prop refers to probability product-based propagation, as explained in Appendix A.3. learned prop represents a learned propagation method where node probabilities are determined by a node-specific network based on ancestor scores.]()

![Figure9: NDCG@1 and recall@1 as a function of the representation size for four datasets.]()

![Figure10: NDCG@10 and recall@10 as a function of the representation size for four datasets.]()

![Figure11: NDCG@100 and recall@100 as a function of the representation size for four datasets.]()

![Retrieval performance with full representations. Best results per metric and dataset are in bold. For HIER-GMM's results, the index is built exclusively with the test contexts.]()

![NQ dataset]()

![REPLIQA dataset]()

This formulation describes a single-head attention mechanism, which can be naturally extended to multi-head attention by using multiple independent sets of projection matrices for queries, keys, and values, and then concatenating the outputs from all heads.

