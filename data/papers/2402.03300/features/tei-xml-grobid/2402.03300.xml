<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-04-27">27 Apr 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
							<email>zhihongshao@deepseek.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
							<email>wangpeiyi@deepseek.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Bi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haowei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingchuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
							<email>guoday@deepseek.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepSeek-AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-04-27">27 Apr 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">9DCF13B2C16B347B3EE21848EAABC312</idno>
					<idno type="arXiv">arXiv:2402.03300v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pretraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b16">(Hendrycks et al., 2021)</ref> <p>without the use of external toolkits and voting techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large language models (LLM) have revolutionized the approach to mathematical reasoning in artificial intelligence, spurring significant advancements in both the quantitative reasoning benchmark <ref type="bibr" target="#b16">(Hendrycks et al., 2021)</ref> and the geometry reasoning benchmark <ref type="bibr" target="#b45">(Trinh et al., 2024)</ref>. Moreover, these models have proven instrumental in assisting humans in solving complex mathematical problems <ref type="bibr" target="#b43">(Tao, 2023)</ref>. However, cutting-edge models such as <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> and Gemini-Ultra <ref type="bibr">(Anil et al., 2023)</ref> are not publicly available, and the currently accessible open-source models considerably trail behind in performance.</p><p>In this study, we introduce DeepSeekMath, a domain-specific language model that significantly outperforms the mathematical capabilities of open-source models and approaches the performance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeek-Math Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This dataset is extracted from the Common Crawl (CC) using a fastText-based classifier <ref type="bibr" target="#b20">(Joulin et al., 2016)</ref>. In the initial iteration, the classifier is trained using instances from OpenWebMath <ref type="bibr" target="#b2">(Paster et al., 2023)</ref> as positive examples, while incorporating a diverse selection of other web pages to serve as negative examples. Subsequently, we employ the classifier to mine additional positive instances from the CC, which are further refined through human annotation. The classifier is then updated with this enhanced dataset to improve its performance. The evaluation results indicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base 7B achieves 64.2% on GSM8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref> and 36.2% on the competition-level MATH dataset <ref type="bibr" target="#b16">(Hendrycks et al., 2021)</ref>, outperforming Minerva 540B <ref type="bibr">(Lewkowycz et al., 2022a)</ref>. In addition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese mathematical benchmarks <ref type="bibr" target="#b50">(Wei et al., 2023;</ref><ref type="bibr">Zhong et al., 2023)</ref>. We believe that our experience in mathematical data processing is a starting point for the research community, and there is significant room for improvement in the future.</p><p>DeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B <ref type="bibr" target="#b14">(Guo et al., 2024)</ref>, as we notice that starting from a code training model is a better choice compared to a general LLM. Furthermore, we observe the math training also improves model capability on MMLU <ref type="bibr" target="#b15">(Hendrycks et al., 2020)</ref> and BBH benchmarks <ref type="bibr" target="#b42">(Suzgun et al., 2022)</ref>, indicating it does not only enhance the model's mathematical abilities but also amplifies general reasoning capabilities.</p><p>After pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with chain-of-thought <ref type="bibr" target="#b49">(Wei et al., 2022)</ref>, program-of-thought <ref type="bibr">(Chen et al., 2022;</ref><ref type="bibr" target="#b12">Gao et al., 2023)</ref>, and tool-integrated reasoning <ref type="bibr">(Gou et al., 2023)</ref> data. The resulting model DeepSeekMath-Instruct 7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models.</p><p>Furthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant reinforcement learning (RL) algorithm of Proximal Policy Optimization (PPO) <ref type="bibr" target="#b39">(Schulman et al., 2017)</ref>. GRPO foregoes the critic model, instead estimating the baseline from group scores, significantly reducing training resources. By solely using a subset of English instruction tuning data, GRPO obtains a substantial improvement over the strong DeepSeekMath-Instruct, including both in-domain (GSM8K: 82.9% → 88.2%, MATH: 46.8% → 51.7%) and out-of-domain mathematical tasks (e.g., CMATH: 84.6% → 88.8%) during the reinforcement learning phase. We also provide a unified paradigm to understand different methods, such as Rejection Sampling Fine-Tuning (RFT) <ref type="bibr">(Yuan et al., 2023a)</ref>, Direct Preference Optimization (DPO) <ref type="bibr" target="#b36">(Rafailov et al., 2023)</ref>, PPO and GRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as either direct or simplified RL techniques. We also conduct extensive experiments, e.g., online v.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on, to deeply investigate the essential elements of this paradigm. At last, we explain why our RL boosts the performance of instruction-tuned models, and further summarize potential directions to achieve more effective RL based on this unified paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>Our contribution includes scalable math pre-training, along with the exploration and analysis of reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Math Pre-Training at Scale</head><p>• Our research provides compelling evidence that the publicly accessible Common Crawl data contains valuable information for mathematical purposes. By implementing a meticulously designed data selection pipeline, we successfully construct the DeepSeekMath Corpus, a high-quality dataset of 120B tokens from web pages filtered for mathematical content, which is almost 7 times the size of the math web pages used by Minerva <ref type="bibr">(Lewkowycz et al., 2022a)</ref> and 9 times the size of the recently released OpenWebMath <ref type="bibr" target="#b2">(Paster et al., 2023</ref>). • Our pre-trained base model DeepSeekMath-Base 7B achieves comparable performance with Minerva 540B <ref type="bibr">(Lewkowycz et al., 2022a)</ref>, indicating the number of parameters is not the only key factor in mathematical reasoning capability. A smaller model pre-trained on high-quality data could achieve strong performance as well. • We share our findings from math training experiments. Code training prior to math training improves models' ability to solve mathematical problems both with and without tool use. This offers a partial answer to the long-standing question: does code training improve reasoning abilities? We believe it does, at least for mathematical reasoning. • Although training on arXiv papers is common, especially in many math-related papers, it brings no notable improvements on all mathematical benchmarks adopted in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploration and Analysis of Reinforcement Learning</head><p>• We introduce Group Relative Policy Optimization (GRPO), an efficient and effective reinforcement learning algorithm. GRPO foregoes the critic model, instead estimating the baseline from group scores, significantly reducing training resources compared to Proximal Policy Optimization (PPO). • We demonstrate that GRPO significantly enhances the performance of our instructiontuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Furthermore, we observe enhancements in the out-of-domain performance during the reinforcement learning process. • We provide a unified paradigm to understand different methods, such as RFT, DPO, PPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so on to deeply investigate the essential elements of this paradigm. • Based on our unified paradigm, we explore the reasons behind the effectiveness of reinforcement learning, and summarize several potential directions to achieve more effective reinforcement learning of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Summary of Evaluations and Metrics</head><p>• English and Chinese Mathematical Reasoning: We conduct comprehensive assessments of our models on English and Chinese benchmarks, covering mathematical problems from grade-school level to college level. English benchmarks include GSM8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref>, MATH <ref type="bibr" target="#b16">(Hendrycks et al., 2021)</ref>, SAT <ref type="bibr" target="#b2">(Azerbayev et al., 2023)</ref>, OCW Courses <ref type="bibr">(Lewkowycz et al., 2022a)</ref>, MMLU-STEM <ref type="bibr" target="#b15">(Hendrycks et al., 2020)</ref>. Chinese benchmarks include MGSM-zh <ref type="bibr" target="#b40">(Shi et al., 2023)</ref>, CMATH <ref type="bibr" target="#b50">(Wei et al., 2023)</ref>, Gaokao-MathCloze <ref type="bibr">(Zhong et al., 2023)</ref>, and Gaokao-MathQA <ref type="bibr">(Zhong et al., 2023)</ref>. We evaluate models' ability to generate self-contained text solutions without tool use, and also the ability to solve problems using Python.</p><p>On English benchmarks, DeepSeekMath-Base is competitive with the closed-source Minerva 540B <ref type="bibr">(Lewkowycz et al., 2022a)</ref>, and surpasses all open-source base models (e.g., Mistral 7B <ref type="bibr" target="#b19">(Jiang et al., 2023)</ref> and Llemma-34B <ref type="bibr" target="#b2">(Azerbayev et al., 2023)</ref>), regardless of whether they've undergone math pre-training or not, often by a significant margin. Notably, DeepSeekMath-Base is superior on Chinese benchmarks, likely because we don't follow previous works <ref type="bibr" target="#b2">(Azerbayev et al., 2023;</ref><ref type="bibr">Lewkowycz et al., 2022a)</ref> to collect English-only math pre-training data, and also include high-quality non-English ones. With mathematical instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct and DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over 50% on the competition-level MATH dataset for the first time within the open-source community. • Formal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal theorem proving task from <ref type="bibr" target="#b18">(Jiang et al., 2022)</ref> on miniF2F <ref type="bibr" target="#b59">(Zheng et al., 2021)</ref> with Isabelle <ref type="bibr" target="#b51">(Wenzel et al., 2008)</ref> chosen to be the proof assistant. DeepSeekMath-Base demonstrates strong few-shot autoformalization performance. • Natural Language Understanding, Reasoning, and Code: To build a comprehensive profile of models' general understanding, reasoning, and coding capabilities, we evaluate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU) benchmark <ref type="bibr" target="#b15">(Hendrycks et al., 2020)</ref> which encompasses 57 multiple-choice tasks covering diverse subjects, BIG-Bench Hard (BBH) <ref type="bibr" target="#b42">(Suzgun et al., 2022)</ref> which consists of 23 challenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval <ref type="bibr" target="#b8">(Chen et al., 2021)</ref> and MBPP <ref type="bibr" target="#b1">(Austin et al., 2021)</ref> which are widely used to evaluate code language models. Math pre-training benefits both language understanding and reasoning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Math Pre-Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Collection and Decontamination</head><p>In this section, we will outline the process of constructing the DeepSeekMath Corpus from Common Crawl. As depicted in Figure <ref type="figure">2</ref>, we present an iterative pipeline that demonstrates how to systematically gather a large-scale mathematical corpus from Common Crawl, starting with a seed corpus (e.g., a small but high-quality collection of math-related dataset). It's worth noting that this approach is also applicable to other domains, such as coding.</p><p>First, we choose OpenWebMath <ref type="bibr" target="#b2">(Paster et al., 2023)</ref>, a collection of high-quality mathematical web texts, as our initial seed corpus. Using this corpus, we train a fastText model <ref type="bibr" target="#b20">(Joulin et al., 2016)</ref> to recall more OpenWebMath-like mathematical web pages. Specifically, we randomly select 500,000 data points from the seed corpus as positive training examples and another 500,000 web pages from Common Crawl as negative ones. We employ an open-source library<ref type="foot" target="#foot_0">foot_0</ref> for training, configuring the vector dimension to 256, learning rate to 0.1, the maximum length</p><p>Math Seed Math Corpus 1. Train a FastText Model 2. Recall Math-Related Webpages From Common Crawl 3. Discover Math-Related Domains 4. Annotate Math-Related URL Path From Labelers Deduplicated Common Crawl 40B HTML pages Figure 2 | An iterative pipeline that collects mathematical web pages from Common Crawl. of word n-gram to 3, the minimum number of word occurrences to 3, and the number of training epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based deduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then recall mathematical web pages from deduplicated Common Crawl with the fastText model. To filter out low-quality mathematical content, we rank the collected pages according to their scores predicted by the fastText model, and only preserve the top-ranking ones. The volume of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and 160B tokens. In the first iteration, we choose to keep the top 40B tokens.</p><p>After the first iteration of data collection, numerous mathematical web pages remain uncollected, mainly because the fastText model is trained on a set of positive examples that lacks sufficient diversity. We therefore identify additional mathematical web sources to enrich the seed corpus, so that we can optimize the fastText model. Specifically, we first organize the entire Common Crawl into disjoint domains; a domain is defined as web pages sharing the same base URL. For each domain, we calculate the percentage of web pages that are collected in the first iteration. Domains where over 10% of the web pages have been collected are classified as math-related (e.g., mathoverflow.net). Subsequently, we manually annotate the URLs associated with mathematical content within these identified domains (e.g., mathoverflow.net/questions).</p><p>Web pages linked to these URLs, yet uncollected, will be added to the seed corpus. This approach enables us to gather more positive examples, thereby training an improved fastText model capable of recalling more mathematical data in the subsequent iteration. After four iterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B tokens. In the fourth iteration, we notice that nearly 98% of the data has already been collected in the third iteration, so we decide to cease data collection.</p><p>To avoid benchmark contamination, we follow <ref type="bibr" target="#b14">Guo et al. (2024)</ref> to filter out web pages containing questions or answers from English mathematical benchmarks such as GSM8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref> and MATH <ref type="bibr" target="#b16">(Hendrycks et al., 2021)</ref> and Chinese benchmarks such as CMATH <ref type="bibr" target="#b50">(Wei et al., 2023)</ref> and AGIEval <ref type="bibr">(Zhong et al., 2023)</ref>. The filtering criteria are as follows: any text segment containing a 10-gram string that matches exactly with any sub-string from the evaluation benchmarks is removed from our math training corpus. For benchmark texts that are shorter than 10 grams but have at least 3 grams, we employ exact matching to filter out contaminated web pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Validating the Quality of the DeepSeekMath Corpus</head><p>We run pre-training experiments to investigate how the DeepSeekMath Corpus is compared with the recently released math-training corpora:</p><p>• MathPile <ref type="bibr">(Wang et al., 2023c)</ref>: a multi-source corpus (8.9B tokens) aggregated from textbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the majority (over 85%) sourced from arXiv; • OpenWebMath <ref type="bibr" target="#b2">(Paster et al., 2023)</ref>: CommonCrawl data filtered for mathematical content, totaling 13.6B tokens; • Proof-Pile-2 <ref type="bibr" target="#b2">(Azerbayev et al., 2023)</ref>: a mathematical corpus consisting of OpenWeb-Math, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B tokens). When experimenting on Proof-Pile-2, we follow <ref type="bibr" target="#b2">Azerbayev et al. (2023)</ref> to use an arXiv:Web:Code ratio of 2:4:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Training Setting</head><p>We apply math training to a general pre-trained language model with 1.3B parameters, which shares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeek-LLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All experiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023) training framework. Following the training practice of DeepSeek LLMs, we use the AdamW optimizer <ref type="bibr" target="#b26">(Loshchilov and Hutter, 2017)</ref> with 𝛽 1 = 0.9, 𝛽 2 = 0.95, and weight_decay = 0.1, along with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000 warmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to 10.0% of the peak after 90% of the training process. We set the maximum value of learning rate to 5.3e-4, and use a batch size of 4M tokens with a 4K context length.</p><p>Math Corpus Size English Benchmarks Chinese Benchmarks GSM8K MATH OCW SAT MMLU STEM CMATH Gaokao MathCloze Gaokao MathQA No Math Training N/A 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9% MathPile 8.9B 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8% OpenWebMath 13.6B 11.5% 8.9% 3.7% 31.3% 29.6% 16.8% 0.0% 14.2% Proof-Pile-2 51.9B 14.3% 11.2% 3.7% 43.8% 29.2% 19.9% 5.1% 11.7% DeepSeekMath Corpus 120.2B 23.8% 13.6% 4.8% 56.3% 33.1% 41.5% 5.9% 23.6%</p><p>Table <ref type="table">1</ref> | Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evaluated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer with a vocabulary size of 100K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Evaluation Results</head><p>The DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and is the largest in size.</p><p>• High-quality: We evaluate downstream performance on 8 mathematical benchmarks using few-shot chain-of-thought prompting <ref type="bibr" target="#b49">Wei et al. (2022)</ref>. As shown in Table <ref type="table">1</ref>, there is a clear performance lead of the model trained on the DeepSeekMath Corpus. Figure <ref type="figure" target="#fig_1">3</ref> shows that the model trained on the DeepSeekMath Corpus demonstrates better performance than </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training and Evaluating DeepSeekMath-Base 7B</head><p>In this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning abilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B <ref type="bibr" target="#b14">(Guo et al., 2024)</ref> and trained for 500B tokens. The distribution of the data is as follows: 56% is from the DeepSeekMath Corpus, 4% from AlgebraicStack, 10% from arXiv, 20% is Github code, and the remaining 10% is natural language data from Common Crawl in both English and Chinese. We mainly adopt the training setting specified in Section 2.2.1, except that we set the maximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens.</p><p>We conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMath-Base 7B, focusing on its ability to produce self-contained mathematical solutions without relying on external tools, solve mathematical problems using tools, and conduct formal theorem proving. Beyond mathematics, we also provide a more general profile of the base model, including its performance of natural language understanding, reasoning, and programming skills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathematical Problem Solving with Step-by-Step Reasoning</head><p>We evaluate DeepSeekMath-Base's performance of solving mathematical problems using few-shot chain-of-thought prompting <ref type="bibr" target="#b49">(Wei et al., 2022)</ref>, across eight benchmarks in English and Chinese. These benchmarks encompass quantitative reasoning (e.g., GSM8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref>, MATH <ref type="bibr" target="#b16">(Hendrycks et al., 2021)</ref>, and CMATH <ref type="bibr" target="#b50">(Wei et al., 2023)</ref>) and multiple-choice problems (e.g., MMLU-STEM <ref type="bibr" target="#b15">(Hendrycks et al., 2020)</ref> and Gaokao-MathQA <ref type="bibr">(Zhong et al., 2023)</ref>), covering diverse fields of mathematics from elementary to college-level complexity.</p><p>As shown in Table <ref type="table" target="#tab_3">2</ref>, DeepSeekMath-Base 7B leads in performance across all eight benchmarks among the open-source base models (including the widely-used general model Mistral 7B <ref type="bibr" target="#b19">(Jiang et al., 2023)</ref> and the recently released Llemma 34B <ref type="bibr" target="#b2">(Azerbayev et al., 2023)</ref> which underwent math training on Proof-Pile-2 <ref type="bibr" target="#b2">(Azerbayev et al., 2023)</ref>). Notably, on the competitionlevel MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over 10% absolute, and outperforms Minerva 540B <ref type="bibr">(Lewkowycz et al., 2022a)</ref>, a closed-source base model 77 times larger which builds on PaLM <ref type="bibr">(Lewkowycz et al., 2022b)</ref> and is further trained on mathematical texts. Model Size English Benchmarks Chinese Benchmarks GSM8K MATH OCW SAT MMLU STEM CMATH Gaokao MathCloze Gaokao MathQA Closed-Source Base Model Minerva 7B 16.2% 14.1% 7.7% -35.6% ---Minerva 62B 52.4% 27.6% 12.0% -53.9% ---Minerva 540B 58.8% 33.6% 17.6% -63.9% ---Open-Source Base Model Mistral 7B 40.3% 14.3% 9.2% 71.9% 51.1% 44.9% 5.1% 23.4% Llemma 7B 37.4% 18.1% 6.3% 59.4% 43.1% 43.4% 11.9% 23.6% Llemma 34B 54.0% 25.3% 10.3% 71.9% 52.9% 56.1% 11.9% 26.2% DeepSeekMath-Base 7B 64.2% 36.2% 15.4% 84.4% 56.5% 71.7% 20.3% 35.3% Table <ref type="table" target="#tab_4">3</ref> | Few-shot evaluation of base models' ability to solve mathematical problems using tools and the ability to conduct informal-to-formal theorem proving in Isabelle.</p><p>Formal Mathematics Formal proof automation is beneficial to ensure the accuracy and reliability of mathematical proofs and enhance efficiency, with increasing attention in recent years. We evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from <ref type="bibr" target="#b18">(Jiang et al., 2022)</ref> which is to generate a formal proof based on an informal statement, a formal counterpart of the statement, and an informal proof. We evaluate on miniF2F <ref type="bibr" target="#b59">(Zheng et al., 2021)</ref>, a benchmark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each problem with few-shot prompting. Following <ref type="bibr" target="#b18">Jiang et al. (2022)</ref>, we leverage models to generate proof sketches, and execute the off-the-shelf automated prover Sledgehammer <ref type="bibr" target="#b33">(Paulson, 2010)</ref> to fill in the missing details. As shown in Table 4 | Evaluation on natural language understanding, reasoning, and code benchmarks.</p><p>DeepSeek-Coder-Base-v1.5 † is the checkpoint right before learning rate decay, which is used to train DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.</p><p>On HumanEval and MBPP, we evaluate model performance under the zero-shot setting and a few-shot setting, respectively.</p><p>Natural Language Understanding, Reasoning, and Code We evaluate model performance of natural language understanding on MMLU <ref type="bibr" target="#b15">(Hendrycks et al., 2020)</ref>, reasoning on BBH <ref type="bibr" target="#b42">(Suzgun et al., 2022)</ref>, and coding capabilities on HumanEval <ref type="bibr" target="#b8">(Chen et al., 2021)</ref> and MBPP <ref type="bibr" target="#b1">(Austin et al., 2021)</ref>. As shown in Table <ref type="table">4</ref>, DeepSeekMath-Base 7B exhibits significant enhancements in performance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 <ref type="bibr" target="#b14">(Guo et al., 2024)</ref>, illustrating the positive impact of math training on language understanding and reasoning. Additionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively maintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Overall, DeepSeekMath-Base 7B significantly outperforms the general model Mistral 7B <ref type="bibr" target="#b19">(Jiang et al., 2023)</ref> on the three reasoning and coding benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Supervised Fine-Tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SFT Data Curation</head><p>We construct a mathematical instruction-tuning dataset covering English and Chinese problems from different mathematical fields and of varying complexity levels: problems are paired with solutions in chain-of-thought (CoT) <ref type="bibr" target="#b49">(Wei et al., 2022)</ref>, program-of-thought (PoT) <ref type="bibr">(Chen et al., 2022;</ref><ref type="bibr" target="#b12">Gao et al., 2023)</ref>, and tool-integrated reasoning format <ref type="bibr">(Gou et al., 2023)</ref>. The total number of training examples is 776K.</p><p>• English mathematical datasets: We annotate GSM8K and MATH problems with toolintegrated solutions, and adopt a subset of MathInstruct <ref type="bibr">(Yue et al., 2023)</ref> along with the training set of Lila-OOD <ref type="bibr" target="#b28">(Mishra et al., 2022)</ref> where problems are solved with CoT or PoT. Our English collection covers diverse fields of mathematics, e.g., algebra, probability, number theory, calculus, and geometry. • Chinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning 76 sub-topics such as linear equations, with solutions annotated in both CoT and toolintegrated reasoning format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training and Evaluating DeepSeekMath-Instruct 7B</head><p>In this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruction tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until reaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch size of 256 and a constant learning rate of 5e-5.</p><p>We evaluate models' mathematical performance both without and with tool use, on 4 quantitative reasoning benchmarks in English and Chinese. We benchmark our model against the leading models of the time:</p><p>• Closed-source models include: (1) the GPT family among which <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> and GPT-4 Code Interpreter<ref type="foot" target="#foot_1">foot_1</ref> are the most capable ones, (2) Gemini Ultra and Pro <ref type="bibr">(Anil et al., 2023)</ref>, (3) Inflection-2 (Inflection AI, 2023), (4) Grok-1<ref type="foot" target="#foot_2">foot_2</ref> , as well as models recently released by Chinese companies including (5) Baichuan-3<ref type="foot" target="#foot_3">foot_3</ref> , (6) the latest GLM-4<ref type="foot" target="#foot_4">foot_4</ref> from the GLM family <ref type="bibr" target="#b11">(Du et al., 2022)</ref>. These models are for general purposes, most of which have undergone a series of alignment procedures. • Open-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeek-AI, 2024), (2) Qwen 72B <ref type="bibr" target="#b3">(Bai et al., 2023)</ref>, (3) SeaLLM-v2 7B <ref type="bibr">(Nguyen et al., 2023)</ref>, and (4) ChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathematics including (5) InternLM2-Math 20B<ref type="foot" target="#foot_5">foot_5</ref> which builds on InternLM2 and underwent math training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO training <ref type="bibr" target="#b39">(Schulman et al., 2017)</ref> to Mistral 7B <ref type="bibr" target="#b19">(Jiang et al., 2023)</ref> with a process-supervised reward model, (7) the WizardMath series <ref type="bibr" target="#b27">(Luo et al., 2023)</ref> which improves mathematical reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e., a version of instruction tuning that uses AI-evolved instructions) and PPO training with training problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al., 2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH, (9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fine-tuned to do tool-integrated mathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B instruction-tuned on MathInstruct.</p><p>As shown in Table <ref type="table">5</ref>, under the evaluation setting where tool use is disallowed, DeepSeekMath-Instruct 7B demonstrates strong performance of step-by-step reasoning. Notably, on the competition-level MATH dataset, our model surpasses all open-source models and the majority of proprietary models (e.g., Inflection-2 and Gemini Pro) by at least 9% absolute. This is true even for models that are substantially larger (e.g., Qwen 72B) or have been specifically enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While DeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH, it still underperforms GPT-4 and Gemini Ultra.</p><p>Under the evaluation setting where models are allowed to integrate natural language reasoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches an accuracy of 60% on MATH, surpassing all existing open-source models. On the other benchmarks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is 10 times larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Reinforcement Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Group Relative Policy Optimization</head><p>Reinforcement learning (RL) has been proven to be effective in further improving the mathematical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage <ref type="bibr" target="#b27">(Luo et al., 2023;</ref><ref type="bibr">Wang et al., 2023b)</ref>. In this section, we introduce our efficient and effective RL algorithm, Group Relative Policy Optimization (GRPO).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">From PPO to GRPO</head><p>Proximal Policy Optimization (PPO) <ref type="bibr" target="#b39">(Schulman et al., 2017</ref>) is an actor-critic RL algorithm that is widely used in the RL fine-tuning stage of LLMs <ref type="bibr" target="#b31">(Ouyang et al., 2022)</ref>. In particular, it optimizes LLMs by maximizing the following surrogate objective:</p><formula xml:id="formula_0">J 𝑃𝑃𝑂 (𝜃) = E[𝑞 ∼ 𝑃(𝑄), 𝑜 ∼ 𝜋 𝜃 𝑜𝑙𝑑 (𝑂|𝑞)] 1 |𝑜| |𝑜| ∑︁ 𝑡=1 min 𝜋 𝜃 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝐴 𝑡 , clip 𝜋 𝜃 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) , 1 -𝜀, 1 + 𝜀 𝐴 𝑡 ,<label>(1)</label></formula><p>where 𝜋 𝜃 and 𝜋 𝜃 𝑜𝑙𝑑 are the current and old policy models, and 𝑞, 𝑜 are questions and outputs sampled from the question dataset and the old policy 𝜋 𝜃 𝑜𝑙𝑑 , respectively. 𝜀 is a clipping-related hyper-parameter introduced in PPO for stabilizing training. 𝐴 𝑡 is the advantage, which is computed by applying Generalized Advantage Estimation (GAE) <ref type="bibr" target="#b38">(Schulman et al., 2015)</ref>, based Model Size English Benchmarks Chinese Benchmarks GSM8K MATH MGSM-zh CMATH Chain-of-Thought Reasoning Closed-Source Model Gemini Ultra -94.4% 53.2% --GPT-4 -92.0% 52.9% -86.0% Inflection-2 -81.4% 34.8% --GPT-3.5 -80.8% 34.1% -73.8% Gemini Pro -86.5% 32.6% --Grok-1 -62.9% 23.9% --Baichuan-3 -88.2% 49.2% --GLM-4 -87.6% 47.9% --Open-Source Model InternLM2-Math 20B 82.6% 37.7% --Qwen 72B 78.9% 35.2% --Math-Shepherd-Mistral 7B 84.1% 33.0% --WizardMath-v1.1 7B 83.2% 33.0% --DeepSeek-LLM-Chat 67B 84.1% 32.6% 74.0% 80.3% MetaMath 70B 82.3% 26.6% 66.4% 70.9% SeaLLM-v2 7B 78.2% 27.5% 64.8% -ChatGLM3 6B 72.3% 25.7% --WizardMath-v1.0 70B 81.6% 22.7% 64.8% 65.4% DeepSeekMath-Instruct 7B 82.9% 46.8% 73.2% 84.6% DeepSeekMath-RL 7B 88.2% 51.7% 79.6% 88.8% Tool-Integrated Reasoning Closed-Source Model GPT-4 Code Interpreter -97.0% 69.7% --Open-Source Model InternLM2-Math 20B 80.7% 54.3% --DeepSeek-LLM-Chat 67B 86.7% 51.1% 76.4% 85.4% ToRA 34B 80.7% 50.8% 41.2% 53.4% MAmmoTH 70B 76.9% 41.8% --DeepSeekMath-Instruct 7B 83.7% 57.4% 72.0% 84.3% DeepSeekMath-RL 7B 86.7% 58.8% 78.4% 87.6% Table 5 | Performance of Open-and Closed-Source models with both Chain-of-Thought and Tool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority votes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all opensource models from 7B to 70B, as well as the majority of closed-source models. Although DeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks. 𝑞𝑞 𝑜𝑜 ! 𝑜𝑜 " 𝑜𝑜 # 𝑟𝑟 ! 𝑟𝑟 " 𝑟𝑟 # 𝐴𝐴 ! 𝐴𝐴 " 𝐴𝐴 # 𝑞𝑞 𝑜𝑜 GAE 𝐴𝐴 𝑟𝑟 𝑣𝑣 Reward Model Policy Model Value Model … … … Policy Model Reference Model Reward Model PPO GRPO Trained Models Frozen Models Reference Model ⊕ 𝐾𝐾𝐾𝐾 𝐾𝐾𝐾𝐾 Group Computation on the rewards {𝑟 ≥𝑡 } and a learned value function 𝑉 𝜓 . Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model, the standard approach is to add a per-token KL penalty from a reference model in the reward at each token <ref type="bibr" target="#b31">(Ouyang et al., 2022)</ref>, i.e.,</p><formula xml:id="formula_1">𝑟 𝑡 = 𝑟 𝜑 (𝑞, 𝑜 ≤𝑡 ) -𝛽 log 𝜋 𝜃 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝜋 𝑟𝑒 𝑓 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) ,<label>(2)</label></formula><p>where 𝑟 𝜑 is the reward model, 𝜋 𝑟𝑒 𝑓 is the reference model, which is usually the initial SFT model, and 𝛽 is the coefficient of the KL penalty.</p><p>As the value function employed in PPO is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden. Additionally, during RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token. To address this, as shown in Figure <ref type="figure" target="#fig_2">4</ref>, we propose Group Relative Policy Optimization (GRPO), which obviates the need for additional value function approximation as in PPO, and instead uses the average reward of multiple sampled outputs, produced in response to the same question, as the baseline. More specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜 1 , 𝑜 2 , • • • , 𝑜 𝐺 } from the old policy 𝜋 𝜃 𝑜𝑙𝑑 and then optimizes the policy model by maximizing the following objective:</p><formula xml:id="formula_2">J 𝐺𝑅𝑃𝑂 (𝜃) = E[𝑞 ∼ 𝑃(𝑄), {𝑜 𝑖 } 𝐺 𝑖=1 ∼ 𝜋 𝜃 𝑜𝑙𝑑 (𝑂|𝑞)] 1 𝐺 𝐺 ∑︁ 𝑖=1 1 |𝑜 𝑖 | |𝑜 𝑖 | ∑︁ 𝑡=1 min 𝜋 𝜃 (𝑜</formula><p>𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) Â𝑖,𝑡 , clip 𝜋 𝜃 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) , 1 -𝜀, 1 + 𝜀 Â𝑖,𝑡 -𝛽D 𝐾𝐿 𝜋 𝜃 ||𝜋 𝑟𝑒 𝑓 ,</p><p>where 𝜀 and 𝛽 are hyper-parameters, and Â𝑖,𝑡 is the advantage calculated based on relative rewards of the outputs inside each group only, which will be detailed in the following subsections. The group relative way that GRPO leverages to calculate the advantages, aligns well with the comparative nature of rewards models, as reward models are typically trained on datasets of comparisons between outputs on the same question. Also note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of Â𝑖,𝑡 .</p><p>Algorithm 1 Iterative Group Relative Policy Optimization Input initial policy model 𝜋 𝜃 init ; reward models 𝑟 𝜑 ; task prompts D; hyperparameters 𝜀, 𝛽, 𝜇 1: policy model 𝜋 𝜃 ← 𝜋 𝜃 init 2: for iteration = 1, . . . , I do 3: reference model 𝜋 𝑟𝑒 𝑓 ← 𝜋 𝜃 4: for step = 1, . . . , M do 5: Sample a batch D 𝑏 from D 6: Update the old policy model 𝜋 𝜃 𝑜𝑙𝑑 ← 𝜋 𝜃 7: Sample 𝐺 outputs {𝑜 𝑖 } 𝐺 𝑖=1 ∼ 𝜋 𝜃 𝑜𝑙𝑑 (• | 𝑞) for each question 𝑞 ∈ D 𝑏 8: Compute rewards {𝑟 𝑖 } 𝐺 𝑖=1 for each sampled output 𝑜 𝑖 by running 𝑟 𝜑 9:</p><p>Compute Â𝑖,𝑡 for the 𝑡-th token of 𝑜 𝑖 through group relative advantage estimation.</p><p>10:</p><p>for GRPO iteration = 1, . . . , 𝜇 do 11:</p><p>Update the policy model 𝜋 𝜃 by maximizing the GRPO objective (Equation <ref type="formula">21</ref>)</p><p>12:</p><p>Update 𝑟 𝜑 through continuous training using a replay mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output 𝜋 𝜃</head><p>And different from the KL penalty term used in ( <ref type="formula" target="#formula_1">2</ref>), we estimate the KL divergence with the following unbiased estimator <ref type="bibr" target="#b37">(Schulman, 2020)</ref>:</p><formula xml:id="formula_4">D 𝐾 𝐿 𝜋 𝜃 ||𝜋 𝑟𝑒 𝑓 = 𝜋 𝑟𝑒 𝑓 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) 𝜋 𝜃 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) -log 𝜋 𝑟𝑒 𝑓 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) 𝜋 𝜃 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) -1,<label>(4)</label></formula><p>which is guaranteed to be positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Outcome Supervision RL with GRPO</head><p>Formally, for each question 𝑞, a group of outputs {𝑜 1 , 𝑜 2 , • • • , 𝑜 𝐺 } are sampled from the old policy model 𝜋 𝜃 𝑜𝑙𝑑 . A reward model is then used to score the outputs, yielding 𝐺 rewards r = {𝑟 1 , 𝑟 2 , • • • , 𝑟 𝐺 } correspondingly. Subsequently, these rewards are normalized by subtracting the group average and dividing by the group standard deviation. Outcome supervision provides the normalized reward at the end of each output 𝑜 𝑖 and sets the advantages Â𝑖,𝑡 of all tokens in the output as the normalized reward, i.e., Â𝑖,𝑡 = 𝑟 𝑖 = 𝑟 𝑖 -mean(r) std(r) , and then optimizes the policy by maximizing the objective defined in equation (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Process Supervision RL with GRPO</head><p>Outcome supervision only provides a reward at the end of each output, which may not be sufficient and efficient to supervise the policy in complex mathematical tasks. Following <ref type="bibr">Wang et al. (2023b)</ref>, we also explore process supervision, which provides a reward at the end of each reasoning step. Formally, given the question 𝑞 and 𝐺 sampled outputs {𝑜 1 , 𝑜 2 , • • • , 𝑜 𝐺 }, a process reward model is used to score each step of the outputs, yielding corresponding rewards:</p><formula xml:id="formula_5">R = {{𝑟 𝑖𝑛𝑑𝑒𝑥 (1) 1 , • • • , 𝑟 𝑖𝑛𝑑𝑒𝑥 (𝐾 1 ) 1 }, • • • , {𝑟 𝑖𝑛𝑑𝑒𝑥 (1) 𝐺 , • • • , 𝑟 𝑖𝑛𝑑𝑒𝑥 (𝐾 𝐺 )</formula><p>𝐺 }}, where 𝑖𝑛𝑑𝑒𝑥 ( 𝑗) is the end token index of the 𝑗-th step, and 𝐾 𝑖 is the total number of steps in the 𝑖-th output. We also normalize these rewards with the average and the standard deviation, i.e., 𝑟 𝑖𝑛𝑑𝑒𝑥 ( 𝑗)</p><formula xml:id="formula_6">𝑖 = 𝑟 𝑖𝑛𝑑𝑒𝑥 ( 𝑗) 𝑖 -mean(R) std(R)</formula><p>. Subsequently, the process supervision calculates the advantage of each token as the sum of the normalized rewards from the following steps, i.e., Â𝑖,𝑡 = 𝑖𝑛𝑑𝑒𝑥 ( 𝑗) ≥𝑡 𝑟 𝑖𝑛𝑑𝑒𝑥 ( 𝑗) 𝑖 , and then optimizes the policy by maximizing the objective defined in equation (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Iterative RL with GRPO</head><p>As the reinforcement learning training process progresses, the old reward model may not be sufficient to supervise the current policy model. Therefore, we also explore the iterative RL with GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the reward model based on the sampling results from the policy model and continually train the old reward model using a replay mechanism that incorporates 10% of historical data. Then, we set the reference model as the policy model, and continually train the policy model with the new reward model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training and Evaluating DeepSeekMath-RL</head><p>We conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-ofthought-format questions related to GSM8K and MATH from the SFT data, which consists of around 144K questions. We exclude other SFT questions to investigate the impact of RL on benchmarks that lack data throughout the RL phase. We construct the training set of reward models following <ref type="bibr">(Wang et al., 2023b)</ref>. We train our initial reward model based on the DeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the policy model as 1e-6. The KL coefficient is 0.04. For each question, we sample 64 outputs. The max length is set to 1024, and the training batch size is 1024. The policy model only has a single update following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks following DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with chain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks can be regarded as out-of-domain tasks.</p><p>Table <ref type="table">5</ref> demonstrates the performance of open-and closed-source models with both chainof-thought and tool-integrated reasoning on English and Chinese benchmarks. We find that: 1) DeepSeekMath-RL 7B attains accuracies of 88.2% and 51.7% on GSM8K and MATH, respectively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source models in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially, DeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope of its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics, showcasing the effectiveness of reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section, we will share our findings in pre-training and RL experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Lessons Learnt in Pre-Training</head><p>We first share our experience in pre-training. Unless otherwise specified, we will adhere to the training settings outlined in Section 2.2.1. It is worth noting that, when referring to the DeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of the data collection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Code Training Benefits Mathematical Reasoning</head><p>A popular yet unverified hypothesis suggests that code training improves reasoning. We attempt to offer a partial response to this, particularly within the mathematical domain: code training</p><p>Training Setting Training Tokens w/o Tool Use w/ Tool Use General Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python No Continual Training ---2.9% 3.0% 12.3% 2.7% 2.3% Two-Stage Training Stage 1: General Training 400B --2.9% 3.2% 14.8% 3.3% 2.3% Stage 2: Math Training --150B 19.1% 14.4% 37.2% 14.3% 6.7% Stage 1: Code Training -400B -5.9% 3.6% 19.9% 12.4% 10.0% Stage 2: Math Training --150B 21.9% 15.3% 39.7% 17.4% 9.4% One-Stage Training Math Training --150B 20.5% 13.1% 37.6% 11.4% 6.5% Code &amp; Math Mixed Training -400B 150B 17.6% 12.1% 36.3% 19.7% 13.5%</p><p>Table 6 | Investigation of how code affects mathematical reasoning under different training settings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning performance without and with tool use via few-shot chain-of-thought prompting and few-shot program-of-thought prompting, respectively. Results Table <ref type="table">6</ref> and Table <ref type="table" target="#tab_13">7</ref> demonstrate the downstream performance under different training settings.</p><p>Code training benefits program-aided mathematical reasoning, both under the two-stage training and one-stage training settings. As shown in Table <ref type="table">6</ref>, under the two-stage training setting, code training alone already significantly enhances the ability to solve GSM8K and MATH problems using Python. Math training in the second stage yields further improvements. Interestingly, under the one-stage training setting, mixing code tokens and math tokens effectively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also synergizes coding (Table <ref type="table" target="#tab_13">7</ref>) and program-aided mathematical reasoning (Table <ref type="table">6</ref>).</p><p>Training Setting Training Tokens MMLU BBH HumanEval (Pass@1) MBPP (Pass@1) General Code Math No Continual Training ---24.5% 28.1% 12.2% 13.0% Two-Stage Training Stage 1: General Training 400B --25.9% 27.7% 15.2% 13.6% Stage 2: Math Training --150B 33.1% 32.7% 12.8% 13.2% Stage 1: Code Training -400B -25.0% 31.5% 25.0% 40.0% Stage 2: Math Training --150B 36.2% 35.3% 12.2% 17.0% One-Stage Training Math Training --150B 32.3% 32.5% 11.6% 13.2% Code &amp; Math Mixed Training -400B 150B 33.5% 35.6% 29.3% 39.4%  Table 9 | Effect of math training on different arXiv corpora, the base model being DeepSeek-Coder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.</p><p>Code training also improves mathematical reasoning without tool use. Under the two-stage training setting, the initial stage of code training already results in moderate enhancements. It also boosts the efficiency of the subsequent math training, eventually leading to the best performance. However, combining code tokens and math tokens for one-stage training compromises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B, due to its limited scale, lacks the capacity to fully assimilate both code and mathematical data simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning</head><p>ArXiv papers are commonly included as a component of math pre-training data <ref type="bibr" target="#b2">(Azerbayev et al., 2023;</ref><ref type="bibr">Lewkowycz et al., 2022a;</ref><ref type="bibr" target="#b35">Polu and Sutskever, 2020;</ref><ref type="bibr">Wang et al., 2023c)</ref>. However, detailed analysis regarding their impact on mathematical reasoning has not been extensively conducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem ineffective in improving mathematical reasoning. We experiment with models of different sizes, including DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B <ref type="bibr" target="#b14">(Guo et al., 2024)</ref>, using arXiv corpora that underwent varied processing pipelines:</p><p>• MathPile <ref type="bibr">(Wang et al., 2023c)</ref>: an 8.9B-token corpus developed with cleaning and filtering heuristic rules, over 85% of which are scientific arXiv papers; • ArXiv-RedPajama <ref type="bibr" target="#b9">(Computer, 2023)</ref>: the entirety of arXiv LaTeX files with preambles, comments, macros, and bibliographies removed, totaling 28.0B tokens.</p><p>In our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeek-Coder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective in improving mathematical reasoning. When trained on a arXiv-only corpus, both models display no notable improvements or even deterioration across various mathematical benchmarks of different complexities employed in this study. These benchmarks include quantitative reasoning datasets like GSM8K and MATH (Table <ref type="table" target="#tab_14">8</ref>), multiple-choice challenges like MMLU-STEM (Table <ref type="table" target="#tab_14">8</ref>), and formal mathematics like miniF2F (Table <ref type="table">9</ref>).</p><p>However, this conclusion has its limitations and should be taken with a grain of salt. We have not yet studied:</p><p>• The impact of arXiv tokens on specific math-related tasks not included in this research, such as informalization of theorems which is to convert formal statements or proofs to their informal versions; • The effect of arXiv tokens when combined with other types of data; • Whether the benefits of arXiv papers would manifest themselves at a larger model scale.</p><p>Thus, further exploration is required, which we leave for future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Insights of Reinforcement Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Towards to a Unified Paradigm</head><p>In this section, we provide a unified paradigm to analyze different training methods, such as SFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the unified paradigm. Generally, the gradient with respect to the parameter 𝜃 of a training method can be written as:</p><formula xml:id="formula_7">∇ 𝜃 J A (𝜃) = E[(𝑞, 𝑜) ∼ D 𝐷𝑎𝑡𝑎 𝑆𝑜𝑢𝑟𝑐𝑒 ] 1 |𝑜| |𝑜| ∑︁ 𝑡=1 𝐺𝐶 A (𝑞, 𝑜, 𝑡, 𝜋 𝑟 𝑓 ) 𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡 𝐶𝑜𝑒 𝑓 𝑓 𝑖𝑐𝑖𝑒𝑛𝑡 ∇ 𝜃 log 𝜋 𝜃 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>There exist three key components: 1) Data Source D, which determines the training data; 2) Reward Function 𝜋 𝑟 𝑓 , which is the source of the training reward signal; 3) Algorithm A: which processes the training data and the reward signal to the gradient coefficient 𝐺𝐶 that determines the magnitude of the penalty or reinforcement for the data. We analyze several representative methods based on such a unified paradigm:</p><p>• Supervised Fine-tuning (SFT): SFT fine-tunes pretrained model on human selected SFT data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Data Source Reward Function Gradient Coefficient SFT 𝑞, 𝑜 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄, 𝑂) -1 RFT 𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 ∼ 𝜋 𝑠 𝑓 𝑡 (𝑂|𝑞) Rule Equation 10 DPO 𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 + , 𝑜 -∼ 𝜋 𝑠 𝑓 𝑡 (𝑂|𝑞) Rule Equation 14 Online RFT 𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 ∼ 𝜋 𝜃 (𝑂|𝑞) Rule Equation 10 PPO 𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 ∼ 𝜋 𝜃 (𝑂|𝑞) Model Equation 18 GRPO 𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), {𝑜 𝑖 } 𝐺 𝑖=1 ∼ 𝜋 𝜃 (𝑂|𝑞) Model Equation 21</p><p>Table 10 | The data source and gradient coefficient of different methods. 𝑃 𝑠 𝑓 𝑡 denotes the data distribution of supervised fine-tuning datasets. 𝜋 𝜃 𝑠 𝑓 𝑡 and 𝜋 𝜃 denote the supervised fine-tuned model and the real-time policy model during the online training process, respectively. We summarize the components of these methods in Table <ref type="table">10</ref>. Please refer to Appendix A.1 for a more detailed derivation process. training data is from the sampling results of the initial SFT model. RFT and DPO follow the offline style, while Online RFT and GRPO follow the online style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation about Data Source</head><p>As shown in Figure <ref type="figure" target="#fig_4">5</ref>, we find that the Online RFT significantly outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation about Gradient Coefficient</head><p>The algorithm processes the reward signal to the gradient coefficient to update the model parameter. We divide the reward function as 'Rule' and 'Model' in our experiments. Rule refers to judging the quality of a response based on the correctness of the answer, and Model denotes that we train a reward model to score each response. The training data of the reward model is based on the rule judgment. Equations 10 and 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its gradient coefficient based on the reward value provided by the reward model. This allows for differential reinforcement and penalization of responses according to their varying magnitudes. In contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly reinforces all responses with correct answers at the same level of intensity.</p><p>As demonstrated in Figure <ref type="figure" target="#fig_4">5</ref>, GRPO surpasses online RFT, thereby highlighting the efficiency of altering positive and negative gradient coefficients. In addition, GRPO+PS shows superior performance compared to GRPO+OS, indicating the benefits of using fine-grained, step-aware gradient coefficients. Furthermore, we explore the iterative RL, in our experiments, we conduct two rounds of iteration. As shown in Figure <ref type="figure" target="#fig_5">6</ref>, we notice that the iterative RL significantly improves the performance, especially at the first iteration.</p><p>the exploration efficiency of policy models, also play an exceedingly important role.</p><p>Algorithms Algorithms process the data and reward signal to the gradient coefficient to update the model parameter. Based on Equation <ref type="formula" target="#formula_7">5</ref>, to some extent, all methods now fully TRUST the signal of the reward function to increase or decrease the conditional probability of a certain token. However, it is impossible to ensure the reward signal is always reliable, especially in extremely complex tasks. For example, even the PRM800K datasets <ref type="bibr" target="#b25">(Lightman et al., 2023)</ref>, which have been carefully annotated by well-trained annotators, still contain approximately 20% of incorrectly annotations<ref type="foot" target="#foot_6">foot_6</ref> . To this end, we will explore the reinforcement learning algorithm that is robust against noisy reward signals. We believe such WEAK-TO-STRONG <ref type="bibr" target="#b4">(Burns et al., 2023)</ref> alignment methods will bring a fundamental change to the learning algorithms.</p><p>Reward Function Reward function is the source of the training signal. In RL, the reward function is usually the neural reward model. We think there exist three important directions for reward models: 1) How to enhance the generalization ability of the reward model. The reward model must be effectively generalized to handle out-of-distribution questions and advanced decoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of LLMs rather than improve their fundamental capabilities; 2) How to reflect the uncertainty of reward model. The uncertainty could potentially act as a linking bridge between the weak reward model and the weak-to-strong learning algorithms; 3) How to efficiently build highquality process reward models that can provide fine-grained training signals for the reasoning process <ref type="bibr" target="#b25">(Lightman et al., 2023;</ref><ref type="bibr">Wang et al., 2023b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion, Limitation, and Future Work</head><p>We present DeepSeekMath, which outperforms all open-source models on the competitionlevel MATH benchmark and approaches the performance of closed models. DeepSeekMath is initialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with a significant component of the training data being 120B math tokens sourced from Common Crawl. Our extensive ablation study shows web pages offer significant potential for high-quality mathematical data, while arXiv may not as beneficial as we expected. We introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which can notably improve mathematical reasoning capabilities with less memory consumption. The experiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached a high score on benchmarks. We also provide a unified paradigm to understand a series of methods and summarize several potential directions for more effective reinforcement learning.</p><p>Although DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while DeepSeekMath shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4. Direct Preference Optimization (DPO)</head><p>The objective of DPO is:</p><formula xml:id="formula_9">J 𝐷𝑃𝑂 (𝜃) = E[</formula><p>𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 + , 𝑜 -∼ 𝜋 𝑠 𝑓 𝑡 (𝑂|𝑞)] log 𝜎 𝛽 1 |𝑜 + | |𝑜 + | ∑︁ 𝑡=1 log 𝜋 𝜃 (𝑜 + 𝑡 |𝑞, 𝑜 + &lt;𝑡 ) 𝜋 ref (𝑜 + 𝑡 |𝑞, 𝑜 + &lt;𝑡 ) -𝛽 1 |𝑜 -| |𝑜 -| ∑︁ 𝑡=1 log 𝜋 𝜃 (𝑜 - &lt;𝑡 |𝑞, 𝑜 - &lt;𝑡 ) 𝜋 ref (𝑜 - &lt;𝑡 |𝑞, 𝑜 - &lt;𝑡 )</p><p>The gradient of J 𝐷𝑃𝑂 (𝜃) is:</p><formula xml:id="formula_11">∇ 𝜃 J 𝐷𝑃𝑂 (𝜃) = E[</formula><p>𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 + , 𝑜 -∼ 𝜋 𝑠 𝑓 𝑡 (𝑂|𝑞)] 1 |𝑜 + | |𝑜 + | ∑︁ 𝑡=1 𝐺𝐶 𝐷𝑃𝑂 (𝑞, 𝑜, 𝑡)∇ 𝜃 log 𝜋 𝜃 (𝑜 + 𝑡 |𝑞, 𝑜 + &lt;𝑡 ) -1 |𝑜 -| |𝑜 -| ∑︁ 𝑡=1 𝐺𝐶 𝐷𝑃𝑂 (𝑞, 𝑜, 𝑡)∇ 𝜃 log 𝜋 𝜃 (𝑜 - 𝑡 |𝑞, 𝑜 - &lt;𝑡 ) (13) Data Source: question in SFT dataset with outputs sampled from SFT model. Reward Function: human preference in the general domain (can be 'Rule' in mathematical tasks). Gradient Coefficient: 𝐺𝐶 𝐷𝑃𝑂 (𝑞, 𝑜, 𝑡) = 𝜎 𝛽 log 𝜋 𝜃 (𝑜 - 𝑡 |𝑞, 𝑜 - &lt;𝑡 ) 𝜋 ref (𝑜 - 𝑡 |𝑞, 𝑜 - &lt;𝑡 ) -𝛽 log 𝜋 𝜃 (𝑜 + 𝑡 |𝑞, 𝑜 + &lt;𝑡 ) 𝜋 ref (𝑜 + 𝑡 |𝑞, 𝑜 + &lt;𝑡 ) (14) A.1.5. Proximal Policy Optimization (PPO) The objective of PPO is: J 𝑃𝑃𝑂 (𝜃) = E[𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 ∼ 𝜋 𝜃 𝑜𝑙𝑑 (𝑂|𝑞)] 1 |𝑜| |𝑜| ∑︁ 𝑡=1 min 𝜋 𝜃 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝐴 𝑡 , clip 𝜋 𝜃 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) , 1 -𝜀, 1 + 𝜀 𝐴 𝑡 . (15) To simplify the analysis, it is assumed that the model only has a single update following each exploration stage, thereby ensuring that 𝜋 𝜃 𝑜𝑙𝑑 = 𝜋 𝜃 . In this case, we can remove the min and clip operation: J 𝑃𝑃𝑂 (𝜃) = E[𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 ∼ 𝜋 𝜃 𝑜𝑙𝑑 (𝑂|𝑞)] 1 |𝑜| |𝑜| ∑︁ 𝑡=1 𝜋 𝜃 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) 𝐴 𝑡 . (16) The gradient of J 𝑃𝑃𝑂 (𝜃) is: ∇ 𝜃 J 𝑃𝑃𝑂 (𝜃) = E[𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), 𝑜 ∼ 𝜋 𝜃 𝑜𝑙𝑑 (𝑂|𝑞)] 1 |𝑜| |𝑜| ∑︁ 𝑡=1 𝐴 𝑡 ∇ 𝜃 log 𝜋 𝜃 (𝑜 𝑡 |𝑞, 𝑜 &lt;𝑡 ) (17) Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function: reward model. Gradient Coefficient: 𝐺𝐶 𝑃𝑃𝑂 (𝑞, 𝑜, 𝑡, 𝜋 𝜃 𝑟𝑚 ) = 𝐴 𝑡 , <ref type="bibr">(18)</ref> where 𝐴 𝑡 is the advantage, which is computed by applying Generalized Advantage Estimation (GAE) <ref type="bibr" target="#b38">(Schulman et al., 2015)</ref>, based on the rewards {𝑟 ≥𝑡 } and a learned value function 𝑉 𝜓 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.6. Group Relative Policy Optimization (GRPO)</head><p>The objective of GRPO is (assume 𝜋 𝜃 𝑜𝑙𝑑 = 𝜋 𝜃 for simplified analysis):</p><p>J 𝐺𝑅𝑃𝑂 (𝜃) = E[𝑞 ∼ 𝑃 𝑠 𝑓 𝑡 (𝑄), {𝑜 𝑖 } 𝐺 𝑖=1 ∼ 𝜋 𝜃 𝑜𝑙𝑑 (𝑂|𝑞)] 1 𝐺 𝐺 ∑︁ 𝑖=1 1 |𝑜 𝑖 | |𝑜 𝑖 | ∑︁ 𝑡=1 𝜋 𝜃 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) 𝜋 𝜃 𝑜𝑙𝑑 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) Â𝑖,𝑡 -𝛽( 𝜋 𝑟𝑒 𝑓 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) 𝜋 𝜃 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) log 𝜋 𝑟𝑒 𝑓 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) 𝜋 𝜃 (𝑜 𝑖,𝑡 |𝑞, 𝑜 𝑖,&lt;𝑡 ) -1) . (19)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 |</head><label>1</label><figDesc>Figure 1 | Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques.</figDesc><graphic coords="1,143.44,513.25,308.41,181.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 |</head><label>3</label><figDesc>Figure 3 | Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora.</figDesc><graphic coords="7,98.08,85.04,399.13,377.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 |</head><label>4</label><figDesc>Figure 4 | Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead estimating the baseline from group scores, significantly reducing training resources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>improves models' ability to do mathematical reasoning both with and without tool use. To study how code training affects mathematical reasoning, we experimented with the following two-stage training and one-stage training settings: Two-Stage Training • Code Training for 400B Tokens → Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 400B code tokens followed by 150B math tokens; • General Training for 400B Tokens → Math Training for 150B Tokens: As a control experiment, we also experiment with general tokens (sampled from a large-scale general corpus created by DeepSeek-AI) instead of code tokens in the first stage of training, in an attempt to investigate the advantages of code tokens over general tokens in improving mathematical reasoning. One-Stage Training • Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens; • Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training following code training degrades coding performance. We investigate whether code tokens, when mixed with math tokens for one-stage training, would still improve mathematical reasoning and also alleviate the problem of catastrophic forgetting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 |</head><label>5</label><figDesc>Figure5| Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained using various methods, on two benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 |</head><label>6</label><figDesc>Figure 6 | Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on two benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 |</head><label>2</label><figDesc>Comparisons between DeepSeekMath-Base 7B and strong base models on English and Chinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting. Minerva results are quoted fromLewkowycz et al. (2022a).Problem Solving with Tool UseWe evaluate program-aided mathematical reasoning on GSM8K and MATH using few-shot program-of-thought prompting(Chen et al.,  2022;<ref type="bibr" target="#b12">Gao et al., 2023)</ref>. Models are prompted to solve each problem by writing a Python program where libraries such as math and sympy can be utilized for intricate computations. The execution result of the program is evaluated as the answer. As shown in Table3, DeepSeekMath-Base 7B outperforms the prior state-of-the-art Llemma 34B.</figDesc><table><row><cell>Mathematical Model</cell><cell>Size</cell><cell cols="2">Problem Solving w/ Tools</cell><cell cols="2">Informal-to-Formal Proving</cell></row><row><cell></cell><cell></cell><cell cols="4">GSM8K+Python MATH+Python miniF2F-valid miniF2F-test</cell></row><row><cell>Mistral</cell><cell>7B</cell><cell>48.5%</cell><cell>18.2%</cell><cell>18.9%</cell><cell>18.0%</cell></row><row><cell>CodeLlama</cell><cell>7B</cell><cell>27.1%</cell><cell>17.2%</cell><cell>16.3%</cell><cell>17.6%</cell></row><row><cell>CodeLlama</cell><cell>34B</cell><cell>52.7%</cell><cell>23.5%</cell><cell>18.5%</cell><cell>18.0%</cell></row><row><cell>Llemma</cell><cell>7B</cell><cell>41.0%</cell><cell>18.6%</cell><cell>20.6%</cell><cell>22.1%</cell></row><row><cell>Llemma</cell><cell>34B</cell><cell>64.6%</cell><cell>26.3%</cell><cell>21.0%</cell><cell>21.3%</cell></row><row><cell cols="2">DeepSeekMath-Base 7B</cell><cell>66.9%</cell><cell>31.4%</cell><cell>25.8%</cell><cell>24.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 ,</head><label>3</label><figDesc>DeepSeekMath-Base 7B demonstrates strong performance in proof autoformalization.</figDesc><table><row><cell>Model</cell><cell cols="4">Size MMLU BBH HumanEval (Pass@1) MBPP (Pass@1)</cell></row><row><cell>Mistral</cell><cell>7B</cell><cell>62.4% 55.7%</cell><cell>28.0%</cell><cell>41.4%</cell></row><row><cell cols="2">DeepSeek-Coder-Base-v1.5  † 7B</cell><cell>42.9% 42.9%</cell><cell>40.2%</cell><cell>52.6%</cell></row><row><cell cols="2">DeepSeek-Coder-Base-v1.5 7B</cell><cell>49.1% 55.2%</cell><cell>43.2%</cell><cell>60.4%</cell></row><row><cell>DeepSeekMath-Base</cell><cell>7B</cell><cell>54.9% 59.5%</cell><cell>40.9%</cell><cell>52.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 |</head><label>7</label><figDesc>Investigation of how different settings of code and math training affect model performance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM 1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting. On HumanEval and MBPP, we conduct zero-shot and few-shot evaluations, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>English Benchmarks</cell><cell></cell><cell cols="3">Chinese Benchmarks</cell></row><row><cell>Model</cell><cell cols="2">Size ArXiv Corpus</cell><cell cols="2">GSM8K MATH OCW SAT</cell><cell>MMLU STEM</cell><cell>CMATH</cell><cell>Gaokao MathCloze</cell><cell>Gaokao MathQA</cell></row><row><cell></cell><cell></cell><cell cols="2">No Math Training 2.9%</cell><cell cols="2">3.0% 2.9% 15.6% 19.5%</cell><cell>12.3%</cell><cell>0.8%</cell><cell>17.9%</cell></row><row><cell>DeepSeek-LLM</cell><cell>1.3B</cell><cell>MathPile</cell><cell>2.7%</cell><cell cols="2">3.3% 2.2% 12.5% 15.7%</cell><cell>1.2%</cell><cell>0.0%</cell><cell>2.8%</cell></row><row><cell></cell><cell></cell><cell cols="2">ArXiv-RedPajama 3.3%</cell><cell cols="2">3.4% 4.0% 9.4% 9.0%</cell><cell>7.4%</cell><cell>0.8%</cell><cell>2.3%</cell></row><row><cell></cell><cell></cell><cell cols="4">No Math Training 29.0% 12.5% 6.6% 40.6% 38.1%</cell><cell>45.9%</cell><cell>5.9%</cell><cell>21.1%</cell></row><row><cell cols="2">DeepSeek-Coder-Base-v1.5 7B</cell><cell>MathPile</cell><cell cols="3">23.6% 11.5% 7.0% 46.9% 35.8%</cell><cell>37.9%</cell><cell>4.2%</cell><cell>25.6%</cell></row><row><cell></cell><cell></cell><cell cols="4">ArXiv-RedPajama 28.1% 11.1% 7.7% 50.0% 35.2%</cell><cell>42.6%</cell><cell>7.6%</cell><cell>24.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 |</head><label>8</label><figDesc>Effect of math training on different arXiv datasets. Model performance is evaluated with few-shot chain-of-thought prompting.</figDesc><table><row><cell>ArXiv Corpus</cell><cell cols="2">miniF2F-valid miniF2F-test</cell></row><row><cell>No Math Training</cell><cell>20.1%</cell><cell>21.7%</cell></row><row><cell>MathPile</cell><cell>16.8%</cell><cell>16.4%</cell></row><row><cell>ArXiv-RedPajama</cell><cell>14.8%</cell><cell>11.9%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://fasttext.cc</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://openai.com/blog/chatgpt-plugins#code-interpreter</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://x.ai/model-card</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.baichuan-ai.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://open.bigmodel.cn/dev/api#glm-4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/InternLM/InternLM-Math</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/openai/prm800k/issues/12#issuecomment-1728491852</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acc (%) MATH Maj@K-Instruct Maj@K-RL Pass@K-Instruct Pass@K-RL Figure <ref type="figure">7</ref> | The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH (temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Why RL Works?</head><p>In this paper, we conduct reinforcement learning based on a subset of instruction tuning data, and it achieves significant performance enhancement upon the instruction tuning model.</p><p>To further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K accuracy of the Instruct and RL models on two benchmarks. As shown in Figure <ref type="figure">7</ref>, RL enhances Maj@K's performance but not Pass@K. These findings indicate that RL enhances the model's overall performance by rendering the output distribution more robust, in other words, it seems that the improvement is attributed to boosting the correct response from TopK rather than the enhancement of fundamental capabilities. Similarly, <ref type="bibr">(Wang et al., 2023a</ref>) identified a misalignment problem in reasoning tasks within the SFT model, showing that the reasoning performance of SFT models can be improved through a series of preference alignment strategies <ref type="bibr" target="#b41">(Song et al., 2023;</ref><ref type="bibr">Wang et al., 2023a;</ref><ref type="bibr">Yuan et al., 2023b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">How to Achieve More Effective RL?</head><p>We demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified paradigm to understand different representative training methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation <ref type="formula">5</ref>, there exist three key components: Data Source, Algorithm, and Reward Function.</p><p>We provide some potential future directions about the three components.</p><p>Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction with advanced sampling (decoding) strategies, like those based on tree-search methods <ref type="bibr" target="#b54">(Yao et al., 2023)</ref>. Also, the efficient inference techniques <ref type="bibr" target="#b21">(Kwon et al., 2023;</ref><ref type="bibr" target="#b22">Leviathan et al., 2023;</ref><ref type="bibr" target="#b52">Xia et al., 2023</ref><ref type="bibr" target="#b53">Xia et al., , 2024))</ref>, which determines</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Analysis of Reinforcement Learning</head><p>We provide the detailed derivation of the data source and gradient coefficient (algorithm and reward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and GRPO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1. Supervised Fine-tuning</head><p>The objective of Supervised Fine-tuning is maximizing the following objective:</p><p>The gradient of J 𝑆𝐹𝑇 (𝜃) is:</p><p>Data Source: The dataset employed for SFT. Reward Function: This can be regarded as human selection. Gradient Coefficient: always set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2. Rejection Sampling Fine-tuning</head><p>Rejection Sampling Fine-tuning first samples multiple outputs from the supervised fine-tuned LLMs for each question, and then trains LLMs on the sampled outputs with the correct answer. Formally, the objective of RFT is to maximize the following objectives:</p><p>The gradient of J 𝑅𝐹𝑇 (𝜃) is:</p><p>Data Source: question in SFT dataset with outputs sampled from SFT model. Reward Function: Rule (whether the answer is correct or not). Gradient Coefficient:</p><p>𝐺𝐶 𝑅𝐹𝑇 (𝑞, 𝑜, 𝑡) = I(𝑜) = 1 the answer of o is correct 0 the answer of o is incorrect (10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3. Online Rejection Sampling Fine-tuning</head><p>The only difference between RFT and Online RFT is that the outputs of Online RFT are sampled from the real-time policy model 𝜋 𝜃 , rather than from the SFT model 𝜋 𝜃 𝑠 𝑓 𝑡 . Therefore, the gradient of online RFT is:</p><p>The gradient of J 𝐺𝑅𝑃𝑂 (𝜃) is: where Â𝑖,𝑡 is computed based on the group reward scores.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gemini: A family of highly capable multimodal models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Piqueras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Von Glehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yagati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2312.11805</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2312.11805" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welleck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10631</idno>
		<title level="m">Llemma: An open language model for mathematics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Weak-to-strong generalization: Eliciting strong capabilities with weak supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aschenbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.09390</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chatglm3 series: Open bilingual chat llms</title>
		<ptr target="https://github.com/THUDM/ChatGLM3" />
	</analytic>
	<monogr>
		<title level="m">ChatGLM3 Team</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2107.03374" />
		<imprint/>
	</monogr>
	<note>Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.12588</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2211.12588" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Redpajama: an open dataset for training large language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Computer</surname></persName>
		</author>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
		<imprint>
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deepseek LLM: scaling open-source language models with longtermism</title>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2401.02954</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2401.02954" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glm: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PAL: programaided language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/gao23f.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="10764" to="10799" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tora: A toolintegrated reasoning agent for mathematical problem solving</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.17452</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.1745" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.17452</idno>
		<title level="m">Deepseek-coder: When the large language model meets programming -the rise of code intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03300</idno>
		<title level="m">Measuring massive multitask language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m">Measuring mathematical problem solving with the math dataset</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>High-Flyer</surname></persName>
		</author>
		<author>
			<persName><surname>Hai-Llm</surname></persName>
		</author>
		<ptr target="https://www.high-flyer.cn/en/blog/hai-llm" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>高效且轻量的大模型训练工具</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Draft, sketch, and prove: Guiding formal theorem provers with informal proofs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jamnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.12283</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext. zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</title>
		<meeting>the ACM SIGOPS 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast inference from transformers via speculative decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Leviathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19274" to="19286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Solving quantitative reasoning problems with language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gutman-Solo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3843" to="3857" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Solving quantitative reasoning problems with language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gutman-Solo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/18" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12-09">November 28 -December 9, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>abbeef8cfe9203fdf9053c9c4fe191-Abstr act-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Lightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.20050</idno>
		<title level="m">Let&apos;s verify step by step</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09583</idno>
		<title level="m">Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LILA: A unified benchmark for mathematical reasoning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rajpurohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalyan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.392</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.emnlp-main.392" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">December 7-11, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="5807" to="5832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Aljunied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2312.00738</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2312.00738" />
		<title level="m">Seallms -large language models for southeast asia</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">GPT4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Openwebmath: An open dataset of high-quality mathematical web text</title>
		<author>
			<persName><forename type="first">K</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.06786</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2310.06786" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Practical Aspects of Automated Reasoning, PAAR-2010</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Schmidt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Schulz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Konev</surname></persName>
		</editor>
		<meeting>the 2nd Workshop on Practical Aspects of Automated Reasoning, PAAR-2010<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-14">July 14, 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><surname>Easychair</surname></persName>
		</author>
		<idno type="DOI">10.29007/tnfd</idno>
		<ptr target="https://doi.org/10.29007/tnfd" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Generative language modeling for automated theorem proving</title>
		<author>
			<persName><forename type="first">S</forename><surname>Polu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR, abs/2009.03393</idno>
		<ptr target="https://arxiv.org/abs/2009.03393" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Approximating kl divergence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="http://joschu.net/blog/kl-approx.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language models are multilingual chain-of-thought reasoners</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=fR" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
			<biblScope unit="page" from="wGCk" to="IXp" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.17492</idno>
		<title level="m">Preference ranking optimization for human alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Challenging big-bench tasks and whether chain-of-thought can solve them</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09261</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Embracing change and resetting expectations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<ptr target="https://unlocked.microsoft.com/ai-anthology/terence-tao/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.09288</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Solving olympiad geometry without human demonstrations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">625</biblScope>
			<biblScope unit="issue">7995</biblScope>
			<biblScope unit="page" from="476" to="482" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.02144</idno>
		<title level="m">Making large language models better reasoners with alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Math-shepherd: Verify and reinforce llms step-by-step without human annotations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<idno>CoRR, abs/2312.08935</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Generative AI for math: Part I -mathpile: A billion-token-scale pretraining corpus for math</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2312.17120</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2312.17120" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/9" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>d5609613524ecf 4f15af0f7b31abca4-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Cmath: Can your language model pass chinese elementary school math test?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The isabelle framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nipkow</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-71067-7_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-71067-7_7" />
	</analytic>
	<monogr>
		<title level="m">Theorem Proving in Higher Order Logics, 21st International Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Muñoz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Tahar</surname></persName>
		</editor>
		<meeting><address><addrLine>TPHOLs; Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008-08-18">2008. August 18-21, 2008. 2008</date>
			<biblScope unit="volume">5170</biblScope>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sui</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-emnlp.257</idno>
		<ptr target="https://aclanthology.org/2023.findings-emnlp.257" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12">Dec. 2023</date>
			<biblScope unit="page" from="3909" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.07851</idno>
		<title level="m">Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10601</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Metamath: Bootstrap your own mathematical questions for large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.12284</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.12284" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Scaling relationship on learning mathematical reasoning with large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.01825</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05302</idno>
		<title level="m">Rrhf: Rank responses to align language models with human feedback without tears</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Mammoth: Building math generalist models through hybrid instruction tuning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.05653</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.05653" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Polu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00110</idno>
		<title level="m">Minif2f: a cross-system benchmark for formal olympiad-level mathematics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">AGIEval: A human-centric benchmark for evaluating foundation models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.06364</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2304.06364" />
		<imprint>
			<publisher>CoRR</publisher>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
