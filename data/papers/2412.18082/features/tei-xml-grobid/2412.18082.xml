<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prompt Tuning for Item Cold-start Recommendation</title>
				<funder ref="#_Yq3y8UG">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-24">24 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuezihan</forename><surname>Jiang</surname></persName>
							<email>yuezihan.jiang@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Gaode</forename><surname>Chen</surname></persName>
							<email>chengaode19@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Wenhan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingchi</forename><surname>Wang</surname></persName>
							<email>wangjingchi@pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yinjie</forename><surname>Jiang</surname></persName>
							<email>jiangyinjie@kuaishou.com</email>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<email>zhangqi38@kuaishou.com</email>
						</author>
						<author>
							<persName><forename type="first">Jingjian</forename><surname>Lin</surname></persName>
							<email>linjingjian@kuaishou.com</email>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaigui</forename><surname>Bian</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Kuaishou Technology Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Kuaishou Technology Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Kuaishou Technology Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Kuaishou Technology Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Kuaishou Technology Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Kuaishou Technology Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prompt Tuning for Item Cold-start Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-24">24 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">32BED26271555479C2F65C964C9FDD3A</idno>
					<idno type="DOI">10.1145/3640457.3688126</idno>
					<idno type="arXiv">arXiv:2412.18082v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>â€¢ Information systems â†’ Information retrieval</term>
					<term>Personalization</term>
					<term>Environment-specific retrieval Prompt Learning, Cold-start Recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero-or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of the positive feedback to the model, which is the core of the cold-start problem that hinders the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that compared to the content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, item cold-start recommendation has garnered significant attention from researchers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>, as every item inevitably undergoes this phase, which significantly shapes its potential for popularity. With the burgeoning success of pre-training techniques in Natural Language Processing (NLP) <ref type="bibr" target="#b26">[27]</ref>, numerous endeavors have been made to integrate pre-training into cold-start recommendations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>These methods leverage pre-trained knowledge to alleviate the sparsity issue, thereby enhancing the quality of cold-start recommendations <ref type="bibr" target="#b41">[41]</ref>. However, amidst the prevailing trend of pretraining, the effective extraction of useful information from pretrained models emerges as a promising avenue. One such method, prompt-tuning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, stands out for its remarkable advantages over classical fine-tuning paradigms, particularly in zero-shot and few-shot scenarios. By employing hard text templates or soft continuous embeddings as prompts, prompt-tuning transforms downstream tasks into analogous well-trained pre-training tasks.</p><p>The merits of prompt-tuning lie in two key aspects: firstly, it bridges the gap between pretraining and downstream objectives, thereby optimizing the utilization of pretraining model knowledge, which is especially advantageous in cold-start scenarios. Secondly, prompt-tuning requires tuning only a small set of parameters for the prompts and labels, making it more parameter-efficient. The positive feedback provides more task-relevant information as the model with positive feedback information gains higher accuracy on user preference.</p><p>Although researchers have applied prompt learning to address cold-start recommendations by reframing them as zero-or fewshot problems <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b43">43]</ref>, the effectiveness of prompt learning in real industrial recommendation settings remains suboptimal from both data and model perspectives.</p><p>â€¢ Data side cost and gap. Existing approaches often necessitate additional human annotations, such as text prompt descriptions for candidate items, resulting in significant practical costs <ref type="bibr" target="#b44">[44]</ref>. Moreover, relying solely on content features as prompt information may not be well-suited for recommender tasks, lacking an end-to-end connection to the recommendation process. â€¢ Model side bias. A critical challenge in item cold-start recommendations arises from the poor model performance on cold-start items due to limited positive feedback contribution. This disparity often leads to situations where the model assigns high scores to warm-start items and low scores to cold-start items. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">44]</ref> employ a shared model architecture for prompt encoding, potentially resulting in insufficient personalization for cold-start items. This is because cold-start items represent only a small fraction of online traffic compared to popular items, causing the model parameters to be primarily optimized for popular items. Besides, the length of the prompt embedding restricts the volume of tunable parameters, and the prompt information is concentrated in the input layer, leading to less impact on the model. To address the data side problem, we propose leveraging positive feedback as effective prompt data for cold-start items. This choice is motivated by the direct relevance of positive feedback to recommender tasks. To verify the relevance of positive feedback to downstream recommender tasks, we pretrain two dual-tower modelsl <ref type="bibr" target="#b17">[18]</ref> on KuaiRand-Pure <ref type="bibr" target="#b11">[12]</ref>, utilizing all user features. The only distinction between the models lies in the input from the item side: one model takes item features as input, while the other takes the user IDs of historical positive feedback users. We sample 300 coldstart items (viewed less than 1,000 times by users in KuaiRand) to examine the performance of the two models. The X-axis in Figure <ref type="figure" target="#fig_0">1</ref> represents the number of content features or IDs from the positive From a model perspective, a critical challenge in item cold-start recommendations arises from poor performance on cold-start items due to limited positive feedback contribution. This leads to a situation where the model assigns high scores to warm-start items and low scores to cold-start items. In Figure <ref type="figure" target="#fig_1">2</ref>, histograms of prediction scores on 500 positive and 500 negative feedback on cold-start items, along with 500 negative feedback on warm-start items, are plotted separately from the online sorting model in the pretrained dual-tower model on KuaiRand dataset. It's observed that the mean values of the prediction scores on cold-start positive samples and warm-start negative samples are very close to each other. Additionally, the overlap of the prediction distributions of cold-start positive samples and warm-start negative samples is much larger than the overlap of the predictions on cold-start positive samples and coldstart negative samples, indicating that the model parameters are predominantly influenced by warm-start items, resulting in limited personalization for cold-start items and a tendency to assign higher scores to popular items.</p><p>To address these challenges, we present a prompt-tuning method specifically designed for item cold-start recommendation, namely PROMO. To avoid data side cost and gap, we propose using significant positive feedback-which we term "pinnacle feedback"-as the prompt information. To mitigate the model side bias, we suggest constructing personalized prompt networks for each item, thereby preventing model bias towards hot items. Additionally, to enhance the pinnacle feedback based prompt tuning and to ensure fair model predictions on items, we introduce positive feedback prompt-enhanced loss and fairness-aware prompt-enhanced loss separately. These enhancements aim to improve the performance of cold-start recommendations.</p><p>The contributions of this paper are summarized as follows:</p><p>â€¢ New challenges: We identify challenges in deploying an industrially efficient prompt-tuning recommendation, including the data cost and gap (from the data perspective) and the limited personalization for cold-start items (from the model perspective).</p><p>â€¢ New technical solutions: PROMO makes the initial attempt to address both the challenges: (1) to avoid extra annotation cost and boost recommender performance, PROMO suggests utilizing pinnacle feedback for cold-start items as prompts and introduces soft prompts for flexible application in downstream tasks; (2) To avoid model bias caused by positive feedback dominance by warm-start items and improve the recommendation quality on cold-start items, we propose a personalized prompt network to adaptively update item representations. Furthermore, we introduce two types of prompt-enhanced loss to boost the performance of cold-start recommendations. â€¢ SOTA performance: Extensive experiments on four benchmark recommender datasets and deployment of PROMO in a realworld platform at Kuaishou demonstrate that PROMO consistently achieves state-of-the-art performance compared to existing baseline methods in terms of a series of recommender statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Cold-start Recommendation</head><p>In this section, we briefly introduce several hybrid recommender systems that can handle the item cold-start scenario. Item cold-start recommendation aims to provide effective recommender results to users on the cold-start items. It can be divided into several kinds. Content-based methods propose to model the distribution of content features, e.g., taking audio information to encode recommender embeddings <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">31]</ref>, or taking domain-related information for downstream tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>. Though content information relieves the data sparsity problem, it is hard to ensure the content information can always supplement the downstream tasks, because there exists a gap between the semantic information and the recommender tasks. Collaborative filtering strategies such as CBF and CF have limited scope in the cold start scenario due to lack of information, e.g., involving social relations for collaborative filtering <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>, or proposing a new similarity measure for neighborhood based CF for item cold-start scenarios <ref type="bibr" target="#b38">[38]</ref>. Meta-learning addresses the cold-start problem by learning to quickly adapt a model to new items or users with minimal data. For example, <ref type="bibr" target="#b47">[47]</ref> introduces a meta-learning strategy for item cold-start recommendations by employing deep neural networks that adapt a neural network's biases using item history. <ref type="bibr" target="#b25">[26]</ref> enhances tasks with multifaceted semantic contexts and employs a co-adaptation meta-learner to effectively address the challenges of new item recommendation. Transfer learning-based recommendation optimizes the cold-start recommendation quality by utilizing cross-domain knowledge. For example, <ref type="bibr" target="#b45">[45]</ref> learns to transfer model knowledge from rich data settings to few data settings by a meta-learner learning the model parameter shifting relations. <ref type="bibr" target="#b40">[40]</ref> utilize prompt learning mode to utilize pretrained knowledge to boost cold-start recommendations. PROMO belongs to this line of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prompt-tuning for Recommendation</head><p>Prompt-tuning, initially proposed in NLP <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>, has gained traction in recommender systems for its performance improvements, particularly in few-shot scenarios. Typically, it involves freezing the backbone model and providing downstream tasks with related tunable prompt embeddings. Recently, inspired by NLP works, researchers have introduced prompt-tuning modes to recommender systems, such as encoding user-profiles and behaviors as prompts <ref type="bibr" target="#b40">[40]</ref>, incorporating graph structures as additional prompts <ref type="bibr" target="#b44">[44]</ref>, or encoding discrete item features for prompting <ref type="bibr" target="#b21">[22]</ref>. Further, considering the relevance of few-shot learning and coldstart recommendations, <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40]</ref> have introduced prompt learning to solve the cold-start recommendation problem. However, these methods often achieve suboptimal performance in item cold-start scenarios because the corresponding prompt information, such as item profiles, lacks crucial contextual relevance to cold-start recommendations. Some research efforts propose leveraging annotated text descriptions and knowledge graphs to activate large language models for recommender tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b42">42]</ref>. While effective in igniting the backbone with extra input prompt data, these approaches are less practical in industrial settings due to the high annotation costs associated with millions or billions of new items in such scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY 3.1 Item Cold-start Problem</head><p>We focus on addressing the item cold-start problem, where new items have no or few prior interactions, like click or rating logs. The item cold-start problem requires to provide a scoring function that can accurately estimate the user's preference for a new item, thus provides effectiveness guarantee for the item cold start problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Notations &amp; Problem Formulation</head><p>In this work, we use boldface lowercase letters (e.g., ğ’†) to denote vectors, boldface uppercase letters (e.g., ğ‘¾ ) to denote matrices, and calligraphic capital letters (e.g., D) to denote sets. Let U,I denote the sets of users and items, respectively. The problem here can be formulated as a typical CTR prediction task. Each user ğ‘¢ âˆˆ U is associated with a set of items I ğ‘¢ with feedback ğ‘¦ ğ‘¢ğ‘– = 1 for ğ‘– âˆˆ I ğ‘¢ , indicating that the user has clicked on the items. e ğ‘¢ , e ğ‘– âˆˆ R ğ‘‘ are the input ID embeddings of ğ‘¢ and ğ‘–, respectively, R is the set of real numbers, and ğ‘‘ is the embedding dimension. In our scenario, given the pair of (ğ‘¢, ğ‘–), where ğ‘¢ âˆˆ U and ğ‘– âˆˆ I, our goal is to learn a function that can forecast how likely the item will be clicked by the user, i.e. Å·ğ‘¢ğ‘– , Å·ğ‘¢ğ‘– ranges from 0 to 1, indicating how likely the user will clicked on the item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED METHOD</head><p>We initiate the pre-training phase of PROMO, showcasing its adaptability across various base models. It consists of three parts.</p><p>(1) Embedding Layer. In our scenario, the input to the base model includes several item content features and categorical ID features, such as item ID. These ID features cannot be directly input into the model as they are not trainable for downstream tasks. Therefore, we adopt the widely used embedding technique to transform the</p><p>Prompt Generator â€¦ â€¦ Embedding Encoder Base Model (fixed) Item ID Sum-pooling Sum-pooling Neg Feedback Pos Feedback â„’ ğ‘ğ‘“ğ‘ğ‘’ User Tower Item Encoder â„’ ğ‘Ÿğ‘’ğ‘ â„’ ğ‘ğ‘ğ‘ğ‘’ Fusion User ID Embedding Encoder Shared Model Parameter original sparse features into low-dimensional vectors. For user ğ‘¢ âˆˆ U and item ğ‘– âˆˆ I, id embeddings e ğ‘¢ and e ğ‘– are concatenated with their other continuous features to generate the overall embeddings.</p><p>(2) Backbone. We first introduce the pre-training stage of PROMO. Following <ref type="bibr" target="#b40">[40]</ref>, we use the classical SASRec <ref type="bibr" target="#b18">[19]</ref> as our pre-training model, which can be flexibly substituted with other representation learning methods in recommendations. SASRec stacks Transformer <ref type="bibr" target="#b33">[33]</ref> blocks to encode the historical behavior sequence. For each user ğ‘¢ with behavior sequence ğ‘  ğ‘¢ , its ğ‘™-layer behavior matrix is formulated as S ğ‘™ ğ‘¢ = (e ğ‘™ ğ‘– 1 , e ğ‘™ ğ‘– 2 , . . . , e ğ‘™ ğ‘– ğ‘˜ ), where e ğ‘™ ğ‘– ğ‘˜ is the ğ‘˜-th behavior's representation of ğ‘¢ at the ğ‘™-th layer. The (ğ‘™ + 1)-layer behavior matrix is learned as follows:</p><formula xml:id="formula_0">H ğ‘™ +1 ğ‘¢ = Transformer ğ‘™ (H ğ‘™ ğ‘¢ ), ğ’‰ ğ‘¢ = ğ‘“ seq (H ğ¿ ğ‘¢ , e ğ‘¢ )<label>(1)</label></formula><p>where ğ‘“ seq is a MLP encoder, ğ’‰ ğ‘¢ is the final user representation of ğ‘¢ learned in the pre-training stage, and ğ¿ is the number of Transformer layers.</p><p>Through the base model, we can obtain the hidden representations h ğ‘¢ âˆˆ R ğ‘‘ and h ğ‘– âˆˆ R ğ‘‘ of user ğ‘¢ and item ğ‘–, respectively. Then, we can make predictions on the unobserved interaction between user ğ‘¢ and item ğ‘– as follows:</p><formula xml:id="formula_1">Å·ğ‘¢,ğ‘– = ğœ [ğ’‰ ğ‘¢ ] âŠ¤ â€¢ ğ’‰ ğ‘– ğœ<label>(2)</label></formula><p>where [â€¢] âŠ¤ represents the transpose of the matrix, ğœ represents the temperature coefficient, and ğœ is the sigmoid function.</p><p>(3) Loss. The objective function used in the base model and PROMO is both the negative log-likelihood function defined as:</p><formula xml:id="formula_2">L = - 1 ğ‘ âˆ‘ï¸ (ğ‘¢,ğ‘–,ğ‘¦) âˆˆD train ğ‘¦ ğ‘¢,ğ‘– log Å·ğ‘¢,ğ‘– + (1 -ğ‘¦ ğ‘¢,ğ‘– ) log(1 -Å·ğ‘¢,ğ‘– )<label>(3)</label></formula><p>where D train is the training set, Å· is the output representing the predicted probability whether item ğ‘– is clicked by ğ‘¢, and ğ‘¦ âˆˆ 0, 1 is the ground-truth label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prompt Generator</head><p>Existing methods face challenges in effectively prompting recommendation models in cold-start scenarios for two primary reasons. Firstly, these methods heavily rely on content features as prompt information. These content features may suffer from a semantic gap between their representation and the requirements of recommender tasks, potentially hindering their efficacy. Secondly, as experimentally illustrated in Sec.1, we argue that positive feedback information is better suited to serve as prompt information, as it offers contextual information tailored to address the scarcity of positive feedback in item cold-start scenarios. From a model perspective, the training samples of popular items, especially the positive samples, provide the majority of training data to the recommender system, resulting in the predominance of training samples from popular items. Consequently, this leads to model bias, where the model tends to assign higher scores to popular items, perpetuating their dominance in recommendations.</p><p>4.1.1 Prompt Data. Initially, we propose utilizing pinnacle feedback as input for prompting, where "pinnacle feedback" refers to users who have provided exceptionally positive feedback for a given item. This approach offers advantages on two fronts: (1) These users are crucial for the recommender system to understand the item, and</p><p>(2) by leveraging user feedback as prompt information and as part of the cold-start item representation, the user-item matching problem can be reframed as a user-user matching problem, potentially increasing the traffic distribution efficiency of cold-start items and aiding more cold-start items in transitioning into popular items.</p><p>To select appropriate positive feedback for item ğ‘– âˆˆ I, we present a criterion to measure the score of each feedback for item ğ‘–, which is also applied in our online recommender system. For each user ğ‘¢ âˆˆ U who has viewed the item ğ‘–, we consider multiple positive feedback from users (i.e., positive comments and forward) on the items to calculate preference score for selecting pinnacle feedback. This measurement standard can be flexibly modified to suit different recommender scenarios, e.g., short-video recommendation and ecommerce recommendation. Specifically, we measure the sample value ğ‘£ ğ‘¢,ğ‘– of user (feedback) ğ‘¢ to item ğ‘– using the following formula.</p><formula xml:id="formula_3">ğ‘£ ğ‘¢,ğ‘– = ğ›¼ â€¢ ğ¶ğ‘… ğ‘¢,ğ‘– + ğ›½ â€¢ ğ¼ğ‘… ğ‘¢,ğ‘–<label>(4)</label></formula><p>where ğ¶ğ‘… ğ‘¢,ğ‘– represents the staying time of the user on the item page, and ğ¼ğ‘… ğ‘¢,ğ‘– is a comprehensive score of user interaction feedback, such as liking the item or following the owner of the item. According to the above criteria, we select the top-ğ‘˜ users with the highest value as the pinnacle sample list for the item ğ‘–, that is,</p><formula xml:id="formula_4">ğ‘ƒğ‘œğ‘  ğ‘– = (ğ‘¢ ğ‘ğ‘œğ‘  1 , ğ‘¢ ğ‘ğ‘œğ‘  2 , ..., ğ‘¢ ğ‘ğ‘œğ‘  ğ‘˜ ).</formula><p>For cases where certain items lack user feedback, referring to ideas behind <ref type="bibr" target="#b2">[3]</ref>, we utilize collaborative filtering information from warm-up items to help cold-start items, we generate pseudo-pinnacle sample information from related popular items. Specifically, for a cold-start item ğ‘ âˆˆ I, we calculate item similarities between itself and popular item ğ‘ âˆˆ I using the formula:</p><formula xml:id="formula_5">ğ‘£ ğ‘,ğ‘ = h ğ‘ â€¢ h ğ‘ |h ğ‘ | 2 + |h ğ‘ | 2 -h ğ‘ â€¢ h ğ‘<label>(5)</label></formula><p>where h ğ‘ and h ğ‘ represent embeddings of items ğ‘ and ğ‘, encoded from the base model. We select the most similar popular item to item ğ‘, and apply Equation ( <ref type="formula" target="#formula_3">4</ref>) to extract related positive feedback as pseudo-prompt information.</p><p>In order to give full play to the role of the pinnacle samples, we also randomly select ğ‘˜ negative feedback of item ğ‘– (users who do not have any positive interaction with the item ğ‘–), that is, ğ‘ ğ‘’ğ‘” ğ‘– = (ğ‘¢ This process mitigates the problem of model bias, where popular items dominate model updates, as each item only updates its itempersonalized prompt networks. Furthermore, it ensures that the prompt information fully takes effect, as the personalized prompt network is independent from the original base model and solely encodes the prompt information.</p><p>(2) Complexity. PROMO is parameter-efficient, since it only needs to tune and store a few parameters compared with fine-tuning the base model. The tunable parameters in PROMO only cost in the personalized prompt network part (W ğ‘ ğ‘› ğ‘– and ğ‘ ğ‘ ğ‘› ğ‘– ), which is very efficient. In PROMO, the number of parameters to be updated merely 20.1%, 27.6%, 17.7% and 25.6% of those fine-tuning in Movie-Lens100K, MovieLens1M, KuaiRand, and TMall datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prompt Learning in PROMO</head><p>In the previous section, we obtain the positive feedback list S are fed into the personalized prompt network and the feature interaction is realized via the concatenation of the output of each layer. Take S ğ‘ğ‘œğ‘  ğ‘– as an example:</p><formula xml:id="formula_6">h ğ‘ğ‘œğ‘  ğ‘™ = ğœ W ğ‘ ğ‘™ -1 ğ‘– âŠ— (â€¢ â€¢ â€¢ (W ğ‘ 0 ğ‘– âŠ— h 0 + ğ‘ ğ‘ 0 ğ‘– ) + ğ‘ ğ‘ ğ‘™ -1 ğ‘– ,<label>(7)</label></formula><p>where âŠ— denotes the matrix multiplication, ğœ indicates the activation function, ğ‘™ represents the layers of the personalized prompt network,</p><formula xml:id="formula_7">h ğ‘ğ‘œğ‘  ğ‘™</formula><p>is the output of the prompt data fed through the prompt network, and h To fully leverage the potential of prompt information in PROMO, we introduce the pinnacle feedback prompt-enhanced loss L ğ‘ ğ‘“ ğ‘ğ‘’ and the popularity-aware prompt-enhanced loss L ğ‘ğ‘ğ‘ğ‘’ .</p><p>Per-sample Pinnacle Feedback Prompt-enhanced Loss. Persample Positive Feedback Prompt-enhanced Loss is designed to fully take advantange of high-value positive samples for cold-start items. To achieve this, for the item ğ‘– âˆˆ I, we leverage the concept of contrastive learning to widen the gap between its pinnacle and negative feedback representations. We calculate the sum of the distances between each pinnacle feedback and each negative feedback of ğ‘– as follows:</p><formula xml:id="formula_8">Î” ğ‘– = âˆ‘ï¸ ğ‘¥ âˆˆğ‘ƒğ‘œğ‘  ğ‘– âˆ‘ï¸ ğ‘¦ âˆˆğ‘ ğ‘’ğ‘” ğ‘– h ğ‘ğ‘œğ‘  ğ‘™,ğ‘¥ -h ğ‘›ğ‘’ğ‘” ğ‘™,ğ‘¦<label>(8)</label></formula><p>where we employ the ğ¿ 1 distance to calculate the discrepancy between each pinnacle feedback vector and each corresponding negative feedback vector. Furthermore, we utilize the log function to implement the loss:</p><formula xml:id="formula_9">L ğ‘ ğ‘“ ğ‘ğ‘’ = log (1 + ğ‘’ğ‘¥ğ‘ (-Î” ğ‘– ) ) .<label>(9)</label></formula><p>Optimizing L ğ‘ ğ‘“ ğ‘Ÿğ‘’ enables PROMO to indirectly memorize valuable feedback information in both the prompt network parameters and the prompt embedding. Additionally, the pair-wise L ğ‘ ğ‘“ ğ‘Ÿğ‘’ supplements order information to the base model, which is trained in a point-wise manner and lacks ordering ability. ğ‘ ğ‘™ ğ‘– <ref type="bibr" target="#b9">(10)</ref> where MLP is the Multi-layer Perceptron network. With the final embeddings, we adopt a simple but widely-used inner product model, i.e., Å·ğ‘¢,ğ‘– = e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-batch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ ğ‘¢</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>âŠ¤</head><p>â€¢ e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ ğ‘–</head><p>, to estimate the value of Å·ğ‘¢,ğ‘– , which is the interaction probability between a given pair of user and item.</p><p>Industrial recommender systems frequently exhibit biased preferences for popular items. This bias emerges because popular items contribute the majority of positive samples. Consequently, as depicted in Figure <ref type="figure" target="#fig_1">2</ref> (b), the estimated scores for cold-start items in positive samples are even lower than those for popular items in negative samples. To address this, PROMO introduces an intrabatch popularity-aware prompt-enhanced loss. Before that, for the samples (pairs of the user ğ‘¢ and the item ğ‘–) in a batch, we can get two sets Dğ‘ğ‘œğ‘ _ğ‘ğ‘œğ‘™ğ‘‘ and Dğ‘›ğ‘’ğ‘”_ğ‘¤ğ‘ğ‘Ÿğ‘š respectively based on the cold-start status of the item and the label. Here, Dğ‘ğ‘œğ‘ _ğ‘ğ‘œğ‘™ğ‘‘ is the positive sample set of cold-start items within a batch, and Dğ‘›ğ‘’ğ‘”_ğ‘¤ğ‘ğ‘Ÿğ‘š is the negative sample set of popular items within the batch. We calculate the distance between positive cold-start samples and negative popular samples within the batch and use the following loss to further push the distance to achieve fair scoring.</p><formula xml:id="formula_10">Î” ğ‘ğ‘ğ‘¡ğ‘â„ = âˆ‘ï¸ ( Å«, Ä« ) âˆˆD ğ‘ğ‘œğ‘ _ğ‘ğ‘œğ‘™ğ‘‘ âˆ‘ï¸ ( ğ‘¢, ğ‘– ) âˆˆDğ‘›ğ‘’ğ‘”_ğ‘¤ğ‘ğ‘Ÿğ‘š Å· Å«, Ä« -Å· ğ‘¢, ğ‘–<label>(11)</label></formula><formula xml:id="formula_11">L ğ‘ğ‘ğ‘ğ‘’ = log (1 + ğ‘’ğ‘¥ğ‘ (-Î” ğ‘ğ‘ğ‘¡ğ‘â„ ))<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization</head><p>To fully leverage knowledge from the pretrained model and PROMO, we directly fuse the representations from the pretrained model and PROMO by concatenating the embeddings together, thus obtaining the final embeddings. The final prediction value is estimated from the final embedding. The overall objective function is formulated as:</p><formula xml:id="formula_12">L = ğœ† 1 L ğ‘ ğ‘“ ğ‘Ÿğ‘’ + ğœ† 2 L ğ‘ğ‘ğ‘ğ‘’ + L ğ‘Ÿğ‘’ğ‘<label>(13)</label></formula><p>where ğœ† 1 and ğœ† 2 are positive coefficients serving as balancing factors for the multiple loss functions. Lğ‘Ÿğ‘’ğ‘ has the same format as Equation 3, which is important to inject representations with personalized recommender knowledge. Following the training paradigm in prompt learning <ref type="bibr" target="#b20">[21]</ref>, the base model is frozen during the optimization. Only tuning on the small set of model parameters without changing the base model is not only parameter-efficient, but it can also better leverage the pre-trained model and avoid the catastrophic forgetting problem <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we conduct an empirical assessment of our framework and present comprehensive results. Our focus lies on addressing the following six questions: Q1: Can PROMO surpass state-of-the-art baseline methods in terms of recommendation performance in cold-start scenarios across real-world datasets? Q2: What are the advantages of the prompt learning module within PROMO for enhancing downstream recommender tasks? Q3: How effective is the proposed positive feedback prompt information? Can it be replaced by other types of prompt information? Q4: How does the personalized prompt network affect the overall system performance? Q5: Does the prompt paradigm in PROMO effectively enhance the high-value positive feedback information? Q6: Does PROMO effective in providing high-quality recommender results in the industrial application?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets &amp; Baselines</head><p>Datasets. We utilize four public recommender datasets -Movie-Lens 100K &amp; 1M <ref type="bibr" target="#b16">[17]</ref> about movie recommendations, KuaiRand <ref type="bibr" target="#b11">[12]</ref> of short-video recommendations and TMall <ref type="bibr" target="#b29">[30]</ref> of E-commerce recommendations, to evaluate the effectiveness of our methods. Specifically, we preprocessing the datasets for the evaluation on cold-start performance. For all four datasets, We divide items into two groups, popular and cold-start based on their positive feedback interaction frequencies, where items with more than positive feedback interactions are popular and others are cold-start. We use of 20, 50, 50 and 20 for MovieLens 100K, MovieLens 1M, KuaiRnad-Pure and Tmall dataset, respectively. Note that the ratio of cold-start items to popular items is approximately 8:2, which is similar to the definition of long-tail items. Following the same leave-one-out technique in existing works <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b46">46]</ref>, we take the last interactive item for each user as the test data, the second-to-last as validation data, and the remaining data as training data to simulate the online recommendation environment. The detailed statistics are summarized in Table <ref type="table" target="#tab_2">1</ref>.</p><p>Baselines. In order to comprehensively assess the efficacy of the prompt tuning mode in our system (PROMO), we conduct a comparison with two approaches: Pre-train, where we directly apply the pre-trained model (SASRec <ref type="bibr" target="#b18">[19]</ref>) to the test set, and Fine-tuning, which involves tuning all parameters of the pre-trained model during the tuning phase. Besides, we compare PROMO with four popular and effective CF model: DCN <ref type="bibr" target="#b35">[35]</ref>, DeepFM <ref type="bibr" target="#b13">[14]</ref>, SASRec <ref type="bibr" target="#b18">[19]</ref>, DSSM <ref type="bibr" target="#b17">[18]</ref>, four state-of-the-art cold-start recommender models: CDN <ref type="bibr" target="#b46">[46]</ref>, DropoutNet <ref type="bibr" target="#b34">[34]</ref>, CB2CF <ref type="bibr" target="#b2">[3]</ref>, MetaEmb <ref type="bibr" target="#b27">[28]</ref>, and two prompt-tuning in recommender systems: PPR <ref type="bibr" target="#b40">[40]</ref>, PLATE <ref type="bibr" target="#b37">[37]</ref>. Our code for PROMO is available at <ref type="url" target="https://github.com/PROMOREC/PROMO">https://github.com/PROMOREC/ PROMO</ref>. The main characteristic of all baselines are listed below:</p><p>â€¢ CDN <ref type="bibr" target="#b46">[46]</ref>: CDN presents a Cross Decoupling Network (CDN) to enhance long-tail item recommendation by addressing biases in user preference predictions. â€¢ DCN <ref type="bibr" target="#b35">[35]</ref>: DCN is a novel cross network that is efficient in learning certain bounded-degree feature interactions. As it is poweful and the feature crossing manner is similar to prompt network in PROMO, we take it as one of the baseline method.</p><p>â€¢ DeepFM <ref type="bibr" target="#b13">[14]</ref>: DeepFM derives an end-to-end learning model that emphasizes both low-and highorder feature interactions by a shared input to the "wide" and "deep" parts. â€¢ SASRec <ref type="bibr" target="#b18">[19]</ref>: SASRec capture both long-short recommender interests by utilizing both long-term semantics (like an RNN) and using an attention mechanism to makes its predictions based on relatively few actions.</p><p>â€¢ DSSM <ref type="bibr" target="#b17">[18]</ref>: SASRec capture both long-short recommender interests by utilizing both long-term semantics (like an RNN) and using an attention mechanism to makes its predictions based on relatively few actions. H@5 H@10 N@5 N@10 H@5 H@10 N@5 N@10 H@5 H@10 N@5 N@10 H@5 H@10 N@5 N@10</p><p>Pre-train 14.6 26.2 9.3 13.3 27.7 42.5 18.1 22.2 13.3 25.8 7.9 11.9 3.6 7.9 2.2 3.6 Fine-tune 17.4 28.2 10.7 14.2 28.1 42.7 18.3 22.8 24.9 47.5 16.6 32.5 6.1 16.7 9.8 8.3 CDN 17.6 27.4 11.6 14.7 17.5 30.4 10.8 14.9 22.2 35.5 14.2 18.5 2.6 4.9 1.7 2.4 DCN 15.0 29.3 9.1 15.4 25.6 40.9 16.4 21.3 20.4 32.3 13.3 17.2 7.8 13.7 5.0 6.8 DeepFM 17.6 28.0 11.3 14.6 30.5 45.7 20.2 25.1 24.1 35.5 16.7 20.4 4.8 10.6 2.7 4.5 SASRec 17.4 30.2 10.8 14.9 56.9 69.2 43.4 47.6 82.5 89.3 70.5 72.7 23.1 33.5 15.8 19.6 DSSM 16.4 29.1 10.7 15.1 28.8 34.7 18.8 23.6 22.3 34.2 14.8 18.6 21.9 32.3 15.0 18.5 DropoutNet 16.1 28.6 10.2 14.2 32.2 46.9 21.1 25.8 24.9 40.7 15.4 20.5 6.4 11.2 4.6 6.2 CB2CF 15.7 26.4 9.9 13.1 19.7 34.0 11.9 16.8 17.3 31.3 10.3 14.8 3.2 6.1 1.9 2.8 MetaEmb 17.1 29.3 10.9 14.9 30.3 44.8 20.2 24.8 16.6 30.0 10.0 14.3 9.0 17.6 5.3 8.0 PPR 10.5 32.8 6.1 17.6 55.9 69.0 41.9 46.1 83.1 90.0 70.3 72.7 22.5 32.3 15.5 18.7 PLATE 15.7 26.0 9.9 13.2 24.2 39.1 15.3 20.1 17.5 30.3 10.8 15.0 3.7 7.5 2.2 3.4 PROMO 33.7 43.3 25.5 28.6 57.5 70.6 43.8 48.0 88.7 92.6 80.3 81.6 24.2 34.2 17.0 20.2</p><p>â€¢ DropoutNet <ref type="bibr" target="#b34">[34]</ref>: DropoutNet modify the learning procedure to explicitly condition the model for the missing input, and train DNNs to generalize to missing input. â€¢ CB2CF <ref type="bibr" target="#b2">[3]</ref>: CB2CF introduces the model for bridging the gap between items' CB profiles and their CF representations. It is supervised by CF information, produces significantly better results than classical CB models that use the same CB data.</p><p>â€¢ MetaEmb <ref type="bibr" target="#b27">[28]</ref>: MetaEmb trains generators by making use of previously learned ads through gradient-based meta-learning.</p><p>â€¢ PPR <ref type="bibr" target="#b40">[40]</ref>: PPR introduces prompt to pre-trained recommendation models for cold-start recommendation.</p><p>â€¢ PLATE <ref type="bibr" target="#b37">[37]</ref>: PLATE conducts prompt tuning with two novel prompt modules, capturing the distinctions among various domains and users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics &amp; Parameter Settings</head><p>For each user in the test set, we take all the items that the user has not interacted with as negative samples. For each testing useritem positive pair, we randomly sample 100 items which are not interacted by the user to generate negative pairs for evaluation, which mirrors similar settings in existing works. We evaluate the proposed PROMO with two metrics: Hitrate@K and NDCG@K. These metrics can be formulated as follows:</p><formula xml:id="formula_13">ğ»ğ‘–ğ‘¡ğ‘Ÿğ‘ğ‘¡ğ‘’@ğ¾ = ğ‘¢ ğ¾ ğ‘— =1 ğ‘Ÿğ‘’ğ‘™ ğ‘— ğ‘¢ |ğ‘‡ (ğ‘¢ ) |<label>(14)</label></formula><formula xml:id="formula_14">ğ‘ ğ·ğ¶ğº@ğ¾ = 1 |ğ‘¢ | âˆ‘ï¸ ğ‘¢ ğ·ğ¶ğº@ğ¾ ğ¼ ğ·ğ¶ğº@ğ¾ , ğ·ğ¶ğº@ğ¾ = ğ¾ âˆ‘ï¸ ğ‘— =1 2 ğ‘Ÿğ‘’ğ‘™ ğ‘— -1 ğ‘™ğ‘œğ‘” 2 ( ğ‘— + 1)<label>(15)</label></formula><p>where ğ‘…(ğ‘¢) and ğ‘‡ (ğ‘¢) represent the model recommendation set and the whole test set respectively; ğ‘Ÿğ‘’ğ‘™ ğ‘— = 0/1 indicates item at rank ğ‘— in ğ‘…(ğ‘¢) is also belonging to ğ‘‡ (ğ‘¢). IDCG is the DCG score for the most ideal ranking, which is ranking the items top down according to their real score. In our experiment, ğ¾ is separately set to be 5 and 10. A higher score indicates a model with better retrieval or ranking ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">End-to-End Comparison</head><p>To address Q1, we present the performance results of our end-toend comparison in Table <ref type="table" target="#tab_3">2</ref>. Across four recommender datasets, our system (PROMO) consistently outperforms all baseline methods across various metrics (H@1, H@5, H@10, N@1, N@5, and N@10).</p><p>Compared to cold-start baseline methods, PROMO achieves superior performance. For instance, PROMO outperforms CB2CF and MetaEmb by large margins of 37.8% and 27.2%, respectively, in HitRate@5 on the MovieLens 1M dataset. This highlights the importance of positive feedback in enhancing recommender tasks compared to content-based information. Moreover, CF-based methods generally achieve better performance than content-based methods, further underscoring the effectiveness of utilizing feedback information for recommendations. Despite this, PROMO achieves significant improvements over CF-based methods, e.g., surpassing SASRec by 16.3%, 13.1%, 14.7%, and 13.7% according to the metrics of H@5, H@10, N@5, and N@10, respectively, on the MovieLens 100K dataset. This superiority indicates the effectiveness of the proposed pinnacle feedback information for recommendations.</p><p>Additionally, it is noteworthy that the cutting-edge prompt recommender method PPR often achieves the second-best performance. This underscores the effectiveness of the prompt paradigm in addressing few-shot learning problems. However, PROMO surpasses cutting-edge prompt recommender methods like PPR and PLATE across all metrics. While PPR adopts SASRec as its backbone model and follows a two-stage training approach, PROMO's superiority is evident due to several factors. PPR's reliance on content features for prompt information may lead to suboptimal results due to the lack of utilization of positive feedback information. Furthermore, PPR's use of a shared neural network to encode prompt information may introduce model bias, favoring popular items over cold-start ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Interpretability of PROMO</head><p>To elucidate why PROMO is effective for downstream recommender tasks to answer Q2, we begin by revaluating the architecture of PROMO. As delineated in the model overview, PROMO operates as a flexible module within a two-stage framework, with the base  model held constant. We contend that PROMO is more efficacious due to the prompt embedding it generates containing aligned information pertinent to the downstream tasks. This alignment stems from the informative nature of pinnacle positive feedback, which guides the recommender system in item distribution. To illustrate this phenomenon, we visualize the item representations produced by PROMO and a baseline of DSSM model. Specifically, upon convergence on the MovieLens100K dataset, we extract the final output embeddings of items from both models and visualize all item embeddings. The left segment of Figure <ref type="figure" target="#fig_8">4</ref> depicts the node representations of the DSSM, while the right segment illustrates the embeddings from PROMO. It becomes apparent that items sharing the classes are more closely mapped together in PROMO, whereas the base model fails to implicate category information. This observation suggests that recommender systems, when better aligned with the recommendation tasks, can enrich representations with collaborative relations concerning content information, such as item class information in this instance. These learned content relations can furnish pertinent information aiding in task optimization in an end-to-end fashion. Consequently, the prompt learning module enhances the model's capacity to produce discriminative item representations by assimilating more task-relevant information from the proposed prompt optimization method in PROMO, thereby augmenting the performance of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Influence on Prompt Information</head><p>To address Q3, we conducted experiments to evaluate the effectiveness of the proposed prompt information. PROMO represents the first work to utilize pinnacle feedback as prompt information. To assess the efficacy of this approach, we replaced pinnacle feedback with commonly used prompt information from other prompt recommendation approaches. Specifically, we replaced the pinnacle feedback prompt information with item-IDs (referred to as PROMO-I), item features (referred to as PROMO-F), and both IDs and item features (referred to as PROMO-IF), while keeping other components of PROMO unchanged. We evaluated the effectiveness of prompt information on the MovieLens 100K, MovieLens 1M, and KuaiRand datasets, with results presented in Table <ref type="table" target="#tab_5">3</ref>.</p><p>It is evident that PROMO outperforms all its variants according to the HitRate and NDCG metrics. This indicates that the prompt information proposed in PROMO is more relevant in indicating user interests, thereby enhancing downstream tasks. Additionally, it's noteworthy that PROMO-I exhibits the poorest performance among the variants. This can be attributed to the fact that cold-start items scarcely provide samples for the recommender system, resulting in their ID embeddings being inadequately updated, thereby failing to provide sufficient information for model prediction. Although PROMO-IF achieves considerable performance compared to other PROMO variants, it still falls behind PROMO. For example, it lags behind by 10.6% and 11.9% according to H@10 and N@10 on MovieLens 100K, and falls short by 2.4% and 8.6% on KuaiRand according to H@10 and N@10 respectively. This underscores the significance of positive feedback information in recommendations, especially in cold-start scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Influence of Personalized Prompt Network</head><p>To address Q4, we assessed the effectiveness of the personalized prompt network by comparing the performance of related variants of PROMO. Specifically, PROMO was evaluated in the following configurations: (i) substituting the personalized prompt network with a global shared MLP prompt network (referred to as "PROMO-M"), and (ii) removing the personalized prompt network module and incorporating the positive feedback information directly as an input feature to the base model, denoted as "PROMO-T". The performance results are provided in Table <ref type="table" target="#tab_6">4</ref>.</p><p>It is evident that PROMO-M outperforms PROMO-T across all metrics in the three datasets. This superiority is attributed to the exclusive prompt network in PROMO-M, which emphasizes the importance of pinnacle feedback, thereby avoiding any reduction in effectiveness that may occur when combined with other features in PROMO-T. However, PROMO-M encounters the model bias problem, as the model parameters become dominated by popular items, leading to suboptimal performance in cold-start recommendations.</p><p>On the contrary, PROMO overcomes this issue with the personalized prompt network, which enhances the pivotal pinnacle feedback information while simultaneously avoiding the model bias problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Effectiveness of Pinnacle Feedback Enhancement</head><p>To address Q5, we evaluated whether the pinnacle feedback information is effectively retained in PROMO. We defined a memorized pinnacle feedback as a pinnacle sample accurately estimated by the model at time ğ‘¡ 0 as well as at ğ‘¡ 1 . The memory retention rate was calculated as the memorized pinnacle feedback amount at ğ‘¡ 1 compared to the total pinnacle amount truly estimated at ğ‘¡ 0 . We randomly selected 500 pinnacle feedback samples and assessed the memory retention rate with increasing accumulation of negative feedback on the corresponding items. For comparison, we selected three representative baseline methods due to page limitations: DeepFM from CF-based methods, MetaEmb from contentbased methods, and PPR from prompt learning methods. The memory retention rate comparison is depicted in Fig. <ref type="figure" target="#fig_9">5</ref>. It was observed that as the cumulative negative feedback amount increased, the memory retention rate declined in all three baseline methods. The decline was more pronounced with higher cumulative negative feedback amounts, particularly evident in the DeepFM baseline. This phenomenon can be attributed to CF-based methods' focus on recent feedback information, leading them to gradually forget earlier samples under online streaming training modes, resulting in the forgetting of original pinnacle feedback information. In comparison, PROMO achieved a high memory retention rate, demonstrating its effectiveness in memorizing pinnacle feedback information and facilitating recommender performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Production A/B Test</head><p>To answer Q6, we assess the performance of PROMO through its deployment on a popular short-video sharing platform in Chinawhere users can upload their short-videos and enjoy short-videos by other users, using the standard A/B testing methodology. We randomly divided users into two groups for online evaluation, with each group comprising more than 30 million users to ensure statistical significance. The sole distinction between these groups lies in the cascading process for online serving: users in the first group experience the recommender system that incorporates PROMO, while users in the second group are exposed to a online baseline method, which is similar to the SASRec baseline method.</p><p>The online serving performance is evaluated using metrics that consider user engagement, such as Click Rate, Video Play Time, Video Like and Video Collecting. The first two metrics reflect the user's implicit satisfaction, while the latter two reflect the user's explicit preference expressed through behavior.</p><p>The comparison was monitored over 14 consecutive days, and the average performance for item cold-start recommendations is presented in Table <ref type="table" target="#tab_7">5</ref>. We observed a consistent growth trend in both the implicit feedback and explicit feedback. For example, PROMO gain the explicit user feedback on video like of 3.9% lifting, along with the implicit user feedback of videp play time of 4.8% lifting. demonstrates that PROMO provides users with more satifactory recommender results. The online evaluation in cold-start recommendation settings highlights the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Item cold-start challenge plays a pivotal role in the success of online recommender systems. Prompt learning, has been adapted for recommender systems to tackle similar hurdles. However, prevailing methods often rely on content-based properties or text descriptions for prompting, potentially falling short due to semantic gaps with downstream tasks. We advocate for leveraging positive feedback as a more task-relevant prompt for recommendations. This is especially crucial for cold-start items, which grapple with limited positive feedback. Moreover, the dominance of positive feedback from popular items introduces model bias, where these items receive higher scores from the recommender system. To combat these challenges, we propose leveraging high-value positive feedback, referred to as pinnacle feedback, as prompt information. Our investigation into the efficacy of pinnacle feedback as prompt data for cold-start items, along with the development of a prompt generator to acquire both pinnacle and negative feedback prompt information. Furthermore, we introduce item-wise personalized prompt networks to mitigate model bias. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Additionally, PROMO has been successfully deployed on a industrial recommender system in China, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The model accuracy on cold-start items with itemside taking different inputs for representations encoding.The positive feedback provides more task-relevant information as the model with positive feedback information gains higher accuracy on user preference.</figDesc><graphic coords="2,77.86,83.77,192.12,144.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The histogram of prediction values on cold-start items and warm-start items. The X-axis represents the prediction value, and the Y-axis represents the sample amount of the corresponding range of scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Overview of PROMO. PROMO utilizes item features and pinnacle feedback as the prompt information to generate prompt embedding and personalized prompt network, then PROMO optimizes the prompt embedding and eliminates the model bias by the pinnacle feedback prompt-enhance loss and the popularity-aware prompt-enhanced loss separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>ğ‘›ğ‘’ğ‘” 1 ,</head><label>1</label><figDesc>ğ‘¢ ğ‘›ğ‘’ğ‘” 2 , ..., ğ‘¢ ğ‘›ğ‘’ğ‘” ğ‘˜ ), as the input prompt information. Based on the pre-trained Embedding Layer in the base model, we use the look-up operation to convert ğ‘ƒğ‘œğ‘  ğ‘– and ğ‘ ğ‘’ğ‘” ğ‘– into embedding representations S ğ‘ğ‘œğ‘  ğ‘– = (e ğ‘ğ‘œğ‘  ğ‘¢ 1 , e ğ‘ğ‘œğ‘  ğ‘¢ 2 , ..., e ğ‘ğ‘œğ‘  ğ‘¢ ğ‘˜ ) and S ğ‘›ğ‘’ğ‘” ğ‘– = (e ğ‘›ğ‘’ğ‘” ğ‘¢ 1 , e ğ‘›ğ‘’ğ‘” ğ‘¢ 2 , ..., e ğ‘›ğ‘’ğ‘” ğ‘¢ ğ‘˜ ). 4.1.2 Personalized Prompt Network. (1) Model Architecture. From the model perspective, we use the prompt generator to generate ğ‘™ independent sets of embeddings e ğ‘ 0 ğ‘– âˆˆ R ğ‘‘ ğ‘ 0 , e ğ‘ 1 ğ‘– âˆˆ R ğ‘‘ ğ‘ 1 , â€¢ â€¢ â€¢ , e ğ‘ ğ‘™ ğ‘– âˆˆ R ğ‘‘ ğ‘ ğ‘™ , for cold start item ğ‘– âˆˆ I, which are different from the Embedding Layer in the base model. For the specific ğ‘›-th item embedding e ğ‘ ğ‘› ğ‘– , we reshape and split them into the weight matrix and bias vector for the personalized prompt network. This process for e ğ‘ ğ‘› ğ‘– can be formulated as: e denote the weight and bias of ğ‘›-th layer of the personalized prompt networks, // indicates the concatenation operation, |â€¢| gets the size of the variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>of the personalized prompt network for the cold-start item ğ‘– âˆˆ I based on the prompt generator. Next, S ğ‘ğ‘œğ‘  ğ‘– and S ğ‘›ğ‘’ğ‘” ğ‘–</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Popularity-aware Prompt-enhanced Loss. The final representation of the item ğ‘– âˆˆ I is composed of the output h ğ‘– of the base model, the average of its positive feedback list S ğ‘ğ‘œğ‘  ğ‘– , and the last layer of personalized prompt network embedding e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>The item representations obtained after training in the proposed PROMO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The visualization of item representation involves the assignment of class labels to nodes through the application of K-means clustering on the original input data. The nodes are differentiated into various classes through the use of distinct colors. The representation of nodes is visualized utilizing t-SNE [32].</figDesc><graphic coords="8,189.11,89.34,98.54,97.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The memory retention rate corresponds with the accumulative negative feedback amount on MovieLens 1M dataset. Each bar represents the memory retention rate correponds to a certain negative feedback amount.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Overview of the dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">#Features</cell></row><row><cell>Dataset</cell><cell cols="3">#Users #Items #Ratings</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">User Item</cell></row><row><cell>MovieLens 100K</cell><cell>943</cell><cell>1,682</cell><cell>100,000</cell><cell>23</cell><cell>18</cell></row><row><cell>MovieLens 1M</cell><cell>6,040</cell><cell>3,706</cell><cell>1,000,209</cell><cell>23</cell><cell>18</cell></row><row><cell>KuaiRand-Pure</cell><cell>27,285</cell><cell>7,583</cell><cell>2,599,187</cell><cell>25</cell><cell>55</cell></row><row><cell>TMall</cell><cell>52,797</cell><cell>22,955</cell><cell>6,330,878</cell><cell>2</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overall Performance Comparison in Item Cold-Start Scenario. The best-performing results are highlighted in bold, while the second-best performance is indicated with underlines.</figDesc><table><row><cell>MovieLens 100K</cell><cell>MovieLens 1M</cell><cell>KuaiRand</cell><cell>TMall</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The test accuracy of variants of PROMO to examine the effectiveness of pinnacle feedback prompt information.</figDesc><table><row><cell></cell><cell cols="4">MovieLens 100K MovieLens 1M</cell><cell cols="2">KuaiRand</cell></row><row><cell></cell><cell cols="6">H@10 N@10 H@10 N@10 H@10 N@10</cell></row><row><cell>PROMO-I</cell><cell>34.8</cell><cell>18.3</cell><cell>68.8</cell><cell>45.9</cell><cell>90.0</cell><cell>72.9</cell></row><row><cell>PROMO-F</cell><cell>35.1</cell><cell>17.6</cell><cell>69.1</cell><cell>46.1</cell><cell>90.1</cell><cell>73.1</cell></row><row><cell>PROMO-IF</cell><cell>35.7</cell><cell>18.7</cell><cell>69.2</cell><cell>46.3</cell><cell>90.2</cell><cell>73.0</cell></row><row><cell>PROMO</cell><cell>43.3</cell><cell>28.6</cell><cell>70.6</cell><cell>48.0</cell><cell>92.6</cell><cell>81.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The performance on MovieLens 100K, MovieLens 1M and KuaiRand to illustrate the effectiveness of personalized prompt network in PROMO.</figDesc><table><row><cell></cell><cell cols="4">MovieLens 100K MovieLens 1M</cell><cell cols="2">KuaiRand</cell></row><row><cell></cell><cell cols="6">H@10 N@10 H@10 N@10 H@10 N@10</cell></row><row><cell>PROMO-M</cell><cell>35.7</cell><cell>22.9</cell><cell>69.4</cell><cell>47.6</cell><cell>90.3</cell><cell>76.6</cell></row><row><cell>PROMO-T</cell><cell>28.7</cell><cell>14.6</cell><cell>62.8</cell><cell>42.9</cell><cell>89.4</cell><cell>73.1</cell></row><row><cell>PROMO</cell><cell>43.3</cell><cell>28.6</cell><cell>70.6</cell><cell>48.0</cell><cell>92.6</cell><cell>81.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Online cold-start recommendation evaluation.</figDesc><table><row><cell></cell><cell cols="4">Click Rate Video Play Time Video Like Video Collecting</cell></row><row><cell>PROMO</cell><cell>+3.2 %</cell><cell>+4.8%</cell><cell>+3.9%</cell><cell>+4.0 %</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is partially sponsored by <rs type="funder">NSFC</rs> (<rs type="grantNumber">62032003</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Yq3y8UG">
					<idno type="grant-number">62032003</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain analysis of information extraction techniques</title>
		<author>
			<persName><forename type="first">Talha</forename><surname>Mahboob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alam</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mazhar</forename><surname>Javed Awan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Multidiscip. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Corporate bankruptcy prediction: An approach towards better corporate world</title>
		<author>
			<persName><forename type="first">Talha</forename><surname>Mahboob Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Shaukat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubbashar</forename><surname>Mushtaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasir</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Khushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhuai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Wahab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1731" to="1746" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CB2CF: a neural multiview content-to-collaborative filtering model for completely cold item recommendations</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eylon</forename><surname>Yogev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="228" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic audio content-based music recommendation and visualization based on user preference examples</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MartÃ­n</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferdinand</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>XambÃ³</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><surname>GÃ³mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="13" to="33" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-task item-attribute graph pre-training for strict cold-start item recommendation</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Recommender Systems</title>
		<meeting>the 17th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="322" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Win-win: a privacy-preserving federated framework for dual-target cross-domain recommendation</title>
		<author>
			<persName><forename type="first">Gaode</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yantong</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4149" to="4156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring Periodicity and Interactivity in Multi-Interest Framework for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Gaode</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1426" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning audio embeddings with user listening data for content-based music recommendation</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beici</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3015" to="3019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Namazifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.14469</idno>
		<title level="m">Inducer-tuning: Connecting Prefix-tuning and Adaptertuning</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Content-based multimedia recommendation systems: definition and application domains</title>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Deldjoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabirella</forename><surname>Pasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Italian Information Retrieval Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Openprompt: An open-source framework for promptlearning</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01998</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">KuaiRand: An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos</title>
		<author>
			<persName><forename type="first">Chongming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3953" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Proceedings of the 16th ACM Conference on Recommender Systems</title>
		<meeting>the 16th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="299" to="315" />
		</imprint>
	</monogr>
	<note>Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A multi-strategy-based pre-training method for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pretraining graph neural networks for cold-start users and items representation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm transactions on interactive intelligent systems (tiis)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on data mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the national academy of sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Personalized prompt learning for explainable recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<title level="m">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty-aware Consistency Learning for Cold-Start Item Recommendation</title>
		<author>
			<persName><forename type="first">Taichi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2466" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-learning on heterogeneous information networks for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1563" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recent advances in natural language processing via large pre-trained language models: A survey</title>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayley</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elior</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Pouran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilana</forename><surname>Heintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Warm up cold-start advertisements: Improving ctr predictions via learning to learn id embeddings</title>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuokai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
	<note>Pingzhong Tang, and Qing He</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Social collaborative filtering for cold-start recommendations</title>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darius</forename><surname>Braziunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender systems</title>
		<meeting>the 8th ACM Conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="345" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m">The TMall Team</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<ptr target="https://tianchi.aliyun.com/dataset/42" />
		<title level="m">The Tmall Dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropoutnet: Addressing cold start in recommender systems</title>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Poutanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards unified conversational recommender systems via knowledge-enhanced prompt learning</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1929" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PLATE: A Prompt-Enhanced Paradigm for Multi-Scenario Recommendations</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qidong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanshuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collaborative filtering and deep learning based recommendation system for cold start items</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuoyin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Xuansheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huachi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.17256</idno>
		<title level="m">Towards Personalized Cold-Start Recommendation with Prompts</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Personalized Prompt for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Uprec: User-aware pre-training for recommender systems</title>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10989</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Lanling</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingqian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04997</idno>
		<title level="m">Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contrastive graph prompttuning for cross-domain recommendation</title>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Knowledge Prompt-tuning for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Jianyang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6451" to="6461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A model of two tales: Dual transfer learning framework for improved long-tail item recommendation</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the web conference 2021</title>
		<meeting>the web conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2220" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Empowering Long-tail Item Recommendation through Cross Decoupling Network (CDN)</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5608" to="5617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to warm up cold item embeddings for coldstart recommendation with meta scaling and shifting networks</title>
		<author>
			<persName><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaikai</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1167" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
