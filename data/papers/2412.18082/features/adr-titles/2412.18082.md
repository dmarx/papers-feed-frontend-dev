- Decision to utilize prompt-tuning for cold-start recommendations
- Choice of pinnacle feedback as prompt information
- Design of item-wise personalized prompt networks
- Implementation of positive feedback prompt-enhanced loss
- Introduction of fairness-aware prompt-enhanced loss
- Selection of datasets for experimental validation
- Decision to deploy PROMO in a real-world application
- Choice of evaluation metrics for performance assessment
- Strategy for addressing model bias towards warm-start items
- Approach to mitigate data side costs and gaps
- Decision to focus on zero-shot and few-shot learning scenarios
- Choice of model architecture for prompt encoding
- Decision to conduct extensive experiments across multiple datasets
- Approach to handle semantic gaps in prompt information
- Strategy for optimizing parameter efficiency in prompt-tuning
- Decision to compare against state-of-the-art methods in experiments