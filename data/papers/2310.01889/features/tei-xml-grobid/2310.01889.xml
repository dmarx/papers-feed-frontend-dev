<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ring Attention with Blockwise Transformers for Near-Infinite Context</title>
				<funder ref="#_q6GxUf8">
					<orgName type="full">Office of Naval Research</orgName>
				</funder>
				<funder>
					<orgName type="full">Jax developers</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-11-27">27 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
							<email>hao.liu@cs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<title level="a" type="main">Ring Attention with Blockwise Transformers for Near-Infinite Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-27">27 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">578F3ED19C2AA31411C3072159824AF2</idno>
					<idno type="arXiv">arXiv:2310.01889v4[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance. 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b36">[37]</ref> have become the backbone of many state-of-the-art AI systems that have demonstrated impressive performance across a wide range of AI problems. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. However, scaling up the context length of Transformers is a challenge <ref type="bibr" target="#b28">[29]</ref>, since the inherited architecture design of Transformers, i.e. the self-attention has memory cost quadratic in the input sequence length, which makes it challenging to scale to longer input sequences. Large context Transformers are essential for tackling a diverse array of AI challenges, ranging from processing books and high-resolution images to analyzing long videos and complex codebases. They excel at extracting information from the interconnected web and hyperlinked content, and are crucial for handling complex scientific experiment data. There have been emerging use cases of language models with significantly expanded context than before: GPT-3.5 <ref type="bibr" target="#b31">[32]</ref> with context length 16K, GPT-4 <ref type="bibr" target="#b28">[29]</ref> with context length 32k, MosaicML's MPT <ref type="bibr" target="#b24">[25]</ref> with context length 65k, and Anthropic's Claude <ref type="bibr" target="#b0">[1]</ref> with context length 100k.</p><p>Driven by the significance, there has been surging research interests in reducing memory cost. One line of research leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix <ref type="bibr" target="#b23">[24]</ref> which has led to the development of blockwise computation of self-attention and feedforward <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref> without making approximations. Despite the reduced memory, a significant challenge still arises from storing the output of each layer. This necessity arises from self-attention's inherent nature, involving interactions among all elements (n to n interactions). The subsequent layer's self-attention relies on accessing all of the prior layer's outputs. Failing to do so would increase computational costs cubically, as every output must be recomputed for each sequence element, rendering it impractical for longer sequences.  <ref type="bibr" target="#b36">[37]</ref>, memory efficient transformers <ref type="bibr" target="#b29">[30]</ref>, and memory efficient attention and feedforward (blockwise parallel transformers) <ref type="bibr" target="#b22">[23]</ref>. Our proposed approach Ring Attention allows training up to device count times longer sequence than baselines and enables the training of sequences that exceed millions in length without making approximations nor adding any overheads to communication and computation.</p><p>These components facilitate the efficient capture of long-range dependencies between input tokens, and enable scalability through highly parallel computations. To put the memory demand in perspective, even when dealing with a batch size of 1, processing 100 million tokens requires over 1000GB of memory for a modest model with a hidden size of 1024. This is much greater than the capacity of contemporary GPUs and TPUs, which typically have less than 100GB of high-bandwidth memory (HBM).</p><p>To tackle this challenge, we make a key observation: by performing self-attention and feedforward network computations in a blockwise fashion <ref type="bibr" target="#b22">[23]</ref>, we can distribute sequence dimensions across multiple devices, allowing concurrent computation and communication. This insight stems from the fact that when we compute the attention on a block-by-block basis, the results are invariant to the ordering of these blockwise computations. Our method distributes the outer loop of computing blockwise attention among hosts, with each device managing its respective input block. For the inner loop, every device computes blockwise attention and feedforward operations specific to its designated input block. Host devices form a conceptual ring, where during the inner loop, each device sends a copy of its key-value blocks being used for blockwise computation to the next device in the ring, while simultaneously receiving key-value blocks from the previous one. As long as block computations take longer than block transfers, overlapping these processes results in no added overhead compared to standard transformers. The use of a ring topology for computing self-attention has also been studied in prior work <ref type="bibr" target="#b20">[21]</ref> but it incurs non-overlapped communication overheads similar to sequence parallelism, making it infeasible for large context sizes. Our work utilizes blockwise parallel transformers <ref type="bibr" target="#b22">[23]</ref> to substantially reduce memory costs, enabling zero-overhead scaling of context size across tens of millions of tokens during both training and inference, and allowing for the use of an arbitrarily large context size. Since our approach overlaps the communication of key-value blocks between hosts in a ring through blockwise computation of transformers, we name it Ring Attention with Blockwise Parallel Transformers (Ring Attention).</p><p>We evaluate the effectiveness of our approach on language modeling benchmarks. Our experiments show that Ring Attention can reduce the memory requirements of Transformers, enabling us to train more than 500 times longer sequence than prior memory efficient state-of-the-arts and enables the training of sequences that exceed 100 million in length without making approximations to attention. Importantly, Ring Attention eliminates the memory constraints imposed by individual devices, empowering the training and inference of sequences with lengths that scale in proportion to the number of devices, essentially achieving near-infinite context size.</p><p>Our contributions are twofold: (a) proposing a memory efficient transformers architecture that allows the context length to scale linearly with the number of devices while maintaining performance, eliminating the memory bottleneck imposed by individual devices, and (b) demonstrating the effectiveness of our approach through extensive experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Large Context Memory Constraint</head><p>Given input sequences Q, K, V ∈ R s×d where s is the sequence length and d is the head dimension. We compute the matrix of outputs as:</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax( QK T √ d )V,</formula><p>where softmax is applied row-wise. Each self-attention sub-layer is accompanied with a feedforward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.</p><formula xml:id="formula_1">FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 .</formula><p>Blockwise Parallel Transformers. Prior state-of-the-arts have led to substantial reductions in memory utilization, achieved through innovative techniques that enable attention computation without full materialization by computing attention in a block by block manner <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>. These advancements lowered the memory overhead of attention to 2bsh bytes per layer, where b represents the batch size, s denotes the sequence length, and h stands for the hidden size of the model. To further reduce memory usage, blockwise parallel transformer (BPT) <ref type="bibr" target="#b22">[23]</ref> introduced a strategy where the feedforward network associated with each self-attention sub-layer is computed in a block-wise fashion. This approach effectively limits the maximum activation size of feedforward network from 8bsh to 2bsh.</p><p>For a more detailed analysis of memory efficiency, please refer to the discussion provided therein. In summary, the state-of-the-art transformer layer's memory cost of activation is 2bsh.</p><p>Large Output of Each Layer. While BPT significantly reduces memory demand in Transformers, it still presents a major challenge for scaling up context length because it requires storing the output of each layer. This storage is crucial due to the inherent nature of self-attention, which involves interactions among all elements (n to n interactions). Without these stored outputs, the subsequent layer's self-attention becomes computationally impractical, necessitating recomputation for each sequence element. To put it simply, processing 100 million tokens with a batch size of 1 requires over 1000GB of memory even for a modest model with a hidden size of 1024. In contrast, modern GPUs and TPUs typically provide less than 100GB of high-bandwidth memory (HBM), and the prospects for significant HBM expansion are hindered by physical limitations and high manufacturing costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ring Attention with Blockwise Parallel Transformers</head><p>Our primary objective is to eliminates the memory constraints imposed by individual devices by efficiently distribute long sequences across multiple hosts without adding overhead. To achieve this goal, we propose an enhancement to the blockwise parallel transformers (BPT) framework <ref type="bibr" target="#b22">[23]</ref>.</p><p>When distributing an input sequence across different hosts, each host is responsible for running one element of the outer loop of blockwise attention corresponding to its designated block, as well as the feedforward network specific to that block. These operations do not necessitate communication with other hosts. However, a challenge arises in the inner loop, which involves key-value block interactions that require fetching blocks from other hosts. Since each host possesses only one key-value block, the naive approach of fetching blocks from other hosts results in two significant issues. Firstly, it introduces a computation delay as the system waits to receive the necessary key-value blocks. Secondly, the accumulation of key-value blocks leads to increased memory usage, which defeats the purpose of reducing memory cost.</p><p>Ring-Based Blockwise Attention. To tackle the aforementioned challenges, we leverage the permutation invariance property of the inner loop's key-value block operations. This property stems from the fact that the self-attention between a query block and a group of key-value blocks can be computed in any order, as long as the statistics of each block are combined correctly for rescaling. We leverage this property by conceptualizing all hosts as forming a ring structure: host-1, host-2, ..., host-N . As we compute blockwise attention and feedforward, each host efficiently coordinates by concurrently sending key-value blocks being used for attention computation to the next host while receiving key-value blocks from the preceding host, effectively overlapping transferring of blocks with blockwise computation. Concretely, for any host-i, during the computation of attention between its query block and a key-value block, it concurrently sends key-value blocks to the next host-(i + 1) while receiving key-value blocks from the preceding host-(i -1). If the computation time exceeds the time required for transferring key-value blocks, this results in no additional communication cost. This overlapping mechanism applies to both forward and backward passes of our approach since the same operations and techniques can be used. Prior work has also proposed leveraging a ring topology to compute self-attention <ref type="bibr" target="#b20">[21]</ref>, aiming to reduce communication costs. Our work differs by utilizing blockwise parallel transformers to substantially reduce memory costs. As we show in the next section, this enables zero-overhead scaling of context size during both training and inference and allows arbitrarily large context size.</p><p>Arithmetic Intensity Between Hosts. In order to determine the minimal required block size to overlap transferring with computation, assume that each host has F FLOPS and that the bandwidth between hosts is denoted as B. It's worth noting that our approach involves interactions only with the immediately previous and next hosts in a circular configuration, thus our analysis applies to both GPU all-to-all topology and TPU torus topology. Let's consider the variables: block size denoted as c and hidden size as d. When computing blockwise self-attention, we require 2dc 2 FLOPs for calculating attention scores using queries and keys, and an additional 2dc 2 FLOPs for multiplying these attention scores by values. In total, the computation demands amount to 4dc 2 FLOPs. We exclude the projection of queries, keys, and values, as well as blockwise feedforward operations, since they only add compute complexity without any communication costs between hosts. This simplification leads to more stringent condition and does not compromise the validity of our approach. On the communication front, both key and value blocks require a total of 2cd bytes. Thus, the combined communication demand is 4cd bytes. To achieve an overlap between communication and Memory Requirement. A host needs to store multiple blocks, including one block size to store the current query block, two block sizes for the current key and value blocks, and two block sizes for receiving key and value blocks. Furthermore, storing the output of blockwise attention and feedforward necessitates one block size, as the output retains the shape of the query block. Therefore, a total of six blocks are required, which translates to 6bch bytes of memory. It's worth noting that the blockwise feedforward network has a maximum activation size of 2bch <ref type="bibr" target="#b22">[23]</ref>. Consequently, the total maximum activation size remains at 6bch bytes. Table <ref type="table" target="#tab_0">1</ref> provides a detailed comparison of the memory costs between our method and other approaches. Notably, our method exhibits the advantage of linear memory scaling with respect to the block size c, and is independent of the input sequence length s.</p><p>Our analysis shows that the model needs to have a sequence length of s = 6c, which is six times the minimal block size. Requirements for popular computing servers are shown in Table <ref type="table" target="#tab_1">2</ref>. The required minimal sequence length (rightmost column) for each host varies between 6K and 10K, and the minimal block size (second-to-rightmost column) for each host is around 1K for TPUs and GPUs with high bandwidth interconnect. For GPUs connected via InfiniBand, which offers lower bandwidth, the requirements are more strict. These requirements are easy to meet with parallelism such as data and tensor parallelism and memory efficient blockwise attention and feedforward <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>, which we will show in experiment Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm and Implementation</head><p>. Algorithm 1 provides the pseudocode of the algorithm. Ring Attention is compatible with existing code for memory efficient transformers: Ring Attention just needs to call whatever available memory efficient computation locally on each host, and overlap the communication of key-value blocks between hosts with blockwise computation. We use collective operation jax.lax.ppermute to send and receive key value blocks between nearby hosts. A Jax implementation is provided in Appendix A. Algorithm 1 Reducing Transformers Memory Cost with Ring Attention. Required: Input sequence x. Number of hosts N h . Initialize Split input sequence into N h blocks that each host has one input block. Compute query, key, and value for its input block on each host. for Each transformer layer do for count = 1 to N h -1 do for For each host concurrently. do Compute memory efficient attention incrementally using local query, key, value blocks. Send key and value blocks to next host and receive key and value blocks from previous host. end for end for for For each host concurrently. do Compute memory efficient feedforward using local attention output. end for end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Setting</head><p>We evaluate the impact of using Ring Attention in improving Transformer models by benchmarking maximum sequence length and model flops utilization.</p><p>Model Configuration. Our study is built upon the LLaMA architecture, we consider 3B, 7B, 13B, and 30B model sizes in our experiments.</p><p>Baselines. We evaluate our method by comparing it with vanilla transformers <ref type="bibr" target="#b36">[37]</ref> which computes self-attention by materializing the attention matrix and computes the feedforward network normally, transformers with memory efficient attention <ref type="bibr" target="#b29">[30]</ref> and its efficient CUDA implementation <ref type="bibr" target="#b8">[9]</ref>, and transformers with both memory efficient attention and feedforward <ref type="bibr" target="#b22">[23]</ref>.</p><p>Training Configuration. For all methods, we apply full gradient checkpointing <ref type="bibr" target="#b4">[5]</ref> to both attention and feedforward, following prior works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref>. The experiments are on both GPUs and TPUs. For GPUs, we consider both single DGX A100 server with 8 GPUs and distributed 32 A100 GPUs. We also experiment with TPUs, from older generations TPUv3 to newer generations of TPUv4 and TPUv5e. We note that all of our results are obtained using full precision instead of mixed precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In our experiments, our primary objective is to comprehensively evaluate the performance of Ring Attention across multiple key metrics, including maximum supported sequence length within accelerator memory, model flops utilization, and throughput. We compare Ring Attention's performance with several baseline models , including the vanilla transformers <ref type="bibr" target="#b36">[37]</ref>, transformers with memory efficient attention <ref type="bibr" target="#b29">[30]</ref>, and transformers with both memory efficient attention and feedforward <ref type="bibr" target="#b22">[23]</ref>, across different model sizes and accelerator configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluating Max Context Size</head><p>We evaluate maximum supported context length using fully sharded tensor parallelsim (FSDP) <ref type="bibr" target="#b10">[11]</ref> which is widely used in prior end-to-end training <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12]</ref>. We note that no tensor parallelism is considered in our evaluations since our approach is independent of tensor parallelism. Practitioners can combine our method with tensor parallelism, which we will show in Section 5.2. Using FSDP allows us to set the same batch size in tokens for baselines and our approach, ensuring a fair comparison. Concretely, on n devices, FSDP is used to shard the model for baselines, which gives a sequence length of l. The total batch size in tokens is nl. We utilize FSDP along with Ring Attention to extend the sequence length to nl m and m sequences. This means that the total batch size in tokens remains the same, but Ring Attention enables a significantly larger context size. Table <ref type="table">3</ref> summarizes the results of our experiments.</p><p>Table <ref type="table">3</ref>: The maximum context length supported in end-to-end training using fully sharded data parallelism and various transformers architectures. We show different model sizes and accelerators. Baselines are vanilla transformer <ref type="bibr" target="#b36">[37]</ref>, transformer with memory efficient attention <ref type="bibr" target="#b29">[30]</ref>, and transformer with memory efficient attention and feedforward <ref type="bibr" target="#b22">[23]</ref>. The context size is reported in tokens (1e3). Our Ring Attention substantially outperforms baselines and enables training sequences that are up to device count times longer than prior state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max</head><p>context size supported (×1e3) Vanilla Memory Efficient Attn Memory Efficient Attn and FFN Ring Attention (Ours) Ours vs SOTA 8x A100 NVLink 3B 4 32 64 512 8x 7B 2 16 32 256 8x 13B 2 4 16 128 8x 32x A100 InfiniBand 7B 4 64 128 4096 32x 13B 4 32 64 2048 32x TPUv3-512 7B 1 4 8 2048 256x 13B 1 2 8 1024 128x TPUv4-1024 3B 8 16 32 16384 512x 7B 4 8 16 8192 512x 13B 4 8 16 4096 256x 30B 2 4 8 2048 256x TPUv5e-256 3B 4 8 32 4096 128x 7B 2 8 16 2048 128x Our Ring Attention model consistently surpasses baselines, delivering superior scalability across diverse hardware setups. For example, with 32 A100 GPUs, we achieve over 1 million tokens in context size for 7B model, a 32 times improvement over previous best. Furthermore, when utilizing larger accelerators like TPUv4-512, Ring Attention enables a 256 times increase in context size, allows training sequences of over 30 million tokens. Furthermore, our Ring Attention model scales linearly with the number of devices, as demonstrated by the 8x improvement over previous best on 8 A100 and the 256x improvement on TPUv3-512. If a model can be trained with context size s on n</p><p>GPUs using the blockwise attention and feedforward, with our Ring Attention approach, it becomes possible to train a model with a context size of ns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating Model Flops Utilization</head><p>We evaluate the model flops utilization (MFU) of Ring Attention in standard training settings using fully sharded data parallelism(FSDP) <ref type="bibr" target="#b10">[11]</ref> and tensor parallelism following LLaMA and OpenLLaMA <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12]</ref> with Jax SPMD. The batch size in tokens are 2M on 8/32x A100 and 4M on TPUv4-256. Our goal is investigating the impact of model size and context length on MFU, a critical performance metrics while highlighting the benefits of our approach. Table <ref type="table" target="#tab_5">5</ref>.1 presents the results of our experiments on MFU for different model sizes and context lengths. We present the achieved MFU using state-of-the-art memory efficient transformers BPT <ref type="bibr" target="#b22">[23]</ref>, compare it to our anticipated MFU based on these results, and demonstrate the actual MFU obtained with our approach (Ring Attention).</p><p>For fair comparison, both BPT and our approach are based on the same BPT implementation on both GPUs and TPUs.</p><p>Ring Attention trains much longer context sizes for self-attention, resulting in higher self-attention FLOPs compared to baseline models. Since self-attention has a lower MFU than feedforward, Ring Attention is expected to have a lower MFU than the baseline models. Our method offers a clear  <ref type="table" target="#tab_5">5</ref>.1, when comparing our approach to prior state-of-the-arts, it is evident that we can train very large context models without compromising MFU or throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Impact on In Context RL Performance</head><p>We present results of applying Ring Attention for learning trial-and-error RL experience using Transformers. We report our results in Table <ref type="table" target="#tab_5">5</ref>, where we evaluate our proposed model on the ExoRL benchmark across six different tasks. On ExoRL, we report the cumulative return, as per ExoRL <ref type="bibr" target="#b38">[39]</ref>. We compare BC, DT <ref type="bibr" target="#b3">[4]</ref>, AT <ref type="bibr" target="#b21">[22]</ref>, and AT with memory efficient attention <ref type="bibr" target="#b29">[30]</ref> (AT+ME), AT with blockwise parallel transformers <ref type="bibr" target="#b22">[23]</ref> (AT+BPT), and AT with our Ring Attention (AT+Ring Attention). The numbers of BC, DT, AT are from the ExoRL and AT paper. AT + Ring Attention numbers are run by ourselves. Since the ExoRL data is highly diverse, having been collected using unsupervised RL <ref type="bibr" target="#b18">[19]</ref>, it has been found that TD learning performs best, while behavior cloning struggles <ref type="bibr" target="#b38">[39]</ref>. AT <ref type="bibr" target="#b21">[22]</ref> shows that conditioning Transformer on multiple trajectories with relabeled target return can achieve competitive results with TD learning. For more details, please refer to their papers. We are interested in applying Ring Attention to improve the performance of AT by conditioning on a larger   <ref type="table" target="#tab_5">5</ref> show that, by scaling up the sequence length (number of trajectories), AT + Ring Attention consistently outperforms oringal AT with BPT across all six tasks, achieving a total average return of 113.66 compared to the AT with BPT model's total average return of 111.13. The results show that the advantage of Ring Attention for training and inference with long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact on LLM Performance</head><p>We evaluate Ring Attention by applying our method to finetune LLaMA model to longer context. In this experiment, while our approach enables training with millions of context tokens, we conducted finetuning on the LLaMA-13B model, limiting the context length to 512K tokens due to constraints on our cloud compute budget. This finetuning was carried out on 32 A100 GPUs, using the ShareGPT dataset, following methodologies as outlined in prior works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. We then evaluated our finetuned model on the line retrieval test <ref type="bibr" target="#b19">[20]</ref>. In this test, the model needs to precisely retrieve a number from a long document, the task can effectively capture the abilities of text generation, retrieval, and information association at long context, reflected by the retrieving accuracy. Figure <ref type="figure" target="#fig_2">3</ref> presents the accuracy results for different models across varying context lengths (measured in tokens). Notably, our model, Ring Attention-13B-512K, stands out as it maintains high accuracy levels even with long contexts. GPT3.5-turbo-16K, Vicuna-16B-16K, and Claude-2-100K demonstrate competitive accuracy within short context lengths. However, they cannot handle extended context lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Transformers have garnered significant attention in the field of AI and have become the backbone for numerous state-of-the-art models. Several works have explored memory-efficient techniques to address the memory limitations of Transformers and enable their application to a wider range of problems. Computing exact self-attention in a blockwise manner using the tiling technique <ref type="bibr" target="#b23">[24]</ref> has led to the development of memory efficient attention mechanisms <ref type="bibr" target="#b29">[30]</ref> and its efficient CUDA implementation <ref type="bibr" target="#b8">[9]</ref>, and blockwise parallel transformer <ref type="bibr" target="#b22">[23]</ref> that proposes computing both feedforward and self-attention block-by-block, resulting in a significant reduction in memory requirements.</p><p>In line with these advancements, our work falls into the category of memory efficient computation for Transformers. Other works have investigated the approximation of attention mechanisms, yet these efforts have often yielded sub-optimal results or encountered challenges during scaling up. For an in-depth review of these techniques, we recommend referring to the surveys <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref>. Another avenue of research explores various parallelism methods, including data parallelism <ref type="bibr" target="#b9">[10]</ref>, tensor parallelism <ref type="bibr" target="#b33">[34]</ref>, pipeline parallelism <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28]</ref>, sequence parallelism <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>, and FSDP <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>. The activations of self-attention take a substantial amount of memory for large context models.</p><p>Tensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication <ref type="bibr" target="#b16">[17]</ref>. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology <ref type="bibr" target="#b20">[21]</ref>, which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios. Our work leverages on blockwise parallel transformers to distribute blockwise attention and feedforward across devices and concurrently overlaps the communication of key-value blocks in a circular of hosts with the computation of query-key-value blocks and feedforward, reducing memory cost substantially and allowing device count times larger context size with zero overheads. Overlapping communication with computation has been studied in high performance computing literature <ref type="bibr">[7, 38, 8, inter alia]</ref>. While ring communication has found applications in other parallel computing scenarios <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>, our work stands out as the first work to show that it can be applied to self-attention as used in Transformers and to make it fit efficiently into Transformer training and inference without adding significant overhead by overlapping blockwise computation and communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In conclusion, we propose a memory efficient approach to reduce the memory requirements of Transformers, the backbone of state-of-the-art AI models. Our approach allows the context length to scale linearly with the number of devices while maintaining performance, eliminating the memory bottleneck imposed by individual devices. Through extensive experiments on language modeling and reinforcement learning, we demonstrate its effectiveness, enabling training sequences that are up to device count times longer than those of prior memory-efficient Transformers, exceeding a context length of 100 million without making approximations to attention. In terms of future prospects, the possibility of near-infinite context introduces a vast array of exciting opportunities, such as large video-audio-language models, learning from extended feedback and trial-and-errors, understanding and generating codebase, adapting AI models to understand scientific data such as gene sequences, and developing strong reasoning from link gathering data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Code</head><p>The implementation of Ring Attention in Jax is provided in Figure <ref type="figure">4</ref>. We use defvjp function to define both the forward and backward passes, and use collective operation jax.lax.ppermute to facilitate the exchange of key-value blocks among a ring of hosts. The provided code snippet highlights essential components of Ring Attention. We provide the complete code of our Ring Attention at <ref type="url" target="https://github.com/lhao499/llm_large_context">https://github.com/lhao499/llm_large_context</ref>.</p><p>For large scale end-to-end training on TPU or on GPU cluster with high bandwidth inter connection, we recommend using FSDP to shard large models and using Ring Attention to achieve large context. If total batch size is too large, add tensor parallelism to reduce the global batch size. The degree of parallelism can be adjusted using the mesh_dim parameter within the codebase. To illustrate, consider a setup with 512 devices, such as 512x A100. If the model size is 30B, you can shard it across 8 devices and allocate the remaining 32 devices for Ring Attention. This setup allows the context size to be expanded 32 times more than if you didn't use Ring Attention. Conversely, for models sized 7B or 3B, there is no need for FSDP. This means you can utilize all 512 devices exclusively to expand the context using Ring Attention by 512 times. Building upon the result that our approach allows for a 256K context size when using 8x A100 GPUs, it suggests that by employing 512 A100 GPUs, the potential context size can be expanded to 16 million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Details B.1 Evaluation of context length</head><p>In the experimental results presented in Section 5.1, we used fully sharded tensor parallelism (FSDP) to partition the model across GPUs or TPU devices. Our evaluation focused on determining the maximum achievable sequence length in commonly used FSDP training scenarios. For TPUs, we utilized its default training configuration, which involved performing matmul operations in bfloat16 format with weight accumulation in float32. On the other hand, for GPUs, we adopted the default setup, where all operations were performed in float32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Evaluation of MFU</head><p>In the evaluation presented in Section 5.2. The batch size in tokens is 2 million per batch on GPU and 4 million per batch on TPU. The training was conducted using FSDP <ref type="bibr" target="#b10">[11]</ref> with Jax SPMD. For gradient checkpointing <ref type="bibr" target="#b4">[5]</ref>, we used nothing_saveable as checkpointing policies for attention and feedforward network (FFN). For more details, please refer to Jax documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Evaluation on line retrieval</head><p>In the evaluation presented in Section 5.4, we finetuned the LLaMA-13B model <ref type="bibr" target="#b35">[36]</ref>, limiting context length to 512K tokens due to constraints on our cloud compute budget, the training was conducted on 32x A100 80GB Cloud GPUs. We use user-shared conversations gathered from ShareGPT.com with its public APIs for finetuning, following methodologies as outlined in prior works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. ShareGPT is a website where users can share their ChatGPT conversations. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples, which results in 125K conversations after data cleaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Inference requirement</head><p>We provide the minimal sequence length required to overlap communication with computation during training in Table <ref type="table" target="#tab_1">2</ref>. Ring Attention enables effortless training of context size that scales linearly with the number of devices. While we focus on introducing training as it is more memory demanding than autoregressive inference where the number of query token is one, Ring Attention is applicable to inference too. For example, serving a LLaMa 7B on 32x TPUv5e, the conventional approach is to distribute the model along the attention heads dimension, with each device computing one attention head. Assuming a batch size of</p><p>1, this can serve up to a 256K context length due to key-value cache activation size. Ring Attention can allow 32 times larger context by circulating the key-value cache between a ring of devices. To overlap the communication with computation, it needs d2/F &gt;= 2*d2/B, where B/F &gt;=2. With a bandwidth of 186 GB/s and flops of 196 TFLOPs, and assuming an 1 def _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs): 2 if float32_logits: 3 q, k = q.astype(jnp.float32), k.astype(jnp.float32) 4 batch, q_len, num_heads, dim_per_head = q.shape 5 batch, kv_len, num_heads, dim_per_head = k.shape 6 numerator = jnp.zeros((batch, q_len, num_heads, dim_per_head)).astype(q.dtype) 7 denominator = jnp.zeros((batch, num_heads, q_len)).astype(q.dtype) 8 axis_size = lax.psum(1, axis_name) 9 block_size = q_len # assumes this function is pre-sharded inside shard_map 10 query_chunk_size = blockwise_kwargs["query_chunk_size"] 11 key_chunk_size = blockwise_kwargs["key_chunk_size"] 12 def scan_kv_block(carry, idx): 13 prev_max_score, numerator, denominator, k, v = carry 14 attn_bias_slice = lax.dynamic_slice_in_dim(attn_bias, 15 (lax.axis_index(axis_name) -idx) % axis_size * kv_len, kv_len, axis=-1) 16 q_block_idx = lax.axis_index(axis_name) 17 k_block_idx = (lax.axis_index(axis_name) -idx) % axis_size 18 q_chunk_idx_start = q_block_idx * (block_size // query_chunk_size) 19 k_chunk_idx_start = k_block_idx * (block_size // key_chunk_size) 20 numerator, denominator, max_score = _blockwise_attention_fwd(q, k, v, 21 (numerator, denominator, prev_max_score), q_chunk_idx_start, k_chunk_idx_start, 22 bias=attn_bias_slice, **blockwise_kwargs) 23 k, v = map(lambda x: lax.ppermute(x, axis_name, perm=[(i, (i + 1) % axis_size) 24 for i in range(axis_size)]), (k, v)) 25 return (max_score, numerator, denominator, k, v), None 26 prev_max_score = jnp.full((batch, num_heads, q_len), -jnp.inf).astype(q.dtype) 27 (max_score, numerator, denominator, _, _), _ = lax.scan(scan_kv_block, 28 init=(prev_max_score, numerator, denominator, k, v), xs=jnp.arange(0, axis_size))</p><p>29 output = numerator / rearrange(denominator, 'b h q -&gt; b q h')[..., None] 30 return output.astype(v.dtype), (output, q, k, v, attn_bias, denominator, max_score) unreasonably high MFU of 40% for this large context, then B/F = 2.4, meaning that Ring Attention allows 32 times larger context for inference without adding overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Training FLOPs Scaling of Context Size</head><p>Given that our proposed approach unlocks the possibility of training with a context size exceeding 100 million tokens and allows for linear scaling of the context size based on the number of devices, it is essential to understand how the training FLOPs per dataset scale with the context size. While a larger context size results in a higher number of FLOPs, the increased ratio does not scale quadratically because the number of tokens remains fixed. We present these results in Figure <ref type="figure" target="#fig_4">5</ref>, which showcases various model sizes and context lengths, representing different computational budgets. The figure shows the ratio of FLOPs for larger context lengths compared to the same model with a shorter 4K context size. We calculated the per sequence FLOPs using (24bsh 2 + 4bs 2 h)n where h is model hidden dimension, b is batch size, s is total sequence length, and n is number of layers. The per dataset FLOPs ratio is then given by ((24bs 2 h 2 + 4bs 2 2 h)/(24bs 1 h 2 + 4bs 1 2 h))/(s 2 /s 1 ) = (6h + s 2 )/(6h + s 1 ), where s 2 and s 1 are new and old context lengths. Model sizes and their hidden dimensions are as follows: LLaMA-7B (4096), LLaMA-13B (5140), LLaMA-33B (7168), LLaMA-65B (8192), GPT3-175B (12288), and 1TB (36864). These model configurations are from LLaMA <ref type="bibr" target="#b35">[36]</ref> and GPT-3 <ref type="bibr" target="#b2">[3]</ref> papers, except the 1TB model size and dimension were defined by us.</p><p>As depicted in Figure <ref type="figure" target="#fig_4">5</ref>, scaling up small models to a 1M context size results in approximately 20-40 times more FLOPs, and even more for 10M and 100M token context sizes. However, as the model sizes increase, the cost ratio decreases. For instance, scaling up the 170B model from 4K to 10M incurs 162.6x higher per dataset FLOPs, despite the context size being 3072 times longer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Maximum context length under endto-end large-scale training on TPUv4-1024. Baselines are vanilla transformers<ref type="bibr" target="#b36">[37]</ref>, memory efficient transformers<ref type="bibr" target="#b29">[30]</ref>, and memory efficient attention and feedforward (blockwise parallel transformers)<ref type="bibr" target="#b22">[23]</ref>. Our proposed approach Ring Attention allows training up to device count times longer sequence than baselines and enables the training of sequences that exceed millions in length without making approximations nor adding any overheads to communication and computation.</figDesc><graphic coords="2,306.00,75.69,197.98,111.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top (a): We use the same model architecture as the original Transformer but reorganize the compute. In the diagram, we explain this by showing that in a ring of hosts, each host holds one query block, and key-value blocks traverse through a ring of hosts for attention and feedforward computations in a block-by-block fashion. As we compute attention, each host sends key-value blocks to the next host while receives key-value blocks from the preceding host. The communication is overlapped with the computation of blockwise attention and feedforward. Bottom (b): We compute the original Transformer block-by-block. Each host is responsible for one iteration of the query's outer loop, while the key-value blocks rotate among the hosts. As visualized, a device starts with the first query block on the left; then we iterate over the key-value blocks sequence positioned horizontally. The query block, combined with the key-value blocks, are used to compute self-attention (yellow box), whose output is pass to feedforward network (cyan box).</figDesc><graphic coords="4,108.73,72.00,392.02,392.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of different models on the long-range line retrieval task. number of trajectories rather than 32 trajectories in prior works. It is worth noting that each trajectory has 1000 × 4 length where 1000 is sequence length while 4 is return-state-action-reward, making training 128 trajectories with modest 350M size model infeasible for prior state-of-the-art blockwise parallel transformers. Results in Table5show that, by scaling up the sequence length (number of trajectories), AT + Ring Attention consistently outperforms oringal AT with BPT across all six tasks, achieving a total average return of 113.66 compared to the AT with BPT model's total average return of 111.13. The results show that the advantage of Ring Attention for training and inference with long sequences.</figDesc><graphic coords="9,108.00,60.04,396.01,133.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>31 32</head><label>31</label><figDesc>def _ring_attention_bwd(axis_name, float32_logits, blockwise_kwargs, res, g): 33 output, q, k, v, attn_bias, denominator, max_score = res 34 batch, kv_len, num_heads, dim_per_head = k.shape 35 axis_size = lax.psum(1, axis_name) 36 dq = jnp.zeros_like(q, dtype=jnp.float32) 37 dk = jnp.zeros_like(k, dtype=jnp.float32) 38 dv = jnp.zeros_like(v, dtype=jnp.float32) 39 query_chunk_size = blockwise_kwargs["query_chunk_size"] 40 key_chunk_size = blockwise_kwargs["key_chunk_size"]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The per dataset trainig FLOPs cost ratio relative to a 4k context size, considering different model dimensions. On the x-axis, you'll find the context length, where, for example, 32x(128k) denotes a context length of 128k, 32x the size of the same model's 4k context length.</figDesc><graphic coords="16,116.66,229.49,376.18,258.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="8,108.20,113.89,395.60,165.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of maximum activation sizes among different Transformer architectures. Here, b is batch size, h is hidden dimension, n is number of head, s is sequence length, c is block size, the block size (c) is independent of the input sequence length (s). The comparison is between vanilla Transformer<ref type="bibr" target="#b36">[37]</ref>, memory efficient attention<ref type="bibr" target="#b29">[30]</ref>, memory efficient attention and feedforward<ref type="bibr" target="#b22">[23]</ref>, and our proposed approach Ring Attention. Numbers are shown in bytes per layer, assuming bfloat16 precision.</figDesc><table><row><cell>Layer Type</cell><cell cols="2">Self-Attention FeedForward</cell><cell>Total</cell></row><row><cell>Vanilla</cell><cell>2bns 2</cell><cell cols="2">8bsh 2bhs 2</cell></row><row><cell>Memory efficient attention</cell><cell>2bsh + 4bch</cell><cell>8bsh</cell><cell>8bsh</cell></row><row><cell>Memory efficient attention and feedforward</cell><cell>2bsh</cell><cell>2bsh</cell><cell>2bsh</cell></row><row><cell>Ring Attention</cell><cell>6bch</cell><cell>2bch</cell><cell>6bch</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Minimal sequence length needed on each device. Interconnect Bandwidth is the unidirectional bandwidth between hosts, i.e., NVLink / InfiniBand bandwidth between GPUs and ICI bandwidth between TPUs. The minimal block size required c = FLOPS/Bandwidth, and minimal sequence length s = 6c.</figDesc><table><row><cell>Spec Per Host</cell><cell cols="2">FLOPS HBM</cell><cell>Interconnect Bandwidth</cell><cell>Minimal Blocksize</cell><cell>Minimal Sequence Len</cell></row><row><cell></cell><cell cols="2">(TF) (GB)</cell><cell>(GB/s)</cell><cell>(×1e3)</cell><cell>(×1e3)</cell></row><row><cell>A100 NVLink</cell><cell>312</cell><cell>80</cell><cell>300</cell><cell>1.0</cell><cell>6.2</cell></row><row><cell>A100 InfiniBand</cell><cell>312</cell><cell>80</cell><cell>12.5</cell><cell>24.5</cell><cell>149.5</cell></row><row><cell>TPU v3</cell><cell>123</cell><cell>16</cell><cell>112</cell><cell>1.1</cell><cell>6.6</cell></row><row><cell>TPU v4</cell><cell>275</cell><cell>32</cell><cell>268</cell><cell>1.0</cell><cell>6.2</cell></row><row><cell>TPU v5e</cell><cell>196</cell><cell>16</cell><cell>186</cell><cell>1.1</cell><cell>6.3</cell></row></table><note><p>computation, the following condition must hold: 4dc 2 /F ≥ 4cd/B. This implies that the block size, denoted as c, should be greater than or equal to F/B. Effectively, this means that the block size needs to be larger than the ratio of FLOPs over bandwidth.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Model flops utilization (MFU) with different training configurations: model sizes, compute, and context lengths. Ring Attention enables training large models (7B-65B) on large input context sizes (over 4M) with negligible overheads.</figDesc><table><row><cell></cell><cell>Model size</cell><cell>7B</cell><cell>13B</cell><cell>13B</cell><cell>30B</cell><cell>65B</cell></row><row><cell></cell><cell cols="6">Compute 8x A100 8x A100 32x A100 TPUv4-1024 TPUv4-1024</cell></row><row><cell>Memory efficient attention &amp; FFN</cell><cell>Context size (×1e3)</cell><cell>32</cell><cell>16</cell><cell>64</cell><cell>16</cell><cell>8</cell></row><row><cell>Ring Attention</cell><cell>Context size (×1e3)</cell><cell>256</cell><cell>128</cell><cell>2048</cell><cell>2048</cell><cell>1024</cell></row></table><note><p>advantage in terms of maintaining MFU while enabling training with significantly longer context lengths. As shown in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Application of Ring Attention on improving Transformer in RL. BC and DT use vanilla attention. AT + ME denotes using memory efficient attention, AT + BPT denotes using blockwise parallel transformer. AT + RA denotes using Ring Attention.</figDesc><table><row><cell>ExoRL</cell><cell>BC-10%</cell><cell>DT</cell><cell>AT + ME</cell><cell>AT + BPT</cell><cell>AT + BPT</cell><cell>AT + RA</cell></row><row><cell>Task</cell><cell></cell><cell></cell><cell cols="4">N Trajs = 32 N Trajs = 32 N Trajs = 128 N Trajs = 128</cell></row><row><cell>Walker Stand</cell><cell cols="2">52.91 34.54</cell><cell>oom</cell><cell>95.45</cell><cell>oom</cell><cell>98.23</cell></row><row><cell>Walker Run</cell><cell cols="2">34.81 49.82</cell><cell>oom</cell><cell>105.88</cell><cell>oom</cell><cell>110.45</cell></row><row><cell>Walker Walk</cell><cell cols="2">13.53 34.94</cell><cell>oom</cell><cell>78.56</cell><cell>oom</cell><cell>78.95</cell></row><row><cell>Cheetah Run</cell><cell cols="2">34.66 67.53</cell><cell>oom</cell><cell>178.75</cell><cell>oom</cell><cell>181.34</cell></row><row><cell>Jaco Reach</cell><cell cols="2">23.95 18.64</cell><cell>oom</cell><cell>87.56</cell><cell>oom</cell><cell>89.51</cell></row><row><cell>Cartpole Swingup</cell><cell cols="2">56.82 67.56</cell><cell>oom</cell><cell>120.56</cell><cell>oom</cell><cell>123.45</cell></row><row><cell>Total Average</cell><cell cols="2">36.11 45.51</cell><cell>oom</cell><cell>111.13</cell><cell>oom</cell><cell>113.66</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code: https://github.com/lhao499/llm_large_context Preprint.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This project is supported in part by <rs type="funder">Office of Naval Research</rs> grant <rs type="grantNumber">N00014-21-1-2769</rs>. We express our gratitude to the BAIR and RLL communities for their insightful discussions and feedback. We are also thankful to <rs type="person">David Patterson</rs> for addressing our questions about TPUs and giving insightful feedback on early versions of this work. Our appreciation goes out to <rs type="person">Yash Katariya</rs> and <rs type="person">Sharad Vikram</rs> from the <rs type="funder">Jax developers</rs>' team for assisting with our Jax related questions. We also thank <rs type="person">Tri Dao</rs> for the valuable feedback on this work. We thank <rs type="person">Google TPU Research Cloud</rs> for granting us access to TPUs.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_q6GxUf8">
					<idno type="grant-number">N00014-21-1-2769</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>block_size = q.shape <ref type="bibr" target="#b0">[1]</ref> # assumes this function is pre-sharded inside shard_map 42 def scan_kv_block(carry, idx):  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Introducing claude</title>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<ptr target="https://www.anthropic.com/index/introducing-claude" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Parallel computing: Architectures, algorithms, and applications</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IOS Press</publisher>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decision transformer: Reinforcement learning via sequence modeling</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15084" to="15097" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformations to parallel codes for communication-computation overlap</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Danalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ki-Yong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Swany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;05: Proceedings of the 2005 ACM/IEEE conference on Supercomputing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="58" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mpi-aware compiler optimizations for improving communication-computation overlap</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Danalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Swany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Cavazos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Supercomputing</title>
		<meeting>the 23rd international conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="316" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16344" to="16359" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fully Sharded Data Parallel: faster AI training with fewer GPUs -engineering</title>
		<author>
			<persName><surname>Facebook</surname></persName>
		</author>
		<ptr target="https://engineering.fb.com/2021/07/15/open-source/fsdp/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Openllama: An open reproduction of llama</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/openlm-research/open_llama" />
		<imprint>
			<date type="published" when="2023-05">may 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Koala: A dialogue model for academic research</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Gudibande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blog post</title>
		<imprint>
			<date type="published" when="2023-04-01">April, 1, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bringing hpc techniques to deep learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Baidu Research</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hy-Oukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a fault tolerant mpi application: A ring communication example</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Hursey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1549" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models</title>
		<author>
			<persName><forename type="first">Ade</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14509</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reducing activation recomputation in large transformer models</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkug</forename><surname>Lym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Andersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mohammad Shoeybi, and Bryan Catanzaro</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Cang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15191</idno>
		<title level="m">Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How long can open-source llms truly promise on context length?</title>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rulin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://lmsys.org/blog/2023-06-29-longchat" />
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence parallelism: Long sequence training from system perspective</title>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.134</idno>
		<ptr target="https://aclanthology.org/2023.acl-long.134" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2391" to="2404" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emergent agentic transformer from chain of hindsight experience</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blockwise parallel transformer for large context models</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Online normalizer calculation for softmax</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Milakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02867</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introducing mpt-7b: A new standard for open-source, commercially usable llms</title>
		<author>
			<persName><surname>Mosaicml</surname></persName>
		</author>
		<ptr target="https://www.mosaicml.com/blog/mpt-7b" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karishma</forename><surname>Malkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11972</idno>
		<title level="m">Do transformer modifications transfer across implementations and applications? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pipedream: Generalized pipeline parallelism for dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Nikhil R Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memoryefficient pipeline-parallel dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7937" to="7947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Self-attention does not need o(n2) memory</title>
		<author>
			<persName><forename type="first">N</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><surname>Staats</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05682</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing language models for dialogue</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F C</forename><surname>Uribe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pokorny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vijayvergiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Balcom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaftan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Turley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><surname>Chatgpt</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt" />
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in tensorflow</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balso</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.10551</idno>
		<title level="m">Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overlap communication with dependent computation via decomposition in large deep learning models</title>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sabne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Ilbeyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Srinivasa Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="93" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Don&apos;t change the algorithm, change the data: Exploratory data for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lerrel</forename><surname>Pinto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13425</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
