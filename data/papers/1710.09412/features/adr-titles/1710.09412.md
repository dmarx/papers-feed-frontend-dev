- Decision to adopt mixup as a data augmentation technique
- Choice of convex combinations for generating virtual training examples
- Selection of the Beta distribution for sampling λ
- Implementation of mixup in PyTorch
- Hyperparameter tuning for α in mixup
- Use of a single data loader for minibatch processing
- Decision to apply mixup after random shuffling of the minibatch
- Comparison of mixup with traditional data augmentation methods
- Evaluation of mixup's impact on generalization across different datasets
- Design choice to avoid using dropout in experiments
- Strategy for learning rate scheduling during training
- Decision to conduct ablation studies to assess design choices
- Choice of neural network architectures for experiments (e.g., ResNet, DenseNet)
- Evaluation metrics selected for performance assessment (top-1 and top-5 error rates)
- Decision to analyze the robustness of mixup against adversarial examples
- Consideration of computational overhead introduced by mixup
- Decision to explore connections to prior work on data augmentation and regularization
- Choice to document the source code for reproducibility purposes