{
  "arxivId": "2501.04697",
  "title": "Grokking at the Edge of Numerical Stability",
  "authors": "Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal",
  "abstract": "Grokking, the sudden generalization that occurs after prolonged overfitting,\nis a surprising phenomenon challenging our understanding of deep learning.\nAlthough significant progress has been made in understanding grokking, the\nreasons behind the delayed generalization and its dependence on regularization\nremain unclear. In this work, we argue that without regularization, grokking\ntasks push models to the edge of numerical stability, introducing floating\npoint errors in the Softmax function, which we refer to as Softmax Collapse\n(SC). We demonstrate that SC prevents grokking and that mitigating SC enables\ngrokking without regularization. Investigating the root cause of SC, we find\nthat beyond the point of overfitting, the gradients strongly align with what we\ncall the na\\\"ive loss minimization (NLM) direction. This component of the\ngradient does not alter the model's predictions but decreases the loss by\nscaling the logits, typically by scaling the weights along their current\ndirection. We show that this scaling of the logits explains the delay in\ngeneralization characteristic of grokking and eventually leads to SC, halting\nfurther learning. To validate our hypotheses, we introduce two key\ncontributions that address the challenges in grokking tasks: StableMax, a new\nactivation function that prevents SC and enables grokking without\nregularization, and $\\perp$Grad, a training algorithm that promotes quick\ngeneralization in grokking tasks by preventing NLM altogether. These\ncontributions provide new insights into grokking, elucidating its delayed\ngeneralization, reliance on regularization, and the effectiveness of existing\ngrokking-inducing methods. Code for this paper is available at\nhttps://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.",
  "url": "https://arxiv.org/abs/2501.04697",
  "issue_number": 996,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/996",
  "created_at": "2025-01-17T05:46:34.882119",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 18,
  "last_read": "2025-01-17T18:18:33.078619",
  "last_visited": "2025-01-17T18:17:21.137Z",
  "main_tex_file": null,
  "published_date": "2025-01-08T18:58:48Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "stat.ML"
  ]
}