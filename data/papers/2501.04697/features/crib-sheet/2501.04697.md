- **Grokking Definition**: Sudden generalization after prolonged overfitting, first described by Power et al. (2022).
  
- **Key Phenomenon**: Grokking is often observed in tasks with regularization (e.g., weight decay) but can occur without it under certain conditions.

- **Softmax Collapse (SC)**: A phenomenon where floating point errors in the Softmax function lead to zero gradients, halting learning. Defined mathematically as:
  \[
  L_{SCE}(f(x), y) \approx -\log e^{z_y} \quad \text{when } \sum_{k=1}^{n} e^{z_k} \approx e^{z_y}
  \]

- **Naïve Loss Minimization (NLM)**: A gradient alignment that leads to scaling logits without changing predictions, causing delayed generalization and SC.

- **StableMax**: A proposed activation function that prevents SC, allowing grokking without regularization.

- **⊥Grad Optimizer**: A new training algorithm that preserves only the part of the gradient orthogonal to the NLM direction, facilitating quick generalization.

- **Dataset Characteristics**:
  - **Modular Arithmetic**: Tasks involve predicting \( y = a * b \mod p \) with one-hot encoded inputs.
  - **Sparse Parity**: Predicting parity of \( k \) bits from a binary vector of length \( n \).
  - **MNIST**: Image classification using a subset of 200 training samples.

- **Model Architecture**: 
  - 2-hidden layer MLP with width 200.
  - One-layer transformer with 4 attention heads.
  - Training with ReLU activations and cross-entropy loss using AdamW and SGD optimizers.

- **Key Contributions**:
  - Identification of SC as a barrier to grokking without regularization.
  - Demonstration that interventions like StableMax can induce grokking.
  - Validation of NLM's role in delaying generalization and leading to SC.

- **Numerical Stability**: Importance of floating point precision in preventing SC; higher precision (e.g., float64) mitigates absorption errors.

- **Figures and Results**: 
  - Figure 2 illustrates the relationship between dataset size and the occurrence of SC, showing that smaller datasets lead to earlier SC.

- **Implications**: Findings provide insights into the mechanisms behind grokking, emphasizing the role of numerical stability and regularization in deep learning models.