<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GROKKING AT THE EDGE OF NUMERICAL STABILITY</title>
				<funder ref="#_BXBYm6S">
					<orgName type="full">UKRI</orgName>
				</funder>
				<funder ref="#_xCHukQx">
					<orgName type="full">UKRI Centre for Doctoral Training in Safe and Trusted AI</orgName>
				</funder>
				<funder ref="#_2aKsQpw">
					<orgName type="full">Engineering and Physical Sciences Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-08">8 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lucas</forename><surname>Prieto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Imperial</orgName>
								<address>
									<settlement>College London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Melih</forename><surname>Barsbey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Imperial</orgName>
								<address>
									<settlement>College London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Imperial</orgName>
								<address>
									<settlement>College London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Imperial</orgName>
								<address>
									<settlement>College London</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GROKKING AT THE EDGE OF NUMERICAL STABILITY</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-08">8 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">3C99AEF91FEFB6F94F24EE939C21A19D</idno>
					<idno type="arXiv">arXiv:2501.04697v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grokking, or sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon that has challenged our understanding of deep learning. While a lot of progress has been made in understanding grokking, it is still not clear why generalization is delayed and why grokking often does not happen without regularization. In this work we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax that we refer to as Softmax Collapse (SC). We show that SC prevents grokking and that mitigating SC leads to grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the naïve loss minimization (NLM) direction. This component of the gradient does not change the predictions of the model but decreases the loss by scaling the logits, usually through the scaling of the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking, and eventually leads to SC, stopping learning altogether. To validate these hypotheses, we introduce two key contributions that mitigate the issues faced in grokking tasks: (i) StableMax, a new activation function that prevents SC and enables grokking without regularization, and (ii) ⊥ Grad, a training algorithm that leads to quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, shedding light on its delayed generalization, reliance on regularization, and the effectiveness of known grokking-inducing methods. Code for this paper can be found at: <ref type="url" target="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability">https://github.com/LucasPrietoAl/ grokking-at-the-edge-of-numerical-stability</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning has been transformative for a variety of fields such as natural language processing <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, computer vision <ref type="bibr" target="#b19">(Krizhevsky et al., 2012)</ref>, geometry processing <ref type="bibr" target="#b31">(Qi et al., 2017)</ref>, and 3D vision <ref type="bibr" target="#b6">(Deng et al., 2018)</ref>. This rapid proliferation has brought with it surprising phenomena that defy the predictions of classical statistical learning theory.</p><p>In this paper we explore one such recently observed phenomenon known as grokking, first described by <ref type="bibr" target="#b30">Power et al. (2022)</ref> as a sudden and unexpected generalization occurring after prolonged overfitting. Although predominantly studied in algorithmic tasks like modular addition or multiplication, recent findings suggest that grokking may be a more pervasive phenomenon, also manifesting in more complex tasks involving vision and language <ref type="bibr" target="#b24">(Lv et al., 2024;</ref><ref type="bibr" target="#b13">Humayun et al., 2024)</ref>.</p><p>Prior research has consistently observed grokking in settings that involve some form of regularization, such as weight decay <ref type="bibr" target="#b1">(Barak et al., 2022;</ref><ref type="bibr" target="#b30">Power et al., 2022;</ref><ref type="bibr" target="#b29">Nanda et al., 2023)</ref>. This pattern has motivated investigations into the implicit biases introduced by weight decay, suggesting it may be critical to triggering delayed generalization. For instance, <ref type="bibr">Liu et al. (2023a)</ref> argued that weight norms need to be in a narrow range or "Goldilocks Zone" for generalization. Similarly, <ref type="bibr" target="#b37">Varma et al. (2023)</ref> highlighted weight efficiency of generalizing solutions, and <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> argued that weight decay favors simpler, more generalizable solutions. However, recent works have argued that regularization may not be necessary for grokking, at least on shallow networks with Mean Squared We show that the delay in generalization induced by NLM can be reversed using the proposed ⊥AdamW ((a) and (b)) and that the numerical errors that lead to overfitting instead of grokking can be avoided by using the proposed StableMax ((b) and (c)).</p><p>Error (MSE) loss <ref type="bibr" target="#b20">(Kumar et al., 2024;</ref><ref type="bibr" target="#b26">Lyu et al., 2024;</ref><ref type="bibr" target="#b11">Gromov, 2023)</ref>. These works tie grokking to a transition from lazy training <ref type="bibr" target="#b3">(Chizat et al., 2018)</ref> to feature learning. Despite this ongoing work, several aspects in this framing of grokking remain unclear. These include why grokking tasks induce lazy training and why weight decay is often needed to enter the feature learning regime when using deeper models or cross-entropy (CE) loss.</p><p>Here we propose a novel account of grokking, outlined in Fig. <ref type="figure" target="#fig_0">1</ref>, that explains several of the main unanswered questions in the grokking literature. We start by showing that without regularization, grokking is prevented by absorption errors in the Softmax, which we call Softmax Collapse (SC). These errors result in zero terms in the gradient and put an end to learning, sometimes before any progress is made in the test performance, resulting in complete overfitting (Fig. <ref type="figure" target="#fig_0">1,</ref> <ref type="figure">c</ref>). We then argue that SC is caused by what we call Naïve Loss Minimization (NLM), as the gradient becomes aligned with a direction that corresponds to scaling up the logits by a constant. While scaling up all the logits does not change the model predictions, it does reduce the CE loss for a network that has reached 100% training accuracy, with the downside that this eventually leads to numerical errors in Softmax. Our findings provide explanations for several key aspects of grokking, including (i) the delayed onset of generalization, (ii) why grokking is often absent without regularization, and (iii) why existing methods designed to induce grokking are effective.</p><p>To validate our hypothesis that SC is responsible for the absence of grokking without regularization, we introduce StableMax as a more numerically stable replacement to Softmax in CE loss. This simple change takes models from complete overfitting to grokking (Fig. <ref type="figure" target="#fig_0">1</ref>, c to b) without regularization, in settings where it is normally not observed without it. Similarly, we validate that NLM is responsible for delaying generalization (Fig. <ref type="figure" target="#fig_0">1</ref>, a to b) and leading to SC by introducing a new optimizer ⊥Grad, which only preserves the part of the gradient that is orthogonal to the NLM direction. By doing this, ⊥Grad quickly leads to generalization without the initial overfitting phase that defines grokking (Fig. <ref type="figure" target="#fig_0">1</ref>, b to a).</p><p>Our primary contributions are as follows:</p><p>• We observe that cases of overfitting without grokking are due to floating point errors caused by extreme values in the Softmax function, which we term Softmax Collapse (SC; Sec. 3). • We show that interventions to avoid SC, like greater floating point precision or a new, numerically stable version of Softmax (StableMax), cause grokking in settings where it was previously absent without regularization (Sec. 3.3). • We observe that models move towards SC because overfitting and cross-entropy loss push the model in a direction of uncontrolled logit growth, which we refer to as Naïve Loss Minimization (NLM; Sec. 4). • We demonstrate that NLM can be avoided through a novel optimizer, ⊥Grad, which removes the delay in generalization (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DATASETS</head><p>We show our findings on the most commonly studied grokking datasets, outlined in this section.</p><p>I. Modular arithmetic. The main results in this paper are shown on arithmetic modulo 113 <ref type="bibr" target="#b30">(Power et al., 2022;</ref><ref type="bibr" target="#b29">Nanda et al., 2023)</ref>. This is a family of supervised learning tasks where two one-hot encoded inputs representing integers a, b &lt; p are used to predict the target y = a * b mod p, where * is some binary operation and p is a prime number. In most of our results, the binary operation is addition, but we show additional results with multiplication and subtraction.</p><p>Modular arithmetic tasks are characterized by a binary operation and a dataset size, with different behaviours being observed for different dataset sizes on the same binary operation. In these settings, we describe the dataset sizes as the percentage of the 113 2 possible pairs that are used for training, with the rest of the data being used for testing as in <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> and <ref type="bibr" target="#b30">Power et al. (2022)</ref>.</p><p>Our main results use a 40%/60% train/test split but we also include results using 60%/40% and 70%/30%. The input integers are represented as one-hot vectors.</p><p>II. Sparse parity. We also validate some of our results on the Sparse Parity task outlined in <ref type="bibr" target="#b1">Barak et al. (2022)</ref>. This is a supervised learning setting where the target is the parity of k bits out of a binary vector of length n, with k ≪ n. In this work we use 2000 samples, split evenly between train and test data and we describe instances of this task by specifying the values of n and k.</p><p>III. MNIST. Finally, we provide some results on a subset the classic image classification dataset MNIST <ref type="bibr" target="#b7">(Deng, 2012)</ref>. For our experiments, we use a subset of 200 training samples from the training set as in <ref type="bibr">Liu et al. (2023b)</ref>, with evaluation on the full test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MODELS</head><p>We study the grokking phenomenon on these datasets using a 2-hidden layer multi-layer perceptron (MLP) of width 200 as in <ref type="bibr">Liu et al. (2023a)</ref> and a one-layer transformer with 4 attention heads as <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> and <ref type="bibr" target="#b30">Power et al. (2022)</ref>. We train both of these models in a full batch setting, using ReLU activations and cross-entropy loss with AdamW and SGD, as well as our own variants of these optimizers, ⊥AdamW and ⊥SGD. Unless specified otherwise we set the weight decay parameter λ = 0. For modular arithmetic datasets, inputs are concatenated as the input of the MLP resulting in a 226 dimensional vector, and treated as separate tokens in the case of the transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SOFTMAX COLLAPSE: FLOATING POINT ERRORS PREVENT GROKKING</head><p>Given our current understanding of grokking, it is surprising that it happens without regularization for some dataset sizes, but regularization becomes crucial as dataset size decreases <ref type="bibr" target="#b30">(Power et al., 2022)</ref>. In this section we highlight that looking at datasets at the boundary of these two regimes reveals that without weight decay, grokking sometimes starts before abruptly stopping (Fig. <ref type="figure">2</ref>). We show that this is caused by floating point errors in the Softmax that lead the gradients from a large fraction of the samples to become zero. We refer to this phenomenon as Softmax Collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SOFTMAX COLLAPSE</head><p>In modern neural network implementations, Floating Point (FP) arithmetic is ubiquitous for representing and computing parameters, activations, and gradients. While FP numbers enable efficient decimal computations, they introduce numerical inaccuracies. This section focuses on absorption errors, as a specific class of FP arithmetic failure. We will use the symbol In this case, after exponent alignment, the significand of b is shifted right by at least p digits, and b cannot be represented in the available precision, resulting in a + b . = a.</p><p>Intuitively, absorption errors can occur during FP addition when operands have significantly different magnitudes. For f loat32 the base β is 2 and p = 24 bits, meaning that adding any number smaller than 2 -(p-1) = 2 -23 to 1 will leave 1 unchanged. 2 -23 is the machine epsilon for float32.</p><p>0 5k 10k 15k 20k Epoch 0 20 40 60 80 100 Test acc. -float16 Test acc. -float32 Test acc. -float64 Train accuracies 50% Softmax Collapse (a) 40% training data 0 5k 10k 15k 20k Epoch (b) 60% training data 0 5k 10k 15k 20k Epoch (c) 70% training data Figure 2: As dataset size increases (subplots a to c), MLPs trained on modular addition begin to generalize without regularization until this is stopped by SC making the gradient from a large fraction of the samples equal to zero. This stopping point comes earlier for float32 than float64 and with small enough datasets it comes before the model makes any progress on test accuracy. Absorption errors in the Softmax. The Softmax function is a fundamental component in numerous deep learning architectures, serving as an activation function or a key element in attention mechanisms. In this case, we focus on its application within the Softmax Cross-Entropy (SCE) loss:</p><p>Definition 2 (Softmax Cross-Entropy (SCE) loss). For a neural network f and a data point x with label y, we define z := f (x) and z y as the logit corresponding to the true class y . We express the SCE loss as well as its equivalent numerically more stable formulation as:</p><formula xml:id="formula_0">L SCE (f (x), y) = -log e zy n k=1 e z k = -z y + max(z) + log n k=1 e z k -max(z)<label>(1)</label></formula><p>Unfortunately, even the rightmost (comparatively more stable) variant does not address this problem, since the kind of FP errors discussed in this work appear in the sum. While the Softmax function outputs are bounded between 0 and 1, the intermediate calculations involve summing exponentials of both positive and negative logits. These values can span several orders of magnitude, particularly in scenarios with large logits where the loss approaches zero. This wide range of values creates conditions that lead to absorption errors -leading to the phenomenon we call Softmax Collapse. Definition 3 (Softmax Collapse (SC)). A specific case of absorption error occurs when, for a given sample x, the logit from the correct class z y is significantly larger than the logits for all other classes. This floating-point absorption of smaller terms, which we call Softmax Collapse, occurs when:</p><formula xml:id="formula_1">n k=1 e z k . = e zy ,<label>(2)</label></formula><p>in which case the SCE loss becomes:</p><formula xml:id="formula_2">L SCE (f (x), y) . = -log</formula><p>e zy e zy = 0 .</p><p>(3) Thus, during SC the loss becomes identical to zero. Furthermore, for the correct class, the gradients become zero as well:</p><formula xml:id="formula_3">∂L SCE ∂z c = e zc n k=1 e z k -1 {c=y} . = 1 -1 {c=y} .<label>(4)</label></formula><p>While weights that contribute to the wrong classes can still get negative updates, we show that disappearance of the gradients from the correct classes is enough to inhibit grokking (Fig. <ref type="figure">2</ref>). We validate this in App. B.1 with an explicit intervention, showing that artificially setting the gradients from the correct class to zero stops generalization in a very similar way to what we observe in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EVIDENCE OF SOFTMAX COLLAPSE IN GROKKING TASKS</head><p>Grokking is often studied using dataset sizes for which the delay in generalization is significant, which is usually when the dataset is small but just large enough that generalization is possible. In this regime, regularization seems necessary for grokking and no improvement in test performance is observed without it <ref type="bibr" target="#b29">(Nanda et al., 2023)</ref>. However, a fact that has received less attention is that grokking can happen without regularization if the dataset is large enough <ref type="bibr" target="#b30">(Power et al., 2022)</ref>.</p><p>Here we hypothesize that as the size of the dataset decreases, overfitting becomes easier and Softmax Collapse (SC) happens earlier. To quantify this, we train an MLP without regularization on modular addition using different levels of FP precision, and calculate at every training epoch the fraction of samples that result in SC as per Eq. ( <ref type="formula" target="#formula_1">2</ref>). The results support our hypothesis that SC is responsible for the model's failure to generalize (Fig. <ref type="figure">2</ref>). Specifically, we see that generalization stops when SC begins -and that this happens earlier under float32 than under float64 (Fig. <ref type="figure">2b</ref>). Furthermore, this point is reached earlier as the dataset size decreases until it is reached before making any progress in the test accuracy, resulting in the common picture of no grokking without regularization (Fig. <ref type="figure">2a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PREVENTING SOFTMAX COLLAPSE LEADS TO GROKKING</head><p>To validate the importance of FP errors in stopping grokking, we show that methods to avoid SC lead to generalization on all the common grokking tasks on both MLPs and transformers. We introduce the following methods to postpone the appearance of FP errors.</p><p>Increasing floating point precision. The simplest way to avoid SC is to extend the FP precision from float32 to float64 for the Softmax calculation. We see in Fig. <ref type="figure">2</ref> that networks trained using float64 in the Softmax face SC later in training which allows for a further increase in test performance. Conversely, using float16 leads to SC earlier in training, leading to lower test performance. While this approach works as expected, FP precision cannot be extended indefinitely to allow for generalization as seen in the lack of grokking in Fig. <ref type="figure">2a</ref>.</p><p>StableMax Cross Entropy (StCE) Loss. As demonstrated above, SC is caused by adding the exponentials of very large positive and negative logits in the Softmax. To avoid these extreme summands, we propose using a softer version of Softmax to transform logits into probabilities before calculating the CE Loss:  Definition 4 (StableMax). We introduce a numerically stable version of the Softmax as:</p><formula xml:id="formula_4">StableMax(x i ) := s(x i ) j s(x j ) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">s(x) := x + 1 if x ≥ 0, 1 1-x if x &lt; 0 .<label>(6)</label></formula><p>As seen in Fig. <ref type="figure" target="#fig_3">3</ref>, s(•) is a simple ramp function that scales linearly instead of exponentially when x ≥ 0 and also approaches 0 more slowly than the exponential function when x &lt; 0. This is similar to the Softplus function <ref type="bibr" target="#b9">(Dugas et al., 2000)</ref> but approaches 0 more slowly with negative logits, further reducing the risk of absorption errors.</p><formula xml:id="formula_6">Proposition 1. StableMax is a modified Softmax, i.e. StableMax (x i ) = Softmax (g (x i ))</formula><p>where</p><formula xml:id="formula_7">g(x) = log(x + 1) if x ≥ 0, -log(-x + 1) if x &lt; 0 . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>The proof of this Proposition is presented in App. A. We then define the numerically stable analogue of</p><formula xml:id="formula_9">L SCE as L StCE (f (x), y) = -log(StableMax(z y ))</formula><p>, where z y again corresponds to the logit of the true class y.</p><p>To show that StCE indeed addresses the problems posed by SC, we repeat our experiments in Sec. 3.2 by replacing Softmax with StableMax. Our results, presented in Fig. <ref type="figure" target="#fig_5">4</ref>, indeed show that StableMax leads to grokking in commonly studied settings without regularization. Notably, this happens while the norm of the weights increases substantially <ref type="bibr">(Fig. 4,</ref><ref type="bibr">middle)</ref>. This suggests that while weight decay may lead to both grokking and a decreasing weight norm, the decreasing Test acc.addition mod 113 Test acc.product mod 113 Test acc.sparse parity (n=40, k=3) Train acc. 0 20k 40k 60k 80k Epoch 0 10k 20k 30k 40k Weight norm (L2) addition mod 113 L2 norm product mod 113 L2 norm Sparse parity L2 norm 0 200 400 600 800 1000 Epoch 0 20 40 60 80 100 Accuracy (%) Test accuracy -2-hot input Test accuracy -random binary input Training accuracies 50% Softmax Collapse weight norm is not necessary for grokking. Overall, these results i) provide additional evidence for the importance of SC in preventing grokking, ii) suggest a novel activation function to address this problem, and iii) show that regularization or weight norm modification is not necessary for grokking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DIAGNOSING THE CAUSES OF SOFTMAX COLLAPSE</head><p>In the previous section we have shown that FP errors arise due to a combination of low losses and large logits, and shown that when FP errors are mitigated, grokking can be observed in conditions where it previously was not. In this section, we dive deeper and ask why extremely low losses and large logits appear in the first place in grokking tasks. We identify two main causes for this tendency: (i) easiness of overfitting in grokking tasks, and (ii) a training dynamic that sees gradients align with what we call naïve loss minimization direction. After diagnosing the causes, the following section will use these insights to develop an optimization algorithm that avoids NLM in the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EASE OF OVERFITTING IN GROKKING TASKS</head><p>The first important characteristic of grokking tasks that lead to SC is their ease of overfitting. It has been observed that as grokking datasets get larger, overfitting becomes harder, eventually leading to a regime where train and test performances increase in tandem <ref type="bibr" target="#b30">(Power et al., 2022;</ref><ref type="bibr" target="#b29">Nanda et al., 2023;</ref><ref type="bibr" target="#b37">Varma et al., 2023)</ref>. It has also been shown that generalization can be delayed in the Sparse Parity task by increasing the amount of noise in the input, which makes overfitting easier <ref type="bibr" target="#b1">(Barak et al., 2022)</ref>. Here we investigate the opposite effect: that by decreasing the dimensionality of the input the data becomes harder to memorize, removing the delay in generalization.</p><p>To do this, we investigate the common grokking task of modular addition, but instead of the highdimensional one-hot representations of the input integers, we use a more compact binary. More specifically, we assign each integer a distinct random binary vector of dimension 14.</p><p>Results confirm our hypothesis, showing that as input representations are decreased in dimension, overfitting is prevented and models generalize without need for regularization <ref type="bibr">(Fig. 4,</ref><ref type="bibr">right)</ref>. This also shows that modular addition only induces grokking depending on the choice of representation. These findings highlight the importance of understanding the training dynamics beyond the point of overfitting (i.e. point of achieving 100% training accuracy), rather than focusing on the specifics of the modular arithmetic tasks as the key to explaining the delay in generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NAÏVE LOSS MINIMIZATION</head><p>We next identify a crucial training dynamic that commonly occurs in grokking tasks as a central cause for increasing logits and SC. We find that after reaching 100% training accuracy, gradient updates are dominated by an update direction we term naïve loss minimization (NLM). This direction</p><p>0 200 400 600 800 Epoch 0.0 0.2 0.4 0.6 0.8 1.0 Cosine Similarity (W, -L) layers.0.weight layers.1.weight layers.2.weight 100% Train accuracy (a) MLP without bias terms 0 200 400 600 800 Epoch 0.0 0.2 0.4 0.6 0.8 1.0 Cosine Similarity (W, -L) layers.0.weight layers.0.bias layers.1.weight layers.1.bias layers.2.weight layers.2.bias 100% Train accuracy (b) MLP with bias terms 0 200 400 600 800 Epoch 0.0 0.2 0.4 0.6 0.8 1.0 Cosine Similarity (W, -L) embed.W_E blocks.0.mlp.W_in blocks.0.mlp.W_h blocks.0.mlp.W_out unembed.W_U 100% Train accuracy (c) Transformer with bias terms does not change the model's decision boundary, but still decreases loss by simply scaling the logits of the predictions, in most cases through scaling of parameters (see below). This means that the logits will continue to increase until they inevitably lead to SC and zero terms in the training gradient. This stops the parameter updates in any direction, including NLM and any other useful component that would have been included in the overall gradient. We now define NLM formally, and proceed to discuss why it might commonly be observed to deteriorate training in grokking tasks. Given the input x ∈ X , output y ∈ Y, a predictor f parametrized by θ ∈ R m that outputs logits z = f (θ; x) ∈ R |Y| , and a loss function L, we now define Naïve Loss Minimization. Definition 5 (Naïve Loss Minimization (NLM)). A function d NLM : R m → R m specifies a direction of naïve loss minimization if it decreases the loss,</p><formula xml:id="formula_10">L(f (θ + d NLM (θ); •)) &lt; L(f (θ; •)),<label>(8)</label></formula><p>while satisfying for some c &gt; 1:</p><formula xml:id="formula_11">f (θ + d NLM (θ); x) = cf (θ; x), ∀x ∈ X ,<label>(9)</label></formula><p>where X denotes the input space and L(f (θ+d NLM (θ); •)) is the total loss over the training dataset.</p><p>We find that under a large class of models, namely those that demonstrate positive homogeneity, when training beyond 100% training accuracy the direction of the weights is an NLM direction. Definition 6 (Positive Homogeneity <ref type="bibr" target="#b25">(Lyu &amp; Li, 2020)</ref>). A function f is positively homogeneous of degree L &gt; 0 if for all weights θ, inputs x, and scalars c &gt; 0, it satisfies:</p><formula xml:id="formula_12">f (cθ; x) = c L f (θ; x) . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>When f is a homogeneous neural network, L corresponds to the number of layers.</p><p>In the case of homogeneous networks, training beyond 100% training accuracy, scaling the logits always leads to a decrease in the training loss. Therefore,</p><formula xml:id="formula_14">d NLM (θ) = αθ for α &gt; 0 is an NLM direction, as it results in f (θ + d NLM (θ); x) = f ((1 + α)θ; x) = (1 + α) L f (θ; x)</formula><p>, where the second equality follows from Eq. (10).</p><p>Many neural network architectures, such as ReLU MLPs and transformers without bias terms, are positively homogeneous or approximately homogeneous in the case of transformers <ref type="bibr" target="#b28">(Merrill et al., 2020)</ref>. While more complex deep learning models with skip connections and bias terms are not homogeneous, they have been shown to be quasi-homogeneous <ref type="bibr" target="#b21">(Kunin et al., 2023)</ref> and in most cases -including all of the models in this work, the last layer is homogeneous. This means that for non-homogeneous models scaling the weights of the last layer corresponds to a direction of NLM.</p><p>The fact that the gradients converge to the direction of the weights has been studied in previous works <ref type="bibr" target="#b16">(Ji &amp; Telgarsky, 2020;</ref><ref type="bibr">2019;</ref><ref type="bibr">2018;</ref><ref type="bibr" target="#b25">Lyu &amp; Li, 2020)</ref> to prove that homogeneous networks converge in direction under gradient flow and gradient descent (GD), and they perform normalized margin maximization even beyond the point of 100% training accuracy <ref type="bibr" target="#b25">(Lyu &amp; Li, 2020)</ref>. However, we argue that gradient alignment also results in scaling of the logits which can lead to SC and put an end to the margin maximization described in <ref type="bibr" target="#b25">Lyu &amp; Li (2020)</ref>, when working with limited floating point precision. While we study delayed generalization, the link between training trajectories and generalization is already established in prior art <ref type="bibr" target="#b2">(Birdal et al., 2021;</ref><ref type="bibr" target="#b0">Andreeva et al., 2024)</ref>.</p><p>Evidence of naïve loss minimization. In practice, we observe that in MLPs and transformers with and without bias terms, the gradients quickly become aligned with the direction of the weights after the point of overfitting (Fig. <ref type="figure" target="#fig_6">5</ref>). Particularly for the later layers of the models, the cosine similarity between the parameter updates and the NLM direction goes up to 0.9 for the output layers. While models with bias terms are not homogeneous and there is no theoretical guarantee that scaling the weights will reduce the SCE loss, in practice, we observe very similar behaviour in MLPs with (Fig. <ref type="figure" target="#fig_6">5b</ref>) and without (Fig. <ref type="figure" target="#fig_6">5a</ref>) bias terms. In the case of a one-layer transformer, the alignment is stronger for the embed and unembed matrices but also substantial for the MLP weights (Fig. <ref type="figure" target="#fig_6">5c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MITIGATING NA ÏVE LOSS MINIMIZATION LEADS TO GROKKING</head><p>While we have shown in Sec. 3 that avoiding numerical instabilities eventually leads to generalization, we can also target the NLM process that causes these numerical issues. To do this, we design an optimizer that only preserves the part of the gradient orthogonal to the direction of the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">⊥Grad: AN OPTIMIZER TO PREVENT NLM</head><p>We propose a new optimizer, ⊥Grad (read "ortho-grad"), that updates the weights based only on the part of the gradient that is orthogonal to the current direction of the weights: Definition 7 (⊥Grad). We propose the following update rule for a given iteration t ∈ N:</p><formula xml:id="formula_15">θ t+1 = θ t -η∇ ⊥ L(θ t ),<label>(11)</label></formula><p>where the orthogonal component of the gradient, ∇ ⊥ L(θ t ), is obtained by projection onto the hyperplane orthogonal to the current weight vector:</p><formula xml:id="formula_16">∇ ⊥ L(θ t ) = ∇L(θ t ) - θ ⊤ t ∇L(θ t ) θ ⊤ t θ t θ t . (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>Proposition 2. Assuming ∇ ⊥ L(θ t ) ̸ = 0, ∃ β &gt; 0 such that for any learning rate 0 &lt; η &lt; β, taking the step η∇ ⊥ L(θ t ) reduces the loss. In other words, any nonzero ∇ ⊥ L(θ t ) is a descent direction.</p><p>Sketch of the proof. We show that any ∇ ⊥ L(θ t ) ∈ R m \{0} is a descent direction by demonstrating that ⟨-∇ ⊥ L(θ t ), ∇L(θ t )⟩ &lt; 0. For a full proof we refer the reader to App. A.</p><p>This projection of the gradient can be incorporated into different optimizers. In Fig. <ref type="figure" target="#fig_7">6a</ref>, we show results for ⊥AdamW and ⊥SGD, the ⊥Grad versions of AdamW and SGD respectively. These results show that ⊥Grad optimizers lead to generalization without a phase of initial overfitting, in contexts where no improvement in test performance is usually observed without weight decay. We  note that similar projections of the gradients have been used in other settings to mitigate the effects of momentum in invariant layers <ref type="bibr" target="#b12">(Heo et al., 2021)</ref>, stabilize training <ref type="bibr" target="#b38">Wang et al. (2024)</ref> or as one part in a more complex optimizer <ref type="bibr" target="#b18">(Kosson et al., 2024)</ref>. We design ⊥Grad as a more precise intervention that direcly prevents scaling along the NLM direction.</p><p>In Fig. <ref type="figure" target="#fig_9">7</ref>, we compare the trajectories of models using SGD with and without weight decay to our new ⊥SGD optimizer. SGD models start on a similar trajectory, reducing the training loss but increasing the test loss, until the model with weight decay changes direction and starts minimizing both the train and test loss. In contrast, the model using ⊥SGD moves directly in a direction that minimizes both the train and test loss. While SGD with weight decay eventually reaches a point of lower loss, note that ⊥SGD reaches 100% test accuracy within 400 iterations (Fig. <ref type="figure" target="#fig_7">6a</ref>). Beyond showing how ⊥SGD prevents NLM, Fig. <ref type="figure" target="#fig_9">7</ref> also suggests that weight decay induces grokking by avoiding NLM. In the following, we highlight that the success of several methods to induce grokking can be explained from this perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EXPLAINING THE SUCCESS OF EXISTING METHODS FOR GROKKING</head><p>In light of our findings, we are able to explain the success of several previously proposed methods to induce grokking. We find that these methods also lead to grokking by mitigating NLM and avoiding the FP errors that come with extremely low losses.</p><p>Weight decay. We have argued that the problem faced in grokking is that the ease of overfitting leads to NLM, which corresponds to scaling up the weights for homogeneous networks. Since weight decay corresponds to pulling back the weights along this same direction at every step during training, it is unsurprising, given our findings, that it is the most reliable way to induce grokking.</p><p>To explain why generalization tends to be delayed when using weight decay, as opposed to ⊥Grad, we look at it from the perspective of L2 regularization which is equivalent to weight decay for SGD. In Fig. <ref type="figure" target="#fig_7">6c</ref>, we see an initial phase where classification loss decreases, at the cost of the L2 loss. Eventually, the decrease in classification loss from NLM stops outweighing the increase in L2 loss, meaning that only updates that are not aligned with the NLM direction are followed. This explains why weight decay leads to generalization in grokking tasks but only after scaling along the NLM direction no longer decreases the overall loss. This balance between weight decay and classification loss is similar to the rotational equilibrium studied in <ref type="bibr" target="#b18">Kosson et al. (2024)</ref>.</p><p>We argue that the main roles of weight decay are preventing floating point errors and preventing NLM. This is in line with recent findings about the role of weight decay in deep learning <ref type="bibr" target="#b5">(D'Angelo et al., 2023)</ref> which point to the fact that it increases the effective learning rate and avoids floating point issues when using mixed-precision training in LLMs.</p><p>MSE loss on shallow networks. While cross-entropy loss can be reduced indefinitely by scaling the logits through NLM, this is not the case with MSE loss. When using MSE loss the logits can overshoot the target, meaning that larger logits often do not lead to a lower MSE loss. This explains why <ref type="bibr" target="#b1">Barak et al. (2022)</ref>, <ref type="bibr" target="#b20">Kumar et al. (2024), and</ref><ref type="bibr" target="#b26">Lyu et al. (2024)</ref> observed grokking with MSE loss without regularization. Interestingly, networks with more than one hidden layer do not generalize in these same settings (Fig. <ref type="figure" target="#fig_14">13</ref>).</p><p>Delaying generalization by scaling the weights. While the lazy training dynamics described in <ref type="bibr" target="#b20">Kumar et al. (2024)</ref> explain an important part of why scaling the weights delays generalization, we show that the reason that regularization is often needed to exit this lazy training regime is that scaling the weights or the logits facilitates SC. In App. D.2, we show that the setting used in <ref type="bibr">Liu et al. (2023b)</ref> to induce grokking on MNIST with SCE also induces SC which prevents further learning in the absence of weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Grokking. <ref type="bibr" target="#b30">Power et al. (2022)</ref> introduced grokking and showed that weight decay can consistently induce it in algorithmic tasks. <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> were able to reverse engineer the inner workings of a grokked transformer and found progress measures for grokking induced by weight decay. <ref type="bibr" target="#b4">Chughtai et al. (2023)</ref> generalized the findings from <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> and showed grokked networks use group representations to solve group composition tasks, although some of these findings were disputed in <ref type="bibr" target="#b35">Stander et al. (2024)</ref> which propose that grokked networks learn a coset based algorithm for these same tasks. <ref type="bibr" target="#b27">Mallinar et al. (2024)</ref> has shown that grokking is not specific to neural networks or gradient-based optimization and cannot be predicted from the training or test loss. <ref type="bibr" target="#b37">Varma et al. (2023)</ref> argued that grokking is driven by weight decay favoring more efficient solutions and <ref type="bibr">Liu et al. (2023b)</ref> hypothesized that the weight norm of the models needs to be in a "Goldilock's zone" to generalize. <ref type="bibr" target="#b20">Kumar et al. (2024)</ref> and <ref type="bibr" target="#b26">Lyu et al. (2024)</ref> connected grokking to a transition between "lazy training" <ref type="bibr" target="#b3">(Chizat et al., 2018)</ref> and feature learning, and <ref type="bibr" target="#b20">Kumar et al. (2024)</ref> showed that this can happen without regularization in the case of shallow networks with MSE loss. Grokking has also been described as a phase transition by <ref type="bibr" target="#b39">Žunkovič &amp; Ilievski (2024)</ref>, <ref type="bibr" target="#b26">Lyu et al. (2024)</ref> and <ref type="bibr" target="#b33">Rubin et al. (2024</ref><ref type="bibr" target="#b13">). Humayun et al. (2024)</ref> show that in many settings, neural networks undergo grokking-like transitions in their adversarial robustness. This aligns with the findings of <ref type="bibr" target="#b25">Lyu &amp; Li (2020)</ref> which attributed this increased robustness to a bias of SGD towards a max-margin solution which was proven for homogeneous models.</p><p>Numerical instability in deep learning. Numerical instability is a common issue in deep learning <ref type="bibr" target="#b17">Kloberdanz et al. (2022)</ref>, especially when dealing with mixed precision training D' <ref type="bibr">Angelo et al. (2023)</ref>. It is known that the Softmax function is particularly prone to numerical stability problems although this often comes in the form of overflow in the exponential <ref type="bibr" target="#b17">(Kloberdanz et al., 2022)</ref> and not from absorption errors in the sum as observed in this case. In the grokking setting, <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> showed that the slingshots observed in <ref type="bibr" target="#b36">Thilak et al. (2022)</ref> can be explained by a very similar mechanism to the one involved in SC, although <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> do not use it to explain any grokking phenomena beyond these spikes that sometimes appear in the training process in grokking tasks. We believe the slingshots observed in <ref type="bibr" target="#b36">Thilak et al. (2022)</ref> could be a mechanism to prevent full SC, explaining why slingshots can lead to grokking without weight decay in some settings. This is further discussed in App. H. Issues with numerical instability when training beyond overfitting with increasing learning rates were also observed in <ref type="bibr" target="#b25">Lyu &amp; Li (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND DISCUSSION</head><p>In this work, we show that naïve loss minimization (NLM) and floating point errors can explain why generalization is delayed in grokking and why it often does not happen without regularization. Using this insight, we are able to explain the success of existing methods to induce grokking. Motivated by our findings, we further design a simple modification to the Softmax that induces grokking by avoiding floating point errors and an optimizer that avoids the delay in generalization in grokking by preventing NLM.</p><p>Limitations &amp; future work. While this work explains several surprising aspects of grokking settings, several questions remain. Notably, we focus our study of NLM on homogeneous or approximately homogeneous models. A a formal characterization quasi-homogenous models could shed light on this kind of dynamics for models including skip connections and bias terms. Additionally, our explanation for why weight decay causes grokking could be enhanced by an analysis of the impact of weight decay on the effective learning rate as a potential explanation for the sudden nature of grokking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In support of the main paper, App. A presents the proofs for the propositions in the paper, App. B includes additional findings that support our main results, and App. D provides further discussion on conditions that lead to grokking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>Proof of Prop. 1.</p><formula xml:id="formula_18">Softmax (g (xi)) = e g(x i ) j e g(x j ) (13) =      e log(x i +1) j e log(x j +1) if xi ≥ 0, e -log(-x i +1) j e -log(-x j +1) if xi &lt; 0 (14) =      x i +1 j x j +1 if xi ≥ 0, 1 -x i +1 j 1 -x j +1 if xi &lt; 0 (15) = StableMax(xi).<label>(16)</label></formula><p>Proof of Prop. 2. To prove that any nonzero -∇ ⊥ L(θt) is a descent direction, we need to show that</p><formula xml:id="formula_19">⟨-∇ ⊥ L(θt), ∇L(θt)⟩ &lt; 0, assuming ∇ ⊥ L(θt) ̸ = 0: ∇L(θt), -∇L(θt) + θ ⊤ t ∇L(θt) θ ⊤ t θt θt ≤ 0.<label>(17)</label></formula><p>Expanding this yields:</p><formula xml:id="formula_20">-∥∇L(θt)∥ 2 2 + ∇L(θt), θt θ ⊤ t ∇L(θt) θ ⊤ t θt ≤ 0.<label>(18)</label></formula><p>Since the inequality is unaffected by the scaling of the left hand side, we can, without loss of generality, assume that the gradients are normalized, leading to:</p><formula xml:id="formula_21">∇L(θt), θt θ ⊤ t ∇L(θt) θ ⊤ t θt ≤1.<label>(19)</label></formula><p>Since θt</p><formula xml:id="formula_22">θ ⊤ t ∇L(θ t ) θ ⊤ t θ t</formula><p>denotes the projection of the gradient onto the space spanned by the weights, ⟨•, •⟩ will measure the acute angle of incidence and hence Eq. ( <ref type="formula" target="#formula_21">19</ref>) holds, with equality iff ∇ ⊥ L(θt) = 0, which is prevented by assumption. This proves that -∇ ⊥ L(θt) is a descent direction while being perpendicular to the weights.</p><p>We note that the ⊥ Grad stops when ∇ ⊥ L(θt) = 0. If ∇L(θt) ̸ = 0, this corresponds to the condition where the gradient is in the same direction with the parameter vector. ∇ ⊥ L(θt) = 0 can also be the case if ∇L(θt) = 0, which corresponds to the loss function being at a local optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL FINDINGS B.1 FURTHER EVIDENCE THAT SC PREVENTS GROKKING</head><p>While SC leads the gradient from correctly predicted samples to be zero, it does not do this for the incorrect classes. To validate that setting the gradients from the correct classes to zero is enough to stop learning, we do this artificially for a model that is generalizing and show that learning stops after this intervention. In Fig. <ref type="figure">8</ref> we see that the baseline model shown in geen generalizes, but this is stopped at epoch 6000 for the model shown in blue, after we perform this intervention. The intervention is implemented by multiplying the logits for the right classes by 0 at each step after epoch 6000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SGD WITH LEARNING RATE SCHEDULING</head><p>To show that our results are not due to the inductive bias of adaptive moments in optimizers like AdamW, we replicate some of the AdamW results using SGD with a learning rate scheduler.</p><p>Our scheduler is similar to the one in <ref type="bibr" target="#b25">Lyu &amp; Li (2020)</ref> except at each step we divide the learning rate by the norm of the full gradient, instead of the loss.</p><p>In Fig. <ref type="figure">9</ref> we observe that SC also puts an end to grokking in this setting. Unexplored in the main paper, NLM also has the effect of reducing the effective learning rate. For a gradient update using regular gradient descent θt+1 = θt -η∇L(θt) it is easy to see that ||θt+1 -θt|| → 0 as ||∇L(θt)|| → 0. This problem has been observed before when training beyond the point of overfitting, for example, <ref type="bibr" target="#b25">Lyu &amp; Li (2020)</ref> addressed it by using a loss based learning rate scheduler to keep up with the gradient. Theoretically, an alternative could be to simply extend the duration of training. According to our hypothesis, training for long enough should eventually lead to generalization on grokking tasks if we prevent SC. However, we find that another kind of floating point error can also appears in these settings, namely, gradient absorption errors in the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EFFECTIVE LEARNING RATE</head><p>For a weight w, gradient absorption errors happen when a gradient update is small enough that it leaves the weight unchanged. Using the notation outlined in this paper this can be formalised as w -η ∂L ∂w . = w. In Fig. <ref type="figure" target="#fig_11">10</ref> we show that this happens for an MLP trained with SGD on modular addition using 30% of the training data. As the norm of the gradient decreases, the percenage of the gradients that are absorbed by the weights increases substantially. Note that the number of gradients that are exactly zero remains stable while the number of absorbed gradients increases substantially.</p><p>This issue is naturally mitigated by second order moments for adaptive optimizers like Adam and AdamW which is why they do not frequently appear. However, they do prevent us from showing grokking with vanilla gradient descent without any learning rate schduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 ADDITIONAL WAYS TO INDUCE GROKKING</head><p>Beyond the interventions described in the main text, we highlight two additional ways to induce grokking that validate our hypothesis. Logit norm regularization. Since we argue that uncontrolled scaling of the logits is responsible for delaying grokking and leading to SC, we validate that preventing this scaling of the logits by adding the norm of the logits to the loss, leads to grokking without additional regularization (Fig. <ref type="figure" target="#fig_12">11b</ref>). Taylor approximation of the Softmax. We have introduced StableMax as a change to the Softmax that leads to grokking without regularization. The motivation behind this is to prevent values in the sum of the Softmax that are very large or very close to zero. To this end, replacing the exponential with any function that is sub-exponential beyond a certain point should have a similar effect. To demonstrate, we perform a further experiment using the second order Taylor approximation of the exponential</p><formula xml:id="formula_23">e x ≈ 1 + x + x 2 2! ,<label>(20)</label></formula><p>replacing the exp in the Softmax. Since the Taylor approximation is decreasing for x &lt; 0, we subtract the minimum logit to avoid this part of the function. We deem this version Taylor -Softmax. In Fig. <ref type="figure" target="#fig_12">11</ref> we see results similar to the ones in Sec. 3.3 but showing the losses instead of the accuracies as well as results for two additional methods to induce grokking. Note that our implementation of Taylor -Softmax (Fig. <ref type="figure" target="#fig_12">11c</ref>) introduces an additional implicit regularization similar to the one in Fig. <ref type="figure" target="#fig_12">11b</ref>, due to the gradient flowing through the subtraction of the mean. While this effectively combines the effects of Fig. <ref type="figure" target="#fig_12">11a</ref> and Fig. <ref type="figure" target="#fig_12">11b</ref>, leading to grokking faster than the other two methods, our main paper shows results using StableMax as a cleaner intervention that does not introduce this additional regularization effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 SOLUTION LEARNED DURING GROKKING WITHOUT WEIGHT DECAY</head><p>Weight decay has been identified as potentially responsible for inducing the periodic structures in the weights studied in <ref type="bibr" target="#b29">Nanda et al. (2023)</ref>. In Fig. <ref type="figure" target="#fig_13">12</ref> we show that MLPs that grok without weight decay on modular addition show a similar sparsity in Fourier space as the one observed in <ref type="bibr" target="#b29">Nanda et al. (2023)</ref>. While these are very superficial results, they suggest that these structures can emerge without a weight decay-induced "clean up" phase as described in <ref type="bibr" target="#b29">Nanda et al. (2023)</ref>.</p><p>D FURTHER DISCUSSION ON CONDITIONS THAT LEAD TO GROKKING</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 L1 REGULARIZATION AND GROKKING</head><p>While it has been observed that L1 regularization can lead to grokking in some settings, <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> consistently found no grokking with L1 regularization and transformers and this setting has received substantially less attention than weight decay.</p><p>We observe that NLM scales the weights along their current direction. This means that larger weights are scaled more than small weights. However, while the sign of the gradient from L1 regularization depends on the sign of the weights, the magnitude of this gradient does not depend on the magnitude of the weights. This means that, particularly on deep networks or transformers with with large weights, L1 can sometimes be insufficient to prevent NLM and the subsequent SC.  We find that MLPs with weights scaled up by 100 operate at the "edge of numerical stability" and in the absence of weight decay, SC eventually reaches 100%, preventing any further generalization. When using weight decay, the weight norm is reduced, mittigating SC and eventually allowing for further generalization as the SC rate drops from 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 DELAYING GENERALIZATION BY SCALING THE WEIGHTS</head><p>Scaling the logits can delay generalization but not induce it. <ref type="bibr">Liu et al. (2023a)</ref>, <ref type="bibr" target="#b20">Kumar et al. (2024)</ref> and <ref type="bibr" target="#b26">Lyu et al. (2024)</ref> showed that an α parameter multiplying the logits can increase or reduce the delay in generalization. We highlight in Fig. <ref type="figure" target="#fig_14">13</ref> that this is true for cases where generalization happens even without changing the scale of the logits (α = 1). However, in most cases when using deeper networks or cross-entropy loss, models do not generalize by default without regularization and we are unable to induce grokking for any value of α.</p><p>We argue in Sec. 5.2 that the observation in <ref type="bibr">Liu et al. (2023a)</ref>, <ref type="bibr" target="#b20">Kumar et al. (2024)</ref> and <ref type="bibr" target="#b26">Lyu et al. (2024)</ref> of grokking without regularization are due to the inductive bias of MSE loss which prevents NLM and leads to grokking in some settings for shallow networks.</p><p>Grokking on MNIST. We replicate the setting from <ref type="bibr">Liu et al. (2023b)</ref> of grokking on MNIST with crossentropy loss and show that without weight decay, the scaling factor of the weights leads to significant FP errors, preventing grokking from happening until this is alleviated by weight decay.</p><p>While SC explains why weight decay is needed to get the jump in performance observed in Fig. <ref type="figure" target="#fig_15">14b</ref>. It could also explain why inducing grokking by scaling the weights is less effective when using SCE. While when using MSE loss, <ref type="bibr">Liu et al. (2023a)</ref> are able to induce full grokking from random level predictions to close to full training accuracy, the same does not seem to be possible when using SCE. In fact, we see in Fig. <ref type="figure" target="#fig_15">14b</ref> that since the begining of training the rate of SC approaches 100%. This could explain why the observations with cross-entropy loss are not the ones predicted by the lazy training theories outlined in <ref type="bibr" target="#b20">Kumar et al. (2024)</ref> which do not take limited floating point precision into account. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ⊥Grad AND WEIGHT DECAY</head><p>In Fig. <ref type="figure" target="#fig_6">15</ref>, we provide a more in depth comparison of ⊥ Grad and weight decay. Fig. <ref type="figure" target="#fig_6">15a</ref> higlights that increasing the weight decay multiplier leads to a smaller delay in generalization, but only up to a point. In this concrete setting, a weight decay multiplier of 8, prevents the model from fully generalizing (Fig. <ref type="figure" target="#fig_6">15a</ref>). We then compare the best value of weight decay in this setting to ⊥Grad, which does not require any hyper-parameter tuning. Fig. <ref type="figure" target="#fig_6">15b</ref> shows that ⊥Grad leads to faster grokking even when compared to a tuned value of weight decay. Note that the models with weight decay overfit immediately before grokking while ⊥ Grad reaches 100% train and test accuracies almost at the same time. While any intervention that prevents SC should lead to grokking or generalization, Fig. <ref type="figure" target="#fig_17">16</ref> shows that scaling the temperature of the Softmax is not enough to prevent SC and label smoothing does prevent SC and lead to some generalization, but at the cost of introducing another inductive bias that prevents full generalization and leads to qualitatively different behavior. By comparison, the simple change introduced in Stablemax prevents SC and leads to grokking, serving as a validation for our hypothesis that gradient descent leads to grokking by default, unless this is stopped by SC.</p><p>G StableMax AND ⊥Grad IN REALISTIC SETTINGS While Stablemax and ⊥ Grad are designed as interventions to show that preventing SC leads to grokking and preventing NLM leads to generalization (Fig. <ref type="figure" target="#fig_0">1</ref>), in this section we explore if these methods are applicable in more realistic settings like language modeling with GPT2-small or ResNets trained on image classification. We train GPT2-Small for 1 epoch on WikiText-103 using a batch size of 16, a block size of 512, a learning rate of 5e -4 and a weight decay of 0.01 using AdamW. The architecture is the regular GPT2-Small architecture from <ref type="bibr" target="#b32">Radford et al. (2019)</ref>, trained with a cosine schedule and 1000 steps of warmup.</p><p>For CIFAR10, CIFAR100 and Imagenet-1k <ref type="bibr" target="#b34">(Russakovsky et al., 2015)</ref>, our baseline is a ResNet18 with SCE loss trained with SGD 0.9 momentum and 1e -4 weight decay. We use standard data transformations such as random crop and random horizontal flip and a step learning rate scheduler every 30 epochs for a full training run of 100 epochs. With respect to this baseline we report results replacing the Softmax with StableMax in the loss function, as well as replacing SGD with ⊥SGD. Since test labels for Imagenet-1k are not publicly available, we use the validation set as a test set and tune hyper-parameters on a fraction of the training set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Our contributions demonstrated through results obtained in addition modulo 113 task. We show that the delay in generalization induced by NLM can be reversed using the proposed ⊥AdamW ((a) and (b)) and that the numerical errors that lead to overfitting instead of grokking can be avoided by using the proposed StableMax ((b) and (c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>. = to refer to equality under FP arithmetic. Definition 1 (Absorption Errors). Let a, b ∈ R \ {0} be floating point numbers in a system with base β and p significand bits. Denote their exponents by e a and e b , respectively. An absorption error occurs in the computation of a + b (denoted a + b . = a) if e a -e b ≥ p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: s(x) vs. e x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: (left) Grokking with StCE loss and no regularization on three common grokking datasets using an MLP with 2 hidden layers of width 200. We use 40% of all pairs modulo 113 which is the same setting as Fig.2awhere regular SCE gets stuck at random level performance (random level is 50% for sparse parity). (middle) Evolution of model weight norms during training for the same models and tasks. This shows that grokking induced without weight decay does not follow the commonly observed trend of rapidly decreasing weight norm during generalization. (right) Changing input representations turns modular addition into regular machine learning tasks with train and test accuracy increasing in tandem, see Sec. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: MLPs with (a) and without (b) bias terms trained on modular addition receive updates that are significantly aligned with the direction of NLM beyond the point of overfitting. In (c) we show these results for a selection of parameters for our one layer transformer. We highlight the embed and unembed matrices as well as the weights of the MLP. These are highlighted in the plot using the notation from<ref type="bibr" target="#b10">Elhage et al. (2021)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparing ⊥AdamW and ⊥SGD with baseline optimizers and AdamW with weight decay on (a) a transformer trained on subtraction mod 113 and (b) an MLP trained on addition modulo 113.In (c) we highlight the trade-off between L2 regularization and SCE loss, initially SCE loss is reduced at the cost of increasing the L2 loss but eventually the two losses decrease simultaneously (Sec. 5.2). we argue that gradient alignment also results in scaling of the logits which can lead to SC and put an end to the margin maximization described in<ref type="bibr" target="#b25">Lyu &amp; Li (2020)</ref>, when working with limited floating point precision. While we study delayed generalization, the link between training trajectories and generalization is already established in prior art<ref type="bibr" target="#b2">(Birdal et al., 2021;</ref><ref type="bibr" target="#b0">Andreeva et al., 2024)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Model trajectories in in parameter space projected to 2D over the SCE loss landscape. SGD with weight decay starts along the same trajectory as SGD decreasing the training loss (a) but increasing the test loss (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :Figure 8 :</head><label>98</label><figDesc>Figure9: We show that the same dynamics observed in Fig.2can be observed with a learning rate scheduler instead of AdamW. This shows that this is not due to an implicit bias of adaptive optimizers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Gradient absorption errors during training on addition modulo 113.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Train and test losses during grokking induced by three different interventions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Fourier components of the weights of the output layer of an MLP trained on addition mod 113. Grokking is induced via StableMax and without weight decay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: The α parameter controls generalization in settings where it happens by default. This is the case for shallow networks with MSE loss as shown in subplot (a). However, in deeper networks (b) or networks with CE loss and no regularization (c), α can control the time of over-fitting, but no value of α is enough to trigger grokking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure14: Replicatting the grokking on MNIST for weight decay setting fromLiu et al. (2023b). We find that MLPs with weights scaled up by 100 operate at the "edge of numerical stability" and in the absence of weight decay, SC eventually reaches 100%, preventing any further generalization. When using weight decay, the weight norm is reduced, mittigating SC and eventually allowing for further generalization as the SC rate drops from 100%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>Figure 15: Increasing weight decay (WD) for an MLP trained on modular addition with AdamW reduces the delay in generalization up to a point where WD prevents convergence Fig. 15a. Without any tunable hyper-parameters and without WD, ⊥Grad leads to grokking faster than the best model with WD Fig. 15b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>FFigure 16 :</head><label>16</label><figDesc>Figure 16: StableMax prevents SC and leads to grokking while temperature scaling with T = 1e5 only gradually delays SC, and label smoothing does prevent SC but at the cost of keeping the model from fully generalizing.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by the <rs type="funder">UKRI Centre for Doctoral Training in Safe and Trusted AI</rs> [<rs type="grantNumber">EP/S0233356/1</rs>]. TB acknowledges support from the <rs type="funder">Engineering and Physical Sciences Research Council</rs> [grant <rs type="grantNumber">EP/X011364/1</rs>]. TB was supported by a <rs type="funder">UKRI</rs> <rs type="grantName">Future Leaders Fellowship</rs> [grant number <rs type="grantNumber">MR/Y018818/1</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xCHukQx">
					<idno type="grant-number">EP/S0233356/1</idno>
				</org>
				<org type="funding" xml:id="_2aKsQpw">
					<idno type="grant-number">EP/X011364/1</idno>
				</org>
				<org type="funding" xml:id="_BXBYm6S">
					<idno type="grant-number">MR/Y018818/1</idno>
					<orgName type="grant-name">Future Leaders Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>H SC AND THE SLINGSHOT EFFECT <ref type="bibr" target="#b36">Thilak et al. (2022)</ref> observed that spikes in the training loss appear when training on grokking tasks with adaptive optimizers like Adam, and that these spikes can lead to generalization without weight decay. Although <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> showed that slingshots are not necessary for grokking, it is still unclear what mechanism of adaptive gradient optimizers induces this behavior and why it leads to generalization. In light of the results in this paper, we believe that slingshots could lead to generalization because they prevent full SC. <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> pointed out that something like SC could be responsible for these slingshots. One possible mechanism would be that zero gradients for some samples due to SC rapidly diminish the second-order moments leading to a large update or slingshot which moves the model away from full SC, although more research would be needed to properly show this.</p><p>While related to our work, slingshots are a different kind of instability which only appears with adaptive optimizers and can allow grokking. In contrast, we identify SC as a very specific issue in the Softmax that can affect any model trained with SCE, not only the ones trained with adaptive optimizers. Additionally SC prevents grokking whereas slingshots can lead to it. Wether and how slingshots are cause by SC remains an open research question, with some supporting evidence from <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> which show that slingshots can disappear when using f loat64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I ADDITIONAL DETAILS ABOUT FLOATING POINTS</head><p>Beyond our main results, we found that in some cases, grokking could be stopped before SC due to the ϵ parameter in Adam being too large. While the ϵ term is designed to give numerical stability to the gradients, in settings with extremely low losses and gradients, the second order moments can be dominated by the ϵ term, putting an end to learning where it would have continued with a smaller ϵ value. This echoes the results in <ref type="bibr" target="#b36">Thilak et al. (2022)</ref> which shows that increasing ϵ halts slingshots and grokking, with <ref type="bibr" target="#b29">Nanda et al. (2023)</ref> also alluding to the ϵ parameter being important in some cases.</p><p>Surprisingly, we also found that a simple re-implementation of torch.nn.f unctional.log sof tmax that does not use the official CUDA kernels can lead the models to keep learning beyond the point where the loss is exactly 0 and some gradients should be 0 with appropriate calculation, outperforming the official implementation for grokking tasks. Learning eventually also stops in this setting and this seems more like a quirk of how gradients are calculated in PyTorch in the absence of an explicitly defined backward pass.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topological generalization bounds for discrete-time stochastic optimization algorithms</title>
		<author>
			<persName><forename type="first">Rayna</forename><surname>Andreeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Tolga Birdal, and Umut S ¸ims ¸ekli</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hidden progress in deep learning: Sgd learns parities near the computational limit</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intrinsic dimension, persistent homology and generalization in neural networks</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><surname>Simsekli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On lazy training in differentiable programming</title>
		<author>
			<persName><forename type="first">Lénaïc</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
			<biblScope unit="page" from="2933" to="2943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A toy model of universality: Reverse engineering how networks learn group operations</title>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Chughtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D'</forename><surname>Francesco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName><surname>Andriushchenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04415</idno>
		<title level="m">Aditya Varre, and Nicolas Flammarion. Why do we need weight decay in modern deep learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ppfnet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating second-order functional knowledge for better option pricing</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claude</forename><surname>Franc ¸ois Bélisle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A mathematical framework for transformer circuits</title>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2021/framework/index.html" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Gromov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.02679</idno>
		<title level="m">Grokking modular arithmetic</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights</title>
		<author>
			<persName><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyuwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Imtiaz Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15555</idno>
		<title level="m">Deep networks always grok and here is why</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Risk and parameter convergence of logistic regression</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07300</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient descent aligns the layers of deep linear networks</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Directional convergence and alignment in deep learning</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17176" to="17186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepstability: A study of unstable numerical methods and their solutions in deep learning</title>
		<author>
			<persName><forename type="first">Eliska</forename><surname>Kloberdanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">G</forename><surname>Kloberdanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International Conference on Software Engineering</title>
		<meeting>the 44th International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="586" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rotational equilibrium: How weight decay balances learning across neural networks</title>
		<author>
			<persName><forename type="first">Atli</forename><surname>Kosson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bettina</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grokking as the transition from lazy to rich training dynamics</title>
		<author>
			<persName><forename type="first">Tanishq</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Bordelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Pehlevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The asymmetric maximum margin bias of quasi-homogeneous neural networks</title>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Daniel Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yamamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Omnigrok: Grokking beyond algorithmic data</title>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grokking as simplification: A nonlinear complexity perspective</title>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqian</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UniReps: the First Workshop on Unifying Representations in Neural Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Ang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingwu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanhui</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.09281</idno>
		<title level="m">Language models&quot; grok&quot; to copy</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient descent maximizes the margin of homogeneous neural networks</title>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dichotomy of early and late phase implicit biases can provably induce grokking</title>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jikai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Shaolei Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Emergence in non-neural models: grokking modular arithmetic via average gradient outer product</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Mallinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Beaglehole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adityanarayanan</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthe</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.20199</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Parameter norm growth during training of transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>CoRR, abs/2010.09697</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Progress measures for grokking via mechanistic interpretability</title>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Grokking</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02177</idno>
		<title level="m">Generalization beyond overfitting on small algorithmic datasets</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grokking as a first order phase transition in two layer networks</title>
		<author>
			<persName><forename type="first">Noa</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbar</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Ringel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grokking group multiplication with cosets</title>
		<author>
			<persName><forename type="first">Dashiell</forename><surname>Stander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon</title>
		<author>
			<persName><forename type="first">Vimal</forename><surname>Thilak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etai</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roni</forename><surname>Paiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Vikrant</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohin</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramana</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.02390</idno>
		<title level="m">Explaining grokking through circuit efficiency</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Achieving margin maximization exponentially fast via progressive norm rescaling</title>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeping</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.14387</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grokking phase transitions in learning local rules with gradient descent</title>
		<author>
			<persName><forename type="first">Bojan</forename><surname>Žunkovič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enej</forename><surname>Ilievski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">199</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
