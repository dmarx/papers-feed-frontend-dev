{
  "arxivId": "2309.06180",
  "title": "Efficient Memory Management for Large Language Model Serving with\n  PagedAttention",
  "authors": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica",
  "abstract": "High throughput serving of large language models (LLMs) requires batching\nsufficiently many requests at a time. However, existing systems struggle\nbecause the key-value cache (KV cache) memory for each request is huge and\ngrows and shrinks dynamically. When managed inefficiently, this memory can be\nsignificantly wasted by fragmentation and redundant duplication, limiting the\nbatch size. To address this problem, we propose PagedAttention, an attention\nalgorithm inspired by the classical virtual memory and paging techniques in\noperating systems. On top of it, we build vLLM, an LLM serving system that\nachieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV\ncache within and across requests to further reduce memory usage. Our\nevaluations show that vLLM improves the throughput of popular LLMs by\n2-4$\\times$ with the same level of latency compared to the state-of-the-art\nsystems, such as FasterTransformer and Orca. The improvement is more pronounced\nwith longer sequences, larger models, and more complex decoding algorithms.\nvLLM's source code is publicly available at\nhttps://github.com/vllm-project/vllm",
  "url": "https://arxiv.org/abs/2309.06180",
  "issue_number": 982,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/982",
  "created_at": "2025-01-15T19:12:38.376361",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 11,
  "last_read": "2025-01-15T19:12:38.377163",
  "last_visited": "2025-01-15T19:11:27.895Z",
  "main_tex_file": null,
  "published_date": "2023-09-12T12:50:04Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.DC"
  ]
}