<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HybridFlow: A Flexible and Efficient RLHF Framework</title>
				<funder>
					<orgName type="full">ByteDance Research Collaboration</orgName>
				</funder>
				<funder ref="#_AqDwNfZ">
					<orgName type="full">Hong Kong RGC</orgName>
				</funder>
				<funder ref="#_wQSCj7R">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-02">2 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guangming</forename><surname>Sheng</surname></persName>
							<email>gmsheng@connect.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xibin</forename><surname>Wu</surname></persName>
							<email>wuxibin@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ru</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
							<email>pengyanghua.yanghua@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
							<email>haibin.lin@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><forename type="middle">2025</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zilingfeng</forename><surname>Ye</surname></persName>
							<email>yezilingfeng@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HybridFlow: A Flexible and Efficient RLHF Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-02">2 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">54CF26A8998E915F3A08C9AC7B7D93DD</idno>
					<idno type="DOI">10.1145/3689031.3696075</idno>
					<idno type="arXiv">arXiv:2409.19256v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Distributed systems, Reinforcement Learning from Human Feedback</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53×∼20.57× throughput improvement when</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) such as GPT <ref type="bibr" target="#b10">[11]</ref>, Llama <ref type="bibr" target="#b72">[73]</ref> and Claude <ref type="bibr" target="#b6">[7]</ref> have revolutionized various artificial intelligence (AI) applications, ranging from writing <ref type="bibr" target="#b1">[2]</ref>, searching <ref type="bibr" target="#b51">[52]</ref> to coding <ref type="bibr" target="#b62">[63]</ref>. LLMs are first pre-trained on trillions of tokens from books, websites, etc,. via next-word prediction to accumulate broad knowledge <ref type="bibr" target="#b10">[11]</ref>. Next, LLMs are trained on domain-specific datasets via supervised fine-tuning (SFT), to be able to follow human instructions <ref type="bibr" target="#b10">[11]</ref>. Despite the outstanding capabilities of LLMs on natural language tasks after pre-training and SFT, the detrimental and biased contents in the training datasets may still mislead an LLM to generate toxic and undesirable content. Reinforcement Learning from Human Feedback (RLHF) is introduced to further align an LLM to human values, for building helpful and harmless AI applications <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>RLHF is built upon traditional RL algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b77">78</ref>], e.g., Proximal Policy Optimization (PPO) <ref type="bibr" target="#b67">[68]</ref> and REIN-FORCE <ref type="bibr" target="#b77">[78]</ref>. The widely adopted PPO-based RLHF system typically consists of four LLMs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55]</ref>: an actor, a critic, a reference policy network and a reward model. PPO-based RLHF proceeds in iterations, each with three stages: (1) response generation using the actor model with a batch of prompts;</p><p>(2) preparation of training data by scoring the generated responses through a single forward pass of the critic, reference policy, and reward models; (3) learning from human preference by updating actor and critic through forward and backward computation. Other RLHF variants <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43]</ref> follow similar stages but involves different numbers of models and data dependencies among the models.</p><p>Traditional RL can be modeled as a dataflow <ref type="bibr" target="#b45">[46]</ref>, which is a directed acyclic graph (DAG): each node in the RL dataflow represents computation of a neural network (e.g., actor or critic network which can be CNN or MLP); each edge denotes data dependency between NN computations (e.g., output of the critic is used as input to actor training <ref type="bibr" target="#b67">[68]</ref>.) RLHF dataflow is more complex, with more complicated models involved (e.g., LLMs for the actor/critic/reference/reward models), each running distinct computation, and more diverse data dependencies among them (i.e., multicast between distributed model partitions). Training and generation of an LLM in the RLHF dataflow requires distributed computation (e.g., using tensor/pipeline/data parallelism) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b70">71]</ref>. Therefore, each node in the RLHF dataflow is a complex distributed program, corresponding to distributed computation of the respective LLM. Models in different nodes typically use different parallelism strategies as their workloads vary. The edge represents data resharding, which is often a many-tomany multicast. Consequently, Flexible representation and efficient execution of the complex and resource intensive RLHF is imperative.</p><p>Traditional RL frameworks such as RLLib <ref type="bibr" target="#b44">[45]</ref> and RLLib Flow <ref type="bibr" target="#b45">[46]</ref> utilize a hierarchical single-controller paradigm to run RL dataflows. A centralized controller assigns nodes in the dataflow to different processes and coordinates their execution order. Each node process can further spawn more workers to perform computation, again following the singlecontroller paradigm. However, they only provide primitives for data-parallel training and are constrained to neural networks that are at most hundreds of MB in size <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. In the RLHF dataflow, each node corresponds to an LLM with up to billions of operators, computed using some complex parallelism. A single-controller paradigm is inefficient due to the substantial overhead of dispatching operators to distributed accelerators <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Existing RLHF systems adopt a multi-controller paradigm to manage intra-node computation and inter-node data resharding <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b79">80]</ref>. Each controller independently manages the computation of one device and uses multiple point-topoint operations to coordinate data dependencies between different nodes. This multi-controller paradigm introduces negligible dispatch overhead when performing LLM computation (detailed in §2.2).</p><p>However, without central control, it is inflexible to implement various RLHF dataflow, as modifying a single node to adapt to different data dependencies requires changing all dependent nodes' implementation, hindering code reuse.</p><p>To address these limitations, we propose HybridFlow, a flexible and efficient RLHF framework to easily represent and execute diverse RLHF dataflows, attaining high throughput.</p><p>Our key observation is that utilizing the single-controller paradigm on the inter-node level enables flexible expression of various data dependencies and easy coordination of inter-node data resharding with minimal overhead, while integrating the multi-controller paradigm within intra-node computation enhances computation efficiency substantially. We advocate a hierarchical hybrid programming model to generate RLHF dataflows. At the node level, multiple model classes are provided that encapsulate distributed computation (training, inference and generation) of different LLMs in the dataflow into primitive APIs. These APIs can seamlessly support various parallelism strategies from the existing LLM frameworks, including 3D parallelism <ref type="bibr" target="#b70">[71]</ref>, ZeRO <ref type="bibr" target="#b58">[59]</ref>, and PyTorch FSDP <ref type="bibr" target="#b56">[57]</ref>), and perform distributed computation under the multi-controller paradigm. Among the nodes, a set of transfer protocols are designed to hide the complexity of data resharding from users, as coordinated by a single controller. This programming model abstracts away the complexity of distributed computing, allowing users to implement an RLHF dataflow in a few lines of code and run RLHF through a single process of the single controller. It also effectively decouples intra-node computation and inter-node data transfer, allowing independent optimization of each model without changing the code of other models in the dataflow.</p><p>Training and generation of the actor model represent major computation in the RLHF dataflow. We further design a 3D-HybridEngine to enable efficient execution of training and generation of the actor model, introducing zero memory redundancy and significantly reduced communication overhead during model parameter resharding between the training and generation stages. Our hybrid programming model also facilitates flexible placement of models onto the same or different sets of GPU devices. This allows us to provide an effective algorithm to optimize GPU allocation and placement of the models, with various model sizes and distinct workloads, for any RLHF dataflow. Our contributions in designing HybridFlow are summarized as follows:</p><p>• We propose a hierarchical hybrid programming model for conveniently building the RLHF dataflow. This programming model enables efficient distributed execution of intra-node computation and flexible inter-node data resharding and transfer, for various RLHF algorithms ( §4).</p><p>• We design a 3D-HybridEngine that executes training and generation of the actor model with high computation efficiency and zero-redundancy transition between the training stage and the generation stage ( §5).</p><p>• We devise an effective mapping algorithm to automatically identify optimized GPU allocation and placement of each node (model) in the RLHF dataflow ( §6).</p><p>• We conduct extensive experiments comparing HybridFlow with state-of-the-art RLHF systems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b81">82]</ref> under various We have open-sourced HybridFlow and believe that Hy-bridFlow can boost future RLHF research and development.</p><p>2 Background and Motivation 2.1 Reinforcement Learning from Human Feedback RLHF Workflow. RLHF aligns the linguistic space of LLMs with human values, using a set of human-ranked candidates of given prompts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b90">91</ref>]. An RLHF system typically consists of multiple models, e.g., an actor, a critic, a reference policy, and one or multiple reward models. The actor and the reference are each pre-trained/fined-tuned LLM (i.e., the LLM that is undergoing RLHF). The critic and reward models can be different LLMs fine-tuned on the human preference dataset, with the language modeling head replaced by a scalar output head <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55]</ref>. The RLHF workflow can be decomposed into 3 stages (Figure <ref type="figure" target="#fig_0">1</ref>) and we take PPO as an example:</p><p>•Stage 1 (Generation): The actor produces responses from a batch of prompts using auto-regressive generation.</p><p>•Stage 2 (Preparation): Using prompts and generated responses, the critic computes their values <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b67">68]</ref>, the reference policy computes their reference log probabilities, and the reward model computes their rewards <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55]</ref>, all via a single pass of forward computation of the respective model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•Stage 3 (Learning/Training):</head><p>The actor and the critic are updated via Adam <ref type="bibr" target="#b37">[38]</ref>, using the batch of data produced by previous stages and the loss function <ref type="bibr" target="#b54">[55]</ref>.</p><p>Other RLHF algorithms largely follow the 3-stage workflow as well (Figure <ref type="figure" target="#fig_0">1</ref>(b)(c)). Safe-RLHF <ref type="bibr" target="#b18">[19]</ref> introduces an auxiliary pretrain loss following PPO-ptx <ref type="bibr" target="#b54">[55]</ref> and includes an additional cost model to fit human preferences and safety labels simultaneously. ReMax <ref type="bibr" target="#b42">[43]</ref> requires an additional generation pass for variance reduction and eliminates the critic model in the dataflow. Researchers are actively exploring novel RLHF algorithms <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b90">91]</ref> and integrating traditional RL methods into RLHF domains <ref type="bibr" target="#b36">[37]</ref>. These variances necessitate a flexible representation of the RLHF dataflow graph to accommodate diverse algorithmic requirements. Parallelism Strategies. LLMs are trained and served with data, pipeline, and tensor parallelism <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54]</ref>. With data parallelism (DP), the input data is split into multiple subsets; each subset is processed by a separate device (e.g., a GPU) <ref type="bibr" target="#b68">[69]</ref>. ZeRO <ref type="bibr" target="#b58">[59]</ref> is a memory-optimized solution for DP training, progressively sharding optimizer states, gradients, and model parameters across GPUs. Pipeline parallelism (PP) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b52">53]</ref> and tensor parallelism (TP) <ref type="bibr" target="#b70">[71]</ref> distribute model parameters, gradients and optimizer states across multiple GPUs. Modern distributed training frameworks like Megatron-LM <ref type="bibr" target="#b70">[71]</ref> and MegaScale <ref type="bibr" target="#b35">[36]</ref> utilize 3D parallelism or PTD parallelism <ref type="bibr" target="#b53">[54]</ref>, where P, T, D stand for PP, TP, DP, respectively. In 3D parallelism, PP size represents the number of pipeline stages in model training, TP size refers to the number of shards that a tensor is partitioned into, and DP size is the number of model replicas. LLM serving systems employ 3D parallelism similar to training while only model parameters and KVCache are sharded <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. LLM models in the RLHF dataflow may perform distinct computations, including training (one forward pass, one backward pass and model update), inference (one forward pass) and generation (auto-regressive generation with multiple forward passes). In particular, training and generation are performed on the actor model, training and inference on the critic, and inference on reference policy and reward models. Distinct parallel strategies can be applied to different models for varied computations to achieve optimal throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Programming Model for Distributed ML</head><p>Single-Controller. It employs a centralized controller to manage the overall execution flow of the distributed program. With centralized control logic, users can build core functionalities of the dataflow as a single process (Figure <ref type="figure" target="#fig_1">2</ref>(b)), while the controller automatically generates distributed workers to carry out the computation. With a global view of the hardware and dataflow graph, the single-controller paradigm allows flexible and optimized resource mapping and execution order coordination among dataflow tasks. However, coordination messages are passed from the controller to all workers, incurring significant dispatch overhead when executing expansive dataflow graphs on large clusters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>. Multi-Controller. Each device (aka worker) has its own controller. State-of-the-art distributed LLM training and serving systems adopt the multi-controller paradigm, due to its scalability and low dispatch overhead (control messaging largely passed from CPU to GPU over fast PCIe links) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b70">71]</ref>. As shown in the example that employs multi-controller RLHF implementation in Figure <ref type="figure" target="#fig_1">2</ref>(a), a separate program is run for each model, and all workers of one model execute the same program. Each worker only possesses a local view of the system state and requires point-to-point communication between two models (blue code and arrows) to coordinate model execution order. To implement an RLHF workflow in the multi-controller architecture, a user must intricately integrate the code for collective communication, computation, and point-to-point data transfer in the program run at each device. This leads to deeply nested code of computation and data transfer, challenging to develop, maintain, and optimize. In Figure <ref type="figure" target="#fig_1">2</ref>(a), each model performs local computation and all_gather operations (black code), while the actor model must explicitly manage send operations to the critic and reward models, and the latter must correspondingly implement receive operations at precise points in their program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">RLHF Characteristics</head><p>Heterogeneous model workloads. The actor, critic, reference and reward models in RLHF may execute training, inference or generation at different stages, with different memory footprint and computation demand. For reference policy and reward models, only their model parameters need to be stored in GPU memory, as they perform only the forward pass computation. For the actor and the critic, their model parameters, gradients, and optimizer states must be stored as they undergo model training. Moreover, a small actor model (e.g., a 7B pre-trained/fine-tuned LLM) can be paired with larger critic and reward models (e.g., 70B LLMs) in RLHF for better alignment <ref type="bibr" target="#b6">[7]</ref>. Given such heterogeneity, different parallelism strategies and tailored optimizations are needed for running each model during RLHF. Unbalanced computation between actor training and generation. In the RLHF dataflow, training and generation of the actor model are represented by two nodes (Figure <ref type="figure" target="#fig_0">1</ref>),  3. Dataflow execution given a model placement plan. Blocks with numbers represent GPUs. In dashed boxes, the models are placed on different sets of devices and can be concurrently computed. Reference model (blue) and reward model (green) are colocated on the same set of GPUs and executed sequentially. which often render majority of the workload in each RLHF iteration (e.g., 58.9% of total RLHF time with HybridFlow). Actor training is computation bound [24], often requiring a larger model-parallel (MP) size (i.e., the number of partitions the model is partitioned into) and distributing the workload to more GPUs, e.g., 8 partitions of a 7B model on 8 GPUs. Using the same parallelism strategy (e.g., the same MP size) for generation can lead to underutilization of GPU computation resources due to its memory-bound nature [40]. Previous studies show that combining a larger DP size with a smaller MP size (hybrid data and model parallelism), e.g., partition a 7B model into two and replicate it four times on 8 GPUs, can improve the generation throughput [44, 92]. Although using different parallelism strategies for actor training and generation may optimize throughput in both stages, resharding the actor model weights at runtime between the two stages can incur significant communication and memory overhead. For example, aligning a 70B actor model requires transferring 140GB of model weights from training to generation per RLHF iteration, taking up to 36.4% of an iteration time when the two stages are on different devices [30]. Diverse model placement requirements. Strategic device placement of models in the RLHF dataflow is necessary, according to computation workloads and data dependencies of the models. Figure 3 gives an example model placement plan and the corresponding RLHF execution flow. Models placed on different sets of devices can be executed in parallel if no data dependencies exist. Models placed on the same set of GPUs, referred to as colocated models, share the GPU memory and are executed sequentially in a time-sharing manner, as out-of-memory (OOM) error may easily happen if colocated LLMs execute concurrently.</p><p>We observe a compromise: placing models on different devices permits parallel processing but may inevitably lead to some GPU idle time, given staged model execution in RLHF. In Figure <ref type="figure">3</ref>, actor and critic are placed separately, performing training in parallel, but incurring 1/3 of their GPU time being idle, during other RLHF stages. Supporting various  Support various execution patterns placement strategies and maximizing device utilization are crucial for optimizing RLHF performance at any model size and cluster scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Limitations of existing RLHF systems</head><p>Inflexible support for various RLHF dataflow graphs. Existing RLHF systems adopt the multi-controller paradigm for dataflow implementation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b81">82]</ref>. To implement various RLHF algorithms, a user must navigate and manage code that mixes collective communication, model computation (potentially using various distributed training/serving frameworks), and point-to-point data transfer. This code structure lacks modularity/function encapsulation, making the RLHF systems tightly coupled with specific LLM training and serving frameworks. Consequently, a user needs to implement and optimize different RLHF dataflows case-bycase <ref type="bibr" target="#b45">[46]</ref>, hindering code reuse and increasing the risk of making mistakes. Existing RLHF frameworks only support the PPO algorithm. In addition, limited parallel strategies are supported due to implementation complexity. For example, to incorporate 3D parallelism for LLM training and generation in DeepSpeed-Chat <ref type="bibr" target="#b81">[82]</ref>, one may have to re-implement the whole system due to the mixed code structure. Inefficient RLHF execution. Table <ref type="table" target="#tab_1">1</ref> summarizes parallelism strategies, model placement, and execution patterns adopted by the existing RLHF systems. DeepSpeed-Chat <ref type="bibr" target="#b81">[82]</ref> and OpenRLHF <ref type="bibr" target="#b29">[30]</ref> adopt ZeRO-3 for actor training and TP for actor generation. OpenRLHF uses different copies of the actor model on different devices for training and generation, incurring redundant memory usage and frequent weight synchronization among devices. DeepSpeed-Chat maintains the same copy of actor model on the same set of devices for training and generation, and reshards model weights between training and generation (due to different parallelisms used in the two stages), which may still incur substantial memory and communication overhead for large models (detailed in §5.4). NeMo-Aligner <ref type="bibr" target="#b16">[17]</ref> uses the same 3D parallelism configurations in actor training and generation, experiencing low generation throughput ( §8.4).</p><p>Existing RLHF frameworks are limited to one model placement plan and hence one RLHF execution pattern, as shown in Table <ref type="table" target="#tab_1">1</ref>. Implementing a different placement is difficult, requiring changing the inner logic of model initialization and inter-node data transfer as highlighted in blue in Figure <ref type="figure" target="#fig_1">2</ref>. OpenRLHF and NeMo-Aligner allow concurrent model computation in the preparation and learning stages; in the generation stage, models except the actor are idle, wasting the GPUs they occupy. DeepSpeed-Chat colocates all models on the same set of devices, and each device runs each model sequentially according to the RLHF dataflow. With unbalanced workloads among the models, such a placement can be inefficient in resource utilization (evaluated in §8.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Design Considerations</head><p>To tackle limitations of existing systems, the key question is -How to design a flexible and efficient programming model to implement RLHF dataflow? A single-controller design is particularly advantageous at the inter-node level due to its flexibility in coordinating data transfer, execution order, and resource virtualization among distributed computation of different models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50]</ref>. The RLHF dataflow graph typically consists of only a few nodes. Dispatching control messages to different nodes from the single-controller incurs negligible overhead as compared to distributed computation required for nodes (models) in the dataflow. The multi-controller paradigm, known for its low latency in dispatching operators to accelerators <ref type="bibr" target="#b19">[20]</ref>, can be leveraged in distributed computation of each model. With these insights, we propose a hierarchical hybrid programming model for RLHF dataflow implementation. Our key design principle is to combine single-controller and multi-controller paradigms in a hybrid manner. This design ensures flexible expression and efficient execution of RLHF dataflow, maintaining low control overhead at both inter-node and intra-node levels. As shown in Figure <ref type="figure" target="#fig_1">2</ref>(b), this paradigm decouples intra-node distributed computation and inter-node data transfer, allowing each model to focus solely on local computation without managing inter-node communication. The workflow of our RLHF system goes as follows. A user provides the following inputs to start the RLHF system: (i) model specifications, including the architecture and size of the actor/critic/reference policy/reward models in the RLHF dataflow; (ii) device placement of the models in the dataflow, as obtained by running the auto-mapping algorithm under given GPU cluster configurations; (iii) parallelism strategy for running each model in each stage, e.g., a tuple of (p, t, d) for 3D parallelism, where p, t, d represent PP size, TP size and DP size, respectively. The single controller program takes these inputs to initialize models in the RLHF dataflow and virtualized resource pool, dispatches operations/models to devices according to the placement plan, and invokes functions run by the multiple controllers on devices to carry out distributed computation of each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HybridFlow Overview</head><p>The multi-controller program implements the Parallel-Worker class: it constructs parallel groups of each model among allocated devices according to its parallelism strategies, invokes the 3D-HybridEngine for actor training and generation, and can be integrated seamlessly with existing LLM engines <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b70">71]</ref> for training, inference and generation of other models. The transfer protocols are coordinated by the single controller program to support resharding of data (including prompts, responses, and other model outputs in RLHF) between models with distinct parallelism strategies. The data resharding of the actor between training and generation is handled by 3D-HybridEngine.  Inter-node: unifying data resharding implementation between models. Many-to-many multicast is involved for data transfer between models employing different parallelism strategies on different devices. We unify this data transfer implementation by associating each operation in each model class with a transfer protocol, using @register. Each transfer protocol consists of a collect function and a distribute function, to aggregate output data and distribute input data according to the parallelism strategy of each model. In the example in Figure <ref type="figure" target="#fig_7">5</ref>(a), update_actor operation is registered to transfer protocol 3D_PROTO, as 3D parallelism is used for actor training. In 3D_PROTO, the collect function gathers all the output data of corresponding model function (e.g., the loss scalar return from the update_actor) in each DP group to the single controller, and the distribute function distributes the input data to the registered function (e.g., advantages for the update_actor) to each DP group. Data resharding is enabled using the source model's output collect function and the destination model's input distribute function. Figure <ref type="figure" target="#fig_7">5</ref>(b) illustrates data resharding between the actor (generation) and the critic (inference), where computation of the models adopts different 3D parallelism strategies. The single controller gathers data futures using the collect function in 3D_PROTO of actor (steps 1 ○-3 ○) and sends it to critic (step 4 ○); critic distributes the received data futures to each DP group using the distribute function in its 3D_PROTO (step 5 ○). Then remote data is retrieved from actor to critic, with each of critic's GPUs only fetching the required local batch of the actor's output data according to its DP rank (step 6 ○). The actual data transfer only occurs between GPUs, avoiding any central bottleneck.</p><formula xml:id="formula_0">t I M U C 6 G D a K x F c m K H J t d H W N 7 Q = " &gt; A A A B + H i c b V B N S 8 N A E N 3 4 W e t H o x 6 9 B I v g Q U o i p X o s e P F Y w X 5 A G 8 p m M 2 2 X b j Z h d y L W 0 F / i x Y M i X v 0 p 3 v w 3 b t s c t P X B w O O 9 m d 2 Z F y S C a 3 T d b 2 t t f W N z a 7 u w U 9 z d 2 z 8 o 2 Y d H L R 2 n i k G T x S J W n Y B q E F x C E z k K 6 C Q K a B Q I a A f j m 5 n f f g C l e S z v c Z K A H 9 G h 5 A P O K B q p b 5 d 6 C I / I u G I C w q w 6 7 d t l t + L O 4 a w S L y d l k q P R t 7 9 6 Y c z S C C Q y Q b X u e m 6 C f k Y V c v P k t N h L N S S U j e k Q u o Z K G o H 2 s / n i U + f M K K E z i J U p i c 5 c / T 2 R 0 U j r S R S Y z o j i S C 9 7 M / E / r 5 v i 4 N r P u E x S B M k W H w 1 S 4 W D s z F J w Q q 6 A o Z g Y Q p n i Z l e H j a i i D E 1 W R R O C t 3 z y K m l d V r x a p X Z X L d c v 8 j g K 5 I S c k n P i k S t S J 7 e k Q Z q E k Z Q 8 k 1 f y Z j 1 Z L 9 a 7 9 b F o X b P y m W P y B 9 b n D y O B k 1 k = &lt; / l a t e x i t &gt; 4 ○ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a o i x u 1 H E d 7 e I U D M b N I f V 3 G r E T j g = " &gt; A A A B + H i c b V D L S g N B E O z 1 G e M j q x 6 9 D A b B g 4 R d 0 e g x 4 M V j B P O A Z A m z s 5 N k y O y D m V 4 x L v k S L x 4 U 8 e q n e P N v n C R 7 0 M S C h q K q e 6 a 7 / E Q K j Y 7 z b a 2 s r q 1 v b B a 2 i t s 7 u 3 s l e / + g q e N U M d 5 g s Y x V 2 6 e a S x H x B g q U v J 0 o T k N f 8 p Y / u p n 6 r Q e u t I i j e x w n 3 A v p I B J 9 w S g a q W e X u s g f k Q n F J A + y y 0 n P L j s V Z w a y T N y c l C F H v W d / d Y O Y p S G P k E m q d c d 1 E v Q y q l C Y J y f F b q p 5 Q t m I D n j H 0 I i G X H v Z b P E J O T F K Q P q x M h U h m a m / J z I a a j 0 O f d M Z U h z q R W 8 q / u d 1 U u x f e 5 m I k h R 5 x O Y f 9 V N J M C b T F E g g F G c o x 4 Z Q p o T Z l b A h V Z S h y a p o Q n A X T 1 4 m z f O K W 6 1 U 7 y 7 K t b M 8 j g I c w T G c g g t X U I N b q E M D G K T w D K / w Z j 1 Z L 9 a 7 9 T F v X b H y m U P 4 A + v z B y U G k 1 o = &lt; / l a t e x i t &gt; 5 ○ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H U b 6 8 j i / 9 A x T 6 w + c v Z j g f c F 6 E z Y = " &gt; A A A B + H i c b V B N S 8 N A E N 3 4 W e t H o x 6 9 B I v g Q U p S p H o s e P F Y w X 5 A G 8 p m M 2 2 X b j Z h d y L W 0 F / i x Y M i X v 0 p 3 v w 3 b t s c t P X B w O O 9 m d 2 Z F y S C a 3 T d b 2 t t f W N z a 7 u w U 9 z d 2 z 8 o 2 Y d H L R 2 n i k G T x S J W n Y B q E F x C E z k K 6 C Q K a B Q I a A f j m 5 n f f g C l e S z v c Z K A H 9 G h 5 A P O K B q p b 5 d 6 C I / I u G I C w q w 6 7 d t l t + L O 4 a w S L y d l k q P R t 7 9 6 Y c z S C C Q y Q b X u e m 6 C f k Y V c v P k t N h L N S S U j e k Q u o Z K G o H 2 s / n i U + f M K K E z i J U p i c 5 c / T 2 R 0 U j r S R S Y z o j i S C 9 7 M / E / r 5 v i 4 N r P u E x S B M k W H w 1 S 4 W D s z F J w Q q 6 A o Z g Y Q p n i Z l e H j a i i D E 1 W R R O C t 3 z y K m l V K 1 6 t U r u 7 L N c v 8 j g K 5 I S c k n P i k S t S J 7 e k Q Z q E k Z Q 8 k 1 f y Z j 1 Z L 9 a 7 9 b F o X b P y m W P y B 9 b n D y B 3 k 1 c = &lt; / l a t e x i t &gt; 2 ○ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N a / p c Q 2 W I U 2 2 F 5 P 4 e h 8 i Y M a 1 q t A = " &gt; A A A B + H i c b V D L S g N B E O z 1 G e M j q x 6 9 D A b B g 4 R d l e g x 4 M V j B P O A Z A m z s 5 N k y O y D m V 4 x L v k S L x 4 U 8 e q n e P N v n C R 7 0 M S C h q K q e 6 a 7 / E Q K j Y 7 z b a 2 s r q 1 v b B a 2 i t s 7 u 3 s l e / + g q e N U M d 5 g s Y x V 2 6 e a S x H x B g q U v J 0 o T k N f 8 p Y / u p n 6 r Q e u t I i j e x w n 3 A v p I B J 9 w S g a q W e X u s g f k Q n F J A + y i 0 n P L j s V Z w a y T N y c l C F H v W d / d Y O Y p S G P k E m q d c d 1 E v Q y q l C Y J y f F b q p 5 Q t m I D n j H 0 I i G X H v Z b P E J O T F K Q P q x M h U h m a m / J z I a a j 0 O f d M Z U h z q R W 8 q / u d 1 U u x f e 5 m I k h R 5 x O Y f 9 V N J M C b T F E g g F G c o x 4 Z Q p o T Z l b A h V Z S h y a p o Q n A X T 1 4 m z f O K W 6 1 U 7 y 7 L t b M 8 j g I c w T G c g g t X U I N b q E M D G K T w D K / w Z j 1 Z L 9 a 7 9 T F v X b H y m U P 4 A + v z B y H 8 k 1 g = &lt; / l a t e x i t &gt; 3 ○ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b D j 2 x s 9 J 0 e + T m Z z d V f 7 q I U e s b / s = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r 1 o / G v X o J V g E D 1 I S k e q x 4 M V j B f s B b S i b z b R d u t m E 3 Y l Y Q 3 + J F w + K e P W n e P P f u G 1 z 0 N Y H A 4 / 3 Z n Z n X p A I r t F 1 v 6 3 C 2 v r G 5 l Z x u 7 S z u 7 d f t g 8 O W z p O F Y M m i 0 W s O g H V I L i E J n I U 0 E k U 0 C g Q 0 A 7 G N z O / / Q B K 8 1 j e 4 y Q B P 6 J D y Q e c U T R S 3 y 7 3 E B 6 R c c U E h J k 3 7 d s V t + r O 4 a w S L y c V k q P R t 7 9 6 Y c z S C C Q y Q b X u e m 6 C f k Y V c v P k t N R L N S S U j e k Q u o Z K G o H 2 s / n i U + f U K K E z i J U p i c 5 c / T 2 R 0 U j r S R S Y z o j i S C 9 7 M / E / r 5 v i 4 N r P u E x S B M k W H w 1 S 4 W D s z F J w Q q 6 A o Z g Y Q p n i Z l e H j a i i D E 1 W J R O C t 3 z y K m l d V L 1 a t X Z 3 W a m f 5 3 E U y T E 5 I W f E I 1 e k T m 5 J g z Q J I y l 5 J q / k z X q y X q x 3 6 2 P R W r D y m S P y B 9 b n D x 7 y k 1 Y = &lt; / l a t e x i t &gt; 1 ○ Actor Single Controller &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y N K + h b f U X n Q F s S t 1 5 A K x R P j 6 M w s = " &gt; A A A B + H i c b V B N S 8 N A E J 3 U r 1 o / G v X o J V g E T y U R q R 6 L X j x W s B / Q h r L Z b N u l m 0 3 Y n Y g 1 9 J d 4 8 a C I V 3 + K N / + N 2 z Y H b X 0 w 8 H h v Z n f m B Y n g G l 3 3 2 y q s r W 9 s b h W 3 S z u 7 e / t l + + C w p e N U U d a k s Y h V J y C a C S 5 Z E z k K 1 k k U I 1 E g W D s Y 3 8 z 8 9 g N T m s f y H i c J 8 y M y l H z A K U E j 9 e 1 y D 9 k j U q 6 o Y G F W m / b t i l t 1 5 3 B W i Z e T C u R o 9 O 2 v X h j T N G I S q S B a d z 0 3 Q T 8 j C r l 5 c l r q p Z o l h I 7 J k H U N l S R i 2 s / m i 0 + d U 6 O E z i B W p i Q 6 c / X 3 R E Y i r S d R Y D o j g i O 9 7 M 3 E / 7 x u i o M r P + M y S Z F J u v h o k A o H Y 2 e W g h N y x S i K i S G E K m 5 2 d e i I K E L R Z F U y I X j L J 6 + S 1 n n V q 1 V r d x e V + n U e R x G O 4 Q T O w I N L q M M t N K A J F F J</formula><p>We provide 8 transfer protocols, including 3D_PROTO, DP _PROTO, ONE_TO_ALL, etc., that cover most data resharding scenarios (detailed in Appendix B). A user can further extend the transfer protocols through implementing customized collect and distribute functions. Facilitating flexible model placement. We provide a ResourcePool class that virtualizes a set of GPU devices. When applying a ResourcePool instance to a model class (Figure <ref type="figure" target="#fig_7">5(a)</ref>), distributed computation of the model will be mapped to the devices. Models utilizing the same ResourcePool instance are colocated on the same set of GPUs; models are placed on different sets of GPUs when different Resource Pool instances are applied in their model classes. We assume no overlap between different ResourcePool instances. Asynchronous dataflow execution. When models are placed on separate sets of devices, their execution is triggered automatically as soon as their inputs become available <ref type="bibr" target="#b49">[50]</ref>. In Figure <ref type="figure" target="#fig_7">5</ref>(b), the data future from actor is immediately returned after the controller's call (steps 1 ○-3 ○); the controller then initiates a new call to critic and distributes the futures following the transfer protocol (steps 4 ○-5 ○). When some models are placed on the same set of devices, they are executed sequentially based on the calling order. With Figure <ref type="figure">6</ref>. Implementation of PPO <ref type="bibr" target="#b54">[55]</ref>, ReMax <ref type="bibr" target="#b42">[43]</ref>, and Safe-RLHF <ref type="bibr" target="#b18">[19]</ref>. Users can adapt to different RLHF algorithms by simply adding or deleting a few lines of code.</p><p>our programming model, HybridFlow is flexible in supporting diverse distributed execution patterns without any code change of the RLHF algorithm (Figure <ref type="figure">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation of different RLHF algorithms</head><p>Our APIs enable streamlined development of various RLHF algorithms (dataflows). Users can implement an RLHF algorithm in a few lines of code as a single process program to run on the single controller, that involves a sequence of primitive API calls to invoke distributed computation of models. Examples of PPO, ReMax, and Safe-RLHF are given in Figure <ref type="figure">6</ref>. PPO can be implemented in just 8 lines by invoking model operations including compute_values and generate_sequences, which are executed under the multicontroller paradigm on multiple GPUs. To adapt to Safe-RLHF which integrates an additional cost model to evaluate safety preferences and the pre-taining loss for actor, only 5 more lines of code are added on top of PPO implementation.</p><p>To adapt to ReMax, one additional call to actor generation is needed, and the critic-related code can be removed.</p><p>Achieving flexible. This flexibility of extension is crucial for researchers to explore different RLHF algorithms: they can reuse distributed computation encapsulated in each model class and simply adjust the code for numerical computations according to specific algorithms, such as GAE <ref type="bibr" target="#b66">[67]</ref> and KL divergence in compute_advantage and loss functions of actor and critic. The streamlined development can be attributed to the hybrid programming model. Our modular API design simplifies development, facilitates extensive code reuse, and enables directly incorporating the codebase of existing LLM training/serving frameworks. It also decouples model computation and data transfer among models. Any change in the distributed frameworks does not affect the code of the RLHF algorithm (Figure <ref type="figure">6</ref>), enabling individualized optimization for each model's execution ( §5). Flexible placement of models with diverse workloads is supported, enabling optimized mapping of RLHF dataflow onto various devices ( §6). q n e P N v n C R 7 0 M S C h q K q e 6 a 7 / E Q K j Y 7 z b a 2 s r q 1 v b B a 2 i t s 7 u 3 s l e / + g q e N U M d 5 g s Y x V 2 6 e a S x H x B g q U v J 0 o T k N f 8 p Y / u p n 6 r Q e u t I i j e x w n 3 A v p I B J 9 w S g a q W e X u s g f k Q n F J A + y y 0 n P L j s V Z w a y T N y q n e P N v n C R 7 0 M S C h q K q e 6 a 7 / E Q K j Y 7 z b a 2 s r q 1 v b B a 2 i t s 7 u 3 s l e / + g q e N U M d 5 g s Y x V 2 6 e a S x H x B g q U v J 0 o T k N f 8 p Y / u p n 6 r Q e u t I i j e x w n 3 A v p I B J 9 w S g a q W e X u s g f k Q n F J A + y i 0 n P L j s V Z w a y T N y  </p><formula xml:id="formula_1">c l C F H v W d / d Y O Y p S G P k E m q d c d 1 E v Q y q l C Y J y f F b q p 5 Q t m I D n j H 0 I i G X H v Z b P E J O T F K Q P q x M h U h m a m / J z I a a j 0 O f d M Z U h z q R W 8 q / u d 1 U u x f e 5 m I k h R 5 x O Y f 9 V N J M C b T F E g g F G c o x 4 Z Q p o T Z l b A h V Z S h y a p o Q n A X T 1 4 m z f O K W 6 1</formula><formula xml:id="formula_2">c l C F H v W d / d Y O Y p S G P k E m q d c d 1 E v Q y q l C Y J y f F b q p 5 Q t m I D n j H 0 I i G X H v Z b P E J O T F K Q P q x M h U h m a m / J z I a a j 0 O f d M Z U h z q R W 8 q / u d 1 U u x f e 5 m I k h R 5 x O Y f 9 V N J M C b T F E g g F G c o x 4 Z Q p o T Z l b A h V Z S h y a p o Q n A X T 1 4 m z f O K W 6 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">3D-HybridEngine</head><p>We design the 3D-HybridEngine to support efficient training and generation of the actor model, targeting significant RLHF throughput improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parallel Groups</head><p>To eliminate redundant actor model copies, we advocate deploying actor training and generation stages on the same set of devices, 𝑁 𝑎 GPUs allocated to the actor, and execute them sequentially on the same copy of actor model weights. Nonetheless, actor training and generation may well adopt different 3D parallelism strategies, i.e., the generation stage typically requires smaller TP and PP sizes but a larger DP size, than the training stage ( §2.3). 3D-HybridEngine enables efficient model parameter resharding between actor training and generation across the same set of devices in this context.</p><p>Let 𝑝-𝑡-𝑑 denote 3D parallel groups constructed for actor training, corresponding to the set of GPUs to host 𝑝 pipeline stages, 𝑡 tensor shards, and 𝑑 model replicas <ref type="bibr" target="#b53">[54]</ref>. 3D-HybridEngine builds different parallel groups for actor training and generation, according to their different 3D parallelism strategies, respectively. We use 𝑝 𝑔 , 𝑡 𝑔 , and 𝑑 𝑔 to denote the size of generation pipeline parallel group, generation tensor parallel group, and micro data parallel group, respectively, in the generation stage. 𝑑 𝑔 indicates the ratio of model replica number in generation over that in training, i.e., each DP replica in training becomes 𝑑 𝑔 micro DP replicas, to process 𝑑 𝑔 microbatches of prompts and responses. We have</p><formula xml:id="formula_3">𝑁 𝑎 =𝑝×𝑡×𝑑=𝑝 𝑔 ×𝑡 𝑔 ×𝑑 𝑔 ×𝑑 such that 𝑑 𝑔 = 𝑝𝑡 𝑝 𝑔 𝑡 𝑔 .</formula><p>The micro DP groups are employed exclusively in actor generation stage to render a larger DP size for full device utilization. The generation parallel groups are denoted by 𝑝 𝑔 -𝑡 𝑔 -𝑑 𝑔 -𝑑.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">3D-HybridEngine Workflow</head><p>Between actor training in iteration 𝑖 of RLHF and actor generation in iteration 𝑖 + 1, the actor model parameters need to be resharded and prompts data to be distributed, following the parallel group configurations in the two stages. In iteration 𝑖 + 1 of RLHF, 3D-HybridEngine gathers the actor </p><formula xml:id="formula_4">G1 G2 G3 G4 G5 G6 G7 G8 G2 G3 G4 G5 G6 G7 G8 G1 G2 G3 G4 G5 G6 G7 G8 G1 G2 G4 G3 All-Gather complete weights G5 G6 G7 G8 G1 G3 G2 G4 G5 G7 G6 G8</formula><p>All-Gather within Micro-DP groups </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Zero redundancy model resharding</head><p>Parallel grouping methods in 3D parallelism are typically as follows: PP and TP groups are formed by assigning consecutive ranks to pipeline stages and tensor shards, respectively; DP groups are constructed by selecting ranks at regular intervals, determined by the product of PP size and TP size. In Figure <ref type="figure" target="#fig_14">8(a)</ref>, actor training uses 3D parallel groups, 1-4-2: there is one PP group for all GPUs (for illustration clarify); the TP groups are [G1, G2, G3, G4], [G5, G6, G7, G8], and the DP groups are [G1, G5], [G2, G6], [G3, G7], [G4, G8]. Suppose the same parallel grouping methods are used but with different parallel sizes, e.g., 1-2-2-2 for generation in Figure <ref type="figure" target="#fig_14">8(a)</ref>. During the transition from training to generation, 3D-HybridEngine applies all-gather operations among the model parallel groups to aggregate all parameters, and then retain only a subset of model weights on each device for its generation, according to the parallel groups the device belongs to. On some GPUs (e.g., G2, G3, G6, G7), there is no overlap between training and generation model weights, and separate memory is needed to maintain weights for subsequent training as well (grey boxes in Figure <ref type="figure" target="#fig_14">8(a)</ref>).We call  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Transition overhead</head><p>In Table <ref type="table" target="#tab_2">2</ref>, we compare communication overhead and memory footprint during the transition between training and generation stages, among different actor engine designs. We assume model size of the actor is 𝑀 and 𝑁 𝑎 GPUs are used for its training and generation. The actor engine in DeepSpeed-Chat conducts an all-gather operation across all GPUs during transition; HybridFlow-V performs this all-gather within training TP and PP groups. The communication volumes for these operations are 𝑁 𝑎 -1</p><formula xml:id="formula_5">𝑁 𝑎 𝑀 = 𝑡𝑝𝑑 -1</formula><p>𝑡𝑝𝑑 𝑀 for DeepSpeed-Chat and 𝑡𝑝 -1 𝑡𝑝 𝑀 for HybridFlow-V, calculated following <ref type="bibr" target="#b12">[13]</ref>.</p><p>Both engines aggregate all model parameters in each GPU's memory before subsequently partitioning model states according to the generation parallel groups, resulting in a peak memory usage of model parameters 𝑀. As they cannot reuse training weights during generation on some GPUs, training weights need to be maintained on them, amounting to 1 𝑡𝑝𝑑 and 1  𝑡𝑝 redundant memory consumption, respectively. With our parallel grouping method for the generation stage, HybridFlow confines the all-gather operation within each micro DP group. The communication overhead is reduced to</p><formula xml:id="formula_6">𝑑 𝑔 -1 𝑡𝑝 𝑀 = 𝑡𝑝 -𝑡 𝑔 𝑝 𝑔</formula><p>𝑡 𝑔 𝑝 𝑔 𝑡𝑝 𝑀. Each GPU only needs to collect remote parameters within its micro DP group and can reuse the training weights in generation. Therefore, the peak memory usage of model parameters in HybridFlow precisely matches the model partition size on each GPU in generation, eliminating any redundancy in GPU memory usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Auto Device Mapping</head><p>Our hybrid programming model requires users to input the following configurations, which are referred to as a mapping of the RLHF dataflow to the given devices: (a) device placement of the models in the dataflow; (b) the corresponding parallelism strategy for running each model in each stage.</p><p>We provide an efficient algorithm (Algorithm 1) for users to identify the optimized mapping of executing the RLHF dataflow on a given cluster of devices, that minimizes the end-to-end latency of each RLHF iteration. Given a dataflow 𝐷, we first explore all possible placement plans P for the models in the given cluster (Line 3). For example, the PPO algorithm involves four models, resulting in 15 possible placements (from the Bell partition problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b61">62]</ref>), ranging from a completely standalone placement where all models are placed on different devices (e.g., OpenRLHF's placement) to colocating all models on the same set of devices (e.g., DeepSpeed-Chat's placement). We refer to colocated models on the same set of GPUs as a colocated set. Models in a colocated set can employ different parallelism strategies across the same set of GPUs. We identify the smallest number of GPUs to be allocated to each of the colocated model sets, 𝐴 𝑚𝑖𝑛 , based on memory consumption of colocated models, ensuring no out-of-memory errors (Line 9).</p><p>Next, starting from the minimal GPU allocation in 𝐴 𝑚𝑖𝑛 , we enumerate all feasible device allocations to each colocated model set (Lines 10-12). Given device allocation 𝐴 to the colocated set and computation workload 𝑊 of models in the set, we explore optimized parallelism strategies for each model in the auto_parallel module, that minimizes model execution latency. The workload 𝑊 includes input and output shapes and computation (training, inference or generation) of each model. In auto_parallel, we utilize a simulator module simu to estimate the latency of different parallel strategies, following previous research <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b91">92]</ref> (outline in Appendix. C).</p><p>The d_cost module estimates the end-to-end latency of the RLHF dataflow under given model placement and parallelism strategies, by iterating through all stages in the dataflow graph and summing up latencies of all stages (Lines 17, 25). For models in the same colocated set and involving computation in the same stage (such as actor and critic both performing model update in RLHF training stage), their execution latencies are summed up (Line 32). For models in different colocated sets, their execution within the same stage Algorithm 1 Device Mapping for an RLHF Dataflow</p><p>1: Input: RLHF dataflow graph 𝐷, LLMs in RLHF dataflow 𝐿=[𝑙 1 , 𝑙 2 , . . . , 𝑙 𝑘 ], workload 𝑊 of LLMs in RLHF dataflow, total # of GPUs 𝑁 , memory capacity per GPU 𝑄 2: Output: device mapping of models in RLHF dataflow 3: P ← get_placements(𝐷, 𝐿, 𝑁 ) 4: 𝐶 * ← ∞ 5: 𝑏𝑒𝑠𝑡_𝑚𝑎𝑝𝑝𝑖𝑛𝑔 ← ∅ 6: for all 𝑝𝑙𝑚 ∈ P do 7: 𝐶 𝑝𝑙𝑚 ← ∞ 8: 𝑏𝑒𝑠𝑡_𝑝𝑙𝑚_𝑎𝑙𝑙𝑜𝑐 ← ∅ 9: 𝐴 𝑚𝑖𝑛 ← get_min_alloc(𝑝𝑙𝑚, 𝑄, 𝑁 ) 10: for all 𝐴 ∈ enum_alloc(𝑁 , 𝐴 𝑚𝑖𝑛 ) do 11: 𝐿 ← [] 12: for all set ∈ 𝑝𝑙𝑚 do 13: for all 𝑙 ∈ set do 14: 𝑙 ← auto_parallel(𝐴, 𝐴 𝑚𝑖𝑛 , 𝑙,𝑊 ) 15: 𝐿.append( 𝑙) 16: 𝑝𝑙𝑚.update( 𝐿) 17: 𝐶 𝑎𝑙𝑙𝑜𝑐 ← d_cost(𝐷, 𝑝𝑙𝑚,𝑊 ) 18: if 𝐶 𝑎𝑙𝑙𝑜𝑐 &lt; 𝐶 𝑝𝑙𝑚 then 19: 𝐶 𝑝𝑙𝑚 ← 𝐶 𝑎𝑙𝑙𝑜𝑐 20: 𝑏𝑒𝑠𝑡_𝑝𝑙𝑚_𝑎𝑙𝑙𝑜𝑐 ← (𝑝𝑙𝑚, 𝐴) 21: if 𝐶 𝑝𝑙𝑚 &lt; 𝐶 * then 22: 𝐶 * ← 𝐶 𝑝𝑙𝑚 23: 𝑏𝑒𝑠𝑡_𝑚𝑎𝑝𝑝𝑖𝑛𝑔 ← 𝑏𝑒𝑠𝑡_𝑝𝑙𝑚_𝑎𝑙𝑙𝑜𝑐 24: return 𝑏𝑒𝑠𝑡_𝑚𝑎𝑝𝑝𝑖𝑛𝑔 25: Procedure d_cost(𝐷, 𝑝𝑙𝑚, 𝑊 ): 26: 𝑠 ← number of stages in 𝐷 for all 𝑙 ∈ set do 32:</p><formula xml:id="formula_7">𝑐 𝑔 [𝑖] ← 𝑐 𝑔 [𝑖] + simu( 𝑙,𝑊 [𝑖]) 33: 𝑐 [𝑖] ← 𝑚𝑎𝑥 {𝑐 [𝑖], 𝑐 𝑔 [𝑖]} 34:</formula><p>return sum(𝑐) can be parallelized, and the latency of the stage is determined by the maximum execution time among different sets (Line 33). We identify the best device placement of the models with their corresponding parallelism strategies, achieving minimal execution time per RLHF iteration (Lines 18-23).</p><p>The complexity of Algorithm 1 is 𝑂 ( (𝑁 -1)! (𝑘 -1)!(𝑁 -𝑘 )! ), where 𝑘 is the number of models in the dataflow and 𝑁 is the total number of devices to run the dataflow. This is the worst-case complexity for enumerating all possible device allocations for a placement strategy (i.e., the standalone placement), calculated by assigning 𝑁 devices to 𝑘 models (known as the integer partition problem <ref type="bibr" target="#b5">[6]</ref>). For better efficiency, we cache parallelism strategies identified for each model on a number of devices 𝐴, to eliminate redundant searches for the same parallelism strategies when the model is placed on different sets of 𝐴 GPUs in different placement strategies.</p><p>Though we assume 𝑁 homogeneous GPUs when running the auto mapping algorithm, Algorithm 1 can be readily extended for optimizing model mapping over heterogeneous devices, by considering heterogeneous devices in simu and auto_parallel modules <ref type="bibr" target="#b87">[88]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Implementation</head><p>HybridFlow is implemented in around 12k lines of Python code (LoC). Hybrid programming model. The hierarchical APIs are implemented with 1.8k LoC. The centralized single controller is built on top of Ray <ref type="bibr" target="#b49">[50]</ref> and uses Remote Process Calls (RPC) to coordinate the execution order of different models and transfer data between models following the dataflow. These intermediate data are stored in TensorDict <ref type="bibr" target="#b56">[57]</ref>. In our multi-controller paradigm for distributed computation, each model function runs on a separate process across various devices, with control messages relayed from each controller's CPU process to the corresponding GPU. Our implementation supports Megatron-LM, PyTorch FSDP, and DeepSpeed as the LLM training and inference engines, and vLLM for autoregressive generation. In vLLM, we replace the centralized KVCache manager with a distributed manager to align with the multi-controller paradigm. 3D-HybridEngine. Its main logic is implemented with 2.4k LoC on top of Megatron-LM and vLLM. We store actor model weights for training and generation stages on separate memory buffers, offload generation weights to the CPU memory during training, reload generation weights back to GPU memory during the transition, and use both buffers in generation. We use NCCL communication primitives <ref type="bibr" target="#b34">[35]</ref> to collect and concatenate model parameters in each micro DP group during the transition between training and generation. We offload KVCache to CPU memory after generation and reload it back to GPU in the next iteration. Auto-Mapping Algorithm is implemented with 1.9k LoC, together with three simulators for training, inference, and generation workloads. The algorithm is run before starting the RLHF dataflow on CPU, to generate device mapping and parallelism strategies for dataflow initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Experimental Setup</head><p>Testbed. We deploy HybridFlow on a cluster of 16 machines (128 GPUs). Each machine is equipped with 8 NVIDIA A100-80GB GPUs inter-connected with 600GB/s NVLink. The inter-machine bandwidth is 200Gbps. Our experiments use the following software versions: CUDA12.1, PyTorch 2.1.2, Megatron-core 0.6.0, NCCL 2.18.1, and vLLM 0.3.1. Models and RLHF algorithms. We run the RLHF dataflow (Figure <ref type="figure" target="#fig_0">1</ref>) of PPO <ref type="bibr" target="#b67">[68]</ref>, ReMax <ref type="bibr" target="#b42">[43]</ref> and Safe-RLHF <ref type="bibr" target="#b18">[19]</ref> algorithms. PPO is one of the most popular algorithms for RLHF <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55]</ref>, consisting of actor, critic, reference policy, and Datasets and hyperparameters. We perform RLHF on "Dahoas/ful-hh-rlhf" dataset <ref type="bibr" target="#b6">[7]</ref> of HuggingFace, which is widely used for LLM alignment <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b84">85]</ref>. As the baseline systems may not incorporate continuous-batching optimization <ref type="bibr" target="#b82">[83]</ref> during generation, for a fair comparison, we enforce the same length on all responses to be generated. In each experiment, the input prompt length and the output response length are both 1024 and the global batch size of input prompts to the actor model is 1024. The number of PPO epochs is 1 and the number of PPO update iterations per epoch is 8, aligning with previous RLHF research <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b80">81]</ref>. <ref type="bibr">Figures 9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">and 11</ref> show RLHF throughput when running PPO, ReMax, and Safe-RLHF respectively. The actor, critic, reference, and reward models in this set of experiments are of the same size, following previous practice <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b81">82]</ref>. The number of GPUs used in experiments of different model sizes ranges from the smallest number of GPUs to run RLHF without OOM to 128 GPUs. We do not enable offloading optimizer states <ref type="bibr" target="#b60">[61]</ref> in the experiments for fair comparison. Overall performance. We observe that HybridFlow consistently outperforms the baselines across all model scales. In   This is mainly because HybridFlow effectively executes generation, inference, and training in all RLHF stages by sharding the models with different parallelism strategies to fit various computation workloads. HybridFlow achieves the highest average speedup of 9.64× when training 70B models, as Hy-bridFlow reduces the transition overhead by up to 71.2% and 89.1% compared to DeepSpeed-Chat and OpenRLHF, which also incurs large inter-machine communication when training with ZeRO-3. Due to the lack of KVCache in generation engine, NeMo-Aligner's main performance bottleneck lies in the generation stage, which accounts for up to 81.2% of its RLHF iteration time. Similar results can be observed in Figures 10, 11 validating the efficiency of HybridFlow on running various RLHF algorithms. Scalability. HybridFlow achieves at least 2.09× speedup on 8 GPUs. With increasing GPUs, the strong scaling efficiency of HybridFlow on various model scales is 66.8%, computed by dividing throughput in largest scale throughput in smallest scale by max. # of GPUs min. # of GPUs [5], averaging over three algorithms and all model scales. Scaling to a large number of GPUs with a fixed global batch size results in smaller local batch sizes for each worker, potentially causing GPU underutilization. Running 7B models on 128 GPUs, HybridFlow still outperforms the best baseline OpenRLHF for 1.68×, 1.53×, and 1.71× on PPO, ReMax, and Safe-RLHF respectively. This can be attributed to Hy-bridFlow's ability to adapt the best placement strategies for different models and cluster sizes to minimize RLHF time. OpenRLHF performs better in a larger GPU cluster but less efficiently on smaller ones. 8.3 Model Placement In this experiment, we implement various model placements of the PPO algorithm in HybridFlow, under the same model and cluster settings as in Sec. 8.2: (i) colocate, the placement strategy in DeepSpeed-Chat; (ii) standalone, that in Open-RLHF and; (iii) split, NeMo-Aligner's colocation placement (actor and reference policy on the same set of devices and critic and reward model on another); (iv) hybridflow, the optimized placement obtained by Algorithm 1. Comparison of different model placements. Figure 12 reveals that optimized placement of HybridFlow under different numbers of GPUs varies. From 16 to 64 GPUs, colocating all models on the same set of devices yields the best performance. For 96 to 128 GPUs with 34B models and 96 GPUs with 13B models, the split strategy becomes optimal. The split strategy divides GPUs evenly between the two sets of models, as their sizes are equal. For 13B models on 128 GPUs, the standalone strategy achieves the highest throughput. In this case, HybridFlow allocates 64 GPUs for the actor, 32 for the critic, and 16 each for the reference and reward model. In smaller clusters, computation of all models can fully utilize GPU resources; the colocate strategy ensures maximum GPU usage in different RLHF stages. In larger clusters, RLHF throughput under colocate placement fails to scale up linearly as the batch size is fixed and the computation-tocommunication ratio decreases with a larger DP size on more GPUs. Standalone and split strategies place models on different devices with a smaller DP size for each model in larger clusters, facilitating parallel execution of different models in the same stages. In all cases, our Algorithm 1 produces the best placement with the highest training throughput. Larger critic and reward model. We further evaluate model placements when running PPO with a 13B actor and reference policy and 70B critic and reward models (larger critic and reward models are expected to produce better alignment [7]). Figure 13 shows that the colocate strategy still outperforms others by 44.8% on average with up to 64 GPUs. The split strategy achieves higher throughput with 96 GPUs. When scaling to 128 GPUs, the best placement obtained by Algorithm 1 colocates actor, reference, and reward models on 64 GPUs while allocating the remaining 64 GPUs to critic. On the same number of GPUs, actor and reference policy's computation time is much smaller than critic and reward model, and colocating the reward model with actor and reference policy reduces the GPU idle time in the experience preparation stage. In general, distributing actor and critic on different devices for parallel execution in the training stage leads to higher throughput in large clusters. 8.4 3D-HybridEngine Transition time comparison. Figure 14 shows the transition time between actor training and generation stages on for parallel execution in the training and preparation stages would help achieve higher throughput. Resource multiplexing. HybridFlow enables colocation of models on shared devices by utilizing time-sharing for GPU computation. Recent research in DNN task scheduling has developed fine-grained resource multiplexing techniques, primarily aimed at achieving the service-level objectives of individual tasks [8, 18, 26, 26, 47, 56, 77]. Although the ResourcePool implementation supports parallel execution of collocated models, HybridFlow generally adheres to sequential execution to prevent GPU resource contention or OOM issues as discussed in Section 2.3. Applying GPU sharing and heterogeneous resources in RLHF training poses distinct challenges, as it seeks to balance the computation workload and manage complex data dependencies among various tasks. Investigating fine-grained auto-mapping algorithms for GPU sharing in RLHF training, coupled with model offload optimization and integration of heterogeneous devices, would be a promising direction for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">End-to-End performance</head><p>From alignment to reasoning. In RLHF for LLM alignment, the reward signal is generated by the reward model. Besides alignment tasks, similar algorithms (e.g., PPO and GRPO <ref type="bibr" target="#b69">[70]</ref>) can be applied to other domains, such as code generation and mathematical reasoning. For these tasks, a ground truth may exist for each prompt, which can be determined by assessing the correctness of the output value for each code test case and verifying the accuracy of mathematical results. Therefore, the reward model can be replaced by non-neural-network reward modules, such as a sandbox environment <ref type="bibr" target="#b86">[87]</ref> for evaluating generated code or a reward function <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b64">65]</ref> to validate mathematical results. HybridFlow can seamlessly integrate these reward modules by wrapping them as remote functions and orchestrating their execution within the singleprocess script, providing a flexible and efficient framework for diverse reinforcement learning applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Related Work</head><p>RL frameworks. There have been plenty of frameworks for RL, ranging from general-purpose RL systems design for small-scale DNNs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> to RLHF systems specifically optimized for LLMs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b81">82]</ref>. We have thoroughly examined closely related work in §2 and we discuss more RL frameworks in this section. These RL frameworks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b73">74]</ref>, similar to recent RLHF systems, use a hodgepodge of multi-controller frameworks to implement their algorithms. They establish multiple long-running distributed programs with each component coordinating the execution order with hard-coded data synchronization. Gear <ref type="bibr" target="#b73">[74]</ref> further optimized the experience replay segment of the RL pipeline. However, all these frameworks fail to support LLM training, inference, and generation in RLHF.</p><p>LLM training and serving systems. TorchDDP <ref type="bibr" target="#b56">[57]</ref> and Horovod <ref type="bibr" target="#b68">[69]</ref> support data parallel training. ByteScheduler <ref type="bibr" target="#b57">[58]</ref> and DeepSpeed <ref type="bibr" target="#b59">[60]</ref> extend data parallelism with communication and memory optimizations. Numerous systems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b88">89]</ref> optimized large model training through model parallelisms such as tensor parallelism and pipeline parallelism to partition models across devices. LLM serving systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b91">92]</ref> also adopts data and model parallelism to accelerate auto-regressive generation with specialized optimizations like continuous-batching <ref type="bibr" target="#b82">[83]</ref> and chunked-prefill <ref type="bibr" target="#b2">[3]</ref>. Note that all the above frameworks adopt multi-controller paradigm for efficient computation. Dataflow systems. Dataflow systems like MapReduce <ref type="bibr" target="#b20">[21]</ref>, Spark <ref type="bibr" target="#b85">[86]</ref>, Dryad <ref type="bibr" target="#b32">[33]</ref>, and Naiad <ref type="bibr" target="#b50">[51]</ref> are popular for analytics and ML workloads but they lack support for dynamic task graphs. Ray <ref type="bibr" target="#b49">[50]</ref> unifies task-parallel and actor programming models in a single dynamic task graph and implements a scalable distributed scheduler and a global control store, which is adopted by many RL frameworks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Pathways [9], a closed-source project for TPUs, are designed to easily express complex parallelism patterns and fine-grain control flow within a single DNN model, such as pipeline parallelism and Mixture-of-Experts with sparse computation. It employs an asynchronous distributed dataflow design that enables parallel control plane execution despite data dependencies, reducing the dispatch overhead from single-controller paradigm. Its main focus lies on single-model training, requiring complex compilations of each sub-network of a DNN model. HybridFlow can integrate Pathways as a submodule to implement the computation of models in the RLHF dataflow. 11 Conclusion HybridFlow is an RLHF framework that enables flexible representation and efficient execution of diverse RLHF algorithms. We propose a hybrid programming model that allows users to easily build RLHF dataflow in a few lines of code by encapsulating distributed computation of different LLMs into primitive APIs and hiding the complexity of data resharding among nodes. Our 3D-HybridEngine ensures efficient execution of training and generation of the actor model, with zero memory redundancy and significantly reduced communication overhead for model parameter resharding. Furthermore, our effective mapping algorithm optimizes GPU allocation and placement of models in the RLHF dataflow. Extensive experiments demonstrate that HybridFlow achieves 1.53× to 20.57× speedup compared to state-of-the-art RLHF systems under various model sizes and cluster scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Primitive APIs in HybridFlow</head><p>In HybridFlow, we implemented the primitive of each model in RLHF training by inheriting the 3DParallelWorker, FSDP Worker and ZeROWorker. The functions of these model classes are designed to decouple the distributed computation code and provide fundamental operations in RLHF for the users. This primitive design is compatible with the auto-regressive generation, forward pass, backward pass, and model update operations in the existing distributed inference and training frameworks. Users can easily customize the RLHF training dataflow (by adapting the numerical computation in the provided functions) according to the algorithm's design and benefit from reusing the underlying distributed computation implementation. We illustrate the meaning and the actual computations of these APIs in Table <ref type="table" target="#tab_6">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Transfer Protocols</head><p>We implemented transfer protocols that cover all common use cases of data resharding between models in RLHF dataflow. Users can utilize these pre-defined protocols to generate any RLHF dataflow. Moreover, Users can easily define their own transfer protocols by implementing a collect function and a distribute function. Transfer protocols decoupled the complicated data resharding and distributed training. We denote p, t, d as the rank of the worker in pipeline-, tensor-and dataparallel group respectively. We illustrate these predefined protocols in Table <ref type="table">3</ref>. best_para ← para_plan 17: return best_para includes three simulators for training, inference, and generation workload, all are analytical models following previous research <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b91">92]</ref>. The training and inference workload is compute-bound while the generation workload is memorybound. For the actor model, we first find the parallelism strategy for training and record the memory usage in the training stage. During actor generation, KVCache requirements are calculated using the batch size and max sequence length. If the model-parallel size for the generation stage cannot accommodate both parameters and KVCache, we increase it. Then, we seek the optimal strategy with corresponding KVCache allocation by comparing the latency estimation. Developing a comprehensive autoregressive generation simulator that accounts for variable KVCache sizes could further enhance the auto-mapping process in RLHF research. The actor model computes the log probability of each token in the prompts and responses. This log probability is the same as the return log probability when performing generation using the same model precision. (Optional in PPO) compute_loss a forward pass The actor model computes the pretrain loss based on the pertaining dataset <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Auto-Parallelism Algorithm</head><p>update_actor a forward, backward pass and model update Based on the advantages, returns (calculated from compute_advantage) and pertaining loss, the actor model calculate the training loss and update its weights. We implement various loss for diverse RLHF algorithms including PPO <ref type="bibr" target="#b54">[55]</ref>, Safe-RLHF <ref type="bibr" target="#b18">[19]</ref>, ReMax <ref type="bibr" target="#b42">[43]</ref>, GRPO <ref type="bibr" target="#b69">[70]</ref> and others.</p><p>Critic compute_values a forward pass The critic model computes the values for each prompt and response. update_critic a forward, backward pass and model update</p><p>Based on the values and returns, the critic computes a squared-error loss to update its weights. We also implement critic loss for diverse RLHF algorithms including PPO <ref type="bibr" target="#b54">[55]</ref>, Safe-RLHF <ref type="bibr" target="#b18">[19]</ref>, ReMax <ref type="bibr" target="#b42">[43]</ref>, GRPO <ref type="bibr" target="#b69">[70]</ref> and others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Policy compute_ref_log_prob a forward pass</p><p>The reference model computes the reference log probability of each token in the prompts and responses. This log probability is utilized as a benchmark to evaluate the divergence of the actor model and constrain its learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward compute_reward a forward pass</head><p>The reward model conducts forward computation to calculate scores for a given set of prompts and responses. The rewards could be token-level or sample-level.</p><p>-compute_advantage numerical computation</p><p>Based on the values rewards from the value model and reward model respectively, the function estimates the advantages on the given prompts and the current policy model's responses. This computation involves no model forward passes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Dataflow graph of 3 RLHF algorithms [19, 43, 55]. Stage 1 ○, 2 ○, 3 ○ represent Generation, Preparation, and Training, respectively. RLHF algorithms, model sizes and cluster scales. Our evaluation demonstrates 1.53×∼20.57× throughput improvements.We have open-sourced HybridFlow and believe that Hy-bridFlow can boost future RLHF research and development.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Programming model used in RLHF systems. (a) Existing RLHF systems adopt the multi-controller paradigm. (b) HybridFlow utilizes a hybrid programming model: the single-controller coordinates models; each model uses multicontroller paradigm in distributed computation. Inactive node in grey represents operation not executed at this time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><figDesc>Figure 3. Dataflow execution given a model placement plan. Blocks with numbers represent GPUs. In dashed boxes, the models are placed on different sets of devices and can be concurrently computed. Reference model (blue) and reward model (green) are colocated on the same set of GPUs and executed sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 Figure 4 .</head><label>44</label><figDesc>Figure 4  depicts the architecture of HybridFlow, which consists of three major components: Hybrid Programming Model,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4</head><figDesc>Hybrid Programming Model4.1 Hierarchical APIs Intra-node: encapsulating distributed program. For distributed computation of each model in different RLHF stages, we provide a base class, 3DParallelWorker. Given allocated Model + (a) Actor model initialization (b) Data resharding and asynchronous execution Critic t e x i t s h a 1 _ b a s e 6 4 = " 5 f U</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>…Figure 5 .</head><label>5</label><figDesc>Figure 5. An illustration of hierarchical APIs. (a) Model with 3D parallel configuration, resource allocation, and 3DParallelWorker initialization. (b) Asynchronous data resharding between two models with collect and distribute functions in 3D_PROTO. devices, it facilitates distributed model weight initialization and establishes 3D parallel groups for each model. A parallel group includes a set of GPUs to host a specific parallel dimension of the model, e.g., different tensor shards in TP and different model replicas in DP. Figure 5(a) illustrates initialization of the actor model with our APIs, while initialization of other models is similar.Inheriting from the 3DParallelWorker class, several model classes, for actor, critic, reference, and reward model, respectively, are provided. Each of these model classes encapsulates APIs to implement the model's distributed forward and backward computation, auto-regressive generation, and optimizer updates, decoupling the distributed computation code with data dependencies with other models. These APIs can be easily implemented by reusing the computation scripts from existing LLM systems. For example, the computation involved in update_actor function of ActorWorker (the class for the actor model) is similar to the pre-training scripts in Megatron-LM<ref type="bibr" target="#b70">[71]</ref>. A model class encapsulates fundamental operations for implementing various RLHF algorithms, e.g., generate_sequences in the actor model class for generating responses based on the prompts and compute_reward in the reward model class for evaluating responses through a forward pass. (More APIs are detailed in Appendix A).Besides base class 3DParallelWorker that implements 3D parallelism, we further provide base classes for PyTorch FSDP (FSDPWorker) and ZeRO (ZeROWorker), and the corresponding model classes inheriting each base class, to support different parallelism strategies in model computation. Paral-lelWorker in Figure4denotes one of these base classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>#</head><figDesc>Initialize cost model by reusing the RewardWorker cost = RewardWorker(cost_config, resource_pool) ... # omit other models initialization algo_type = "Safe-RLHF" # specify different RLHF numerical computation. # Examples of PPO and Safe-RLHF for (prompts, pretrain_batch) in dataloader: # Stage 1: Generate responses batch = actor.generate_sequences(prompts) batch = actor.generate_sequences(prompts, do_sample=False) # Stage 2: Prepare experience batch = critic.compute_values(batch) batch = reference.compute_log_prob(batch) batch = reward.compute_reward(batch) batch = cost.compute_cost(batch) batch = compute_advantages(batch, algo_type) # Stage 3: Actor and critic training critic_metrics = critic.update_critic(batch, loss_func=algo_type) pretrain_loss = actor.compute_loss(pretrain_batch) batch["pretrain_loss"] = pretrain_loss actor_metrics = actor.update_actor(batch, loss_func=algo_type) is added for Safe-RLHF is added for ReMax Not necessary in ReMax</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " b D j 2 x s 9 J 0 e + T m Z z d V f 7 q I U e s b / s = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r 1 o / G v X o J V g E D 1 I S k e q x 4 M V j B f s B b S i b z b R d u t m E 3 Y l Y Q 3 + J F w + K e P W n e P P f u G 1 z 0N Y H A 4 / 3 Z n Z n X p A I r t F 1 v 6 3 C 2 v r G 5 l Z x u 7 S z u 7 d f t g 8 O W z p O F Y M m i 0 W s O g H V I L i E J n I U 0 E k U 0 C g Q 0 A 7 G N z O / / Q B K 8 1 j e 4 y Q B P 6 J D y Q e c U T R S 3 y 7 3 E B 6 R c c U E h J k 3 7 d s V t + r O 4 a w S L y c V k q P R t 7 9 6 Y c z S C C Q y Q b X u e m 6 C f k Y V c v P k t N R L N S S U j e k Q u o Z K G o H 2 s / n i U + f U K K E z i J U p i c 5 c / T 2 R 0 U j r S R S Y z o j i S C 9 7 M / E / r 5 v i 4 N r P u E x S B M k W H w 1 S 4 W D s z F J w Q q 6 A o Z g Y Q p n i Z l e H j a i i D E 1 W J R O C t 3 z y K m l d V L 1 a t X Z 3 W a m f 5 3 E U y T E 5 I W f E I 1 ek T m 5 J g z Q J I y l 5 J q / k z X q y X q x 3 6 2 P R W r D y m S P y B 9 b n D x 7 y k 1 Y = &lt; / l a t e x i t &gt; 1 ○ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H U b 6 8 j i / 9 A x T 6 w + c v Z j g f c F 6 E z Y = " &gt; A A A B + H i c b V B N S 8 N A E N 3 4 W e t H o x 6 9 B I v g Q U p S p H o s e P F Y w X 5 A G 8 p m M 2 2 X b j Z h d y L W 0 F / i x Y M i X v 0 p 3 v w 3 b t s c t P X B w O O 9 m d 2 Z F y S C a 3 T d b 2 t t f W N z a 7 u w U 9 z d 2 z 8 o 2 Y d H L R 2 n i k G T x S J W n Y B q E F x C E z k K 6 C Q K a B Q I a A f j m 5 n f f g C l e S z v c Z K A H 9 G h 5 A P O K B q p b 5 d 6 C I / I u G I C w q w 6 7 d t l t + L O 4 a w S L y d l k q P R t 7 9 6 Y c z S C C Q y Q b X u e m 6 C f k Y V c v P k t N h L N S S U j e k Q u o Z K G o H 2 s / n i U + f M K K E z i J U p i c 5 c / T 2 R 0 U j r S R S Y z o j i S C 9 7 M / E / r 5 v i 4 N r P u E x S B M k W H w 1 S 4 W D s z F J w Q q 6 A o Z g Y Q p n i Z l e H j a i i D E 1 W R R O C t 3 z y K m l V K 1 6 t U r u 7 L N c v 8 j g K 5 I S c k n P i k S t S J 7 e k Q Z q E k Z Q 8 k 1 f y Z j 1 Z L 9 a 7 9 b F o X b P y m W P y B 9 b n D y B 3 k 1 c = &lt; / l a t e x i t &gt; 2 ○ Load prompts &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a o i x u 1 H E d 7 e I U D M b N I f V 3 G r E T j g = " &gt; A A A B + H i c b V D L S g N B E O z 1 G e M j q x 6 9 D A b B g 4 R d 0 e g x 4 M V j B P O A Z A m z s 5 N k y O y D m V 4 x L v k S L x 4 U 8 e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>U 7 y 7 K t b M 8 j g I c w T G c g g t X U I N b q E M D G K T w D K / w Z j 1 Z L 9 a 7 9 T F v X b H y m U P 4 A + v z B y U G k 1 o = &lt; / l a t e x i t &gt; 5 ○ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N a / p c Q 2 W I U 2 2 F 5 P 4 e h 8 i Y M a 1 q t A = " &gt; A A A B + H i c b V D L S g N B E O z 1 G e M j q x 6 9 D A b B g 4 R d l e g x 4 M V j B P O A Z A m z s 5 N k y O y D m V 4 x L v k S L x 4 U 8 e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>U 7 y 7 L t b M 8 j g I c w T G c g g t X U I N b q E M D G K T w D K / w Z j 1 Z L 9 a 7 9 T F v X b H y m U P 4 A + v z B y H 8 k 1 g = &lt; / l a t e x i t &gt; 3 ○ Generate and AllGather responses &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 f U t I M U C 6 G D a K x F c m K H J t d H W N 7 Q = " &gt; A A A B + H i c b V B N S 8 N A E N 3 4 W e t H o x 6 9 B I v g Q U o i p X o s e P F Y w X 5 A G 8 p m M 2 2 X b j Z h d y L W 0 F / i x Y M i X v 0 p 3 v w 3 b t s c t P X B w O O 9 m d 2 Z F y S C a 3 T d b 2 t t f W N z a 7 u w U 9 z d 2 z 8 o 2 Y d H L R 2 n i k G T x S J W n Y B q E F x C E z k K 6 C Q K a B Q I a A f j m 5 n f f g C l e S z v c Z K A H 9 G h 5 A P O K B q p b 5 d 6 C I / I u G I C w q w 6 7 d t l t + L O 4 a w S L y d l k q P R t 7 9 6 Y c z S C C Q y Q b X u e m 6 C f k Y V c v P k t N h L N S S U j e k Q u o Z K G o H 2 s / n i U + f M K K E z i J U p i c 5 c / T 2 R 0 U j r S R S Y z o j i S C 9 7 M / E / r 5 v i 4 N r P u E x S B M k W H w 1 S 4 W D s z F J w Q q 6 A o Z g Y Q p n i Z l e H j a i i D E 1 W R R O C t 3 z y K m l d V r x a p X Z X L d c v 8 j g K 5 I S c k n P i k S t S J 7 e k Q Z q E k Z Q 8 k 1 f y Z j 1 Z L 9 a 7 9 b F o X b P y m W P y B 9 b n D y O B k 1 k = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. 3D-HybridEngine workflow in one RLHF iteration. 4 GPUs are used for actor training and generation. 1-2-2 (𝑝-𝑡-𝑑) parallel groups are used in training and 1-1-2-2 (𝑝 𝑔 -𝑡 𝑔 -𝑑 𝑔 -𝑑) parallel groups are used in generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><figDesc>unused weights from other ranks (a) Same grouping methods between training and generation (HybridFlow-V) (b) Optimized parallel grouping methods (HybridFlow)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Model weights resharding. 2 machines each with 4 GPUs are used for actor training and generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><figDesc>-V, when 3D-HybridEngine uses the above vanilla parallel grouping methods in the two stages.We further design a new parallel grouping method for 3D-HybridEngine to use in the generation stage, that eliminates the redundancy in weights storage and leads to minimal memory footprint and communication due to actor model resharding between training and generation. Specifically, we form generation TP and PP groups by selecting ranks at regular intervals, determined by 𝑡 𝑡 𝑔 and 𝑝 𝑝 𝑔 , and construct micro DP groups by sequentially assigning ranks along the generation TP or PP dimensions. In Figure8(b), 1-2-2-2 parallel groups are used in generation: the generation TP groups are [G1, G3], [G2, G4], [G5, G7], [G6, G8]; and the micro DP groups are [G1, G2], [G3, G4], [G5, G6], [G7, G8]. This strategic rearrangement of generation parallel groups leads to overlap between training and generation model weights on each device, enabling reuse of training weights during generation and zero redundancy in device memory usage due to model resharding. In addition, 3D-HybridEngine conducts several all-gather operations concurrently, one within each micro DP group, leading to significantly reduced communication overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>27 :</head><label>27</label><figDesc>𝑐 ← [0] × 𝑠 // Initialize latency for each stage to 0 28: for all set ∈ 𝑝𝑙𝑚 do 29: 𝑐 𝑔 ← [0] × 𝑠 30:for all 𝑖 ∈ {0, ..., 𝑠 -1} do 31:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Figure 12. Throughput of HybridFlow under different placements</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 9</head><label>9</label><figDesc>Figure9for PPO, HybridFlow outperforms DeepSpeed-Chat, OpenRLHF and NeMo-Aligner by 3.67× (up to 7.84×), 3.25× (up to 5.93×) and 12.52× (up to 20.57×), respectively. This is mainly because HybridFlow effectively executes generation, inference, and training in all RLHF stages by sharding the models with different parallelism strategies to fit various computation workloads. HybridFlow achieves the highest average speedup of 9.64× when training 70B models, as Hy-bridFlow reduces the transition overhead by up to 71.2% and 89.1% compared to DeepSpeed-Chat and OpenRLHF, which also incurs large inter-machine communication when training with ZeRO-3. Due to the lack of KVCache in generation engine, NeMo-Aligner's main performance bottleneck lies in the generation stage, which accounts for up to 81.2% of its RLHF iteration time. Similar results can be observed inFigures 10,<ref type="bibr" target="#b10">11</ref> validating the efficiency of HybridFlow on running various RLHF algorithms. Scalability. HybridFlow achieves at least 2.09× speedup on 8 GPUs. With increasing GPUs, the strong scaling efficiency of HybridFlow on various model scales is 66.8%, computed by di-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Algorithm 2</head><label>2</label><figDesc>outlines the search process of the optimal parallelism strategy of each model. Starting from the minimal model parallelism size of each model (to prevent OOM when colocating with multiple workers), we enumerate all feasible parallel configurations based on the number of GPUs and the number of GPUs per machine 𝑈 . The default number of 𝑈 is set to 8. We use simu module to estimate the latency of each model based on their workload. This module Algorithm 2 Auto Parallelism Algorithm 1: Input: Device allocation 𝐴, minimal device allocation and model parallel size for each model in a set 𝐴 𝑚𝑖𝑛 , workload 𝑊 , the number of GPUs per machine 𝑈 2: Output: the parallelism strategy for the model in a set 3: Procedure auto_parallel(𝐴, 𝐴 𝑚𝑖𝑛 , 𝑙, 𝑊 ): 4: 𝑁 𝑙 = 𝐴[𝑙] // Get device allocation of the model 5: 𝑡 𝑚𝑖𝑛 = 𝐴 𝑚𝑖𝑛 [𝑙].𝑡 // Get minimal model parallel size 6: 𝑝 𝑚𝑖𝑛 = 𝐴 𝑚𝑖𝑛 [𝑙].𝑝 7: best_para ← ∅ 8: best_para.cost ← ∞ 9: for all t ∈ {𝑡 𝑚𝑖𝑛 , 𝑡 𝑚𝑖𝑛 + 1..., 𝑈 } do 10: for all p ∈ {𝑝 𝑚𝑖𝑛 , 𝑝 𝑚𝑖𝑛 + 1..., 𝑁 𝑙 𝑈 } do ← simu(𝑝𝑎𝑟𝑎_𝑝𝑙𝑎𝑛, 𝑙,𝑊 [𝑙]) 14: if best_para.𝑐𝑜𝑠𝑡 &gt; cost then 15: best_para.𝑐𝑜𝑠𝑡 ← cost 16:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of RLHF frameworks. Figures illustrate execution of one PPO iteration. Numbers 1-6 represent response generation, reward model inference, reference model inference, critic inference, actor training, and critic training, respectively.</figDesc><table><row><cell cols="2">RLHF system</cell><cell>DeepSpeed-Chat</cell><cell>OpenRLHF</cell><cell>NeMo-Aligner</cell><cell>HybridFlow</cell></row><row><cell cols="2">Parallelism</cell><cell>Training: ZeRO Generation:TP</cell><cell>Training: ZeRO Generation:TP</cell><cell>3D Parallelism for both training and generation</cell><cell>Training: 3D, ZeRO, FSDP Generation: 3D Parallelism</cell></row><row><cell cols="2">Actor weights</cell><cell>Model resharding</cell><cell>Using two copies of actor</cell><cell>Using identical model partition</cell><cell>Zero-redundancy</cell></row><row><cell cols="2">in training &amp; generation</cell><cell>from ZeRO to TP</cell><cell>weights for the two stages</cell><cell>in two stages (shared weights)</cell><cell>model resharding</cell></row><row><cell cols="2">Model</cell><cell>Colocate all models</cell><cell>Each model placed</cell><cell>Actor/Ref colocated on some GPUs</cell><cell>Support various</cell></row><row><cell cols="2">Placement</cell><cell>on the same set of devices</cell><cell>on separate devices</cell><cell>Critic/RM colocated on other GPUs</cell><cell>model placement</cell></row><row><cell cols="2">Execution</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pattern</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Actor</cell><cell>GPU Process</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Critic</cell><cell>Reward model</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Reference Policy</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Transition overhead between training &amp; generation 𝑡𝑝 𝑀 𝑡𝑝 -𝑡 𝑔 𝑝 𝑔 𝑡 𝑔 𝑝 𝑔 𝑡𝑝 𝑀</figDesc><table><row><cell></cell><cell cols="3">DS-Chat HybridFlow-V HybridFlow</cell></row><row><cell>Comm. Vol</cell><cell>𝑡𝑝𝑑 -1 𝑡𝑝𝑑 𝑀</cell><cell>𝑡𝑝 -1</cell><cell></cell></row><row><cell>Peak Mem.</cell><cell>𝑀</cell><cell>𝑀</cell><cell>1 𝑡 𝑔 𝑝 𝑔 𝑀</cell></row><row><cell>Redundancy</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Key functions provided in each model class. The users can use these provided functions to construct various RLHF algorithms in a few lines of code.Based on a batch of prompts, the actor model generates a batch of responses and returns the log probability of each token in the responses.</figDesc><table><row><cell>Model</cell><cell>APIs</cell><cell>Computation</cell><cell>Interpretation</cell></row><row><cell>Actor</cell><cell>generate_sequence</cell><cell>auto-regressive generation</cell><cell></cell></row><row><cell></cell><cell>compute_log_prob</cell><cell>a forward pass</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank our shepherd <rs type="person">Y. Charlie Hu</rs> and the anonymous reviewers for their constructive feedback. We thank <rs type="person">Xin Liu</rs>, <rs type="person">Yangrui Chen</rs>, and <rs type="person">Ningxin Zheng</rs> for their insightful feedback on this project. This work was supported in part by a <rs type="funder">ByteDance Research Collaboration</rs> Project, and grants from <rs type="funder">Hong Kong RGC</rs> under the contracts <rs type="grantNumber">HKU 17204423</rs> and <rs type="grantNumber">C7004-22G</rs> (CRF).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AqDwNfZ">
					<idno type="grant-number">HKU 17204423</idno>
				</org>
				<org type="funding" xml:id="_wQSCj7R">
					<idno type="grant-number">C7004-22G</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b72">[73]</ref> <p>model with sizes ranging from 7B to 70B. Safe-RLHF has an additional cost model whose architecture and size are the same as the reward model and ReMax eliminates the critic model. We use mixed precision for actor and critic training, i.e., BF16 for model parameters and FP32 for gradient and optimizer states, with Adam <ref type="bibr" target="#b37">[38]</ref> optimizer in all experiments. BF16 is used in model inference and auto-regressive generation. If not specified, the experiment results are obtained from PPO. Baselines. We compare HybridFlow with state-of-the-art RLHF systems including DeepSpeed-Chat <ref type="bibr" target="#b81">[82]</ref> v0.14.0, Open-RLHF <ref type="bibr" target="#b29">[30]</ref> v0.2.5, and NeMo-Aligner <ref type="bibr" target="#b16">[17]</ref> v0.2.0 (detailed in Table <ref type="table">1</ref>). NeMo-Alginer doesn't support ReMax algorithm. We do not compare HybridFlow to other frameworks such as Trlx <ref type="bibr" target="#b26">[27]</ref>, HuggingFaceDDP <ref type="bibr" target="#b78">[79]</ref>, and Collosal-Chat <ref type="bibr" target="#b14">[15]</ref> as they are less representative and slower than the above baselines (as reported in <ref type="bibr" target="#b81">[82]</ref>).</p><p>We use RLHF throughput (tokens/sec) as the performance metric, computed by dividing the total number of tokens in prompts and responses in a global batch by one RLHF iteration time. All reported performance numbers are averaged over 5 training iterations after a warm-up of 10 iterations. ) with 70B models, while maintaining consistent overhead across different cluster scales. This is attributed to our new parallel grouping method for the generation stage ( §5.4). In baseline methods, all model parameters must be collected during transition, necessitating layer-by-layer collections multiple times to prevent OOM. HybridFlow enables zero memory redundancy during transition and requires only one all-gather operation per micro DP group. Transition and generation time We further validate the need to use different parallel sizes in actor training and generation in HybridFlow. In this experiment, all models are colocated on the same set of GPUs, and the KVCache for generation is allocated using the remaining GPU memory (i.e., best-effort allocation). Figure <ref type="figure">15</ref> gives the transition and generation time when running RLHF on 16 GPUs with 7B and 13B models, respectively, with training parallel groups 1-8-2 (following p-t-d convention) and varying generation TP group size 𝑡 𝑔 from 1 to 8. The generation PP group size remains constant at 𝑝 𝑔 =1 and the micro DP group size 𝑑 𝑔 is computed as 8  𝑡 𝑔 . We observe that applying a smaller generation TP group size, 𝑡 𝑔 =2, for 7B models and 𝑡 𝑔 =4 for 13B models reduces the generation latency by 60.3% and 36.4%, respectively. Conversely, using the same TP size as training (𝑡 𝑔 =8), following the NeMo-Aligner approach, results in the largest generation latency due to GPU underutilization. Further reducing 𝑡 𝑔 fails to achieve higher speedup, as a smaller 𝑡 𝑔 necessitates maintaining a larger KVCache per GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Algorithm Runtime</head><p>Figure <ref type="figure">16</ref> shows the running time of Algorithm 1, which is significantly shorter than days of actual RLHF training. A linear growth of running time is exhibited, revealing good scalability of the device mapping algorithm with model size and cluster size. Most of the running time is spent on estimating the execution latency of each model's parallel strategies. More parallelism strategies are available for a larger model, requiring more simulations to identify the optimal one for each placement plan. Our caching of optimal parallelism strategies of the models to be reapplied across different placements reduces the search time for the best placement to at most half an hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussions</head><p>Fault Tolerance. HybridFlow is orthogonal to existing faulttolerance approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b92">93]</ref>   Gather the data from all ranks.</p><p>All the worker methods have the same input and run the ssme codes, e.g. model initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D_PROTO</head><p>Split the data, scatter across all DP ranks and broadcast within the group.</p><p>Gather and concatenate the data from the p=-1, t=0 worker in all DP groups.</p><p>The model is sharded among multiple workers within each data-parallel group. The output of the model only exists in the last pipeline stage and is duplicated across the data-parallel groups. This is a typical scenario in 3D parallel training in Megatron-LM, Deepspeed, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D_ALL_MICRO_DP</head><p>Split the data by micro DP size, scatter across all micro DP groups and broadcast among all ranks within the group.</p><p>Gather and concatenate the data from the local_rank=0 worker in all micro DP groups.</p><p>Used with HybridEngine. It is used to handle the 3D-parallel scheme of the policy model, when switching between training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D_PP_ONLY</head><p>Broadcast the data to all ranks. Gather and concatenate the data from the t=0, d=0 worker in all PP groups.</p><p>Used to examine weight names as they are identical in TP and DP groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DP_PROTO</head><p>Split the data into batches and scatter across all DP ranks.</p><p>Gather and concatenate the data from all DP ranks.</p><p>Training model in data-parallel mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALL_TO_ALL</head><p>No operation.</p><p>Gather the data from all ranks. Used when debugging. Users can manually define the inputs of each worker and examine their outputs respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: learning functions at scale</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN international conference on functional programming</title>
		<meeting>the 21st ACM SIGPLAN international conference on functional programming</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. 2023. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills</title>
		<author>
			<persName><forename type="first">Amey</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayashree</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nipun</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramachandran</forename><surname>Gulavani</surname></persName>
		</author>
		<author>
			<persName><surname>Ramjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.16369</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Preferencebased policy learning</title>
		<author>
			<persName><forename type="first">Riad</forename><surname>Akrour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011-09-05">2011. September 5-9, 2011</date>
			<biblScope unit="page" from="12" to="27" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 11</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Validity of the single processor approach to achieving large scale computing capabilities</title>
		<author>
			<persName><forename type="first">Gene</forename><forename type="middle">M</forename><surname>Amdahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="1967-04-18">1967. April 18-20, 1967</date>
			<biblScope unit="page" from="483" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Integer partitions</title>
		<author>
			<persName><forename type="first">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimmo</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><surname>Eriksson</surname></persName>
		</author>
		<imprint>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<title level="m">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">{PipeSwitch}: Fast pipelined context switching for deep learning applications</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="499" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudip</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pathways: Asynchronous distributed dataflow for ml. Proceedings of Machine Learning and Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="430" to="449" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exponential polynomials</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="page" from="258" to="277" />
			<date type="published" when="1934">1934. 1934</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><surname>Mccandlish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<ptr target="https://arxiv.org/abs/2005.14165" />
	</analytic>
	<monogr>
		<title level="m">Language Models are Few-Shot Learners</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Caspi</surname></persName>
		</author>
		<ptr target="https://github.com/NervanaSystems/coach" />
		<title level="m">Reinforcement learning coach by Intel</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collective communication: theory, practice, and experience</title>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Heimlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Purkayastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1749" to="1783" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<ptr target="https://github.com/binmakeswell/ColossalChat" />
	</analytic>
	<monogr>
		<title level="j">Collosal-Chat</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Collosal-AI Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="https://github.com/NVIDIA/TensorRT-LLM" />
		<title level="m">TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language Model Inference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="https://github.com/NVIDIA/NeMo-Aligner" />
		<title level="m">NeMo-Aligner: Scalable toolkit for efficient model alignment</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">{DVABatch}: Diversity-aware {Multi-Entry} {Multi-Exit} batching for efficient processing of {DNN} services on {GPUs}</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deze</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 USENIX Annual Technical Conference</title>
		<title level="s">USENIX ATC</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="183" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Safe RLHF: Safe Reinforcement Learning from Human Feedback</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mickel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=TyFrPOKYXw" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The spmd model: Past, present and future</title>
		<author>
			<persName><forename type="first">Frederica</forename><surname>Darema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Parallel Virtual Machine and Message Passing Interface: 8th European PVM/MPI Users&apos; Group Meeting Santorini/Thera</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001-09-23">2001. September 23-26. 2001</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MapReduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2022. {Check-N-Run}: A checkpointing system for training deep learning recommendation models</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Kumar Matam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnakumar</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<biblScope unit="page" from="929" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DAPPLE: A pipelined data parallel approach for training large models</title>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixue</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Habitat: A {Runtime-Based} computational performance predictor for deep neural network training</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Golikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 USENIX Annual Technical Conference (USENIX ATC 21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="503" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tensorflow agents: Efficient batched reinforcement learning in tensorflow</title>
		<author>
			<persName><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02878</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences</title>
		<author>
			<persName><forename type="first">Mingcong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="539" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">trlX: A framework for large scale reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Havrilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Zhuravinskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duy</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Castricato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8578" to="8595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
	</analytic>
	<monogr>
		<title level="j">OpenAI baselines</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Bakhtiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Kurilenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.08671</idno>
		<title level="m">DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Jian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xibin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xianyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daoning</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/OpenLLMAI/OpenRLHF" />
		<title level="m">OpenRLHF: A Ray-based Highperformance RLHF framework</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Shengyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Noukhovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arian</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.17031</idno>
		<title level="m">The N+ Implementation Details of RLHF with PPO: A Case Study on TL; DR Summarization</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dryad: distributed data-parallel programs from sequential building blocks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGOPS/EuroSys European conference on computer systems</title>
		<meeting>the 2nd ACM SIGOPS/EuroSys European conference on computer systems</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Oobleck: Resilient distributed training of large models using pipeline templates</title>
		<author>
			<persName><forename type="first">Insu</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenning</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Symposium on Operating Systems Principles</title>
		<meeting>the 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="382" to="395" />
		</imprint>
	</monogr>
	<note>Xin Jin, and Mosharaf Chowdhury</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Jeaugey</surname></persName>
		</author>
		<title level="m">GPU Technology Conference (GTC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinmin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibiao</forename><surname>Nong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15627</idno>
		<title level="m">MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Timo</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Bengs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.14925</idno>
		<title level="m">A survey of reinforcement learning from human feedback</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980[cs.LG]</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">PyTorch implementation of advantage actor critic (A2C), proximal policy optimization (PPO) and scalable trust-region method for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<ptr target="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Symposium on Operating Systems Principles</title>
		<meeting>the 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="611" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samrat</forename><surname>Phatale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colton</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Carbune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.00267</idno>
		<title level="m">Rlaif: Scaling reinforcement learning from human feedback with ai feedback</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://github.com/cli99/llm-analysis" />
		<title level="m">LLM-Analysis: Latency and Memory Analysis of Transformer Models for Training and Inference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Quan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10505</idno>
		<title level="m">ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">2023. {AlpaServe}: Statistical multiplexing with model parallelism for deep learning serving</title>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinmin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<biblScope unit="page" from="663" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">RLlib: Abstractions for distributed reinforcement learning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3053" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5506" to="5517" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient GPU spatial-temporal multitasking</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phung</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick Siow Mong</forename><surname>Rupnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="748" to="760" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Wenyan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="553" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">{CheckFreq}: Frequent,{Fine-Grained} {DNN} Checkpointing</title>
		<author>
			<persName><forename type="first">Jayashree</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Chidambaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th USENIX Conference on File and Storage Technologies</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="203" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ray: A distributed framework for emerging {AI} applications</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melih</forename><surname>Elibol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Michael I Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX symposium on operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Naiad: a timely dataflow system</title>
		<author>
			<persName><forename type="first">Derek G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="439" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09332</idno>
		<title level="m">Webgpt: Browser-assisted questionanswering with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PipeDream: generalized pipeline parallelism for DNN training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Nikhil R Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM symposium on operating systems principles</title>
		<meeting>the 27th ACM symposium on operating systems principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient large-scale language model training on gpu clusters using megatronlm</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dynamic resource management for efficient utilization of multitasking GPUs</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-second international conference on architectural support for programming languages and operating systems</title>
		<meeting>the twenty-second international conference on architectural support for programming languages and operating systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="527" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Generic Communication Scheduler for Distributed DNN Training Acceleration</title>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bairen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341301.3359642</idno>
		<ptr target="https://doi.org/10.1145/3341301.3359642" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles<address><addrLine>Huntsville Ontario Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">2021. {Zero-offload}: Democratizing {billion-scale} model training</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 USENIX Annual Technical Conference (USENIX ATC 21</title>
		<imprint>
			<biblScope unit="page" from="551" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The number of partitions of a set</title>
		<author>
			<persName><forename type="first">Gian-Carlo</forename><surname>Rota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Mathematical Monthly</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="498" to="504" />
			<date type="published" when="1964">1964. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Baptiste Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sten</forename><surname>Gloeckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itai</forename><surname>Sootla</surname></persName>
		</author>
		<author>
			<persName><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Sauvestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémy</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artyom</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Kozhevnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Grattafiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12950</idno>
		<title level="m">Code Llama: Open Foundation Models for Code</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Santacroce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.00754</idno>
		<title level="m">Efficient RLHF: Reducing the Memory Usage of PPO</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Grefenstette</forename><surname>Hill Kohli Saxton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01557</idno>
		<title level="m">Analysing Mathematical Reasoning Abilities of Neural Models</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438[cs.LG]</idno>
		<title level="m">High-Dimensional Continuous Control Using Generalized Advantage Estimation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balso</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03300</idno>
		<title level="m">Deepseekmath: Pushing the limits of mathematical reasoning in open language models</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multibillion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.12456[cs.LG]</idno>
		<title level="m">PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">GEAR: a GPU-centric experience replay system for large reinforcement learning models</title>
		<author>
			<persName><forename type="first">Hanjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man-Kit</forename><surname>Sit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congjie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Mai</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="36380" to="36390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Supporting very large models using automatic dataflow graph partitioning</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference</title>
		<meeting>the Fourteenth EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Gemini: Fast failure recovery in distributed training with in-memory checkpoints</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Symposium on Operating Systems Principles</title>
		<meeting>the 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="364" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Simultaneous multikernel GPU: Multitasking throughput processors via fine-grained sharing</title>
		<author>
			<persName><forename type="first">Zhenning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Melhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Childers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international symposium on high performance computer architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="358" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Youshao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fagui</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangchun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11819</idno>
		<title level="m">An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10719</idno>
		<title level="m">Is dpo superior to ppo for llm alignment? a comprehensive study</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.01320</idno>
		<title level="m">DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Orca: A distributed serving system for {Transformer-Based} generative models</title>
		<author>
			<persName><forename type="first">Gyeong-In</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joo</forename><surname>Seong Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geon-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soojeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="521" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">Zhihang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16363</idno>
		<title level="m">LLM Inference Unveiled: Survey and Roofline Model Insights</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05302</idno>
		<title level="m">Rrhf: Rank responses to align language models with human feedback without tears</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Apache spark: a unified engine for big data processing</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><surname>Michael J Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangming</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zherui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization. NL2Code Workshop of ACM KDD 2024</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lansong</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.05965</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Accelerating large-scale distributed neural network training with SPMD parallelism</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lansong</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Symposium on Cloud Computing</title>
		<meeting>the 13th Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="403" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="559" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Improving generalization of alignment with human preferences through group invariant learning</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihan</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11971</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">Yinmin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.09670</idno>
		<title level="m">DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Swift: Expedited Failure Recovery for Large-Scale DNN Training</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangming</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3572848.3577510</idno>
		<ptr target="https://doi.org/10.1145/3572848.3577510" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming<address><addrLine>Montreal, QC, Canada; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="447" to="449" />
		</imprint>
	</monogr>
	<note>PPoPP &apos;23</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
