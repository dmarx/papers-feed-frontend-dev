# HybridFlow: A Flexible and Efficient RLHF Framework

## Abstract

## 

Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53Ã—âˆ¼20.57Ã— throughput improvement when

## Introduction

Large language models (LLMs) such as GPT [[11]](#b10), Llama [[73]](#b72) and Claude [[7]](#b6) have revolutionized various artificial intelligence (AI) applications, ranging from writing [[2]](#b1), searching [[52]](#b51) to coding [[63]](#b62). LLMs are first pre-trained on trillions of tokens from books, websites, etc,. via next-word prediction to accumulate broad knowledge [[11]](#b10). Next, LLMs are trained on domain-specific datasets via supervised fine-tuning (SFT), to be able to follow human instructions [[11]](#b10). Despite the outstanding capabilities of LLMs on natural language tasks after pre-training and SFT, the detrimental and biased contents in the training datasets may still mislead an LLM to generate toxic and undesirable content. Reinforcement Learning from Human Feedback (RLHF) is introduced to further align an LLM to human values, for building helpful and harmless AI applications [[7,](#b6)[55]](#b54).

RLHF is built upon traditional RL algorithms [[4,](#b3)[68,](#b67)[78](#b77)], e.g., Proximal Policy Optimization (PPO) [[68]](#b67) and REIN-FORCE [[78]](#b77). The widely adopted PPO-based RLHF system typically consists of four LLMs [[7,](#b6)[55]](#b54): an actor, a critic, a reference policy network and a reward model. PPO-based RLHF proceeds in iterations, each with three stages: (1) response generation using the actor model with a batch of prompts;

(2) preparation of training data by scoring the generated responses through a single forward pass of the critic, reference policy, and reward models; (3) learning from human preference by updating actor and critic through forward and backward computation. Other RLHF variants [[19,](#b18)[43]](#b42) follow similar stages but involves different numbers of models and data dependencies among the models.

Traditional RL can be modeled as a dataflow [[46]](#b45), which is a directed acyclic graph (DAG): each node in the RL dataflow represents computation of a neural network (e.g., actor or critic network which can be CNN or MLP); each edge denotes data dependency between NN computations (e.g., output of the critic is used as input to actor training [[68]](#b67).) RLHF dataflow is more complex, with more complicated models involved (e.g., LLMs for the actor/critic/reference/reward models), each running distinct computation, and more diverse data dependencies among them (i.e., multicast between distributed model partitions). Training and generation of an LLM in the RLHF dataflow requires distributed computation (e.g., using tensor/pipeline/data parallelism) [[40,](#b39)[71]](#b70). Therefore, each node in the RLHF dataflow is a complex distributed program, corresponding to distributed computation of the respective LLM. Models in different nodes typically use different parallelism strategies as their workloads vary. The edge represents data resharding, which is often a many-tomany multicast. Consequently, Flexible representation and efficient execution of the complex and resource intensive RLHF is imperative.

Traditional RL frameworks such as RLLib [[45]](#b44) and RLLib Flow [[46]](#b45) utilize a hierarchical single-controller paradigm to run RL dataflows. A centralized controller assigns nodes in the dataflow to different processes and coordinates their execution order. Each node process can further spawn more workers to perform computation, again following the singlecontroller paradigm. However, they only provide primitives for data-parallel training and are constrained to neural networks that are at most hundreds of MB in size [[45,](#b44)[46]](#b45). In the RLHF dataflow, each node corresponds to an LLM with up to billions of operators, computed using some complex parallelism. A single-controller paradigm is inefficient due to the substantial overhead of dispatching operators to distributed accelerators [[1,](#b0)[9]](#b8).

Existing RLHF systems adopt a multi-controller paradigm to manage intra-node computation and inter-node data resharding [[17,](#b16)[30,](#b29)[80]](#b79). Each controller independently manages the computation of one device and uses multiple point-topoint operations to coordinate data dependencies between different nodes. This multi-controller paradigm introduces negligible dispatch overhead when performing LLM computation (detailed in Â§2.2).

However, without central control, it is inflexible to implement various RLHF dataflow, as modifying a single node to adapt to different data dependencies requires changing all dependent nodes' implementation, hindering code reuse.

To address these limitations, we propose HybridFlow, a flexible and efficient RLHF framework to easily represent and execute diverse RLHF dataflows, attaining high throughput.

Our key observation is that utilizing the single-controller paradigm on the inter-node level enables flexible expression of various data dependencies and easy coordination of inter-node data resharding with minimal overhead, while integrating the multi-controller paradigm within intra-node computation enhances computation efficiency substantially. We advocate a hierarchical hybrid programming model to generate RLHF dataflows. At the node level, multiple model classes are provided that encapsulate distributed computation (training, inference and generation) of different LLMs in the dataflow into primitive APIs. These APIs can seamlessly support various parallelism strategies from the existing LLM frameworks, including 3D parallelism [[71]](#b70), ZeRO [[59]](#b58), and PyTorch FSDP [[57]](#b56)), and perform distributed computation under the multi-controller paradigm. Among the nodes, a set of transfer protocols are designed to hide the complexity of data resharding from users, as coordinated by a single controller. This programming model abstracts away the complexity of distributed computing, allowing users to implement an RLHF dataflow in a few lines of code and run RLHF through a single process of the single controller. It also effectively decouples intra-node computation and inter-node data transfer, allowing independent optimization of each model without changing the code of other models in the dataflow.

Training and generation of the actor model represent major computation in the RLHF dataflow. We further design a 3D-HybridEngine to enable efficient execution of training and generation of the actor model, introducing zero memory redundancy and significantly reduced communication overhead during model parameter resharding between the training and generation stages. Our hybrid programming model also facilitates flexible placement of models onto the same or different sets of GPU devices. This allows us to provide an effective algorithm to optimize GPU allocation and placement of the models, with various model sizes and distinct workloads, for any RLHF dataflow. Our contributions in designing HybridFlow are summarized as follows:

â€¢ We propose a hierarchical hybrid programming model for conveniently building the RLHF dataflow. This programming model enables efficient distributed execution of intra-node computation and flexible inter-node data resharding and transfer, for various RLHF algorithms ( Â§4).

â€¢ We design a 3D-HybridEngine that executes training and generation of the actor model with high computation efficiency and zero-redundancy transition between the training stage and the generation stage ( Â§5).

â€¢ We devise an effective mapping algorithm to automatically identify optimized GPU allocation and placement of each node (model) in the RLHF dataflow ( Â§6).

â€¢ We conduct extensive experiments comparing HybridFlow with state-of-the-art RLHF systems [[17,](#b16)[30,](#b29)[82]](#b81) under various We have open-sourced HybridFlow and believe that Hy-bridFlow can boost future RLHF research and development.

2 Background and Motivation 2.1 Reinforcement Learning from Human Feedback RLHF Workflow. RLHF aligns the linguistic space of LLMs with human values, using a set of human-ranked candidates of given prompts [[7,](#b6)[19,](#b18)[41,](#b40)[43,](#b42)[55,](#b54)[70,](#b69)[91](#b90)]. An RLHF system typically consists of multiple models, e.g., an actor, a critic, a reference policy, and one or multiple reward models. The actor and the reference are each pre-trained/fined-tuned LLM (i.e., the LLM that is undergoing RLHF). The critic and reward models can be different LLMs fine-tuned on the human preference dataset, with the language modeling head replaced by a scalar output head [[7,](#b6)[55]](#b54). The RLHF workflow can be decomposed into 3 stages (Figure [1](#fig_0)) and we take PPO as an example:

â€¢Stage 1 (Generation): The actor produces responses from a batch of prompts using auto-regressive generation.

â€¢Stage 2 (Preparation): Using prompts and generated responses, the critic computes their values [[66,](#b65)[68]](#b67), the reference policy computes their reference log probabilities, and the reward model computes their rewards [[7,](#b6)[55]](#b54), all via a single pass of forward computation of the respective model.

## â€¢Stage 3 (Learning/Training):

The actor and the critic are updated via Adam [[38]](#b37), using the batch of data produced by previous stages and the loss function [[55]](#b54).

Other RLHF algorithms largely follow the 3-stage workflow as well (Figure [1](#fig_0)(b)(c)). Safe-RLHF [[19]](#b18) introduces an auxiliary pretrain loss following PPO-ptx [[55]](#b54) and includes an additional cost model to fit human preferences and safety labels simultaneously. ReMax [[43]](#b42) requires an additional generation pass for variance reduction and eliminates the critic model in the dataflow. Researchers are actively exploring novel RLHF algorithms [[41,](#b40)[70,](#b69)[91]](#b90) and integrating traditional RL methods into RLHF domains [[37]](#b36). These variances necessitate a flexible representation of the RLHF dataflow graph to accommodate diverse algorithmic requirements. Parallelism Strategies. LLMs are trained and served with data, pipeline, and tensor parallelism [[36,](#b35)[40,](#b39)[54]](#b53). With data parallelism (DP), the input data is split into multiple subsets; each subset is processed by a separate device (e.g., a GPU) [[69]](#b68). ZeRO [[59]](#b58) is a memory-optimized solution for DP training, progressively sharding optimizer states, gradients, and model parameters across GPUs. Pipeline parallelism (PP) [[32,](#b31)[53]](#b52) and tensor parallelism (TP) [[71]](#b70) distribute model parameters, gradients and optimizer states across multiple GPUs. Modern distributed training frameworks like Megatron-LM [[71]](#b70) and MegaScale [[36]](#b35) utilize 3D parallelism or PTD parallelism [[54]](#b53), where P, T, D stand for PP, TP, DP, respectively. In 3D parallelism, PP size represents the number of pipeline stages in model training, TP size refers to the number of shards that a tensor is partitioned into, and DP size is the number of model replicas. LLM serving systems employ 3D parallelism similar to training while only model parameters and KVCache are sharded [[16,](#b15)[29,](#b28)[40]](#b39). LLM models in the RLHF dataflow may perform distinct computations, including training (one forward pass, one backward pass and model update), inference (one forward pass) and generation (auto-regressive generation with multiple forward passes). In particular, training and generation are performed on the actor model, training and inference on the critic, and inference on reference policy and reward models. Distinct parallel strategies can be applied to different models for varied computations to achieve optimal throughput.

## Programming Model for Distributed ML

Single-Controller. It employs a centralized controller to manage the overall execution flow of the distributed program. With centralized control logic, users can build core functionalities of the dataflow as a single process (Figure [2](#fig_1)(b)), while the controller automatically generates distributed workers to carry out the computation. With a global view of the hardware and dataflow graph, the single-controller paradigm allows flexible and optimized resource mapping and execution order coordination among dataflow tasks. However, coordination messages are passed from the controller to all workers, incurring significant dispatch overhead when executing expansive dataflow graphs on large clusters [[1,](#b0)[9]](#b8). Multi-Controller. Each device (aka worker) has its own controller. State-of-the-art distributed LLM training and serving systems adopt the multi-controller paradigm, due to its scalability and low dispatch overhead (control messaging largely passed from CPU to GPU over fast PCIe links) [[36,](#b35)[40,](#b39)[60,](#b59)[71]](#b70). As shown in the example that employs multi-controller RLHF implementation in Figure [2](#fig_1)(a), a separate program is run for each model, and all workers of one model execute the same program. Each worker only possesses a local view of the system state and requires point-to-point communication between two models (blue code and arrows) to coordinate model execution order. To implement an RLHF workflow in the multi-controller architecture, a user must intricately integrate the code for collective communication, computation, and point-to-point data transfer in the program run at each device. This leads to deeply nested code of computation and data transfer, challenging to develop, maintain, and optimize. In Figure [2](#fig_1)(a), each model performs local computation and all_gather operations (black code), while the actor model must explicitly manage send operations to the critic and reward models, and the latter must correspondingly implement receive operations at precise points in their program.

## RLHF Characteristics

Heterogeneous model workloads. The actor, critic, reference and reward models in RLHF may execute training, inference or generation at different stages, with different memory footprint and computation demand. For reference policy and reward models, only their model parameters need to be stored in GPU memory, as they perform only the forward pass computation. For the actor and the critic, their model parameters, gradients, and optimizer states must be stored as they undergo model training. Moreover, a small actor model (e.g., a 7B pre-trained/fine-tuned LLM) can be paired with larger critic and reward models (e.g., 70B LLMs) in RLHF for better alignment [[7]](#b6). Given such heterogeneity, different parallelism strategies and tailored optimizations are needed for running each model during RLHF. Unbalanced computation between actor training and generation. In the RLHF dataflow, training and generation of the actor model are represented by two nodes (Figure [1](#fig_0)),  3. Dataflow execution given a model placement plan. Blocks with numbers represent GPUs. In dashed boxes, the models are placed on different sets of devices and can be concurrently computed. Reference model (blue) and reward model (green) are colocated on the same set of GPUs and executed sequentially. which often render majority of the workload in each RLHF iteration (e.g., 58.9% of total RLHF time with HybridFlow). Actor training is computation bound [24], often requiring a larger model-parallel (MP) size (i.e., the number of partitions the model is partitioned into) and distributing the workload to more GPUs, e.g., 8 partitions of a 7B model on 8 GPUs. Using the same parallelism strategy (e.g., the same MP size) for generation can lead to underutilization of GPU computation resources due to its memory-bound nature [40]. Previous studies show that combining a larger DP size with a smaller MP size (hybrid data and model parallelism), e.g., partition a 7B model into two and replicate it four times on 8 GPUs, can improve the generation throughput [44, 92]. Although using different parallelism strategies for actor training and generation may optimize throughput in both stages, resharding the actor model weights at runtime between the two stages can incur significant communication and memory overhead. For example, aligning a 70B actor model requires transferring 140GB of model weights from training to generation per RLHF iteration, taking up to 36.4% of an iteration time when the two stages are on different devices [30]. Diverse model placement requirements. Strategic device placement of models in the RLHF dataflow is necessary, according to computation workloads and data dependencies of the models. Figure 3 gives an example model placement plan and the corresponding RLHF execution flow. Models placed on different sets of devices can be executed in parallel if no data dependencies exist. Models placed on the same set of GPUs, referred to as colocated models, share the GPU memory and are executed sequentially in a time-sharing manner, as out-of-memory (OOM) error may easily happen if colocated LLMs execute concurrently.

We observe a compromise: placing models on different devices permits parallel processing but may inevitably lead to some GPU idle time, given staged model execution in RLHF. In Figure [3](#), actor and critic are placed separately, performing training in parallel, but incurring 1/3 of their GPU time being idle, during other RLHF stages. Supporting various  Support various execution patterns placement strategies and maximizing device utilization are crucial for optimizing RLHF performance at any model size and cluster scale.

## Limitations of existing RLHF systems

Inflexible support for various RLHF dataflow graphs. Existing RLHF systems adopt the multi-controller paradigm for dataflow implementation [[17,](#b16)[30,](#b29)[80,](#b79)[82]](#b81). To implement various RLHF algorithms, a user must navigate and manage code that mixes collective communication, model computation (potentially using various distributed training/serving frameworks), and point-to-point data transfer. This code structure lacks modularity/function encapsulation, making the RLHF systems tightly coupled with specific LLM training and serving frameworks. Consequently, a user needs to implement and optimize different RLHF dataflows case-bycase [[46]](#b45), hindering code reuse and increasing the risk of making mistakes. Existing RLHF frameworks only support the PPO algorithm. In addition, limited parallel strategies are supported due to implementation complexity. For example, to incorporate 3D parallelism for LLM training and generation in DeepSpeed-Chat [[82]](#b81), one may have to re-implement the whole system due to the mixed code structure. Inefficient RLHF execution. Table [1](#tab_1) summarizes parallelism strategies, model placement, and execution patterns adopted by the existing RLHF systems. DeepSpeed-Chat [[82]](#b81) and OpenRLHF [[30]](#b29) adopt ZeRO-3 for actor training and TP for actor generation. OpenRLHF uses different copies of the actor model on different devices for training and generation, incurring redundant memory usage and frequent weight synchronization among devices. DeepSpeed-Chat maintains the same copy of actor model on the same set of devices for training and generation, and reshards model weights between training and generation (due to different parallelisms used in the two stages), which may still incur substantial memory and communication overhead for large models (detailed in Â§5.4). NeMo-Aligner [[17]](#b16) uses the same 3D parallelism configurations in actor training and generation, experiencing low generation throughput ( Â§8.4).

Existing RLHF frameworks are limited to one model placement plan and hence one RLHF execution pattern, as shown in Table [1](#tab_1). Implementing a different placement is difficult, requiring changing the inner logic of model initialization and inter-node data transfer as highlighted in blue in Figure [2](#fig_1). OpenRLHF and NeMo-Aligner allow concurrent model computation in the preparation and learning stages; in the generation stage, models except the actor are idle, wasting the GPUs they occupy. DeepSpeed-Chat colocates all models on the same set of devices, and each device runs each model sequentially according to the RLHF dataflow. With unbalanced workloads among the models, such a placement can be inefficient in resource utilization (evaluated in Â§8.3).

## Design Considerations

To tackle limitations of existing systems, the key question is -How to design a flexible and efficient programming model to implement RLHF dataflow? A single-controller design is particularly advantageous at the inter-node level due to its flexibility in coordinating data transfer, execution order, and resource virtualization among distributed computation of different models [[9,](#b8)[50]](#b49). The RLHF dataflow graph typically consists of only a few nodes. Dispatching control messages to different nodes from the single-controller incurs negligible overhead as compared to distributed computation required for nodes (models) in the dataflow. The multi-controller paradigm, known for its low latency in dispatching operators to accelerators [[20]](#b19), can be leveraged in distributed computation of each model. With these insights, we propose a hierarchical hybrid programming model for RLHF dataflow implementation. Our key design principle is to combine single-controller and multi-controller paradigms in a hybrid manner. This design ensures flexible expression and efficient execution of RLHF dataflow, maintaining low control overhead at both inter-node and intra-node levels. As shown in Figure [2](#fig_1)(b), this paradigm decouples intra-node distributed computation and inter-node data transfer, allowing each model to focus solely on local computation without managing inter-node communication. The workflow of our RLHF system goes as follows. A user provides the following inputs to start the RLHF system: (i) model specifications, including the architecture and size of the actor/critic/reference policy/reward models in the RLHF dataflow; (ii) device placement of the models in the dataflow, as obtained by running the auto-mapping algorithm under given GPU cluster configurations; (iii) parallelism strategy for running each model in each stage, e.g., a tuple of (p, t, d) for 3D parallelism, where p, t, d represent PP size, TP size and DP size, respectively. The single controller program takes these inputs to initialize models in the RLHF dataflow and virtualized resource pool, dispatches operations/models to devices according to the placement plan, and invokes functions run by the multiple controllers on devices to carry out distributed computation of each model.

## HybridFlow Overview

The multi-controller program implements the Parallel-Worker class: it constructs parallel groups of each model among allocated devices according to its parallelism strategies, invokes the 3D-HybridEngine for actor training and generation, and can be integrated seamlessly with existing LLM engines [[40,](#b39)[57,](#b56)[60,](#b59)[71]](#b70) for training, inference and generation of other models. The transfer protocols are coordinated by the single controller program to support resharding of data (including prompts, responses, and other model outputs in RLHF) between models with distinct parallelism strategies. The data resharding of the actor between training and generation is handled by 3D-HybridEngine.  Inter-node: unifying data resharding implementation between models. Many-to-many multicast is involved for data transfer between models employing different parallelism strategies on different devices. We unify this data transfer implementation by associating each operation in each model class with a transfer protocol, using @register. Each transfer protocol consists of a collect function and a distribute function, to aggregate output data and distribute input data according to the parallelism strategy of each model. In the example in Figure [5](#fig_7)(a), update_actor operation is registered to transfer protocol 3D_PROTO, as 3D parallelism is used for actor training. In 3D_PROTO, the collect function gathers all the output data of corresponding model function (e.g., the loss scalar return from the update_actor) in each DP group to the single controller, and the distribute function distributes the input data to the registered function (e.g., advantages for the update_actor) to each DP group. Data resharding is enabled using the source model's output collect function and the destination model's input distribute function. Figure [5](#fig_7)(b) illustrates data resharding between the actor (generation) and the critic (inference), where computation of the models adopts different 3D parallelism strategies. The single controller gathers data futures using the collect function in 3D_PROTO of actor (steps 1 â—‹-3 â—‹) and sends it to critic (step 4 â—‹); critic distributes the received data futures to each DP group using the distribute function in its 3D_PROTO (step 5 â—‹). Then remote data is retrieved from actor to critic, with each of critic's GPUs only fetching the required local batch of the actor's output data according to its DP rank (step 6 â—‹). The actual data transfer only occurs between GPUs, avoiding any central bottleneck.


our programming model, HybridFlow is flexible in supporting diverse distributed execution patterns without any code change of the RLHF algorithm (Figure [6](#)).

## Implementation of different RLHF algorithms

Our APIs enable streamlined development of various RLHF algorithms (dataflows). Users can implement an RLHF algorithm in a few lines of code as a single process program to run on the single controller, that involves a sequence of primitive API calls to invoke distributed computation of models. Examples of PPO, ReMax, and Safe-RLHF are given in Figure [6](#). PPO can be implemented in just 8 lines by invoking model operations including compute_values and generate_sequences, which are executed under the multicontroller paradigm on multiple GPUs. To adapt to Safe-RLHF which integrates an additional cost model to evaluate safety preferences and the pre-taining loss for actor, only 5 more lines of code are added on top of PPO implementation.

To adapt to ReMax, one additional call to actor generation is needed, and the critic-related code can be removed.

Achieving flexible. This flexibility of extension is crucial for researchers to explore different RLHF algorithms: they can reuse distributed computation encapsulated in each model class and simply adjust the code for numerical computations according to specific algorithms, such as GAE [[67]](#b66) and KL divergence in compute_advantage and loss functions of actor and critic. The streamlined development can be attributed to the hybrid programming model. Our modular API design simplifies development, facilitates extensive code reuse, and enables directly incorporating the codebase of existing LLM training/serving frameworks. It also decouples model computation and data transfer among models. Any change in the distributed frameworks does not affect the code of the RLHF algorithm (Figure [6](#)), enabling individualized optimization for each model's execution ( Â§5). Flexible placement of models with diverse workloads is supported, enabling optimized mapping of RLHF dataflow onto various devices ( Â§6). q n e P N v n C R 7 0 M S C h q K q e 6 a 7 / E Q K j Y 7 z b a 2 s r q 1 v b B a 2 i t s 7 u 3 s l e / + g q e N U M d 5 g s Y x V 2 6 e a S x H x B g q U v J 0 o T k N f 8 p Y / u p n 6 r Q e u t I i j e x w n 3 A v p I B J 9 w S g a q W e X u s g f k Q n F J A + y y 0 n P L j s V Z w a y T N y q n e P N v n C R 7 0 M S C h q K q e 6 a 7 / E Q K j Y 7 z b a 2 s r q 1 v b B a 2 i t s 7 u 3 s l e / + g q e N U M d 5 g s Y x V 2 6 e a S x H x B g q U v J 0 o T k N f 8 p Y / u p n 6 r Q e u t I i j e x w n 3 A v p I B J 9 w S g a q W e X u s g f k Q n F J A + y i 0 n P L j s V Z w a y T N y  

$c l C F H v W d / d Y O Y p S G P k E m q d c d 1 E v Q y q l C Y J y f F b q p 5 Q t m I D n j H 0 I i G X H v Z b P E J O T F K Q P q x M h U h m a m / J z I a a j 0 O f d M Z U h z q R W 8 q / u d 1 U u x f e 5 m I k h R 5 x O Y f 9 V N J M C b T F E g g F G c o x 4 Z Q p o T Z l b A h V Z S h y a p o Q n A X T 1 4 m z f O K W 6 1$$c l C F H v W d / d Y O Y p S G P k E m q d c d 1 E v Q y q l C Y J y f F b q p 5 Q t m I D n j H 0 I i G X H v Z b P E J O T F K Q P q x M h U h m a m / J z I a a j 0 O f d M Z U h z q R W 8 q / u d 1 U u x f e 5 m I k h R 5 x O Y f 9 V N J M C b T F E g g F G c o x 4 Z Q p o T Z l b A h V Z S h y a p o Q n A X T 1 4 m z f O K W 6 1$
## 3D-HybridEngine

We design the 3D-HybridEngine to support efficient training and generation of the actor model, targeting significant RLHF throughput improvement.

## Parallel Groups

To eliminate redundant actor model copies, we advocate deploying actor training and generation stages on the same set of devices, ğ‘ ğ‘ GPUs allocated to the actor, and execute them sequentially on the same copy of actor model weights. Nonetheless, actor training and generation may well adopt different 3D parallelism strategies, i.e., the generation stage typically requires smaller TP and PP sizes but a larger DP size, than the training stage ( Â§2.3). 3D-HybridEngine enables efficient model parameter resharding between actor training and generation across the same set of devices in this context.

Let ğ‘-ğ‘¡-ğ‘‘ denote 3D parallel groups constructed for actor training, corresponding to the set of GPUs to host ğ‘ pipeline stages, ğ‘¡ tensor shards, and ğ‘‘ model replicas [[54]](#b53). 3D-HybridEngine builds different parallel groups for actor training and generation, according to their different 3D parallelism strategies, respectively. We use ğ‘ ğ‘” , ğ‘¡ ğ‘” , and ğ‘‘ ğ‘” to denote the size of generation pipeline parallel group, generation tensor parallel group, and micro data parallel group, respectively, in the generation stage. ğ‘‘ ğ‘” indicates the ratio of model replica number in generation over that in training, i.e., each DP replica in training becomes ğ‘‘ ğ‘” micro DP replicas, to process ğ‘‘ ğ‘” microbatches of prompts and responses. We have

$ğ‘ ğ‘ =ğ‘Ã—ğ‘¡Ã—ğ‘‘=ğ‘ ğ‘” Ã—ğ‘¡ ğ‘” Ã—ğ‘‘ ğ‘” Ã—ğ‘‘ such that ğ‘‘ ğ‘” = ğ‘ğ‘¡ ğ‘ ğ‘” ğ‘¡ ğ‘” .$The micro DP groups are employed exclusively in actor generation stage to render a larger DP size for full device utilization. The generation parallel groups are denoted by ğ‘ ğ‘” -ğ‘¡ ğ‘” -ğ‘‘ ğ‘” -ğ‘‘.

## 3D-HybridEngine Workflow

Between actor training in iteration ğ‘– of RLHF and actor generation in iteration ğ‘– + 1, the actor model parameters need to be resharded and prompts data to be distributed, following the parallel group configurations in the two stages. In iteration ğ‘– + 1 of RLHF, 3D-HybridEngine gathers the actor 

$G1 G2 G3 G4 G5 G6 G7 G8 G2 G3 G4 G5 G6 G7 G8 G1 G2 G3 G4 G5 G6 G7 G8 G1 G2 G4 G3 All-Gather complete weights G5 G6 G7 G8 G1 G3 G2 G4 G5 G7 G6 G8$All-Gather within Micro-DP groups 

## Zero redundancy model resharding

Parallel grouping methods in 3D parallelism are typically as follows: PP and TP groups are formed by assigning consecutive ranks to pipeline stages and tensor shards, respectively; DP groups are constructed by selecting ranks at regular intervals, determined by the product of PP size and TP size. In Figure [8(a)](#fig_14), actor training uses 3D parallel groups, 1-4-2: there is one PP group for all GPUs (for illustration clarify); the TP groups are [G1, G2, G3, G4], [G5, G6, G7, G8], and the DP groups are [G1, G5], [G2, G6], [G3, G7], [G4, G8]. Suppose the same parallel grouping methods are used but with different parallel sizes, e.g., 1-2-2-2 for generation in Figure [8(a)](#fig_14). During the transition from training to generation, 3D-HybridEngine applies all-gather operations among the model parallel groups to aggregate all parameters, and then retain only a subset of model weights on each device for its generation, according to the parallel groups the device belongs to. On some GPUs (e.g., G2, G3, G6, G7), there is no overlap between training and generation model weights, and separate memory is needed to maintain weights for subsequent training as well (grey boxes in Figure [8(a)](#fig_14)).We call  

## Transition overhead

In Table [2](#tab_2), we compare communication overhead and memory footprint during the transition between training and generation stages, among different actor engine designs. We assume model size of the actor is ğ‘€ and ğ‘ ğ‘ GPUs are used for its training and generation. The actor engine in DeepSpeed-Chat conducts an all-gather operation across all GPUs during transition; HybridFlow-V performs this all-gather within training TP and PP groups. The communication volumes for these operations are ğ‘ ğ‘ -1

$ğ‘ ğ‘ ğ‘€ = ğ‘¡ğ‘ğ‘‘ -1$ğ‘¡ğ‘ğ‘‘ ğ‘€ for DeepSpeed-Chat and ğ‘¡ğ‘ -1 ğ‘¡ğ‘ ğ‘€ for HybridFlow-V, calculated following [[13]](#b12).

Both engines aggregate all model parameters in each GPU's memory before subsequently partitioning model states according to the generation parallel groups, resulting in a peak memory usage of model parameters ğ‘€. As they cannot reuse training weights during generation on some GPUs, training weights need to be maintained on them, amounting to 1 ğ‘¡ğ‘ğ‘‘ and 1  ğ‘¡ğ‘ redundant memory consumption, respectively. With our parallel grouping method for the generation stage, HybridFlow confines the all-gather operation within each micro DP group. The communication overhead is reduced to

$ğ‘‘ ğ‘” -1 ğ‘¡ğ‘ ğ‘€ = ğ‘¡ğ‘ -ğ‘¡ ğ‘” ğ‘ ğ‘”$ğ‘¡ ğ‘” ğ‘ ğ‘” ğ‘¡ğ‘ ğ‘€. Each GPU only needs to collect remote parameters within its micro DP group and can reuse the training weights in generation. Therefore, the peak memory usage of model parameters in HybridFlow precisely matches the model partition size on each GPU in generation, eliminating any redundancy in GPU memory usage.

## Auto Device Mapping

Our hybrid programming model requires users to input the following configurations, which are referred to as a mapping of the RLHF dataflow to the given devices: (a) device placement of the models in the dataflow; (b) the corresponding parallelism strategy for running each model in each stage.

We provide an efficient algorithm (Algorithm 1) for users to identify the optimized mapping of executing the RLHF dataflow on a given cluster of devices, that minimizes the end-to-end latency of each RLHF iteration. Given a dataflow ğ·, we first explore all possible placement plans P for the models in the given cluster (Line 3). For example, the PPO algorithm involves four models, resulting in 15 possible placements (from the Bell partition problem [[10,](#b9)[62]](#b61)), ranging from a completely standalone placement where all models are placed on different devices (e.g., OpenRLHF's placement) to colocating all models on the same set of devices (e.g., DeepSpeed-Chat's placement). We refer to colocated models on the same set of GPUs as a colocated set. Models in a colocated set can employ different parallelism strategies across the same set of GPUs. We identify the smallest number of GPUs to be allocated to each of the colocated model sets, ğ´ ğ‘šğ‘–ğ‘› , based on memory consumption of colocated models, ensuring no out-of-memory errors (Line 9).

Next, starting from the minimal GPU allocation in ğ´ ğ‘šğ‘–ğ‘› , we enumerate all feasible device allocations to each colocated model set (Lines 10-12). Given device allocation ğ´ to the colocated set and computation workload ğ‘Š of models in the set, we explore optimized parallelism strategies for each model in the auto_parallel module, that minimizes model execution latency. The workload ğ‘Š includes input and output shapes and computation (training, inference or generation) of each model. In auto_parallel, we utilize a simulator module simu to estimate the latency of different parallel strategies, following previous research [[42,](#b41)[84,](#b83)[90,](#b89)[92]](#b91) (outline in Appendix. C).

The d_cost module estimates the end-to-end latency of the RLHF dataflow under given model placement and parallelism strategies, by iterating through all stages in the dataflow graph and summing up latencies of all stages (Lines 17, 25). For models in the same colocated set and involving computation in the same stage (such as actor and critic both performing model update in RLHF training stage), their execution latencies are summed up (Line 32). For models in different colocated sets, their execution within the same stage Algorithm 1 Device Mapping for an RLHF Dataflow

1: Input: RLHF dataflow graph ğ·, LLMs in RLHF dataflow ğ¿=[ğ‘™ 1 , ğ‘™ 2 , . . . , ğ‘™ ğ‘˜ ], workload ğ‘Š of LLMs in RLHF dataflow, total # of GPUs ğ‘ , memory capacity per GPU ğ‘„ 2: Output: device mapping of models in RLHF dataflow 3: P â† get_placements(ğ·, ğ¿, ğ‘ ) 4: ğ¶ * â† âˆ 5: ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘šğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘” â† âˆ… 6: for all ğ‘ğ‘™ğ‘š âˆˆ P do 7: ğ¶ ğ‘ğ‘™ğ‘š â† âˆ 8: ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘™ğ‘š_ğ‘ğ‘™ğ‘™ğ‘œğ‘ â† âˆ… 9: ğ´ ğ‘šğ‘–ğ‘› â† get_min_alloc(ğ‘ğ‘™ğ‘š, ğ‘„, ğ‘ ) 10: for all ğ´ âˆˆ enum_alloc(ğ‘ , ğ´ ğ‘šğ‘–ğ‘› ) do 11: ğ¿ â† [] 12: for all set âˆˆ ğ‘ğ‘™ğ‘š do 13: for all ğ‘™ âˆˆ set do 14: ğ‘™ â† auto_parallel(ğ´, ğ´ ğ‘šğ‘–ğ‘› , ğ‘™,ğ‘Š ) 15: ğ¿.append( ğ‘™) 16: ğ‘ğ‘™ğ‘š.update( ğ¿) 17: ğ¶ ğ‘ğ‘™ğ‘™ğ‘œğ‘ â† d_cost(ğ·, ğ‘ğ‘™ğ‘š,ğ‘Š ) 18: if ğ¶ ğ‘ğ‘™ğ‘™ğ‘œğ‘ < ğ¶ ğ‘ğ‘™ğ‘š then 19: ğ¶ ğ‘ğ‘™ğ‘š â† ğ¶ ğ‘ğ‘™ğ‘™ğ‘œğ‘ 20: ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘™ğ‘š_ğ‘ğ‘™ğ‘™ğ‘œğ‘ â† (ğ‘ğ‘™ğ‘š, ğ´) 21: if ğ¶ ğ‘ğ‘™ğ‘š < ğ¶ * then 22: ğ¶ * â† ğ¶ ğ‘ğ‘™ğ‘š 23: ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘šğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘” â† ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘™ğ‘š_ğ‘ğ‘™ğ‘™ğ‘œğ‘ 24: return ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘šğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘” 25: Procedure d_cost(ğ·, ğ‘ğ‘™ğ‘š, ğ‘Š ): 26: ğ‘  â† number of stages in ğ· for all ğ‘™ âˆˆ set do 32:

$ğ‘ ğ‘” [ğ‘–] â† ğ‘ ğ‘” [ğ‘–] + simu( ğ‘™,ğ‘Š [ğ‘–]) 33: ğ‘ [ğ‘–] â† ğ‘šğ‘ğ‘¥ {ğ‘ [ğ‘–], ğ‘ ğ‘” [ğ‘–]} 34:$return sum(ğ‘) can be parallelized, and the latency of the stage is determined by the maximum execution time among different sets (Line 33). We identify the best device placement of the models with their corresponding parallelism strategies, achieving minimal execution time per RLHF iteration (Lines 18-23).

The complexity of Algorithm 1 is ğ‘‚ ( (ğ‘ -1)! (ğ‘˜ -1)!(ğ‘ -ğ‘˜ )! ), where ğ‘˜ is the number of models in the dataflow and ğ‘ is the total number of devices to run the dataflow. This is the worst-case complexity for enumerating all possible device allocations for a placement strategy (i.e., the standalone placement), calculated by assigning ğ‘ devices to ğ‘˜ models (known as the integer partition problem [[6]](#b5)). For better efficiency, we cache parallelism strategies identified for each model on a number of devices ğ´, to eliminate redundant searches for the same parallelism strategies when the model is placed on different sets of ğ´ GPUs in different placement strategies.

Though we assume ğ‘ homogeneous GPUs when running the auto mapping algorithm, Algorithm 1 can be readily extended for optimizing model mapping over heterogeneous devices, by considering heterogeneous devices in simu and auto_parallel modules [[88]](#b87).

## Implementation

HybridFlow is implemented in around 12k lines of Python code (LoC). Hybrid programming model. The hierarchical APIs are implemented with 1.8k LoC. The centralized single controller is built on top of Ray [[50]](#b49) and uses Remote Process Calls (RPC) to coordinate the execution order of different models and transfer data between models following the dataflow. These intermediate data are stored in TensorDict [[57]](#b56). In our multi-controller paradigm for distributed computation, each model function runs on a separate process across various devices, with control messages relayed from each controller's CPU process to the corresponding GPU. Our implementation supports Megatron-LM, PyTorch FSDP, and DeepSpeed as the LLM training and inference engines, and vLLM for autoregressive generation. In vLLM, we replace the centralized KVCache manager with a distributed manager to align with the multi-controller paradigm. 3D-HybridEngine. Its main logic is implemented with 2.4k LoC on top of Megatron-LM and vLLM. We store actor model weights for training and generation stages on separate memory buffers, offload generation weights to the CPU memory during training, reload generation weights back to GPU memory during the transition, and use both buffers in generation. We use NCCL communication primitives [[35]](#b34) to collect and concatenate model parameters in each micro DP group during the transition between training and generation. We offload KVCache to CPU memory after generation and reload it back to GPU in the next iteration. Auto-Mapping Algorithm is implemented with 1.9k LoC, together with three simulators for training, inference, and generation workloads. The algorithm is run before starting the RLHF dataflow on CPU, to generate device mapping and parallelism strategies for dataflow initialization.

## Evaluation

## Experimental Setup

Testbed. We deploy HybridFlow on a cluster of 16 machines (128 GPUs). Each machine is equipped with 8 NVIDIA A100-80GB GPUs inter-connected with 600GB/s NVLink. The inter-machine bandwidth is 200Gbps. Our experiments use the following software versions: CUDA12.1, PyTorch 2.1.2, Megatron-core 0.6.0, NCCL 2.18.1, and vLLM 0.3.1. Models and RLHF algorithms. We run the RLHF dataflow (Figure [1](#fig_0)) of PPO [[68]](#b67), ReMax [[43]](#b42) and Safe-RLHF [[19]](#b18) algorithms. PPO is one of the most popular algorithms for RLHF [[7,](#b6)[55]](#b54), consisting of actor, critic, reference policy, and Datasets and hyperparameters. We perform RLHF on "Dahoas/ful-hh-rlhf" dataset [[7]](#b6) of HuggingFace, which is widely used for LLM alignment [[64,](#b63)[85]](#b84). As the baseline systems may not incorporate continuous-batching optimization [[83]](#b82) during generation, for a fair comparison, we enforce the same length on all responses to be generated. In each experiment, the input prompt length and the output response length are both 1024 and the global batch size of input prompts to the actor model is 1024. The number of PPO epochs is 1 and the number of PPO update iterations per epoch is 8, aligning with previous RLHF research [[31,](#b30)[55,](#b54)[81]](#b80). [Figures 9,](#)[10,](#b9)[and 11](#b10) show RLHF throughput when running PPO, ReMax, and Safe-RLHF respectively. The actor, critic, reference, and reward models in this set of experiments are of the same size, following previous practice [[7,](#b6)[55,](#b54)[82]](#b81). The number of GPUs used in experiments of different model sizes ranges from the smallest number of GPUs to run RLHF without OOM to 128 GPUs. We do not enable offloading optimizer states [[61]](#b60) in the experiments for fair comparison. Overall performance. We observe that HybridFlow consistently outperforms the baselines across all model scales. In   This is mainly because HybridFlow effectively executes generation, inference, and training in all RLHF stages by sharding the models with different parallelism strategies to fit various computation workloads. HybridFlow achieves the highest average speedup of 9.64Ã— when training 70B models, as Hy-bridFlow reduces the transition overhead by up to 71.2% and 89.1% compared to DeepSpeed-Chat and OpenRLHF, which also incurs large inter-machine communication when training with ZeRO-3. Due to the lack of KVCache in generation engine, NeMo-Aligner's main performance bottleneck lies in the generation stage, which accounts for up to 81.2% of its RLHF iteration time. Similar results can be observed in Figures 10, 11 validating the efficiency of HybridFlow on running various RLHF algorithms. Scalability. HybridFlow achieves at least 2.09Ã— speedup on 8 GPUs. With increasing GPUs, the strong scaling efficiency of HybridFlow on various model scales is 66.8%, computed by dividing throughput in largest scale throughput in smallest scale by max. # of GPUs min. # of GPUs [5], averaging over three algorithms and all model scales. Scaling to a large number of GPUs with a fixed global batch size results in smaller local batch sizes for each worker, potentially causing GPU underutilization. Running 7B models on 128 GPUs, HybridFlow still outperforms the best baseline OpenRLHF for 1.68Ã—, 1.53Ã—, and 1.71Ã— on PPO, ReMax, and Safe-RLHF respectively. This can be attributed to Hy-bridFlow's ability to adapt the best placement strategies for different models and cluster sizes to minimize RLHF time. OpenRLHF performs better in a larger GPU cluster but less efficiently on smaller ones. 8.3 Model Placement In this experiment, we implement various model placements of the PPO algorithm in HybridFlow, under the same model and cluster settings as in Sec. 8.2: (i) colocate, the placement strategy in DeepSpeed-Chat; (ii) standalone, that in Open-RLHF and; (iii) split, NeMo-Aligner's colocation placement (actor and reference policy on the same set of devices and critic and reward model on another); (iv) hybridflow, the optimized placement obtained by Algorithm 1. Comparison of different model placements. Figure 12 reveals that optimized placement of HybridFlow under different numbers of GPUs varies. From 16 to 64 GPUs, colocating all models on the same set of devices yields the best performance. For 96 to 128 GPUs with 34B models and 96 GPUs with 13B models, the split strategy becomes optimal. The split strategy divides GPUs evenly between the two sets of models, as their sizes are equal. For 13B models on 128 GPUs, the standalone strategy achieves the highest throughput. In this case, HybridFlow allocates 64 GPUs for the actor, 32 for the critic, and 16 each for the reference and reward model. In smaller clusters, computation of all models can fully utilize GPU resources; the colocate strategy ensures maximum GPU usage in different RLHF stages. In larger clusters, RLHF throughput under colocate placement fails to scale up linearly as the batch size is fixed and the computation-tocommunication ratio decreases with a larger DP size on more GPUs. Standalone and split strategies place models on different devices with a smaller DP size for each model in larger clusters, facilitating parallel execution of different models in the same stages. In all cases, our Algorithm 1 produces the best placement with the highest training throughput. Larger critic and reward model. We further evaluate model placements when running PPO with a 13B actor and reference policy and 70B critic and reward models (larger critic and reward models are expected to produce better alignment [7]). Figure 13 shows that the colocate strategy still outperforms others by 44.8% on average with up to 64 GPUs. The split strategy achieves higher throughput with 96 GPUs. When scaling to 128 GPUs, the best placement obtained by Algorithm 1 colocates actor, reference, and reward models on 64 GPUs while allocating the remaining 64 GPUs to critic. On the same number of GPUs, actor and reference policy's computation time is much smaller than critic and reward model, and colocating the reward model with actor and reference policy reduces the GPU idle time in the experience preparation stage. In general, distributing actor and critic on different devices for parallel execution in the training stage leads to higher throughput in large clusters. 8.4 3D-HybridEngine Transition time comparison. Figure 14 shows the transition time between actor training and generation stages on for parallel execution in the training and preparation stages would help achieve higher throughput. Resource multiplexing. HybridFlow enables colocation of models on shared devices by utilizing time-sharing for GPU computation. Recent research in DNN task scheduling has developed fine-grained resource multiplexing techniques, primarily aimed at achieving the service-level objectives of individual tasks [8, 18, 26, 26, 47, 56, 77]. Although the ResourcePool implementation supports parallel execution of collocated models, HybridFlow generally adheres to sequential execution to prevent GPU resource contention or OOM issues as discussed in Section 2.3. Applying GPU sharing and heterogeneous resources in RLHF training poses distinct challenges, as it seeks to balance the computation workload and manage complex data dependencies among various tasks. Investigating fine-grained auto-mapping algorithms for GPU sharing in RLHF training, coupled with model offload optimization and integration of heterogeneous devices, would be a promising direction for future research.

## End-to-End performance

From alignment to reasoning. In RLHF for LLM alignment, the reward signal is generated by the reward model. Besides alignment tasks, similar algorithms (e.g., PPO and GRPO [[70]](#b69)) can be applied to other domains, such as code generation and mathematical reasoning. For these tasks, a ground truth may exist for each prompt, which can be determined by assessing the correctness of the output value for each code test case and verifying the accuracy of mathematical results. Therefore, the reward model can be replaced by non-neural-network reward modules, such as a sandbox environment [[87]](#b86) for evaluating generated code or a reward function [[14,](#b13)[65]](#b64) to validate mathematical results. HybridFlow can seamlessly integrate these reward modules by wrapping them as remote functions and orchestrating their execution within the singleprocess script, providing a flexible and efficient framework for diverse reinforcement learning applications.

## Related Work

RL frameworks. There have been plenty of frameworks for RL, ranging from general-purpose RL systems design for small-scale DNNs [[12,](#b11)[25,](#b24)[28,](#b27)[39,](#b38)[45,](#b44)[46]](#b45) to RLHF systems specifically optimized for LLMs [[15,](#b14)[17,](#b16)[30,](#b29)[80,](#b79)[82]](#b81). We have thoroughly examined closely related work in Â§2 and we discuss more RL frameworks in this section. These RL frameworks [[12,](#b11)[25,](#b24)[28,](#b27)[39,](#b38)[74]](#b73), similar to recent RLHF systems, use a hodgepodge of multi-controller frameworks to implement their algorithms. They establish multiple long-running distributed programs with each component coordinating the execution order with hard-coded data synchronization. Gear [[74]](#b73) further optimized the experience replay segment of the RL pipeline. However, all these frameworks fail to support LLM training, inference, and generation in RLHF.

LLM training and serving systems. TorchDDP [[57]](#b56) and Horovod [[69]](#b68) support data parallel training. ByteScheduler [[58]](#b57) and DeepSpeed [[60]](#b59) extend data parallelism with communication and memory optimizations. Numerous systems [[23,](#b22)[36,](#b35)[48,](#b47)[54,](#b53)[71,](#b70)[75,](#b74)[89]](#b88) optimized large model training through model parallelisms such as tensor parallelism and pipeline parallelism to partition models across devices. LLM serving systems [[3,](#b2)[16,](#b15)[40,](#b39)[72,](#b71)[83,](#b82)[92]](#b91) also adopts data and model parallelism to accelerate auto-regressive generation with specialized optimizations like continuous-batching [[83]](#b82) and chunked-prefill [[3]](#b2). Note that all the above frameworks adopt multi-controller paradigm for efficient computation. Dataflow systems. Dataflow systems like MapReduce [[21]](#b20), Spark [[86]](#b85), Dryad [[33]](#b32), and Naiad [[51]](#b50) are popular for analytics and ML workloads but they lack support for dynamic task graphs. Ray [[50]](#b49) unifies task-parallel and actor programming models in a single dynamic task graph and implements a scalable distributed scheduler and a global control store, which is adopted by many RL frameworks [[45,](#b44)[46]](#b45). Pathways [9], a closed-source project for TPUs, are designed to easily express complex parallelism patterns and fine-grain control flow within a single DNN model, such as pipeline parallelism and Mixture-of-Experts with sparse computation. It employs an asynchronous distributed dataflow design that enables parallel control plane execution despite data dependencies, reducing the dispatch overhead from single-controller paradigm. Its main focus lies on single-model training, requiring complex compilations of each sub-network of a DNN model. HybridFlow can integrate Pathways as a submodule to implement the computation of models in the RLHF dataflow. 11 Conclusion HybridFlow is an RLHF framework that enables flexible representation and efficient execution of diverse RLHF algorithms. We propose a hybrid programming model that allows users to easily build RLHF dataflow in a few lines of code by encapsulating distributed computation of different LLMs into primitive APIs and hiding the complexity of data resharding among nodes. Our 3D-HybridEngine ensures efficient execution of training and generation of the actor model, with zero memory redundancy and significantly reduced communication overhead for model parameter resharding. Furthermore, our effective mapping algorithm optimizes GPU allocation and placement of models in the RLHF dataflow. Extensive experiments demonstrate that HybridFlow achieves 1.53Ã— to 20.57Ã— speedup compared to state-of-the-art RLHF systems under various model sizes and cluster scales.

## A Primitive APIs in HybridFlow

In HybridFlow, we implemented the primitive of each model in RLHF training by inheriting the 3DParallelWorker, FSDP Worker and ZeROWorker. The functions of these model classes are designed to decouple the distributed computation code and provide fundamental operations in RLHF for the users. This primitive design is compatible with the auto-regressive generation, forward pass, backward pass, and model update operations in the existing distributed inference and training frameworks. Users can easily customize the RLHF training dataflow (by adapting the numerical computation in the provided functions) according to the algorithm's design and benefit from reusing the underlying distributed computation implementation. We illustrate the meaning and the actual computations of these APIs in Table [4](#tab_6).

## B Transfer Protocols

We implemented transfer protocols that cover all common use cases of data resharding between models in RLHF dataflow. Users can utilize these pre-defined protocols to generate any RLHF dataflow. Moreover, Users can easily define their own transfer protocols by implementing a collect function and a distribute function. Transfer protocols decoupled the complicated data resharding and distributed training. We denote p, t, d as the rank of the worker in pipeline-, tensor-and dataparallel group respectively. We illustrate these predefined protocols in Table [3](#). best_para â† para_plan 17: return best_para includes three simulators for training, inference, and generation workload, all are analytical models following previous research [[42,](#b41)[84,](#b83)[92]](#b91). The training and inference workload is compute-bound while the generation workload is memorybound. For the actor model, we first find the parallelism strategy for training and record the memory usage in the training stage. During actor generation, KVCache requirements are calculated using the batch size and max sequence length. If the model-parallel size for the generation stage cannot accommodate both parameters and KVCache, we increase it. Then, we seek the optimal strategy with corresponding KVCache allocation by comparing the latency estimation. Developing a comprehensive autoregressive generation simulator that accounts for variable KVCache sizes could further enhance the auto-mapping process in RLHF research. The actor model computes the log probability of each token in the prompts and responses. This log probability is the same as the return log probability when performing generation using the same model precision. (Optional in PPO) compute_loss a forward pass The actor model computes the pretrain loss based on the pertaining dataset [[7,](#b6)[19,](#b18)[55]](#b54).

## C Auto-Parallelism Algorithm

update_actor a forward, backward pass and model update Based on the advantages, returns (calculated from compute_advantage) and pertaining loss, the actor model calculate the training loss and update its weights. We implement various loss for diverse RLHF algorithms including PPO [[55]](#b54), Safe-RLHF [[19]](#b18), ReMax [[43]](#b42), GRPO [[70]](#b69) and others.

Critic compute_values a forward pass The critic model computes the values for each prompt and response. update_critic a forward, backward pass and model update

Based on the values and returns, the critic computes a squared-error loss to update its weights. We also implement critic loss for diverse RLHF algorithms including PPO [[55]](#b54), Safe-RLHF [[19]](#b18), ReMax [[43]](#b42), GRPO [[70]](#b69) and others.

## Reference

Policy compute_ref_log_prob a forward pass

The reference model computes the reference log probability of each token in the prompts and responses. This log probability is utilized as a benchmark to evaluate the divergence of the actor model and constrain its learning process.

## Reward compute_reward a forward pass

The reward model conducts forward computation to calculate scores for a given set of prompts and responses. The rewards could be token-level or sample-level.

-compute_advantage numerical computation

Based on the values rewards from the value model and reward model respectively, the function estimates the advantages on the given prompts and the current policy model's responses. This computation involves no model forward passes.

![Figure 1. Dataflow graph of 3 RLHF algorithms [19, 43, 55]. Stage 1 â—‹, 2 â—‹, 3 â—‹ represent Generation, Preparation, and Training, respectively. RLHF algorithms, model sizes and cluster scales. Our evaluation demonstrates 1.53Ã—âˆ¼20.57Ã— throughput improvements.We have open-sourced HybridFlow and believe that Hy-bridFlow can boost future RLHF research and development.]()

![Figure 2. Programming model used in RLHF systems. (a) Existing RLHF systems adopt the multi-controller paradigm. (b) HybridFlow utilizes a hybrid programming model: the single-controller coordinates models; each model uses multicontroller paradigm in distributed computation. Inactive node in grey represents operation not executed at this time.]()

![Figure 3. Dataflow execution given a model placement plan. Blocks with numbers represent GPUs. In dashed boxes, the models are placed on different sets of devices and can be concurrently computed. Reference model (blue) and reward model (green) are colocated on the same set of GPUs and executed sequentially.]()

![Figure 4 depicts the architecture of HybridFlow, which consists of three major components: Hybrid Programming Model,]()


![Figure 5. An illustration of hierarchical APIs. (a) Model with 3D parallel configuration, resource allocation, and 3DParallelWorker initialization. (b) Asynchronous data resharding between two models with collect and distribute functions in 3D_PROTO. devices, it facilitates distributed model weight initialization and establishes 3D parallel groups for each model. A parallel group includes a set of GPUs to host a specific parallel dimension of the model, e.g., different tensor shards in TP and different model replicas in DP. Figure 5(a) illustrates initialization of the actor model with our APIs, while initialization of other models is similar.Inheriting from the 3DParallelWorker class, several model classes, for actor, critic, reference, and reward model, respectively, are provided. Each of these model classes encapsulates APIs to implement the model's distributed forward and backward computation, auto-regressive generation, and optimizer updates, decoupling the distributed computation code with data dependencies with other models. These APIs can be easily implemented by reusing the computation scripts from existing LLM systems. For example, the computation involved in update_actor function of ActorWorker (the class for the actor model) is similar to the pre-training scripts in Megatron-LM[71]. A model class encapsulates fundamental operations for implementing various RLHF algorithms, e.g., generate_sequences in the actor model class for generating responses based on the prompts and compute_reward in the reward model class for evaluating responses through a forward pass. (More APIs are detailed in Appendix A).Besides base class 3DParallelWorker that implements 3D parallelism, we further provide base classes for PyTorch FSDP (FSDPWorker) and ZeRO (ZeROWorker), and the corresponding model classes inheriting each base class, to support different parallelism strategies in model computation. Paral-lelWorker in Figure4denotes one of these base classes.]()

![Initialize cost model by reusing the RewardWorker cost = RewardWorker(cost_config, resource_pool) ... # omit other models initialization algo_type = "Safe-RLHF" # specify different RLHF numerical computation. # Examples of PPO and Safe-RLHF for (prompts, pretrain_batch) in dataloader: # Stage 1: Generate responses batch = actor.generate_sequences(prompts) batch = actor.generate_sequences(prompts, do_sample=False) # Stage 2: Prepare experience batch = critic.compute_values(batch) batch = reference.compute_log_prob(batch) batch = reward.compute_reward(batch) batch = cost.compute_cost(batch) batch = compute_advantages(batch, algo_type) # Stage 3: Actor and critic training critic_metrics = critic.update_critic(batch, loss_func=algo_type) pretrain_loss = actor.compute_loss(pretrain_batch) batch["pretrain_loss"] = pretrain_loss actor_metrics = actor.update_actor(batch, loss_func=algo_type) is added for Safe-RLHF is added for ReMax Not necessary in ReMax]()




![Figure 7. 3D-HybridEngine workflow in one RLHF iteration. 4 GPUs are used for actor training and generation. 1-2-2 (ğ‘-ğ‘¡-ğ‘‘) parallel groups are used in training and 1-1-2-2 (ğ‘ ğ‘” -ğ‘¡ ğ‘” -ğ‘‘ ğ‘” -ğ‘‘) parallel groups are used in generation.]()

![unused weights from other ranks (a) Same grouping methods between training and generation (HybridFlow-V) (b) Optimized parallel grouping methods (HybridFlow)]()

![Figure 8. Model weights resharding. 2 machines each with 4 GPUs are used for actor training and generation.]()

![-V, when 3D-HybridEngine uses the above vanilla parallel grouping methods in the two stages.We further design a new parallel grouping method for 3D-HybridEngine to use in the generation stage, that eliminates the redundancy in weights storage and leads to minimal memory footprint and communication due to actor model resharding between training and generation. Specifically, we form generation TP and PP groups by selecting ranks at regular intervals, determined by ğ‘¡ ğ‘¡ ğ‘” and ğ‘ ğ‘ ğ‘” , and construct micro DP groups by sequentially assigning ranks along the generation TP or PP dimensions. In Figure8(b), 1-2-2-2 parallel groups are used in generation: the generation TP groups are [G1, G3], [G2, G4], [G5, G7], [G6, G8]; and the micro DP groups are [G1, G2], [G3, G4], [G5, G6], [G7, G8]. This strategic rearrangement of generation parallel groups leads to overlap between training and generation model weights on each device, enabling reuse of training weights during generation and zero redundancy in device memory usage due to model resharding. In addition, 3D-HybridEngine conducts several all-gather operations concurrently, one within each micro DP group, leading to significantly reduced communication overhead.]()

![ğ‘ â† [0] Ã— ğ‘  // Initialize latency for each stage to 0 28: for all set âˆˆ ğ‘ğ‘™ğ‘š do 29: ğ‘ ğ‘” â† [0] Ã— ğ‘  30:for all ğ‘– âˆˆ {0, ..., ğ‘  -1} do 31:]()

![Figure 12. Throughput of HybridFlow under different placements]()

![Figure9for PPO, HybridFlow outperforms DeepSpeed-Chat, OpenRLHF and NeMo-Aligner by 3.67Ã— (up to 7.84Ã—), 3.25Ã— (up to 5.93Ã—) and 12.52Ã— (up to 20.57Ã—), respectively. This is mainly because HybridFlow effectively executes generation, inference, and training in all RLHF stages by sharding the models with different parallelism strategies to fit various computation workloads. HybridFlow achieves the highest average speedup of 9.64Ã— when training 70B models, as Hy-bridFlow reduces the transition overhead by up to 71.2% and 89.1% compared to DeepSpeed-Chat and OpenRLHF, which also incurs large inter-machine communication when training with ZeRO-3. Due to the lack of KVCache in generation engine, NeMo-Aligner's main performance bottleneck lies in the generation stage, which accounts for up to 81.2% of its RLHF iteration time. Similar results can be observed inFigures 10,11 validating the efficiency of HybridFlow on running various RLHF algorithms. Scalability. HybridFlow achieves at least 2.09Ã— speedup on 8 GPUs. With increasing GPUs, the strong scaling efficiency of HybridFlow on various model scales is 66.8%, computed by di-]()

![outlines the search process of the optimal parallelism strategy of each model. Starting from the minimal model parallelism size of each model (to prevent OOM when colocating with multiple workers), we enumerate all feasible parallel configurations based on the number of GPUs and the number of GPUs per machine ğ‘ˆ . The default number of ğ‘ˆ is set to 8. We use simu module to estimate the latency of each model based on their workload. This module Algorithm 2 Auto Parallelism Algorithm 1: Input: Device allocation ğ´, minimal device allocation and model parallel size for each model in a set ğ´ ğ‘šğ‘–ğ‘› , workload ğ‘Š , the number of GPUs per machine ğ‘ˆ 2: Output: the parallelism strategy for the model in a set 3: Procedure auto_parallel(ğ´, ğ´ ğ‘šğ‘–ğ‘› , ğ‘™, ğ‘Š ): 4: ğ‘ ğ‘™ = ğ´[ğ‘™] // Get device allocation of the model 5: ğ‘¡ ğ‘šğ‘–ğ‘› = ğ´ ğ‘šğ‘–ğ‘› [ğ‘™].ğ‘¡ // Get minimal model parallel size 6: ğ‘ ğ‘šğ‘–ğ‘› = ğ´ ğ‘šğ‘–ğ‘› [ğ‘™].ğ‘ 7: best_para â† âˆ… 8: best_para.cost â† âˆ 9: for all t âˆˆ {ğ‘¡ ğ‘šğ‘–ğ‘› , ğ‘¡ ğ‘šğ‘–ğ‘› + 1..., ğ‘ˆ } do 10: for all p âˆˆ {ğ‘ ğ‘šğ‘–ğ‘› , ğ‘ ğ‘šğ‘–ğ‘› + 1..., ğ‘ ğ‘™ ğ‘ˆ } do â† simu(ğ‘ğ‘ğ‘Ÿğ‘_ğ‘ğ‘™ğ‘ğ‘›, ğ‘™,ğ‘Š [ğ‘™]) 14: if best_para.ğ‘ğ‘œğ‘ ğ‘¡ > cost then 15: best_para.ğ‘ğ‘œğ‘ ğ‘¡ â† cost 16:]()

![Comparison of RLHF frameworks. Figures illustrate execution of one PPO iteration. Numbers 1-6 represent response generation, reward model inference, reference model inference, critic inference, actor training, and critic training, respectively.]()

![Transition overhead between training & generation ğ‘¡ğ‘ ğ‘€ ğ‘¡ğ‘ -ğ‘¡ ğ‘” ğ‘ ğ‘” ğ‘¡ ğ‘” ğ‘ ğ‘” ğ‘¡ğ‘ ğ‘€]()

![Key functions provided in each model class. The users can use these provided functions to construct various RLHF algorithms in a few lines of code.Based on a batch of prompts, the actor model generates a batch of responses and returns the log probability of each token in the responses.]()

