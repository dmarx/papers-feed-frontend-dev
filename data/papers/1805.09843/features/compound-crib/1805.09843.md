The paper "Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms" presents a compelling case for the efficacy of Simple Word-Embedding-based Models (SWEMs) in natural language processing (NLP). Below is a detailed technical explanation of the researchers' decisions regarding SWEMs, their pooling strategies, and the rationale behind their findings.

### SWEM Overview

**Rationale for SWEMs**: The researchers aimed to challenge the prevailing notion that complex models (like RNNs and CNNs) are always necessary for effective NLP. SWEMs utilize parameter-free pooling operations, which significantly reduce the computational burden associated with training deep learning models. This decision is rooted in the observation that many NLP tasks can be effectively addressed with simpler models that leverage the rich semantic information encoded in word embeddings.

### Key Findings

**Performance Comparison**: The researchers found that SWEMs often achieve performance that is comparable to or even superior to that of more complex models across various NLP tasks. This finding suggests that the additional complexity and parameters in RNNs and CNNs do not always translate to better performance, particularly in tasks where the semantic content of the text is more critical than the order of words.

### Pooling Strategies

The researchers explored several pooling strategies to create text representations from word embeddings:

1. **Average Pooling (SWEM-aver)**:
   - **Formula**: 
     \[
     z = \frac{1}{L} \sum_{i=1}^{L} v_i
     \]
   - **Justification**: This method treats all words equally, providing a straightforward way to aggregate information. It is computationally efficient and captures the overall semantic meaning of the text.

2. **Max Pooling (SWEM-max)**:
   - **Formula**: 
     \[
     z = \max(v_1, v_2, \ldots, v_L)
     \]
   - **Justification**: By focusing on the maximum values in each dimension of the word embeddings, this approach emphasizes the most salient features of the text. This is particularly useful in tasks where certain key words are more informative than others, allowing the model to ignore less relevant information.

3. **Concatenated Pooling (SWEM-concat)**:
   - **Justification**: This strategy combines the strengths of both average and max pooling, providing a richer representation that captures both the overall semantic meaning and the most salient features of the text.

### Hierarchical Pooling

**Hierarchical Pooling**:
- **Justification**: The hierarchical pooling strategy incorporates spatial information by performing average pooling on local windows of words followed by global max pooling. This approach preserves the n-gram structure of the text, allowing the model to capture local patterns while still maintaining a global perspective. This is particularly beneficial for tasks sensitive to word order, such as sentiment analysis.

### Computational Complexity

The researchers provided a detailed comparison of computational complexity across different models:

- **SWEM**: \( O(L \cdot K) \) for sequential operations, with no additional parameters.
- **CNN**: \( O(n \cdot L \cdot K \cdot d) \) with \( n \) filters, indicating a much higher computational cost.
- **LSTM**: \( O(L \cdot d^2 + L \cdot K \cdot d) \), which also involves significant computational overhead due to the number of parameters.

**Rationale**: This comparison highlights the efficiency of SWEMs, making them attractive for scenarios where computational resources are limited or where rapid training is essential.

### Experimental Setup

The researchers evaluated SWEMs on 17 datasets across three tasks:
- **Document Classification**: Tasks like Yahoo news and Yelp reviews.
- **Text Sequence Matching**: Tasks such as SNLI and WikiQA.
- **Short Text Tasks**: Including the Stanford sentiment treebank and TREC.

**Justification**: This diverse experimental setup allows for a comprehensive assessment of SWEMs across different types of NLP challenges, reinforcing the generalizability of their findings.

### Conclusion

The study emphasizes the tradeoff between computation and expressiveness, advocating for the use of simpler models like SWEMs in NLP tasks. The empirical evidence supporting their effectiveness suggests that simpler models can often achieve results on par with more complex architectures, challenging the assumption that increased complexity is always necessary for success in NLP.

In summary, the researchers' decisions regarding SWEMs and their pooling strategies are grounded in a desire to explore the limits of simplicity in NLP, demonstrating that effective text representation can be achieved without the computational overhead associated with more complex models. This work encourages further exploration of simple models in the field, potentially leading to more efficient and accessible NLP solutions.