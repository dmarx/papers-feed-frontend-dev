- Decision to focus on Simple Word-Embedding-based Models (SWEMs) for text representation.
- Choice of pooling mechanisms: average pooling, max pooling, and hierarchical pooling.
- Selection of datasets for comparative analysis (17 datasets across three tasks).
- Decision to compare SWEMs with RNN and CNN models in a point-by-point manner.
- Assumption regarding the computational efficiency of SWEMs versus complex models.
- Rationale for proposing max-pooling for improved interpretability.
- Justification for hierarchical pooling to preserve spatial information.
- Decision to conduct experiments on distinct NLP tasks (document classification, sequence matching, short text tasks).
- Choice of evaluation metrics for model performance comparison.
- Decision to leverage subspace training to constrain trainable parameters.
- Assumption about the sufficiency of simple pooling strategies for certain NLP tasks.
- Decision to highlight the computation-vs.-expressiveness tradeoff in model selection.
- Choice to document findings on the importance of word-order information in predictions.
- Decision to make source code and datasets publicly available for reproducibility.
- Rationale for preferring simple models based on Occam's razor principle.