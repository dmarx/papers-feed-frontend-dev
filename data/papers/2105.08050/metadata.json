{
  "arxivId": "2105.08050",
  "title": "Pay Attention to MLPs",
  "authors": "Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le",
  "abstract": "Transformers have become one of the most important architectural innovations\nin deep learning and have enabled many breakthroughs over the past few years.\nHere we propose a simple network architecture, gMLP, based on MLPs with gating,\nand show that it can perform as well as Transformers in key language and vision\napplications. Our comparisons show that self-attention is not critical for\nVision Transformers, as gMLP can achieve the same accuracy. For BERT, our model\nachieves parity with Transformers on pretraining perplexity and is better on\nsome downstream NLP tasks. On finetuning tasks where gMLP performs worse,\nmaking the gMLP model substantially larger can close the gap with Transformers.\nIn general, our experiments show that gMLP can scale as well as Transformers\nover increased data and compute.",
  "url": "https://arxiv.org/abs/2105.08050",
  "issue_number": 817,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/817",
  "created_at": "2025-01-05T18:41:00.743341",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 45,
  "last_read": "2025-01-05T23:08:39.880665",
  "last_visited": "2025-01-05T23:07:26.844000+00:00",
  "main_tex_file": null,
  "published_date": "2021-05-17T17:55:04Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CL",
    "cs.CV"
  ]
}