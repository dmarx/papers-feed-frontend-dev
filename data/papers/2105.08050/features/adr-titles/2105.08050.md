- Decision to propose gMLP as an alternative to Transformers
- Choice of using MLPs with gating instead of self-attention
- Design of the Spatial Gating Unit (SGU)
- Decision to follow BERT's input and output protocols
- Choice of linear projections for channel and spatial projections
- Decision to initialize weights for training stability
- Choice to split input into two independent parts for gating
- Decision to apply gMLP to both image classification and masked language modeling
- Choice of hyperparameters for training gMLP models
- Decision to compare gMLP performance with existing Transformer models
- Choice to use a larger gMLP model to close performance gaps
- Decision to blend in a small amount of self-attention for improved performance
- Choice of regularization techniques to prevent overfitting
- Decision to conduct ablation studies on SGU variants
- Choice to evaluate gMLP on multiple downstream NLP tasks
- Decision to analyze the scaling behavior of gMLP with increased data and compute