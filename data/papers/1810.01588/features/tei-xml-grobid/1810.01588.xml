<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpreting Layered Neural Networks via Hierarchical Modular Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-10-04">October 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chihiro</forename><surname>Watanabe</surname></persName>
							<email>watanabe.chihiro@lab.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">NTT Communication Science Laboratories</orgName>
								<address>
									<addrLine>3-1, Morinosato Wakamiya Atsugi-shi</addrLine>
									<settlement>Kanagawa Pref</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interpreting Layered Neural Networks via Hierarchical Modular Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-04">October 4, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">A468C9C1D10F0F94C78A830A970D5C59</idno>
					<idno type="arXiv">arXiv:1810.01588v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. In this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To construct a method for interpreting the prediction mechanism of complex statistical models is currently one of the most important tasks in the machine learning field, especially with layered neural networks (or LNNs), which have achieved high predictive performance in various practical tasks. Due to their complex hierarchical structure and the nonlinear parameters that they use to process the input data, we cannot understand the function of a trained LNN as it is, and we need some kind of approximation method to convert the original function of an LNN into a simpler interpretable representation.</p><p>Recently, various methods have been proposed for interpreting the function of an LNN, and they can be roughly classified into <ref type="bibr" target="#b0">(1)</ref> the approximation of an LNN with an interpretable model, and (2) the investigation of the roles of the partial structures constituting an LNN (e.g. units or layers). As for approach (1), various methods have been investigated for approximating an LNN with a linear model <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> or a decision tree <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>. For image classification tasks in particular, methods for visualizing an LNN function have been extensively studied in terms of which part of an input image affects the prediction result <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Approach <ref type="bibr" target="#b1">(2)</ref> has been studied by several authors who examined the function of a given part of an LNN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>. There has also been an approach designed to automatically extract the cluster structure of a trained LNN <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> based on network analysis.</p><p>Although the above studies have made it possible to provide us with an interpretable representation of an LNN function with a fixed resolution (or number of clusters), there is a problem in that we do not know in advance the optimal resolution for interpreting the original network. In the methods described in the previous studies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref>, the unit clustering results may change greatly with the cluster size setting, and there is no criterion for determining the optimal cluster size. Another problem is that the previous studies could only provide us with information about the magnitude of the relationship between a cluster and each input or output dimension value, and we could not determine whether this relationship was positive or negative.</p><p>In this paper, we propose a method for extracting a hierarchical modular representation from a trained LNN, which provides us with both hierarchical clustering results with every possible number of clusters and the function of each cluster. Our proposed method mainly consists of three parts: (a) training an LNN for a given data set based on error back propagation, (b) determining the feature vectors of each hidden layer unit based on its correlation with the input and output dimension values, and (c) the hierarchical clustering of the feature vectors. Unlike the clustering methods in the previous studies, the role of each cluster is computed as a centroid of the feature vectors defined by the correlations in step (b), which enables us to know the representative mapping performed by the cluster in terms of both sign and magnitude for each input or output dimension.</p><p>We show experimentally the effectiveness of our proposed method in interpreting the internal mechanism of a trained LNN, by applying it to two kinds of data sets: the MNIST data set that contains digit image data and a sequential data set of food consumer price indices. Based on the experimental results for the extracted hierarchical cluster structure and the role of each cluster, we discuss how the overall LNN function is structured as a collection of individual units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Training a Layered Neural Network</head><p>An LNN can be trained to approximate the input-output relationship of an arbitrary data set (x, y) that consists of input data x ∈ R M and output data y ∈ R N , by using a function f (x, w) from x ∈ R M and a parameter w ∈ R L to R N . An LNN parameter is defined by w = {ω d ij , θ d i }, where ω d ij is the connection weight between the i-th unit in a depth d layer and the j-th unit in a depth d + 1 layer, and θ d i is the bias of the i-th unit in the depth d layer. Here, d = 1 and d = d 0 , respectively, correspond to the input and output layers. The LNN function f (x, w) is a set of functions {f j (x, w)} for all output dimensions j, each of which is defined by</p><formula xml:id="formula_0">f j (x, w) = σ( i ω d0-1 ij o d0-1 i + θ d0-1 j</formula><p>). Here, σ(x) = 1/(1 + exp(-x)), and o d i is the output value of the i-th unit in the depth d layer and o 1 i = x i holds in the input layer. Such output values in each layer are given by o</p><formula xml:id="formula_1">d j = σ( i ω d-1 ij o d-1 i + θ d-1 j</formula><p>). The purpose of training an LNN is to find an optimal parameter w to approximate the true input-output relationship with a finite size training data set {(X n , Y n )} n1 n=1 , where n 1 is the sample size. The training error E(w) of an LNN is given by E 2 , where • is the Euclidean norm of R N . Since the minimization of the training error E(w) leads to overfitting to a training data set, we adopt the L1 regularization method <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> to delete redundant connection weights and obtain a sparse solution.</p><formula xml:id="formula_2">(w) = 1 n1 n1 n=1 Y n -f (X n , w)</formula><p>Here, the objective function to be minimized is given by H</p><formula xml:id="formula_3">(w) = n1 2 E(w) + λ d,i,j |ω d ij |</formula><p>, where λ is a hyperparameter used to determine the strength of regularization. The minimization of such a function H(w) with the stochastic steepest descent method can be executed by an iterative update of the parameters from the output layer to the input layer, which is called error back propagation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref>. The parameter update is given by</p><formula xml:id="formula_4">∆ω d-1 ij = -η(δ d j o d-1 i + λ sgn(ω d-1 ij )), ∆θ d j = -ηδ d j ,</formula><p>where δ d0 j = (o d0 j -y j ) (o d0 j (1-o d0 j )+ 1 ), and</p><formula xml:id="formula_5">δ d j = l d+1 k=1 δ d+1 k ω d jk (o d j (1-o d j )+ 1 ) for d = d 0 -1, • • • , 2.</formula><p>Here, y j is the j-th output dimension value of a randomly chosen n-th sample (X n , Y n ), 1 is a hyperparameter for the LNN convergence, and η is the step size for training time t that is determined such that η(t) ∝ 1/t. In the experiments, we adopt 1 = 0.001 and η = 0.7 × a 1 n 1 /(a 1 n 1 + 5t), where a 1 is the mean iteration number for LNN training per dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Modular Representation of LNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Determining Feature Vectors of Hidden Layer Units</head><p>To apply hierarchical clustering to a trained LNN, we define a feature vector for each hidden layer unit. Let v k be the feature vector of the k-th hidden layer unit in a hidden layer. Such a feature vector should reflect the role of its corresponding unit in LNN inference. Here, we propose defining such a feature vector v k of the k-th hidden layer unit based on its correlations between each input or output dimension. In previous studies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29]</ref>, methods have been proposed for determining the role of a unit or a unit cluster based on the square root error. However, these methods can only provide us with knowledge about the magnitude of the effect of each input dimension on a unit and the effect of a unit on each output dimension, not information about how a hidden layer unit is affected by each input dimension and how each output dimension is affected by a hidden layer unit. In other words, there is no method that can reveal whether an increase in the input dimension value has a positive or negative effect on the output value of a hidden layer unit, or whether an increase in the output value of a hidden layer unit has a positive or negative effect on the output dimension value. To obtain such sign information regarding the roles of each hidden layer unit, we use the following definition based on the correlation. Definition 1 (Effect of i-th input dimension on k-th hidden layer unit). We define the effect of the i-th input dimension on the k-th hidden layer unit as v in ik , where</p><formula xml:id="formula_6">v in ik = E X (n) i -E[X (n) i ] o (n) k -E[o (n) k ] E X (n) i -E[X (n) i ] 2 E o (n) k -E[o (n) k ] 2 .</formula><p>Here, E[•] represents the mean for all the data samples, X</p><p>(n) i</p><p>is the i-th input dimension value of the n-th data sample, and o</p><formula xml:id="formula_7">(n) k</formula><p>is the output of the k-th hidden layer unit for the n-th input data sample.</p><p>Definition 2 (Effect of k-th hidden layer unit on j-th output dimension). We define the effect of the k-th hidden layer unit on the j-th output dimension as v out kj , where</p><formula xml:id="formula_8">v out kj = E o (n) k -E[o (n) k ] y (n) j -E[y (n) j ] E o (n) k -E[o (n) k ] 2 E y (n) j -E[y (n) j ] 2 .</formula><p>Here, y</p><formula xml:id="formula_9">(n) j</formula><p>is the value of the j-th output layer unit for the n-th input data sample.</p><p>We define a feature vector of each hidden layer unit based on the above definitions.</p><p>Definition 3 (Feature vector of k-th hidden layer unit). We define the feature vector of the k-th hidden layer unit as</p><formula xml:id="formula_10">v k ≡ [v in 1k , • • • , v in i0k , v out k1 , • • • , v out kj0 ].</formula><p>Here, i 0 and j 0 , respectively, represent the dimensions of the input and output data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment of signs of feature vectors based on cosine similarity</head><p>The feature vectors of Definition 3 represent the roles of the hidden layer units in terms of input-output mapping. When interpreting such roles of hidden layer units, it is natural to regard the roles of any pair of units (k 1 , k 2 ) as being the same iff they satisfy v k1 = v k2 or v k1 = -v k2 . The latter condition corresponds to the case where the k 1 -th and k 2 -th units have the same correlations with input and output dimensions except that their signs are the opposite, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>. To regard the roles of unit pairs that satisfy one of the above conditions as the same, we propose an algorithm for aligning the signs of the feature vectors based on cosine similarity (Algorithm 1). By randomly selecting a feature vector and aligning its sign according to the sum of the cosine similarities with all the other feature vectors, the sum of the cosine similarities of all the pairs of feature vectors increases monotonically. We show experimentally the effect of this sign alignment algorithm in Appendix 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Clustering of Units in a Trained LNN</head><p>Once we have obtained the feature vectors of all the hidden layer units as described in section 3.1, we can extract a hierarchical modular representation of an LNN by applying hierarchical clustering to the feature vectors. Among the several existing methods for such hierarchical clustering including single-link and complete-link, Ward's method <ref type="bibr" target="#b25">[26]</ref> has been shown experimentally to be effective in terms of its classification sensitivity, so we employ this method in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Alignment of signs of feature vectors based on cosine similarity</head><p>1: Let v k and a 0 respectively be the feature vector for the k-th hidden layer unit and the number of iterations. layer units. 2: for a = 1 to a 0 do 3:</p><p>Randomly choose the k-th hidden layer unit according to the uniform distribution.</p><p>4:</p><formula xml:id="formula_11">if l =k v k •v l √ v k •v k √ v l •v l &lt; 0 then 5: v k ← -v k . 6:</formula><p>end if 7: end for We start with k 0 individual hidden layer units, and sequentially combine clusters with the minimum error sum of squares (ESS), which is given by</p><formula xml:id="formula_12">ESS ≡ m k:u k ∈Cm v k 2 - 1 |C m | k:u k ∈Cm v k 2 ,<label>(1)</label></formula><p>where u k and v k , respectively, are the k-th hidden layer unit (k = 1, • • • , k 0 ) and its corresponding feature vector, C m is the unit set assigned to the m-th cluster, and | • | represents the cluster size. From Equation (1), the ESS is the value given by first computing the cluster size (|C m |) times the variance of the feature vectors in each cluster, and then by taking the sum of all these values for all the clusters. When combining a pair of clusters (C m1 , C m2 ) into one cluster, the ESS increases by</p><formula xml:id="formula_13">∆ESS = |C m1 ||C m2 | |C m1 | + |C m2 | 1 |C m1 | k:u k ∈Cm 1 v k - 1 |C m2 | k:u k ∈Cm 2 v k 2 .</formula><p>(</p><formula xml:id="formula_14">)<label>2</label></formula><p>Therefore, in each iteration, we do not have to compute the error sum of squares for all the clusters, instead we simply have to compute the error increase ∆ESS given by Equation ( <ref type="formula" target="#formula_14">2</ref>) for all the pairs of current clusters (C m1 , C m2 ), find the optimal pair of clusters that achieves the minimum error increase, and combine them. We describe the whole procedure of Ward's method in Algorithm 2. This procedure to combine a pair of clusters is repeated until all the hidden layer units are assigned to one cluster, and from the clustering result {C</p><formula xml:id="formula_15">(t) m } in each iteration t = 1, • • • , k 0 -1,</formula><p>we can obtain a hierarchical modular representation of an LNN, which connects the two extreme resolutions given by "all units are in a single cluster" and "all clusters consist of a single unit." The role of each extracted cluster can be determined from the centroid of the feature vectors of the units assigned to the cluster, which can be interpreted as a representative input-output mapping of the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We apply our proposed method to two kinds of data sets to show its effectiveness in interpreting the mechanism of trained LNNs. The experimental settings are detailed in the Appendix 3. In Appendix 1, we provide a qualitative comparison with the previous method <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Using the MNIST Data Set</head><p>First, we applied our proposed method to an LNN trained with the MNIST data set <ref type="bibr" target="#b10">[11]</ref> to recognize 10 types of digits from input images. Before the LNN training, we sharpened the top, bottom, left and right margins and then resized the images to 14 × 14 pixels. Figure <ref type="figure" target="#fig_1">2</ref> shows sample images for each class of digits. Although our proposed method provided us with a clustering result for all the possible resolutions or Algorithm 2 Ward's hierarchical clustering method <ref type="bibr" target="#b25">[26]</ref> 1: Let u k and v k , respectively, be the k-th hidden layer unit (k = 1, • • • , k 0 ) and its corresponding feature vector, and let {C (t) m } be the unit set assigned to the m-th cluster in the t-th iteration (m = 1, • • • , k 0t + 1). Initially, we set t ← 1 and C</p><p>(1)</p><formula xml:id="formula_16">m ← {u m }. 2: for t = 2 to k 0 -1 do 3: (C (t-1) m1 , C (t-1) m2 ) ← arg min (C (t-1) i ,C (t-1) j ) ∆ESS(C (t-1) i , C (t-1) j ), where ∆ESS(C, C ) ≡ |C||C | |C| + |C | 1 |C| k:u k ∈C v k - 1 |C | k:u k ∈C v k 2 .</formula><p>Here, we assume m 1 &lt; m 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Update the clusters as follows:</p><formula xml:id="formula_17">C (t) m ←      C (t-1) m1 ∪ C (t-1) m2 (m = m 1 ) C (t-1) m (1 ≤ m ≤ m 2 -1, m = m 1 ) C (t-1) m+1 (m 2 ≤ m ≤ k 0 -t + 1)</formula><p>. 5: end for the numbers of clusters c, we have only plotted the results for c = 4, 8, 16, for ease of visibility. Figures <ref type="figure" target="#fig_2">3</ref> and <ref type="figure" target="#fig_3">4</ref>, respectively, show the hierarchical cluster structure extracted from the trained LNN and the roles or representative input-output mappings of the extracted clusters. From these figures, we can gain knowledge about the LNN structure as follows.</p><p>-At the coarsest resolution, the main function of the trained LNN is decomposed into Clusters 1, 2, 3 and 4. Cluster 1 captures the input information about black pixels in the shape of a 6 and white pixels in the shape of a 7, and it has a positive and negative correlation with the output dimensions corresponding to "6" and "7", respectively. Cluster 2 correlates negatively with the region in the shape of a 9, and positively with the other areas. It has a positive correlation with the recognition of "2" and "6," and it has a negative one with "0," "4" and "9." Cluster 3 correlates positively with the black pixels in the left part of an image, and it has a positive correlation with "0," "4" and "6," and a negative correlation with "3" and "7." Cluster 4 captures the 0-shaped region, and it has a larger correlation with the output of "0" compared with the other digits.</p><p>-Cluster 2 is decomposed into three smaller clusters, 7, 8 and 9. Cluster 7 captures similar input information to Cluster 2, and it also correlates strongly with the lower area of an image. This cluster mainly affects the recognition result for "5" and "6." Cluster 8 uses the input information of the area with the shape of a 9, however, its main recognition target is "2." Cluster 9 correlates positively with the area extending from the upper right to the lower left of an image, and it correlates negatively with the digits "4" and "9." -Cluster 8 consists of two smaller clusters, 17 and 18. Cluster 17 is mainly affected by the upper part and lower right part of an image, and the absolute value of its correlations with output dimensions are all less than 0.2, while the role of Cluster 18 is almost the same as that of Cluster 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Using the Consumer Price Index Data Set</head><p>We also applied the proposed method to an LNN trained with a data set of a consumer price index <ref type="bibr" target="#b6">[7]</ref> to predict the consumer price indices of taro, radish and carrot for a month from 36 months' input data. With this data set, we plotted the results for c = 3, 6, 12, where c is the number of clusters. Figures <ref type="figure" target="#fig_4">5</ref> and <ref type="figure" target="#fig_5">6</ref>, respectively, show the hierarchical cluster structure extracted from the trained LNN and the roles or representative input-output mappings of the extracted clusters. From these figures, we can gain knowledge about the LNN structure as follows.</p><p>-Clusters 1, 2 and 3 represent the main input-output function of the hidden layer units. Interestingly, all of these clusters have similar correlations with the output dimensions (0 &lt; radish &lt; taro &lt; carrot). However, these three clusters use different input information: Cluster 1 strongly reflects seasonal information, and its correlation is especially high with the consumer price indices of the three vegetables one month before and one, two and three years earlier. Cluster 3 also reflects seasonal information, however, the absolute values of the correlations are less than 0.3 and it correlates strongly with the input information of eight, 20 and 32 months before. On the other hand, Cluster 2 does not use such a seasonal effect very much, and it is affected almost equally by the information of all months, except the recent information of radish from nine months before.</p><p>-Cluster 1 is composed of smaller clusters of 16 and 17. Cluster 16 is mainly used to predict the consumer price index of taro and it strongly correlates with the input information for taro from one month before and one, two and three years before. Compared with Cluster 16, Cluster 17 affects the three output dimensions more equally.</p><p>-Cluster 7 is a part of Cluster 3, and consists of smaller clusters of 11, 12 and 13. These clusters have mutually different relationships with the output dimension values: Cluster 11 correlates positively with consumer price indices of taro and carrot, and negatively with that of radish. It mainly uses recent information about carrot (within a year) and the values of taro of five, 17 and 29 months before. Cluster 13 is mainly used to predict the radish output value. It has a positive correlation with the input information for taro, radish and carrot of about six, 18 and 30 months earlier, and it has a negative correlation with values for one month before and one, two and three years before. The absolute values of the correlations between Cluster 12 and the output dimension values are less than 0.2, so, unlike with Clusters 11 and 13, it does not significantly affect the prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Here, we discuss our proposed method for obtaining a hierarchical modular representation from the perspectives of statistical evaluation and visualization.</p><p>Our proposed method provides us with a series of clustering results for an arbitrary cluster size, and the resulting structure does not change if we use the same criterion (e.g. error sum of squares for Ward's method) for evaluating the similarity of the feature vectors. However, there is no way to determine which criterion yields the optimal clustering result to represent a trained LNN, due to the fact that interpretability of acquired knowledge cannot be formulated mathematically (although there has been an attempt to quantify the interpretability for a specific task, especially image recognition <ref type="bibr" target="#b3">[4]</ref>). This problem makes it impossible to compare different methods for interpreting LNNs quantitatively, as pointed out in the previous studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>. Therefore, the provision of a statistical evaluation method as regards both interpretability and accuracy for the resulting cluster structure constitutes important future work.</p><p>Although we can apply our proposed method to an arbitrary network structure, as long as it contains a set of units that outputs some value for a given input data sample, the visualization of the resulting hierarchical modular representations becomes more difficult with a deeper and a larger scale network structure, since a cluster may contain units in mutually distant layers. Additionally, the number of possible cluster sizes increases with the scale (or the number of units) of a network, and so it is necessary to construct a method for automatically selecting a set of representative resolutions, instead of visualizing the entire hierarchical cluster structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Finding a way to unravel the function of a trained LNN is an important issue in the machine learning field. While LNNs have achieved high prediction accuracy with various data sets, their highly complex and nonlinear parameters have made it difficult to interpret their internal inference mechanism. Recent studies have enabled us to decompose a trained LNN into simpler cluster structure, however, there is no method for (1) determining the optimal number of clusters, or (2) knowing whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. In this paper, we proposed a method for extracting the hierarchical modular representation of a trained LNN, which consists of sequential clustering results with every possible number of clusters. By determining the feature vectors of the hidden layer units based on their correlations with input and output dimension values, it also enabled us to know what range of input each cluster maps to what range of output. We showed the effectiveness of our proposed method experimentally by applying it to two kinds of practical data sets and by interpreting the resulting cluster structure.             </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of two hidden layer units with the same function. The corresponding feature vectors are the same, except that their signs are opposite.</figDesc><graphic coords="4,334.41,77.02,155.90,119.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Input image examples of MNIST data set.</figDesc><graphic coords="8,148.43,157.15,311.80,180.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hierarchical clusters of an LNN (MNIST data set).</figDesc><graphic coords="8,120.78,361.63,85.04,76.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Representative input-output mappings of extracted clusters.</figDesc><graphic coords="8,122.82,645.26,65.20,58.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Hierarchical clusters of an LNN (food consumer price index data set).</figDesc><graphic coords="9,148.43,70.87,311.81,194.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Representative input-output mappings of extracted clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Cluster structure of an LNN acquired by non-negative matrix factorization (MNIST data set).</figDesc><graphic coords="14,120.09,72.84,368.48,213.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Representative input-output mappings of extracted clusters.</figDesc><graphic coords="14,92.43,610.71,99.22,89.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Cluster structure of an LNN acquired by non-negative matrix factorization (food consumer price index data set).</figDesc><graphic coords="15,134.26,70.87,340.13,212.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Representative input-output mappings of extracted clusters. 15</figDesc><graphic coords="15,130.87,660.83,170.05,58.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Left: Feature vectors of Definition 3. Each row corresponds to a feature vector for a hidden layer unit. Center: Feature vectors after the alignment of the signs. Right: Sum of the cosine similarities of all the pairs of feature vectors (MNIST data set).</figDesc><graphic coords="16,71.49,138.21,132.28,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Dendrograms of the hierarchical clustering results with the original feature vectors of Definition 3 (top) and with the feature vectors after the alignment of the signs (bottom).</figDesc><graphic coords="16,77.57,400.13,453.54,212.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Left: Feature vectors of Definition 3. Each row corresponds to a feature vector for a hidden layer unit. Center: Feature vectors after the alignment of the signs. Right: Sum of the cosine similarities of all the pairs of feature vectors (food consumer price index data set).</figDesc><graphic coords="17,71.49,138.21,132.28,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Dendrograms of the hierarchical clustering results with the original feature vectors of Definition 3 (top) and with the feature vectors after the alignment of the signs (bottom).</figDesc><graphic coords="17,77.57,400.13,453.54,212.60" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 1: Comparison with Clustering Method Based on Nonnegative Matrix Factorization</head><p>Here, we show the effectiveness of our proposed method by comparing it with the clustering method based on non-negative matrix factorization (or NNMF), which was proposed in a previous study <ref type="bibr" target="#b28">[29]</ref>. We applied this NNMF-based clustering method to the same data sets that we used in the experiments described in section 4. In the previous study <ref type="bibr" target="#b28">[29]</ref>, the feature vectors of the hidden layer units are defined by the magnitude of the effect of each input dimension value on a cluster and the effect of a cluster on each output dimension value, computed by the square root error of the unit output values. By definition, the elements of such feature vectors are all non-negative, which is a necessary condition for applying NNMF to the feature vectors.</p><p>We applied the NNMF-based clustering method to the trained network with exactly the same parameter as the network shown in Figures <ref type="figure">3</ref> and <ref type="figure">5</ref>. With the MNIST data set <ref type="bibr" target="#b10">[11]</ref> and the data set of a consumer price index <ref type="bibr" target="#b6">[7]</ref>, respectively, we decomposed the trained networks into 16 and 12 clusters. With both data sets, we set the number of iterations of the NNMF algorithm at 1000. We applied the NNMF algorithm for 10000 times, and used the best result in terms of the approximation error. Initial values of the two low-dimensional matrices were randomly chosen according to the normal distribution N (0.5, 0.5).</p><p>Figures <ref type="figure">7,</ref> <ref type="figure">8</ref>, 9, and 10 show the resulting cluster structures and the representative roles of the clusters. Comparing these figures with the results in Figures <ref type="figure">3,</ref> <ref type="figure">4,</ref> <ref type="figure">5</ref>, and 6, we can observe that the previous NNMFbased method could not capture the structures of the input and output dimension values in as much detail as our proposed method, since it does not take the sign information into account. Furthermore, with the NNMFbased method, we should define the number of clusters in advance, and we cannot observe the hierarchical structure of clusters to find the optimal resolution for interpreting the roles of partial structures of an LNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 2: Effect of Sign Alignment of Feature Vectors</head><p>Here, we discuss the effect of the sign alignment of the feature vectors based on cosine similarity (Algorithm 1).</p><p>Figures <ref type="figure">11</ref> shows the effect of the sign alignment of the feature vectors extracted from an LNN trained with the MNIST data set <ref type="bibr" target="#b10">[11]</ref>. The left and center figures, respectively, show the feature vectors before and after the alignment of the signs. The right figure shows the monotonic increase of the sum of the cosine similarities through the alignment algorithm. Figure <ref type="figure">12</ref> shows the dendrograms of the hierarchical clustering results with the original feature vectors of Definition 3 and with the feature vectors after the alignment of the signs. From this figure, we can observe that the height of the dendrogram, which shows the similarity of all the hidden layer units, is higher with the original feature vectors than with the feature vectors after the sign alignment. In other words, it was shown that the algorithm successfully aligned the feature vectors so that they became similar to each other. Figures <ref type="figure">13</ref> and <ref type="figure">14</ref> show the effect of the sign alignment of the feature vectors extracted from an LNN trained with the food consumer price index data set <ref type="bibr" target="#b6">[7]</ref>. These figures show similar results to those of the MNIST data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 3: Experimental Settings</head><p>Here, we detail the experimental settings. E1 and E2, respectively, represent the settings of the experiments described in sections 4.1 and 4.2.</p><p>-The training sample size n 1 was: 500 per class (E1), and 270 (E2).</p><p>-We normalized the input data so that the minimum and maximum values of an element, respectively, were -1 and 1. Similarly, we normalized the output data so that the minimum and maximum values of an element, respectively, were 0.01 and 0.99.</p><p>-The mean iteration number for LNN training per dataset a 1 was: 100 per class (E1), and 500 (E2).</p><p>-We generated the initial connection weights and biases of a layered neural network as follows: ω d , • • • , Z (1)  n1 , • • • , Z (10)  n1 , Z -The weight removing hyperparameter ξ was: 0.6 (E1), and 0.001 (E2) In Figures <ref type="figure">3</ref> and <ref type="figure">5</ref>, we only draw connections where the absolute values of weights were ξ or more.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2017 Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards better understanding of gradient-based attribution methods for deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting tree-structured representations of trained networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<title level="m">Towards a rigorous science of interpretable machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Consumer price index of food nationwide from January</title>
		<author>
			<persName><surname>Stat</surname></persName>
		</author>
		<ptr target="https://www.e-stat.go.jp/dbview?sid=0003143513" />
		<imprint>
			<date type="published" when="1970-01">1970 to January 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A structural connectionist learning algorithm with forgetting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Japanese Society for Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="595" to="603" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evolving decision trees using oracle guides</title>
		<author>
			<persName><forename type="first">U</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Symposium on Computational Intelligence and Data Mining</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="238" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting decision trees from trained neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1999" to="2009" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning</title>
		<meeting>the 2016 ICML Workshop on Human Interpretability in Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the representation and computation of multilayer perceptrons: A case study in speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nagamine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2564" to="2573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SVCCA: Singular vector canonical correlation analysis for deep learning dynamics and interpretability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6076" to="6085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2014 Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015 Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Treeview: Peeking into deep neural networks via feature-space partitioning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ramamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical grouping to optimize an objective function</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modular representation of autoencoder networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE Symposium on Deep Learning</title>
		<meeting>2017 IEEE Symposium on Deep Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recursive extraction of modular structure from layered neural networks using variational Bayes method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Discovery Science</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>Discovery Science</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">10558</biblScope>
			<biblScope unit="page" from="207" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Knowledge discovery from layered neural networks based on non-negative task decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07137v2</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modular representation of layered neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="62" to="73" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04778</idno>
		<title level="m">Understanding community structure in layered neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Beyond regression : new tools for prediction and analysis in the behavioral sciences</title>
		<author>
			<persName><forename type="first">P</forename><surname>Werbos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graying the black box: Understanding DQNs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ben-Zrihem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1899" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
