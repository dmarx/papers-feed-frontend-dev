<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEA02022CC582A67E75465AB42DA72A8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bibliography</head><p>Linear Model and Extensions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Statistical properties of under-fitted OLS</head><p>and cov(ε) = σ 2 I n . However, we only fit the OLS of Y on X 2 with coefficient β2 and estimated variance σ2 . Show that</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acronyms</head><p>I try hard to avoid using acronyms to reduce the unnecessary burden for reading. The following are standard and will be used repeatedly.</p><p>ANOVA <ref type="bibr">(Fisher's)</ref> analysis of variance CLT central limit theorem CV cross-validation EHW Eicker-Huber-White (robust covariance matrix or standard error) FWL Frisch-Waugh-Lovell (theorem) GEE generalized estimating equation GLM generalized linear model HC heteroskedasticity-consistent (covariance matrix or standard error) IID independent and identically distributed LAD least absolute deviations lasso least absolute shrinkage and selection operator MLE maximum likelihood estimate OLS ordinary least squares RSS residual sum of squares WLS weighted least squares Symbols All vectors are column vectors as in R unless stated otherwise. Let the superscript " t " denote the transpose of a vector or matrix. a ∼ approximation in distribution R the set of all real numbers β regression coefficient ε error term H hat matrix H = X(X t X) -1 X t h ii leverage score: the (i, i)the element of the hat matrix H I n identity matrix of dimension n × n x i covariate vector for unit i X covariate matrix Y outcome vector y i outcome for unit i independence and conditional independence xv Useful R packages This book uses the following R packages and functions. package function or data use car hccm Eicker-Huber-White robust standard error linearHypothesis testing linear hypotheses in linear models foreign read.dta read stata data gee gee Generalized estimating equation HistData GaltonFamilies Galton's data on parents' and children's heights MASS lm.ridge ridge regression glm.nb Negative-Binomial regression glmnet cv.glmnet Lasso with cross-validation mlbench BostonHousing Boston housing data polr proportional odds logistic regression Matching lalonde LaLonde data nnet multinom Multinomial logistic regression quantreg rq quantile regression survival coxph Cox proportional hazards regression survdiff log rank test survfit Kaplan-Meier curve xvii</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preface</head><p>The importance of studying the linear model A central task in statistics is to use data to build models to make inferences about the underlying data-generating processes or make predictions of future observations. Although real problems are very complex, the linear model can often serve as a good approximation to the true data-generating process. Sometimes, although the true data-generating process is nonlinear, the linear model can be a useful approximation if we properly transform the data based on prior knowledge. Even in highly nonlinear problems, the linear model can still be a useful first attempt in the data analysis process.</p><p>Moreover, the linear model has many elegant algebraic and geometric properties. Under the linear model, we can derive many explicit formulas to gain insights about various aspects of statistical modeling. In more complicated models, deriving explicit formulas may be impossible. Nevertheless, we can use the linear model to build intuition and make conjectures about more complicated models.</p><p>Pedagogically, the linear model serves as a building block in the whole statistical training. This book builds on my lecture notes for a master's level "Linear Model" course at UC Berkeley, taught over the past eight years. Most students are master's students in statistics. Some are undergraduate students with strong technical preparations. Some are Ph.D. students in statistics. Some are master's or Ph.D. students in other departments. This book requires the readers to have basic training in linear algebra, probability theory, and statistical inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recommendations for instructors</head><p>This book has twenty-seven chapters in the main text and four chapters as the appendices. As I mentioned before, this book grows out of my teaching of "Linear Model" at UC Berkeley. In different years, I taught the course in different ways, and this book is a union of my lecture notes over the past eight years. Below I make some recommendations for instructors based on my own teaching experience. Since UC Berkeley is on the semester system, instructors on the quarter system should make some adjustments to my recommendations below.  Preface my teaching assistants to review the appendices in the first two lab sessions and assigned homework problems from the appendices to remind the students to review the background materials. Then you can cover Chapters 2-24. You can omit Chapter 18 and some sections in other chapters due to their technical complications. If time permits, you can consider covering Chapter 25 due to the importance of the generalized estimating equation as well as its byproduct called the "cluster-robust standard error", which is important for many social science applications. Furthermore, you can consider covering Chapter 27 due to the importance of the Cox proportional hazards model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part</head><p>I Introduction 1 Motivations for Statistical Models 1.1 Data and statistical models A wide range of problems in statistics and machine learning have the data structure as below: Unit outcome/response covariates/features/predictors i Y X 1 X 2 • • • X p 1 y 1 x 11 x 12 • • • x 1p 2 y 2 x 21 x 22 • • • x 2p . . . . . . . . . . . . . . . n y n x n1 x n2 • • • x np</p><p>For each unit i, we observe the outcome/response of interest, y i , as well as p covariates/features/predictors. We often use</p><formula xml:id="formula_0">Y =      y 1 y 2</formula><p>. . .</p><formula xml:id="formula_1">y n     </formula><p>to denote the n-dimensional outcome/response vector, and</p><formula xml:id="formula_2">X =      x 11 x 12 • • • x 1p x 21 x 22 • • • x 2p . . . . . . . . . x n1 x n2 • • • x np     </formula><p>to denote the n × p covariate/feature/predictor matrix, also called the design matrix. In most cases, the first column of X contains constants 1s.</p><p>Based on the data (X, Y ) , we can ask the following questions:</p><p>(Q1) Describe the relationship between X and Y , i.e., their association or correlation. For example, how is the patients' average height related to the children's average height? How is one's height related to one's weight? How are one's education and working experience related to one's income?</p><p>(Q2) Predict Y * based on new data X * . In particular, we want to use the current data (X, Y ) to train a predictor, and then use it to predict future Y * based on future X * . This is called supervised learning in the field of machine learning. For example, how do we predict whether an email is spam or not based on the frequencies of the most commonly occurring words and punctuation marks in the email? How do we predict cancer patients' survival time based on their clinical measures? (Q3) Estimate the causal effect of some components in X on Y . What if we change some components of X? How do we measure the impact of the hypothetical intervention of some components of X on Y ? This is a much harder question because most statistical tools are designed to infer association, not causation. For example, the U.S. Food and Drug Administration (FDA) approves drugs based on randomized controlled trials (RCT) because RCTs are most credible to infer causal effects of drugs on health outcomes. Economists are interested in evaluating the effect of a job training program on employment and wages. However, this is a notoriously difficult problem with only observational data.</p><p>The above descriptions are about generic X and Y , which can be many different types. We often use different statistical models to capture the features of different types of data. I give a brief overview of models that will appear in later parts of this book.</p><p>(T1) X and Y are univariate and continuous. In Francis Galton's<ref type="foot" target="#foot_0">foot_0</ref> classic example, X is the parents' average height and Y is the children's average height <ref type="bibr" target="#b120">(Galton, 1886)</ref>. Galton derived the following formula:</p><formula xml:id="formula_3">y = ȳ + ρ σy σx (x -x)</formula><p>which is equivalent to </p><formula xml:id="formula_4">y - ȳ σy = ρ x - x σx , (<label>1</label></formula><formula xml:id="formula_5">(x i -x)(y i -ȳ).</formula><p>This is the famous formula of "regression towards mediocrity" or "regression towards the mean". Galton first introduced the terminology "regression." Galton called regression because the relative deviation of the children's average height is smaller than that of the parents' average height if |ρ| &lt; 1. We will derive the above Galtonian formula in Chapter 2. The name "regression" is widely used in statistics now. For instance, we sometimes use "linear regression" interchangeably with "linear model"; we also extend the name to "logistic regression" or "Cox regression" which will be discussed in later chapters of this book.</p><p>(T2) Y univariate and continuous, and X multivariate of mixed types. In the R package ElemStatLearn, the dataset prostate has an outcome of interest as the log of the prostatespecific antigenlpsa and some potential predictors including the log cancer volume</p><p>lcavol, the log prostate weight lweight, age age, etc. (T3) Y binary or indicator of two classes, and X multivariate of mixed types. For example, in the R package wooldridge, the dataset mroz contains an outcome of interest being the binary indicator for whether a woman was in the labor force in 1975, and some useful covariates are covariate name covariate meaning kidslt6 number of kids younger than six years old kidsge6 number of kids between six and eighteen years old age age educ years of education husage husband's age huseduc husband's years of education (T4) Y categorical without ordering. For example, the choice of housing type, single-family house, townhouse, or condominium, is a categorical variable.</p><p>(T5) Y categorical and ordered. For example, the final course evaluation at UC Berkeley can take value in {1, 2, 3, 4, 5, 6, 7}. These numbers have clear ordering but they are not the usual real numbers.</p><p>(T6) Y counts. For example, the number of times one went to the gym last week is a non-negative integer representing counts.</p><p>(T7) Y time-to-event outcome. For example, in medical trials, a major outcome of interest is the survival time; in labor economics, a major outcome of interest is the time to find the next job. The former is called survival analysis in biostatistics and the latter is called duration analysis in econometrics.</p><p>(T8) Y multivariate and correlated. In medical trials, the data are often longitudinal, meaning that the patient's outcomes are measured repeatedly over time. So each patient has a multivariate outcome. In field experiments of public health and development economics, the randomized interventions are often at the village level but the outcome data are collected at the household level. So within villages, the outcomes are correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Why linear models?</head><p>Why do we study linear models if most real problems may have nonlinear structures? There are important reasons.</p><p>(R1) Linear models are simple but non-trivial starting points for learning.</p><p>(R2) Linear models can provide insights because we can derive explicit formulas based on elegant algebra and geometry.</p><p>(R3) Linear models can handle nonlinearity by incorporating nonlinear terms, for example, X can contain the polynomials or nonlinear transformations of the original covariates. In statistics, "linear" often means linear in parameters, not necessarily in covariates.</p><p>(R4) Linear models can be good approximations to nonlinear data-generating processes.</p><p>(R5) Linear models are simpler than nonlinear models, but they do not necessarily perform worse than more complicated nonlinear models. We have finite data so we cannot fit arbitrarily complicated models.</p><p>If you are interested in nonlinear models, you can take another machine learning course.</p><p>Ordinary Least Squares (OLS) with a Univariate Covariate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Univariate ordinary least squares</head><p>Figure <ref type="figure" target="#fig_1">2</ref>.1 shows the scatterplot of Galton's dataset which can be found in the R package HistData as GaltonFamilies. In this dataset, father denotes the height of the father and mother denotes the height of the mother. The x-axis denotes the mid-parent height, calculated as (father + 1.08*mother)/2, and the y-axis denotes the height of a child. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q fitted line: y=22.64+0.64x  With n data points (x i, y i ) n i=1 , our goal is to find the best linear fit of the data</p><formula xml:id="formula_6">(x i, ŷi = α + βx i ) n i=1 .</formula><p>What do we mean by the "best" fit? Gauss proposed to use the following criterion, called</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Model and Extensions</head><p>the ordinary least squares (OLS) 1 :</p><p>(α, β) = arg min</p><formula xml:id="formula_7">a,b n -1 n i=1 (y i -a -bx i ) 2 .</formula><p>The OLS criterion is based on the squared "misfits" y i -a -bx i . Another intuitive criterion is based on the absolute values of those misfits, which is called the least absolute deviation (LAD). However, OLS is simpler because the objective function is smooth in (a, b). We will discuss LAD in Chapter 26.</p><p>How to solve the OLS minimization problem? The objective function is quadratic, and as a and b diverge, it diverges to infinity. So it must has a unique minimizer ( α, β) which satisfies the first-order condition:</p><formula xml:id="formula_8">-2 n n i=1 (y i -α -βx i ) = 0, -2 n n i=1 x i (y i -α -βx i ) = 0.</formula><p>These two equations are called the Normal Equations of OLS. The first equation implies ȳ = α + β x, <ref type="bibr">(2.1)</ref> that is, the OLS line must go through the sample mean of the data (x, ȳ). The second equation implies</p><formula xml:id="formula_9">xy = αx + βx 2 ,<label>(2.2)</label></formula><p>where xy is the sample mean of the x i y i 's, and x 2 is the sample mean of the x 2 i 's. Subtracting (2.1)×x from (2.2), we have</p><formula xml:id="formula_10">xy -xȳ = β(x 2 -x2 )</formula><p>=⇒ σxy = β σ2</p><p>x =⇒ β = σxy σ2</p><p>x .</p><p>So the OLS coefficient of x equals the sample covariance between x and y divided by the sample variance of x. From (2.1), we obtain that α = ȳ -β x.</p><p>Finally, the fitted line is</p><formula xml:id="formula_11">y = α + βx = ȳ -β x + βx =⇒ y -ȳ = β(x -x)</formula><p>=⇒ y -ȳ = σxy σ2</p><p>x (x -x) = ρxy σx σy σ2</p><p>x</p><formula xml:id="formula_12">(x -x) =⇒ y - ȳ σy = ρxy x - x σx ,</formula><p>which is the Galtonian formula mentioned in Chapter 1.</p><p>We can obtain the fitted line based on Galton's data using the R code below.</p><p>1 The idea of OLS is often attributed to Gauss and Legendre. Gauss used it in the process of discovering Ceres, and his work was published in 1809. Legendre's work appeared in 1805 but Gauss claimed that he had been using it since 1794 or 1795. <ref type="bibr" target="#b212">Stigler (1981)</ref> reviews the history of OLS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Final comments</head><p>We can write the sample mean as the solution to the OLS with only the intercept:</p><formula xml:id="formula_13">ȳ = arg min µ n -1 n i=1 (y i -µ) 2 .</formula><p>It is rare to fit OLS of y i on x i without the intercept: where x and y are the n-dimensional vectors containing all observations, and ⟨x, y⟩ = n i=1 x i y i denotes the inner product. Although not directly useful, this formula will be the building block for many discussions later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pairwise slopes</head><p>Given (x i , y i ) n i=1 with univariate x i and y i , show that Galton's slope equals β =</p><p>(i,j)</p><formula xml:id="formula_14">w ij b ij ,</formula><p>where the summation is over all pairs of observations (i, j), b ij = (y i -y j )/(x i -x j )</p><p>is the slope determined by two points (x i , y i ) and (x j , y j ), and</p><formula xml:id="formula_15">w ij = (x i -x j ) 2 / (i ′ ,j ′ ) (x i ′ -x j ′ ) 2</formula><p>is the weight proportional to the squared distance between x i and x j . In the above formulas, we define b ij = 0 if x i = x j . Remark: <ref type="bibr" target="#b244">Wu (1986)</ref> and <ref type="bibr" target="#b124">Gelman and Park (2009)</ref> used this formula. Problem 3.9 gives a more general result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part II</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OLS and Statistical Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OLS with Multiple Covariates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The OLS formula</head><p>Recall that we have the outcome vector</p><formula xml:id="formula_16">Y =      y 1 y 2</formula><p>. . .</p><formula xml:id="formula_17">y n     </formula><p>and covariate matrix</p><formula xml:id="formula_18">X =      x 11 x 12 • • • x 1p x 21 x 22 • • • x 2p . . . . . . . . . x n1 x n2 • • • x np      =      x t 1 x t 2 . . . x t n      = (X 1 , . . . , X p )</formula><p>where x t i = (x i1 , . . . , x ip ) is the row vector consisting of the covariates of unit i, and X j = (x 1j , . . . , x nj ) t is the column vector of the j-th covariate for all units.</p><p>We want to find the best linear fit of the data (x i , ŷi ) n i=1 with ŷi = x t i β = β1 x i1 + • • • + βp x ip in the sense that</p><formula xml:id="formula_19">β = arg min b n -1 n i=1 (y i -x t i b) 2 = arg min b n -1 ∥Y -Xb∥ 2 ,</formula><p>where β is called the OLS coefficient, the ŷi 's are called the fitted values, and the y i -ŷi 's are called the residuals. The objective function is quadratic in b which diverges to infinity when b diverges to infinity. So it must have a unique minimizer β satisfying the first order condition -2 n n i=1</p><p>x i (y i -x t i β) = 0, which simplifies to n i=1</p><p>x i (y i -x t i β) = 0 ⇐⇒ X t (Y -X β) = 0.</p><p>(3.1)</p><p>The above equation (3.1) is called the Normal equation of the OLS, which implies the main theorem:</p><p>Linear Model and Extensions Theorem 3.1 The OLS coefficient equals</p><formula xml:id="formula_20">β = n i=1 x i x t i -1 n i=1 x i y i = (X t X) -1 X t Y if X t X = n i=1</formula><p>x i x t i is non-degenerate. The equivalence of the two forms of the OLS coefficient follows from</p><formula xml:id="formula_21">X t X = (x 1 , . . . , x n )      x t 1 x t 2 . . . x t n      = n i=1 x i x t i ,</formula><p>and</p><formula xml:id="formula_22">X t Y = (x 1 , . . . , x n )      y 1 y 2 . . . y n      = n i=1</formula><p>x i y i .</p><p>For different purposes, both forms can be useful. The non-degeneracy of X t X in Theorem 3.1 requires that for any non-zero vector α ∈ R p , we must have α t X t Xα = ∥Xα∥ 2 ̸ = 0 which is equivalent to Xα ̸ = 0, i.e., the columns of X are linearly independent<ref type="foot" target="#foot_1">foot_1</ref> . This effectively rules out redundant columns in the design matrix X. If X 1 can be represented by other columns X 1 = c 2 X 2 + • • • + c p X p for some (c 2 , . . . , c p ), then X t X is degenerate.</p><p>Throughout the book, we invoke the following condition unless stated otherwise.</p><p>Condition 3.1 The column vectors of X are linearly independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The geometry of OLS</head><p>The OLS has very clear geometric interpretations. Figure <ref type="figure" target="#fig_8">3</ref>.1 illustrate its geometry with n = 3 and p = 2. For any b = (b 1 , . . . , b p ) t ∈ R p and X = (X 1 , . . . , X p ) ∈ R n×p ,</p><formula xml:id="formula_23">Xb = b 1 X 1 + • • • + b p X p</formula><p>represents a linear combination of the column vectors of the design matrix X. So the OLS problem is to find the best linear combination of the column vectors of X to approximate the response vector Y . Recall that all linear combinations of the column vectors of X constitute FIGURE 3.1: The geometry of OLS the column space of X, denoted by C(X) 2 . So the OLS problem is to find the vector in C(X) that is the closest to Y . Geometrically, the vector must be the projection of Y onto C(X).</p><p>By projection, the residual vector ε = Y -X β must be orthogonal to C(X), or, equivalently, the residual vector is orthogonal to X 1 , . . . , X p . This geometric intuition implies that X t 1 ε = 0, . . . , X t p ε = 0,</p><formula xml:id="formula_24">⇐⇒ X t ε =    X t 1 ε . . . X t p ε   = 0, ⇐⇒ X t (Y -X β) = 0,</formula><p>which is essentially the Normal equation (3.1). The above argument gives a geometric derivation of the OLS formula in Theorem 3.1.</p><p>In Figure <ref type="figure" target="#fig_8">3</ref>.1, since the triangle ABC is rectangular, the fitted vector Ŷ = X β is orthogonal to the residual vector ε, and moreover, the Pythagorean Theorem implies that ∥Y ∥ 2 = ∥X β∥ 2 + ∥ε∥ 2 .</p><p>In most applications, X contains a column of intercepts 1 n = (1, . . . , 1) t . In those cases, we have</p><formula xml:id="formula_25">1 t n ε = 0 =⇒ n -1 n i=1 εi = 0,</formula><p>2 Please review Chapter A for some basic linear algebra background.</p><p>so the residuals are automatically centered.</p><p>The following theorem states an algebraic fact that gives an alternative proof of the OLS formula. It is essentially the Pythagorean Theorem for the rectangular triangle BCD in Figure <ref type="figure" target="#fig_8">3</ref>.1. Theorem 3.2 For any b ∈ R p , we have the following decomposition</p><formula xml:id="formula_26">∥Y -Xb∥ 2 = ∥Y -X β∥ 2 + ∥X( β -b)∥ 2 ,</formula><p>where implies that ∥Y -Xb∥ 2 ≥ ∥Y -X β∥ 2 with equality holding if and only if b = β.</p><p>Proof of Theorem 3.2: We have the following decomposition:</p><formula xml:id="formula_27">∥Y -Xb∥ 2 = (Y -Xb) t (Y -Xb) = (Y -X β + X β -Xb) t (Y -X β + X β -Xb) = (Y -X β) t (Y -X β) + (X β -Xb) t (X β -Xb) +(Y -X β) t (X β -Xb) + (X β -Xb) t (Y -X β).</formula><p>The first term equals ∥Y -X β∥ 2 and the second term equals ∥X( β -b)∥ 2 . We need to show the last two terms are zero. By symmetry of these two terms, we only need to show that the last term is zero. This is true by the Normal equation (3.1) of the OLS:</p><formula xml:id="formula_28">(X β -Xb) t (Y -X β) = ( β -b) t X t (Y -X β) = 0. □</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The projection matrix from OLS</head><p>The geometry in Section 3.2 also shows that Ŷ = X β is the solution to the following problem Ŷ = arg min v∈C(X) ∥Y -v∥ 2 .</p><p>Using Theorem 3.1, we have Ŷ = X β = HY , where H = X(X t X) -1 X t is an n × n matrix. It is called the hat matrix because it puts a hat on Y when multiplying Y . Algebraically, we can show that H is a projection matrix because</p><formula xml:id="formula_29">H 2 = X(X t X) -1 X t X(X t X) -1 X t = X(X t X) -1 X t = H, and H t = X(X t X) -1 X t t = X(X t X) -1 X t = H.</formula><p>Its rank equals its trace, so equals rank(H) = trace(H) = trace X(X t X) -1 X t = trace (X t X) -1 X t X = trace(I p ) = p.</p><p>The projection matrix H has the following geometric interpretations.</p><p>Proposition 3.1 The projection matrix H = X(X t X) -1 X t satisfies (G1) Hv = v ⇐⇒ v ∈ C(X);</p><p>(G2) Hw = 0 ⇐⇒ w ⊥ C(X).</p><p>Recall that C(X) is the column space of X. (G1) states that projecting any vector in C(X) onto C(X) does not change the vector, and (G2) states that projecting any vector orthogonal to C(X) onto C(X) results in a zero vector. Proof of Proposition 3.1: I first prove (G1). If v ∈ C(X), then v = Xb for some b, which implies that Hv = X(X t X) -1 X t Xb = Xb = v. Conversely, if v = Hv, then v = X(X t X) -1 X t v = Xu with u = (X t X) -1 X t v, which ensures that v ∈ C(X).</p><p>I then prove (G2). If w ⊥ C(X), then w is orthogonal to all column vectors of X. So X t j w = 0 (j = 1, . . . , p) =⇒ X t w = 0 =⇒ Hw = X(X t X) -1 X t w = 0.</p><p>Conversely, if Hw = X(X t X) -1 X t w = 0, then w t X(X t X) -1 X t w = 0. Because (X t X) -1 is positive definite, we have X t w = 0 ensuring that w ⊥ C(X).</p><p>□ Writing H = (h ij ) 1≤i,j≤n and ŷ = (ŷ 1 , . . . , ŷn ) t , we have another basic identity ŷi = n j=1 h ij y j = h ii y i + j̸ =i h ij y j .</p><p>It shows that the predicted value ŷi is a linear combination of all the outcomes. Moreover, if X contains a column of intercepts 1 n = (1, . . . , 1) t , then</p><formula xml:id="formula_30">H1 n = 1 n =⇒ n j=1 h ij = 1 (i = 1, . . . , n),</formula><p>which implies that ŷi is a weighted average of all the outcomes. Although the sum of the weights is one, some of them can be negative.</p><p>In general, the hat matrix has complex forms, but when the covariates are dummy variables, it has more explicit forms. I give two examples below.</p><p>Example 3.1 In a treatment-control experiment with m treated and n control units, the matrix X contains 1 and a dummy variable for the treatment:</p><formula xml:id="formula_31">X = 1 m 1 m 1 n 0 n .</formula><p>We can show that</p><formula xml:id="formula_32">H = diag{m -1 1 m 1 t m , n -1 1 n 1 t n }.</formula><p>Example 3.2 In an experiment with n j units receiving treatment level j (j = 1, . . . , J), the covariate matrix X contains J dummy variables for the treatment levels:</p><formula xml:id="formula_33">X = diag{1 n1 , . . . , 1 n J }.</formula><p>We can show that H = diag{n -1 1 1 n1 1 t n1 , . . . , n -1 J 1 n J 1 t n J }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Univariate and multivariate OLS</head><p>Derive the univariate OLS based on the multivariate OLS formula with</p><formula xml:id="formula_34">X =    1 x 1 . . . . . . 1 x n   </formula><p>where the x i 's are scalars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OLS via vector and matrix calculus</head><p>Using vector and matrix calculus, show that the OLS estimator minimizes (Y -Xb) t (Y -Xb).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OLS based on pseudo inverse</head><p>Show that β = X + Y . Remark: Recall the definition of the pseudo inverse in Chapter A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Invariance of OLS</head><p>Assume that X t X is non-degenerate and Γ is a p×p non-degenerate matrix. Define X = XΓ.</p><p>From the OLS fit of Y on X, we obtain the coefficient β, the fitted value Ŷ , and the residual ε; from the OLS fit of Y on X, we obtain the coefficient β, the fitted value Ỹ , and the residual ε.</p><p>Prove that β = Γ β, Ŷ = Ỹ , ε = ε.</p><p>Remark: From a linear algebra perspective, X and XΓ have the same column space if Γ is a non-degenerate matrix:</p><formula xml:id="formula_35">{Xb : b ∈ R p } = {XΓc : c ∈ R p }.</formula><p>Consequently, there must be a unique projection of Y onto the common column space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Invariance of the hat matrix</head><p>Show that H does not change if we change X to XΓ where Γ ∈ R p×p is a non-degenerate matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Special hat matrices</head><p>Verify the formulas of the hat matrices in Examples 3.1 and 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">OLS with multiple responses</head><p>For each unit i = 1, . . . , n, we have multiple responses y i = (y i1 , . . . , y iq ) t ∈ R q and multiple covariates x i = (x i1 , . . . , x ip ) t ∈ R p . Define</p><formula xml:id="formula_36">Y =    y 11 • • • y 1q . . . . . . y n1 • • • y nq    =    y t 1 . . . y t n    = (Y 1 , . . . , Y q ) ∈ R n×q and X =    x 11 • • • x 1p . . . . . . x n1 • • • x np    =    x t 1 . . . x t n    = (X 1 , . . . , X p ) ∈ R n×p</formula><p>as the response and covariate matrices, respectively. Define the multiple OLS coefficient matrix as B = arg min</p><formula xml:id="formula_37">B∈R p×q n i=1 ∥y i -B t x i ∥ 2</formula><p>Show that B = ( B1 , . . . , Bq ) has column vectors</p><formula xml:id="formula_38">B1 = (X t X) -1 X t Y 1 , . . . Bq = (X t X) -1 X t Y q .</formula><p>Remark: This result tells us that the OLS fit with a vector outcome reduces to multiple separate OLS fits, or, the OLS fit of a matrix Y on a matrix X reduces to the column-wise OLS fits of Y on X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Full sample and subsample OLS coefficients</head><p>Partition the full sample into K subsamples:</p><formula xml:id="formula_39">X =    X (1)</formula><p>. . .</p><formula xml:id="formula_40">X (K)    , Y =    Y (1)</formula><p>. . .</p><formula xml:id="formula_41">Y (K)    ,</formula><p>where the kth sample consists of (X (k) , Y (k) ) with X (k) ∈ R n k ×p and Y (k) ∈ R n k being the covariate matrix and outcome vector. Note that n = K k=1 n k Let β be the OLS coefficient based on the full sample, and β(k) be the OLS coefficient based on the kth sample. Show that</p><formula xml:id="formula_42">β = K k=1 W (k) β(k) ,</formula><p>where the weight matrix equals</p><formula xml:id="formula_43">W (k) = (X t X) -1 X t (k) X (k) .</formula><p>3.9 Jacobi's theorem</p><p>The set {1, . . . , n} has n p size-p subsets. Each subset S defines a linear equation for b ∈ R p :</p><formula xml:id="formula_44">Y S = X S b</formula><p>where Y S ∈ R p is the subvector of Y and X S ∈ R p×p is the submatrix of X, corresponding to the units in S. Define the subset coefficient βS = X -1 S Y S if X S is invertible and βS = 0 otherwise. Show that the OLS coefficient equals a weighted average of these subset coefficients:</p><formula xml:id="formula_45">β = S w S βS</formula><p>where the summation is over all subsets and</p><formula xml:id="formula_46">w S = | det(X S )| 2 S ′ | det(X S ′ )| 2 .</formula><p>Remark: To prove this result, we can use Cramer's rule to express the OLS coefficient and use the Cauchy-Binet formula to expand the determinant of X t X. This result extends Problem 2.1. <ref type="bibr" target="#b59">Berman (1988)</ref> attributed it to Jacobi. <ref type="bibr" target="#b244">Wu (1986)</ref> used it in analyzing the statistical properties of OLS.</p><p>The Gauss-Markov Model and Theorem</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gauss-Markov model</head><p>Without any stochastic assumptions, the OLS in Chapter 3 is purely algebraic. From now on, we want to discuss the statistical properties of β and associated quantities, so we need to invoke some statistical modeling assumptions. A simple starting point is the following Gauss-Markov model with a fixed design matrix X and unknown parameters (β, σ 2 ). where the design matrix X is fixed with linearly independent column vectors, and the random error term ε has the first two moments</p><formula xml:id="formula_47">E(ε) = 0, cov(ε) = σ 2 I n .</formula><p>The unknown parameters are (β, σ 2 ).</p><p>The Gauss-Markov model assumes that Y has mean Xβ and covariance matrix σ 2 I n . At the individual level, we can also write it as</p><formula xml:id="formula_48">y i = x t i β + ε i , (i = 1, . . . , n)</formula><p>where the error terms are uncorrelated with mean 0 and variance σ 2 .</p><p>The assumption that X is fixed is not essential, because we can condition on X even if we think X is random. The mean of each y i is linear in x i with the same β coefficient, which is a rather strong assumption. So is the homoskedasticity<ref type="foot" target="#foot_2">foot_2</ref> assumption that the error terms have the same variance σ 2 . The critiques on the assumptions aside, I will derive the properties of β under the Gauss-Markov model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Properties of the OLS estimator</head><p>I first derive the mean and covariance of β = (X t X) -1 X t Y .</p><p>Theorem 4.1 Under Assumption 4.1, we have</p><formula xml:id="formula_49">E( β) = β, cov( β) = σ 2 (X t X) -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Model and Extensions</head><p>Proof of Theorem 4.1: Because E(Y ) = Xβ, we have</p><formula xml:id="formula_50">E( β) = E (X t X) -1 X t Y = (X t X) -1 X t E(Y ) = (X t X) -1 X t Xβ = β.</formula><p>Because cov(Y ) = σ 2 I n , we have</p><formula xml:id="formula_51">cov( β) = cov (X t X) -1 X t Y = (X t X) -1 X t cov(Y )X(X t X) -1 = σ 2 (X t X) -1 X t X(X t X) -1 = σ 2 (X t X) -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>We can decompose the response vector as</p><formula xml:id="formula_52">Y = Ŷ + ε,</formula><p>where the fitted vector is Ŷ = X β = HY and the residual vector is ε = Y -Ŷ = (I n -H)Y. The two matrices H and I n -H are the keys, which have the following properties.</p><p>Lemma 4.1 Both H and I n -H are projection matrices. In particular, HX = X, (I n -H)X = 0, and they are orthogonal:</p><p>H(I n -H) = (I n -H)H = 0.</p><p>These follow from simple linear algebra, and I leave the proof as Problem 4.1. It states that H and I n -H are projection matrices onto the column space of X and its complement. Algebraically, Ŷ and ε are orthogonal by the OLS projection because Lemma 4.1 implies</p><formula xml:id="formula_53">Ŷ t ε = Y t H t (I n -H)Y = Y t H(I n -H)Y = 0.</formula><p>This is also coherent with the geometry in Figure <ref type="figure" target="#fig_8">3</ref>.1.</p><p>Moreover, we can derive the mean and covariance matrix of Ŷ and ε.</p><p>Theorem 4.2 Under Assumption 4.1, we have</p><formula xml:id="formula_54">E Ŷ ε = Xβ 0 and cov Ŷ ε = σ 2 H 0 0 I n -H .</formula><p>So Ŷ and ε are uncorrelated.</p><p>Please do not be confused with the two statements above. First, Ŷ and ε are orthogonal. Second, Ŷ and ε are uncorrelated. They have different meanings. The first statement is an algebraic fact of the OLS procedure. It is about a relationship between two vectors Ŷ and ε which holds without assuming the Gauss-Markov model. The second statement is stochastic. It is about a relationship between two random vectors Ŷ and ε which requires the Gauss-Markov model assumption. Proof of Theorem 4.2: The conclusion follows from the simple fact that</p><formula xml:id="formula_55">Ŷ ε = HY (I n -H)Y = H I n -H Y</formula><p>is a linear transformation of Y . It has mean</p><formula xml:id="formula_56">E Ŷ ε = H I n -H E(Y ) = H I n -H Xβ = HXβ (I n -H) Xβ = Xβ 0 ,</formula><p>and covariance matrix</p><formula xml:id="formula_57">cov Ŷ ε = H I n -H cov(Y ) H t (I n -H) t = σ 2 H I n -H H I n -H = σ 2 H 2 H(I n -H) (I n -H)H (I n -H) 2 = σ 2 H 0 0 I n -H ,</formula><p>where the last step follows from Lemma 4.1. □ Assume the Gauss-Markov model. Although the original responses and error terms are uncorrelated between units with cov(ε i , ε j ) = 0 for i ̸ = j, the fitted values and the residuals are correlated with</p><formula xml:id="formula_58">cov(ŷ i , ŷj ) = σ 2 h ij , cov(ε i , εj ) = σ 2 (1 -h ij )</formula><p>for i ̸ = j based on Theorem 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Variance estimation</head><p>Theorem 4.1 quantifies the uncertainty of β by its covariance matrix. However, it is not directly useful because σ 2 is still unknown. Our next task is to estimate σ 2 based on the observed data. It is the variance of each ε i , but the ε i 's are not observable either. Their empirical analogues are the residuals εi = y i -x t i β. It seems intuitive to estimate σ 2 by σ2 = rss/n where rss = n i=1 ε2 i is the residual sum of squares. However, Theorem 4.2 shows that εi has mean zero and variance σ 2 (1 -h ii ), which is not the same as the variance of original ε i . Consequently, rss has mean</p><formula xml:id="formula_59">E(rss) = n i=1 σ 2 (1 -h ii ) = σ 2 {n -trace(H)} = σ 2 (n -p),</formula><p>which implies the following theorem. i /(n -p).</p><p>Then E(σ 2 ) = σ 2 under Assumption 4.1.</p><p>Theorem 4.3 implies that σ2 is a biased estimator for σ 2 because E(σ 2 ) = σ 2 (n -p)/n. It underestimates σ 2 but with a large sample size n, the bias is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Gauss-Markov Theorem</head><p>So far, we have focused on the OLS estimator. It is intuitive, but we have not answered the fundamental question yet. Why should we focus on it? Are there any other better estimators? Under the Gauss-Markov model, the answer is definite: we focus on the OLS estimator because it is optimal in the sense of having the smallest covariance matrix among all linear unbiased estimators. The following famous Gauss-Markov theorem quantifies this claim, which was named after Carl Friedrich Gauss and Andrey Markov 2 . It is for this reason that I call the corresponding model the Gauss-Markov model. The textbook by <ref type="bibr" target="#b181">Monahan (2008)</ref> also uses this name.</p><p>Theorem 4.4 Under Assumption 4.1, the OLS estimator β for β is the best linear unbiased estimator (BLUE) in the sense that 3 cov( β) ⪰ cov( β) for any estimator β satisfying (C1) β = AY for some A ∈ R p×n not depending on Y ;</p><p>(C2) E( β) = β for any β. 2 David and Neyman (1938) used the name Markoff theorem. <ref type="bibr" target="#b162">Lehmann (1951)</ref> appeared to first use the name Gauss-Markov theorem.</p><p>3 We write M 1 ≻ M 2 is M 1 -M 2 is positive semi-definite. See Chapter A for a review.</p><p>Before proving Theorem 4.4, we need to understand its meaning and immediate implications. We do not compare the OLS estimator with any arbitrary estimators. In fact, we restrict to the estimators that are linear and unbiased. Condition (C1) requires that β is a linear estimator. More precisely, it is a linear transformation of the response vector Y , where A can be any complex and possibly nonlinear function of X. Condition (C2) requires that β is an unbiased estimator for β, no matter what true value β takes.</p><p>Why do we restrict the estimator to be linear? The class of linear estimator is actually quite large because A can be any nonlinear function of X, and the only requirement is that the estimator is linear in Y . The unbiasedness is a natural requirement for many problems. However, in many modern applications with many covariates, some biased estimators can perform better than unbiased estimators if they have smaller variances. We will discuss these estimators in Part V of this book.</p><p>We compare the estimators based on their covariances, which are natural extensions of variances for scalar random variables. The conclusion cov( β) ⪰ cov( β) implies that for any vector c ∈ R p , we have</p><formula xml:id="formula_60">c t cov( β)c ⪰ c t cov( β)c</formula><p>which is equivalent to var(c t β) ≥ var(c t β), So any linear transformation of the OLS estimator has a variance smaller than or equal to the same linear transformation of any other estimator. In particular, if c = (0, . . . , 1, . . . , 0) t with only the jth coordinate being 1, then the above inequality implies that var( βj ) ≥ var( βj ), (j = 1, . . . , p).</p><p>So the OLS estimator has a smaller variance than other estimators for all coordinates. Now we prove the theorem. Proof of Theorem 4.4: We must verify that the OLS estimator itself satisfies (C1) and (C2). We have β = ÂY with Â = (X t X) -1 X t , and it is unbiased by Theorem 4.1.</p><p>First, the unbiasedness requirement implies that</p><formula xml:id="formula_61">E( β) = β =⇒ E(AY ) = AE(Y ) = AXβ = β =⇒ AXβ = β</formula><p>for any value of β. So AX = I p (4.1) must hold. In particular, the OLS estimator satisfies ÂX = (X t X) -1 X t X = I p . Second, we can decompose the covariance of β as</p><formula xml:id="formula_62">cov( β) = cov( β + β -β) = cov( β) + cov( β -β) + cov( β, β -β) + cov( β -β, β).</formula><p>The last two terms are in fact zero. By symmetry, we only need to show that the third term is zero:</p><formula xml:id="formula_63">cov( β, β -β) = cov ÂY, (A -Â)Y = Âcov(Y )(A -Â) t = σ 2 Â(A -Â) t = σ 2 ( ÂA t -Â Ât ) = σ 2 (X t X) -1 X t A t -(X t X) -1 X t X(X t X) -1 = σ 2 (X t X) -1 I p -(X t X) -1</formula><p>(by (4.1)) = 0. They hold only when β is BLUE. They do not hold when comparing two general estimators. Theorem 4.4 is elegant but abstract. It says that in some sense, we can just focus on the OLS estimator because it is the best one in terms of the covariance among all linear unbiased estimators. Then we do not need to consider other estimators. However, we have not mentioned any other estimators for β yet, which makes Theorem 4.4 not concrete enough. From the proof above, a linear unbiased estimator β = AY only needs to satisfy AX = I p , which imposes p 2 constraints on the p × n matrix A. Therefore, we have p(n -p) free parameters to choose from and have infinitely many linear unbiased estimators in general. A class of linear unbiased estimators discussed more thoroughly in Chapter 19, are the weighted least squares estimators</p><formula xml:id="formula_64">β = (X t Σ -1 X) -1 X t Σ -1 Y,</formula><p>where Σ is a positive definite matrix not depending on Y such that Σ and X t Σ -1 X are invertible. It is linear, and we can show that it is unbiased for β:</p><formula xml:id="formula_65">E( β) = E (X t Σ -1 X) -1 X t Σ -1 Y = (X t Σ -1 X) -1 X t Σ -1 Xβ = β.</formula><p>Different choices of Σ give different β, but Theorem 4.4 states that the OLS estimator with Σ = I n has the smallest covariance matrix under the Gauss-Markov model.</p><p>I will give an extension and some applications of the Gauss-Markov Theorem as homework problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Projection matrices</head><p>Prove Lemma 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Univariate OLS and the optimal design</head><p>Assume the Gauss-Markov model y i = α + βx i + ε i (i = 1, . . . , n) with a scalar x i . Show that the variance of the OLS coefficient for x i equals var( β) = σ 2 n i=1 (x i -x) 2 .</p><p>Assume x i must be in the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. We want to choose their values to minimize var( β). Assume that n is an even number. Find the minimizers x i 's.</p><p>Hint: You may find the following probability result useful. For a random variable ξ in the interval [0, 1], we have the following inequality</p><formula xml:id="formula_66">var(ξ) = E(ξ 2 ) -{E(ξ)} 2 ≤ E(ξ) -{E(ξ)} 2 = E(ξ){1 -E(ξ)} ≤ 1/4.</formula><p>The first inequality becomes an equality if and only if ξ = 0 or 1; the second inequality becomes an equality if and only if E(ξ) = 1/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BLUE estimator for the mean</head><p>Assume that y i has mean µ and variance σ 2 , and y i (i = 1, . . . , n) are uncorrelated. A linear estimator of the mean µ has the form μ = n i=1 a i y i , which is unbiased as long as n i=1 a i = 1. So there are infinitely many linear unbiased estimators for µ. Find the BLUE for µ and prove why it is BLUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Consequence of useless regressors</head><p>Partition the covariate matrix and parameter into</p><formula xml:id="formula_67">X = (X 1 , X 2 ), β = β 1 β 2 ,</formula><p>where X 1 ∈ R n×k , X 2 ∈ R n×l , β 1 ∈ R k and β 2 ∈ R l with k + l = p. Assume the Gauss-Markov model with β 2 = 0. Let β1 be the first k coordinates of β = (X t X) -1 X t Y and β1 = (X t 1 X 1 ) -1 X t 1 Y be the coefficient based on the partial OLS fit of Y on X 1 only. Show that cov( β1 ) ⪰ cov( β1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Simple average of subsample OLS coefficients</head><p>Inherit the setting of Problem 3.8. Define the simple average of the subsample OLS coefficients as β = K -1 K k=1 β(k) . Assume the Gauss-Markov model. Show that cov( β) ⪰ cov( β).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Gauss-Markov theorem for prediction</head><p>Under Assumption 4.1, the OLS predictor Ŷ = X β for the mean Xβ is the best linear unbiased predictor in the sense that cov( Ỹ ) ⪰ cov( Ŷ ) for any predictor Ỹ satisfying (C1) Ỹ = HY for some H ∈ R n×n not depending on Y ;</p><p>(C2) E( Ỹ ) = Xβ for any β.</p><p>Prove this theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Nonlinear unbiased estimator under the Gauss-Markov model</head><p>Under Assumption 4.1, prove that if X t Q j X = 0, trace(Q j ) = 0, (j = 1, . . . , p)</p><p>Linear Model and Extensions</p><formula xml:id="formula_68">then β = β +    Y t Q 1 Y . . . Y t Q p Y   </formula><p>is unbiased for β. Remark: The above estimator β is a quadratic function of Y . It is a nonlinear unbiased estimator for β. It is not difficult to show the unbiasedness. More remarkably, <ref type="bibr">Koopmann (1982, Theorem 4.3)</ref> showed that under Assumption 4.1, any unbiased estimator for β must have the form of β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal Linear Model: Inference and Prediction</head><p>Under the Gauss-Markov model, we have calculated the first two moments of the OLS estimator β = (X t X) -1 X t Y :</p><formula xml:id="formula_69">E( β) = β, cov( β) = σ 2 (X t X) -1 ,</formula><p>and have shown that σ2 = εt ε/(n -p) is unbiased for σ 2 , where ε = Y -X β is the residual vector. The Gauss-Markov theorem further ensures that the OLS estimator is BLUE. Although these results characterize the nice properties of the OLS estimator, they do not fully determine its distribution and are thus inadequate for statistical inference. This chapter will derive the joint distribution of ( β, σ2 ) under the Normal linear model with stronger distribution assumptions. where the design matrix X is fixed with linearly independent column vectors. The unknown parameters are (β, σ 2 ).</p><p>We can also write the Normal linear model as a linear function of covariates with error terms: Y = Xβ + ε or, equivalently,</p><formula xml:id="formula_70">y i = x t i β + ε i , (i = 1, . . . , n),</formula><p>where ε ∼ N(0, σ 2 I n ) or ε i iid ∼ N(0, σ 2 ), (i = 1, . . . , n).</p><p>Assumption 5.1 implies Assumption 4.1. Beyond the Gauss-Markov model, it further requires IID Normal error terms. Assumption 5.1 is extremely strong, but it is canonical in statistics. It allows us to derive elegant formulas and also justifies the outputs of the linear regression functions in many statistical packages. I will relax it in Chapter 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Joint distribution of the OLS coefficient and variance estimator</head><p>We first state the main theorem on the joint distribution of ( β, σ2 ) via the joint distribution of ( β, ε).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Model and Extensions</head><p>Theorem 5.1 Under Assumption 5.1, we have</p><formula xml:id="formula_71">β ε ∼ N β 0 , σ 2 (X t X) -1 0 0 I n -H ,</formula><p>and σ2 /σ 2 ∼ χ 2 n-p /(n -p). So β ε, β σ2 .</p><p>Proof of Theorem 5.1: First,</p><formula xml:id="formula_72">β ε = (X t X) -1 X t Y (I n -H)Y = (X t X) -1 X t I n -H Y</formula><p>is a linear transformation of Y , so they are jointly Normal. We have verified their means and variances in Chapter 4, so we only need to show that their covariance is zero:</p><formula xml:id="formula_73">cov( β, ε) = (X t X) -1 X t cov(Y )(I n -H) t = σ 2 (X t X) -1 X t (I n -H t ) = 0</formula><p>which holds because (I n -H)X = 0 by Lemma 4.1. Second, because σ2 = rss/(n -p) = εt ε/(n -p) is a quadratic function of ε, it is independent of β. We only need to show that it is a scaled chi-squared distribution. This follows from Theorem B.10 in Chapter B due to the Normality of ε/σ with the projection matrix I n -H as its covariance matrix.</p><p>□ The second theorem is on the joint distribution of ( Ŷ , ε). We have shown their means and covariance matrix in the last chapter. Because they are linear transformations of Y , they are jointly Normal and independent.</p><p>Theorem 5.2 Under Assumption 5.1, we have</p><formula xml:id="formula_74">Ŷ ε ∼ N Xβ 0 , σ 2 H 0 0 I n -H , so Ŷ ε.</formula><p>Recall that we have shown that Y = Ŷ + ε with Ŷ ⊥ ε by the OLS properties, which is a pure linear algebra fact without assumptions. Theorem 4.2 ensures that Ŷ and ε are uncorrelated under Assumption 4.1. Now Theorem 5.2 further ensures that Ŷ ε under Assumption 5.1. The first result states that Ŷ and ε are orthogonal. The second result states that Ŷ and ε are uncorrelated. The third result states Ŷ and ε are independent. They hold under different assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pivotal quantities and statistical inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Scalar parameters</head><p>We first consider statistical inference for c t β, a one-dimensional linear function of β where c ∈ R p . For example, if c = e j ≡ (0, . . . , 1, . . . , 0) t with only the jth element being one, then c t β = β j is the jth element of β which measures the impact of x ij on y i on average. Standard software packages report statistical inference for each element of β. Sometimes we may also be interested in β j -β j ′ , the difference between the coefficients of two covariates, which corresponds to c = (0, . . . , 0, 1, 0, . . . , 0, -1, 0, . . . , 0) t = e j -e j ′ .</p><p>Theorem 5.1 implies that c t β ∼ N c t β, σ 2 c t (X t X) -1 c .</p><p>However, this is not directly useful because σ 2 is unknown. With σ 2 replaced by σ2 , the standardized distribution</p><formula xml:id="formula_75">T c ≡ c t β -c t β σ2 c t (X t X) -1 c</formula><p>does not follow N(0, 1) anymore. In fact, it is a t distribution as shown in Theorem 5.3 below.</p><p>Theorem 5.3 Under Assumption 5.1, for a fixed vector c ∈ R p , we have</p><formula xml:id="formula_76">T c ∼ t n-p .</formula><p>Proof of Theorem 5.3: From Theorem 5.1, the standardized distribution with the true</p><formula xml:id="formula_77">σ 2 follows c t β -c t β σ 2 c t (X t X) -1 c ∼ N(0, 1), σ2 /σ 2 ∼ χ 2 n-p /(n -p)</formula><p>, and they are independent. These facts imply that</p><formula xml:id="formula_78">T c = c t β -c t β σ2 c t (X t X) -1 c = c t β -c t β σ 2 c t (X t X) -1 c σ2 σ 2 ∼ N(0, 1) χ 2 n-p /(n -p)</formula><p>,</p><p>where N(0, 1) and χ 2 n-p denote independent standard Normal and χ 2 n-p random variables, respectively, with a little abuse of notation. Therefore, T c ∼ t n-p by the definition of the t distribution.</p><p>□ In Theorem 5.3, the left-hand side depends on the observed data and the unknown true parameters, but the right-hand side is a random variable depending on only the dimension (n, p) of X, but neither the data nor the true parameters. We call the quantity on the left-hand side a pivotal quantity. Based on the quantiles of the t n-p random variable, we can tie the data and the true parameter via the following probability statement</p><formula xml:id="formula_79">pr c t β -c t β σ2 c t (X t X) -1 c ≤ t 1-α/2,n-p = 1 -α</formula><p>for any 0 &lt; α &lt; 1, where t 1-α/2,n-p is the 1 -α/2 quantile of t n-p . When n -p is large (e.g. larger than 30), the 1 -α/2 quantile of t n-p is close to that of N(0, 1). In particular, t 97.5%,n-p ≈ 1.96, the 97.5% quantile of N(0, 1), which is the critical value for the 95% confidence interval. Define σ2 c t (X t X) -1 c ≡ ŝe c which is often called the (estimated) standard error of c t β. Using this definition, we can equivalently write the above probability statement as</p><formula xml:id="formula_80">pr c t β -t 1-α/2,n-p ŝe c ≤ c t β ≤ c t β + t 1-α/2,n-p ŝe c = 1 -α.</formula><p>We use</p><formula xml:id="formula_81">c t β ± t 1-α/2,n-p ŝe c</formula><p>as a 1 -α level confidence interval for c t β. By duality of confidence interval and hypothesis testing, we can also construct a level α test for c t β. More precisely, we reject the null hypothesis c t β = d if the above confidence interval does not cover d, for a fixed number d.</p><p>As an important case, c = e j so c t β = β j . Standard software packages, for example, R, report the point estimator βj , the standard error ŝe j = σ2 [(X t X) -1 ] jj , the t statistic T j = βj / ŝe j , and the two-sided p-value pr(|t n-p | ≥ |T j |) for testing whether β j equals zero or not. Section 5.4 below gives some examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Vector parameters</head><p>We then consider statistical inference for Cβ, a multi-dimensional linear function of β where C ∈ R l×p . If l = 1, then it reduces to the one-dimensional case. If l &gt; 1, then</p><formula xml:id="formula_82">C =    c t 1 . . . c t l    =⇒ Cβ =    c t 1 β . . . c t l β    correspond to the joint value of l parameters c t 1 β, . . . , c t l β. Example 5.1 If C =      0 1 0 • • • 0 0 0 1 • • • 0 . . . . . . . . . • • • . . . 0 0 0 • • • 1      , then Cβ =    β 2 . . . β p    ,</formula><p>contains all the coefficients except for the first one (the intercept in most cases). Most software packages report the test of the joint significance of (β 2 , . . . , β p ). Section 5.4 below gives some examples.</p><p>Example 5.2 Another leading application is to test whether β 2 = 0 in the following regression partitioned by X = (X 1 , X 2 ) where X 1 and X 2 are n × k and n × l matrices:</p><formula xml:id="formula_83">Y = X 1 β 1 + X 2 β 2 + ε, with C = 0 l×k I l β 1 β 2 =⇒ Cβ = β 2 .</formula><p>We will discuss this partitioned regression in more detail in Chapters 7 and 8. Now we will focus on the generic problem of inferring Cβ. To avoid degeneracy, we assume that C does not have redundant rows, quantified below.</p><p>Assumption 5.2 C has linearly independent rows.</p><formula xml:id="formula_84">Theorem 5.1 implies that C β -Cβ ∼ N 0, σ 2 C(X t X) -1 C t</formula><p>and therefore the standardized quadratic form has a chi-squared distribution</p><formula xml:id="formula_85">(C β -Cβ) t σ 2 C(X t X) -1 C t -1 (C β -Cβ) ∼ χ 2 l .</formula><p>The above chi-squared distribution follows from the property of the quadratic form of a Normal in Theorem B.10, where σ 2 C(X t X) -<ref type="foot" target="#foot_3">foot_3</ref> C t is a positive definite matrix 1 . Again this is not directly useful with unknown σ 2 . Replacing σ 2 with the unbiased estimator σ2 and using a scaling factor l, we can obtain a pivotal quantity that has an F distribution as summarized in Theorem 5.4 below.</p><p>Theorem 5.4 Under Assumptions 5.1 and 5.2, we have</p><formula xml:id="formula_86">F C ≡ (C β -Cβ) t C(X t X) -1 C t -1 (C β -Cβ) lσ 2 ∼ F l,n-p .</formula><p>Proof of Theorem 5.4: Similar to the proof of Theorem 5.3, we apply Theorem 5.1 to derive that</p><formula xml:id="formula_87">F C = (C β -Cβ) t σ 2 C(X t X) -1 C t -1 (C β -Cβ)/l σ2 /σ 2 ∼ χ 2 l /l χ 2 n-p /(n -p)</formula><p>, where χ 2 l and χ 2 n-p denote independent χ 2 l and χ 2 n-p random variables, respectively, with a little abuse of notation. Therefore, F C ∼ F l,n-p by the definition of the F distribution. □ Theorem 5.4 motivates the following confidence region for Cβ:</p><formula xml:id="formula_88">r : (C β -r) t C(X t X) -1 C t -1 (C β -r) ≤ lσ 2 f 1-α,l,n-p ,</formula><p>where f 1-α,l,n-p is the upper α quantile of the F l,n-p distribution. By duality of the confidence region and hypothesis testing, we can also construct a level α test for Cβ. Most statistical packages automatically report the p-value based on the F statistic in Example 5.1. As a final remark, the statistics in Theorems 5.3 and 5.4 are called the Wald-type statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Prediction based on pivotal quantities</head><p>Practitioners use OLS not only to infer β but also to predict future outcomes. For the pair of future data (x n+1 , y n+1 ), we observe only x n+1 and want to predict y n+1 based on (X, Y ) and x n+1 . Assume a stable relationship between y n+1 and x n+1 , that is,</p><formula xml:id="formula_89">y n+1 ∼ N(x t n+1 β, σ 2 )</formula><p>with the same (β, σ 2 ). First, we can predict the mean of y n+1 which is x t n+1 β. It is just a one-dimensional linear function of β, so the theory in Theorem 5.3 is directly applicable. A natural unbiased predictor is x t n+1 β with 1 -α level prediction interval</p><formula xml:id="formula_90">x t n+1 β ± t 1-α/2,n-p ŝe xn+1 .</formula><p>Second, we can predict y n+1 itself, which is a random variable. We can still use x t n+1 β as a natural unbiased predictor but need to modify the prediction interval. Because y n+1 β, we have</p><formula xml:id="formula_91">y n+1 -x t n+1 β ∼ N 0, σ 2 + σ 2 x t n+1 (X t X) -1</formula><p>x n+1 , and therefore</p><formula xml:id="formula_92">y n+1 -x t n+1 β σ2 + σ2 x t n+1 (X t X) -1 x n+1 = y n+1 -x t n+1 β σ 2 + σ 2 x t n+1 (X t X) -1 x n+1 σ2 σ 2 ∼ N(0, 1) χ 2 n-p /(n -p)</formula><p>,</p><p>where N(0, 1) and χ 2 n-p denote independent standard Normal and χ 2 n-p random variables, respectively, with a little abuse of notation. Therefore,</p><formula xml:id="formula_93">y n+1 -x t n+1 β σ2 + σ2 x t n+1 (X t X) -1 x n+1 ∼ t n-p</formula><p>is a pivotal quantity. Define the squared prediction error as</p><formula xml:id="formula_94">pe 2 xn+1 = σ2 + σ2 x t n+1 (X t X) -1 x n+1 = σ2    1 + n -1 x t n+1 n -1 n i=1 x i x t i -1 x n+1    ,</formula><p>which has two components. The first one has magnitude close to σ 2 which is of constant order. The second one has a magnitude decreasing in n if n -1 n i=1 x i x t i converges to a finite limit with large n. Therefore, the first component dominates the second one with large n, which results in the main difference between predicting the mean of y n+1 and predicting y n+1 itself. Using the notation pe xn+1 , we can construct the following 1 -α level prediction interval:</p><p>x t n+1 β ± t 1-α/2,n-p pe xn+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Examples and R implementation</head><p>Below I illustrate the theory in this chapter with two classic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Univariate regression</head><p>Revisiting Galton's data, we have the following result:</p><p>&gt; library ( " HistData " ) &gt; galton _ fit = lm ( childHeight ~midparentHeight , + data = Galt onFamil ies ) &gt; round ( summary ( galton _ fit )$ coef , 3 ) Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 2 2 . 6 3 6 4 . 2 6 5 5 . 3 0 7 0 m id pa r en tH ei g ht 0 . 6 3 7 0 . 0 6 2 1 0 . 3 4 5 0</p><p>With the fitted line, we can predict childHeight at different values of midparentHeight. In the predict function, if we specify interval = "confidence", it gives the confidence intervals for the means of the new outcomes; if we specify interval = "prediction", it gives the prediction intervals for the new outcomes themselves.</p><formula xml:id="formula_95">&gt; new _ mph = seq ( 6 0 , 8 0 , by = 0 . 5 ) &gt; new _ data = data . frame ( mi d pa re nt H ei gh t = new _ mph ) &gt; new _ ci = predict ( galton _ fit , new _ data , + interval = " confidence " ) &gt; new _ pi = predict ( galton _ fit , new _ data , + interval = " prediction " ) &gt; round ( head ( cbind ( new _ ci , new _ pi )) , 3 )</formula><p>fit lwr upr fit lwr upr 1 6 0 . 8 7 8 5 9 . 7 4 4 6 2 . 0 1 2 6 0 . 8 7 8 5 4 . 1 2 6 6 7 . 6 3 0 2 6 1 . 1 9 7 6 0 . 1 2 2 6 2 . 2 7 2 6 1 . 1 9 7 5 4 . 4 5 4 6 7 . 9 3 9 3 6 1 . 5 1 5 6 0 . 4 9 9 6 2 . 5 3 1 6 1 . 5 1 5 5 4 . 7 8 2 6 8 . 2 4 9 4 6 1 . 8 3 4 6 0 . 8 7 7 6 2 . 7 9 1 6 1 . 8 3 4 5 5 . 1 0 9 6 8 . 5 5 9 5 6 2 . 1 5 3 6 1 . 2 5 4 6 3 . 0 5 1 6 2 . 1 5 3 5 5 . 4 3 6 6 8 . 8 6 9 6 6 2 . 4 7 1 6 1 . 6 3 2 6 3 . 3 1 1 6 2 . 4 7 1 5 5 . 7 6 2 6 9 . 1 8 0 Figure 5.1 plots the fitted line as well as the confidence intervals and prediction intervals at level 95%. The file code5.4.1.R contains the R code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Multivariate regression</head><p>The R package Matching contains an experimental dataset lalonde from LaLonde <ref type="bibr">(1986)</ref>. Units were randomly assigned to the job training program, with treat being the treatment indicator. The outcome re78 is the real earnings in 1978, and other variables are pretreatment covariates. From the simple OLS, the treatment has a significant positive effect, whereas none of the covariates are predictive of the outcome.</p><p>&gt; library ( " Matching " ) &gt; data ( lalonde ) &gt; lalonde _ fit = lm ( re 7 8 ~. , data = lalonde ) &gt; summary ( lalonde _ fit ) Call : lm ( formula = re 7 8 ~. , data = lalonde )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residuals :</head><p>Min 1 Q Median 3 Q Max -9 6 1 2 -4 3 5 5 -1 5 7 2 3 0 5 4 5 3 1 1 9 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 60 70 80 Coefficients : Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 2 . 5 6 7 e + 0 2 3 . 5 2 2 e + 0 3 0 . 0 7 3 0 . 9 4 1 9 3 age 5 . 3 5 7 e + 0 1 4 . 5 8 1 e + 0 1 1 . 1 7 0 0 . 2 4 2 8 4 educ 4 . 0 0 8 e + 0 2 2 . 2 8 8 e + 0 2 1 . 7 5 1 0 . 0 8 0 5 8 . black -2 . 0 3 7 e + 0 3 1 . 1 7 4 e + 0 3 -1 . 7 3 6 0 . 0 8 3 3 1 . hisp 4 . 2 5 8 e + 0 2 1 . 5 6 5 e + 0 3 0 . 2 7 2 0 . 7 8 5 6 2 married -1 . 4 6 3 e + 0 2 8 . 8 2 3 e + 0 2 -0 . 1 6 6 0 . 8 6 8 3 5 nodegr -1 . 5 1 8 e + 0 1 1 . 0 0 6 e + 0 3 -0 . 0 1 5 0 . 9 8 7 9 7 re 7 4 1 . 2 3 4e -0 1 8 . 7 8 4e -0 2 1 . 4 0 5 0 . 1 6 0 7 9 re 7 5 1 . 9 7 4e -0 2 1 . 5 0 3e -0 1 0 . 1 3 1 0 . 8 9 5 5 4 u 7 4</p><p>1 . 3 8 0 e + 0 3 1 . 1 8 8 e + 0 3 1 . 1 6 2 0 . 2 4 5 9 0 u 7 5 -1 . 0 7 1 e + 0 3 1 . 0 2 5 e + 0 3 -1 . 0 4 5 0 . 2 9 6 5 1 treat 1 . 6 7 1 e + 0 3 6 . 4 1 1 e + 0 2 2 . 6 0 6 0 . 0 0 9 4 8 **</p><p>Residual standard error : 6 5 1 7 on 4 3 3 degrees of freedom Multiple R -squared : 0 . 0 5 8 2 2 , Adjusted R -squared : 0 . 0 3 4 3 F -statistic : 2 . 4 3 3 on 1 1 and 4 3 3 DF , p -value : 0 . 0 0 5 9 7 4</p><p>The above result shows that none of the pretreatment covariates is significant. It is also of interest to test whether they are jointly significant. The result below shows that they are only marginally significant at the level 0.05 based on a joint test.</p><p>&gt; library ( " car " ) &gt; l i n e a r H y p o t he s i s ( lalonde _ fit , + c ( " age = 0 " , " educ = 0 " , " black = 0 " , + " hisp = 0 " , " married = 0 " , " nodegr = 0 " , + " re 7 4 = 0 " , " re 7 5 = 0 " , " u 7 4 = 0 " , + " u 7 5 = 0 " )) Linear hypothesis test Hypothesis : age = 0 educ = 0 black = 0 hisp = 0 married = 0 nodegr = 0 re 7 4 = 0 re 7 5 = 0 u 7 4 = 0 u 7 5 = 0 Model 1 : restricted model Model 2 : re 7 8 ~age + educ + black + hisp + married + nodegr + re 7 4 + re 7 5 + u 7 4 + u 7 5 + treat</p><p>Res . Df RSS Df Sum of Sq F Pr ( &gt; F ) 1 4 4 3 1 . 9 1 7 8 e + 1 0 2 4 3 3 1 . 8 3 8 9 e + 1 0 1 0 7 8 8 7 9 9 0 2 3 1 . 8 5 7 4 0 . 0 4 9 2 9 * Below I create two pseudo datasets: one with all units assigned to the treatment, and the other with all units assigned to the control, fixing all the pretreatment covariates. The predicted outcomes are the counterfactual outcomes under the treatment and control. I further calculate their means and verify that their difference equals the OLS coefficient of</p><formula xml:id="formula_96">treat. &gt; new _ treat = lalonde &gt; new _ treat $ treat = 1 &gt; predict _ lalonde 1 = predict ( lalonde _ fit , new _ treat , + interval = " none " ) &gt; new _ control = lalonde &gt; new _ control $ treat = 0 &gt; predict _ lalonde 0</formula><p>= predict ( lalonde _ fit , new _ control , + interval = " none " ) &gt; mean ( predict _ lalonde 1 ) [ 1 ] 6 2 7 6 . 9 1 &gt; mean ( predict _ lalonde 0 ) [ 1 ] 4 6 0 6 . 2 0 1 &gt; &gt; mean ( predict _ lalonde 1 ) -mean ( predict _ lalonde 0 ) [ 1 ] 1 6 7 0 . 7 0 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MLE</head><p>Under the Normal linear model, show that the maximum likelihood estimator (MLE) for β is the OLS estimator, but the MLE for σ 2 is σ2 = rss/n. Compare the mean squared errors of σ2 and σ2 for estimating σ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MLE with Laplace errors</head><p>Assume that y i = x t i β + σε i where the ε i 's are i.i.d. Laplace distribution with density f (ε) = 2 -1 e -|ε| (i = 1, . . . , n). Find the MLEs of (β, σ 2 ).</p><p>Remark: We will revisit this problem in Chapter 26.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Joint prediction</head><p>With multiple future data points (X n+1 , Y n+1 ) where X n+1 ∈ R l×p and Y n+1 ∈ R l , construct the joint predictors and prediction region for Y n+1 based on (X, Y ) and X n+1 . As a starting point, you can assume that l ≤ p and the rows of X n+1 are linearly independent. You can then consider the case in which the rows of X n+1 are not linearly independent.</p><p>Hint: Use Theorem B.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Two-sample problem</head><formula xml:id="formula_97">1. Assume that z 1 , . . . , z m iid ∼ N(µ 1 , σ 2 ) and w 1 , . . . , w n iid ∼ N(µ 2 , σ 2 )</formula><p>, and test H 0 : µ 1 = µ 2 . Show that under H 0 , the t statistic with pooled variance estimator have the following distribution:</p><formula xml:id="formula_98">t equal = z -w σ2 (m -1 + n -1 ) ∼ t m+n-2 , where σ2 = (m -1)S 2 z + (n -1)S 2 w /(m + n -2) with the sample means z = m -1 m i=1 z i , w = n -1 n i=1 w i ,</formula><p>and the sample variances</p><formula xml:id="formula_99">S 2 z = (m -1) -1 m i=1 (z i -z) 2 , S 2 w = (n -1) -1 n i=1 (w i -w) 2 .</formula><p>Remark: The name "equal" is motivated by the "var.equal" parameter of the R function t.test.</p><p>2. We can write the above problem as testing hypothesis H 0 : β 1 = 0 in the linear regression</p><formula xml:id="formula_100">Y = Xβ + ε with Y =           z 1 . . . z m w 1 . . . w n           , X =           1 1 . . . . . . 1 1 1 0 . . . . . . 1 0           , β = β 0 β 1 , ε =           ε 1 . . . ε m ε m+1 . . . ε m+n           .</formula><p>Based on the Normal linear model, we can compute the t statistic. Show that it is identical to t equal .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis of Variance (ANOVA) with a multi-level treatment</head><p>Let x i be the indicator vector for J treatment levels in a completely randomized experiment, for example, x i = e j = (0, . . . , 1, . . . , 0) t with the jth element being one if unit i receives treatment level j (j = 1, . . . , J). Let y i be the outcome of unit i (i = 1, . . . , n). Let T j be the indices of units receiving treatment j, and let n j = |T j | be the sample size and ȳj = n -1 j i∈Tj y i be the sample mean of the outcomes under treatment j. Define ȳ = n -1 n i=1 y i as the grand mean. We can test whether the treatment has any effect on the outcome by testing the null hypothesis</p><formula xml:id="formula_101">H 0 : β 1 = • • • = β J in the Normal linear model Y = Xβ + ε assuming ε ∼ N(0, σ 2 I n )</formula><p>. This is a special case of testing Cβ = 0. Find C and show that the corresponding F statistic is identical to</p><formula xml:id="formula_102">F = J j=1 n j (ȳ j -ȳ) 2 /(J -1) J j=1 i∈Tj (y i -ȳj ) 2 /(n -J) ∼ F J-1,n-J .</formula><p>Remarks: (1) This is Fisher's F statistic. <ref type="bibr">(2)</ref> In this linear model formulation, X does not contain a column of 1's. (3) The choice of C is not unique, but the final formula for F is. (4) You may use the Sherman-Morrison formula in Problem 1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Confidence interval for σ 2</head><p>Based on Theorem 5.1, construct a 1 -α level confidence interval for σ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Relationship between t and F</head><p>Show that when C containing only one row c t , then T 2 c = F C , where T c is defined in Theorem 5.3 and F C is defined in Theorem 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">rss and t-statistic in univariate OLS</head><p>Focus on univariate OLS discussed in Chapter 2:</p><formula xml:id="formula_103">y i = α + βx i + εi (i = 1, . . . , n). Show that rss equals n i=1 ε2 i = n i=1 (y i -ȳ) 2 (1 -ρ2 xy )</formula><p>and under the homoskedasticity assumption, the t-statistic associated with β equals ρxy</p><formula xml:id="formula_104">(1 -ρ2 xy )/(n -2)</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Equivalence of the t-statistics</head><p>With the data (x i , y i ) n i=1 where both x i and y i are scalars. Run OLS fit of y i on (1, x i ) to obtain t y|x , the t-statistic of the coefficient of x i , under the homoskedasticity assumption. Run OLS fit of x i on (1, y i ) to obtain t x|y , the t-statistic of the coefficient of y i , under the homoskedasticity assumption.</p><p>Show t y|x = t x|y . Remark: This is a numerical result that holds without any stochastic assumptions. I give an example below.</p><p>&gt; library ( MASS ) &gt; # simulate bivariate normal distribution &gt; xy = mvrnorm ( n = 1 0 0 , mu = c ( 0 , 0 ) , + Sigma = matrix ( c ( 1 , 0 . 5 , 0 . 5 , 1 ) , ncol = 2 )) &gt; xy = as . data . frame ( xy ) &gt; colnames ( xy ) = c ( " x " , " y " ) &gt; # # OLS &gt; reg . y . x = lm ( y ~x , data = xy ) &gt; reg . x . y = lm ( x ~y , data = xy ) &gt; # # compare t statistics based on homoskedastic errors &gt; summary ( reg . y . x )$ coef <ref type="bibr">[ 2 , 3 ]</ref> [ 1 ] 4 . 4 7 0 3 3 1 &gt; summary ( reg . x . y )$ coef <ref type="bibr">[ 2 , 3 ]</ref> [ 1 ] 4 . 4 7 0 3 3 1</p><p>The equivalence of the t-statistics from the OLS fit of y on x and that of x on y demonstrates that based on OLS, the data do not contain any information about the direction of the relationship between x and y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">An application</head><p>The R package sampleSelection <ref type="bibr" target="#b223">(Toomet and Henningsen, 2008)</ref> describes the dataset RandHIE as follows: "The RAND Health Insurance Experiment was a comprehensive study of health care cost, utilization and outcome in the United States. It is the only randomized study of health insurance, and the only study which can give definitive evidence as to the causal effects of different health insurance plans." You can find more detailed information about other variables in this package. The main outcome of interest lnmeddol means the log of medical expenses. Use linear regression to investigate the relationship between the outcome and various important covariates.</p><p>Note that the solution to this problem is not unique, but you need to justify your choice of covariates and model, and need to interpret the results.</p><p>Asymptotic Inference in OLS: the Eicker-Huber-White (EHW) robust standard error</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Motivation</head><p>Standard software packages, for example, R, report the point estimator, standard error, and p-value for each coordinate of β based on the Normal linear model:</p><formula xml:id="formula_105">Y = Xβ + ε ∼ N(Xβ, σ 2 I n ).</formula><p>Statistical inference based on this model is finite-sample exact. However, the assumptions of this model are extremely strong: the functional form is linear, the error terms are additive with distributions not dependent on X, and the error terms are IID Normal with the same variance. If we do not believe some of these assumptions, can we still trust the associated statistical inference? Let us start with some simple numerical examples, with the R code in code6.1.R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Numerical examples</head><p>The first one is the ideal Normal linear model:</p><formula xml:id="formula_106">&gt; library ( car ) &gt; n = 2 0 0 &gt; x = runif (n , -2 , 2 ) &gt; beta = 1 &gt; xbeta = x * beta &gt; Simu 1 = replicate ( 5 0 0 0 , + { y = xbeta + rnorm ( n ) + ols . fit = lm ( y ~x ) + c ( summary ( ols . fit )$ coef [ 2 , 1 : 2 ] , + sqrt ( hccm ( ols . fit )[ 2 , 2 ])) + })</formula><p>In the above, we generate outcomes from a simple linear model</p><formula xml:id="formula_107">y i = x i + ε i with ε i iid ∼ N(0, σ 2 = 1)</formula><p>. Over 5000 replications of the data, we computed the OLS coefficient β of x i and reported two standard errors. One is the standard error discussed in Chapter 5 under the Normal linear model, which is also the default choice of the lm function of R. The other one, computed by the hccm function in the R package car, will be the main topic of this chapter. The (1, 1) the panel of Figure <ref type="figure">6</ref>.1 shows the histogram of the estimator and reports the standard error (se0), as well as two estimated standard errors (se1 and se2). The distribution of β is symmetric and bell-shaped around the true parameter 1, and the estimated standard errors are close to the true one.</p><p>To investigate the impact of Normality, we change the error terms to be IID exponential with mean 1 and variance 1.</p><p>&gt; Simu 2 = replicate ( 5 0 0 0 ,</p><formula xml:id="formula_108">+ { y = xbeta + rexp ( n ) + ols . fit = lm ( y ~x ) + c ( summary ( ols . fit )$ coef [ 2 , 1 : 2 ] , + sqrt ( hccm ( ols . fit )[ 2 , 2 ])) + })</formula><p>The (1, 2) panel of Figure <ref type="figure">6</ref>.1 corresponds to this setting. With non-Normal errors, β is still symmetric and bell-shaped around the true parameter 1, and the estimated standard errors are close to the true one. So the Normality of the error terms does not seem to be a crucial assumption for the validity of the inference procedure under the Normal linear model.</p><p>We then generate errors from Normal with variance depending on x:</p><p>&gt; Simu 3 = replicate ( 5 0 0 0 , + { y = xbeta + rnorm (n , 0 , abs ( x )) + ols . fit = lm ( y ~x ) + c ( summary ( ols . fit )$ coef [ 2 , 1 : 2 ] , + sqrt ( hccm ( ols . fit ) <ref type="bibr">[ 2 , 2 ]</ref>)) + })</p><p>The (2, 1) panel of Figure <ref type="figure">6</ref>.1 corresponds to this setting. With heteroskedastic Normal errors, β is symmetric and bell-shaped around the true parameter 1, se2 is close to se0, but se1 underestimates se0. So the heteroskedasticity of the error terms does not change the Normality of the OLS estimator dramatically, although the statistical inference discussed in Chapter 5 can be invalid. Finally, we generate heteroskedastic non-Normal errors:</p><formula xml:id="formula_109">&gt; Simu 4 = replicate ( 5 0 0 0 , + { y = xbeta + runif (n , -x ^2 , x ^2 ) + ols . fit = lm ( y ~x ) + c ( summary ( ols . fit )$ coef [ 2 , 1 : 2 ] , + sqrt ( hccm ( ols . fit )[ 2 , 2 ])) + })</formula><p>The (2, 2) panel of Figure <ref type="figure">6</ref>.1 corresponds to this setting, which has a similar pattern as the (2, 1) panel. So the Normality of the error terms is not crucial, but the homoskedasticity is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Goal of this chapter</head><p>In this chapter, we will still impose the linearity assumption, but relax the distributional assumption on the error terms. We assume the following heteroskedastic linear model.</p><p>Assumption 6.1 (Heteroskedastic linear model) We have</p><formula xml:id="formula_110">y i = x t i β + ε i ,</formula><p>where the ε i 's are independent with mean zero and variance σ 2 i (i = 1, . . . , n). The design matrix X is fixed with linearly independent column vectors, and (β, σ 2 1 , . . . , σ 2 n ) are unknown parameters.</p><p>Because the error terms can have different variances, they are not IID in general under the heteroskedastic linear model. Their variances can be functions of the x i 's, and the variances σ 2 i are n free unknown numbers. Again, treating the x i 's as fixed is not essential, because we can condition on them if they are random. Without imposing Normality on the error terms, we cannot determine the finite sample exact distribution of the OLS estimator. This chapter will use the asymptotic analysis, assuming that the sample size n is large so that certain limiting theorems hold.</p><p>The asymptotic analysis later will show that if the error terms are homoskedastic, i.e., σ 2 i = σ 2 for all i = 1, . . . , n, we can still trust the statistical inference discussed in Chapter 5 based on the Normal linear model as long the central limit theorem (CLT) for the OLS estimator holds as n → ∞. If the error terms are heteroskedastic, i.e., their variances are different, we must adjust the standard error with the so-called Eicker-Huber-White heteroskedasticity robust standard error. I will give the technical details below. If you are unfamiliar with the asymptotic analysis, please first review the basics in Chapter C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Consistency of OLS</head><p>Under the heteroskedastic linear model, the OLS estimator β is still unbiased for β because the error terms have to mean zero. Moreover, we can show that it is consistent for β with large n and some regularity conditions. We start with a useful lemma. Lemma 6.1 Under Assumption 6.1, the OLS estimator has the representation β -β = B -1 n ξ n , where</p><formula xml:id="formula_111">B n = n -1 n i=1 x i x t i , ξ n = n -1 n i=1 x i ε i Proof of Lemma 6.1: Since y i = x t i β + ε i , we have β = B -1 n n -1 n i=1 x i y i = B -1 n n -1 n i=1 x i (x t i β + ε i ) = B -1 n B n β + B -1 n n -1 n i=1 x i ε i = β + B -1 n n -1 n i=1 x i ε i . □ In the representation of Lemma 6.1, B n is fixed and ξ n is random. Since E(ξ n ) = 0, we know that E( β) = β, so the OLS estimator is unbiased. Moreover, cov(ξ n ) = cov n -1 n i=1 x i ε i = n -2 n i=1 σ 2 i x i x t i = M n /n,</formula><p>where</p><formula xml:id="formula_112">M n = n -1 n i=1 σ 2 i x i x t i .</formula><p>So the covariance of the OLS estimator is</p><formula xml:id="formula_113">cov( β) = n -1 B -1 n M n B -1 n .</formula><p>It has a sandwich form, justifying the choice of notation B n for the "bread" and M n for the "meat." Intuitively, if B n and M n have finite limits, then the covariance of β shrinks to zero with large n, implying that β will concentrate near its mean β. This is the idea of consistency, formally stated below. Assumption 6.2 B n → B and M n → M where B and M are finite with B invertible. Theorem 6.1 Under Assumptions 6.1 and 6.2, we have β → β in probability.</p><p>Proof of Theorem 6.1: We only need to show that ξ n → 0 in probability. It has mean zero and covariance matrix M n /n, so it converges to zero in probability using Proposition C.4 in Chapter C. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Asymptotic Normality of the OLS estimator</head><p>Intuitively, ξ n is the sample average of some independent terms, and therefore, the classic Lindberg-Feller theorem guarantees that it enjoys a CLT under some regularity conditions. Consequently, β also enjoys a CLT with mean β and covariance matrix n -1 B -1 n M n B -1 n . The asymptotic results in this chapter require rather tedious regularity conditions. I give them for generality, and they hold automatically if we are willing to assume that the covariates and error terms are all bounded by a constant not depending on n. These general conditions are basically moment conditions required by the law of large numbers and CLT. You do not have to pay too much attention to the conditions when you first read this chapter.</p><p>The CLT relies on an additional condition on a higher-order moment</p><formula xml:id="formula_114">d 2+δ,n = n -1 n i=1 ∥x i ∥ 2+δ E(ε 2+δ i ).</formula><p>Theorem 6.2 Under Assumptions 6.1 and 6.2, if there exist a δ &gt; 0 and C &gt; 0 not depending on n such that d 2+δ,n ≤ C, then</p><formula xml:id="formula_115">√ n( β -β) → N(0, B -1 M B -1 )</formula><p>in distribution.</p><p>Proof of Theorem 6.2:</p><p>The key is to show the CLT for ξ n , and the CLT for β holds due to the Slutsky's Theorem; see Chapter C for a review. Define</p><formula xml:id="formula_116">z n,i = n -1/2 x i ε i , (i = 1, . . . , n)</formula><p>with mean zero and finite covariance, and we need to verify the two conditions required by the Lindeberg-Feller CLT stated as Proposition C.8 in Chapter C. First, the Lyapunov condition holds because</p><formula xml:id="formula_117">n i=1 E ∥z n,i ∥ 2+δ = n i=1 E n -(2+δ)/2 ∥x i ∥ 2+δ ε 2+δ i = n -δ/2 × n -1 n i=1 ∥x i ∥ 2+δ E(ε 2+δ i ) = n -δ/2 × d 2+δ,n → 0. Second, n i=1 cov(z n,i ) = n -1 n i=1 σ 2 i x i x t i = M n → M. So the Lindberg-Feller CLT implies that n -1/2 n i=1 x i ε i = n i=1 z n,i → N(0, M ) in distri- bution. □</formula><p>6.4 Eicker-Huber-White standard error</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Sandwich variance estimator</head><p>The CLT in Theorem 6.2 shows that</p><formula xml:id="formula_118">β a ∼ N(β, n -1 B -1 M B -1 ),</formula><p>where a ∼ denotes "approximation in distribution." However, the asymptotic covariance is unknown, and we need to use the data to construct a reasonable estimator for statistical inference. It is relatively easy to replace B with its sample analog B n , but</p><formula xml:id="formula_119">Mn = n -1 n i=1 ε 2 i x i x t i</formula><p>as the sample analog for M is not directly useful because the error terms are unknown either. It is natural to use ε2 i to replace ε 2 i to obtain the following estimator for M :</p><formula xml:id="formula_120">Mn = n -1 n i=1 ε2 i x i x t i .</formula><p>Although each ε2 i is a poor estimator for σ 2 i , the sample average Mn turns out to be well-behaved with large n and the regularity conditions below. Theorem 6.3 Under Assumptions 6.1 and 6.2, we have Mn → M in probability if</p><formula xml:id="formula_121">n -1 n i=1 var(ε 2 i )x 2 ij1 x 2 ij2 , n -1 n i=1 x ij1 x ij2 x ij3 x ij4 , n -1 n i=1 σ 2 i x 2 ij1 x 2 ij2 x 2 ij3 (6.1)</formula><p>are bounded from above by a constant C not depending on n for any j 1 , j 2 , j 3 , j 4 = 1, . . . , p.</p><p>Proof of Theorem 6.3: Assumption 6.2 ensures that β → β in probability by Theorem 6.1. Markov's inequality and the boundedness of the first term in (6.1) ensure that Mn -M n → 0 in probability. So we only need to show that Mn -Mn → 0 in probability. The (j 1 , j 2 )th element of their difference is</p><formula xml:id="formula_122">( Mn -Mn ) j1,j2 = n -1 n i=1 ε2 i x i,j1 x i,j2 -n -1 n i=1 ε 2 i x i,j1 x i,j2 = n -1 n i=1 ε i + x t i β -x t i β 2 -ε 2 i x i,j1 x i,j2 = n -1 n i=1 x t i β -x t i β 2 + 2ε i x t i β -x t i β x i,j1 x i,j2 = (β -β) t n -1 n i=1 x i x t i x i,j1 x i,j2 (β -β) + 2(β -β) t n -1 n i=1 x i x i,j1 x i,j2 ε i .</formula><p>It converges to zero in probability because the first term converges to zero due to the boundedness of the second term in (6.1), and the second term converges to zero in probability due to Markov's inequality and the boundedness of the third term in (6.1). □ The final variance estimator for β is</p><formula xml:id="formula_123">Vehw = n -1 n -1 n i=1 x i x t i -1 n -1 n i=1 ε2 i x i x t i n -1 n i=1 x i x t i -1</formula><p>, which is called the Eicker-Huber-White (EHW) heteroskedasticity robust covariance matrix. In matrix form, it equals Vehw = (X t X) -1 (X t ΩX)(X t X) -1 ,</p><p>where Ω = diag ε2 1 , . . . , ε2 n . <ref type="bibr" target="#b96">Eicker (1967)</ref> first proposed to use Vehw . <ref type="bibr" target="#b236">White (1980a)</ref> popularized it in economics which has been influential in empirical research. Related estimators appeared in many other contexts of statistics. <ref type="bibr" target="#b81">Cox (1961)</ref> and <ref type="bibr" target="#b146">Huber (1967)</ref> discussed the sandwich variance in the context of misspecified parametric models; see Section D. <ref type="bibr">2. Fuller (1975)</ref> proposed a more general form of Vehw in the context of survey sampling. The square root of the diagonal terms of Vehw , denoted by ŝe ehw,j (j = 1, . . . , p), are called the heteroskedasticity-consistent standard errors, heteroskedasticity-robust standard errors, White standard errors, Huber-White standard errors, or Eicker-Huber-White standard errors, among many other names.</p><p>We can conduct statistical inference based on Normal approximations. For example, we can test linear hypotheses based on</p><formula xml:id="formula_124">β a ∼ N(β, Vehw ),</formula><p>and in particular, we can infer each element of the coefficient based on βj a ∼ N(β j , ŝe 2 ehw,j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Other "HC" standard errors</head><p>Statistical inference based on the EHW standard error relaxes the parametric assumptions of the Normal linear model. However, its validity relies strongly on the asymptotic argument.</p><p>At finite samples, it can have poor behaviors. Since <ref type="bibr" target="#b236">White (1980a)</ref> published his paper, several modifications of V appeared aiming for better finite-sample properties. I summarize them below. They all rely on the h ii 's, which are the diagonal elements of the projection matrix H and called the leverage scores. Define</p><formula xml:id="formula_125">Vehw,k = n -1 n -1 n i=1 x i x t i -1 n -1 n i=1 ε2 i,k x i x t i n -1 n i=1 x i x t i -1 , where εi,k =                εi , (k = 0, HC0); εi n n-p , (k = 1, HC1); εi / √ 1 -h ii , (k = 2, HC2); εi /(1 -h ii ), (k = 3, HC3); εi /(1 -h ii ) min{2,nhii/(2p)} , (k = 4, HC4).</formula><p>The HC1 correction is similar to the degrees of freedom correction in the OLS covariance estimator. The HC2 correction was motivated by the unbiasedness of covariance when the error terms have the same variance; see Problem 6.8 for more details. The HC3 correction was motivated by a method called jackknife which will be discussed in Chapter 11. This version appeared even early than <ref type="bibr" target="#b236">White (1980a)</ref>; see <ref type="bibr" target="#b179">Miller (1974)</ref>, <ref type="bibr" target="#b138">Hinkley (1977), and</ref><ref type="bibr" target="#b198">Reeds (1978)</ref>. I do not have a good intuition for the HC4 correction. See MacKinnon and <ref type="bibr" target="#b172">White (1985)</ref>, <ref type="bibr" target="#b169">Long and Ervin (2000)</ref> and <ref type="bibr" target="#b86">Cribari-Neto (2004)</ref> for reviews. Using simulation studies, <ref type="bibr" target="#b169">Long and Ervin (2000)</ref> recommended HC3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Special case with homoskedasticity</head><p>As an important special case with σ 2 i = σ 2 for all i = 1, . . . , n, we have</p><formula xml:id="formula_126">M n = σ 2 n -1 n i=1 x i x t i = σ 2 B n ,</formula><p>which simplifies the covariance of β to cov( β) = σ 2 B -1 n /n, and the asymptotic Normality to √ n( β -β) → N(0, σ 2 B -1 ) in distribution. We have shown that under the Gauss-Markov model, σ2 = (n -p) -1 n i=1 ε2</p><p>i is unbiased for σ 2 . Moreover, σ2 is consistent for σ 2 under the same condition as Theorem 6.1, justifying the use of</p><formula xml:id="formula_127">V = σ2 n i=1 x i x t i = σ2 (X t X) -1</formula><p>as the covariance estimator. So under homoskedasticity, we can conduct statistical inference based on the following approximate Normality:</p><formula xml:id="formula_128">β a ∼ N β, σ2 (X t X) -1 .</formula><p>It is slightly different from the inference based on t and F distributions. But with large n, the difference is very small. I will end this section with a formal result on the consistency of σ2 .</p><p>Theorem 6.4 Under Assumptions 6.1 and 6.2, we have σ2 → σ 2 in probability if σ 2 i = σ 2 &lt; ∞ for all i = 1, . . . , n and n -1 n i=1 var(ε 2 i ) is bounded above by a constant not depending on n.</p><p>Proof of Theorem 6.4: Using Markov's inequality, we can show that n -1 n i=1 ε 2 i → σ 2 in probability. In addition, n -1 n i=1 ε2 i has the same probability limit as σ2 . So we only need to show that n</p><formula xml:id="formula_129">-1 n i=1 ε2 i -n -1 n i=1 ε 2 i → 0 in probability. Their difference is n -1 n i=1 ε2 i -n -1 n i=1 ε 2 i = n -1 n i=1 ε i + x t i β -x t i β 2 -ε 2 i = n -1 n i=1 x t i β -x t i β 2 + 2 x t i β -x t i β ε i = (β -β) t n -1 n i=1 x i x t i (β -β) + 2(β -β) t n -1 n i=1 x i ε i = -(β -β) t n -1 n i=1 x i x t i (β -β),</formula><p>where the last step follows from Lemma 6.1. So the difference converges to zero in probability because β -β → 0 in probability by Theorem 6.1 and B n → B by Assumption 6.2. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Examples</head><p>I use three examples to compare various standard errors for the regression coefficients, with the R code in code6.5.R. The car package contains the hccm function that implements the EHW standard errors.</p><p>&gt; library ( " car " )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">LaLonde experimental data</head><p>First, I revisit the lalonde data. In the following analysis, different standard errors give similar t-values. Only treat is significant, but all other pretreatment covariates are not.</p><p>&gt; library ( " Matching " ) &gt; data ( lalonde ) &gt; ols . fit = lm ( re 7 8 ~. , data = lalonde ) &gt; ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) &gt; ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) &gt; ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) &gt; ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) &gt; ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) &gt; ols . fit . coef = summary ( ols . fit )$ coef &gt; tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) &gt; colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) &gt; round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept ) 0 . 0 7 0 . 0 7 0 . 0 7 0 . 0 7 0 . 0 7 0 . 0 7 age 1 . 1 7 1 . 2 9 1 . 2 8 1 . 2 7 1 . 2 5 1 . 2 5 educ 1 . 7 5 2 . 0 3 2 . 0 0 1 . 9 9 1 . 9 4 1 . 9 2 black -1 . 7 4 -2 . 0 0 -1 . 9 7 -1 . 9 5 -1 . 9 1 -1 . 9 1 hisp 0 . 2 7 0 . 3 0 0 . 3 0 0 . 3 0 0 . 2 9 0 . 2 9 married -0 . 1 7 -0 . 1 7 -0 . 1 7 -0 . 1 7 -0 . 1 6 -0 . 1 6 nodegr -0 . 0 2 -0 . 0 1 -0 . 0 1 -0 . 0 1 -0 . 0 1 -0 . 0 1 re 7 4 1 . 4 0 0 . 9 8 0 . 9 6 0 . 9 2 0 . 8 7 0 . 7 7 re 7 5 0 . 1 3 0 . 1 4 0 . 1 4 0 . 1 3 0 . 1 3 0 . 1 2 u 7 4</p><p>1 . 1 6 0 . 8 9 0 . 8 8 0 . 8 7 0 . 8 5 0 . 8 3 u 7 5 -1 . 0 5 -0 . 7 6 -0 . 7 5 -0 . 7 5 -0 . 7 4 -0 . 7 4 treat 2 . 6 1 2 . 4 9 2 . 4 6 2 . 4 5 2 . 4 1 2 . 4 0 6.5.2 Data from <ref type="bibr" target="#b156">King and Roberts (2015)</ref> The following example comes from <ref type="bibr" target="#b156">King and Roberts (2015)</ref>. The outcome variable is the multilateral aid flows, and the covariates include log population, log population squared, gross domestic product, former colony status, distance from the Western world, political freedom, military expenditures, arms imports, and the indicators for the years. Different robust standard errors are quite different for some coefficients.</p><p>&gt; library ( foreign ) &gt; dat = read . dta ( " isq . dta " ) &gt; dat = na . omit ( dat [ , c ( " multish " , " lnpop " , " lnpopsq " , + " lngdp " , " lncolony " , " lndist " , + " freedom " , " militexp " , " arms " , + " year 8 3 " , " year 8 6 " , " year 8 9 " , " year 9 2 " )]) &gt; ols . fit = lm ( multish ~lnpop + lnpopsq + lngdp + lncolony + + lndist + freedom + militexp + arms + + year 8 3 + year 8 6 + year 8 9 + year 9 2 , data = dat ) &gt; ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) &gt; ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) &gt; ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) &gt; ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) &gt; ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) &gt; ols . fit . coef = summary ( ols . fit )$ coef &gt; tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) &gt; colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) &gt; round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept ) 7 . 4 0 4 . 6 0 4 . 5 4 4 . 4 3 4 . 2 7 4 . 1 4 lnpop -8 . 2 5 -4 . 4 6 -4 . 4 0 -4 . 3 0 -4 . 1 4 -4 . 0 1 lnpopsq 9 . 5 6 4 . 7 9 4 . 7 2 4 . 6 1 4 . 4 4 4 . 3 1 lngdp -6 . 3 9 -6 . 1 4 -6 . 0 6 -6 . 0 1 -5 . 8 8 -5 . 8 6 lncolony 4 . 7 0 4 . 7 5 4 . 6 9 4 . 6 4 4 . 5 3 4 . 4 7 lndist -0 . 1 4 -0 . 1 6 -0 . 1 6 -0 . 1 6 -0 . 1 5 -0 . 1 6 freedom 2 . 2 5 1 . 8 0 1 . 7 8 1 . 7 5 1 . 6 9 1 . 6 5 militexp 0 . 5 1 0 . 5 9 0 . 5 9 0 . 5 7 0 . 5 5 0 . 5 2 arms 1 . 3 4 1 . 1 7 1 . 1 5 1 . 1 0 1 . 0 3 0 . 9 1 year 8 3 1 . 0 5 0 . 8 5 0 . 8 4 0 . 8 3 0 . 8 0 0 . 7 9 year 8 6 0 . 3 5 0 . 4 0 0 . 3 9 0 . 3 9 0 . 3 8 0 . 3 8 year 8 9 0 . 7 0 0 . 8 1 0 . 8 0 0 . 8 0 0 . 7 8 0 . 7 9 year 9 2 0 . 3 1 0 . 4 0 0 . 4 0 0 . 4 0 0 . 3 9 0 . 4 0</p><p>However, if we apply the log transformation on the outcome, then all standard errors give similar t-values.</p><p>&gt; ols . fit = lm ( log ( multish + 1 ) ~lnpop + lnpopsq + lngdp + lncolony + + lndist + freedom + militexp + arms + + year 8 3 + year 8 6 + year 8 9 + year 9 2 , data = dat ) &gt; ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) &gt; ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) &gt; ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) &gt; ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) &gt; ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) &gt; ols . fit . coef = summary ( ols . fit )$ coef &gt; tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) &gt; colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) &gt; round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept ) 2 . 9 6 2 . 8 1 2 . 7 7 2 . 7 2 2 . 6 3 2 . 5 3 lnpop -2 . 8 7 -2 . 6 3 -2 . 6 0 -2 . 5 4 -2 . 4 5 -2 . 3 5 lnpopsq 4 . 2 1 3 . 7 2 3 . 6 7 3 . 5 9 3 . 4 6 3 . 3 2 lngdp -8 . 0 2 -7 . 4 9 -7 . 3 8 -7 . 3 8 -7 . 2 7 -7 . 3 3 lncolony 6 . 3 1 6 . 1 9 6 . 1 1 6 . 0 8 5 . 9 7 5 . 9 5 lndist -0 . 1 6 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 freedom 1 . 4 7 1 . 5 3 1 . 5 1 1 . 5 0 1 . 4 7 1 . 4 6 militexp -0 . 3 2 -0 . 3 2 -0 . 3 1 -0 . 3 1 -0 . 3 0 -0 . 2 9 arms 1 . 2 7 1 . 1 2 1 . 1 0 1 . 0 5 0 . 9 8 0 . 8 6 year 8 3 0 . 1 0 0 . 1 0 0 . 1 0 0 . 1 0 0 . 1 0 0 . 1 0 year 8 6 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 -0 . 1 4 year 8 9 0 . 4 6 0 . 4 5 0 . 4 4 0 . 4 4 0 . 4 4 0 . 4 4 year 9 2 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3</p><p>In general, the difference between the OLS and EHW standard errors may be due to the heteroskedasticity or the poor approximation of the linear model. The above two analyses based on the original and transformed outcomes suggest that the linear approximation works better for the log-transformed outcome. We will discuss the issues of transformation and model misspecification later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3">Boston housing data</head><p>I also re-analyze the classic Boston housing data <ref type="bibr" target="#b133">(Harrison Jr and Rubinfeld, 1978)</ref>. The outcome variable is the median value of owner-occupied homes in US dollars 1000, and the covariates include per capita crime rate by town, the proportion of residential land zoned for lots over 25,000 square feet, the proportion of non-retail business acres per town, etc. You can find more details in the R package. In this example, different standard errors yield very different t-values.</p><p>&gt; library ( " mlbench " ) &gt; data ( BostonHousing ) &gt; ols . fit = lm ( medv ~. , data = BostonHousing ) &gt; summary ( ols . fit ) Call : lm ( formula = medv ~. , data = BostonHousing )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residuals :</head><p>Min 1 Q Median 3 Q Max -1 5 . 5 9 5 -2 . 7 3 0 -0 . 5 1 8 1 . 7 7 7 2 6 . 1 9 9</p><p>Coefficients : Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 3 . 6 4 6 e + 0 1 5 . 1 0 3 e + 0 0 7 . 1 4 4 3 . 2 8e -1 2 *** crim -1 . 0 8 0e -0 1 3 . 2 8 6e -0 2 -3 . 2 8 7 0 . 0 0 1 0 8 7 ** zn 4 . 6 4 2e -0 2 1 . 3 7 3e -0 2 3 . 3 8 2 0 . 0 0 0 7 7 8 *** indus 2 . 0 5 6e -0 2 6 . 1 5 0e -0 2 0 . 3 3 4 0 . 7 3 8 2 8 8 chas 1 2 . 6 8 7 e + 0 0 8 . 6 1 6e -0 1 3 . 1 1 8 0 . 0 0 1 9 2 5 ** nox -1 . 7 7 7 e + 0 1 3 . 8 2 0 e + 0 0 -4 . 6 5 1 4 . 2 5e -0 6 *** rm 3 . 8 1 0 e + 0 0 4 . 1 7 9e -0 1 9 . 1 1 6 &lt; 2e -1 6 *** age 6 . 9 2 2e -0 4 1 . 3 2 1e -0 2 0 . 0 5 2 0 . 9 5 8 2 2 9 dis -1 . 4 7 6 e + 0 0 1 . 9 9 5e -0 1 -7 . 3 9 8 6 . 0 1e -1 3 *** rad 3 . 0 6 0e -0 1 6 . 6 3 5e -0 2 4 . 6 1 3 5 . 0 7e -0 6 *** tax -1 . 2 3 3e -0 2 3 . 7 6 0e -0 3 -3 . 2 8 0 0 . 0 0 1 1 1 2 ** ptratio -9 . 5 2 7e -0 1 1 . 3 0 8e -0 1 -7 . 2 8 3 1 . 3 1e -1 2 *** b 9 . 3 1 2e -0 3 2 . 6 8 6e -0 3 3 . 4 6 7 0 . 0 0 0 5 7 3 *** lstat -5 . 2 4 8e -0 1 5 . 0 7 2e -0 2 -1 0 . 3 4 7 &lt; 2e -1 6 *** Residual standard error : 4 . 7 4 5 on 4 9 2 degrees of freedom Multiple R -squared : 0 . 7 4 0 6 , Adjusted R -squared : 0 . 7 3 3 8 F -statistic : 1 0 8 . 1 on 1 3 and 4 9 2 DF , p -value : &lt; 2 . 2e -1 6 &gt; &gt; ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) &gt; ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) &gt; ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) &gt; ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) &gt; ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) &gt; ols . fit . coef = summary ( ols . fit )$ coef &gt; tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) &gt; colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) &gt; round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept )</p><p>7 . 1 4 4 . 6 2 4 . 5 6 4 . 4 8 4 . 3 3 4 . 2 5 crim -3 . 2 9 -3 . 7 8 -3 . 7 3 -3 . 4 8 -3 . 1 7 -2 . 5 8 zn 3 . 3 8 3 . 4 2 3 . 3 7 3 . 3 5 3 . 2 7 3 . 2 8 indus 0 . 3 3 0 . 4 1 0 . 4 1 0 . 4 1 0 . 4 0 0 . 4 0 chas 1 3 . 1 2 2 . 1 1 2 . 0 8 2 . 0 5 2 . 0 0 2 . 0 0 nox -4 . 6 5 -4 . 7 6 -4 . 6 9 -4 . 6 4 -4 . 5 3 -4 . 5 2 rm 9 . 1 2 4 . 5 7 4 . 5 1 4 . 4 3 4 . 2 8 4 . 1 8 age 0 . 0 5 0 . 0 4 0 . 0 4 0 . 0 4 0 . 0 4 0 . 0 4 dis -7 . 4 0 -6 . 9 7 -6 . 8 7 -6 . 8 1 -6 . 6 6 -6 . 6 6 rad 4 . 6 1 5 . 0 5 4 . 9 8 4 . 9 1 4 . 7 6 4 . 6 5 tax -3 . 2 8 -4 . 6 5 -4 . 5 8 -4 . 5 4 -4 . 4 3 -4 . 4 2 ptratio -7 . 2 8 -8 . 2 3 -8 . 1 1 -8 . 0 6 -7 . 8 9 -7 . 9 3 b 3 . 4 7 3 . 5 3 3 . 4 8 3 . 4 4 3 . 3 4 3 . 3 0 lstat -1 0 . 3 5 -5 . 3 4 -5 . 2 7 -5 . 1 8 -5 . 0 1 -4 . 9 3</p><p>The log transformation of the outcome does not remove the discrepancy among the standard errors. In this example, heteroskedasticity seems an important problem.</p><p>&gt; ols . fit = lm ( log ( medv ) ~. , data = BostonHousing ) &gt; summary ( ols . fit ) Call : lm ( formula = log ( medv ) ~. , data = BostonHousing )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residuals :</head><p>Min 1 Q Median 3 Q Max -0 . 7 3 3 6 1 -0 . 0 9 7 4 7 -0 . 0 1 6 5 7 0 . 0 9 6 2 9 0 . 8 6 4 3 5 Coefficients :</p><p>Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 4 . 1 0 2 0 4 2 3 0 . 2 0 4 2 7 2 6 2 0 . 0 8 1 &lt; 2e -1 6 *** crim -0 . 0 1 0 2 7 1 5 0 . 0 0 1 3 1 5 5 -7 . 8 0 8 3 . 5 2e -1 4 *** zn 0 . 0 0 1 1 7 2 5 0 . 0 0 0 5 4 9 5 2 . 1 3 4 0 . 0 3 3 3 4 9 * indus 0 . 0 0 2 4 6 6 8 0 . 0 0 2 4 6 1 4 1 . 0 0 2 0 . 3 1 6 7 5 5 chas 1 0 . 1 0 0 8 8 7 6 0 . 0 3 4 4 8 5 9 2 . 9 2 5 0 . 0 0 3 5 9 8 ** nox -0 . 7 7 8 3 9 9 3 0 . 1 5 2 8 9 0 2 -5 . 0 9 1 5 . 0 7e -0 7 *** rm 0 . 0 9 0 8 3 3 1 0 . 0 1 6 7 2 8 0 5 . 4 3 0 8 . 8 7e -0 8 *** age 0 . 0 0 0 2 1 0 6 0 . 0 0 0 5 2 8 7 0 . 3 9 8 0 . 6 9 0 5 6 7 dis -0 . 0 4 9 0 8 7 3 0 . 0 0 7 9 8 3 4 -6 . 1 4 9 1 . 6 2e -0 9 *** rad 0 . 0 1 4 2 6 7 3 0 . 0 0 2 6 5 5 6 5 . 3 7 3 1 . 2 0e -0 7 *** tax -0 . 0 0 0 6 2 5 8 0 . 0 0 0 1 5 0 5 -4 . 1 5 7 3 . 8 0e -0 5 *** ptratio -0 . 0 3 8 2 7 1 5 0 . 0 0 5 2 3 6 5 -7 . 3 0 9 1 . 1 0e -1 2 *** b 0 . 0 0 0 4 1 3 6 0 . 0 0 0 1 0 7 5 3 . 8 4 7 0 . 0 0 0 1 3 5 *** lstat -0 . 0 2 9 0 3 5 5 0 . 0 0 2 0 2 9 9 -1 4 . 3 0 4 &lt; 2e -1 6 *** Residual standard error : 0 . 1 8 9 9 on 4 9 2 degrees of freedom Multiple R -squared : 0 . 7 8 9 6 , Adjusted R -squared : 0 . 7 8 4 1 F -statistic : 1 4 2 . 1 on 1 3 and 4 9 2 DF , p -value : &lt; 2 . 2e -1 6 &gt; &gt; ols . fit . hc 0 = sqrt ( diag ( hccm ( ols . fit , type = " hc 0 " ))) &gt; ols . fit . hc 1 = sqrt ( diag ( hccm ( ols . fit , type = " hc 1 " ))) &gt; ols . fit . hc 2 = sqrt ( diag ( hccm ( ols . fit , type = " hc 2 " ))) &gt; ols . fit . hc 3 = sqrt ( diag ( hccm ( ols . fit , type = " hc 3 " ))) &gt; ols . fit . hc 4 = sqrt ( diag ( hccm ( ols . fit , type = " hc 4 " ))) &gt; ols . fit . coef = summary ( ols . fit )$ coef &gt; tvalues = ols . fit . coef [ , 1 ]/ + cbind ( ols . fit . coef [ , 2 ] , ols . fit . hc 0 , ols . fit . hc 1 , + ols . fit . hc 2 , ols . fit . hc 3 , ols . fit . hc 4 ) &gt; colnames ( tvalues ) = c ( " ols " , " hc 0 " , " hc 1 " , " hc 2 " , " hc 3 " , " hc 4 " ) &gt; round ( tvalues , 2 ) ols hc 0 hc 1 hc 2 hc 3 hc 4 ( Intercept ) 2 0 . 0 8 1 4 . 2 9 1 4 . 0 9 1 3 . 8 6 1 3 . 4 3 1 3 . 1 3 crim -7 . 8 1 -5 . 3 1 -5 . 2 4 -4 . 8 5 -4 . 3 9 -3 . 5 6 zn 2 . 1 3 2 . 6 8 2 . 6 4 2 . 6 2 2 . 5 6 2 . 5 6 indus 1 . 0 0 1 . 4 6 1 . 4 4 1 . 4 3 1 . 4 0 1 . 4 1 chas 1 2 . 9 3 2 . 6 9 2 . 6 6 2 . 6 2 2 . 5 6 2 . 5 6 nox -5 . 0 9 -4 . 7 9 -4 . 7 2 -4 . 6 7 -4 . 5 6 -4 . 5 4 rm 5 . 4 3 3 . 3 1 3 . 2 6 3 . 2 0 3 . 1 0 3 . 0 2 age 0 . 4 0 0 . 3 3 0 . 3 2 0 . 3 2 0 . 3 1 0 . 3 1 dis -6 . 1 5 -6 . 1 2 -6 . 0 3 -5 . 9 8 -5 . 8 4 -5 . 8 2 rad 5 . 3 7 5 . 2 3 5 . 1 6 5 . 0 5 4 . 8 7 4 . 6 7 tax -4 . 1 6 -5 . 0 5 -4 . 9 8 -4 . 9 0 -4 . 7 6 -4 . 6 9 ptratio -7 . 3 1 -8 . 8 4 -8 . 7 2 -8 . 6 7 -8 . 5 1 -8 . 5 5 b 3 . 8 5 2 . 8 0 2 . 7 6 2 . 7 2 2 . 6 5 2 . 5 9 lstat -1 4 . 3 0 -7 . 8 6 -7 . 7 5 -7 . 6 3 -7 . 4 0 -7 . 2 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Final remarks</head><p>The beauty of the asymptotic analysis and the EHW standard error is that they hold under weak parametric assumptions on the error term. We do not need to modify the OLS estimator but only need to modify the covariance estimator. However, this framework has limitations. First, the proofs are based on limiting theorems that require the sample size to be infinity. We are often unsure whether the sample size is large enough for a particular application we have. Second, the EHW standard errors can be severely biased and have large variability. Finally, under the heteroskedastic linear model, the Gauss-Markov theorem does not hold, so the OLS can be inefficient. We will discuss possible improvements in Chapter 19. Finally, unlike Section 5.3, we cannot create any reasonable prediction intervals for a future observation y n+1 based on (X, Y, x n+1 ) since its variance σ 2 n+1 is fundamentally unknown without further assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Testing linear hypotheses under heteroskedasticity</head><p>Under the heteroskedastic linear model, how to test the hypotheses</p><formula xml:id="formula_130">H 0 : c t β = 0, for c ∈ R p ,</formula><p>and H 0 : Cβ = 0 for C ∈ R l×p with linearly independent rows? 6.2 Two-sample problem continued Continue Problem 5.4. 1. Assume that z 1 , . . . , z m are IID with mean µ 1 and variance σ 2 1 , and w 1 , . . . , w n are IID with mean µ 2 and variance σ 2 2 , and test H 0 : µ 1 = µ 2 . Show that under H 0 , the following t statistic has an asymptotically Normal distribution:</p><formula xml:id="formula_131">t unequal = z -w S 2 z /m + S 2 w /n → N(0, 1)</formula><p>in distribution.</p><p>Remark: The name "unequal" is motivated by the "var.equal" parameter of the R function t.test.</p><p>2. We can write the above problem as testing hypothesis H 0 : β 1 = 0 in the heteroskedastic linear regression. Based on the EHW standard error, we can compute the t statistic. Show that it is identical to t unequal with the HC2 correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">ANOVA with heteroskedasticity</head><p>This is an extension of Problem 5.5 in Chapter 5. Assume y i | i ∈ T j has mean β j and variance σ 2 j , which can be rewritten as a linear model without the Normality and homoskedasticity. In the process of solving Problem 5.5, you have derived the estimator of the covariance matrix of the OLS estimator under homoskedasticity. Find the HC0 and HC2 versions of the EHW covariance matrix. Which covariance matrices do you recommend, and why?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Invariance of the EHW covariance estimator</head><p>If we transform X to X = XΓ where Γ is a p×p non-degenerate matrix, the OLS fit changes from</p><formula xml:id="formula_132">Y = X β + ε to Y = X β + ε,</formula><p>and the associated EHW covariance estimator changes from Vehw to Ṽehw . Show that</p><formula xml:id="formula_133">V = Γ Ṽ Γ t ,</formula><p>and the above result holds for HCj (j = 0, 1, 2, 3, 4). Show that the relationship also holds for the covariance estimator assuming homoskedasticity. Hint: You can use the results in Problems 3.4 and 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Breakdown of the equivalence of the t-statistics based on the EHW standard error</head><p>This problem parallels Problem 5.9. With the data (x i , y i ) n i=1 where both x i and y i are scalars. Run OLS fit of y i on (1, x i ) to obtain t y|x , the t-statistic of the coefficient of x i , based on the EHW standard error. Run OLS fit of x i on (1, y i ) to obtain t x|y , the t-statistic of the coefficient of y i , based on the EHW standard error.</p><p>Give a counterexample with t y|x ̸ = t x|y .</p><p>6.6 Empirical comparison of the standard errors <ref type="bibr" target="#b169">Long and Ervin (2000)</ref> reviewed and compared several commonly-used standard errors in OLS. Redo their simulation and replicate their Figures <ref type="figure" target="#fig_95">1</ref><ref type="figure" target="#fig_1">2</ref><ref type="figure" target="#fig_8">3</ref><ref type="figure" target="#fig_26">4</ref>. They specified more details of their covariate generating process in a technical report <ref type="bibr" target="#b168">(Long and Ervin, 1998)</ref>.</p><p>6.7 Robust standard error in practice <ref type="bibr" target="#b156">King and Roberts (2015)</ref> gave three examples where the EHW standard errors differ from the OLS standard error. I have replicated one example in Section 6.5.</p><p>2. Replicate another one using linear regression although the original analysis used Poisson regression. You can find the datasets used by King and Roberts (2015) at Harvard Dataverse (<ref type="url" target="https://dataverse.harvard.edu/">https://dataverse.harvard.edu/</ref>). 6.8 Unbiased sandwich variance estimator under the Gauss-Markov model Under the Gauss-Markov model with σ 2 i = σ 2 for i = 1, . . . , n, show that the HC0 version of Vehw is biased but the HC2 version of Ṽehw is unbiased for cov( β). se0=0.064 se1=0.064 se2=0.064 se0=0.093 se1=0.071 se2=0.094 se0=0.064 se1=0.064 se2=0.064 se0=0.088 se1=0.06 se2=0.089 Normal non-Normal homoskedastic heteroskedastic 0.6 0.8 1.0 1.2 1.4 0.6 0.8 1.0 1.2 1.4 0 2 4 6 0 1 2 3 4 β de nsity FIGURE 6.1: Simulation with 5000 replications: "se0" denotes the true standard error of β, "se1" denotes the estimated standard error based on the homoskedasticity assumption, and "se2" denotes the Eicker-Huber-White standard error allowing for heteroskedasticity. The density curves are Normal with mean 1 and standard deviation se0.</p><p>Part III</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretation of OLS Based on Partial Regressions</head><p>The Frisch-Waugh-Lovell Theorem</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Long and short regressions</head><p>If we partition X and β into</p><formula xml:id="formula_134">X = X 1 X 2 , β = β 1 β 2 ,</formula><p>where</p><formula xml:id="formula_135">X 1 ∈ R n×k , X 2 ∈ R n×l , β 1 ∈ R k and β 2 ∈ R l , then we can consider the long regression Y = X β + ε = X 1 X 2 β1 β2 + ε = X 1 β1 + X 2 β2 + ε,</formula><p>and the short regression Y = X 2 β2 + ε, where β = β1 β2 and β2 are the OLS coefficients, and ε and ε are the residual vectors from the long and short regressions, respectively. These two regressions are of great interest in practice. For example, we can ask the following questions:</p><p>(Q1) if the true β 1 is zero, then what is the consequence of including X 1 in the long regression?</p><p>(Q2) if the true β 1 is not zero, then what is the consequence of omitting X 1 in the short regression?</p><p>(Q3) what is the difference between β2 and β2 ? Both of them are measures of the "impact" of X 2 on Y . Then why are they different? Does their difference give us any information about β 1 ?</p><p>Many problems in statistics are related to the long and short regressions. We will discuss some applications in Chapter 8 and give a related result in Chapter 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">FWL theorem for the regression coefficients</head><p>The following theorem helps to answer these questions.</p><p>Theorem 7.1 The OLS estimator for β 2 in the short regression is β2 = (X t 2 X 2 ) -1 X t 2 Y , and the OLS estimator for β 2 in the long regression has the following equivalent forms β2 = (X t X) -1 X t Y last l elements (7.1)</p><formula xml:id="formula_136">= {X t 2 (I n -H 1 )X 2 } -1 X t 2 (I n -H 1 )Y where H 1 = X 1 (X t 1 X 1 ) -1 X t 1 (7.2) = ( Xt 2 X2 ) -1 Xt 2 Y where X2 = (I n -H 1 )X 2 (7.3) = ( Xt 2 X2 ) -1 Xt 2 Ỹ where Ỹ = (I n -H 1 )Y. (7.4)</formula><p>This result is often called the Frisch-Waugh-Lovell (FWL) Theorem in econometrics <ref type="bibr" target="#b118">(Frisch and Waugh, 1933;</ref><ref type="bibr" target="#b170">Lovell, 1963)</ref>, although its equivalent forms were also known in classic statistics 1 .</p><p>Before proving Theorem 7.1, I will first discuss its meanings and interpretations. Equation (7.1) follows from the definition of the OLS coefficient. The matrix I n -H 1 in equation (7.2) is the projection matrix onto the space orthogonal to the column space of X 1 . Equation (7.3) states that β2 equals the OLS coefficient of Y on X2 = (I n -H 1 )X 2 , which is the residual matrix from the column-wise OLS fit of X 2 on X 1 2 . So β2 measures the "impact" of X 2 on Y after "adjusting" for the impact of X 1 , that is, it measures the partial or pure "impact" of X 2 on Y . Equation (7.4) is a slight modification of Equation (7.3), stating that β2 equals the OLS coefficient of Ỹ on X2 , where Ỹ = (I n -H 1 )Y is the residual vector from the OLS fit of Y on X 1 . From (7.3) and (7.4), it is not crucial to residualize Y , but it is crucial to residualize X 2 .</p><p>The forms (7.3) and (7.4) suggest the interpretation of β2 as the "impact" of X 2 on Y holding X 1 constant, or in an econometric term, the "impact" of X 2 on Y ceteris paribus. Marshall (1890) used the Latin phrase ceteris paribus. Its English meaning is "with other conditions remaining the same." However, the algebraic meaning of the FWL theorem is that the OLS coefficient of a variable equals the partial regression coefficient based on the residuals. Therefore, taking the Latin phase too seriously may be problematic because Theorem 7.1 is a pure algebraic result without any distributional assumptions. We cannot hold X 1 constant using pure linear algebra. Sometimes, we can manipulate the value of X 1 in an experimental setting, but this relies on the assumption of the data-collecting process.</p><p>There are many ways to prove Theorem 7.1. Below I first take a detour to give an unnecessarily complicated proof because some intermediate steps will be useful for later parts of the book. I will then give a simpler proof which requires a deep understanding of OLS as a linear projection.</p><p>The first proof relies on the following lemma.</p><p>Lemma 7.1 The inverse of X t X is</p><formula xml:id="formula_137">(X t X) -1 = S 11 S 12 S 21 S 22 ,</formula><p>where</p><formula xml:id="formula_138">S 11 = (X t 1 X 1 ) -1 + (X t 1 X 1 ) -1 X t 1 X 2 ( Xt 2 X2 ) -1 X t 2 X 1 (X t 1 X 1 ) -1 , S 12 = -(X t 1 X 1 ) -1 X t 1 X 2 ( Xt 2 X2 ) -1 , S 21 = S t 12 , S 22 = ( Xt 2 X2 ) -1 .</formula><p>1 Professor Alan Agresti gave me the reference of Yule (1907).</p><p>2 See Problem 3.7 for more details.</p><p>I leave the proof of Lemma 7.1 as Problem 7.1. With Lemma 7.1, we can easily prove Theorem 7.1. Proof of Theorem 7.1: (Version 1) The OLS coefficient is</p><formula xml:id="formula_139">β1 β2 = (X t X) -1 X t Y = S 11 S 12 S 21 S 22 X t 1 Y X t 2 Y</formula><p>.</p><p>Then using Lemma 7.1, we can simplify β2 as</p><formula xml:id="formula_140">β2 = S 21 X t 1 Y + S 22 X t 2 Y = -( Xt 2 X2 ) -1 X t 2 X 1 (X t 1 X 1 ) -1 X t 1 Y + ( Xt 2 X2 ) -1 X t 2 Y = -( Xt 2 X2 ) -1 X t 2 H 1 Y + ( Xt 2 X2 ) -1 X t 2 Y = ( Xt 2 X2 ) -1 X t 2 (I n -H 1 )Y (7.5) = ( Xt 2 X2 ) -1 Xt 2 Y.</formula><p>(7.6) Equation (7.5) is the form (7.2), and Equation (7.6) is the form (7.3). Because we also have</p><formula xml:id="formula_141">X t 2 (I n -H 1 )Y = X t 2 (I n -H 1 ) 2 Y = Xt 2 Ỹ</formula><p>, we can write β2 as β2 = ( Xt 2 X2 ) -1 Xt 2 Ỹ , giving the form (7.4).</p><p>□ The second proof does not invert the block matrix of X t X directly. Proof of Theorem 7.1: (Version 2) The OLS decomposition Y = X 1 β1 +X 2 β2 + ε satisfies</p><formula xml:id="formula_142">X t ε = 0 =⇒ X 1 X 2 t ε = 0 =⇒ X t 1 ε = 0, X t 2 ε = 0.</formula><p>Multiplying I n -H 1 on both sides of the OLS decomposition, we have</p><formula xml:id="formula_143">(I n -H 1 )Y = (I n -H 1 )X 1 β1 + (I n -H 1 )X 2 β2 + (I n -H 1 )ε, which reduces to (I n -H 1 )Y = (I n -H 1 )X 2 β2 + ε because (I n -H 1 )X 1 = 0 and (I n -H 1 )ε = ε -H 1 ε = ε -X 1 (X t 1 X 1 ) -1 X t 1 ε = ε. Further multiplying X t</formula><p>2 on both sides of the identity, we have</p><formula xml:id="formula_144">X t 2 (I n -H 1 )Y = X t 2 (I n -H 1 )X 2 β2</formula><p>because X t 2 ε = 0. The FWL theorem follows immediately. A subtle issue in this proof is to verify that X t 2 (I n -H 1 )X 2 is invertible. It is easy to show that matrix X t 2 (I n -H 1 )X 2 is positive semi-definite. To show it has rank l, we only need to show that</p><formula xml:id="formula_145">u t 2 X t 2 (I n -H 1 )X 2 u 2 = 0 =⇒ u 2 = 0. We have u t 2 X t 2 (I n -H 1 )X 2 u 2 = ∥(I n -H 1 )X 2 u 2 ∥ 2 = 0, so (I n -H 1 )X 2 u 2 = 0, which further implies X 2 u 2 ∈ C(X 1 ) by Proposition 3.1. That is, X 2 u 2 = X 1 u 1 for some u 1 . So X 1 u 1 -X 2 u 2 = 0.</formula><p>Since the columns of X are linearly independent, we must have u 1 = 0 and u 2 = 0. □ I will end this section with two byproducts of the FWL theorem. First, X2 is the residual matrix from the OLS fit of X 2 on X 1 . It is an n×l matrix with linearly independent columns as shown in the proof of Theorem 7.1 (Version 2) and induces a projection matrix</p><formula xml:id="formula_146">H2 = X2 ( Xt 2 X2 ) -1 Xt 2 .</formula><p>This projection matrix is closely related to the projection matrices induced by X and X 1 as shown in the following lemma.</p><p>Lemma 7. <ref type="bibr">2</ref> We have</p><formula xml:id="formula_147">H 1 H2 = H2 H 1 = 0, H = H 1 + H2 .</formula><p>Lemma 7.2 is purely algebraic. I leave the proof as Problem 7.3. The first two identities imply that the column space of X2 is orthogonal to the column space of X 1 . The last identity H = H 1 + H2 has a clear geometric interpretation. For any vector v ∈ R n , we have Hv = H 1 v + H2 v, so the projection of v onto the column space of X equals the summation of the projection of v onto the column space of X 1 and the projection of v onto the column space of X2 . Importantly, H ̸ = H 1 + H 2 in general.</p><p>Second, we can obtain β2 from (7.3) or (7.4), which corresponds to the partial regression of Y on X2 or the partial regression of Ỹ on X2 . We can verify that the residual vector from the second partial regression equals the residual vector from the full regression.</p><p>Corollary 7.1 We have ε = ê, where ε is the residual vector from the OLS fit of Y on X and ê is the residual vector from the OLS fit of Ỹ on X2 , respectively.</p><p>It is important to note that this conclusion is only true if both Y and X 2 are residualized. The conclusion does not hold if we only residualize X 2 . See Problem 7. It suffices to show that I-H = (I-H2 )(I-H 1 ), or, equivalently, I-H = I-H 1 -H2 + H2 H 1 . This holds due to Lemma 7.2. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">FWL theorem for the standard errors</head><p>Based on the OLS fit of Y on X, we have two estimated covariances for the second component β2 : V assuming homoskedasticity and Vehw allowing for heteroskedasticity.</p><p>The FWL theorem demonstrates that we can also obtain β2 from the OLS fit of Ỹ on X2 . Then based on this partial regression, we have two estimated covariances for β2 : Ṽ assuming homoskedasticity and Ṽehw allowing for heteroskedasticity.</p><p>The following theorem establishes their equivalence.</p><p>Theorem 7.2 (n -k -l) V = (n -l) Ṽ and Vehw = Ṽehw .</p><p>Theorem 7.1 is well known for a long time but Theorem 7.2 is less well known. <ref type="bibr" target="#b170">Lovell (1963)</ref> hinted at the first identity in Theorem 7.2, and Ding (2021a) proved Theorem 7.2. Proof of Theorem 7.2: By Corollary 7.1, the full regression and partial regression have the same residual vector, denoted by ε. Therefore, Ωehw = Ωehw = diag{ε 2 } in the EHW covariance estimator.</p><p>Based on the full regression, define σ2 = ∥ε∥ 2 2 /(n -k -l). Then V equals the (2, 2)th block of σ2 (X t X) -1 , and Vehw equals the (2, 2)th block of (X t X) -1 X t Ωehw X(X t X) -1 . Based on the partial regression, define σ2 = ∥ε∥ 2 2 /(n -l). Then Ṽ = σ2 ( Xt 2 X2 ) -1 and Ṽehw = ( Xt 2 X2 ) -1 Xt 2 Ωehw X2 ( Xt 2 X2 ) -1 . Let σ2 = ∥ε∥ 2 /(n -k -l) and σ2 = ∥ε∥ 2 /(n -l) be the common variance estimators. They are identical up to the degrees of freedom correction. Under homoskedasticity, the covariance estimator for β2 is the (2, 2)th block of σ2 (X t X) -1 , that is, σ2 S 22 = σ2 ( Xt 2 X2 ) -1 by Lemma 7.1, which is identical to the covariance estimator for β2 up to the degrees of freedom correction.</p><p>The EHW covariance estimator from the full regression is the (2, 2) block of Â Ωehw Ât , where</p><formula xml:id="formula_148">Â = (X t X) -1 X t = * -( Xt 2 X2 ) -1 X t 2 H 1 + ( Xt 2 X2 ) -1 X t 2 = * ( Xt 2 X2 ) -</formula><p>1 Xt 2 by Lemma 7.1. I omit the * term because it does not affect the final calculation. Define Ã2 = ( Xt 2 X2 ) -1 Xt 2 , and then Vehw = Ã2 Ωehw Ãt 2 = Ã2 Ωehw Ãt 2 , which equals the EHW covariance estimator Ṽehw from the partial regression. □ 7.4 Gram-Schmidt orthogonalization, QR decomposition, and the computation of OLS</p><p>When the regressors are orthogonal, the coefficients from the long and short regressions are identical, which simplifies the calculation and theoretical discussion.</p><p>Corollary 7.2 If X t 1 X 2 = 0, i.e., the columns of X 1 and X 2 are orthogonal, then X2 = X 2 and β2 = β2 .</p><p>Proof of Corollary 7.2: We can directly prove Corollary 7.2 by verifying that X t X is block diagonal.</p><p>Alternatively, Corollary 7.2 follows directly from</p><formula xml:id="formula_149">X2 = (I n -H 1 )X 2 = X 2 -X 1 (X t 1 X 1 ) -1 X t 1 X 2 = X 2 ,</formula><p>and Theorem 7.1. □ This simple fact motivates us to orthogonalize the columns of the covariate matrix, which in turn gives the famous QR decomposition in linear algebra. Interestingly, the lm function in R uses the QR decomposition to compute the OLS estimator. To facilitate the discussion, I will use the notation βV2|V1 V 1 as the linear projection of the vector</p><formula xml:id="formula_150">V 2 ∈ R n onto the vector V 1 ∈ R n , where βV2|V1 = V t 2 V 1 /V t 1 V 1 . This is from the univariate OLS of V 2 on V 1 .</formula><p>With a slight abuse of notation, partition the covariate matrix into column vectors X = (X 1 , . . . , X p ). The goal is to find orthogonal vectors (U 1 , . . . , U p ) that generate the same column space as X. Start with</p><formula xml:id="formula_151">X 1 = U 1 .</formula><p>Regress X 2 on U 1 to obtain the fitted and residual vector</p><formula xml:id="formula_152">X 2 = βX2|U1 U 1 + U 2 ;</formula><p>by OLS, U 1 and U 2 must be orthogonal. Regress X 3 on (U 1 , U 2 ) to obtain the fitted and residual vector</p><formula xml:id="formula_153">X 3 = βX3|U1 U 1 + βX3|U2 U 2 + U 3 ;</formula><p>by Corollary 7.2, the OLS reduces to two univariate OLS and ensures that U 3 is orthogonal to both U 1 and U 2 . This justifies the notation βX3|U1 and βX3|U2 . Continue this procedure to the last column vector:</p><formula xml:id="formula_154">X p = p-1 j=1 βXp|Uj U j + U p ;</formula><p>by OLS, U p is orthogonal to all U j (j = 1, . . . , p -1). We further normalize the U vectors to have unit length:</p><formula xml:id="formula_155">Q j = U j /∥U j ∥, (j = 1, . . . , p).</formula><p>The whole process is called the Gram-Schmidt orthogonalization, which is essentially the sequential OLS fits. This process generates an n×p matrix with orthonormal column vectors</p><formula xml:id="formula_156">Q = (Q 1 , . . . , Q p ).</formula><p>More interestingly, the column vectors of X and Q can linearly represent each other because</p><formula xml:id="formula_157">X = (X 1 , . . . , X p ) = (U 1 , . . . , U p )      1 βX2|U1 βX3|U1 • • • βXp|U1 0 1 βX3|U2 • • • βXp|U2 . . . . . . . . . • • • . . . 0 0 0 • • • 1      = Qdiag{∥U j ∥} p j=1      1 βX2|U1 βX3|U1 • • • βXp|U1 0 1 βX3|U2 • • • βXp|U2 . . . . . . . . . • • • . . . 0 0 0 • • • 1     </formula><p>.</p><p>We can verify that the product of the second and the third matrix is an upper triangular matrix, denoted by R. By definition, the jth diagonal element of R equals ∥U j ∥, and the (j, j ′ )th element of R equals ∥U j ∥ βX j ′ |Uj for j ′ &gt; j. Therefore, we can decompose X as</p><formula xml:id="formula_158">X = QR</formula><p>where Q is an n × p matrix with orthonormal columns and R is a p × p upper triangular matrix. This is called the QR decomposition of X.</p><p>Most software packages, for example, R, do not calculate the inverse of X t X directly. Instead, they first find the QR decomposition of X = QR. Since the Normal equation simplifies to</p><formula xml:id="formula_159">X t X β = X t Y, R t Q t QR β = R t Q t Y, R β = Q t Y,</formula><p>they then backsolve the linear equation since R is upper triangular.</p><p>In R, the qr function returns the QR decomposition of a matrix.</p><formula xml:id="formula_160">&gt; X = matrix ( rnorm ( 7 * 3 ) , 7 , 3 ) &gt; X [ , 1 ] [ , 2 ] [ , 3 ] [<label>1</label></formula><p>,] -0 . 5 7 2 3 1 2 2 3 0 . 1 1 9 6 3 2 5 0 . 8 0 8 7 5 0 5 [ 2 ,] -1 . 7 6 0 9 0 2 2 5 1 . 0 6 2 7 6 3 1 1 . 8 1 7 0 3 6 1 [ 3 ,] -0 . 0 4 1 4 4 2 8 1 -0 . 2 9 0 4 7 4 9 -1 . 8 3 7 2 2 4 7 [ 4 ,] -0 . 3 7 6 2 7 8 2 1 0 . 4 4 7 6 9 3 2 -0 . 9 6 2 9 3 2 0 [ 5 ,] -1 . 4 0 8 4 8 0 2 7 0 . 2 7 3 5 4 0 8 -0 . 8 0 4 7 9 1 7 [ 6 ,] 1 . 8 4 8 7 8 5 1 8 0 . 7 2 9 0 0 0 5 1 . 2 6 8 8 9 2 9 [ 7 ,] 0 . 0 6 4 3 2 8 5 6 0 . 2 2 5 6 2 8 4 0 . 3 9 7 2 2 2 9 &gt; qrX = qr ( X ) <ref type="table">1 9 1 0 0 8 7 8 -0 . 0 3 4 6 0 6 1 7 0 . 3 0 3 4 0 4 8 1  [ 2 ,] -0 . 5 8 7 6 9 9 8 1 -0 . 6 0 4 4 2 9 2 8 0 . 2 3 7 5 3 9 0 0  [ 3 ,] -0 . 0 1 3 8 3 1 5 1 0 . 2 1 1 9 1 9 9 1 -0 . 5 5 8 3 9 9 2 8  [ 4 ,] -0 . 1 2 5 5 8 2 5 7 -0 . 2 8 7 2 8 4 0 3 -0 . 6 2 8 6 4 7 5 0  [ 5 ,] -0 . 4 7 0 0 7 9 2 4 -0 . 0 7 0 2 0 0 7 6 -0 . 3 6 6 4 0 9 3 8  [ 6</ref> ,] 0 . 6 1 7 0 3 0 6 7 -0 . 6 8 7 7 8 4 1 1 -0 . 0 9 9 9 9 8 5 9 [ 7 ,] 0 . 0 2 1 4 6 9 6 1 -0 . 1 6 7 4 8 2 4 6 0 . 0 1 6 0 5 4 9 3 &gt; qr . R ( qrX )</p><formula xml:id="formula_161">&gt; qr . Q ( qrX ) [ , 1 ] [ , 2 ] [ , 3 ] [ 1 ,] -0 .</formula><p>[ , 1 ] [ , 2 ] [ , 3 ] [ 1 ,] 2 . 9 9 6 2 6 1 -0 . 3 7 3 5 6 7 3 0 . 0 9 3 7 7 8 8 [ 2 ,] 0 . 0 0 0 0 0 0 -1 . 3 9 5 0 6 4 2 -2 . 1 2 1 7 2 2 3 [ 3 ,] 0 . 0 0 0 0 0 0 0 . 0 0 0 0 0 0 0 2 . 4 8 2 6 1 8 6</p><p>If we specify qr = TRUE in the lm function, it will also return the QR decomposition of the covariate matrix.</p><formula xml:id="formula_162">&gt; Y = rnorm ( 7 ) &gt; lmfit = lm ( Y ~0 + X , qr = TRUE ) &gt; qr . Q ( lmfit $ qr ) [ , 1 ] [ , 2 ] [ , 3 ] [<label>1</label></formula><p>,] -0 . 4 3 5 3 5 0 5 4 -0 . 2 5 6 7 9 8 2 3 -0 . 6 5 4 8 0 4 0 0 [ 2 ,] -0 . 4 7 0 9 1 2 7 5 -0 . 1 3 6 3 9 4 5 9 0 . 1 4 7 4 6 4 4 4 [ 3 ,] 0 . 6 6 4 9 4 5 3 2 0 . 0 7 7 2 5 4 3 5 -0 . 3 9 4 3 6 2 6 5 [ 4 ,] 0 . 2 1 1 3 6 3 4 7 -0 . 7 8 8 1 4 7 3 7 0 . 3 4 6 1 1 8 2 0 [ 5 ,] -0 . 0 4 4 9 3 3 5 6 -0 . 4 0 4 1 3 8 2 9 -0 . 0 1 1 5 6 2 7 3 [ 6 ,] -0 . 2 8 0 4 6 5 0 4 0 . 1 0 6 5 5 7 5 5 -0 . 1 6 1 9 3 2 5 1 [ 7 ,] 0 . 1 4 5 6 1 8 0 8 -0 . 3 3 7 0 8 2 1 9 -0 . 4 9 7 8 0 5 6 1 &gt; qr . R ( lmfit $ qr ) X 1 X 2 X 3 1 3 . 1 9 0 0 3 5 -0 . 6 9 6 4 2 6 9 1 . 8 6 9 3 2 6 0 2 0 . 0 0 0 0 0 0 2 . 0 7 1 9 7 8 7 1 . 9 2 1 0 2 1 2 3 0 . 0 0 0 0 0 0 0 . 0 0 0 0 0 0 0 -0 . 9 2 6 1 9 2 1</p><p>The replicating R code is in code7.4.R<ref type="foot" target="#foot_4">foot_4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Homework problems</head><p>7.1 Inverse of a block matrix Prove Lemma 7.1 and the following alternative form:</p><formula xml:id="formula_163">(X t X) -1 = Q 11 Q 12 Q 21 Q 22 ,</formula><p>where</p><formula xml:id="formula_164">H 2 = X 2 (X t 2 X 2 ) -1 X t 2 , X1 = (I n -H 2 )X 1 , and Q 11 = ( Xt 1 X1 ) -1 , Q 12 = -( Xt 1 X1 ) -1 X t 1 X 2 (X t 2 X 2 ) -1 , Q 21 = Q t 12 , Q 22 = (X t 2 X 2 ) -1 + (X t 2 X 2 ) -1 X t 2 X 1 ( Xt 1 X1 ) -1 X t 1 X 2 (X t 2 X 2 ) -1 .</formula><p>Hint: Use the formula in Problem 1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Residuals in the FWL theorem</head><p>Give an example in which the residual vector from the partial regression of Y on X2 does not equal to the residual vector from the full regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Projection matrices</head><p>Prove Lemma 7.2. Hint: Use Lemma 7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">FWL theorem and leverage scores</head><p>Consider the partitioned regression Y = X 1 β1 + X 2 β2 + ε. To obtain the coefficient β2 , we can run two OLS fits:</p><p>(R1) regress X 2 on X 1 to obtain the residual X2 ;</p><p>(R2) regress Y on X2 to obtain the coefficient, which equals β2 by the FWL Theorem.</p><p>Although partial regression (R2) can recover the OLS coefficient, the leverage scores from (R2) are not the same as those from the long regression. Show that the summation of the corresponding leverage scores from (R1) and (R2) equals the leverage scores from the long regression.</p><p>Remark: The leverage scores are the diagonal elements of the hat matrix from OLS fits. Chapter 6 before mentioned them and Chapter 11 later will discuss them in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Another invariance property of the OLS coefficient</head><p>Partition the covariate matrix as X = (X 1 , X 2 ) where X 1 ∈ R n×k and X 2 ∈ R n×l . Given any A ∈ R k×l , define X2 = X 2 -X 1 A. Fit two OLS:</p><formula xml:id="formula_165">Y = β1 X 1 + β2 X 2 + ε and Y = β1 X 1 + β2 X2 + ε.</formula><p>Show that β2 = β2 , ε = ε.</p><p>Hint: Use the result in Problem 3.4. Remark: Choose A = (X t 1 X 1 ) -1 X t 1 X 2 to be the coefficient matrix of the OLS fit of X 2 on X 1 . The above result ensures that β2 equals β2 from the OLS fit of Y on X 1 and (I n -H 1 )X 2 , which is coherent with the FWL theorem since X t 1 (I n -H 1 )X 2 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Alternative formula for the EHW standard error</head><p>Consider the partition regression Y = X 1 β1 + X 2 β2 + ε with X 1 is an n × (p -1) matrix and X 2 is an n dimensional vector. So β2 is a scalar, and the (p, p)th element of Vehw equals ŝe 2 ehw,2 , the squared EHW standard error for β2 . Define</p><formula xml:id="formula_166">X2 = (I n -H 1 )X 2 =    x12 . . . xn2    .</formula><p>Prove that under Assumption 6.1, we have</p><formula xml:id="formula_167">var( β2 ) = n i=1 w i σ 2 i , ŝe 2 ehw,2 = n i=1 w i ε2 i</formula><p>where</p><formula xml:id="formula_168">w i = x2 i2 ( n i=1 x2 i2 ) 2</formula><p>. Remark: You can use Theorems 7.1 and 7.2 to prove the result. The original formula of the EHW covariance matrix has a complex form. However, using the FWL theorems, we can simplify each of the squared EHW standard errors as a weighted average of the squared residuals, or, equivalently, a simple quadratic form of the residual vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">A counterexample to the Gauss-Markov theorem</head><p>The Gauss-Markov theorem does not hold under the heteroskedastic linear model. This problem gives a counterexample in a simple linear model.</p><p>Assume y i = βx i + ε i without the intercept and with potentially different var</p><formula xml:id="formula_169">(ε i ) = σ 2 i across i = 1, . . . ,</formula><p>n. Consider two OLS estimators: the first OLS estimator does not contain the intercept β = n i=1 x i y i / n i=1 x 2 i ; the second OLS estimator contains the intercept β 2 even though the true linear model does not contain the intercept.</p><formula xml:id="formula_170">= n i=1 (x i -x)y i / n i=1 (x i -x)</formula><p>The Gauss-Markov theorem ensures that if σ 2 i = σ 2 for all i's, then the variance of β is smaller than or equal to the variance of β. However, it does not hold when σ 2 i 's vary. Give a counterexample in which the variance of β is larger than the variance of β.</p><p>7.8 QR decomposition of X and the computation of OLS Verify that the R matrix equals</p><formula xml:id="formula_171">R =      Q t 1 X 1 Q t 1 X 2 • • • Q t 1 X p 0 Q t 2 X 2 • • • Q t 2 X p . . . . . . • • • . . . 0 0 • • • Q t p X p      .</formula><p>Based on the QR decomposition of X, show that</p><formula xml:id="formula_172">H = QQ t ,</formula><p>and h ii equals the squared length of the i-th row of Q.</p><p>7.9 Uniqueness of the QR decomposition Show that if X has linearly independent column vectors, the QR decomposition must be unique. That is, if X = QR = Q 1 R 1 where Q and Q 1 have orthonormal columns and R and R 1 are upper triangular, then we must have</p><formula xml:id="formula_173">Q = Q 1 , R = R 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications of the Frisch-Waugh-Lovell Theorem</head><p>The FWL theorem has many applications, and I will highlight some of them in this chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Centering regressors</head><p>As a special case, partition the covariate matrix into X = (X 1 , X 2 ) with X 1 = 1 n . This is the usual case including the constant as the first regressor. The projection matrix</p><formula xml:id="formula_174">H 1 = 1 n (1 t n 1 n ) -1 1 t n = n -1 1 n 1 t n =    n -1 • • • n -1 . . . . . . n -1 • • • n -1    ≡ A n</formula><p>contains n -1 's as its elements, and</p><formula xml:id="formula_175">C n = I n -n -1 1 n 1 t n</formula><p>is the projection matrix orthogonal to 1 n . Multiplying any vector by A n is equivalent to obtaining the average of its components, and multiplying any vector by C n is equivalent to centering that vector, for example,</p><formula xml:id="formula_176">A n Y =    ȳ . . . ȳ    = ȳ1 n ,<label>and</label></formula><formula xml:id="formula_177">C n Y =    y 1 - ȳ . . . y n - ȳ    .</formula><p>More generally, multiplying any matrix by A n is equivalent to averaging each column, and multiplying any matrix by C n is equivalent to centering each column of that matrix, for example,</p><formula xml:id="formula_178">A n X 2 =    xt 2 . . . xt 2    = 1 n xt 2 ,</formula><p>and</p><formula xml:id="formula_179">C n X 2 =    x t 12 -xt 2 . . . x t n2 -xt 2    ,</formula><p>where X 2 contains row vectors x t 12 , . . . , x t n2 with average x2 = n -1 n i=1 x i2 . The FWL theorem implies that the coefficient of X 2 in the OLS fit of Y on (1 n , X 2 ) equals the coefficient of C n X 2 in the OLS fit of C n Y on C n X 2 , that is, the OLS fit of the centered response vector on the column-wise centered X 2 . An immediate consequence is that if each column is centered in the design matrix, then to obtain the OLS coefficients, it does not matter whether to include the column 1 n or not.</p><p>The centering matrix C n has another property: its quadratic form equals the sample variance multiplied by n -1, for example,</p><formula xml:id="formula_180">Y t C n Y = Y t C t n C n Y = (y 1 -ȳ, . . . , y n -ȳ)    y 1 - ȳ . . . y n - ȳ    = n i=1 (y i -ȳ) 2 = (n -1)σ 2 y ,</formula><p>where σ2 y is the sample variance of the outcomes. For an n × p matrix X,</p><formula xml:id="formula_181">X t C n X =    X t 1 . . . X t p    C n X 1 • • • X p =    X t 1 C n X 1 • • • X t 1 C n X p . . . . . . X t p C n X 1 • • • X t p C n X p    = (n -1)    σ11 • • • σ1p . . . . . . σp1 • • • σpp    , where σj1j2 = (n -1) -1 n i=1 (x ij1 -x•j1 )(x ij2 -x•j2 )</formula><p>is the sample covariance between X j1 and X j2 . So (n -1) -1 X t C n X equals the sample covariance matrix of X. For these reason, I choose the notation C n with "C" for both "centering" and "covariance."</p><p>In another important special case, X 1 contains the dummies for a discrete variable, for example, the indicators for different treatment levels or groups. See Example 3.2 for the background. With k groups, X 1 can take the following two forms:</p><formula xml:id="formula_182">X 1 =                     1 1 • • • 0 . . . . . . . . . 1 1 • • • 0 . . . . . . . . . 1 0 • • • 1 . . . . . . . . . 1 0 1 1 0 0 . . . . . . . . . 1 0 • • • 0                     n×k or X 1 =              1 • • • 0 . . . . . . 1 • • • 0 . . . . . . 0 • • • 1 . . . . . . 0 • • • 1              n×k , (8.1)</formula><p>where the first form of X 1 contains 1 n and k -1 dummy variables, and the second form of X 1 contains k dummy variables. In both forms of X 1 , the observations are sorted according to the group indicators. If we regress Y on X 1 , the residual vector is</p><formula xml:id="formula_183">Y -              ȳ[1] . . . ȳ[1] . . . ȳ[k] . . . ȳ[k]              ,<label>(8.2)</label></formula><p>where ȳ[1] , . . . , ȳ[k] are the averages of the outcomes within groups 1, . . . , k. Effectively, we center Y by group-specific means. Similarly, if we regress X 2 on X 1 , we center each column of X 2 by the group-specific means. Let Y c and X c 2 be the centered response vector and design matrix. The FWL theorem implies that the OLS coefficient of X 2 in the long regression is the OLS coefficient of X c 2 in the partial regression of Y c on X c 2 . When k is large, running the OLS with centered variables can reduce the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Partial correlation coefficient and Simpson's paradox</head><p>The sample Pearson correlation coefficient between n observations of two scalars (x i , y i )</p><formula xml:id="formula_184">n i=1 ρyx = n i=1 (x i -x)(y i -ȳ) n i=1 (x i -x) 2 n i=1 (y i -ȳ) 2</formula><p>measures the linear relationship between x and y. How do we measure the linear relationship between x and y after controlling for some other variables w ∈ R k-1 ? Intuitively, we can measure it with the sample Pearson correlation coefficient based on the residuals from the following two OLS fits: (R1) run OLS of Y on (1, W ) and obtain residual vector εy and residual sum of squares rss y ;</p><p>With εy and εx , we can define the sampling partial correlation coefficient between x and y given w as ρyx|w = n i=1 εx,i εy,i</p><formula xml:id="formula_185">n i=1 ε2 x,i n i=1 ε2 y,i .</formula><p>In the above definition, we do not center the residuals because they have zero sample means due to the inclusions of the intercept in the OLS fits (R1) and <ref type="bibr">(R2)</ref>. The sample partial correlation coefficient determines the coefficient of εx in the OLS fit of εy on εx :</p><formula xml:id="formula_186">βyx|w = n i=1 εx,i εy,i n i=1 ε2 x,i = ρyx|w n i=1 ε2 y,i n i=1 ε2 x,i = ρyx|w σy|w σx|w ,<label>(8.3)</label></formula><p>where σ2 y|w = rss y /(n -k) and σ2 x|w = rss x /(n -k) are the variance estimators based on regressions (R1) and (R2) motivated by the Gauss-Markov model. Based on the FWL theorem, βyx|w equals the OLS coefficient of X in the long regression of Y on (1, X, W ). Therefore, (8.3) is the Galtonian formula for multiple regression, which is analogous to that for univariate regression (1.1).</p><p>To investigate the relationship between y and x, different researchers may run different regressions. One may run OLS of Y on (1, X, W ), and the other may run OLS of Y on (1, X, W ′ ), where W ′ is a subset of W . Let βyx|w be the coefficient of X in the first regression, and let βyx|w ′ be the coefficient of X in the second regression. Mathematically, it is possible that these two coefficients have different signs, which is called Simpson's paradox<ref type="foot" target="#foot_5">foot_5</ref> . It is a paradox because we expect both coefficients to measure the "impact" of X on Y . Because these two coefficients have the same signs as the partial correlation coefficients ρyx|w and ρyx|w ′ , Simpson's paradox is equivalent to ρyx|w ρyx|w ′ &lt; 0.</p><p>To simplify the presentation, we discuss the special case with w ′ being an empty set. Simpson's paradox is then equivalent to ρyx|w ρyx &lt; 0.</p><p>The following theorem gives an expression linking ρyx|w and ρyx .</p><formula xml:id="formula_187">Theorem 8.1 For Y, X, W ∈ R n , we have ρyx|w = ρyx -ρyw ρxw 1 -ρ2 yw 1 -ρ2 xw .</formula><p>Its proof is purely algebraic, so I leave it as Problem 8.6. Theorem 8.1 states that we can obtain the sample partial correlation coefficient based on the three pairwise correlation coefficients. Figure <ref type="figure" target="#fig_13">8</ref>.1 illustrates the interplay among three variables. In particular, the correlation between x and y is due to two "pathways": the one acting through w and the one acting independent of w. The first path way is related to the product term ρyw ρxw , and the second pathway is related to ρyx|w . This gives some intuition for Theorem 8.1. We can observe Simpson's Paradox in the following simulation with the R code in code8.2.R.</p><p>&gt; n = 1 0 0 0 &gt; w = rbinom (n , 1 , 0 . 5 ) &gt; x 1 = rnorm (n , -1 , 1 ) &gt; x 0 = rnorm (n , 2 , 1 ) &gt; x = ifelse (w , x 1 , x 0 ) &gt; y = x + 6 * w + rnorm ( n ) &gt; fit . xw = lm ( y ~x + w )$ coef &gt; fit . x = lm ( y ~x )$ coef &gt; fit . xw ( Intercept ) x w 0 . 0 5 6 5 5 4 4 2 0 . 9 7 9 6 9 9 0 7 5 . 9 2 5 1 7 0 7 2 &gt; fit . x ( Intercept ) x 3 . 6 4 2 2 9 7 8 -0 . 3 7 4 3 3 6 8</p><p>Because w is binary, we can plot (x, y) in each group of w = 1 and w = 0 in Figure <ref type="figure" target="#fig_13">8</ref>.2. In both groups, y and x are positively associated with positive regression coefficients; but in the pooled data, y and x are negatively associated with a negative regression coefficient. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -2.5 0.0 The two solid regression lines are fitted separately using the data from two groups, and the dash regression line is fitted using the pooled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Hypothesis testing and analysis of variance</head><p>Partition X and β into</p><formula xml:id="formula_188">X = X 1 X 2 , β = β 1 β 2 ,</formula><p>where</p><formula xml:id="formula_189">X 1 ∈ R n×k , X 2 ∈ R n×l , β 1 ∈ R k and β 2 ∈ R l .</formula><p>We are often interested in testing <ref type="bibr">(8.4)</ref> where ε ∼ N(0, σ 2 I n ). If H 0 holds, then X 2 is redundant and a short regression suffices:</p><formula xml:id="formula_190">H 0 : β 2 = 0 in the long regression Y = Xβ + ε = X 1 β 1 + X 2 β 2 + ε,</formula><formula xml:id="formula_191">Y = X 1 β + ε. (8.5)</formula><p>This is a special case of testing Cβ = 0 with</p><formula xml:id="formula_192">C = 0 l×k I l×l .</formula><p>As discussed before, we can use β2 ∼ N(0, σ 2 S 22 )</p><p>with S 22 = ( Xt 2 X2 ) -1 being the (2, 2)th block of (X t X) -1 by Lemma 7.1, to construct the Wald-type statistic for hypothesis testing:</p><formula xml:id="formula_193">F Wald = βt 2 (S 22 ) -1 β2 lσ 2 = βt 2 Xt 2 X2 β2 lσ 2 ∼ F l,n-p .</formula><p>Now I will discuss testing H 0 from an alternative perspective based on comparing the residual sum of squares in the long regression (8.4) and the short regression (8.5). This technique is called the analysis of variance (ANOVA), pioneered by R. A. Fisher in the design and analysis of experiments. Intuitively, if β 2 = 0, then the residual vectors from the long regression (8.4) and the short regression (8.5) should not be "too different." However, with the error term ε, these residuals are random, then the key is to quantify the magnitude of the difference. Define</p><formula xml:id="formula_194">rss long = Y t (I n -H)Y and rss short = Y t (I n -H 1 )Y</formula><p>as the residual sum of squares from the long and short regressions, respectively. By the definition of OLS, it must be true that</p><formula xml:id="formula_195">rss long ≤ rss short and rss short -rss long = Y t (H -H 1 )Y ≥ 0. (8.6)</formula><p>To understand the magnitude of the change in the residual sum of squares, we can standardize the above difference and define</p><formula xml:id="formula_196">F anova = (rss short -rss long )/l rss long /(n -p) ,</formula><p>In the definition of the above statistic, l and n -p are the degrees of freedom to make the mathematics more elegant, but they do not change the discussion fundamentally. The denominator of F anova is σ2 , so we can also write it as</p><formula xml:id="formula_197">F anova = rss short -rss long lσ 2 . (8.7)</formula><p>The following theorem states that these two perspectives yield an identical test statistic.</p><p>Theorem 8.2 Under Assumption 5.1, if β 2 = 0, then F anova ∼ F l,n-p . In fact, F anova = F Wald which is a numerical result without Assumption 5.1.</p><p>I divide the proof into two parts. The first part derives the exact distribution of F anova under the Normal linear model. It relies on the following lemma on the basic properties of the projection matrices. I relegate its proof to Problem 8.9. Lemma 8.1 We have</p><formula xml:id="formula_198">HX 1 = X 1 , HX 2 = X 2 , HH 1 = H 1 , H 1 H = H 1</formula><p>Moreover, H -H 1 is a projection matrix of rank p -k = l, I n -H is a projection matrix of rank n -p, and they are orthogonal:</p><formula xml:id="formula_199">(H -H 1 )(I n -H) = 0. (8.8)</formula><p>Proof of Theorem 8.2 (Part I):</p><p>The residual vector from the long regression is ε = (I n -H)Y = (I n -H)(Xβ + ε) = (I n -H)ε, so the residual sum of squares is</p><formula xml:id="formula_200">rss long = εt ε = ε t (I n -H)ε;</formula><p>since β 2 = 0, the residual vector from the short regression is ε = (</p><formula xml:id="formula_201">I n -H 1 )Y = (I n - H 1 )(X 1 β 1 + ε) = (I n -H 1 )ε, so the residual sum of squares is rss short = εt ε = ε t (I n -H 1 )ε.</formula><p>Let ε 0 = ε/σ ∼ N(0, I n ) be a standard Normal random vector, then we can write F anova as</p><formula xml:id="formula_202">F anova = ε t (H -H 1 )ε/l ε t (I n -H)ε/(n -p) = ε t 0 (H -H 1 )ε 0 /l ε t 0 (I n -H)ε 0 /(n -p) = ∥(H -H 1 )ε 0 ∥ 2 /l ∥(I n -H)ε 0 ∥ 2 /(n -p)</formula><p>.</p><p>(8.9) Therefore, we have the following joint Normality using the basic fact (8.8):</p><formula xml:id="formula_203">(H -H 1 )ε 0 (I n -H)ε 0 = H -H 1 I n -H ε 0 ∼ N 0 0 , H -H 1 0 0 I n -H .</formula><p>So (H -H 1 )ε 0 and (I n -H)ε 0 are Normal with mean zero and two projection matrices H -H 1 and I n -H as covariances, respectively, and moreover, they are independent. These imply that their squared lengths are chi-squared:</p><formula xml:id="formula_204">∥(H -H 1 )ε 0 ∥ 2 ∼ χ 2 l , ∥(I n -H)ε 0 ∥ 2 ∼ χ 2 n-p ,</formula><p>and they are independent. These facts, coupled with (8.9), imply that F anova ∼ F l,n-p . □</p><p>The second part demonstrates that F anova = F Wald without assuming the Normal linear model, which gives an indirect proof for the exact distribution of F anova under the Normal linear model. Proof of Theorem 8.2 (Part II): Using the FWL theorem that β2 = ( Xt <ref type="bibr">(8.10)</ref> recalling that H2 = X2 ( Xt 2 X2 ) -1 Xt 2 is the projection matrix onto the column space of X2 . Therefore, F anova = F Wald follows from the basic identity H -H 1 = H2 ensured by Lemma 7.2.</p><formula xml:id="formula_205">2 X2 ) -1 Xt 2 Y , we can rewrite F Wald as F Wald = Y t X2 ( Xt 2 X2 ) -1 Xt 2 X2 ( Xt 2 X2 ) -1 Xt 2 Y lσ 2 = Y t X2 ( Xt 2 X2 ) -1 Xt 2 Y lσ 2 = Y t H2 Y lσ 2 ,</formula><p>□ We can use the anova function in R to compute the F statistic and the p-value. Below I revisit the lalonde data with the R code in code8.3.R. The result is identical as in Section 5.4.2.</p><p>&gt; library ( " Matching " ) &gt; data ( lalonde ) &gt; lalonde _ full = lm ( re 7 8 ~. , data = lalonde ) &gt; lalonde _ treat = lm ( re 7 8 ~treat , data = lalonde ) &gt; anova ( lalonde _ treat , lalonde _ full ) Analysis of Variance</p><p>Table Model 1 : re 7 8 ~treat Model 2 : re 7 8 ~age + educ + black + hisp + married + nodegr + re + re 7 5 + u 7 4 + u 7 5 + treat Res . Df RSS Df Sum of Sq F Pr ( &gt; F ) 1</p><p>4 4 3 1 . 9 1 7 8 e + 1 0 2 4 3 3 1 . 8 3 8 9 e + 1 0 1 0 7 8 8 7 9 9 0 2 3 1 . 8 5 7 4 0 . 0 4 9 2 9 *</p><p>In fact, we can conduct an analysis of variance in a sequence of models. For example, we can supplement the above analysis with a model containing only the intercept. The function anova works for a sequence of nested models with increasing complexities.</p><p>&gt; lalonde 1 = lm ( re 7 8 ~1 , data = lalonde ) &gt; anova ( lalonde 1 , lalonde _ treat , lalonde _ full ) Analysis of Variance</p><p>Table Model 1 : re 7 8 ~1 Model 2 : re 7 8 ~treat Model 3 : re 7 8 ~age + educ + black + hisp + married + nodegr + re + re 7 5 + u 7 4 + u 7 5 + treat Res . Df RSS Df Sum of Sq F Pr ( &gt; F ) 1</p><p>4 4 4 1 . 9 5 2 6 e + 1 0 2 4 4 3 1 . 9 1 7 8 e + 1 0 1 3 4 8 0 1 3 4 5 6 8 . 1 9 4 6 0 . 0 0 4 4 0 5 ** 3 4 3 3 1 . 8 3 8 9 e + 1 0 1 0 7 8 8 7 9 9 0 2 3 1 . 8 5 7 4 0 . 0 4 9 2 8 6 *</p><p>Overall, the treatment variable is significantly related to the outcome but none of the pretreatment covariate is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">FWL with intercept</head><p>The following result is an immediate extension of Theorem 7.1.</p><p>Partition X and β into</p><formula xml:id="formula_206">X = 1 n X 1 X 2 , β =   β 0 β 1 β 2   ,</formula><p>where</p><formula xml:id="formula_207">X 1 ∈ R n×k , X 2 ∈ R n×l , β 0 ∈ R, β 1 ∈ R k and β 2 ∈ R l . then we can consider the long regression Y = X β + ε = 1 n X 1 X 2   β0 β1 β2   + ε = 1 n β0 + X 1 β1 + X 2 β2 + ε,</formula><p>and the short regression</p><formula xml:id="formula_208">Y = 1 n β0 + X 2 β2 + ε, where β =   β0 β1 β2</formula><p>  and β0 β2 are the OLS coefficients, and ε and ε are the residual vectors from the long and short regressions, respectively. Prove the following theorem.</p><p>Theorem 8.3 The OLS estimator for β 2 in the long regression equals the coefficient of X2 in the OLS fit of Y on (1 n , X2 ), where X2 is the residual matrix of the column-wise OLS fit of X 2 on (1 n , X 1 ), and also equals the coefficient of X2 in the OLS fit of Ỹ on (1 n , X2 ), where Ỹ is the residual vector of the OLS fit of Y on (1 n , X 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">General centering</head><p>Verify (8.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Two-way centering of a matrix</head><p>Given X ∈ R n×p , show that all rows and columns of C n XC p have mean 0, where C n = I n -n -1 1 n 1 t n and C p = I p -p -1 1 p 1 t p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">t-statistic in multivariate OLS</head><p>This problem extends Problem 5.8. Focus on multivariate OLS discussed in Chapter 5: y i = α + β1 x i1 + βt 2 x i2 + εi (i = 1, . . . , n), where x i1 is a scalar and x i2 can be a vector. Show that under homoskedasticity, the t-statistic associated with β1 equals ρyx1|x2</p><formula xml:id="formula_209">(1 -ρ2 yx1|x2 )/(n -p) ,</formula><p>where p is the total number of regressors and ρyx1|x2 is the sample partial correlation coefficient between y and x 1 given x 2 . Remark: <ref type="bibr" target="#b105">Frank (2000)</ref> applied this formula to causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Equivalence of the t-statistics in multivariate OLS</head><p>This problem extends Problems 5.9 and 6.5. With the data (x i1 , x i2 , y i ) n i=1 where both x i1 and y i are scalars and x i2 can be a vector. Run OLS fit of y i on (1, x i1 , x i2 ) to obtain t y|x1,x2 , the t-statistic of the coefficient of x i1 , under the homoskedasticity assumption. Run OLS fit of x i1 on (1, y i , x i2 ) to obtain t x1|y,x2 , the t-statistic of the coefficient of y i , under the homoskedasticity assumption.</p><p>Show t y|x1,x2 = t x1|y,x2 . Give a counterexample in which the numerical equivalence of the t-statistics breaks down based on the EHW standard error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Formula of the partial correlation coefficient</head><p>Prove Theorem 8.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7">Examples of Simpson's Paradox</head><p>Give three numerical examples of (Y, X, W ) which causes Simpson's Paradox. Report the mean and covariance matrix for each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.8">Simpson's Paradox in reality</head><p>Find a real-life dataset with Simpson's Paradox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.9">Basic properties of projection matrices</head><p>Prove Lemma 8.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.10">Correlation of the regression coefficients</head><p>1. Regress Y on (1 n , X 1 , X 2 ) where X 1 and X 2 are two n-vectors with positive sample Pearson correlation ρx1x2 &gt; 0. Show that the corresponding OLS coefficients β1 and β2 are negatively correlated under the Gauss-Markov model.</p><p>2. Regress Y on (1 n , X 1 , X 2 , X 3 ) where X 1 and X 2 are two n-vectors and X 3 is an n × L dimensional matrix. If the partial correlation coefficient between X 1 and X 2 given X 3 is positive, then show that the corresponding OLS coefficients β1 and β2 are negatively correlated under the Gauss-Markov model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.11">Inverse of sample covariance matrix and partial correlation coefficient</head><p>This is the sample version of Problem 2.7.</p><p>Based on X ∈ R n×p , we can compute the sample covariance matrix Σ. Denote its inverse by Σ-1 = (σ jk ) 1≤j,k≤p . Show that for any pair j ̸ = k, we have σjk = 0 ⇐⇒ ρxjx k |x \(j,k) = 0 where ρxjx k |x \(j,k) is the partial correlation coefficient of X j and X k given all other variables.</p><p>Cochran's Formula and Omitted-Variable Bias</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Cochran's formula</head><p>Consider an n × 1 vector Y , an n × k matrix X 1 , and an n × l matrix X 2 . Similar to the FWL theorem, we do not impose any statistical models. We can fit the following OLS:</p><formula xml:id="formula_210">Y = X 1 β1 + X 2 β2 + ε, (9.1) Y = X 2 β2 + ε, (9.2) X 1 = X 2 δ + Û , (9.3)</formula><p>where ε, ε are the residual vectors, and Û is the residual matrix. The last OLS fit means the OLS fit of each column of X 1 on X 2 , and therefore the corresponding residual Û is an n × k matrix.</p><p>Theorem 9.1 Under the OLS fits (9.1)-( <ref type="formula">9</ref>.3), we have</p><formula xml:id="formula_211">β2 = β2 + δ β1 .</formula><p>This is a pure linear algebra fact similar to the FWL theorem. It is called Cochran's formula in statistics. Sir <ref type="bibr">David Cox (Cox, 2007)</ref> attributed the formula to <ref type="bibr" target="#b78">Cochran (1938)</ref> although Cochran himself attributed the formula to <ref type="bibr">Fisher (1925a)</ref>.</p><p>Cochran's formula may seem familiar to readers knowing the chain rule in calculus. In a deterministic world with scalar y, x 1 , x 2 , if</p><formula xml:id="formula_212">y(x 1 , x 2 ) = x 1 β 1 + x 2 β 2 and x 1 (x 2 ) = x 2 δ, then dy dx 2 = ∂y ∂x 1 ∂x 1 ∂x 2 + ∂y ∂x 2 = δβ 1 + β 2 .</formula><p>But the OLS decompositions above do not establish any deterministic relationships among Y and (X 1 , X 2 ).</p><p>In some sense, the formula is obvious. From the first and the third OLS fits, we have</p><formula xml:id="formula_213">Y = X 1 β1 + X 2 β2 + ε = X 2 δ + Û β1 + X 2 β2 + ε = X 2 δ β1 + Û β1 + X 2 β2 + ε = X 2 δ β1 + β2 + Û β1 + ε . (9.4) FIGURE 9.1: A diagram for Cochran's formula</formula><p>This suggests that β2 = β2 + δ β1 . The above derivation follows from simple algebraic manipulations and does not use any properties of the OLS. To prove Theorem 9.1, we need to verify that the last line is indeed the OLS fit of Y on X 2 . The proof is fact very simple.</p><p>Proof of Theorem 9.1: Based on the above discussion, we only need to show that (9.4) is the OLS fit of Y on X 2 , which is equivalent to show that Û β1 + ε is orthogonal to all columns of X 2 . This follows from</p><formula xml:id="formula_214">X t 2 Û β1 + ε = X t 2 Û β1 + X t 2 ε = 0, because X t 2 Û = 0 based on the OLS fit in (9.</formula><p>3) and X t 2 ε = 0 based on the OLS fit in (9.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>Figure <ref type="figure" target="#fig_98">9</ref>.1 illustrates Theorem 9.1. Intuitively, β2 measures the total impact of X 2 on Y , which has two channels: β2 measures the impact acting directly and δ β1 measures the impact acting indirectly through X 1 .</p><p>Figure <ref type="figure" target="#fig_98">9</ref>.1 shows the interplay among three variables. Theoretically, we can discuss a system of more than three variables which is called the path model. This more advanced topic is beyond the scope of this book. <ref type="bibr" target="#b242">Wright (1921</ref><ref type="bibr" target="#b243">Wright ( , 1934))</ref>'s initial discussion of this approach was motivated by genetic studies. See <ref type="bibr" target="#b111">Freedman (2009)</ref> for a textbook introduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Omitted-variable bias</head><p>The proof of Theorem 9.1 is very simple. However, it is one of the most insightful formulas in statistics. Econometricians often call it the omitted-variable bias formula because it quantifies the bias of the OLS coefficient of X 2 in the short regression omitting possibly important variables in X 1 . If the OLS coefficient from the long regression is unbiased then the the OLS coefficient from the short regression has a biased term δ β1 , which equals the product of the coefficient of X 2 in the OLS fit of X 1 on X 2 and the coefficient of X 1 in the long regression.</p><p>Below I will discuss a canonical example of using OLS to estimate the treatment effect in observational studies. For unit i (i = 1, . . . , n), let y i be the outcome, z i be the binary treatment indicator (1 for the treatment group and 0 for the control group) and x i be the observed covariate vector. Practitioners often fit the following OLS:</p><formula xml:id="formula_215">y i = β0 + β1 z i + βt</formula><p>and interpret β1 as the treatment effect estimate. However, observational studies may suffer from unmeasured confounding, that is, the treatment and control units differ in unobserved but important ways. In the simplest case, the above OLS may have omitted a variable u i for each unit i, which is called a confounder. The oracle OLS is</p><formula xml:id="formula_216">y i = β0 + β1 z i + βt 2 x i + β3 u i + εi</formula><p>and the coefficient β1 is an unbiased estimator if the model with u i is correct. With X 1 containing the values of the u i 's and X 2 containing the values of the (1,</p><formula xml:id="formula_217">z i , x t i )'s, Cochran's formula implies that   β0 β1 β2   =   β0 β1 β2   + β3   δ0 δ1 δ2  </formula><p>where ( δ0 , δ1 , δt 2 ) t is the coefficient vector in the OLS fit of u i on (1, z i , x i ). Therefore, we can quantify the difference between the observed estimate β1 and oracle estimate β1 : β1 -β1 = β3 δ1 , which is sometimes called the confounding bias.</p><p>Using the basic properties of OLS, we can show that δ1 equals the difference in means of e i = u i -δt 2 x i across the treatment and control groups:</p><formula xml:id="formula_218">δ1 = ē1 -ē0 ,</formula><p>where the bar and subscript jointly denote the sample mean of a particular variable within a treatment group. So</p><formula xml:id="formula_219">β -β1 = β3 (ē 1 -ē0 ). (9.5)</formula><p>Moreover, we can obtain a more explicit formula for δ1 :</p><formula xml:id="formula_220">δ1 = ū1 -ū0 -δt 2 (x 1 -x0 ). So β -β1 = β3 (ū 1 -ū0 ) -β3 δt 2 (x 1 -x0 ). (9.6)</formula><p>Both (9.5) and (9.6) give some insights into the bias due to omitting an important covariate u. It is clear that the bias depends on β3 , which quantifies the relationship between u and y. The formula (9.5) shows that the bias also depends on the imbalance in means of u across the treatment and control groups, after adjusting for the observed covariates x, that is, the imbalance in means of the residual confounding. The formula (9.6) shows a more explicit formula of the bias. The above discussion is often called bias analysis in epidemiology or sensitivity analysis in statistics and econometrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Baron-Kenny method for mediation analysis</head><p>The Baron-Kenny method, popularized by <ref type="bibr" target="#b54">Baron and Kenny (1986)</ref>, is one of the most cited methods in social science. It concerns the interplay among three variables z, m, y controlling FIGURE 9.2: The graph for the Baron-Kenny method for some other variables x. Let Z, M, Y be n × 1 vectors representing the observed values of z, m, y, and let X be the n × p matrix representing the observations of x. The question of interest is to assess the "direct" and "indirect" effects of z on y, acting independently and through m, respectively. We do not need to define these notions precisely since we are only interested in the algebraic property below.</p><p>The Baron-Kenny method runs the OLS</p><formula xml:id="formula_221">Y = β0 1 n + β1 Z + β2 M + X β3 + εY</formula><p>and interprets β1 as the estimator of the "direct effect" of z on y. The "indirect effect" of z on y through m has two estimators. First, based on the OLS</p><formula xml:id="formula_222">Y = β0 1 n + β1 Z + X β3 + εY ,</formula><p>define the difference estimator as β1 -β1 . Second, based on the OLS</p><formula xml:id="formula_223">M = γ0 1 n + γ1 Z + X γ2 + εM ,</formula><p>define the product estimator as γ1 β2 . Figure <ref type="figure" target="#fig_98">9</ref>.2 illustrates the OLS fits used in defining the estimators.</p><p>Prove that β1 -β1 = γ1 β2 that is, the difference estimator and product estimator are numerically identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">A special case of path analysis</head><p>Figure <ref type="figure" target="#fig_98">9</ref>.3 represents the order of the variables X 1 , X 2 , X 3 , Y ∈ R n . Run the following OLS: Remark: The OLS coefficient of X 1 in the short regression of Y on (1 n , X 1 ) equals the summation of all the path coefficients from X 1 to Y as illustrated by Figure <ref type="figure" target="#fig_98">9</ref>.3. This problem is a special case of the path model, but the conclusion holds in general.</p><formula xml:id="formula_224">Y = β0 1 n + β1 X 1 + β2 X 2 + β3 X 3 + εY , X 3 = δ0 1 n + δ1 X 1 + δ2 X 2 + ε3 , X 2 = θ0 1 n + θ1 X 1 + ε2 , and Y = β0 1 n + β1 X 1 + εY .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">EHW in long and short regressions</head><p>Theorem 9.1 gives Cochran's formula related to the coefficients from three OLS fits. This problem concerns the covariance estimation. There are at least two ways to estimate the covariance of β2 in the short regression (9.2). First, from the second OLS fit, the EHW covariance estimator is</p><formula xml:id="formula_225">Ṽ2 = (X t 2 X 2 ) -1 X t 2 diag(ε 2 )X 2 (X t 2 X 2 ) -1 .</formula><p>Second, Cochran's formula implies that β2 = ( δ, I l ) β1 β2 is a linear transformation of the coefficient from the long regression, which further justifies the EHW covariance estimator</p><formula xml:id="formula_226">Ṽ ′ 2 = ( δ, I l )(X t X) -1 X t diag(ε 2 )X(X t X) -1 δt I l . Show that Ṽ ′ 2 = (X t 2 X 2 ) -1 X t 2 diag(ε 2 )X 2 (X t 2 X 2 ) -1 . Hint: Use the result in Problem 7.1. Remark: Based on Theorem 7.2, the EHW covariance estimator for β2 is V2 = ( Xt 2 X2 ) -1 Xt 2 diag(ε 2 ) X2 ( Xt 2 X2 ) -1 , where X2 = (I n -H 1 )X 2 .</formula><p>Part IV</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Fitting, Checking, and Misspecification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Correlation Coefficient</head><p>This chapter will introduce the R 2 , the multiple correlation coefficient, also called the coefficient of determination <ref type="bibr" target="#b242">(Wright, 1921)</ref>. It can achieve two goals: first, it extends the sample Pearson correlation coefficient between two scalars to a measure of correlation between a scalar outcome and a vector covariate; second, it measures how well multiple covariates can linearly represent an outcome.</p><p>10.1 Equivalent definitions of R 2</p><p>I start with the standard definition of R 2 between Y and X. Slightly different from other chapters, X excludes the column of 1's, so now X is an n × (p -1) matrix. Based on the OLS of Y on (1 n , X), we define</p><formula xml:id="formula_227">R 2 = n i=1 (ŷ i -ȳ) 2 n i=1 (y i -ȳ) 2 .</formula><p>We have discussed before that including 1 n in the OLS ensures</p><formula xml:id="formula_228">n -1 n i=1 εi = 0, =⇒ n -1 n i=1 y i = n -1 n i=1 ŷi , =⇒ ȳ = ȳ,</formula><p>i.e., the average of the fitted values equals the average of the original observed outcomes. So I use ȳ for both the means of outcomes and the fitted values. With scaling factor (n -1) -1 , the denominator of R 2 is the sample variance of the outcomes, and the numerator of R 2 is the sample variance of the fitted values. We can verify the following decomposition: Lemma 10.1 We have the following variance decomposition:</p><formula xml:id="formula_229">n i=1 (y i -ȳ) 2 = n i=1 (ŷ i -ȳ) 2 + n i=1 (y i -ŷi ) 2 .</formula><p>I leave the proof of Lemma 10.1 as Problem 10.1. Lemma 10.1 states that the total sum of squares n i=1 (y i -ȳ) 2 equals the regression sum of squares n i=1 (ŷ i -ȳ) 2 plus the residual sum of squares n i=1 (y i -ŷi ) 2 . From Lemma 10.1, R 2 must be lie within the interval [0, 1] which measures the proportion of the regression sum of squares in the total sum of squares. An immediate consequence of Lemma 10.1 is that</p><formula xml:id="formula_230">rss = (1 -R 2 ) n i=1 (y i -ȳ) 2 .</formula><p>We can also verify that R 2 is the squared sample Pearson correlation coefficient between Y and Ŷ .</p><formula xml:id="formula_231">Theorem 10.1 We have R 2 = ρ2 y ŷ where ρyŷ = n i=1 (y i -ȳ)(ŷ i -ȳ) n i=1 (y i -ȳ) 2 n i=1 (ŷ i -ȳ) 2</formula><p>.</p><p>(10.1) I leave the proof of Theorem 10.1 as Problem 10.2. It states that the multiple correlation coefficient equals the squared Pearson correlation coefficient between y i and ŷi . Although the sample Pearson correlation coefficient can be positive or negative, R 2 is always nonnegative. Geometrically, R 2 equals the squared cosine of the angle between the centered vectors Y -ȳ1 n and Ŷ -ȳ1 n ; see Chapter A.1.</p><p>In terms of long and short regressions, we can partition the design matrix into 1 n and X, then the OLS fit of the long regression is</p><formula xml:id="formula_232">Y = 1 n β0 + X β + ε, (10.2)</formula><p>and the OLS fit of the short regression is</p><formula xml:id="formula_233">Y = 1 n β0 + ε, (10.3)</formula><p>with β0 = ȳ. The total sum of squares is the residual sum of squares from the short regression so by Lemma 10.1, R 2 also equals</p><formula xml:id="formula_234">R 2 = rss short -rss long rss short . (10.4)</formula><p>10.2 R 2 and the F statistic</p><p>Under the Normal linear model</p><formula xml:id="formula_235">Y = 1 n β 0 + Xβ + ε, ε ∼ N(0, σ 2 I n ),<label>(10.5)</label></formula><p>we can use the F statistic to test whether β = 0. This F statistic is a monotone function of R 2 . Most standard software packages report both F and R 2 . I first give a numeric result without assuming that model (10.5) is correct.</p><p>Theorem 10.2 We have</p><formula xml:id="formula_236">F = n -p p -1 × R 2 1 -R 2 .</formula><p>Proof of Theorem 10.2: Based on the long regression (10.2) and the short regression (10.3), we have (10.4) and</p><formula xml:id="formula_237">F = (rss short -rss long )/(p -1)</formula><p>rss long /(n -p) .</p><p>So the conclusion follows. □ I then give the exact distribution of R 2 under the Normal linear model.</p><p>Corollary 10.1 Under the Normal linear model (10.5), if β = 0, then</p><formula xml:id="formula_238">R 2 ∼ Beta p -1 2 , n -p 2 .</formula><p>Proof of Corollary 10.1: By definition, the F statistic can be represented as</p><formula xml:id="formula_239">F = χ 2 p-1 /(p -1) χ 2 n-p /(n -p)</formula><p>where χ 2 p-1 and χ 2 n-p denote independent χ 2 p-1 and χ 2 n-p random variables, respectively, with a little abuse of notation. Using Theorem 10.2, we have</p><formula xml:id="formula_240">R 2 1 -R 2 = F × p -1 n -p = χ 2 p-1 χ 2 n-p which implies R 2 = χ 2 p-1 χ 2 p-1 + χ 2 n-p . Because χ 2 p-1 ∼ Gamma p-1 2 , 1 2 and χ 2 n-p ∼ Gamma n-p 2 , 1 2 by Proposition B.1, we have R 2 = Gamma p-1 2 , 1 2 Gamma p-1 2 , 1 2 + Gamma n-p 2 , 1<label>2</label></formula><p>where Gamma p-1 2 , 1 2 and Gamma n-p 2 , 1 2 denote independent Gamma random variables, with a little abuse of notation. The R 2 follows the Beta distribution by the Beta-Gamma duality in Theorem B.1. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Numerical examples</head><p>Below I first use the LaLonde data to verify Theorems 10.1 and 10.2 numerically.</p><p>&gt; library ( " Matching " ) &gt; data ( lalonde ) &gt; ols . fit = lm ( re 7 8 ~. , y = TRUE , data = lalonde ) &gt; ols . summary = summary ( ols . fit ) &gt; r 2 = ols . summary $ r . squared &gt; all . equal ( r 2 , ( cor ( ols . fit $y , ols . fit</p><formula xml:id="formula_241">$ fitted . values ))^2 , + check . names = FALSE ) [ 1 ] TRUE &gt; &gt; fstat = ols . summary $ fstatistic &gt; all . equal ( fstat [ 1 ] , fstat [ 3 ]/ fstat [ 2 ]* r 2 /( 1 -r 2 ) , + check . names = FALSE ) [ 1 ] TRUE</formula><p>I then use the data from <ref type="bibr" target="#b156">King and Roberts (2015)</ref> to verify Theorems 10.1 and 10.2 numerically.</p><p>&gt; library ( foreign ) &gt; dat = read . dta ( " isq . dta " ) &gt; dat = na . omit ( dat [ , c ( " multish " , " lnpop " , " lnpopsq " , + " lngdp " , " lncolony " , " lndist " , + " freedom " , " militexp " , " arms " , + " year 8 3 " , " year 8 6 " , " year 8 9 " , " year 9 2 " )]) &gt; &gt; ols . fit = lm ( log ( multish + 1 ) ~lnpop + lnpopsq + lngdp + lncolony + + lndist + freedom + militexp + arms + + year 8 3 + year 8 6 + year 8 9 + year 9 2 , + y = TRUE , data = dat ) &gt; ols . summary = summary ( ols . fit ) &gt; r 2 = ols . summary $ r . squared &gt; all . equal ( r 2 , ( cor ( ols . fit $y , ols . fit $ fitted . values ))^2 , </p><formula xml:id="formula_242">+ check . names = FALSE ) [ 1 ] TRUE &gt; &gt; fstat = ols . summary $ fstatistic &gt; all . equal ( fstat [ 1 ] , fstat [ 3 ]/ fstat [ 2 ]* r 2 /( 1 -r 2 ) , + check . names = FALSE ) [ 1 ] TRUE The R code is in code10.3.R.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Exact distribution of ρ</head><p>Assume the Normal linear model y i = α + βx i + ε i with a univariate x i with β = 0 and ε i 's IID N(0, σ 2 ). Find the exact distribution of ρxy .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Partial R 2</head><p>The form (10.4) of R 2 is well defined in more general long and short regressions:</p><formula xml:id="formula_243">Ŷ = 1 n β0 + X β + W γ + εY and Ŷ = 1 n β0 + W γ + εY</formula><p>where X is an n × k matrix and W is an n × l matrix. Define the partial R 2 between Y and X given W as</p><formula xml:id="formula_244">R 2 Y.X|W = rss(Y ∼ 1 n + W ) -rss(Y ∼ 1 n + X + W ) rss(Y ∼ 1 n + W )</formula><p>which spells out the formulas of the long and short regressions. This is an intuitive measure of the multiple correlation between Y and X after controlling for W . The following properties make this intuition more explicit.</p><formula xml:id="formula_245">1. The partial R 2 equals R 2 Y.X|W = R 2 Y.XW -R 2 Y.W 1 -R 2 Y.W</formula><p>where R 2 Y.XW is the multiple correlation between Y and (X, W ), and R 2 Y.W is the multiple correlation between Y and W .</p><p>2. The partial R 2 equals the R 2 between εY and εX :</p><formula xml:id="formula_246">R 2 Y.X|W = R 2 εY .ε X</formula><p>where εX is the residual matrix from the OLS fit of X on (1 n , W ).</p><p>Prove the above two results. Do the following two results hold?</p><formula xml:id="formula_247">R 2 Y.XW = R 2 Y.W + R 2 Y.X|W , R 2 Y.XW = R 2 Y.W |X + R 2 Y.X|W .</formula><p>For each result, give a proof if it is correct, or give a counterexample if it is incorrect in general.</p><p>10.5 Omitted-variable bias in terms of the partial R 2</p><p>Revisit Section 9.2 on the following three OLS fits. The first one involves only the observed variables:</p><formula xml:id="formula_248">y i = β0 + β1 z i + βt 2 x i + εi</formula><p>, and the second and third ones involve the unobserved u:</p><formula xml:id="formula_249">y i = β0 + β1 z i + βt 2 x i + β3 u i + εi , u i = δ0 + δ1 z i + δt 2 x i + vi .</formula><p>The omitted-variable bias formula states that β -β1 = β3 δ1 . This formula is simple but may be difficult to interpret since u is unobserved and its scale is unclear to researchers. Prove that the formula has an alternative form:</p><formula xml:id="formula_250">| β1 -β1 | 2 = R 2 Y.U |ZX × R 2 Z.U |X 1 -R 2 Z.U |X × rss(Y ∼ 1 n + Z + X) rss(Z ∼ 1 n + X) .</formula><p>Remark: <ref type="bibr" target="#b77">Cinelli and Hazlett (2020)</ref> suggested the partial R 2 parametrization for the omitted-variable bias formula. The formula has three factors: the first two factors depend on the unknown sensitivity parameters R 2 Y.U |ZX and R 2 Z.U |X , and the third factor equals the ratio of the two residual sums of squares based on the observed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leverage Scores and Leave-One-Out Formulas</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Leverage scores</head><p>We have seen the use of the hat matrix H = X(X t X) -<ref type="foot" target="#foot_7">foot_7</ref> X t in previous chapters. Because</p><formula xml:id="formula_251">H =    x t 1 . . . x t n    (X t X) -1 x 1 • • • x n , its (i, j)th element equals h ij = x t i (X t X) -1 x j .</formula><p>In this chapter, we will pay special attention to its diagonal elements</p><formula xml:id="formula_252">h ii = x t i (X t X) -1 x i (i = 1, . . . , n)</formula><p>often called the leverage scores, which play important roles in many discussions later.</p><p>First, because H is a projection matrix of rank p, we have</p><formula xml:id="formula_253">n i=1 h ii = trace(H) = rank(H) = p which implies that n -1 n i=1 h ii = p/n,</formula><p>i.e., the average of the leverage scores equals p/n and the maximum of the leverage scores must be larger than or equal to p/n, which is close to zero when p is small relative to n.</p><p>Second, because H = H 2 and H = H t , we have</p><formula xml:id="formula_254">h ii = n j=1 h ij h ji = n j=1 h 2 ij = h 2 ii + j̸ =i h 2 ij ≥ h 2 ii which implies h ii ∈ [0, 1],</formula><p>i.e., each leverage score is bounded between zero and one 1 . Third, because Ŷ = HY , we have</p><formula xml:id="formula_255">ŷi = n j=1 h ij y j = h ii y i + j̸ =i h ij y j which implies that ∂ ŷi ∂y i = h ii .</formula><p>So h ii measures the contribution of y i in the predicted value ŷi . In general, we do not want the contribution of y i in predicting itself to be too large, because this means we do not borrow enough information from other observations, making the prediction very noisy. This is also clear from the variance of the predicted value ŷi = x t i β under the Gauss-Markov model:</p><formula xml:id="formula_256">2 var(ŷ i ) = x t i cov( β)x i = σ 2 x t i (X t X) -1 x i = σ 2 h ii .</formula><p>So the variance of ŷi increases with h ii .</p><p>The final property of h ii is less obvious: it measures whether observation i is an outlier based on its covariate value, that is, whether x i is far from the center of the data. Partition the design matrix as</p><formula xml:id="formula_257">X = 1 n X 2 with H 1 = n -1 1 n 1 t n . The covariates X 2 has sample mean x2 = n -1 n i=1 x i2 and sample covariance S = (n -1) -1 n i=1 (x i2 -x2 )(x i2 -x2 ) t = (n -1) -1 X t 2 (I n -H 1 )X 2 .</formula><p>The sample Mahalanobis distance between x i2 and the center x2 is</p><formula xml:id="formula_258">D 2 i = (x i2 -x2 ) t S -1 (x i2 -x2 ).</formula><p>The following theorem shows that h ii is a monotone function of D<ref type="foot" target="#foot_8">foot_8</ref> i :</p><p>Theorem 11.1 We have</p><formula xml:id="formula_259">h ii = 1 n + D 2 i n -1 , (11.1) so h ii ≥ 1/n.</formula><p>Proof of Theorem 11.1: The definition of D 2 i implies that it is the (i, i)th element of the following matrix:</p><formula xml:id="formula_260">   x 12 -x2 . . . x n2 -x2    t S -1 x 12 -x2 • • • x n2 -x2 =(I n -H 1 )X 2 (n -1) -1 X t 2 (I n -H 1 )X 2 -1 X t 2 (I n -H 1 ) =(n -1) X2 ( Xt 2 X2 ) -1 Xt 2 =(n -1) H2 =(n -1)(H -H 1 ), recalling that X2 = (I n -H 1 )X 2 , H2 = X2 ( Xt 2 X2 ) -1 Xt 2 , and H = H 1 + H2 by Lemma 7.2. Therefore, D 2 i = (n -1)(h ii -1/n)</formula><p>which implies (11.1). □ These are the basic properties of the leverage scores. <ref type="bibr" target="#b72">Chatterjee and Hadi (1988)</ref> provided an in-depth discussion of the properties of the leverage scores. We will see their roles frequently in later parts of the chapter.</p><p>Another advanced result on the leverage scores is due to <ref type="bibr" target="#b147">Huber (1973)</ref>. He proved that in the linear model with non-Normal IID ε i ∼ [0, σ 2 ] and σ 2 &lt; ∞, all linear combinations of the OLS coefficient are asymptotically Normal if and only if the maximum leverage score converges to 0. This is a very elegant asymptotic result on the OLS coefficient. I give more details in Chapter C as an application of the Lindeberg-Feller CLT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Leave-one-out formulas</head><p>To measure the impact of the ith observation on the final OLS estimator, a natural approach is to delete the ith row from the full data</p><formula xml:id="formula_261">X =    x t 1 . . . x t n    , Y =    y 1 . . . y n    ,</formula><p>and check how much the OLS estimator changes. Let</p><formula xml:id="formula_262">X [-i] =           x t 1 . . . x t i-1 x t i+1 . . . x t n           , Y [-i] =           y 1 . . . y i-1 y i+1 . . . y n          </formula><p>denote the leave-i-out data, and define</p><formula xml:id="formula_263">β[-i] = (X t [-i] X [-i] ) -1 X t [-i] Y [-i] (11.2)</formula><p>as the corresponding OLS estimator. We can fit n OLS by deleting the ith row (i = 1, . . . , n). However, this is computationally intensive especially when n is large. The following theorem shows that we need only to fit OLS once.</p><p>Theorem 11.2 Recalling that β is the full data OLS, εi is the residual and h ii is the leverage score for the ith observation, we have</p><formula xml:id="formula_264">β[-i] = β -(1 -h ii ) -1 (X t X) -1 x i εi if h ii ̸ = 1.</formula><p>Proof of Theorem 11.2: From (11.2), we need to invert</p><formula xml:id="formula_265">X t [-i] X [-i] = i ′ ̸ =i x i ′ x t i ′ = X t X -x i x t i and calculate X t [-i] Y [-i] = i ′ ̸ =i x i y i = X t Y -x i y i ,</formula><p>which are the original X t X and X t Y without the contribution of the ith observation. Using the following Sherman-Morrison formula in Problem 1.3:</p><formula xml:id="formula_266">(A + uv t ) -1 = A -1 -(1 + v t A -1 u) -1 A -1 uv t A -1 with A = X t X, u = x i , and v = -x i we can invert X t [-i] X [-i] as (X t [-i] X [-i] ) -1 = (X t X) -1 + 1 -x t i (X t X) -1 x i -1 (X t X) -1 x i x t i (X t X) -1 = (X t X) -1 + (1 -h ii ) -1 (X t X) -1 x i x t i (X t X) -1 . Therefore, β[-i] = (X t [-i] X [-i] ) -1 X t [-i] Y [-i] = (X t X) -1 + (1 -h ii ) -1 (X t X) -1 x i x t i (X t X) -1 (X t Y -x i y i ) = (X t X) -1 X t Y -(X t X) -1 x i y i + (1 -h ii ) -1 (X t X) -1 x i x t i (X t X) -1 X t Y -(1 -h ii ) -1 (X t X) -1 x i x t i (X t X) -1 x i y i = β -(X t X) -1 x i y i + (1 -h ii ) -1 (X t X) -1 x i x t i β -h ii (1 -h ii ) -1 (X t X) -1 x i y i = β -(1 -h ii ) -1 (X t X) -1 x i y i + (1 -h ii ) -1 (X t X) -1 x i ŷi = β -(1 -h ii ) -1 (X t X) -1 x i εi . □ With the leave-i-out OLS estimator β[-i] , we can define the predicted residual ε[-i] = y i -x t i β[-i] ,</formula><p>which is different from the original residual εi . The predicted residual based on leave-one-out can better measure the performance of the prediction because it mimics the real problem of predicting a future observation. In contrast, the original residual based on the full data εi = y i -x t i β gives an overly optimistic measure of the performance of the prediction. This is related to the overfitting issue discussed later. Under the Gauss-Markov model, Theorem 4.2 implies that the original residual has mean zero and variance</p><formula xml:id="formula_267">var(ε i ) = σ 2 (1 -h ii ),<label>(11.3)</label></formula><p>and we can show that the predicted residual has mean zero and variance</p><formula xml:id="formula_268">var(ε [-i] ) = var(y i -x t i β[-i] ) = σ 2 + σ 2 x t i (X t [-i] X [-i] ) -1 x i . (11.4)</formula><p>The following theorem further simplifies the predicted residual and its variance.</p><formula xml:id="formula_269">Theorem 11.3 We have ε[-i] = εi /(1 -h ii ),</formula><p>and under Assumption 4.1, we have</p><formula xml:id="formula_270">var(ε [-i] ) = σ 2 /(1 -h ii ). (11.5)</formula><p>Proof of Theorem 11.3: By definition and Theorem 11.2, we have</p><formula xml:id="formula_271">ε[-i] = y i -x t i β[-i] = y i -x t i β -(1 -h ii ) -1 (X t X) -1 x i εi = y i -x t i β + (1 -h ii ) -1 x t i (X t X) -1 x i εi = εi + h ii (1 -h ii ) -1 εi = εi /(1 -h ii ). (11.6)</formula><p>Combining (11.3) and (11.6), we can derive its variance formula. □ Comparing formulas (11.4) and (11.5), we obtain that</p><formula xml:id="formula_272">1 + x t i (X t [-i] X [-i] ) -1 x i = (1 -h ii ) -1 = 1 -x t i (X t X) -1 x i -1 .</formula><p>This is not an obvious linear algebra identity, but it follows immediately from the two ways of calculating the variance of the predicted residual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">Applications of the leave-one-out formulas</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.1">Gauss updating formula</head><p>Consider an online setting in which the data points come sequentially as illustrated by the figure below:</p><p>In this setting, we can update the OLS estimator step by step: based on the first n data points (x i , y i ) n i=1 , we calculate the OLS estimator β(n) , and with an additional data point (x n+1 , y n+1 ), we update the OLS estimator as β(n+1) . These two OLS estimators are closely related as shown in the following theorem.</p><p>Theorem 11.4 Let X (n) be the design matrix and Y (n) be the outcome vector for the first n observations. We have</p><formula xml:id="formula_273">β(n+1) = β(n) + γ (n+1) ε[n+1] ,</formula><p>where</p><formula xml:id="formula_274">γ (n+1) = (X t (n+1) X (n+1) ) -1 x n+1 and ε[n+1] = y n+1 -x t n+1 β(n)</formula><p>is the predicted residual of the (n + 1)the outcome based on the OLS of the first n observations. Proof of Theorem 11.4: This is the reverse form of the leave-one-out formula. We can view the first n + 1 data points as the full data, and β(n) as the OLS estimator leaving the (n + 1)the observation out. Applying Theorem 11.2, we have</p><formula xml:id="formula_275">β(n) = β(n+1) -(X t (n+1) X (n+1) ) -1 x n+1 εn+1 1 -h n+1,n+1 = β(n+1) -γ (n+1) ε[n+1] ,</formula><p>where εn+1 is the (n + 1)th residual based on the full data OLS, and the (n + 1)th predicted residual equals ε[n+1] = εn+1 /(1 -h n+1,n+1 ) based on Theorem 11.3. □ Theorem 11.4 shows that to obtain β(n+1) from β(n) , the adjustment depends on the predicted residual ε[n+1] . If we have a perfect prediction of the (n + 1)th observation based on β(n) , then we do not need to make any adjustment to obtain β(n+1) ; if the predicted residual is large, then we need to make a large adjustment.</p><p>Theorem 11.4 suggests an algorithm for sequentially computing the OLS estimators. But it gives a formula that involves inverting X t (n+1) X (n+1) at each step. Using the Sherman-Morrison formula in Problem 1.3 for updating the inverse of X t (n+1) X (n+1) based on the inverse of X t (n) X (n) , we have an even simpler algorithm below:</p><formula xml:id="formula_276">(G1) Start with V (n) = (X t (n) X (n) ) -1 and β(n) . (G2) Update V (n+1) = V (n) -1 + x t n+1 V (n) x n+1 -1 V (n) x n+1 x t n+1 V (n) . (G3) Calculate γ (n+1) = V (n+1) x n+1 and ε[n+1] = y n+1 -x t n+1 β(n) . (G4) Update β(n+1) = β(n) + γ (n+1) ε[n+1] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.2">Outlier detection based on residuals</head><p>Under the Normal linear model Y = Xβ + ε with ε ∼ N(0, σ 2 I n ), we know some basic probabilistic properties of the residual vector:</p><formula xml:id="formula_277">E(ε) = 0, var(ε) = σ 2 (I n -H).</formula><p>At the same time, the residual vector is computable based on the data. So it is sensible to check whether these properties of the residual vector are plausible based on data, which in turn serves as modeling checking for the Normal linear model. The first quantity is the standardized residual</p><formula xml:id="formula_278">standr i = εi σ2 (1 -h ii ) .</formula><p>We may hope that it has mean 0 and variance 1. However, because of the dependence between εi and σ2 , it is not easy to quantify the exact distribution of standr i .</p><p>The second quantity is the studentized residual based on the predicted residual:</p><formula xml:id="formula_279">studr i = ε[-i] σ2 [-i] /(1 -h ii ) = y i -x t i β[-i] σ2 [-i] /(1 -h ii )</formula><p>, where β[-i] and σ2 <ref type="bibr">[-i]</ref> are the estimates of the coefficient and variance based on the leave-iout OLS. Because (y i , β[-i] , σ2</p><p>[-i] ) are mutually independent under the Normal linear model, we can show that studr i ∼ t n-p-1 .</p><p>(11.7)</p><p>Because we know the distribution of studr i , we can compare it to the quantiles of the t distribution.</p><p>The third quantity is Cook's distance <ref type="bibr" target="#b79">(Cook, 1977)</ref>:</p><formula xml:id="formula_280">cook i = ( β[-i] -β) t X t X( β[-i] -β)/(pσ 2 ) = (X β[-i] -X β) t (X β[-i] -X β)/(pσ 2 ),</formula><p>where the first form measures the change of the OLS estimator and the second form measures the change in the predicted values based on leaving-i-out. It has a slightly different motivation, but eventually, it is related to the previous two residuals due to the leave-one-out formulas.</p><p>Theorem 11.5 Cook's distance is related to the standardized residual via:</p><formula xml:id="formula_281">cook i = standr 2 i × h ii p(1 -h ii )</formula><p>.</p><p>I leave the proof of Theorem 11.5 as Problem 11.4. I will end this subsubsection with two examples. The R code is in code11.3.2.R. The first one is simulated. I generate data from a univariate Normal linear model without outliers. I then use hatvalues, r.standard, r.student and cooks.distance to an lm.object to calculate the leverage scores, standardized residuals, studentized residuals, and Cook's distances. Their plots are in the first column of Figure <ref type="figure" target="#fig_95">11</ref>.1.</p><formula xml:id="formula_282">n = 1 0 0 x = seq ( 0 , 1 , length = n ) y = 1 + 3 * x + rnorm ( n ) lmmod = lm ( y ~x ) hatvalues ( lmmod ) rstandard ( lmmod ) rstudent ( lmmod ) cooks . distance ( lmmod )</formula><p>If I add 8 to the outcome of the last observation, the plots change to the second column of Figure <ref type="figure" target="#fig_95">11</ref>.1. If I add 8 to the 50th observation, the plots change to the last column of Figure <ref type="figure" target="#fig_95">11</ref>.1. Both visually show the outliers. In this example, the three residual plots give qualitatively the same pattern, so the choice among them does not matter much. In general cases, we may prefer studr i because it has a known distribution under the Normal linear model.</p><p>The second one is a further analysis of the Lalonde data. Based on the plots in Figure <ref type="figure" target="#fig_95">11</ref>.2, there are indeed some outliers in the data. It is worth investigating them more carefully.</p><p>Although the outliers detection methods above are very classic, they are rarely implemented in modern data analyses. They are simple and useful diagnostics. I recommend using them at least as a part of the exploratory data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.3">Jackknife</head><p>Jackknife is a general strategy for bias reduction and variance estimation proposed by <ref type="bibr" target="#b195">Quenouille (1949</ref><ref type="bibr" target="#b196">Quenouille ( , 1956) )</ref> and popularized by <ref type="bibr" target="#b227">Tukey (1958)</ref>. Based on independent data (Z 1 , . . . , Z n ), how to estimate the variance of a general estimator θ(Z 1 , . . . , Z n ) of the parameter θ? Define θ[-i] as the estimator without observation i, and the pseudo-value as The jackknife point estimator is θj = n -1 n i=1 θi , and the jackknife variance estimator is</p><formula xml:id="formula_283">θi = n θ -(n -1) θ[-i] .</formula><formula xml:id="formula_284">Vj = 1 n(n -1) n i=1 ( θi -θj )( θi -θj ) t .</formula><p>We have already shown that the OLS coefficient is unbiased and derived several variance estimators for it. Here we focus on the jackknife in OLS using the leave-one-out formula for the coefficient. The pseudo-value is</p><formula xml:id="formula_285">βi = n β -(n -1) β[-i] = n β -(n -1) β -(1 -h ii ) -1 (X t X) -1 x i εi = β + (n -1)(1 -h ii ) -1 (X t X) -1 x i εi .</formula><p>The jackknife point estimator is It is a little unfortunate that the jackknife point estimator is not identical to the OLS estimator, which is BLUE under the Gauss-Markov model. We can show that E( βj ) = β and it is a linear estimator. So the Gauss-Markov theorem ensures that cov( βj ) ⪰ cov( β). Nevertheless, the difference between βj and β is quite small. I omit their difference in the following derivation. Assuming that βj ∼ = β, we can continue to calculate the approximate jackknife variance estimator:</p><formula xml:id="formula_286">βj = β + n -1 n n -1 n i=1 x i x t i -1 n -1 n i=1 x i εi 1 -h ii .</formula><formula xml:id="formula_287">Vj ∼ = 1 n(n -1) n i=1 ( βi -β)( βi -β) t = n -1 n (X t X) -1 n i=1 εi 1 -h ii 2 x i x t i (X t X) -1 ,</formula><p>which is almost identical to the HC3 form of the EHW covariance matrix introduced in Chapter 6.4. <ref type="bibr">2. Miller (1974)</ref> first analyzed the jackknife in OLS but dismissed it immediately. <ref type="bibr" target="#b138">Hinkley (1977)</ref> modified the original jackknife and proposed a version that is identical to HC1, and <ref type="bibr" target="#b244">Wu (1986)</ref> proposed some further modifications and proposed a version that is identical to HC2. <ref type="bibr" target="#b234">Weber (1986)</ref> made connections between EHW and jackknife standard errors. However, <ref type="bibr" target="#b169">Long and Ervin (2000)</ref>'s finite-sample simulation seems to favor the original jackknife or HC3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4">Homework problems</head><p>11.1 Implementing the Gauss updating formula</p><p>Implement the algorithm in (G1)-( <ref type="formula">G4</ref>), and try it on simulated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">The distribution of the studentized residual</head><p>Prove (11.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">Leave-one-out coefficient</head><formula xml:id="formula_288">Prove β = n i=1 w i β[-i] ,</formula><p>and find the weights w i 's. Show they are positive and sum to one. Does β = n -1 n i=1 β[-i] hold in general?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4">Cook's distance and the standardized residual</head><p>Prove Theorem 11.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.5">The relationship between the standardized and studentized residual</head><p>Prove the following results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">(n</head><formula xml:id="formula_289">-p -1)σ 2 [-i] = (n -p)σ 2 -ε2 i /(1 -h ii ). 2.</formula><p>There is a monotone relationship between the standardized and studentized residual:</p><formula xml:id="formula_290">studr i = standr i n -p -1 n -p -standr 2 i .</formula><p>Remark: The right-hand side of the first result must be nonnegative so</p><formula xml:id="formula_291">n k=1 ε2 k ≥ ε2 i /(1 -h ii )</formula><p>, which implies the following inequality:</p><formula xml:id="formula_292">h ii + ε2 i n k=1 ε2 k ≤ 1.</formula><p>From this inequality, if h ii = 1 then εi = 0 which further implies that h ij = 0 for all j ̸ = i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.6">Subset and full-data OLS coefficients</head><p>Leaving one observation out, we have the OLS coefficient formula in Theorem 11.2. Leave a subset of observations out, we have the OLS coefficient formula below. Partition the covariate matrix and outcome vector based on a subset S of {1, . . . , n}:</p><formula xml:id="formula_293">X = X S X \S , Y = Y S Y \S .</formula><p>Without using the observations in set S, we have the OLS coefficient</p><formula xml:id="formula_294">β\S = (X t \S X \S ) -1 X \S Y \S .</formula><p>The following formula facilitates the computation of many β\S 's simultaneously, which relies crucially on the subvector of the residuals εS = Y S -X S β\S and the submatrix of</p><formula xml:id="formula_295">H H SS = X \S (X t X) -1 X t \S corresponding to the set S.</formula><p>Theorem 11.6 Recalling that β is the full data OLS, we have</p><formula xml:id="formula_296">β\S = β -(X t X) -1 X t \S (I -H SS ) -1 εS</formula><p>where I is the identity matrix with the same dimension of H SS .</p><p>Prove Theorem 11.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.7">Bounding the leverage score</head><p>Theorem 11.1 shows n -1 ≤ h ii ≤ 1 for all i = 1, . . . , n. Prove the following stronger result:</p><formula xml:id="formula_297">n -1 ≤ h ii ≤ s -1 i</formula><p>where s i is the number of rows that are identical to x i or -x i .</p><p>11.8 More on the leverage score</p><formula xml:id="formula_298">Show that det(X t [-i] X [-i] ) = (1 -h ii )det(X t X). Remark: If h ii = 1, then X t [-i] X [-i]</formula><p>is degenerate with determinant 0. Therefore, if we delete an observation i with leverage score 1, the columns of the covariate matrix become linearly dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Population Ordinary Least Squares and Inference with a Misspecified Linear Model</head><p>Previous chapters assume fixed X and random Y . We can also view each observation (x i , y i ) as IID draws from a population and discuss population OLS. We will view the OLS from the level of random variables instead of data points. Besides its theoretical interest, the population OLS facilitates the discussion of the properties of misspecified linear models and motivates a prediction procedure without imposing any distributional assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.1">Population OLS</head><p>Assume that (x i , y i ) n i=1 are IID with the same distribution as (x, y), where x ∈ R p and y ∈ R. Below I will use (x, y) to denote a general observation, dropping the subscript i for simplicity. Let the joint distribution be F (x, y), and E(•), var(•), and cov(•) be the expectation, variance, and covariance under this joint distribution. We want to use x to predict y. The following theorem states that the conditional expectation E(y | x) is the best predictor in terms of the mean squared error.</p><p>Theorem 12.1 For any function m(x), we have the decomposition</p><formula xml:id="formula_299">E {y -m(x)} 2 = E {E(y | x) -m(x)} 2 + E{var(y | x)}, (12.1)</formula><p>provided the existence of the moments in (12.1). This implies</p><formula xml:id="formula_300">E(y | x) = arg min m(•) E {y -m(x)} 2</formula><p>with the minimum value equaling E{var(y | x)}, the expectation of the conditional variance of y given x.</p><p>Theorem 12.1 is well known in probability theory. I relegate its proof as Problem 12.1. We have finite data points, but the function E(y | x) lies in an infinite dimensional space. Nonparametric estimation of E(y | x) is generally a hard problem, especially with a multidimensional x. As a starting point, we often use a linear function of x to approximate E(y | x) and define the population OLS coefficient as</p><formula xml:id="formula_301">β = arg min b∈R p L(b),<label>where</label></formula><formula xml:id="formula_302">L(b) = E (y -x t b) 2 = E y 2 + b t xx t b -2yx t b = E(y 2 ) + b t E (xx t ) b -2E (yx t ) b is a quadratic function of b.</formula><p>From the first-order condition, we have</p><formula xml:id="formula_303">∂L(b) ∂b | b=β = 2E (xx t ) β -2E (xy) = 0 based on Proposition A.7, so β = {E (xx t )} -1 E (xy) ; (12.2)</formula><p>from the second order condition, we have</p><formula xml:id="formula_304">∂ 2 L(b) ∂b∂b t = 2E (xx t ) ≥ 0. So β is the unique minimizer of L(b).</formula><p>The above derivation shows that x t β is the best linear predictor, and the following theorem states precisely that x t β is the best linear approximation to the possibly nonlinear conditional mean E(y | x).</p><p>Theorem 12.2 We have</p><formula xml:id="formula_305">β = arg min b∈R p E {E(y | x) -x t b} 2 = {E (xx t )} -1 E {xE(y | x)} .</formula><p>As a special case, when the covariate "x" in the above formulas contains 1 and a scalar x, the OLS coefficient has the following form.</p><p>Corollary 12.1 For scalar x and y, we have With β, we can define ε = y -x t β (12.3) as the population residual. Because we usually do not use the upper-case letter E for ε, this notation may cause confusions with previous discussion on OLS where ε denotes the vector of the error terms. Here ε is a scalar in (12.3). By the definition of β, we can verify</p><formula xml:id="formula_306">(α, β) = arg min (a,b) E(y -a -bx) 2 ,</formula><formula xml:id="formula_307">E(xε) = E {x(y -x t β)} = E(xy) -E(xx t )β = 0. (12.4)</formula><p>If we include 1 as a component of x, then E(ε) = 0, which, coupled with (12.4), implies cov(x, ε) = 0. So with an intercept in β, the mean of the population residual must be zero, and it is uncorrelated with other covariates by construction. We can also rewrite (12.3) as y = x t β + ε, (12.5) which holds by the definition of the population OLS coefficient and residual without any modeling assumption. We call (12.5) the population OLS decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.2">Population FWL theorem and Cochran's formula</head><p>To aid interpretation of the population OLS coefficient, we have the population FWL theorem.</p><p>Theorem 12.3 Consider the population OLS decomposition</p><formula xml:id="formula_308">y = β 0 + β 1 x 1 + • • • + β p-1 x p-1 + ε (12.6)</formula><p>and the partial population OLS decompositions:</p><formula xml:id="formula_309">x k = γ 0 + γ 1 x 1 + • • • + no x k + • • • + γ p-1 x p-1 + xk , (12.7) y = δ 0 + δ 1 x 1 + • • • + no x k + • • • + δ p-1 x p-1 + ỹ, (12.8) ỹ = βk xk + ε, (12.9)</formula><p>where xk is the residual from (12.7) and ỹ is the residual from (12.8). The coefficient β k from (12.6) equals cov(x k , ỹ)/var(x k ), the coefficient of xk from the population OLS of ỹ on xk in (12.9), which also equals cov(x k , y)/var(x k ), the coefficient of xk from the population OLS of y on xk . Moreover, ε from (12.6) equals ε from (12.9).</p><p>Similar to the proof of Theorem 7.1, we can invert the matrix E(xx t ) in (12.2) to prove Theorem 12.3 directly. Below I adopt an alternative proof which is a modification of the one given by <ref type="bibr" target="#b50">Angrist and Pischke (2008)</ref>. Proof of Theorem 12.3: Some basic identities of population OLS help to simplify the proof below. First, the OLS decomposition (12.7) ensures</p><formula xml:id="formula_310">cov(x k , x k ) = cov(x k , γ 0 + γ 1 x 1 + • • • + no x k + γ p-1 x p-1 + xk ) = cov(x k , xk ) that is, cov(x k , x k ) = var(x k ). (12.10) It also ensures cov(x k , x l ) = 0, (l ̸ = k). (12.11)</formula><p>Second, the OLS decompositions (12.6) and (12.8) ensure cov(x k , ε) = 0, (12.12) because xk is a linear combination of x by (12.7). It also ensures</p><formula xml:id="formula_311">cov(x k , δ 0 + δ 1 x 1 + • • • + no x k + • • • + δ p-1 x p-1 ) = 0, which further implies cov(x k , y) = cov(x k , ỹ). (12.13)</formula><p>Now I prove the equivalent forms of the coefficient β k . From (12.6), we have</p><formula xml:id="formula_312">cov(x k , y) = cov (x k , β 0 + β 1 x 1 + • • • + β p-1 x p-1 + ε) = β k cov(x k , x k ) + l̸ =k β l cov(x k , x l ) + cov(x k , ε) = β k var(x k ),</formula><p>by (12.10)-(12.12). Therefore,</p><formula xml:id="formula_313">β k = cov(x k , y) var(x k ) ,</formula><p>which also equals βk by (12.13).</p><p>Finally, I prove ε = ε. It suffices to show that ε = ỹ -βk xk satisfies</p><formula xml:id="formula_314">E(ε) = 0, cov(x k , ε) = 0, cov(x l , ε) = 0 (l ̸ = k).</formula><p>The first identity holds by (12.9). The second identity holds because</p><formula xml:id="formula_315">cov(x k , ε) = cov(x k , ỹ) -βk cov(x k , xk ) = cov(x k , ỹ) -βk cov(x k , xk ) = 0</formula><p>by (12.13), (12.10), and the population OLS of (12.9). The third identity holds because</p><formula xml:id="formula_316">cov(x l , ε) = cov(x l , ỹ) -βk cov(x l , xk ) = 0</formula><p>by (12.8) and (12.11). □ We also have a population version of Cochran's formula. Assume (y i , x i1 , x i2 ) n i=1 are IID, where y i is a scalar, x i1 has dimension k, and x i2 has dimension l. We have the following OLS decompositions of random variables <ref type="bibr">(12.14</ref>)</p><formula xml:id="formula_317">y i = β t 1 x i1 + β t 2 x i2 + ε i ,</formula><formula xml:id="formula_318">y i = βt 2 x i2 + εi ,<label>(12.15)</label></formula><p>x i1 = δ t x i2 + u i .</p><p>(12.16) Equation (12.14) defines the population long regression, and Equation (12.15) defines the population short regression. In Equation (12.16), δ is a l × k matrix because it is the OLS decomposition of a vector on a vector. We can view (12.16) as OLS decomposition of each component of x i1 on x i2 . The following theorem states the population version of Cochran's formula.</p><p>Theorem 12.4 Based on (12.14)-(12.16), we have β2 = β 2 + δβ 1 .</p><p>I relegate the proof of Theorem 12.4 as Problem 12.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3">Population R 2 and partial correlation coefficient</head><p>Exclude 1 from x and assume x ∈ R p-1 in this subsection. Assume that covariates and outcome are centered with mean zero and covariance matrix</p><formula xml:id="formula_319">cov x y = Σ xx Σ xy Σ yx σ 2 y .</formula><p>There are multiple equivalent definitions of R 2 . I start with the following definition</p><formula xml:id="formula_320">R 2 = Σ yx Σ -1 xx Σ xy σ 2 y ,</formula><p>and will give several equivalent definitions below. Let β be the population OLS coefficient of y on x, and ŷ = x t β be the best linear predictor.</p><p>Theorem 12.5 R 2 equals the ratio of the variance of the best linear predictor of y and the variance of y itself: R 2 = var(ŷ) var(y) .</p><p>Proof of Theorem 12.5: Because of the centering of x, we can verify that</p><formula xml:id="formula_321">var(ŷ) = β t cov(x)β = cov(y, x)cov(x) -1 cov(x)cov(x) -1 cov(x, y)</formula><p>= cov(y, x)cov(x) -1 cov(x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>Theorem 12.6 R 2 equals the maximum value of the squared Pearson correlation coefficient between y and a linear combination of x:</p><formula xml:id="formula_322">R 2 = max b∈R p-1 ρ 2 (y, x t b) = ρ 2 (y, ŷ).</formula><p>Proof of Theorem 12.6: We have</p><formula xml:id="formula_323">ρ 2 (y, x t b) = cov 2 (y, x t b) var(y)var(x t b) = b t Σ xy Σ yx b σ 2 y × b t Σ xx b . Define γ = Σ 1/2 xx b and b = Σ -1/2</formula><p>xx γ such that b and γ have one-to-one mapping. So the maximum value of</p><formula xml:id="formula_324">σ 2 y × ρ 2 (y, x t b) = γ t Σ -1/2 xx Σ xy Σ yx Σ -1/2 xx γ γ t γ , equals the maximum eigenvalue of Σ -1/2 xx Σ xy Σ yx Σ -1/2</formula><p>xx , attained when γ equals the corresponding eigenvector; see Theorem A.4. This matrix is positive semi-definite and has rank one due to Proposition A.1, so it has exactly one non-zero eigenvalue which must equal its trace. So</p><formula xml:id="formula_325">max b∈R p-1 ρ 2 (y, x t b) = σ -2 y trace(Σ -1/2 xx Σ xy Σ yx Σ -1/2 xx ) = σ -2 y trace(Σ xy Σ yx Σ -1/2 xx Σ -1/2 xx ) = σ -2 y trace(Σ yx Σ -1 xx Σ xy ) = σ -2 y Σ yx Σ -1 xx Σ xy = R 2 .</formula><p>We can easily verify that R 2 = ρ 2 (y, ŷ).</p><p>□ We can also define population partial correlation and R 2 . For scalar y and x with another scalar or vector w, we can define the population OLS decomposition based on (1, w) as <ref type="bibr">(12.17)</ref> where</p><formula xml:id="formula_326">y = ŷ + ỹ, x = x + x,</formula><formula xml:id="formula_327">ỹ = {y -E(y)} -{w -E(w)} t β y , x = {x -E(x)} -{w -E(w)} t β x ,</formula><p>with β y and β x being the coefficients of w in these population OLS. We then define the population partial correlation coefficient as</p><formula xml:id="formula_328">ρ yx|w = ρ ỹ x.</formula><p>If the marginal correlation and partial correlation have different signs, then we have Simpson's paradox at the population level. With a scalar w, we have a more explicit formula below.</p><p>Theorem 12.7 For scalar (y, x, w), we have</p><formula xml:id="formula_329">ρ yx|w = ρ yx -ρ xw ρ yw 1 -ρ 2 xw 1 -ρ 2 yw .</formula><p>I relegate the proof of Theorem 12.7 as Problem 12.7.</p><p>We can also extend the definition of ρ yx|w to the partial R 2 with a scalar y and possibly vector x and w. The population OLS decompositions (12.17) still hold in the general case. Then we can define the partial R 2 between y and x given w as the R 2 between ỹ and x:</p><formula xml:id="formula_330">R 2 y.x|w = R 2 ỹ.x .</formula><p>12.4 Inference for the population OLS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.4.1">Inference with the EHW standard errors</head><p>Based on the IID data (x i , y i ) n i=1 , we can easily obtain the moment estimator for the population OLS coefficient</p><formula xml:id="formula_331">β = n -1 n i=1 x i x t i -1 n -1 n i=1 x i y i ,</formula><p>and the residuals εi = y i -x t i β. Assume finite fourth moments of (x, y). We can use the law of large numbers to show that</p><formula xml:id="formula_332">n -1 n i=1 x i x t i → E(xx t ), n -1 n i=1 x i y i → E(xy), so β → β in probability. We can use the CLT to show that n -1/2 n i=1 x i ε i → N(0, M ) in distribution, where M = E(ε 2 xx t ), so √ n( β -β) → N(0, V ) (12.18) in distribution, where V = B -1 M B -1 with B = E(xx t ).</formula><p>The moment estimator for the asymptotic variance of β is again the Eicker-Huber-White robust covariance estimator:</p><formula xml:id="formula_333">Vehw = n -1 n -1 n i=1 x i x t i -1 n -1 n i=1 ε2 i x i x t i n -1 n i=1 x i x t i -1 . (12.19)</formula><p>Following the almost the same proof of Theorem 6.3, we can show that Vehw is consistent for the asymptotic covariance V . I summarize the formal results below, with the proof relegated as Problem 12.4.</p><p>Theorem 12.8 Assume that (x i , y i ) n i=1 iid ∼ (x, y) with E(∥x∥ 4 ) &lt; ∞ and E(y 4 ) &lt; ∞. We have (12.18) and n Vehw → V in probability.</p><p>So the EHW standard error is not only robust to the heteroskedasticity of the errors but also robust to the misspecification of the linear model <ref type="bibr" target="#b146">(Huber, 1967;</ref><ref type="bibr" target="#b237">White, 1980b;</ref><ref type="bibr" target="#b50">Angrist and Pischke, 2008;</ref><ref type="bibr">Buja et al., 2019a)</ref>. Of course, when the linear model is wrong, we need to modify the interpretation of β: it is the coefficient of x in the best linear prediction of y or the best linear approximation of the conditional mean function E(y | x).</p><p>12.5 To model or not to model? I start with a simple example. In the following calculation, I will use the fact that the kth moment of a Uniform(0, 1) random variable equals 1/(k + 1).</p><p>Example 12.1 Assume that x ∼ F (x), ε ∼ N(0, 1), x ε, and y = x 2 + ε.</p><formula xml:id="formula_334">1. If x ∼ F 1 (x) is Uniform(-1, 1), then the best linear approximation is 1/3 + 0 • x. We can see this result from β(F 1 ) = cov(x, y) var(x) = cov(x, x 2 ) var(x) = E(x 3 ) var(x) = 0, and α(F 1 ) = E(y) = E(x 2 ) = 1/3. 2. If x ∼ F 2 (x)</formula><p>is Uniform(0, 1), then the best linear approximation is -1/6 + x. We can see this result from</p><formula xml:id="formula_335">β(F 2 ) = cov(x, y) var(x) = cov(x, x 2 ) var(x) = E(x 3 ) -E(x)E(x 2 ) E(x 2 ) -{E(x)} 2 = 1/4 -1/2 × 1/3 1/3 -(1/2) 2 = 1, and α(F 2 ) = E(y) -βE(x) = E(x 2 ) -E(x) = 1/3 -1/2 = -1/6 3. If x ∼ F 3 (x) is Uniform(-1, 0)</formula><p>, then the best linear approximation is -1/6 -x. This result holds by symmetry.</p><p>Figure <ref type="figure" target="#fig_110">12</ref>.1 shows the true conditional mean function x 2 and the best linear approximations. As highlighted in the notation above, the best linear approximation depends on the distribution of X.</p><p>From the above, we can see that the best linear approximation depends on the distribution of X. This complicates the interpretation of β from the population OLS decomposition <ref type="bibr">(Buja et al., 2019a)</ref>. More importantly, this can cause problems if we care about the external validity of statistical inference <ref type="bibr">(Sims, 2010, page 66)</ref>.</p><p>However, if we believe the following restricted mean model</p><formula xml:id="formula_336">E(y | x) = x t β (12.20)</formula><p>or, equivalently,</p><formula xml:id="formula_337">y = x t β + ε, E(ε | x) = 0,</formula><p>then the population OLS coefficient is the true parameter of interest:</p><formula xml:id="formula_338">{E(xx t )} -1 E(xy) = {E(xx t )} -1 E {xE(y | x)} = {E(xx t )} -1 E(xx t β) = β.</formula><p>Moreover, the population OLS coefficient does not depend on the distribution of x. The above asymptotic inference applies to this model too. <ref type="bibr" target="#b107">Freedman (1981)</ref> distinguished two types of OLS: the regression model and the correlation model, as shown in Figure <ref type="figure" target="#fig_110">12</ref>.2. The left-hand side represents the regression model, or the restricted mean model (12.20). In the regression model, we first generate x and ε under some restrictions, for example, E(ε | x) = 0, and then generate the outcome based on y = x t β + ε, a linear function of x with error ε. In the correlation model, we start with a pair (x, y), then decompose y into the best linear predictor x t β and the leftover residual ε. The latter ensures E(εx) = 0, but the former requires E(ε | x) = 0. So the former imposes a stronger assumption since E(ε | x) = 0 implies</p><formula xml:id="formula_339">E(εx) = E{E(εx | x)} = E{E(ε | x)x} = 0.</formula><p>12.5.2 Anscombe's Quartet: the importance of graphical diagnostics <ref type="bibr" target="#b51">Anscombe (1973)</ref> used four simple datasets to illustrate the importance of graphical diagnostics in linear regression. His datasets are in anscombe in the R package datasets: x1 and y1 constitute the first dataset, and so on.</p><p>&gt; library ( datasets ) &gt; # # Anscombe ' s Quartet &gt; anscombe</p><p>x 1 x 2 x 3 x 4 y 1 y 2 y 3 y 4 1 1 0 1 0 1 0 8 8 . 0 4 9 . 1 4 7 . 4 6 6 . 5 8 2 8 8 8 8 6 . 9 5 8 . 1 4 6 . 7 7 5 . 7 6 3 1 3 1 3 1 3 8 7 . 5 8 8 . 7 4 1 2 . 7 4 7 . 7 1 4 9 9 9 8 8 . 8 1 8 . 7 7 7 . 1 1 8 . 8 4 5 1 1 1 1 1 1 8 8 . 3 3 9 . 2 6 7 . 8 1 8 . 4 7 6 1 4 1 4 1 4 8 9 . 9 6 8 . 1 0 8 . 8 4 7 . 0 4 regression model correlation model FIGURE 12.2: Freedman's classification of OLS 7 6 6 6 8 7 . 2 4 6 . 1 3 6 . 0 8 5 . 2 5 8 4 4 4 1 9 4 . 2 6 3 . 1 0 5 . 3 9 1 2 . 5 0 9 1 2 1 2 1 2 8 1 0 . 8 4 9 . 1 3 8 . 1 5 5 . 5 6 1 0 7 7 7 8 4 . 8 2 7 . 2 6 6 . 4 2 7 . 9 1 1 1 5 5 5 8 5 . 6 8 4 . 7 4 5 . 7 3 6 . 8 9</p><p>The four datasets have similar sample moments.</p><formula xml:id="formula_340">&gt; # # mean of x &gt; c ( mean ( anscombe $ x 1 ) , + mean ( anscombe $ x 2 ) , + mean ( anscombe $ x 3 ) , + mean ( anscombe $ x 4 )) [ 1 ] 9 9 9 9 &gt; # # variance of x &gt; c ( var ( anscombe $ x 1 ) , + var ( anscombe $ x 2 ) , + var ( anscombe $ x 3 ) , + var ( anscombe $ x 4 )) [ 1 ] 1 1 1 1 1 1 1 1 &gt; # # mean of y &gt; c ( mean ( anscombe $ y 1 ) , +</formula><p>mean ( anscombe $ y 2 ) , + mean ( anscombe $ y 3 ) , + mean ( anscombe $ y 4 )) [ 1 ] 7 . 5 0 0 9 0 9 7 . 5 0 0 9 0 9 7 . 5 0 0 0 0 0 7 . 5 0 0 9 0 9 &gt; # # variance of y &gt; c ( var ( anscombe $ y 1 ) , + var ( anscombe $ y 2 ) , + var ( anscombe $ y 3 ) , + var ( anscombe $ y 4 )) [ 1 ] 4 . 1 2 7 2 6 9 4 . 1 2 7 6 2 9 4 . 1 2 2 6 2 0 4 . 1 2 3 2 4 9</p><p>The results based on linear regression are almost identical.</p><p>&gt; ols 1 = lm ( y 1 ~x 1 , data = anscombe ) &gt; summary ( ols 1 ) Call : lm ( formula = y 1 ~x 1 , data = anscombe ) <ref type="table">9 2 1 2 7 -0 . 4 5 5 7 7 -0 . 0 4 1 3 6 0 . 7 0 9 4 1 1 . 8 3 8 8 2</ref> Coefficients :</p><formula xml:id="formula_341">Residuals : Min 1 Q Median 3 Q Max -1 .</formula><p>Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept )</p><p>3 . 0 0 0 1 1 . 1 2 4 7 2 . 6 6 7 0 . 0 2 5 7 3 * x 1 0 . 5 0 0 1 0 . 1 1 7 9 4 . 2 4 1 0 . 0 0 2 1 7 ** Residual standard error : 1 . 2 3 7 on 9 degrees of freedom Multiple R -squared : 0 . 6 6 6 5 , Adjusted R -squared : 0 . 6 2 9 5 F -statistic : 1 7 . 9 9 on 1 and 9 DF , p -value : 0 . 0 0 2 1 7 &gt; ols 2 = lm ( y 2 ~x 2 , data = anscombe ) &gt; summary ( ols 2 ) Call : lm ( formula = y 2 ~x 2 , data = anscombe ) Residuals :</p><p>Min 1 Q Median 3 Q Max -1 . 9 0 0 9 -0 . 7 6 0 9 0 . 1 2 9 1 0 . 9 4 9 1 1 . Residual standard error : 1 . 2 3 6 on 9 degrees of freedom Multiple R -squared : 0 . 6 6 6 7 , Adjusted R -squared : 0 . 6 2 9 7 F -statistic :</p><p>1 8 on 1 and 9 DF , p -value : 0 . 0 0 2 1 6 5</p><p>However, the scatter plots of the datasets in Figure <ref type="figure" target="#fig_110">12</ref>.3 reveal fundamental differences between the datasets. The first dataset seems ideal for linear regression. The second dataset shows a quadratic form of y versus x, and therefore, the linear model is misspecified. The third dataset shows a linear trend of y versus x, but an outlier has severely distorted the slope of the linear line. The fourth dataset is supported on only two values of x and thus may suffer from severe extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.5.3">More on residual plots</head><p>Most standard statistical theory for inference assumes a correctly specified linear model (e.g., Gauss-Markov model, Normal linear model, or restricted mean model). However, the corresponding inferential procedures are often criticized since it is challenging to ensure that the model is correctly specified. Alternatively, we can argue that without assuming the linear model, the OLS estimator is consistent for the coefficient in the best linear approximation of the conditional mean function E(y | x), which is often a meaningful quantify even the linear model is misspecified. This can be misleading. Example 12.1 shows that the best linear approximation can be a bad approximation to a nonlinear conditional mean function, and it depends on the distribution of the covariates.</p><p>A classic statistical approach is to check whether the residual εi has any nonlinear trend with respect to the covariates. With only a few covariates, we can plot the residual against each covariate; with many covariates, we can plot the residual against the fitted value ŷi . Figure <ref type="figure" target="#fig_110">12</ref>.4 gives four examples with the R code in code12.5.2.R. In these examples, the covariates are the same:</p><formula xml:id="formula_342">n = 2 0 0 x 1 = rexp ( n ) x 2 = runif ( n )</formula><p>The outcome models differ:</p><formula xml:id="formula_343">1. Example a: y = x1 + x2 + rnorm(n) 2. Example b: y = x1 + x2 + rnorm(n, 0, x1+x2) 3. Example c: y = x1^2+ x2^2 + rnorm(n) 4. Example d: y = x1^2+ x2^2 + rnorm(n, 0, x1+x2)</formula><p>In the last two examples, the residuals indeed show some nonlinear relationship with the covariates and the fitted value. This suggests that the linear function can be a poor approximation to the true conditional mean function. It requires the Normal linear model assumption. Chapter 6 relaxes the Normality assumption on the error term in statistical inference but does not discuss prediction. Under the heteroskedastic linear model assumption, we can predict the mean E(y n+1 ) = x t n+1 β by x t n+1 β with asymptotic standard error (x t n+1 Vehw x n+1 ) 1/2 , where Vehw is the EHW covariance matrix for the OLS coefficient. However, it is fundamentally challenging to predict y n+1 itself since the heteroskedastic linear model allows it to have a completely unknown variance σ 2 n+1 . Under the population OLS formulation, it seems even more challenging to predict the future outcome since we do not even assume that the linear model is correctly specified. In particular, x t n+1 β does not have the same mean as y n+1 in general. Perhaps surprisingly, we can construct a prediction interval for y n+1 based on x n+1 and (X, Y ) using an idea called conformal prediction <ref type="bibr" target="#b233">(Vovk et al., 2005;</ref><ref type="bibr" target="#b163">Lei et al., 2018)</ref>. It leverages the exchangeability<ref type="foot" target="#foot_9">foot_9</ref> of the data points (x 1 , y 1 ), . . . , (x n , y n ), (x n+1 , y n+1 ).</p><p>Pretending that we know the value y n+1 = y * , we can fit OLS using n + 1 data points and obtain residuals εi (y * ) = y i -x t i β(y * ), (i = 1, . . . , n + 1) where we emphasize the dependence of the OLS coefficient and residuals on the unknown y * . The absolute values of the residuals |ε i (y * )|'s are also exchangeable, so the rank of</p><formula xml:id="formula_344">|ε n+1 (y * )|, denoted by Rn+1 (y * ) = 1 + n i=1 1{|ε i (y * )| ≤ |ε n+1 (y * )|},</formula><p>must have a uniform distribution over {1, 2, . . . , n, n + 1}, a known distribution not depending on anything else. It is a pivotal quantity satisfying pr Rn+1 (y * ) ≤ ⌈(1 -α)(n + 1)⌉ ≥ 1 -α.</p><p>(12.21)</p><p>Equivalently, this is a statement linking the unknown quantity y * and observed data, so it gives a confidence set for y * at level 1 -α. In practice, we can use a grid search to solve for the inequality (12.21) involving y * . Below we evaluate the leave-one-out prediction with the Boston housing data. </p><formula xml:id="formula_345">= qr . Q ( X . QR ) X . R = qr . R ( X . QR ) Gram . inv = solve ( t ( X . R )%*% X . R ) hatmat = X . Q %*% t ( X . Q ) resmat = diag ( n ) -hatmat leverage = diag ( hatmat ) Resvec</formula><p>= ols . fit . full $ residuals cvt = qt ( 0 . 9 7 5 , df = n -p -1 ) cvr = ceiling ( 0 . 9 5 *( n + 1 ))</p><p>loo . pred = matrix ( 0 , n , 5 ) loo . cov = matrix ( 0 , n , 2 )</p><formula xml:id="formula_346">for ( i in 1 : n ) { beta . i = beta -Gram . inv %*% X [i , ]* Resvec [ i ]/( 1 -leverage [ i ]) e . sigma . i = sqrt ( e . sigma ^2 *( n -p ) - ( Resvec [ i ])^2 /( 1 -leverage [ i ]))/ sqrt ( n -p -1 ) pred . i = sum ( X [i , ]* beta . i ) lower . i = pred . i -cvt * e . sigma . i / sqrt ( 1 -leverage [ i ]) upper . i = pred . i + cvt * e . sigma . i / sqrt ( 1 -leverage [ i ]) loo . pred [i , 1 : 3 ] = c ( pred .i , lower .i , upper . i ) loo . cov [i , 1 ] = findInterval ( Y [ i ] , c ( lower .i , upper . i )) grid . r = sapply ( grid .y , FUN = function ( y ){ Res = Resvec + resmat [ , i ]*( y -Y [ i ])</formula><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q first 20 middle 20 last 20 In the above code, I use the QR decomposition to compute X t X and H. Moreover, the calculations of lower.i, upper.i, and Res use some tricks to avoid fitting n OLS. I relegate the justification of them in Problem 12.10.</p><p>The variable loo.pred has five columns corresponding to the point predictors, lower and upper intervals based on the Normal linear model and conformal prediction.</p><p>&gt; colnames ( loo . pred ) = c ( " point " , " G . l " , " G . u " , " c . l " , " c . u " ) &gt; head ( loo . pred ) point G . l G . u c . l c . u [ 1 ,] 6 . 6 3 3 5 1 4 -2 . 9 4 1 5 3 2 1 6 . 2 0 8 5 5 9 -3 . 5 1 6 . 7 [ 2 ,] 8 . 8 0 6 6 4 1 -1 . 3 4 9 3 6 7 1 8 . 9 6 2 6 4 9 -2 . 6 2 0 . 1 [ 3 ,] 1 2 . 0 4 4 1 5 4 2 . 6 0 8 2 9 0 2 1 . 4 8 0 0 1 8 2 . 2 2 1 . 8 [ 4 ,] 1 1 . 0 2 5 2 5 3 1 . 5 6 5 1 5 2 2 0 . 4 8 5 3 5 5 1 . 2 2 1 . 0 [ 5 ,] -5 . 1 8 1 1 5 4 -1 4 . 8 1 9 0 4 1 4 . 4 5 6 7 3 3 -1 5 . 0 4 . 9 [ 6 ,] 8 . 3 2 4 1 1 4 -1 . 3 8 2 9 1 0 1 8 . 0 3 1 1 3 8 -2 . 0 1 8 . 8</p><p>Figure <ref type="figure" target="#fig_110">12</ref>.5 plots the observed outcomes and the prediction intervals for the 20 observations with the outcomes at the bottom, middle, and top. The Normal and conformal intervals are almost indistinguishable. For the observations with the highest outcome, the predictions are quite poor. Surprisingly, the overall coverage rates across observations are close to 95% for both methods.</p><p>&gt; apply ( loo . cov == 1 , 2 , mean ) [ 1 ] 0 . 9 4 8 6 1 6 6 0 . 9 5 2 5 6 9 2 Figure 12.6 compares the lengths of the two prediction intervals. Although the conformal prediction intervals are slightly wider than the Normal prediction interval, the differences are rather small, with the ratio of the length above 0.96.</p><p>The R code is in code12.6.R. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.85</p><p>0.90 0.95 1.00 0 100 200 300 400 500 observation i Normal over conformal ratio of the lengths of leave-one-out prediction intervals FIGURE 12.6: Boston housing data 12.7 Homework problems 12.1 Conditional mean Prove Theorem 12.1. 12.2 Best linear approximation Prove Theorem 12.2. Hint: It is similar to Problem 8.6. 12.3 Univariate population OLS Prove Corollary 12.1. 12.4 Asymptotics for the population OLS Prove Theorem 12.8. 12.5 Population Cochran's formula Prove Theorem 12.4. 12.6 Canonical correlation analysis (CCA) Assume that (x, y), where x ∈ R p and y ∈ R k , has the joint non-degenerate covariance matrix: Σ xx Σ xy Σ yx Σ yy . 1. Find the best linear combinations (α, β) that give the maximum Pearson correlation coefficient: (α, β) = arg max a∈R k ,b∈R p ρ(y t a, x t b).</p><p>Note that you need to detail the steps in calculating (α, β) based on the covariance matrix above.</p><p>2. Define the maximum value as cc(x, y). Show that cc(x, y) ≥ 0 and cc(x, y) = 0 if x y.</p><p>Remark: The maximum value cc(x, y) is called the canonical correlation between x and y. We can also define partial canonical correlation between x and y given w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.7">Population partial correlation coefficient</head><p>Prove Theorem 12.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.8">Independence and correlation</head><p>With scalar random variables x and y, show that if x y, then ρ yx = 0. With another random variable w, if x y | w, does ρ yx|w = 0 hold? If so, give a proof; otherwise, give a counterexample.</p><p>12.9 Best linear approximation of a cubic curve Assume that x ∼ N(0, 1), ε ∼ N(0, σ 2 ), x ε, and y = x 3 + ε. Find the best linear approximation of E(y | x) based on x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.10">Leave-one-out formula in conformal prediction</head><p>Justify the calculations of lower.i, upper.i, and Res in Section 12.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.11">Conformal prediction for multiple outcomes</head><p>Assuming exchangeability of (x 1 , y 1 ), . . . , (x n , y n ), (x n+1 , y n+1 ), . . . , (x n+k , y n+k ).</p><p>Propose a method to construct joint conformal prediction regions for (y n+1 , . . . , y n+k ) based on (X, Y ) and (x n+1 , . . . , x n+k ).</p><p>12.12 Cox's theorem <ref type="bibr" target="#b80">Cox (1960)</ref> considered the data-generating process x 1 -→ x 2 -→ y under the following linear models: for i = 1, . . . , n, we have</p><formula xml:id="formula_347">x i2 = α 0 + α 1 x i1 + η i and y i = β 0 + β 1 x i2 + ε i</formula><p>where η i has mean 0 and variance σ 2 η , ε i has mean 0 and variance σ 2 ε , and the η i s and ε i s are independent. The linear model implies</p><formula xml:id="formula_348">y i = (β 0 + β 1 α 0 ) + (β 1 α 1 )x i1 + (ε i + β 1 η i )</formula><p>where ε i + β 1 η i s are independent with mean 0 and variance σ 2 ε + β 2 1 σ 2 η . Therefore, we have two ways to estimate β 1 α 1 :</p><p>1. the first estimator is γ1 , the OLS estimator of the y i 's on the x i1 's with the intercept; 2. the second estimator is α1 β1 , the product of the OLS estimator of the x i2 's on the x i1 's with the intercept and that of the y i 's on the x i2 's with the intercept. <ref type="bibr" target="#b80">Cox (1960)</ref> proved the following theorem.</p><p>Theorem 12.9 Let X 1 = (x 11 , . . . , x n1 ). We have var( α1 β1 | X 1 ) ≤ var(γ 1 | X 1 ), and more precisely,</p><formula xml:id="formula_349">var(γ 1 | X 1 ) = σ 2 ε + β 2 1 σ 2 η n i=1 (x i1 -x1 ) 2 and var( α1 β1 | X 1 ) = σ 2 ε E(ρ 2 12 | X 1 ) + β 2 1 σ 2 η n i=1 (x i1 -x1 ) 2</formula><p>where ρ12 ∈ [-1, 1] is the sample Pearson correlation coefficient between the x i1 's and the x i2 's.</p><p>Prove Theorem 12.9. Remark: If we further assume that the error terms are Normal, then α1 β1 is the maximum likelihood estimator for α 1 β 1 . Therefore, the asymptotic optimality theory for the maximum likelihood estimator justifies the superiority of α1 β1 over γ1 . Cox's theorem provides a stronger finite-sample result without assuming the Normality of the error terms.</p><p>12.13 Measurement error and Frisch's bounds 1. Given scalar random variables x and y, we can obtain the population OLS coefficient (α, β) of y on (1, x). However, x and y may be measured with errors, that is, we observe x * = x + u and y * = y + v, where u and v are mean zero error terms satisfying u v and (u, v) (x, y). We can obtain the population OLS coefficient (α * , β * ) of y * on (1, x * ) and the population OLS coefficient (a * , b * ) of x * on (1, y * ).</p><p>Prove that if β = 0 then</p><formula xml:id="formula_350">β * = b * = 0; if β ̸ = 0 then |β * | ≤ |β| ≤ 1/|b * |.</formula><p>2. Given scalar random variables x, y and a random vector w, we can obtain the population OLS coefficient (α, β, γ) of y on (1, x, w). When x and y are measured with error as above with mean zero errors satisfying u v and (u, v) (x, y, w), we can obtain the population OLS coefficient (α * , β * , γ * ) of y on (1, x * , w), and the population OLS coefficient (a * , b * , c * ) of x * on (1, y * , w).</p><p>Prove that the same result holds as in the first part of the problem.</p><p>Remark: Tamer (2010) reviewed Frisch (1934)'s upper and lower bounds for the univariate OLS coefficient based on the two OLS coefficients of the observables. The second part of the problem extends the result to the multivariate OLS with a covariate subject to measurement error. The lower bound is well documented in most books on measurement errors, but the upper bound is much less well known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.14">A three-way decomposition</head><p>The main text focuses on the two-way decomposition of the outcome: y = x t β + ε, where β is the population OLS coefficient and ε is the population OLS residual. However, x t β is only the best linear approximation to the true conditional mean function µ(x) ≡ E(y | x). This suggests the following three-way decomposition of the outcome:</p><formula xml:id="formula_351">y = x t β + {µ(x) -x t β} + {y -µ(x)},</formula><p>which must hold without any assumptions. Introduce the notation for the linear term ŷ = x t β, the notation for the approximation error:</p><formula xml:id="formula_352">δ = µ(x) -x t β,</formula><p>and the notation for the "ideal residual":</p><formula xml:id="formula_353">e = y -µ(x).</formula><p>Then we can decompose the outcome as</p><formula xml:id="formula_354">y = ŷ + δ + e</formula><p>and the population OLS residual as</p><formula xml:id="formula_355">ε = {µ(x) -x t β} + {y -µ(x)} = δ + e. 1. Prove that E(ŷe | x) = 0, E(δe | x) = 0 and E(ŷe) = 0, E(δe) = 0, E(ŷδ) = 0. Further, prove that E(ε 2 ) = E(δ 2 ) + E(e 2 ).</formula><p>2. Introduce an intermediate quantity between the population OLS coefficient β and the OLS coefficient β:</p><formula xml:id="formula_356">β = n -1 n i=1 x i x t i -1 n -1 n i=1 x i µ(x i ) . Equation (12.18) states that √ n( β -β) → N(0, B -1 M B -1 ) in distribution, where B = E(xx t ) and M = E(ε 2 xx t ). Prove that cov( β -β) = cov( β -β) + cov( β -β),</formula><p>and moreover,</p><formula xml:id="formula_357">√ n( β -β) → N(0, B -1 M 1 B -1 ), √ n( β -β) → N(0, B -1 M 2 B -1 )</formula><p>in distribution, where</p><formula xml:id="formula_358">M 1 = E(e 2 xx t ), M 2 = E(δ 2 xx t ) Verify that M = M 1 + M 2 .</formula><p>Remark: To prove the result, you may find the law of total covariance formula in (B.4) helpful. We can also write M 1 as M 1 = E{var(y | x)xx t }. So the meat matrix M has two sources of uncertainty, one is from the conditional variance of y given x, and the other is from the approximation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part V</head><p>Overfitting, Regularization, and Model Selection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perils of Overfitting</head><p>Previous chapters assume that the covariate matrix X is given and the linear model, correctly specified or not, is also given. Although including useless covariates in the linear model results in less precise estimators, this problem is not severe when the total number of covariates is small compared to the sample size. In many modern applications, however, the number of covariates can be large compared to the sample size. Sometimes, it can be a nonignorable fraction of the sample size; sometimes, it can even be larger than the sample size. For instance, modern DNA sequencing technology often generates covariates of millions of dimensions, which is much larger than the usual sample size under study. In these applications, the theory in previous chapters is inadequate. This chapter introduces an important notation in statistics: overfitting.</p><p>13.1 David Freedman's simulation <ref type="bibr" target="#b108">Freedman (1983)</ref> used a simple simulation to illustrate the problem with a large number of covariates. He simulated data from the following Normal linear model Y = Xβ + ε with ε ∼ N(0, σ 2 I n ) and β = (µ, 0, . . . , 0) t . He then computed the sample R 2 . Since the covariates do not explain any variability of the outcome at all in the true model, we would expect R 2 to be extremely small over repeated sampling. However, he showed, via both simulation and theory, that R 2 is surprisingly large when p is large compared to n. Figure <ref type="figure" target="#fig_95">13</ref>.1 shows the results from Freedman's simulation setting with n = 100 and p = 50, over 1000 replications. The R code is in code13.1.R. The (1, 1)th subfigure shows the histogram of the R 2 , which centers around 0.5. This can be easily explained by the exact distribution of R 2 proved in Corollary 10.1:</p><formula xml:id="formula_359">R 2 ∼ Beta p -1 2 , n -p 2 ,</formula><p>with the density shown in the (1, 1)th and (1, 2)th subfigure of Figure <ref type="figure" target="#fig_95">13</ref>.1. The beta distribution above has mean</p><formula xml:id="formula_360">E(R 2 ) = p-1 2 p-1 2 + n-p 2 = p -1 n -1 and variance var(R 2 ) = p-1 2 × n-p 2 p-1 2 + n-p 2 2 p-1 2 + n-p 2 + 1 = 2(p -1)(n -p) (n -1) 2 (n + 1) .</formula><p>When p/n → 0, we have E(R 2 ) → 0, var(R 2 ) → 0, so Markov's inequality implies that R 2 → 0 in probability. However, when p/n → γ ∈ (0, 1), we have E(R 2 ) → γ, var(R 2 ) → 0, so Markov's inequality implies that R 2 → γ in probability. This means that when p has the same order as n, the sample R 2 is close to the ratio p/n even though there is no association between the covariates and the outcome in the true data-generating process. In Freedman's simulation, γ = 0.5 so R 2 is close to 0.5. The (1, 2)th subfigure shows the histogram of the R 2 based on a model selection first step by dropping all covariates with p-values larger than 0.25. The R 2 in the (1, 2)th subfigure are slightly smaller but still centered around 0.37. The joint F test based on the selected model does not generate uniform p-values in the (2, 2)th subfigure, in contrast to the uniform p-values in the (2, 1)th subfigure. With a model selection step, statistical inference becomes much more complicated. This is a topic called selective inference which is beyond the scope of this book.</p><p>The above simulation and calculation give an important warning: we cannot overinterpret the sample R 2 since it can be too optimistic about model fitting. In many empirical research, R 2 is at most 0.1 with a large number of covariates, making us wonder whether those researchers are just chasing the noise rather than the signal. So we do not trust R 2 as a model-fitting measure with a large number of covariates. In general, R 2 cannot avoid overfitting, and we must modify it in model selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2">Variance inflation factor</head><p>The following theorem quantifies the potential problem of including too many covariates in OLS.</p><p>Theorem 13.1 Consider a fixed design matrix X. Let βj be the coefficient of X j of the OLS fit of Y on (1 n , X 1 , . . . , X q ) with q ≤ p. Under the model y i = f (x i ) + ε i with an unknown f (•) and the ε i 's uncorrelated with mean zero and variance σ 2 , the variance of βj equals</p><formula xml:id="formula_361">var( βj ) = σ 2 n i=1 (x ij -xj ) 2 × 1 1 -R 2 j ,</formula><p>where R 2 j is the sample R 2 from the OLS fit of X j on 1 n and all other covariates.</p><p>Theorem 13.1 does not even assume that the true mean function is linear. It states that the variance of βj has two multiplicative components. If we run a short regression of Y on 1 n and X j = (x 1j , . . . , x nj ) t , the coefficient equals</p><formula xml:id="formula_362">βj = n i=1 (x ij -xj )y i n i=1 (x ij -xj ) 2 where xj = n -1 n i=1 x ij . It has variance var( βj ) = var n i=1 (x ij -xj )y i n i=1 (x ij -xj ) 2 = n i=1 (x ij -xj ) 2 σ 2 { n i=1 (x ij -xj ) 2 } 2 = σ 2 n i=1 (x ij -xj ) 2 .</formula><p>So the first component is the variance of the OLS coefficient in the short regression. The second component 1/(1 -R 2 j ) is called the variance inflation factor (VIF). The VIF indeed inflates the variance of βj , and the more covariates are added into the long regression, the larger the variance inflation factor is. In R, the car package provides the function vif to compute the VIF for each covariate.</p><p>The proof of Theorem 13.1 below is based on the FWL theorem. Proof of Theorem 13.1: Let Xj = (x 1j , . . . , xnj ) t be the residual vector from the OLS fit of X j on 1 n and other covariates, which have a sample mean of zero. The FWL theorem implies that</p><formula xml:id="formula_363">βj = n i=1 xij y i n i=1 x2 ij which has variance var( βj ) = n i=1 x2 ij σ 2 n i=1 x2 ij 2 = σ 2 n i=1 x2 ij . (13.1) Because n i=1</formula><p>x2 ij is the residual sum of squares from the OLS of X j on 1 n and other covariates, it is related to R 2 j via</p><formula xml:id="formula_364">R 2 j = 1 - n i=1 x2 ij n i=1 (x ij -xj ) 2 or, equivalently, n i=1 x2 ij = (1 -R 2 j ) n i=1 (x ij -xj ) 2 . (13.2)</formula><p>Combining (13.1) and (13.2) gives Theorem 13.1. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.3">Bias-variance trade-off</head><p>Theorem 13.1 characterizes the variance of the OLS coefficient, but it does not characterize its bias. In general, a more complex model is closer to the true mean function f (x i ), and can then reduce the bias of approximating the mean function. However, Theorem 13.1 implies that a more complex model results in larger variances of the OLS coefficients. So we face a bias-variance trade-off. Consider a simple case where the true data-generating process is linear:</p><formula xml:id="formula_365">y i = β 1 + β 2 x i1 + • • • + β s-1 x is + ε i . (13.3)</formula><p>Ideally, we want to use the model (13.3) with exactly s covariates. In practice, we may not know which covariates to include in the OLS. If we underfit the data using a short regression with q &lt; s:</p><formula xml:id="formula_366">y i = β1 + β2 x i1 + • • • + βq-1 x iq + εi , (i = 1, . . . , n) (13.4)</formula><p>then the OLS coefficients are biased. If we increase the complexity of the model to overfit the data using a long regression with p &gt; s:</p><formula xml:id="formula_367">y i = β1 + β2 x i1 + • • • + βp-1 x ip + εi , (i = 1, . . . , n) (13.5)</formula><p>then the OLS coefficients are unbiased. Theorem 13.1, however, shows that the OLS coefficients from the under-fitted model (13.4) have smaller variances than those from the overfitted model (13.5).</p><p>Example 13.1 In general, we have a sequence of models with increasing complexity. For simplicity, we consider nested models containing 1 n and covariates</p><formula xml:id="formula_368">{X 1 } ⊂ {X 1 , X 2 } ⊂ • • • ⊂ {X 1 , . . . , X p }</formula><p>in the following simulation setting. The true linear model is y i = x t i β + N(0, 1) with p = 40 but only the first 10 covariates have non-zero coefficients 1 and all other covariates have coefficients 0. We generate two datasets: both have sample size n = 200, all covariates have IID N(0, 1) entries, and the error terms are IID. We use the first dataset to fit the OLS and thus call it the training dataset. We use the second dataset to assess the performance of the fitted OLS from the training dataset, and thus call it the testing dataset<ref type="foot" target="#foot_10">foot_10</ref> . Figure <ref type="figure" target="#fig_95">13</ref>.2 plots the residual sum of squares against the number of covariates in the training and testing datasets. By definition of OLS, the residual sum of squares decreases with the number of covariates in the training dataset, but it first decreases and then increases in the testing dataset with minimum value attained at 10, the number of covariates in the true data generating process.</p><p>The following example has a nonlinear true mean function but still uses OLS with polynomials of covariates to approximate the truth<ref type="foot" target="#foot_11">foot_11</ref> .</p><p>Example 13.2 The true nonlinear model is y i = sin(2πx i ) + N(0, 1) with the x i 's equally spaced in [0, 1] and the error terms are IID. The training and testing datasets both have sample sizes n = 200. Figure <ref type="figure" target="#fig_95">13</ref>.3 plots the residual sum of squares against the order of the polynomial in the OLS fit</p><formula xml:id="formula_369">y i = p-1 j=0 β j x j i + ε i .</formula><p>By the definition of OLS, the residual sum of squares decreases with the order of polynomials in the training dataset, but it achieves the minimum near p = 5 in the testing dataset. We can show that the residual sum of squares decreases to zero with p = n in the training dataset; see Problem 13.7. However, it is larger than that under p = 5 in the testing dataset.</p><p>The R code for Figures 13.2 and 13.3 is in code13.3.R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4">Model selection criteria</head><p>With a large number of covariates X 1 , . . . , X p , we want to select a model that has the best performance for prediction. In total, we have 2 p possible models. Which one is the best? What is the criterion for the best model? Practitioners often use the linear model for multiple purposes. A dominant criterion is the prediction performance of the linear model in a new dataset <ref type="bibr" target="#b245">(Yu and Kumbier, 2020)</ref>. However, we do not have the new dataset yet in the statistical modeling stage. So we need to find criteria that are good proxies for the prediction performance.</p><p>13.4.1 RSS, R 2 and adjusted R 2</p><p>The first obvious criterion is the rss, which, however, is not a good criterion because it favors the largest model. The sample R 2 has the same problem of favoring the largest model. Most model selection criteria are in some sense modifications of rss or R 2 . The adjusted R 2 takes into account the complexity of the model:</p><formula xml:id="formula_370">R2 = 1 - n -1 n -p (1 -R 2 ) = 1 - n i=1 (y i -ŷi ) 2 /(n -p) n i=1 (y i -ȳ) 2 /(n -1) = 1 - σ2 σ2 y .</formula><p>So based on R2 , the best model has the smallest σ2 , the estimator for the variance of the error term in the Gauss-Markov model. The following theorem shows that R2 is closely related to the F statistic in testing two nested Normal linear models.</p><p>Theorem 13.2 Consider the setting of Chapter 8.3. Test two nested Normal linear models:</p><formula xml:id="formula_371">Y = X 1 β 1 + ε versus Y = X 1 β 1 + X 2 β 2 + ε,</formula><p>or, equivalently, test β 2 = 0. We can use the standard F statistic defined in Chapter 8.3, and we can also compare the adjusted R 2 's from these two models: R2 1 and R2 2 . They are related via</p><formula xml:id="formula_372">F &gt; 1 ⇐⇒ R2 1 &lt; R2 2 .</formula><p>I leave the proof of the theorem as Problem 13.3. From Theorem 13.2, R2 does not necessarily favor the largest model. However, R2 still favors unnecessarily large models compared with the usual hypothesis testing based on the Normal linear model because the mean of F is approximately 1, but the upper quantile of F is much larger than 1 (for example, the 95% quantile of F 1,n-p is larger than 3.8, and the 95% quantile of F 2,n-p is larger than 2.9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.2">Information criteria</head><p>Taking into account the model complexity, we can find the model with the smallest aic or bic, defined as aic = n log rss n + 2p and bic = n log rss n + p log n, with full names "Akaike's information criterion " and "Bayes information criterion." aic and bic are both monotone functions of the rss penalized by the number of parameters p in the model. The penalty in bic is larger so it favors smaller models than aic. Shao (1997)'s results suggested that bic can consistently select the true model if the linear model is correctly specified, but aic can select the model that minimizes the prediction error if the linear model is misspecified. In most statistical practice, the linear model assumption cannot be justified, so we recommend using aic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.3">Cross-validation (CV)</head><p>The first choice is the leave-one-out cross-validation based on the predicted residual:</p><formula xml:id="formula_373">press = n i=1 ε2 [-i] = n i=1 ε2 i (1 -h ii ) 2 (13.6)</formula><p>which is called the predicted residual error sum of squares (PRESS) statistic.</p><p>Because the average value of h ii is n -1 n i=1 h ii = p/n, we can approximate PRESS by the generalized cross-validation (GCV) criterion:</p><formula xml:id="formula_374">gcv = n i=1 ε2 i (1 -p/n) 2 = rss × 1 - p n -2 . When p/n ≈ 0, we have 3 log gcv = log rss -2 log 1 - p n ≈ log rss + 2p n = aic/n + log n,</formula><p>so GCV is approximately equivalent to AIC with small p/n. With large p/n, they may have large differences. GCV is not crucial for OLS because it is easy to compute PRESS. However, it is much more useful in other models where we need to fit the data n times to compute PRESS. For a general model without simple leave-one-out formulas, it is computationally intensive to obtain PRESS. The K-fold cross-validation (K-CV) is computationally more attractive. The best model has the smallest K-CV, computed as follows:</p><p>1. randomly shuffle the observations; 2. split the data into K folds; 3. for each fold k, use all other folds as the training data, and compute the predicted errors on fold k (k = 1, . . . , K);</p><p>4. aggregate the prediction errors across K folds, denoted by K-CV.</p><p>When K = 3, we split the data into 3 folds. Run OLS to obtain a fitted function with folds 2, 3 and use it to predict on fold 1, yielding prediction error r 1 ; run OLS with folds 1, 3 and predict on fold 2, yielding prediction error r 2 ; run OLS with folds 1, 2 and predict on fold 2, yielding prediction error r 3 . The total prediction error is r = r 1 + r 2 + r 3 . We want to select a model that minimizes r. Usually, practitioners choose K = 5 or 10, but this can depend on the computational resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5">Best subset and forward/backward selection</head><p>Given a model selection criterion, we can select the best model.</p><p>For a small p, we can enumerate all 2 p models. The function regsubsets in the R package leaps implements this<ref type="foot" target="#foot_13">foot_13</ref> . Figure <ref type="figure" target="#fig_95">13</ref>.4 shows the results of the best subset selection in two applications, with the code in code13.4.4.R.</p><p>For large p, we can use forward or backward regressions. Forward regression starts with a model with only the intercept. In step one, it finds the best covariate among the p candidates based on the prespecified criterion. In step two, it keeps this covariate in the model and finds the next best covariate among the remaining p -1 candidates. It proceeds by adding the next best covariate one by one.</p><p>The backward regression does the opposite. It starts with the full model with all p covariates. In step one, it drops the worst covariate among the p candidates based on the prespecified criterion. In step two, it drops the next worst covariate among the remaining p -1 candidates. It proceeds by dropping the next worst covariate one by one.</p><p>Both methods generate a sequence of models, and select the best one based on the prespecified criterion. Forward regression works in the case with p ≥ n but it stops at step n -1; backward regression works only in the case with p &lt; n. The functions step or stepAIC in the MASS package implement these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.1">Inflation and deflation of the estimated variance</head><p>This problem extends Theorem 13.1 to the estimated variance.</p><p>The covariate matrix X has columns 1 n , X 1 , . . . , X p . Compare the coefficient of X 1 in the following long and short regressions:</p><formula xml:id="formula_375">Y = β0 1 n + β1 X 1 + • • • + βp X p + ε, and Y = β0 1 n + β1 X 1 + • • • + βq X q + ε,</formula><p>where q &lt; p. Under the condition in Theorem 13.1,</p><formula xml:id="formula_376">var( β1 ) var( β1 ) = 1 -R 2 X1.X2•••Xq 1 -R 2 X1.X2•••Xp ≥ 1, recalling that R 2 U.</formula><p>V denotes the R 2 of U on V . Now we compare the corresponding estimated variances v ar( β1 ) and ṽ ar( β1 ) based on homoskedasticity.</p><formula xml:id="formula_377">1. Show that v ar( β1 ) ṽ ar( β1 ) = 1 -R 2 Y.X1•••Xp 1 -R 2 Y.X1•••Xq × 1 -R 2 X1.X2•••Xq 1 -R 2 X1.X2•••Xp × n -q -1 n -p -1 .</formula><p>2. Using the definition of the partial R 2 in Problem 10.4, show that v ar( β1 ) ṽ ar</p><formula xml:id="formula_378">( β1 ) = 1 -R 2 Y.Xq+1•••Xp|X1•••Xq 1 -R 2 X1.Xq+1•••Xp|X2...Xq × n -q -1 n -p -1 .</formula><p>Remark: The first result shows that the ratio of the estimated variances has three factors: the first one corresponds to the R 2 's of the outcome on the covariates, the second one is identical to the one for the ratio of the true variances, and the third one corresponds to the degrees of freedom correction. The first factor deflates the estimated variance since the R 2 increases with more covariates included in the regression, and the second and the third factors inflate the estimated variance. Overall, whether adding more covariates inflate or deflate the estimated variance depends on the interplay of the three factors. The answer is not definite as Theorem 13.1.</p><p>The variance inflation result in Theorem 13.1 sometimes causes confusion. It only concerns the variance. When we view some covariates as random, then the bias term can also contribute to the variance of the OLS estimator. In this case, we should interpret Theorem 13.1 with caution. See Ding (2021b) for a related discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2">Inflation and deflation of the variance under heteroskedasticity</head><p>Relax the condition in Theorem 13.1 as var(ε i ) = σ 2 i with possibly different variances. Give a counterexample in which the variance decreases with more covariates included in the regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.3">Equivalence of F and R2</head><p>Prove Theorem 13.2. (1 -h ii ) -1 is unbiased for σ 2 under the Gauss-Markov model in Assumption 4.1, recalling press in (13.6) and the leverage score h ii of unit i.</p><p>Remark: Theorem 4.3 shows that σ2 = rss/(n -p) is unbiased for σ 2 under the Gauss-Markov model. rss is the "in-sample" residual sum of squares, whereas press is the "leaveone-out" residual sum of squares. The estimator σ2 is standard, whereas σ2 press appeared in <ref type="bibr" target="#b209">Shen et al. (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5">Simulation with misspecified linear models</head><p>Replicate the simulation in Example 13.1 with correlated covariates and an outcome model with quadratic terms of covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6">Best subset selection in lalonde data</head><p>Produce the figure similar to the ones in Figure <ref type="figure" target="#fig_95">13</ref>.4 based on the lalonde data in the Matching package. Report the selected model based on aic, bic, press, and gcv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.7">Perfect polynomial</head><p>Prove that given distinct x i (i = 1, . . . , n) within [0, 1] and any y i (i = 1, . . . , n), we can always find an nth order polynomial</p><formula xml:id="formula_379">p n (x) = n-1 j=0 b j x j such that p n (x i ) = y i , (i = 1, . . . , n).</formula><p>Hint: Use the formula in (A.4). The first column corresponds to the full model without testing, and the second column corresponds to the selected model with testing.</p><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 30 40 q training data testing data FIGURE 13.2: Training and testing errors: linear mean function q q q q q q q q q q q q q q q q q q q q q 5 10 15 20 0.9 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ridge Regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.1">Introduction to ridge regression</head><p>The OLS estimator has many nice properties. For example, Chapter 4 shows that it is BLUE under the Gauss-Markov model, and Chapter 5 shows that it follows a Normal distribution that allows for finite-sample exact inference under the Normal linear model. However, it can have the following problems that motivate ridge regression in this chapter.</p><p>The first motivation is quite straightforward. From the formula</p><formula xml:id="formula_380">β = (X t X) -1 X t Y,</formula><p>if the columns of X are highly correlated, then X t X will be nearly singular; more extremely, if the number of covariates is larger than the sample size, then X t X has a rank smaller than or equal to n and thus is not invertible. So numerically, the OLS estimator can be unstable due to inverting X t X. Because X t X must be positive semi-definite, its smallest eigenvalue determines whether it is invertible or not. <ref type="bibr" target="#b141">Hoerl and Kennard (1970)</ref> proposed the following ridge estimator as a modification of OLS:</p><formula xml:id="formula_381">βridge (λ) = (X t X + λI p ) -1 X t Y, (λ &gt; 0) (14.1)</formula><p>which involves a positive tuning parameter λ. Because the smallest eigenvalue of X t X +λI p is larger than or equal to λ &gt; 0, the ridge estimator is always well defined. Now I turn to the second equivalent motivation. The OLS estimator minimizes the residual sum of squares</p><formula xml:id="formula_382">rss(b 0 , b 1 , . . . , b p ) = n i=1 (y i -b 0 -b 1 x i1 -• • • -b p x ip ) 2 .</formula><p>From Theorem 13.1 on the variance inflation factor, the variances of the OLS estimators increase with additional covariates included in the regression, leading to unnecessarily large estimators by chance. To avoid large OLS coefficients, we can penalize the residual sum of squares criterion with the squared length of the coefficients<ref type="foot" target="#foot_14">foot_14</ref> and use βridge (λ) = arg min b0,b1,...,bp</p><formula xml:id="formula_383">   rss(b 0 , b 1 , . . . , b p ) + λ p j=1 b 2 j    . (14.2)</formula><p>Again in (14.2), λ is a tuning parameter that ranges from zero to infinity. We first discuss the ridge estimator with a fixed λ and then discuss how to choose it. When λ = 0, it reduces to OLS; when λ = ∞, all coefficients must be zero except that βridge</p><formula xml:id="formula_384">0 (∞) = ȳ.</formula><p>With λ ∈ (0, ∞), the ridge coefficients are generally smaller than the OLS coefficients, and the penalty shrinks the OLS coefficients toward zero. So the parameter λ controls the magnitudes of the coefficients or the "complexity" of the model. In (14.2), we only penalize the slope parameters not the intercept.</p><p>As a dual problem in optimization, we can also define the ridge estimator as Definitions (14.2) and ( <ref type="formula">14</ref>.3) are equivalent because for a given λ, we can always find a t such that the solutions from (14.2) and ( <ref type="formula">14</ref>.3) are identical. In fact, the corresponding t and</p><formula xml:id="formula_385">λ satisfy t = ∥ βridge (λ)∥ 2 .</formula><p>However, the ridge estimator has an obvious problem: it is not invariant to linear transformations of X. In particular, it is not equivalent under different scaling of the covariates. Intuitively, the b j 's depend on the scale of X j 's, but the penalty term p j=1 b<ref type="foot" target="#foot_15">foot_15</ref> j puts equal weight on each coefficient. A convention in practice is to standardize the covariates before applying the ridge estimator 2 . Condition 14.1 (standardization) The covariates satisfy</p><formula xml:id="formula_386">n -1 n i=1 x ij = 0, n -1 n i=1 x 2 ij = 1, (j = 1, . . . , p)</formula><p>and the outcome satisfy ȳ = 0.</p><p>With all covariates centered at zero, the ridge estimator for the intercept, given any values of the slopes and the tuning parameter λ, equals βridge 0 = ȳ. So if we center the outcomes at mean zero, then we can drop the intercept in the ridge estimators defined in (14.2) and (14.3).</p><p>For descriptive simplicity, we will assume Condition 14.1 and call it standardization from now on. This allows us to drop the intercept. Using the matrix form, the ridge estimator minimizes</p><formula xml:id="formula_387">(Y -Xb) t (Y -Xb) + λb t b,</formula><p>which is a quadratic function of b. From the first order condition, we have</p><formula xml:id="formula_388">-2X t Y -X βridge (λ) + 2λ βridge (λ) = 0 =⇒ βridge (λ) = (X t X + λI p ) -1 X t Y,</formula><p>which coincides with the definition in (14.1). We also have the second-order condition</p><formula xml:id="formula_389">2X t X + 2λI p ≻ 0, (λ &gt; 0)</formula><p>which verifies that the ridge estimator is indeed the minimizer. The predicted vector is</p><formula xml:id="formula_390">Ŷ ridge (λ) = X βridge (λ) = X(X t X + λI p ) -1 X t Y = H(λ)Y,</formula><p>where H(λ) = X(X t X + λI p ) -1 X t is the hat matrix for ridge regression. When λ = 0, it reduces to the hat matrix for the OLS; when λ &gt; 0, it is not a projection matrix because {H(λ)} 2 ̸ = H(λ).</p><p>14.2 Ridge regression via the SVD of X I will focus on the case with n ≥ p and relegate the discussion of the case with n ≤ p to Problem 14.11. To facilitate the presentation, I will use the SVD decomposition of the covariate matrix:</p><formula xml:id="formula_391">X = U DV t</formula><p>where U ∈ R n×p has orthonormal columns such that U t U = I p , V ∈ R p×p is an orthogonal matrix with V V t = V t V = I p , and D ∈ R p×p is a diagonal matrix consisting of the singular values. Figure <ref type="figure" target="#fig_95">14</ref>.1 illustrates this important decomposition.</p><formula xml:id="formula_392">orthonormal columns diagonal orthogonal FIGURE 14.1: SVD of X</formula><p>The SVD of X implies the eigen-decomposition of X t X:</p><formula xml:id="formula_393">X t X = V D 2 V t</formula><p>with eigen-vectors V j being the column vectors of V and eigen-values d 2 j being the squared singular values. The following lemma is crucial for simplifying the theory and computation.</p><p>Lemma 14.1 The ridge coefficient equals</p><formula xml:id="formula_394">βridge (λ) = V diag d j d 2 j + λ U t Y,</formula><p>where the diagonal matrix is p × p.</p><p>Proof of Lemma 14.1: The ridge coefficient equals</p><formula xml:id="formula_395">βridge (λ) = (X t X + λI p ) -1 X t Y = (V DU t U DV t + λI p ) -1 V DU t Y = V (D 2 + λI p ) -1 V t V DU t Y = V (D 2 + λI p ) -1 DU t Y = V diag d j d 2 j + λ U t Y. □</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.3">Statistical properties</head><p>The Gauss-Markov theorem shows that the OLS estimator is BLUE under the Gauss-Markov model: Y = Xβ + ε, where ε has mean zero and covariance σ 2 I n . Then in what sense, can ridge regression improve OLS? I will discuss the statistical properties of the ridge estimator under the Gauss-Markov model.</p><p>Based on Lemma 14.1, we can calculate the mean of the ridge estimator:</p><formula xml:id="formula_396">E{ βridge (λ)} = V diag d j d 2 j + λ U t Xβ = V diag d j d 2 j + λ U t U DV t β = V diag d 2 j d 2 j + λ V t β</formula><p>which does not equal β in general. So the ridge estimator is biased. We can also calculate the covariance matrix of the ridge estimator:</p><formula xml:id="formula_397">cov{ βridge (λ)} = σ 2 V diag d j d 2 j + λ U t U diag d j d 2 j + λ V t = σ 2 V diag d 2 j (d 2 j + λ) 2 V t .</formula><p>The mean squared error (MSE) is a measure capturing the bias-variance trade-off:</p><formula xml:id="formula_398">mse(λ) = E βridge (λ) -β t βridge (λ) -β .</formula><p>Using Theorem B.8 on the expectation of a quadratic form, we have</p><formula xml:id="formula_399">mse(λ) = [E{ βridge (λ)} -β] t [E{ βridge (λ)} -β] C1 + trace[cov{ βridge (λ)}] C2 .</formula><p>The following theorem simplifies C 1 and C 2 .</p><p>Theorem 14.1 Under Assumption 4.1, the ridge estimator satisfies <ref type="bibr">(14.4)</ref> where γ = V t β = (γ 1 , . . . , γ p ) t has the jth coordinate γ j = V t j β, and</p><formula xml:id="formula_400">C 1 = λ 2 p j=1 γ 2 j (d 2 j + λ) 2 ,</formula><formula xml:id="formula_401">C 2 = σ 2 p j=1 d 2 j (d 2 j + λ) 2 .</formula><p>(14.5)</p><p>Proof of Theorem 14.1: First, we have</p><formula xml:id="formula_402">C 1 = β t V diag d 2 j d 2 j + λ V t -I p 2 β = β t V diag λ 2 (d 2 j + λ) 2 V t β = γ t diag λ 2 (d 2 j + λ) 2 γ = λ 2 p j=1 γ 2 j (d 2 j + λ) 2 .</formula><p>Second, we have</p><formula xml:id="formula_403">C 2 = σ 2 trace V diag d 2 j (d 2 j + λ) 2 V t = σ 2 trace diag d 2 j (d 2 j + λ) 2 = σ 2 p j=1 d 2 j (d 2 j + λ) 2 .</formula><p>□ Theorem 14.1 shows the bias-variance trade-off for the ridge estimator. The MSE is</p><formula xml:id="formula_404">mse(λ) = C 1 + C 2 = λ 2 p j=1 γ 2 j (d 2 j + λ) 2 + σ 2 p j=1 d 2 j (d 2 j + λ) 2 .</formula><p>When λ = 0, the ridge estimator reduces to the OLS estimator: the bias is zero and the variance σ 2 p j=1 d -2 j dominates. When λ = ∞, the ridge estimator reduces to zero: the bias p j=1 γ 2 j dominates and the variance is zero. As we increase λ from zero, the bias increases and the variance decreases. So we face a bias-variance trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.4">Selection of the tuning parameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.4.1">Based on parameter estimation</head><p>For parameter estimation, we want to choose the λ that minimizes the MSE. So the optimal λ must satisfy the following first-order condition:</p><formula xml:id="formula_405">∂mse(λ) ∂λ = 2 p j=1 γ 2 j λ d 2 j + λ d 2 j + λ -λ (d 2 j + λ) 2 -2σ 2 p j=1 d 2 j (d 2 j + λ) 3 = 0 which is equivalent to λ p j=1 γ 2 j d 2 j (d 2 j + λ) 3 = σ 2 p j=1 d 2 j (d 2 j + λ) 3 .</formula><p>(14.6) However, (14.6) is not directly useful because we do not know γ and σ 2 . Three methods below try to solve (14.6) approximately. <ref type="bibr" target="#b90">Dempster et al. (1977)</ref> used OLS to construct an unbiased estimator σ2 and γ = V t β, and then solve λ from <ref type="bibr" target="#b140">Hoerl et al. (1975)</ref> assumed that X t X = I p . Then d 2 j = 1 (j = 1, . . . , p) and γ = β, and solve λ from</p><formula xml:id="formula_406">λ p j=1 γ2 j d 2 j (d 2 j + λ) 3 = σ2 p j=1 d 2 j (d 2 j + λ) 3 , which is a nonlinear equation of λ.</formula><formula xml:id="formula_407">λ p j=1 β2 j (1 + λ) 3 = σ2 p j=1 1 (1 + λ) 3 , resulting in λ hkb = pσ 2 /∥ β∥ 2 .</formula><p>Lawless (1976) used</p><formula xml:id="formula_408">λ lw = pσ 2 / βt D 2 β</formula><p>to weight the β j 's based on the eigenvalues of X t X. But all these methods require estimating (β, σ 2 ). If the initial OLS estimator is not reliable, then these estimates of λ are unlikely to be reliable. None of these methods work for the case with p &gt; n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.4.2">Based on prediction</head><p>For prediction, we need slightly different criteria. Without estimating (β, σ 2 ), we can use the leave-one-out cross-validation. The leave-one-out formula for the ridge below is similar to that for OLS. Theorem 14.2 Define β(λ) = (X t X + λI p ) -1 X t Y as the ridge coefficient (dropping the superscript "ridge" for simplicity), ε(λ) = Y -X β(λ) as the residual vector using the full data, and h ii (λ) = x t i (X t X + λI p ) -1 x i as the (i, i)th diagonal element of H(λ) = X(X t X + λI p ) -1 X t . Define β[-i] (λ) as the ridge coefficient without observation i, and</p><formula xml:id="formula_409">ε[-i] (λ) = y i -x t i β[-i] (λ)</formula><p>as the predicted residual. The leave-one-out formulas for ridge regression are</p><formula xml:id="formula_410">β[-i] (λ) = β(λ) -{1 -h ii (λ)} -1 (X t X + λI p ) -1 x i εi (λ) and ε[-i] (λ) = εi (λ)/{1 -h ii (λ)}.</formula><p>I leave the proof of Theorem 14.2 as Problem 14.5. By Theorem 14.2, the PRESS statistic for ridge is <ref type="bibr" target="#b126">Golub et al. (1979)</ref> proposed the GCV criterion to simplify the calculation of the PRESS statistic by replacing h ii (λ) with their average value n -1 trace{H(λ)}:</p><formula xml:id="formula_411">press(λ) = n i=1 ε[-i] (λ) 2 = n i=1 {ε i (λ)} 2 {1 -h ii (λ)} 2 .</formula><formula xml:id="formula_412">gcv(λ) = n i=1 {ε i (λ)} 2 [1 -n -1 trace {H(λ)}] 2 .</formula><p>In the R package MASS, the function lm.ridge implements the ridge regression, kHKB and kLW report two estimators for λ, and GCV contains the GCV values for a sequence of λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.5">Computation of ridge regression</head><p>Lemma 14.1 gives the ridge coefficients. So the predicted vector equals</p><formula xml:id="formula_413">Ŷ (λ) = X βridge (λ) = U DV t V diag d j d 2 j + λ U t Y = U Ddiag d j d 2 j + λ U t Y = U diag d 2 j d 2 j + λ U t Y,</formula><p>and the hat matrix equals</p><formula xml:id="formula_414">H(λ) = U diag d 2 j d 2 j + λ U t .</formula><p>These formulas allow us to compute the ridge coefficient and predictor vector for many values of λ without inverting each X t X + λI p . We have similar formulas for the case with n &lt; p; see Problem 14.11.</p><p>A subtle point is due to the standardization of the covariates of the outcome. In R, the lm.ridge function first computes the ridge coefficient based on the standardized covariates and outcome, and then transforms them back to the original scale. Let x1 , . . . , xp , ȳ be the means of the covariates and outcome, and let sd j = {n -1 n i=1 (x ij -xi ) 2 } 1/2 be the standard deviation of the covariates which are report as scales in the output of lm.ridge. From the ridge coefficients { βridge </p><formula xml:id="formula_415">(λ)(x i1 -x1 )/sd 1 + • • • + βridge p (λ)(x ip -xp )/sd p or, equivalently, ŷi (λ) = αridge (λ) + βridge 1 (λ)/sd 1 × x i1 + • • • + βridge p (λ)/sd p × x ip where αridge (λ) = ȳ -βridge 1 (λ)x 1 /sd 1 -• • • -βridge p (λ)x p /sd p .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6">Numerical examples</head><p>We can use the following numerical example to illustrate the bias-variance trade-off in selecting λ in the ridge. The code is in code14.5.R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6.1">Uncorrelated covariates</head><p>I first simulate data from a Normal linear model with uncorrelated covariates.</p><p>library ( MASS ) n = 2 0 0 p = 1 0 0 beta = rep ( 1 / sqrt ( p ) , p ) sig = 1 / 2 X = matrix ( rnorm ( n * p ) , n , p ) X = scale ( X ) X = X * sqrt ( n /( n -1 )) Y = as . vector ( X %*% beta + rnorm (n , 0 , sig ))</p><p>The following code calculates the theoretical bias, variance, and mean squared error, reported in the (1, 1)th panel of Figure <ref type="figure" target="#fig_95">14</ref>.2. eigenxx = eigen ( t ( X )%*% X ) xis = eigenxx $ values gammas = t ( eigenxx $ vectors )%*% beta lambda . seq = seq ( 0 , 7 0 , 0 . 0 1 ) bias 2 . seq = lambda . seq var . seq = lambda . seq mse . seq = lambda . seq for ( i in 1 : length ( lambda . seq )) { ll = lambda . seq [ i ] bias 2 . seq [ i ] = ll ^2 * sum ( gammas ^2 /( xis + ll )^2 ) var . seq [ i ] = sig ^2 * sum ( xis /( xis + ll )^2 ) mse . seq [ i ] = bias 2 . seq [ i ] + var . seq [ i ] }</p><p>y . min = min ( bias 2 . seq , var . seq , mse . seq ) y . max = max ( bias 2 . seq , var . seq , mse . seq ) par ( mfrow = c ( 2 , 2 )) plot ( bias 2 . seq ~lambda . seq , type = " l " , ylim = c ( y . min , y . max ) , xlab = expression ( lambda ) , main = " " , ylab = " bias -variance ␣ tradeoff " , lty = 2 , bty = " n " ) lines ( var . seq ~lambda . seq , lty = 3 ) lines ( mse . seq ~lambda . seq , lwd = 3 , lty = 1 ) abline ( v = lambda . seq [ which . min ( mse . seq )] , lty = 1 , col = " grey " ) legend ( " topright " , c ( " bias " , " variance " , " mse " ) , lty = c ( 2 , 3 , 1 ) , lwd = c ( 1 , 1 , 4 ) , bty = " n " )</p><p>The (1, 1)th panel also reported the λ's based on different approaches.</p><p>ridge . fit = lm . ridge ( Y ~X , lambda = lambda . seq ) abline ( v = lambda . seq [ which . min ( ridge . fit $ GCV )] , lty = 2 , col = " grey " ) abline ( v = ridge . fit $ kHKB , lty = 3 , col = " grey " ) abline ( v = ridge . fit $ kLW , lty = 4 , col = " grey " ) legend ( " bottomright " , c ( " MSE " , " GCV " , " HKB " , " LW " ) , lty = 1 : 4 , col = " grey " , bty = " n " )</p><p>I also calculate the prediction error of the ridge estimator in the testing dataset, which follows the same data-generating process as the training dataset. The (1, 2)th panel of Figure <ref type="figure" target="#fig_95">14</ref>.2 shows its relationship with λ. Overall, GCV, HKB, and LW are similar, but the λ selected by the MSE criterion is the worst for prediction.</p><p>X . new = matrix ( rnorm ( n * p ) , n , p ) X . new = scale ( X . new ) X . new = X . new * matrix ( sqrt ( n /( n -1 )) , n , p ) Y . new = as . vector ( X . new %*% beta + rnorm (n , 0 , sig )) predict . error = Y . new -X . new %*% ridge . fit $ coef predict . mse = apply ( predict . error ^2 , 2 , mean ) plot ( predict . mse ~lambda . seq , type = " l " , xlab = expression ( lambda ) , ylab = " predicted ␣ MSE " , bty = " n " ) abline ( v = lambda . seq [ which . min ( mse . seq )] , lty = 1 , col = " grey " ) abline ( v = lambda . seq [ which . min ( ridge . fit $ GCV )] , lty = 2 , col = " grey " ) abline ( v = ridge . fit $ kHKB , lty = 3 , col = " grey " ) abline ( v = ridge . fit $ kLW , lty = 4 , col = " grey " ) legend ( " bottomright " , c ( " MSE " , " GCV " , " HKB " , " LW " ) , lty = 1 : 4 , col = " grey " , bty = " n " ) mtext ( " independent ␣ covariates " , side = 1 , line = -5 8 , outer = TRUE , font . main = 1 , cex = 1 . 5 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6.2">Correlated covariates</head><p>I then simulate data from a Normal linear model with correlated covariates.</p><p>n = 2 0 0 p = 1 0 0 beta = rep ( 1 / sqrt ( p ) , p ) sig = 1 / 2 # # correlated Normals X = matrix ( rnorm ( n * p ) , n , p ) + rnorm (n , 0 , 0 . 5 ) # # standardize the covariates X = scale ( X ) X = X * matrix ( sqrt ( n /( n -1 )) , n , p ) Y = as . vector ( X %*% beta + rnorm (n , 0 , sig ))</p><p>The second row of Figure <ref type="figure" target="#fig_95">14</ref>.2 shows the bias-variance trade-off. Overall, GCV works the best for selecting λ for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.7">Further commments on OLS, ridge, and PCA</head><p>The SVD of X is closely related to the principal component analysis (PCA), so is OLS and ridge regression. Assume that the columns of X are centered, so X t X = V D 2 V t is proportional to the sample covariance matrix of X. Assume d 1 ≥ d 2 ≥ • • • . PCA tries to find linear combinations of the covariate x i that contain maximal information. For a vector v ∈ R p , the linear combination v t x i has sample variance proportional to</p><formula xml:id="formula_416">Q(v) = v t X t Xv.</formula><p>If we multiply v by a constant c, the above sample variance will change by the factor c 2 . So a meaningful criterion is to maximize Q(v) such that ∥v∥ = 1. This is exactly the setting of Theorem A.3. The maximum value equals d 2 1 which is achieved by V 1 , the first column of V . We call</p><formula xml:id="formula_417">XV 1 =    x t 1 V 1 . . . x t n V 1   </formula><p>the first principal component of X. Similar to Theorem A.3, we can further maximize Q(v) such that ∥v∥ = 1 and v ⊥ V 1 , yielding the maximum value d 2 2 which is achieved by V 2 . We call XV 2 the second principal component of X. By induction, we can define all the p principal components, stacked in the following n × p matrix:</p><formula xml:id="formula_418">(XV 1 , . . . , XV p ) = XV = U DV t V = U D.</formula><p>So U D in the SVD decomposition contains the principal components of X. Since D is a diagonal matrix that only changes the scales of the columns of U , we also call U = (U 1 , . . . , U p ) the principal components of X. They are orthogonal since U t U = I p . Section 14.5 shows that the ridge estimator yields the predicted value</p><formula xml:id="formula_419">Ŷ (λ) = U diag d 2 j d 2 j + λ U t Y = p j=1 d 2 j d 2 j + λ ⟨U j , Y ⟩U j</formula><p>where ⟨U j , Y ⟩ = U t j Y denotes the inner product of vectors U j and Y. As a special case with λ = 0, the OLS estimator yields the predicted value</p><formula xml:id="formula_420">Ŷ = U U t Y = p j=1 ⟨U j , Y ⟩U j ,</formula><p>which is identical to the predicted value based on OLS of Y on the principal components U . Moreover, the principal components in U are orthogonal and have unit length, so the OLS fit of Y on U is equivalent to the component-wise OLS of Y on U j with coefficient ⟨U j , Y ⟩ (j = 1, . . . , p). So the predicted value based OLS equals a linear combination of the principal components with coefficients ⟨U j , Y ⟩; the predicted value based on ridge also equals a linear combination of the principal components but the coefficients are shrunk by the factors d 2 j /(d 2 j + λ). When the columns of X are not linearly independent, for example, p &gt; n, we cannot run OLS of Y on X or OLS of Y on U , but we can still run ridge. Motivated by the formulas above, another approach is to run OLS of Y on the first p * principal components Ũ = (U 1 , . . . , U p * ) with p * &lt; p. This is called the principal component regression (PCR). The predicted value is</p><formula xml:id="formula_421">Ŷ (p * ) = ( Ũ t Ũ ) -1 Ũ t Y = p * j=1 ⟨U j , Y ⟩U j ,</formula><p>which truncates the summation in the formula of Ŷ based on OLS. Compared to the predicted values of OLS and ridge, Ŷ (p * ) effectively imposes zero weights on the principal components corresponding to small singular values. It depends on a tuning parameter p * similar to λ in the ridge. Since p * must be a positive integer and λ can be any positive real value, PCR is a discrete procedure while ridge is a continuous procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.8">Homework problems</head><p>14.1 Ridge coefficient as a posterior mode under a Normal prior Assume fixed X, σ 2 and τ 2 . Show that if Remark: This result ensures that the ridge estimator must have a smaller MSE than OLS in a neighborhood of λ = 0, which is coherent with the pattern in Figure <ref type="figure" target="#fig_95">14</ref>.2.</p><formula xml:id="formula_422">Y | β ∼ N(Xβ, σ 2 I n ), β ∼ N(0, τ 2 I p ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.3">Ridge and OLS</head><p>Show that if X has linearly independent columns, then</p><formula xml:id="formula_423">βridge (λ) = (X t X + λI p ) -1 X t X β = V diag d 2 j d 2 j + λ V t β</formula><p>where β is the OLS coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.4">Ridge as OLS with augmented data</head><p>Show that βridge (λ) equals the OLS coefficient of Ỹ on X with augmented data</p><formula xml:id="formula_424">Ỹ = Y 0 p , X = X √ λI p ,</formula><p>where Ỹ is an n + p dimensional vector and X is an (n + p) × p matrix. Remark: The columns of X must be linearly independent, so the inverse of Xt X always exists. This is a theoretical result of the ridge regression. It should not be used for computation especially when p is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.5">Leave-one-out formulas for ridge</head><p>Prove Theorem 14.2.</p><p>Hint: You can use the result in Problem 14.4 and apply the leave-one-out formulas for OLS in Theorems 11.2 and 11.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6">Generalized ridge regression</head><p>Covariates have different importance, so it is reasonable to use different weights in the penalty term. Find the explicit formula for the ridge regression with general quadratic penalty:</p><p>arg min b∈R p {(Y -Xb) t (Y -Xb) + λb t Qb} where Q is a p × p positive definite matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.7">Degrees of freedom of ridge regression</head><p>For a predictor Ŷ for Y , define the degrees of freedom of the predictor as</p><formula xml:id="formula_425">n i=1 cov(y i , ŷi )/σ 2 .</formula><p>Calculate the degrees of freedom of ridge regression in terms of the eigenvalues of X t X.</p><p>14.8 Extending the simulation in Figure <ref type="figure" target="#fig_95">14</ref>.2</p><p>Re-run the simulation that generates Figure <ref type="figure" target="#fig_95">14</ref>.2, and report the λ selected by <ref type="bibr" target="#b90">Dempster et al. (1977)</ref>'s method, PRESS, and K-fold CV. Extend the simulation to the case with p &gt; n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.9">Unification of OLS, ridge, and PCR</head><p>We can unify the predicted values of the OLS, ridge, and PCR as</p><formula xml:id="formula_426">Ŷ = p j=1 s j ⟨U j , Y ⟩U j ,</formula><p>where</p><formula xml:id="formula_427">s j =        1, OLS, d 2 j d 2 j +λ , ridge, 1(j ≤ p * ), PCR.</formula><p>Based on the unified formula, show that under Assumption 4.1, we have</p><formula xml:id="formula_428">E( Ŷ ) = p j=1 s j d j γ j U j</formula><p>with the γ j 's defined in Theorem 14.1, and</p><formula xml:id="formula_429">cov( Ŷ ) = σ 2 p j=1 s 2 j U j U t j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.10">An equivalent form of ridge coefficient</head><p>Show that the ridge coefficient has two equivalent forms: for λ &gt; 0,</p><formula xml:id="formula_430">(X t X + λI p ) -1 X t Y = X t (XX t + λI n ) -1 Y.</formula><p>Remark: This formula has several interesting implications. First, the left-hand side involves inverting a p × p matrix, and it is more useful when p &lt; n; the right-hand side involves inverting an n × n matrix, so it is more useful when p &gt; n. Second, from the form on the right-hand side, we can see that the ridge estimator lies in C(X t ), the row space of X. That is, the ridge estimator can be written as X t δ, where δ</p><formula xml:id="formula_431">= (XX t + λI n ) -1 Y ∈ R p .</formula><p>This always holds but is particularly interesting in the case with p &gt; n when the row space of X is not the entire R p . Third, if p &gt; n and XX t is invertible, then we can let λ go to zero on the right-hand side, yielding βridge (0) = X t (XX t ) -1 Y which is the minimum norm estimator; see Problem 18.7. Using the definition of the pseudoinverse in Chapter A, we can further show that βridge (0) = X + Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.11">Computation of ridge with n &lt; p</head><p>When n &lt; p, X has singular value decomposition X = U DV t , where D ∈ R n×n is a diagonal matrix containing the singular values, U ∈ R n×n is an orthogonal matrix with U U t = U t U = I n , and V ∈ R p×n has orthonormal columns with V t V = I n . Show that the ridge coefficient, the predicted value, and the hat matrix have the same form as the case with n &gt; p. The only subtle difference is that the diagonal matrices have dimension n × n.</p><p>Remark: The above result also ensures that Theorem 14.1 holds when p &gt; n if we modify the summation as from j = 1 to n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.12">Recommended reading</head><p>To celebrate the 50th anniversary of <ref type="bibr" target="#b141">Hoerl and Kennard (1970)</ref>'s paper in Technometrics, the editor invited Roger W. Hoerl, the son of Art Hoerl, to review the historical aspects of the original paper, and Trevor Hastie to review the essential role of the idea of ridge regression in data science. See <ref type="bibr" target="#b142">Hoerl (2020)</ref> and <ref type="bibr" target="#b134">Hastie (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lasso</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.1">Introduction to the lasso</head><p>Ridge regression works well for prediction, but it may be difficult to interpret many small but non-zero coefficients. <ref type="bibr" target="#b217">Tibshirani (1996)</ref> proposed to use the lasso, the acronym for the Least Absolute Shrinkage and Selection Operator, to achieve the ambitious goal of simultaneously estimating parameters and selecting important variables in the linear regression. By changing the penalty term in the ridge regression, the lasso automatically estimates some parameters as zero, dropping them out of the model and thus selecting the remaining variables as important predictors. <ref type="bibr" target="#b217">Tibshirani (1996)</ref> defined the lasso as βlasso (t) = arg min The two forms of lasso are equivalent in the sense that for a given λ in (15.2), there exists a t such that the solution for (15.1) is identical to the solution for <ref type="bibr">(15.2)</ref>. In particular, t = p j=1 βlasso j (λ). Technically, the minimizer of the lasso problem may not be unique especially when p &gt; n, so the right-hand sides of the optimization problems should be a set. Fortunately, even though the minimizer may not be unique, the resulting predictor is always unique. <ref type="bibr" target="#b219">Tibshirani (2013)</ref> clarifies this issue.</p><p>Both forms of the lasso are useful. We will use the form (15.2) for computation and use the form (15.1) for geometric intuition. Similar to the ridge estimator, the lasso is not invariant to the linear transformation of X. We proceed after standardizing the covariates and outcome as Condition 14.1. For the same reason as the ridge, we can drop the intercept after standardization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.2">Comparing the lasso and ridge: a geometric perspective</head><p>The ridge and lasso are very similar: both minimize a penalized version of the residual sum of squares. They differ in the penalty term: ridge uses an L 2 penalty, i.e., the L 2 norm of the contour plot <ref type="bibr">FIGURE 15</ref>.1: Lasso with a sparse solution coefficient ∥b∥ 2 = p j=1 b 2 j , and lasso uses an L 1 penalty, i.e., the L 1 norm of the coefficient ∥b∥ 1 = p j=1 |b j |. Compared to the ridge, the lasso can give sparse solutions due to the non-smooth penalty term. That is, estimators of some coefficients are exactly zero.</p><p>Focus on the form (15.1). We can gain insights from the contour plot of the residual sum of squares as a function of b. With a well-defined OLS estimator β, Theorem 3.2 ensures</p><formula xml:id="formula_432">(Y -Xb) t (Y -Xb) = (Y -X β) t (Y -X β) + (b -β) t X t X(b -β),</formula><p>which equals a constant term plus a quadratic function centered at the OLS coefficient. Without any penalty, the minimizer is of course the OLS coefficient. With the L 1 penalty, the OLS coefficient may not be in the region defined by p j=1 |b j | ≤ t. If this happens, the intersection of the contour plot of (Y -Xb) t (Y -Xb) and the border of the restriction region p j=1 |b j | ≤ t can be at some axis. For example, Figure <ref type="figure" target="#fig_40">15</ref>.1 shows a case with p = 2, and the lasso estimator hits the x-axis, resulting in a zero coefficient for the second coordinate. However, this does not mean that lasso always generates sparse solutions because sometimes the intersection of the contour plot of (Y -Xb) t (Y -Xb) and the border of the restriction region is at an edge of the region. For example, Figure <ref type="figure" target="#fig_40">15</ref>.2 shows a case with a non-sparse lasso solution.</p><p>In contrast, the restriction region of the ridge is a circle, so the ridge solution does not hit any axis unless the original OLS coefficient is zero. Figure <ref type="figure" target="#fig_40">15</ref>.3 shows the general ridge estimator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.3">Computing the lasso via coordinate descent</head><p>Many efficient algorithms can solve the lasso problem. The glmnet package in R uses the coordinate descent algorithm based on the form (15.2) <ref type="bibr" target="#b114">(Friedman et al., 2007</ref><ref type="bibr" target="#b116">(Friedman et al., , 2010))</ref>. I will first review a lemma which is the stepstone for the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.3.1">The soft-thresholding lemma</head><p>Let sign(x) denote the sign of a real number x, which equals 1, 0, -1 if x &gt; 0, x = 0, x &lt; 0, respectively. Let (x) + = max(x, 0) denote the positive part of a real number x.</p><p>Lemma 15.1 Given b 0 and λ ≥ 0, we have </p><formula xml:id="formula_433">arg min b∈R 1 2 (b -b 0 ) 2 + λ|b| = sign(b 0 ) (|b 0 | -λ) + =      b 0 -λ, if b 0 ≥ λ, 0 if -λ ≤ b 0 ≤ λ, b 0 + λ if b 0 ≤ -λ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.3.2">Coordinate descent for the lasso</head><p>For a given λ &gt; 0, we can use the following algorithm:</p><p>1. Standardize the data as Condition 14.1. So we need to solve a lasso problem without the intercept. For simplicity of derivation, we change the scale of the residual sum of squares without essentially changing the problem: min b1,...,bp</p><formula xml:id="formula_434">1 2n n i=1 (y i -b 1 x i1 -• • • -b p x ip ) 2 + λ p j=1 |b j |.</formula><p>Initialize β.</p><p>2. Update βj given all other coefficients. Define the partial residual as r ij = y i -</p><formula xml:id="formula_435">k̸ =j βk x ik . Updating βj is equivalent to minimizing 1 2n n i=1 (r ij -b j x ij ) 2 + λ|b j |. Define βj,0 = n i=1 x ij r ij n i=1 x 2 ij = n -1 n i=1 x ij r ij -4 -2 0 2 4 -3 -2 -1 0 1 2 3 λ = 2 b 0 S(b 0 , λ) FIGURE 15.4: Soft-thresholding</formula><p>as the OLS coefficient of the r ij 's on the x ij 's, so</p><formula xml:id="formula_436">1 2n n i=1 (r ij -b j x ij ) 2 = 1 2n n i=1 (r ij -βj,0 x ij ) 2 + 1 2n n i=1 x 2 ij (b j -βj,0 ) 2 = constant + 1 2 (b j -βj,0 ) 2 .</formula><p>Then updating βj is equivalent to minimizing 1 2 (b j -βj,0 ) 2 + λ|b j |. Lemma 15.1 implies βj = S( βj,0 , λ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Iterate until convergence.</head><p>Does the algorithm always converge? The theory of <ref type="bibr" target="#b225">Tseng (2001)</ref> ensures it converges, but this is beyond the scope of this book. We can start with a large λ and all zero coefficients. We then gradually decrease λ, and for each λ, we apply the above algorithm. We finally select λ via K-fold cross-validation. Since we gradually decrease λ, the initial values from the last step are very close to the minimizer and the algorithm converges fairly fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.4">Example: comparing OLS, ridge and lasso</head><p>In the Boston housing data, the OLS, ridge, and lasso have similar performance in out-ofsample prediction. Lasso and ridge have similar coefficients. See Figure <ref type="figure" target="#fig_40">15</ref>.5(a). But if we artificially add 200 columns of covariates of pure noise N(0, 1), then the ridge and lasso perform much better. Lasso can automatically shrink many coefficients to zero. See <ref type="bibr">Figure 15.5(b)</ref>. q q q q q q q q q q q q q q q q q q q q q q q q q ridge lasso 5 10 5 10 -15</p><p>-10 -5 0 indices of covariates coefficients (a) original data q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qqq q qq q q q q q q qq q q qqqqqqqqqq qq qq q q q qqq q qqq q qq q q qq q qq q q q qq q qq q q qqqq q qq q q q qqq q qq q qqq q q q q q q q q q qqq q qq q qq q q qq q qqq q q q q q qq q q qqqqqqq q qq q qqqqqq q qq q q q qqqq q q q qqqqqq q q q qqqqqqqqqqqqqqqq q q q qqqqqqq q q q q q q q qq q qqq q qq q q qq ridge lasso   Figure <ref type="figure" target="#fig_40">15</ref>.7 compares the constraints corresponding to the ridge, lasso, and elastic net.</p><formula xml:id="formula_437">A A B B E E J J L L (a) 0 &lt; q &lt; 1 A A B B E E J J L L (b) q = 1 A A B B E E J J L L (c) q = 2</formula><p>Because the constraint of the elastic net is not smooth, it encourages sparse solutions in the same way as the lasso. Due to the ridge penalty, the elastic net can deal with the collinearity of the covariates better than the lasso. <ref type="bibr" target="#b114">Friedman et al. (2007)</ref> proposed to use the coordinate descent algorithm to solve for the elastic net estimator, and <ref type="bibr" target="#b115">Friedman et al. (2009)</ref> implemented it in an R package called Show that if β(1) and β(2) are two solutions, then α β(1) + (1 -α) β( <ref type="formula" target="#formula_9">2</ref>) is also a solution for any 0 ≤ α ≤ 1. Show that X β(1) = X β(2) must hold. Hint: The function ∥ • ∥ 2 is strongly convex. That is, for any v 1 , v 2 and 0 &lt; α &lt; 1, we have</p><formula xml:id="formula_438">∥αv 1 + (1 -α)v 2 ∥ 2 ≤ α∥v 1 ∥ 2 + (1 -α)∥v 2 ∥ 2</formula><p>and the inequality holds when v 1 ̸ = v 2 . The function ∥ • ∥ is convex. That is, for any v 1 , v 2 and 0 &lt; α &lt; 1, we have</p><formula xml:id="formula_439">∥αv 1 + (1 -α)v 2 ∥ 1 ≤ α∥v 1 ∥ 1 + (1 -α)∥v 2 ∥ 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.2">The soft-thresholding lemma</head><p>Prove Lemma 15.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.3">Penalized OLS with an orthogonal design matrix</head><p>Consider the special case with standardized and orthogonal design matrix:</p><formula xml:id="formula_440">X t 1 n = 0, X t X = I p .</formula><p>For a fixed λ ≥ 0, find the explicit formulas of the jth coordinates of the following estimators in terms of the corresponding jth coordinate of the OLS estimator βj and λ (j = 1, . . . , p):</p><formula xml:id="formula_441">βridge (λ) = arg min b∈R p ∥Y -Xb∥ 2 + λ∥b∥ 2 , βlasso (λ) = arg min b∈R p ∥Y -Xb∥ 2 + λ∥b∥ 1 , βenet (λ) = arg min b∈R p ∥Y -Xb∥ 2 + λ(α∥b∥ 2 + (1 -α)∥b∥ 1 ) , βsubset (λ) = arg min b∈R p ∥Y -Xb∥ 2 + λ∥b∥ 0 ,</formula><p>where</p><formula xml:id="formula_442">∥b∥ 2 = p j=1 b 2 j , ∥b∥ 1 = p j=1 |b j |, ∥b∥ 0 = p j=1</formula><p>1(b j ̸ = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.4">Standardization in the elastic net</head><p>For fixed λ and α, show that the intercept in βenet (λ, α) equals zero under the standardization in Condition 14.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.5">Coordinate descent for the elastic net</head><p>Give the detailed coordinate descent algorithm for the elastic net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.6">Reducing elastic net to lasso</head><p>Consider the following form of the elastic net:</p><formula xml:id="formula_443">arg min b∈R p ∥Y -Xb∥ 2 + λ{α∥b∥ 2 + (1 -α)∥b∥ 1 }.</formula><p>Show that it reduces to the following lasso:</p><formula xml:id="formula_444">arg min b∈R p ∥ Ỹ -Xb∥ 2 + λ∥b∥ 1 , where Ỹ = Y 0 p , X = X √ λαI p , λ = λ(1 -α).</formula><p>Hint: Use the result in Problem 14.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.7">Reducing lasso to iterative ridge</head><p>Based on the simple result min</p><formula xml:id="formula_445">ac=b (a 2 + c 2 )/2 = |b|,</formula><p>for scalars a, b, c, Hoff (2017) rewrote the lasso problem</p><formula xml:id="formula_446">min b∈R p {∥Y -Xb∥ 2 + λ∥b∥ 1 } as min u,v∈R p {∥Y -X(u • v)∥ 2 + λ(∥u∥ 2 + ∥v∥ 2 )/2}</formula><p>where • denotes the component-wise product of vectors. Hoff (2017, Lemma 1) showed that a local minimum of the new problem must be a local minimum of the lasso problem.</p><p>Show that the new problem can be solved based on the following iterative ridge regressions:</p><p>Part VI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformation and Weighting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformations in OLS</head><p>Transforming the outcome and covariates is fundamental in linear models. Whenever we specify a linear model y i = x t i β + ε i , we implicitly have transformed the original y and x, or at least we have chosen the scales of them. <ref type="bibr" target="#b71">Carroll and Ruppert (1988)</ref> is a textbook on this topic. This chapter discusses some important special cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.1">Transformation of the outcome</head><p>Although we can view y i = x t i β + ε i , (i = 1, . . . , n) as a linear projection that works for any type of outcome y i ∈ R, the linear model works the best for continuous outcomes and especially for Normally distributed outcomes. Sometimes, the linear model can be a poor approximation of the original outcome but may perform well for certain transformations of the outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.1.1">Log transformation</head><p>With positive, especially heavy-tailed outcomes, a standard transformation is the log transformation. So we fit a linear model</p><formula xml:id="formula_447">log y i = x t i β + ε i , (i = 1, . . . , n).</formula><p>The interpretation of the coefficients changes a little bit. Because</p><formula xml:id="formula_448">∂ logŷ i ∂x ij = ∂ ŷi ŷi ∂x ij = βj ,</formula><p>we can interpret βj in the following way: ceteris paribus, if x ij increases by one unit, then the proportional increase in the average outcome is βj . In economics, βj is the semi-elasticity of y on x j in the model with log transformation on the outcome. Sometimes, we may apply the log transformation on both the outcome and a certain covariate:</p><p>log</p><formula xml:id="formula_449">y i = β 1 x i1 + • • • + β j log x ij + • • • + ε i , (i = 1, . . . , n).</formula><p>The jth fitted coefficient becomes</p><formula xml:id="formula_450">∂ logŷ i ∂ log x ij = ∂ ŷi ŷi ∂x ij x ij = βj ,</formula><p>so ceteris paribus, if x ij increases by 1%, then the average outcome will increase by βj %.</p><p>In economics, βj is the x j -elasticity of y in the model with log transformation on both the outcome and x j . The log transformation only works for positive variables. For a nonnegative outcome, we can modify the log transformation to log(y i + 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.1.2">Box-Cox transformation</head><p>Power transformation is another important class. The Box-Cox transformation unifies the log transformation and the power transformation:</p><formula xml:id="formula_451">g λ (y) = y λ -1 λ , λ ̸ = 0, log y, λ = 0. L'Hôpital's rule implies that lim λ→0 y λ -1 λ = lim λ→0 dy λ /dλ 1 = lim λ→0 y λ log y = log y,</formula><p>so as a function of λ, g λ (y) is continuous at λ = 0. The log transformation is a limiting version of the power transformation. Can we choose λ based on data? <ref type="bibr" target="#b64">Box and Cox (1964)</ref> proposed a strategy based on the maximum likelihood under the Normal linear model:</p><formula xml:id="formula_452">Y λ =    y λ1 . . . y λn    =    g λ (y 1 ) . . . g λ (y 1 )    ∼ N(Xβ, σ 2 I n ).</formula><p>The density function of Y λ is</p><formula xml:id="formula_453">f (Y λ ) = (2πσ 2 ) -n/2 exp - 1 2σ 2 (Y λ -Xβ) t (Y λ -Xβ) .</formula><p>The Jacobian of the transformation from</p><formula xml:id="formula_454">Y to Y λ is det ∂Y λ ∂Y = det      y λ-1 1 y λ-1 2 . . . y λ-1 n      = n i=1 y λ-1 i , so the density function of Y is f (Y ) = (2πσ 2 ) -n/2 exp - 1 2σ 2 (Y λ -Xβ) t (Y λ -Xβ) n i=1 y λ-1 i .</formula><p>If we treat the density function of Y as a function of (β, σ 2 , λ), then it is the likelihood function, defined as L(β, σ 2 , λ). Given (σ 2 , λ), maximizing the likelihood function is equivalent to minimizing (Y λ -Xβ) t (Y λ -Xβ), i.e., we can run OLS of Y λ on X to obtain</p><formula xml:id="formula_455">β(λ) = (X t X) -1 X t Y λ .</formula><p>Given λ, maximizing the likelihood function is equivalent to first obtaining β(λ) and then obtaining σ2 (λ</p><formula xml:id="formula_456">) = n -1 Y λ (I n -H)Y λ .</formula><p>The final step is to maximize the profile likelihood as a function of λ:</p><formula xml:id="formula_457">L( β(λ), σ2 (λ), λ) = 2πσ 2 (λ) -n/2 exp - nσ 2 (λ) 2σ 2 (λ) n i=1 y λ-1 i .</formula><p>Dropping some constants, the log profile likelihood function of λ is</p><formula xml:id="formula_458">l p (λ) = - n 2 log σ2 (λ) + (λ -1) n i=1 log y i .</formula><p>The boxcox function in the R package MASS plots l p (λ), finds it maximizer λ, and construct a 95% confidence interval [ λl , λU ] based on the following asymptotic pivotal quantity</p><formula xml:id="formula_459">2 l p ( λ) -l p (λ) a ∼ χ 2 1 ,</formula><p>which holds by Wilks' Theorem. In practice, we often use the λ values within [ λl , λU ] that have more scientific meanings. I use two datasets to illustrate the Box-Cox transformation, with the R code in code16.1.2.R. For the jobs data, λ = 2 seems a plausible value. library ( MASS ) library ( mediation ) par ( mfrow = c ( 1 , 3 )) jobslm = lm ( job _ seek ~treat + econ _ hard + depress 1 + sex + age + occp + marital + nonwhite + educ + income , data = jobs ) boxcox ( jobslm , lambda = seq ( 1 . 5 , 3 , 0 . 1 ) , plotit = TRUE ) jobslm 2 = lm ( I ( job _ seek ^2 ) ~treat + econ _ hard + depress 1 + sex + age + occp + marital + nonwhite + educ + income , data = jobs ) hist ( jobslm $ residuals , xlab = " residual " , ylab = " " , main = " job _ seek " , font . main = 1 ) hist ( jobslm 2 $ residuals , , xlab = " residual " , ylab = " " , main = " job _ seek ^2 " , font . main = 1 ) Linear approximations may not be adequate, so we can consider a polynomial specification. With one-dimensional x, we can use</p><formula xml:id="formula_460">y i = β 1 + β 2 x i + β 3 x 2 i • • • + β p x p-1 i + ε i .</formula><p>In economics, it is almost the default choice to include the quadratic term of working experience in the log wage equation. I give an example below using the data from <ref type="bibr" target="#b49">Angrist et al. (2006)</ref>. The quadratic term of exper is significant.</p><p>&gt; library ( foreign ) &gt; census 0 0 = read . dta ( " census 0 0 . dta " ) &gt; head ( census 0 0 ) age educ logwk perwt exper exper 2 black 1 4 8 1 2 6 . 6 7 0 5 7 6 1 . 0 8 5 0 0 2 1 3 0 9 0 0 0 2 4 2 1 3 6 . 7 8 3 9 0 5 0 . 9 6 6 6 3 8 3 2 3 5 2 9 0 3 4 9 1 3 6 . 7 6 2 3 8 3 1 . 2 1 3 2 2 9 7 3 0 9 0 0 0 4 4 4 1 3 6 . 3 0 2 8 5 1 0 . 4 8 3 3 1 9 1 2 5 6 2 5 0 5 4 5 1 6 6 . 0 4 3 3 8 6 0 . 9 6 6 6 3 8 3 2 3 5 2 9 0 6 4 3 1 3 5 . 0 6 1 1 3 8 1 . 0 8 5 0 0 2 1 2 4 5 7 6 0 &gt; &gt; census 0 0 ols 1 = lm ( logwk ~educ + exper + black , + data = census 0 0 ) &gt; census 0 0 ols 2 = lm ( logwk ~educ + exper + I ( exper ^2 ) + black , + data = census 0 0 ) &gt; round ( summary ( census 0 0 ols 1 )$ coef , 4 ) Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 4 . 8 9 1 8 0 . 0 3 1 5 1 5 5 . 0 5 4 0 0 . 0 0 0 0 educ 0 . 1 1 5 2 0 . 0 0 1 2 9 9 . 1 4 7 2 0 . 0 0 0 0 exper 0 . 0 0 0 2 0 . 0 0 0 8 0 . 2 2 9 4 0 . 8 1 8 5 black -0 . 2 4 6 6 0 . 0 0 8 5 -2 9 . 1 6 7 4 0 . 0 0 0 0 &gt; round ( summary ( census 0 0 ols 2 )$ coef , 4 ) Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 5 . 0 7 7 7 0 . 0 8 8 7 5 7 . 2 2 5 4 0 . 0 0 0 0 educ 0 . 1 1 4 8 0 . 0 0 1 2 9 7 . 6 5 0 6 0 . 0 0 0 0 exper -0 . 0 1 4 8 0 . 0 0 6 7 -2 . 2 0 1 3 0 . 0 2 7 7 I ( exper ^2 ) 0 . 0 0 0 3 0 . 0 0 0 1 2 . 2 4 2 5 0 . 0 2 4 9 black -0 . 2 4 6 7 0 . 0 0 8 5 -2 9 . 1 7 3 2 0 . 0 0 0 0</p><p>We can also include polynomial terms of more than one covariate, for example,</p><formula xml:id="formula_461">(1, x 1i , . . . , x d i1 , x i2 , . . . , x l i2 ) or (1, x 1i , . . . , x d i1 , x i2 , . . . , x l i2 , x i1 x i2 , . . . , x d i1 x l i2</formula><p>). We can also approximate the conditional mean function of the outcome by a linear combination of some basis functions:</p><formula xml:id="formula_462">y i = f (x i ) + ε i ∼ = J j=1 β j S j (x i ) + ε i ,</formula><p>where the S j (x i )'s are basis functions. The gam function in the mgcv package uses this strategy including the automatic procedure of choosing the number of basis functions J. The following example has a sine function as the truth, and the basis expansion approximation yields reasonable performance with sample size n = 1000. Figure <ref type="figure" target="#fig_95">16</ref>.3 plots both the true and estimated curves. library ( mgcv ) n = 1 0 0 0 dat = data . frame ( x &lt;-seq ( 0 , 1 , length . out = n ) , true &lt;-sin ( x * 1 0 ) , y &lt;-true + rnorm ( n )) np . fit = gam ( y ~s ( x ) , data = dat ) plot ( y ~x , data = dat , bty = " n " , pch = 1 9 , cex = 0 . 1 , col = " grey " ) lines ( true ~x , col = " grey " ) lines ( np . fit $ fitted . values ~x , lty = 2 ) legend ( " bottomright " , c ( " true " , " estimated " ) , lty = 1 : 2 , col = c ( " grey " , " black " ) , bty = " n " )</p><p>The generalized additive model is an extension of the multivariate case:</p><formula xml:id="formula_463">y i = f 1 (x i1 ) + • • • + f p (x ip ) + ε i ∼ = J1 j=1 β 1j S j (x i1 ) + • • • + Jp j=1 β pj S j (x ip ) + ε i .</formula><p>The gam function in the mgcv package implements this strategy. Again I use the dataset from <ref type="bibr" target="#b49">Angrist et al. (2006)</ref> to illustrate the procedure with nonlinearity in educ and exper shown in Figure <ref type="figure" target="#fig_95">16</ref>.4. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0. The R code in this section is in code16.2.1.R. See <ref type="bibr" target="#b239">Wood (2017)</ref> for more details about the generalized additive model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.2.2">Regression discontinuity and regression kink</head><p>The left panel of Figure <ref type="figure" target="#fig_95">16</ref>.5 shows an example of regression discontinuity, where the linear functions before and after a cutoff point can differ with a possible jump. A simple way to capture the two regimes of linear regression is to fit the following model:</p><formula xml:id="formula_464">y i = β 1 + β 2 x i + β 3 1 (x i &gt; c) + β 4 x i 1 (x i &gt; c) + ε i . So y i = β 1 + β 2 x i + ε i x i ≤ c, (β 1 + β 3 ) + (β 2 + β 4 ) x i + ε i , x i &gt; c.</formula><p>Testing the discontinuity at c is equivalent to testing</p><formula xml:id="formula_465">(β 1 + β 3 ) + (β 2 + β 4 ) c = β 1 + β 2 c ⇐⇒ β 3 + β 4 c = 0.</formula><p>If we center the covariates at c, then </p><formula xml:id="formula_466">y i = β 1 + β 2 (x i -c) + β 3 1 (x i &gt; c) + β 4 (x i -c)1 (x i &gt; c) + ε i</formula><formula xml:id="formula_467">y i = β 1 + β 2 (x i -c) + ε i x i ≤ c, (β 1 + β 3 ) + (β 2 + β 4 ) (x i -c) + ε i , x i &gt; c.</formula><p>So testing the discontinuity at c is equivalent to testing β 3 = 0.</p><p>The right panel of Figure <ref type="figure" target="#fig_95">16</ref>.5 shows an example of regression kink, where the linear functions before and after a cutoff point can differ but the whole regression line is continuous. A simple way to capture the two regimes of linear regression is to fit the following model:</p><formula xml:id="formula_468">y i = β 1 + β 2 R c (x i ) + β 3 (x i -c) + ε i using R c (x) = max(0, x -c) = 0, x ≤ c, x -c, x &gt; c.</formula><p>So</p><formula xml:id="formula_469">y i = β 1 + β 3 (x i -c) + ε i , x i ≤ c, β 1 + (β 2 + β 3 ) (x i -c) + ε i , x i &gt; c.</formula><p>This ensures that the mean function is continuous at c with both left and right limits equaling β 1 . Testing the kink is equivalent to testing β 2 = 0.</p><p>These regressions have many applications in economics, but I omit the economic background. Readers can find more discussions in <ref type="bibr" target="#b50">Angrist and Pischke (2008)</ref> and <ref type="bibr" target="#b69">Card et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction</head><p>Interaction is an important notion in applied statistics. It measures the interplay of two or more variables acting simultaneously on an outcome. Epidemiologists find that cigarette smoking and alcohol consumption both increase the risks of many cancers. Then they want to measure how cigarette smoking and alcohol consumption jointly increase the risks. That is, does cigarette smoking increase the risks of cancers more in the presence of alcohol consumption than in the absence of it? Political scientists are interested in measuring the interplay of different get-out-to-vote interventions on voting behavior. This chapter will review many aspects of interaction in the context of linear regression. <ref type="bibr" target="#b83">Cox (1984)</ref> and Berrington de González and <ref type="bibr" target="#b84">Cox (2007)</ref> reviewed interaction from a statistical perspective. VanderWeele (2015) offers a textbook discussion on interaction with a focus on applications in epidemiology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.1">Two binary covariates interact</head><p>Let's start with the simplest yet nontrivial example with two binary covariates x i1 , x i2 ∈ {0, 1}. We can fit the OLS:</p><formula xml:id="formula_470">y i = β0 + β1 x i1 + β2 x i2 + β12 x i1 x i2 + εi . (17.1)</formula><p>We can express the coefficients in terms of the means of the outcomes within four combinations of the covariates. The following proposition is an algebraic result.</p><p>Proposition 17.1 From (17.1), we have</p><formula xml:id="formula_471">β0 = ȳ00 , β1 = ȳ10 -ȳ00 , β2 = ȳ01 -ȳ00 , β12 = (ȳ 11 -ȳ10 ) -(ȳ 01 -ȳ00 ),</formula><p>where ȳf1f2 is the average value of the y i 's with x i1 = f 1 and</p><formula xml:id="formula_472">x i2 = f 2 .</formula><p>The proof of Proposition 17.1 is pure algebraic which is relegated to Problem 17.1. The proposition generalizes to OLS with more than two binary covariates. See <ref type="bibr" target="#b250">Zhao and Ding (2022)</ref> for more details.</p><p>Practitioners also interpret the coefficient of the product term of two continuous variables as an interaction. The coefficient β12 equals the difference between ȳ11 -ȳ10 , the effect of x i2 on y i holding x i1 at level 1, and ȳ01 -ȳ00 , the effect of x i2 on y i holding x i1 at level 0. It also equals β12 = (ȳ 11 -ȳ01 ) -(ȳ 10 -ȳ00 ), that is, the difference between ȳ11 -ȳ01 , the effect of x i1 on y i holding x i2 at level 1, and ȳ10 -ȳ00 , the effect of x i1 on y i holding x i2 at level 0. The formula shows the symmetry of x i1 and x i2 in defining interaction. 17.2 A binary covariate interacts with a general covariate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.1">Treatment effect heterogeneity</head><p>In many studies, we are interested in the effect of a binary treatment z i on an outcome y i , adjusting for some background covariates x i . The covariates can play many roles in this problem. They may affect the treatment, enter the outcome model, and modify the effect of the treatment on the outcome. We can formulate the problem in terms of linear regression:</p><formula xml:id="formula_473">y i = β 0 + β 1 z i + β t 2 x i + β t 3 x i z i + ε i ,<label>(17.2)</label></formula><p>where</p><formula xml:id="formula_474">E(ε i | z i , x i ) = 0. So E(y i | z i = 1, x i ) = β 0 + β 1 + (β 2 + β 3 ) t x i and E(y i | z i = 0, x i ) = β 0 + β t 2 x i , which implies that E(y i | z i = 1, x i ) -E(y i | z i = 0, x i ) = β 1 + β t 3 x i .</formula><p>The conditional average treatment effect is thus a linear function of the covariates. As long as β 3 ̸ = 0, we have treatment effect heterogeneity, which is also called effect modification.</p><p>A statistical test for β 3 = 0 is straightforward based on OLS and EHW standard error. Note that (17.2) includes the interaction of the treatment and all covariates. With prior knowledge, we may believe that the treatment effect varies with respect to a subset of covariates, or, equivalently, we may set some components of β 3 to be zero. <ref type="bibr" target="#b150">Johnson and Neyman (1936)</ref> proposed a technique to identify the region of covariates in which the conditional average treatment β 1 + β t 3 x is zero. For a given x, we can test the null hypothesis that β 1 + β t 3 x = 0, which is a linear combination of the regression coefficients of (17.2). If we fail to reject the null hypothesis, then this x belongs to the region of zero conditional average effect. See <ref type="bibr" target="#b200">Rogosa (1981)</ref> for more discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.2">Johnson-Neyman technique</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.3">Blinder-Oaxaca decomposition</head><p>The linear regression (17.2) also applies to descriptive statistics when z i is a binary indicator for subgroups. For example, z i can be a binary indicator for age, racial, or gender groups, y i can be the log wage, and x i can be a vector of explanatory variables such as education, experience, industry, and occupation. Sometimes, it is more insightful to write (17.2) in terms of two possibly non-parallel linear regressions:</p><formula xml:id="formula_475">y i = γ 0 + θ t 0 x i + ε i , E(ε i | z i = 0, x i ) = 0 (17.3)</formula><p>for the group with z i = 0, and</p><formula xml:id="formula_476">y i = γ 1 + θ t 1 x i + ε i , E(ε i | z i = 1, x i ) = 0 (17.4)</formula><p>for the group with z i = 1. Regressions (17.3) and (17.4) are just a reparametrization of (17.2) with</p><formula xml:id="formula_477">γ 0 = β 0 , θ 0 = β 2 , γ 1 = β 0 + β 1 , θ 1 = β 2 + β 3 .</formula><p>Based on (17.3) and (17.4), we can decompose the difference in the outcome means as</p><formula xml:id="formula_478">E(y i | z i = 1) -E(y i | z i = 0) = {γ 1 + θ t 1 E(x i | z i = 1)} -{γ 0 + θ t 0 E(x i | z i = 0)} = θ t 0 {E(x i | z i = 1) -E(x i | z i = 0)} +(θ 1 -θ 0 ) t E(x i | z i = 0) + γ 1 -γ 0 +(θ 1 -θ 0 ) t {E(x i | z i = 1) -E(x i | z i = 0)}.</formula><p>The decomposition has three components: the first component</p><formula xml:id="formula_479">E = θ t 0 {E(x i | z i = 1) -E(x i | z i = 0)} = β t 2 {E(x i | z i = 1) -E(x i | z i = 0</formula><p>)} measures the endowment effect since it is due to the difference in the covariates; the second component</p><formula xml:id="formula_480">C = (θ 1 -θ 0 ) t E(x i | z i = 0) + γ 1 -γ 0 = β t 3 E(x i | z i = 0) + β 1 measures the difference in coefficients; the third component I = (θ 1 -θ 0 ) t {E(x i | z i = 1) -E(x i | z i = 0)} = β t 3 {E(x i | z i = 1) -E(x i | z i = 0</formula><p>)} measures the interaction between the endowment and coefficients. This is called the Blinder-Oaxaca decomposition. <ref type="bibr" target="#b149">Jann (2008)</ref> reviews other forms of the decomposition, extending the original forms in <ref type="bibr" target="#b62">Blinder (1973)</ref> and <ref type="bibr" target="#b185">Oaxaca (1973)</ref>.</p><p>Estimation and testing for E, C, and I are straightforward. Based on the OLS of (17.2) and the sample means x1 and x0 of the covariates, we have point estimators</p><formula xml:id="formula_481">Ê = βt 2 (x 1 -x0 ), Ĉ = βt 3 x0 + β1 , Î = βt 3 (x 1 -x0 ).</formula><p>Given the covariates, they are just linear transformations of the OLS coefficients. Statistical inference is thus straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.4">Chow test</head><p>Chow (1960) proposed to test whether the two regressions (17.3) and (17.4) are identical. Under the null hypothesis that γ 0 = γ 1 and θ 0 = θ 1 , he proposed an F test assuming homoskedasticity, which is called the Chow test in econometrics. In fact, this is just a special case of the standard F test for the null hypothesis that β 1 = 0 and β 3 = 0 in (17.2). Moreover, based on the OLS in (17.2), we can also derive the robust test based on the EHW covariance estimator. <ref type="bibr" target="#b74">Chow (1960)</ref> discussed a subtle case in which one group has a small size rending the OLS fit underdetermined. I relegate the details to Problem 17.3. Note that under this null hypothesis, C = I = 0, so the difference in the outcome means is purely due to the difference in the covariate means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3">Difficulties of intereaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3.1">Removable interaction</head><p>The significance of the interaction term differs with y and log(y).</p><p>&gt; n = 1 0 0 0 &gt; x 1 = rnorm ( n ) &gt; x 2 = rnorm ( n ) &gt; y = exp ( x 1 + x 2 + rnorm ( n )) &gt; ols . fit = lm ( log ( y ) ~x 1 * x 2 ) &gt; summary ( ols . fit ) Call : lm ( formula = log ( y ) ~x 1 * x 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residuals :</head><p>Min 1 Q Median 3 Q Max -3 . 7 3 7 3 -0 . 6 8 2 2 -0 . 0 1 1 1 0 . 7 0 8 4 3 . 1 0 3 9 Coefficients :</p><p>Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 0 . 0 0 3 2 1 4 0 . 0 3 1 2 8 6 0 . 1 0 3 0 . 9 1 8 x 1 1 . 0 5 6 8 0 1 0 . 0 3 0 6 4 9 3 4 . 4 8 0 &lt;2e -1 6 *** x 2 1 . 0 0 9 4 0 4 0 . 0 3 0 7 7 8 3 2 . 7 9 7 &lt;2e -1 6 *** x 1 : x 2 -0 . 0 1 7 5 2 8 0 . 0 3 0 5 2 6 -0 . 5 7 4 0 . 5 6 6 &gt; ols . fit = lm ( y ~x 1 * x 2 ) &gt; summary ( ols . fit )</p><p>Call : lm ( formula = y ~x 1 * x 2 )</p><p>Residuals : Min 1 Q Median 3 Q Max -3 5 . 9 5 -5 . 1 7 -0 . 9 7 2 . 3 4 5 1 3 . 3 5</p><p>Coefficients : Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 5 . 2 8 4 2 0 . 6 6 8 6 7 . 9 0 3 7 . 1 7e -1 5 *** x 1 6 . 7 5 6 5 0 . 6 5 5 0 1 0 . 3 1 5 &lt; 2e -1 6 *** x 2 4 . 9 5 4 8 0 . 6 5 7 7 7 . 5 3 3 1 . 1 1e -1 3 *** x 1 : x 2 7 . 3 8 1 0 0 . 6 5 2 4 1 1 . 3 1 4 &lt; 2e -1 6 ***</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3.2">Main effect in the presence of interaction</head><p>In the OLS fit below, we observe significant main effects.</p><p>&gt; # # data from " <ref type="url" target="https://stats">https :// stats</ref> . idre . ucla . edu / stat / data / hsbdemo . dta " &gt; hsbdemo = read . table ( " hsbdemo . txt " ) &gt; ols . fit = lm ( read ~math + socst , data = hsbdemo ) &gt; summary ( ols . fit ) Call : lm ( formula = read ~math + socst , data = hsbdemo ) Residuals : Min 1 Q Median 3 Q Max -1 8 . 8 7 2 9 -4 . 8 9 8 7 -0 . 6 2 8 6 5 . 2 3 8 0 2 3 . 6 9 9 3 Coefficients : Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 7 . 1 4 6 5 4</p><p>3 . 0 4 0 6 6 2 . 3 5 0 0 . 0 1 9 7 * math 0 . 5 0 3 8 4 0 . 0 6 3 3 7 7 . 9 5 1 1 . 4 1e -1 3 *** socst 0 . 3 5 4 1 4 0 . 0 5 5 3 0 6 . 4 0 4 1 . 0 8e -0 9 ***</p><p>Then we add the interaction term into the OLS, and suddenly we have significant interaction but not significant main effects. Min 1 Q Median 3 Q Max -1 8 . 6 0 7 1 -4 . 9 2 2 8 -0 . 7 1 9 5 4 . 5 9 1 2 2 1 . 8 5 9 2</p><p>Coefficients : Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 3 7 . 8 4 2 7 1 5 1 4 . 5 4 5 2 1 0 2 . 6 0 2 0 . 0 0 9 9 8 ** math -0 . 1 1 0 5 1 2 0 . 2 9 1 6 3 4 -0 . 3 7 9 0 . 7 0 5 1 4 socst -0 . 2 2 0 0 4 4 0 . 2 7 1 7 5 4 -0 . 8 1 0 0 . 4 1 9 0 8 math : socst 0 . 0 1 1 2 8 1 0 . 0 0 5 2 2 9 2 . 1 5 7 0 . 0 3 2 2 1 * However, if we center the covariates, the main effects are significant again. Residuals : Min 1 Q Median 3 Q Max -1 8 . 6 0 7 1 -4 . 9 2 2 8 -0 . 7 1 9 5 4 . 5 9 1 2 2 1 . 8 5 9 2</p><p>Coefficients : Estimate Std . Error t value Pr ( &gt;| t |) ( Intercept ) 5 1 . 6 1 5 3 2 7 0 . 5 6 8 6 8 5 9 0 . 7 6 3 &lt; 2e -1 6 *** math . c 0 . 4 8 0 6 5 4 0 . 0 6 3 7 0 1 7 . 5 4 5 1 . 6 5e -1 2 *** socst . c 0 . 3 7 3 8 2 9 0 . 0 5 5 5 4 6 6 . 7 3 0 1 . 8 2e -1 0 *** math . c : socst . c 0 . 0 1 1 2 8 1 0 . 0 0 5 2 2 9 2 . 1 5 7 0 . 0 3 2 2 *</p><p>Based on the linear model with interaction</p><formula xml:id="formula_482">E(y i | x i1 , x i2 ) = β 0 + β 1 x i1 + β 2 x i2 + β 12 x i1 x i2 ,</formula><p>better definitions of the main effects are</p><formula xml:id="formula_483">n -1 n i=1 ∂E(y i | x i1 , x i2 ) ∂x i1 = n -1 n i=1 (β 1 + β 12 x i2 ) = β 1 + β 12 x2 and n -1 n i=1 ∂E(y i | x i1 , x i2 ) ∂x i2 = n -1 n i=1 (β 2 + β 12 x i1 ) = β 2 + β 12 x1 ,</formula><p>which are called the average partial or marginal effects. So when the covariates are centered, we can interpret β 1 and β 2 as the main effects. In contrast, the interpretation of the interaction term does not depend on the centering of the covariates because</p><formula xml:id="formula_484">∂ 2 E(y i | x i1 , x i2 ) ∂x i1 ∂x i2 = β 12 .</formula><p>The R code in this section is in code17.3.R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3.3">Power</head><p>Usually, statistical tests for interaction do not have enough power. Proposition 17.1 provides a simple explanation.</p><p>The variance of the interaction equals var( β12 ) = σ 2 11 n 11 + σ 2 10 n 10 + σ 2 01 n 01 + σ 2 00 n 00 , where σ 2 f1f2 = var(y i</p><formula xml:id="formula_485">| x i1 = f 1 , x i2 = f 2 )</formula><p>. Therefore, its variance is driven by the smallest value of n 11 , n 10 , n 01 , n 00 . Even when the total sample size is large, one of the subgroup sample sizes can be small, resulting in a large variance of the estimator of the interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.4">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.1">Interaction and difference-in-difference</head><p>Prove Proposition 17.1. Moreover, simplify the HC0 and HC2 versions of the EHW standard errors of the coefficients in terms of n f1f2 and σ2 f1f2 , where n f1f2 is the sample size and σ2 f1f2 is the sample variance of the outcomes for units with x i1 = f 1 and x i2 = f 2 . Hint: You can prove the proposition by inverting the 4 × 4 matrix X t X. However, this method is a little too tedious. Moreover, this proof does not generalize to OLS with K &gt; 2 binary covariates. So it is better to find alternative proofs. For the EHW standard errors, you can use the result in Problems 6.3 and 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2">Two OLS</head><p>Given data (x i , z i , y i ) n i=1 where x i denotes the covariates, z i denotes the binary group indicator, and y i denotes the outcome. We can fit two separate OLS: ŷi = γ1 + x t i β1 and ŷi = γ0 + x t i β0 with data in group 1 and group 0, respectively. We can also fit a joint OLS using the pooled data: ŷi = α0 + αz z i + x t i αx + z i x t i αzx .</p><p>1. Find ( α0 , αz , αx , αzx ) in terms of (γ 1 , β1 , γ0 , β0 ).</p><p>2. Show that the fitted values ŷi 's are the same from the separate and the pooled OLS for all units i = 1, . . . , n.</p><p>3. Show that the leverage scores h ii 's are the same from the separate and the pooled OLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3">Chow test when one group size is too small</head><p>Assume (17.3) and (17.4) with homoskedastic Normal error terms. Let n 1 and n 0 denote the sample sizes of groups with z i = 1 and z i = 0. Consider the case with n 0 larger than the number of covariates but n 1 smaller than the number of covariates. So we can fit OLS and estimate the variance based on (17.3), but we cannot do so based on (17.4). The statistical test discussed in the main paper does not apply. <ref type="bibr" target="#b74">Chow (1960)</ref> proposed the following test based on prediction. Let γ0 and θ0 be the coefficients, and σ 2 0 be the variance estimate based on OLS with units z i = 0. Under the null hypothesis that γ 0 = γ 1 and θ 0 = θ 1 , predict the outcomes of the units z i = 1: ŷi = γ0 + θt 0 x i with the prediction error d i = y i -ŷi following a multivariate Normal distribution. Propose an F test based on d i with z i = 1. Hint: It is more convenient to use the matrix form of OLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.4">Invariance of the interaction</head><p>In Section 17.3.2, the point estimate and standard error of the coefficient of the interaction term remain the same no matter whether we center the covariates or not. This result holds in general. This problem quantifies this phenomenon. With scalars x i1 , x i2 , y i (i = 1, . . . , n), we can fit the OLS</p><formula xml:id="formula_486">y i = β0 + β1 x i1 + β2 x i2 + β12 x i1 x i2 + εi .</formula><p>Under any location transformations of the covariates</p><formula xml:id="formula_487">x ′ i1 = x i1 -c 1 , x ′ i2 = x i2 -c 2 , we can fit the OLS y i = β0 + β1 x ′ i1 + β2 x ′ i2 + β12 x ′ i1 x ′ i2 + εi .</formula><p>1. Express β0 , β1 , β2 , β12 in terms of β0 , β1 , β2 , β12 . Verify that β12 = β12 .</p><p>2. Show that the EHW standard errors for β12 and β12 are identical.</p><p>Hint: Use the results in Problems 3.4 and 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restricted OLS</head><p>Assume that in the standard linear model Y = Xβ + ε, the parameter has restriction</p><formula xml:id="formula_488">Cβ = r (18.1)</formula><p>where C is an l × p matrix and r is a l dimensional vector. Assume that C has linearly independent row vectors; otherwise, some restrictions are redundant. We can use the restricted OLS: βr = arg min b∈R p ∥Y -Xb∥ 2 under the restriction Cb = r.</p><p>I first give some examples of linear models with restricted parameters, then derive the algebraic properties of the restricted OLS estimator βr , and finally discuss statistical inference with restricted OLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.1">Examples</head><p>Example 18.1 (Short regression) Partition X into X 1 and X 2 with k and l columns, respectively, with p = k + l. The short regression of Y on X 1 yields OLS coefficient β1 . So ( βt 1 , 0 t l ) = βr with C = (0 l×k , I l×l ), r = 0 l .</p><p>Example 18.2 (Testing linear hypothesis) Consider testing the linear hypothesis Cβ = r in the linear model. We have discussed in Chapter 5 the Wald test based on the OLS estimator and its estimated covariance matrix under the Normal linear model. An alternative strategy is to test the hypothesis based on comparing the residual sum of squares under the OLS and restricted OLS. Therefore, we need to compute both β and βr .</p><p>Example 18.3 (One-way analysis of variance) If x i contains the intercept and Q 1 dummy variables of a discrete regressor of Q 1 levels, (f i1 , . . . , f iQ1 ) t , then we must impose a restriction on the parameter in the linear model</p><formula xml:id="formula_489">y i = α + Q1 j=1 β j f ij + ε i .</formula><p>A canonical choice is β Q1 = 0, which is equivalent to dropping the last dummy variable due to its redundancy. Another canonical choice is Q1 j=1 β j = 0. This restriction keeps the symmetry of the regressors in the linear model and changes the interpretation of β j as the deviation from the "effect" of level j with respect to the average "effect." Both are special cases of restricted OLS.</p><p>Example 18.4 (Two-way analysis of variance) With two factors of levels Q 1 and Q 2 , respectively, the regressor x i contains the Q 1 dummy variables of the first factor, (f i1 , . . . , f iQ1 ) t , the Q 2 dummies of the second factor, (g i1 , . . . , g iQ2 ) t , and the Q 1 Q 2 dummy variables of the interaction terms, (f i1 g i1 , . . . , f iQ1 g iQ2 ) t . We must impose restrictions on the parameters in the linear model</p><formula xml:id="formula_490">y i = α + Q1 j=1 β j f ij + Q2 k=1 γ k g ik + Q1 j=1 Q2 k=1 δ jk f ij g ik + ε i .</formula><p>Similar to the discussion in Example 18.3, two canonical choices of restrictions are</p><formula xml:id="formula_491">β Q1 = 0, γ Q2 = 0, δ Q1,k = δ j,Q2 = 0, (j = 1, . . . , Q 1 ; k = 1, . . . , Q 2 ). and Q1 j=1 β j = 0, Q2 k=1 γ k = 0, Q1 j=1 δ jk = Q2 k=1 δ jk = 0, (j = 1, . . . , Q 1 ; k = 1, . . . , Q 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.2">Algebraic properties</head><p>I first give an explicit formula of the restricted OLS <ref type="bibr" target="#b216">(Theil, 1971;</ref><ref type="bibr" target="#b197">Rao, 1973)</ref>. For simplicity, the following theorem assumes that X t X is invertible. This condition may not hold in general; see Examples 18.3 and 18.4. <ref type="bibr" target="#b129">Greene and Seaks (1991)</ref> discussed the results without this assumption; see Problem 18.8 for more details.</p><formula xml:id="formula_492">Theorem 18.1 If X t X is invertible, then βr = β -(X t X) -1 C t {C(X t X) -1 C t } -1 (C β -r),</formula><p>where β is the unrestricted OLS coefficient.</p><p>Proof of Theorem 18.1: The Lagrangian for the restricted optimization problem is</p><formula xml:id="formula_493">(Y -Xb) t (Y -Xb) -2λ t (Cb -r). So the first order condition is 2X t (Y -Xb) -2C t λ = 0 which implies X t Xb = X t Y -C t λ. (18.2) Solve the linear system in (18.2) to obtain b = (X t X) -1 (X t Y -C t λ).</formula><p>Using the linear restriction Cb = r, we have</p><formula xml:id="formula_494">C(X t X) -1 (X t Y -C t λ) = r which implies that λ = {C(X t X) -1 C t } -1 (C β -r). So the restricted OLS coefficient is βr = (X t X) -1 (X t Y -C t λ) = β -(X t X) -1 C t λ = β -(X t X) -1 C t {C(X t X) -1 C t } -1 (C β -r).</formula><p>Since the objective function is convex and the restrictions are linear, the solution from the first-order condition is indeed the minimizer. □ In the special case with r = 0, Theorem 18.1 has a simpler form.</p><p>Corollary 18.1 Under the restriction (18.1) with r = 0, we have</p><formula xml:id="formula_495">βr = M r β, where M r = I p -(X t X) -1 C t {C(X t X) -1 C t } -1 C.</formula><p>Moreover, M r satisfies the following properties</p><formula xml:id="formula_496">M r (X t X) -1 C t = 0, CM r = 0, {I p -C t (CC t ) -1 C}M r = M r .</formula><p>The M r matrix plays central roles below.</p><p>The following result is also an immediate corollary of Theorem 18.1.</p><p>Corollary 18.2 Under the restriction (18.1), we have</p><formula xml:id="formula_497">βr -β = M r ( β -β).</formula><p>I leave the proofs of Corollaries 18.1 and 18.2 as Problem 18.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.3">Statistical inference</head><p>I first focus on the Gauss-Markov model with the restriction (18.1). As direct consequences of Corollary 18.2, we can show that the restricted OLS estimator is unbiased for β, and obtain its covariance matrix below.</p><p>Corollary 18.3 Assume the Gauss-Markov model with the restriction (18.1). We have</p><formula xml:id="formula_498">E( βr ) = β, cov( βr ) = σ 2 M r (X t X) -1 M t r .</formula><p>Moreover, under the Normal linear model with the restriction (18.1), we can derive the exact distribution of the restricted OLS estimator and propose an unbiased estimator for σ 2 .</p><p>Theorem 18.2 Assume the Normal linear model with the restriction (18.1). We have</p><formula xml:id="formula_499">βr ∼ N(β, σ 2 M r (X t X) -1 M t r ). An unbiased estimator for σ 2 is σ2 r = εt r εr /(n -p + l),</formula><p>where εr = Y -X βr . Moreover, βr σ2 r .</p><p>Based on the results in Theorem 18.2, we can derive the t and F statistics for finitesample inference of β based on the estimator βr and the estimated covariance matrix σ2 r M r (X t X) -1 M t r .</p><p>Corollary 18.3 and Theorem 18.2 extend the results for the OLS estimator. I leave their proofs as Problem 18.3.</p><p>I then discuss statistical inference under the heteroskedastic linear model with the restriction (18.1). Corollary 18.2 implies that cov</p><formula xml:id="formula_500">( βr ) = M r (X t X) -1 X t diag{σ 2 1 , . . . , σ 2 n }X(X t X) -1 M t r .</formula><p>Therefore, the EHW-type estimated covariance matrix is</p><formula xml:id="formula_501">Vehw,r = M r (X t X) -1 X t diag{ε 2 i,r , . . . , ε2 n,r }X(X t X) -1 M t r .</formula><p>where the εi,r 's are the residuals from the restricted OLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.4">Final remarks</head><p>This chapter follows <ref type="bibr" target="#b216">Theil (1971)</ref> and <ref type="bibr" target="#b197">Rao (1973)</ref>. <ref type="bibr" target="#b215">Tarpey (2000)</ref> contains additional algebraic results on restricted OLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.5">Homework problems</head><p>18.1 Algebraic details of restricted OLS Prove Corollaries 18.1 and 18.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.2">Invariance of restricted OLS</head><p>Consider an N × 1 vector Y and two N × p matrices, X and X ′ , that satisfy X ′ = XΓ for some nonsingular p × p matrix Γ. The restricted OLS fits of Y = X βr + εr subject to C βr = r, Y = X βr + εr subject to C βr = r, with X = XΓ and C = CΓ yield ( βr , εr , Vehw,r ) and ( βr , εr , Ṽehw,r ) as the coefficient vectors, residuals, and robust covariances. Prove that they must satisfy βr = Γ βr , εr = εr , Vehw,r = Γ Ṽehw,r Γ t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.3">Moments and distribution of restricted OLS</head><p>Prove Corollary 18.3 and Theorem 18.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.7">Minimum normal estimator as restricted OLS</head><p>An application of the formula of βr is the minimum norm estimator for under-determined linear equations. When X has more columns than rows, Y = Xβ can have infinitely many solutions, but we may only be interested in the solution with the minimum norm. Assume p ≥ n and the rows of X are linearly independent. Show that the solution to</p><formula xml:id="formula_502">min b ∥b∥ 2 such that Y = Xb is βm = X t (XX t ) -1 Y.</formula><p>18.8 Restricted OLS with degenerate design matrix <ref type="bibr" target="#b129">Greene and Seaks (1991)</ref> pointed out that restricted OLS does not require that X t X be invertible, although the proof of Theorem 18.1 does. Modify the proof to show that the restricted OLS and the Lagrange multiplier satisfy</p><formula xml:id="formula_503">βr λ = W -1 X t Y r as long as W = X t X C t C 0 is invertible.</formula><p>Derive the statistical results in parallel with Section 18.3. Remark: If X has full column rank p, then W must be invertible. Even if X does not have full column rank, W can still be invertible. See Problem 18.9 below for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.9">Restricted OLS with degenerate design matrix: more algebra</head><p>This problem provides more algebraic details for Problem 18.8. Prove Lemma 18.1 below.</p><formula xml:id="formula_504">Lemma 18.1 Consider W = X t X C t C 0</formula><p>where X t X may not be invertible and C has full row rank.</p><p>The matrix W is invertible if and only if X C has full column rank p. Remark: When X has full column rank p, then X C must have full column rank p, which ensures that W is invertible by Lemma 18.1. I made the comment in Problem 18.8. The invertibility of W plays an important role in other applications. See Benzi et al. (2005) and Bai and Bai (2013) for more general results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Least Squares</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.1">Generalized least squares</head><p>We can extend the Gauss-Markov model to allow for a general covariance structure of the error term. The following model is due to <ref type="bibr" target="#b47">Aitkin (1936)</ref>.</p><formula xml:id="formula_505">Assumption 19.1 (Generalized Gauss-Markov model) We have Y = Xβ + ε, E(ε) = 0, cov(ε) = σ 2 Σ. (19.1)</formula><p>where X is a fixed matrix with linearly independent columns. The unknown parameters are β and σ 2 . The Σ is a known positive definite matrix.</p><p>Two leading cases of generalized least squares are</p><formula xml:id="formula_506">Σ = diag w -1 1 , . . . , w -1 n ,<label>(19.2)</label></formula><p>which corresponds to a diagonal covariance matrix, and</p><formula xml:id="formula_507">Σ = diag {Σ 1 , . . . , Σ K } (19.3)</formula><p>which corresponds to a block diagonal covariance matrix where Σ k is n k ×n k and</p><formula xml:id="formula_508">K k=1 n k = n.</formula><p>Under model (19.1), we can still use the OLS estimator β = (X t X) -1 X t Y . It is unbiased</p><formula xml:id="formula_509">E( β) = (X t X) -1 X t E(Y ) = (X t X) -1 X t Xβ = β. It has covariance matrix cov( β) = cov (X t X) -1 X t Y = (X t X) -1 X t cov(Y )X(X t X) -1 = σ 2 (X t X) -1 X t ΣX(X t X) -1 . (19.4)</formula><p>The OLS estimator is BLUE under the Gauss-Markov model, but it is not under the generalized Gauss-Markov model. Then what is the BLUE? We can transform (19.1) into the Gauss-Markov model by standardizing the error term:</p><formula xml:id="formula_510">Σ -1/2 Y = Σ -1/2 Xβ + Σ -1/2 ε. Define Y * = Σ -1/2 Y, X * = Σ -1/2 X and ε * = Σ -1/2 ε. The model (19.1) reduces to Y * = Xβ + ε * , E(ε * ) = 0, cov(ε * ) = σ 2 I n ,</formula><p>which is the Gauss-Markov model for the transformed variables Y * and X * . The Gauss-Markov theorem ensures that the BLUE is</p><formula xml:id="formula_511">βΣ = (X t * X * ) -1 X t * Y * = (X t Σ -1 X) -1 X t Σ -1 Y.</formula><p>It is unbiased because</p><formula xml:id="formula_512">E( βΣ ) = (X t Σ -1 X) -1 X t Σ -1 E(Y ) = (X t Σ -1 X) -1 X t Σ -1 Xβ = β. It has covariance matrix cov( βΣ ) = cov (X t Σ -1 X) -1 X t Σ -1 Y = (X t Σ -1 X) -1 X t Σ -1 cov(Y )Σ -1 X(X t Σ -1 X) -1 = σ 2 (X t Σ -1 X) -1 X t Σ -1 ΣΣ -1 X(X t Σ -1 X) -1 = σ 2 (X t Σ -1 X) -1 . (19.5)</formula><p>In particular, cov( βΣ ) is smaller than or equal to cov( β) in the matrix sense<ref type="foot" target="#foot_17">foot_17</ref> . So based on (19.4) and (19.5), we have the following pure linear algebra inequality:</p><p>Corollary 19.1 If X has linear independent columns and Σ is invertible, then</p><formula xml:id="formula_513">(X t Σ -1 X) -1 ⪯ (X t X) -1 X t ΣX(X t X) -1 .</formula><p>Problem 19.1 gives a more general result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.2">Weighted least squares</head><p>This chapter focuses on the first covariance structure in (19.2) and Chapter 25 will discuss the second in (19.3). The Σ in (19.2) results in the weighted least squares (WLS) estimator</p><formula xml:id="formula_514">βw = βΣ = (X t Σ -1 X) -1 X t Σ -1 Y = n i=1 w i x i x t i -1 n i=1 w i x i y i .</formula><p>From the derivation above, we can also write the WLS estimator as</p><formula xml:id="formula_515">βw = arg min b∈R p (Y -Xb) t Σ -1 (Y -Xb) = arg min b∈R p n i=1 w i (y i -x t i b) 2 = arg min b∈R p (Y * -X * b) t (Y * -X * b) = arg min b∈R p n i=1 (y * i -x t * i b) 2 ,</formula><p>where y * i = w 1/2 i y i and x * i = w 1/2 i x i . So WLS is equivalent to the OLS with transformed variables, with the weights inversely proportional to the variances of the errors. By this equivalence, WLS inherits many properties of OLS. See the problems in Section 19.5 for more details.</p><p>Analogous to OLS, we can derive finite-sample exact inference based on the generalized Normal linear model:</p><formula xml:id="formula_516">y i = x t i β + ε i , ε i ∼ N(0, σ 2 /w i ), or, equivalently, y * i = x t * i β + ε * i , ε * i ∼ N(0, σ 2</formula><p>). The lm function with weights reports the standard error, t-statistic, and p-value based on this model. This assumes that the weights fully capture the heteroskedasticity, which is unrealistic in many problems.</p><p>In addition, we can derive asymptotic inference based on the following heteroskedastic model y i = x t i β + ε i where the ε i 's are independent with mean zero and variances σ 2 i (i = 1, . . . , n). It is possible that w i ̸ = 1/σ 2 i , i.e., the variances used to construct the WLS estimator can be misspecified. Even though there is no guarantee that βw is BLUE, it is still unbiased. From the decomposition βw =</p><formula xml:id="formula_517">n i=1 w i x i x t i -1 n i=1 w i x i y i = n i=1 w i x i x t i -1 n i=1 w i x i (x t i β + ε i ) = β + n -1 n i=1 w i x i x t i -1 n -1 n i=1 w i x i ε i ,</formula><p>we can apply the law of large numbers to show that βw is consistent for β and apply the CLT to show that βw</p><formula xml:id="formula_518">a ∼ N (β, V w ) ,</formula><p>where</p><formula xml:id="formula_519">V w = n -1 n -1 n i=1 w i x i x t i -1 n -1 n i=1 w 2 i σ 2 i x i x t i n -1 n i=1 w i x i x t i -1</formula><p>.</p><p>The EHW robust covariance generalizes to</p><formula xml:id="formula_520">Vehw,w = n -1 n -1 n i=1 w i x i x t i -1 n -1 n i=1 w 2 i ε2 w,i x i x t i n -1 n i=1 w i x i x t i -1</formula><p>, where εw,i = y i -x t i βw is the residual from the WLS. Note that in the sandwich covariance, w i appears in the "bread" but w 2 i appears in the "meat." This formula appeared in Magee (1998) and <ref type="bibr" target="#b201">Romano and Wolf (2017)</ref>. The function hccm in the R package car can compute various EHW covariance estimators based on WLS. To save space in the examples below, I report only the standard errors based on the generalized Normal linear model and leave the calculations of the EHW covariances as a homework problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.3">WLS motivated by heteroskedasticity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.3.1">Feasible generalized least squares</head><p>Assume that ε has mean zero and covariance diag σ 2 1 , . . . , σ 2 n . If the σ 2 i 's are known, we can simply apply the WLS above; if they are unknown, we need to estimate them first. This gives the following feasible generalized least squares estimator (FGLS):</p><p>1. Run OLS of y i on x i to obtain the residuals εi . Then obtain the squared residuals ε2 i .</p><p>2. Run OLS of log(ε 2 i ) on x i to obtain the fitted values and exponentiate them to obtain (σ 2 i ) n i=1 ;</p><p>3. Run WLS of y i on x i with weights σ-2</p><formula xml:id="formula_521">i to obtain βfgls = n i=1 σ-2 i x i x t i -1 n i=1 σ-2 i x i y i .</formula><p>In Step 2, we can change the model based on our understanding of heteroskedasticity. Here I use the Boston housing data to compare the OLS and FGLS, with R code in code18.3.1.R. &gt; library ( mlbench ) &gt; data ( BostonHousing ) &gt; ols . fit = lm ( medv ~. , data = BostonHousing ) &gt; dat . res = BostonHousing &gt; dat . res $ medv = log (( ols . fit $ residuals )^2 ) &gt; t . res . ols = lm ( medv ~. , data = dat . res ) &gt; w . fgls = exp ( -t . res . ols $ fitted . values ) &gt; fgls . fit = lm ( medv ~. , weights = w . fgls , data = BostonHousing ) &gt; ols . fgls = cbind ( summary ( ols . fit )$ coef [ , 1 : 3 ] , + summary ( fgls . fit )$ coef [ , 1 : 3 ]) &gt; round ( ols . fgls , 3 ) Estimate Std . Error t value Estimate Std . Error t value ( Intercept ) 3 6 . 4 5 9 5 . 1 0 3 7 . 1 4 4 9 . 4 9 9 4 . 0 2 . 3 4 crim -0 . 1 0 8 0 . 0 3 3 -3 . 2 8 7 -0 . 0 8 1 0 . 0 -1 . 8 2 zn 0 . 0 4 6 0 . 0 1 4 3 . 3 8 2 0 . 0 3 0 0 . 0 2 . 6 7 indus 0 . 0 2 1 0 . 0 6 1 0 . 3 3 4 -0 . 0 3 5 0 . 0 -0 . 9 2 chas 1 2 . 6 8 7 0 . 8 6 2 3 . 1 1 8 1 . 4 6 2 1 . 1 1 . 3 1 nox -1 7 . 7 6 7 3 . 8 2 0 -4 . 6 5 1 -7 . 1 6 1 2 . 7 -2 . 5 7 rm</p><p>3 . 8 1 0 0 . 4 1 8 9 . 1 1 6 5 . 6 7 5 0 . 3 1 5 . 5 9 age 0 . 0 0 1 0 . 0 1 3 0 . 0 5 2 -0 . 0 4 4 0 . 0 -5 . 5 0 dis -1 . 4 7 6 0 . 1 9 9 -7 . 3 9 8 -0 . 9 2 7 0 . 1 -6 . 6 8 rad 0 . 3 0 6 0 . 0 6 6 4 . 6 1 3 0 . 1 7 0 0 . 0 3 . 3 1 tax -0 . 0 1 2 0 . 0 0 4 -3 . 2 8 0 -0 . 0 1 0 0 . 0 -4 . 1 4 ptratio -0 . 9 5 3 0 . 1 3 1 -7 . 2 8 3 -0 . 7 0 0 0 . 0 -7 . 4 5 b 0 . 0 0 9 0 . 0 0 3 3 . 4 6 7 0 . 0 1 4 0 . 0 6 . 5 4 lstat -0 . 5 2 5 0 . 0 5 1 -1 0 . 3 4 7 -0 . 1 5 8 0 . 0 -4 . 3 8</p><p>Unfortunately, the coefficients, including the point estimates and standard errors, from OLS and FGLS are quite different for several covariates. This suggests that the linear model may be misspecified. Otherwise, both estimators are consistent for the same true coefficient, and they should not be so different even in the presence of randomness.</p><p>The above FGLS estimator is close to <ref type="bibr">Wooldridge (2012, Chapter 8)</ref>. <ref type="bibr" target="#b201">Romano and Wolf (2017)</ref> propose to regress log(max(δ 2 , ε2 i )) on log |x i1 |, . . . , log |x ip | to estimate the individual variances. Their modification has two features: first, they truncate the small residuals by a pre-specified positive number δ 2 ; second, their regressors are the logs of the absolute values of the original covariates. <ref type="bibr" target="#b201">Romano and Wolf (2017)</ref> highlighted the efficiency gain from the  <ref type="formula">2019</ref>) proposed some improved versions of the FGLS estimator even if the variance function is misspecified. However, it is unusual for practitioners to use FGLS even though it can be more efficient than OLS. There are several reasons. First, the EHW standard errors are convenient for correcting the standard error of OLS under heteroskedasticity. Second, the efficiency gain is usually small, and it is even possible that the FGLS is less efficient than OLS when the variance function is misspecified. Third, the linear model is very likely to be misspecified, and if so, OLS and FGLS estimate different parameters. The OLS has the interpretations as the best linear predictor and the best linear approximation of the conditional mean, but the FGLS has more complicated interpretations when the linear model is wrong. Based on these reasons, we need to carefully justify the choice of FGLS over OLS in real data analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.3.2">Aggregated data and ecological regression</head><p>In some case, (y i , x i ) come from aggregated data, for example, y i can be the average test score and x i can be the average parents' income of students within classroom i. If we believe that the student-level test score and parents' income follow a homoskedastic linear model, then the model based on the classroom average must be heteroskedastic, with the variance inversely proportional to the classroom size. In this case, a natural choice of weight is w i = n i , the classroom size. Below I use the lavoteall dataset from the R package ei. It contains the the fraction of black registered voters x, the fraction of voter turnout t, and the total number of people n in each Louisiana precinct. Figure <ref type="figure" target="#fig_50">19</ref>.1 is the scatterplot. In this example, OLS and WLS give similar results although n varies a lot across precincts.</p><p>&gt; library ( " ei " ) &gt; data ( lavoteall ) &gt; ols . fit = lm ( t ~x , data = lavoteall ) &gt; wls . fit = lm ( t ~x , weights = n , data = lavoteall ) &gt; compare = cbind ( summary ( ols . fit )$ coef [ , 1 : 3 ] , + summary ( wls . fit )$ coef [ , 1 : 3 ]) &gt; round ( compare , 3 ) Estimate Std . Error t value Estimate Std . Error t value ( Intercept ) 0 . 7 1 1 0 . 0 0 2 4 0 8 . 2 1 1 0 . 7 0 6 0 . 0 0 2 4 2 1 . 6 6 2 x -0 . 0 8 3 0 . 0 0 4 -1 9 . 9 5 3 -0 . 0 8 0 0 . 0 0 4 -1 9 . 9 3 8</p><p>In the above, we can interpret the coefficient of x as the precinct-level relationship between the fraction of black registered voters and the fraction voting. Political scientists are interested in using aggregated data to infer individual voting behavior. Hypothetically, the precinct i has individual data {x ij , y ij : j = 1, . . . , n i } where x ij and y ij are the binary racial and voting status of individual (i, j) (i = 1, . . . , n; j = 1, . . . , n i ). However, we only observe the aggregated data {x i• , ȳi• , n i : i = 1, . . . , n}, where</p><formula xml:id="formula_522">xi• = n -1 i ni j=1 x ij , ȳi• = n -1 i ni j=1 y ij</formula><p>are the fraction of black registered voters and the fraction voting, respectively. Can we infer the individual voting behavior based on the aggregated data? In general, this is almost impossible. Under some assumptions, we can make progress. Goodman's ecological regression below is one possibility.</p><p>Assume that for precinct i = 1, . . . , n, we have</p><formula xml:id="formula_523">y ij | x ij = 1 iid ∼ Bernoulli(p i1 ), y ij | x ij = 0 iid ∼ Bernoulli(p i0 ), (j = 1, . . . , n i ).</formula><p>This is the individual-level model, where the p i1 's and p i0 's measure the association between race and voting. We further assume that they are random and independent of the x ij 's, with means</p><formula xml:id="formula_524">E(p i1 ) = p 1 , E(p i0 ) = p 0 . (19.6)</formula><p>Then we can decompose the aggregated outcome variable as</p><formula xml:id="formula_525">ȳi• = n -1 i ni j=1 y ij = n -1 i ni j=1 {x ij y ij + (1 -x ij )y ij } = n -1 i ni j=1 {x ij p 1 + (1 -x ij )p 0 } + ε i = p 1 xi• + p 0 (1 -xi• ) + ε i ,</formula><p>where</p><formula xml:id="formula_526">ε i = n -1 i ni j=1 {x ij (y ij -p 1 ) + (1 -x ij )(y ij -p 0 )}.</formula><p>So we have a linear relationship between the aggregated outcome and covariate ȳi</p><formula xml:id="formula_527">• = p 1 xi• + p 0 (1 -xi• ) + ε i , where E(ε i | xi• ) = 0.</formula><p>Goodman <ref type="bibr">(1953)</ref> suggested to use the OLS of ȳi• on {x i• , (1 -xi• )} to estimate (p 1 , p 0 ), and <ref type="bibr" target="#b128">Goodman (1959)</ref> suggested to use the corresponding WLS with weight n i since the variance of ε i has the magnitude n -1 i . Moreover, the variance of ε i has a rather complicated form of heteroskedasticity, so we should use the EHW standard error for inference. This is called Goodman's regression or ecological regression. The R code in code18.3.2.R implements ecological regression based on the lavoteall data.</p><p>&gt; ols . fit = lm ( t ~0 + x + I ( 1 -x ) , data = lavoteall ) &gt; wls . fit = lm ( t ~0 + x + I ( 1 -x ) , weights = n , data = lavoteall ) &gt; compare = cbind ( summary ( ols . fit )$ coef [ , 1 : 3 ] , + summary ( wls . fit )$ coef [ , 1 : 3 ]) &gt; round ( compare , 3 ) Estimate Std . Error t value Estimate Std . Error t value x 0 . 6 2 8 0 . 0 0 3 1 8 8 . 2 9 2 0 . 6 2 6 0 . 0 0 3 1 9 4 . 4 9 3 I ( 1 -x ) 0 . 7 1 1 0 . 0 0 2 4 0 8 . 2 1 1 0 . 7 0 6 0 . 0 0 2 4 2 1 . 6 6 2</p><p>The assumption in (19.6) is crucial, which can be too strong when the precinct level p i1 's and p i0 's vary in systematic but unobserved ways. When the assumption is violated, it is possible that the ecological regression yields the opposite result compared to the individual regression. This is called the ecological fallacy.</p><p>Another obvious problem of ecological regression is that the estimated coefficients may lie outside of the interval [0, 1]. Problem 19.18 gives an example. <ref type="bibr" target="#b125">Gelman et al. (2001)</ref> gave an alternative set of assumptions justifying the ecological regression. <ref type="bibr" target="#b155">King (1997)</ref> proposed some extensions. <ref type="bibr" target="#b199">Robinson (1950)</ref> warned that the ecological correlation might not inform individual correlation. <ref type="bibr" target="#b113">Freedman et al. (1991)</ref> warned that the assumptions underlying the ecological regression might not be plausible in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.4">WLS with other motivations</head><p>WLS can be used in other settings unrelated to heteroskedasticity. I review two examples below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.4.1">Local linear regression</head><p>Calculus tells us that locally we can approximate any smooth function f (x) by a linear function even though the original function can be highly nonlinear:</p><formula xml:id="formula_528">f (x) ≈ f (x 0 ) + f ′ (x 0 )(x -x 0 )</formula><p>when x is near x 0 . The left panel of Figure <ref type="figure" target="#fig_50">19</ref>.2 shows that in the neighborhood of x 0 = 0.4, even a sine function can be well approximated by a line. Based on data (x i , y i ) n i=1 , if we want to predict the mean value of y given x = x 0 , then we can predict based on a line with the local data points close to x 0 . It is also reasonable to down weight the points that are far from x 0 , which motivates the following WLS:</p><formula xml:id="formula_529">( α, β) = arg min a,b n i=1 w i {y i -a -b(x i -x 0 )} 2 with w i = K {(x i -x 0 )/</formula><p>h} where K(•) is called the kernel function and h is called the bandwidth parameter. With the fitted line ŷ(x) = α + β(x -x 0 ), the predicted value at x = x 0 is the intercept α.</p><p>Technically, K(•) can be any density function, and two canonical choices are the standard Normal density and the Epanechikov kernel K(t) = 0.75(1 -t 2 )1(|t| ≤ 1). The choice of q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0.2 0.4 0.6 0.8 1.0 -2 -1 0 1 2 local linear approximation x y q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0.2 0.4 0.6 0.8 1.0 With large bandwidth, we have a poor linear approximation, leading to bias; with small bandwidth, we have few data points, leading to large variance. In practice, we face a biasvariance trade-off. In practice, we can either use cross-validation or other criterion to select h.</p><p>In general, we can approximate a smooth function by a polynomial:</p><formula xml:id="formula_530">f (x) ≈ K k=0 f (k) (x 0 ) k! (x -x 0 ) k</formula><p>when x is near x 0 . So we can even fit a polynomial function locally, which is called local polynomial regression <ref type="bibr" target="#b97">(Fan and Gijbels, 1996)</ref>. In the R package Kernsmooth, the function locpoly fits local polynomial regression, and the function dpill selects h based on <ref type="bibr" target="#b205">Ruppert et al. (1995)</ref>. The default specification of locpoly is the local linear regression.</p><p>&gt; library ( " KernSmooth " ) &gt; n = 5 0 0 &gt; x = seq ( 0 , 1 , length . out = n ) &gt; fx = sin ( 8 * x ) &gt; y = fx + rnorm (n , 0 , 0 . 5 ) &gt; plot ( y ~x , pch = 1 9 , cex = 0 . 2 , col = " grey " , bty = " n " , + main = " local ␣ linear ␣ fit " , font . main = 1 ) &gt; lines ( fx ~x , lwd = 2 , col = " grey " ) &gt; h = dpill (x , y ) &gt; locp . fit = locpoly (x , y , bandwidth = h ) &gt; lines ( locp . fit , lty = 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.4.2">Regression with survey data</head><p>Most discussions in this book are based on i.i.d. samples, or, at least, the sample represents the population of interest. Sometimes, researchers over sample some units and under sample some other units from a population of interest. x i y i . However, we do not have all the data points in the large population, but sample each data point independently with probability</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sampling with probability</head><formula xml:id="formula_531">π i = pr(I i = 1 | x i , y i ),</formula><p>where I i is a binary indicator for being included in the sample. Conditioning on</p><formula xml:id="formula_532">X N = (x i ) N i=1 and Y N = (y i ) N i=1</formula><p>, βideal is a fixed number, and an estimator is the following WLS estimator</p><formula xml:id="formula_533">β1/π = N i=1 I i π i x i x t i -1 N i=1 I i π i x i y i = n i=1 π -1 i x i x t i -1 n i=1 π -1 i x i y i ,</formula><p>with weights inversely proportional to the sampling probability. This inverse probability weighting estimator is reasonable because</p><formula xml:id="formula_534">E N i=1 I i π i x i x t i | X N , Y N = N i=1 x i x t i , E N i=1 I i π i x i y i | X N , Y N = N i=1</formula><p>x i y i .</p><p>The inverse probability weighting estimators are called the Horvitz-Thompson estimators <ref type="bibr" target="#b144">(Horvitz and Thompson, 1952)</ref>, which are the cornerstones of survey sampling. Below I use the dataset census00.dta to illustrate the use of sampling weight, which is the perwt variable. The R code is in code18.4.2.R. &gt; library ( foreign ) &gt; census 0 0 = read . dta ( " census 0 0 . dta " ) &gt; ols . fit = lm ( logwk ~age + educ + exper + exper 2 + black , + data = census 0 0 ) &gt; wls . fit = lm ( logwk ~age + educ + exper + exper 2 + black , + weights = perwt , data = census 0 0 ) &gt; compare = cbind ( summary ( ols . fit )$ coef</p><p>[ , 1 : 3 ] , + summary ( wls . fit )$ coef [ , 1 : 3 ]) &gt; round ( compare , 4 ) Estimate Std . Error t value Estimate Std . Error t value ( Intercept ) 5 . 1 6 6 7 0 . 1 2 8 2 4 0 . 3 5 . 0 7 4 0 0 . 1 2 6 8 4 0 . 0 age -0 . 0 1 4 8 0 . 0 0 6 7 -2 . 2 -0 . 0 0 8 4 0 . 0 0 6 7 -1 . 3 educ 0 . 1 2 9 6 0 . 0 0 6 6 1 9 . 7 0 . 1 2 2 8 0 . 0 0 6 5 1 8 . 8 exper 2 0 . 0 0 0 3 0 . 0 0 0 1 2 . 2 0 . 0 0 0 2 0 . 0 0 0 1 1 . 3 black -0 . 2 4 6 7 0 . 0 0 8 5 -2 9 . 2 -0 . 2 5 7 4 0 . 0 0 8 0 -3 2 . 0 19.5 Homework problems 19.1 A linear algebra fact related to WLS This problem extends Corollary 19.1. Show that</p><formula xml:id="formula_535">(X t Σ -1 X) -1 ⪯ (X t ΩX) -1 X t ΩΣΩX(X t ΩX) -1 .</formula><p>When Ω = Σ -1 , the equality holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.2">Generalized least squares with a block diagonal covariance</head><p>Partition X and Y into</p><formula xml:id="formula_536">X =    X 1 . . . X K    , Y =    Y 1 . . . Y K    corresponding to Σ in (19.3) such that X k ∈ R n k ×p and Y k ∈ R n k . Show that the generalized least squares estimator is βΣ = K k=1 X t k Σ -1 k X k -1 K k=1 X t k Σ -1 k Y k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.3">Univariate WLS</head><p>Prove the following Galtonian formula for the univariate WLS: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.4">Difference-in-means with weights</head><p>With a binary covariate x i , show that the coefficient of x i in the WLS of y i on (1, x i ) with weights w i (i = 1, . . . , n) equals ȳw,1 -ȳw,0 , where</p><formula xml:id="formula_537">ȳw,1 = n i=1 w i x i y i n i=1 w i x i , ȳw,0 = n i=1 w i (1 -x i )y i n i=1 w i (1 -x i )</formula><p>are the weighted averages of the outcome under treatment and control, respectively.</p><p>Hint: You can use the result in Problem 19.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.5">Asymptotic Normality of WLS and robust covariance estimator</head><p>Under the heteroskedastic linear model, show that βw is consistent and asymptotically Normal, and show that n Vw is consistent for the asymptotic covariance of √ n( βw -β). Specify the regularity conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.6">WLS in ANOVA</head><p>This problem extends Problems 5.5 and 6.3.</p><p>For units i = 1, . . . , n, assume y i denotes the outcome, x i denotes the p-vector with entries as the dummy variables for a discrete covariate with p levels, w i &gt; 0 denotes a weight, and π i &gt; 0 denotes another weight that is a function of x i only (for example, π i = n j /n if x i = e j ). Run the following regressions:</p><p>• WLS of y i on x i with weight w i for i = 1, . . . , n to obtain the coefficient vector β and EHW covariance matrix V ;</p><p>• WLS of y i on x i with weight w i π i for i = 1, . . . , n to obtain the coefficient vector β′ and EHW covariance matrix V ′ .</p><p>Show that β = β′ with the jth entry βj = β′ j = i:xi=ej w i y i i:xi=ej w i , and moreover, V = V ′ are diagonal with the (j, j)th entry</p><formula xml:id="formula_538">Vjj = V ′ jj = i:xi=ej w 2 i (y i -βj ) 2 ( i:xi=ej w i ) 2</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.7">An infeasible generalized least squares estimator</head><p>Can we skip Step 2 in Section 19.3.1 and directly apply the following WLS estimator:</p><formula xml:id="formula_539">βigls = n i=1 ε-2 i x i x t i -1 n i=1 ε-2 i x i y i</formula><p>with εi = y i -x t i β. If so, give a theoretical justification; if not, give a counterexample. Evaluate the finite-sample properties of βigls using simulated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.8">FWL theorem in WLS</head><p>This problem is an extension of Theorem 7.1.</p><p>Consider the WLS with an n × 1 vector Y , an n × k matrix X 1 , an n × l matrix X 2 , and weights w i 's. Show that βw,2 in the long WLS fit Y = X 1 βw,1 + X 2 βw,2 + εw equals the coefficient of Xw,2 in the WLS fit of Ỹw on Xw,2 , where Xw,2 are the residual vectors from the column-wise WLS of X 2 on X 1 , and Ỹw is the residual vector from the WLS of Y on X 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.9">The sample version of Cochran's formula in WLS</head><p>This problem is an extension of Theorem 9.1.</p><p>Consider the WLS with an n × 1 vector Y , an n × k matrix X 1 , an n × l matrix X 2 , and weights w i 's. We can fit the following WLS:</p><formula xml:id="formula_540">Y = X 1 βw,1 + X 2 βw,2 + εw , Y = X 2 βw,2 + εw , X 1 = X 2 δw + Ûw ,</formula><p>where εw , εw , Ûw are the residuals. The last WLS fit means the WLS fit of each column of X 1 on X 2 . Similar to Theorem 9.1, we have βw,2 = βw,2 + δw βw,1 .</p><p>Prove this result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.10">EHW robust covariance estimator in WLS</head><p>We have shown in Section 19.1 that the coefficients from WLS are identical to those from OLS with transformed variables. Further show that the corresponding HC0 version of EHW covariance estimators are also identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.11">Invariance of covariance estimators in WLS</head><p>Problem 6.4 states the invariance of covariance estimators in OLS. Show that the same result holds for covariance estimators in WLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.12">Ridge with weights</head><p>Define the ridge regression with weights w i 's, and derive the the formula for the ridge coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.13">Coordinate descent algorithm in lasso with weights</head><p>Define the lasso with weights w i 's, and give the coordinate descent algorithm for solving the weighted lasso problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.14">General leave-one-out formula via WLS</head><p>With data (X, Y ), we can define β[-i] (w) as the WLS estimator of Y on X with weights w i ′ = 1(i ′ ̸ = i) + w1(i ′ = i) for i ′ = 1, . . . , n, where 0 ≤ w ≤ 1. It reduces to the OLS estimator β when w = 1 and the leave-one-out OLS estimator β[-i] when w = 0.</p><p>Show the general formula</p><formula xml:id="formula_541">β[-i] (w) = β - 1 -w 1 -(1 -w)h ii (X t X) -1 x i εi</formula><p>recalling that h ii is the leverage score and εi is the residual of observation i. Remark: Based on the above formula, we can compute the derivative of β[-i] (w) with respect to w: <ref type="bibr" target="#b193">Pregibon (1981)</ref> reviewed related formulas for OLS. <ref type="bibr" target="#b65">Broderick et al. (2020)</ref> discussed related formulas for general statistical models.</p><formula xml:id="formula_542">∂ β[-i] (w) ∂w = 1 {1 -(1 -w)h ii } 2 (X t X) -1 x i εi , which reduces to ∂ β[-i] (0) ∂w = 1 (1 -h ii ) 2 (X t X) -1 x i εi at w = 0 and ∂ β[-i] (1) ∂w = (X t X) -1 x i εi at w = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.15">Hat matrix and leverage score in WLS</head><p>Based on the WLS estimator βw = (X t W X) -1 X t W Y with W = diag(w 1 , . . . , w n ), we have the predicted vector</p><formula xml:id="formula_543">Ŷw = X βw = X(X t W X) -1 X t W Y.</formula><p>This motivates the definition of the hat matrix</p><formula xml:id="formula_544">H w = X(X t W X) -1 X t W such that Ŷw = H w Y .</formula><p>First, show the following basic properties of the hat matrix:</p><formula xml:id="formula_545">W H w = H t w W, X t W (I n -H w ) = 0.</formula><p>Second, prove an extended version of Theorem 11.1: with</p><formula xml:id="formula_546">x i = (1, x t i2 ) t , the (i, i)th diagonal element of H w satisfies h w,ii = w i n i ′ =1 w i ′ (1 + D 2 w,i ) where D 2 w,i = (x i2 -xw,2 ) t S -1 w (x i2 -xw,2 ) with xw,2 = n i=1 w i x i2 / n i=1 w i being the weighted average of x i2 's and S w = n i=1 w i (x i2 -xw,2 )(x i2 -xw,2 ) t / n i=1 w i being the corresponding sample covariance ma- trix.</formula><p>Remark: <ref type="bibr" target="#b164">Li and Valliant (2009)</ref> presented the basic properties of H w for WLS in the context of survey data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.16">Leave-one-out formula for WLS</head><p>Use the notation in Problem 19.15. Let βw be the WLS estimator of Y on X with weights w i 's. Let βw[-i] be the WLS estimator without using the ith observation. Show that</p><formula xml:id="formula_547">βw[-i] = βw - w i 1 -h w,ii (X t W X) -1 x i εw,i .</formula><p>19.17 EHW standard errors in WLS</p><p>Report the EHW standard errors in the examples in Sections 19.3.1, 19.3.2, and 19.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.18">Another example of ecological inference</head><p>The fultongen dataset in the ri package contains aggregated data from 289 precincts in Fulton County, Georgia. The variable t represents the fraction voting in 1994 and x the fraction in 1992. The variable n represents the total number of people. Run ecological regression similar to Section 19.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part VII</head><p>Generalized Linear Models 20</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic Regression for Binary Outcomes</head><p>Many applications have binary outcomes y i ∈ {0, 1}. This chapter discusses statistical models of binary outcomes, focusing on the logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.1">Regression with binary outcomes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.1.1">Linear probability model</head><p>For simplicity, we can still use the linear model for a binary outcome. It is also called the linear probability model:</p><formula xml:id="formula_548">y i = x t i β + ε i , E(ε i | x i ) = 0</formula><p>because the conditional probability of y i given x i is a linear function of x i :</p><formula xml:id="formula_549">pr(y i = 1 | x i ) = E(y i | x i ) = x t i β.</formula><p>An advantage of this linear model is that the interpretation of the coefficient remains the same as linear models for general outcomes:</p><formula xml:id="formula_550">∂pr(y i = 1 | x i ) ∂x ij = β j ,</formula><p>that is, β j measures the partial impact of x ij on the probability of y i .</p><p>A minor technical issue is that the linear probability model implies heteroskedasticity because var(</p><formula xml:id="formula_551">y i | x i ) = x t i β(1 -x t i β)</formula><p>. Therefore, we must use the EHW covariance based on OLS. We can also use FGLS to improve efficiency over OLS.</p><p>A more severe problem with the linear probability model is its plausibility in general. We may not believe that a linear model is the correct model for a binary outcome because the probability pr(y i = 1 | x i ) on the left-hand side is bounded between zero and one, but the linear combination x t i β on the right-hand side can be unbounded for general covariates and coefficient. Nevertheless, the OLS decomposition y i = x t i β + ε i works for any y i ∈ R, so it is applicable for binary y i . Sometimes, practitioners feel that the linear model is not natural for binary outcomes because the predicted value can be outside the range of [0, 1]. Therefore, it is more reasonable to build a model that automatically accommodates the binary feature of the outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.1.2">General link functions</head><p>A linear combination of general covariates may be outside the range of [0, 1], but we can find a monotone transformation to force it to lie within the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. This motivates us to consider the following model:</p><formula xml:id="formula_552">pr(y i = 1 | x i ) = g(x t i β),</formula><p>where g(•) : R → [0, 1] is a monotone function, and its inverse is often called the link function. Mathematically, the distribution function of any continuous random variable is a monotone function that maps from R to [0, 1]. So we have infinitely many choices for g(•). Four canonical choices "logit", "probit", "cauchit", and "cloglog" are below which are the standard options in R:</p><formula xml:id="formula_553">name functional form logit g(z) = e z 1+e z probit g(z) = Φ(z) cauchit g(z) = 1 π arctan(z) + 1 2 cloglog g(z) = 1 -exp(-e z )</formula><p>The above g(z)'s correspond to different distribution functions. The g(z) for the logit model<ref type="foot" target="#foot_18">foot_18</ref> is the distribution function of the standard logistic distribution with density</p><formula xml:id="formula_554">g ′ (z) = e z (1 + e z ) 2 = g(z) {1 -g(z)} .</formula><p>(20.1)</p><p>The g(z) for the probit model<ref type="foot" target="#foot_19">foot_19</ref> is the distribution function of a standard Normal distribution. The g(z) for the cauchit model is the distribution function of the standard Cauchy distribution with density</p><formula xml:id="formula_555">g ′ (z) = 1 π(1 + z 2 )</formula><p>.</p><p>The g(z) for the cloglog model is the distribution function of the standard log-Weilbull distribution with density g ′ (z) = exp(z -e z ).</p><p>I will give more motivations for the first three link functions in Section 20.7.1 and for the fourth link function in Problem 22.4. Figure <ref type="figure" target="#fig_52">20</ref>.1 shows the distributions and densities of the corresponding link functions. The distribution functions are quite similar for all links, but the density for cloglog is asymmetric although all other three densities are symmetric. This chapter will focus on the logit model, and extensions to other models are conceptually straightforward. We can also write the logit model as</p><formula xml:id="formula_556">pr(y i = 1 | x i ) ≡ π(x i , β) = e x t i β 1 + e x t i β , (20.2)</formula><p>for the conditional probability of y i given x i , or, equivalently, logit {pr(</p><formula xml:id="formula_557">y i = 1 | x i )} ≡ log pr(y i = 1 | x i ) 1 -pr(y i = 1 | x i ) = x t i β,</formula><p>for the log of the odds of y i given x i , with the logit function logit(π) = log π 1 -π . Because y i is a binary random variable, its probability completely determines its distribution. So we can also write the logit model as</p><formula xml:id="formula_558">y i | x i ∼ Bernoulli e x t i β 1 + e x t i β</formula><p>.</p><p>Each coefficient β j measures the impact of x ij on the log odds of the outcome:</p><formula xml:id="formula_559">∂ ∂x ij logit{pr(y i = 1 | x i )} = β j .</formula><p>Epidemiologists also call β j the conditional log odds ratio because</p><formula xml:id="formula_560">β j = logit {pr(y i = 1 | . . . , x ij + 1, . . .)} -logit {pr(y i = 1 | . . . , x ij , . . .)} = log pr(y i = 1 | . . . , x ij + 1) 1 -pr(y i = 1 | . . . , x ij + 1) -log pr(y i = 1 | . . . , x ij , . . .) 1 -pr(y i = 1 | . . . , x ij , . . .) = log pr(y i = 1 | . . . , x ij + 1, . . .) 1 -pr(y i = 1 | . . . , x ij + 1, . . .) pr(y i = 1 | . . . , x ij , . . .) 1 -pr(y i = 1 | . . . , x ij , . . .) ,</formula><p>that is, the change of the log odds of y i if we increase x ij by a unit holding other covariates unchanged. Qualitatively, if β j &gt; 0, then larger values of x ij lead to larger probability of y i = 1; if β j &lt; 0, then larger values of x ij lead to smaller probability of y i = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.2">Maximum likelihood estimator of the logistic model</head><p>Because we have specified a fully parametric model for y i given x i , we can estimate β using the maximum likelihood. With independent observations, the likelihood function for general binary outcomes is<ref type="foot" target="#foot_20">foot_20</ref> </p><formula xml:id="formula_561">L(β) = n i=1 f (y i | x i ) = n i=1 {π(x i , β) if y i = 1 or 1 -π(x i , β) if y i = 0} = n i=1 {π(x i , β)} yi {1 -π(x i , β)} 1-yi .</formula><p>Under the logit form <ref type="bibr">(20.2)</ref>, the likelihood function simplifies to</p><formula xml:id="formula_562">L(β) = n i=1 π(x i , β) 1 -π(x i , β) yi {1 -π(x i , β)} = n i=1 e x t i β yi 1 1 + e x t i β = n i=1 e yix t i β 1 + e x t i β .</formula><p>The log-likelihood function is</p><formula xml:id="formula_563">log L(β) = n i=1 y i x t i β -log(1 + e x t i β ) ,</formula><p>the score function is</p><formula xml:id="formula_564">∂ log L(β) ∂β = n i=1 x i y i - x i e x t i β 1 + e x t i β = n i=1 x i y i - e x t i β 1 + e x t i β = n i=1 x i {y i -g(x t i β)} = n i=1 x i {y i -π(x i , β)} ,</formula><p>and the Hessian matrix</p><formula xml:id="formula_565">∂ 2 log L(β) ∂β∂β t = ∂ 2 log L(β) ∂β j ∂β j ′ 1≤j,j ′ ≤p = - n i=1 x i ∂g(x t i β) ∂β t (20.1) = - n i=1 x i x t i g(x t i β) {1 -g(x t i β)} = - n i=1 π(x i , β) {1 -π(x i , β)} x i x t i .</formula><p>For any α ∈ R p , we have</p><formula xml:id="formula_566">α t ∂ 2 log L(β) ∂β∂β t α = - n i=1 π(x i , β) {1 -π(x i , β)} (α t x i ) 2 ≤ 0</formula><p>so the Hessian matrix is negative semi-definite. If it is negative definite, then the likelihood function has a unique maximizer.</p><p>The maximum likelihood estimate (MLE) must satisfy the following score or Normal equation:</p><formula xml:id="formula_567">n i=1 x i y i -π(x i , β) = n i=1 x i y i - e x t i β 1 + e x t i β = 0.</formula><p>If we view π(x i , β) as the fitted probability for y i , then y i -π(x i , β) is the residual, and the score equation is similar to that of OLS. Moreover, if x i contains 1, then</p><formula xml:id="formula_568">n i=1 y i -π(x i , β) = 0 =⇒ n -1 n i=1 y i = n -1 n i=1 π(x i , β),</formula><p>that is the average of the outcomes equals the average of their fitted values. However, the score equation is nonlinear, and in general, there is no explicit formula for the MLE. We usually use Newton's method to solve for the MLE based on the linearization of the score equation. Starting from the old value β old , we can approximate the score equation by a linear equation:</p><formula xml:id="formula_569">0 = ∂ log L(β) ∂β ∼ = ∂ log L(β old ) ∂β + ∂ 2 log L(β old ) ∂β∂β t (β -β old ),</formula><p>and then update</p><formula xml:id="formula_570">β new = β old - ∂ 2 log L(β old ) ∂β∂β t -1 ∂ log L(β old )</formula><p>∂β .</p><p>Using the matrix form, we can gain more insight from Newton's method. Recall that</p><formula xml:id="formula_571">Y =    y 1 . . . y n    , X =    x t 1 . . . x t n    ,</formula><p>and define</p><formula xml:id="formula_572">Π old =    π(x 1 , β old ) . . . π(x n , β old )    , W old = diag π(x i , β old ) 1 -π(x i , β old ) n i=1 .</formula><p>Then</p><formula xml:id="formula_573">∂ log L(β old ) ∂β = X t (Y -Π old ), ∂ 2 log L(β old ) ∂β∂β t = -X t W old X,</formula><p>and Newton's method simplifies to</p><formula xml:id="formula_574">β new = β old + (X t W old X) -1 X t (Y -Π old ) = (X t W old X) -1 X t W old Xβ old + X t (Y -Π old ) = (X t W old X) -1 X t W old Z old ,</formula><p>where</p><formula xml:id="formula_575">Z old = Xβ old + (W old ) -1 (Y -Π old ).</formula><p>So we can obtain β new based on the WLS fit of Z old on X with weights W old , the diagonal elements of which are the conditional variances of the y i 's given the x i 's at β old . The glm function in R uses the Fisher scoring algorithm, which is identical to Newton's method for the logit model 4 . Sometimes, it is also called the iteratively reweighted least squares algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.3">Statistics with the logit model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.3.1">Inference</head><p>Based on the general theory of MLE, β is consistent for β and is asymptotically Normal. Approximately, we can conduct statistical inference based on</p><formula xml:id="formula_576">β a ∼ N    β, - ∂ 2 log L( β) ∂β∂β t -1    = N β, (X t Ŵ X) -1 , where Ŵ = diag π(x i , β) 1 -π(x i , β) n i=1</formula><p>.</p><p>Based on this, the glm function reports the point estimate, standard error, z-value, and pvalue for each coordinate of β. It is almost identical to the output of the lm function, except that the interpretation of the coefficient becomes the conditional log odds ratio. I use the data from <ref type="bibr" target="#b139">Hirano et al. (2000)</ref> to illustrate logistic regression, where the main interest is the effect of the encouragement of receiving the flu shot via email on the binary indicator of flu-related hospitalization. We can fit a logistic regression using the glm function in R with family = binomial(link = logit). 4 The Fisher scoring algorithm uses a slightly different approximation:</p><formula xml:id="formula_577">0 = ∂ log L(β) ∂β ∼ = ∂ log L(β old ) ∂β + E ∂ 2 log L(β old ) ∂β∂β t | X (β -β old ),</formula><p>with the expected Fisher information instead of the observed Fisher information. For other link functions, the Fisher scoring algorithm is different from Newton's method. age -0 . 0 0 7 9 8 6 0 . 0 0 5 5 6 9 -1 . 4 3 4 0 . 1 5 1 5 4 copd 0 . 3 3 7 0 3 7 0 . 1 5 3 9 3 9 2 . 1 8 9 0 . 0 2 8 5 7 * dm 0 . 4 5 4 3 4 2 0 . 1 4 3 5 9 3 3 . 1 6 4 0 . 0 0 1 5 6 ** heartd 0 . 6 7 6 1 9 0 0 . 1 5 3 3 8 4 4 . 4 0 8 1 . 0 4e -0 5 *** race -0 . 2 4 2 9 4 9 0 . 1 4 3 0 1 3 -1 . 6 9 9 0 . 0 8 9 3 6 . renal 1 . 5 1 9 5 0 5 0 . 3 6 5 9 7 3 4 . 1 5 2 3 . 3 0e -0 5 *** sex -0 . 2 1 2 0 9 5 0 . 1 4 4 4 7 7 -1 . 4 6 8 0 . 1 4 2 1 0 liverd 0 . 0 9 8 9 5 7 1 . 0 8 4 6 4 4 0 . 0 9 1 0 . 9 2 7 3 1 ( Dispersion parameter for binomial family taken to be 1 )</p><p>Null deviance : 1 6 6 7 . 9 on 2 8 6 0 degrees of freedom Residual deviance : 1 5 9 8 . 4 on 2 8 5 1 degrees of freedom AIC : 1 6 1 8 . 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Fisher Scoring iterations : 5</head><p>Three subtle issues arise in the above code. First, flu = within(flu, rm(receive)) drops receive, which is the indicator of whether a patient received the flu shot or not. The reason is that assign is randomly assigned but receive is subject to selection bias, that is, patients receiving the flu shot can be quite different from patients not receiving the flu shot.</p><p>Second, the Null deviance and Residual deviance are defined as -2 log L( β) and -2 log L( β), respectively, where β is the MLE assuming that all coefficients except the intercept are zero, and β is the MLE without any restrictions. They are not of independent interest, but their difference is: Wilks' theorem ensures that</p><formula xml:id="formula_578">{-2 log L( β)} -{-2 log L( β)} = 2 log L( β) L( β) a ∼ χ 2 p-1 .</formula><p>So we can test whether the coefficients of the covariates are all zero, which is analogous to the joint F test in linear models.</p><p>&gt; pchisq ( assign . logit $ null . deviance -assign . logit $ deviance , + df = assign . logit $ df . null -assign . logit $ df . residual , + lower . tail = FALSE ) [ 1 ] 1 . 9 1 2 9 5 2e -1 1 Third, the AIC is defined as -2 log L( β) + 2p, where p is the number of parameters in the logit model. This is also the general formula of AIC for other parametric models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.3.2">Prediction</head><p>The logit model is often used for prediction or classification since the outcome is binary. With the MLE β, we can predict the probability of being one as πn+1 = g(x t n+1 β) for a unit with covariate value x n+1 , and we can easily dichotomize the fitted probability to predict the outcome itself by ŷn+1 = 1(π n+1 ≥ c), for example, with c = 0.5.</p><p>We can even quantify the uncertainty in the fitted probability based on a linear approximation (i.e., the delta method). Based on</p><formula xml:id="formula_579">πn+1 = g(x t n+1 β) ∼ = g(x t n+1 β) + g ′ (x t n+1 β)x t n+1 ( β -β) = g(x t n+1 β) + g(x t n+1 β){1 -g(x t n+1 β)}x t n+1 ( β -β),</formula><p>we can approximate the asymptotic variance of πn+1 by</p><formula xml:id="formula_580">[g(x t n+1 β){1 -g(x t n+1 β)}] 2 x t n+1 (X t Ŵ X) -1 x n+1 .</formula><p>We can use the predict function in R to calculate the predicted values based on a glm object in the same way as the linear model. If we specify type="response", then we obtain the fitted probabilities; if we specify se.fit = TRUE, then we also obtain the standard errors of the fitted probabilities. In the following, I predict the probabilities of flu-related hospitalization if a patient receives the email encouragement or not, fixing other covariates at their empirical means.</p><p>&gt; emp . mean = apply ( flu , 2 , mean ) &gt; data . ave = rbind ( emp . mean , emp . mean ) &gt; data . ave [ 1 , 1 ] = 1 &gt; data . ave <ref type="bibr">[ 2 , 1 ]</ref> = 0 &gt; data . ave = data . frame ( data . ave ) &gt; predict ( assign . logit , newdata = data . ave , + type = " response " , se . fit = TRUE ) $ fit emp . mean emp . mean . 1 0 . 0 6 9 8 1 8 2 8 0 . 0 8 3 7 8 8 1 8 $ se . fit emp . mean emp . mean . 1 0 . 0 0 6 6 8 9 6 6 5 0 . 0 0 7 5 2 6 3 0 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.4">More on interpretations of the coefficients</head><p>Many practitioners find the coefficients in the logit model difficult to interpret. Another measure of the impact of the covariate on the outcome is the average marginal effect or average partial effect. For a continuous covariate x ij , the average marginal effect is defined as</p><formula xml:id="formula_581">ame j = n -1 n i=1 ∂pr(y i = 1 | x i ) ∂x ij = n -1 n i=1 g ′ (x t i β)β j ,</formula><p>which reduces to the following form for the logit model</p><formula xml:id="formula_582">ame j = β j × n -1 n i=1 π(x i , β) {1 -π(x i , β)} .</formula><p>For a binary covariate x ij , the average marginal effect is defined as</p><formula xml:id="formula_583">ame j = n -1 n i=1 {pr(y i = 1 | . . . , x ij = 1, . . .) -pr(y i = 1 | . . . , x ij = 0, . . .)}</formula><p>The margins function in the margins package can compute the average marginal effects and the corresponding standard errors. In particular, the average marginal effect of assign is not significant as shown below. The R code in this section is in code19.3.R.</p><p>&gt; library ( " margins " ) &gt; ape = margins ( assign . logit ) &gt; summary ( ape ) factor AME SE z p lower upper age -0 . 0 0 0 6 0 . 0 0 0 4 -1 . 4 3 2 2 0 . 1 5 2 1 -0 . 0 0 1 4 0 . 0 0 0 2 assign -0 . 0 1 5 0 0 . 0 1 0 3 -1 . 4 4 8 0 0 . 1 4 7 6 -0 . 0 3 5 2 0 . 0 0 5 3 copd 0 . 0 2 5 5 0 . 0 1 1 7 2 . 1 8 3 0 0 . 0 2 9 0 0 . 0 0 2 6 0 . 0 4 8 5 dm 0 . 0 3 4 4 0 . 0 1 0 9 3 . 1 4 6 5 0 . 0 0 1 7 0 . 0 1 3 0 0 . 0 5 5 9 heartd 0 . 0 5 1 2 0 . 0 1 1 8 4 . 3 4 4 1 0 . 0 0 0 0 0 . 0 2 8 1 0 . 0 7 4 3 liverd 0 . 0 <ref type="table">0 7 5 0 . 0 8 2 2 0 . 0 9 1 2 0 . 9 2 7 3 -0 . 1 5 3 6 0 . 1 6 8 6</ref> race -0 . 0 1 8 4 0 . 0 1 0 9 -1 . 6 9 5 8 0 . 0 8 9 9 -0 . 0 3 9 7 0 . 0 0 2 9 renal 0 . <ref type="table">1 1 5 1 0 . 0 2 7 8 4 . 1 4 6 1 0 . 0 0 0 0 0 . 0 6 0 7 0 . 1 6 9 6</ref> sex -0 . 0 1 6 1 0 . 0 1 1 0 -1 . 4 6 6 0 0 . 1 4 2 6 -0 . 0 3 7 6 0 . 0 0 5 4</p><p>The interaction term is much more complicated. Contradictory suggestions are given across fields. Consider the following model</p><formula xml:id="formula_584">pr(y i = 1 | x i1 , x i2 ) = g(β 0 + β 1 x i1 + β 2 x i2 + β 12 x i1 x i2 ).</formula><p>If the link is logit, then epidemiologists interpret e β12 as the interaction between x i1 and x i2 on the odds ratio scale. Consider a simple case with binary x i1 and x i2 . Given x i2 = 1, the odds ratio of x i1 on y i equals e β1+β12 ; given x i2 = 0, the odds ratio of x i1 on y i equals e β1 . Therefore, the ratio of the two odds ratio equals e β12 . When we measure effects on the odds ratio scale, the logistic model is a natural choice. The interaction term in the logistic model indeed measures the interaction of x i1 and x i2 . <ref type="bibr" target="#b46">Ai and Norton (2003)</ref> gave a different suggestion. Define</p><formula xml:id="formula_585">z i = β 0 + β 1 x i1 + β 2 x i2 + β 12 x i1 x i2 .</formula><p>We have two ways to define the interaction effect: first,</p><formula xml:id="formula_586">n -1 n i=1 ∂pr(y i = 1 | x i1 , x i2 ) ∂(x i1 x i2 ) = n -1 n i=1 g ′ (z i )β 12 .</formula><p>second,</p><formula xml:id="formula_587">n -1 n i=1 ∂ 2 pr(y i = 1 | x i1 , x i2 ) ∂x i1 ∂x i2 = n -1 n i=1 ∂ ∂x i2 ∂pr(y i = 1 | x i1 , x i2 ) ∂x i1 = n -1 n i=1 ∂ ∂x i2 {g ′ (z i )(β 1 + β 12 x i2 )} = n -1 n i=1 {g ′′ (z i )(β 2 + β 12 x i1 )(β 1 + β 12 x i2 ) + g ′ (z i )β 12 } ;</formula><p>Although the first one is more straightforward based on the definition of the average partial effect, the second one is more reasonable based on the natural definition of interaction based on the mixed derivative. Note that even if β 12 = 0, the second definition of interaction does not necessarily equal 0 since</p><formula xml:id="formula_588">n -1 n i=1 ∂ 2 pr(y i = 1 | x i1 , x i2 ) ∂x i1 ∂x i2 = n -1 n i=1 g ′′ (z i )β 1 β 2 .</formula><p>This is due to the nonlinearity of the link function. The second definition quantifies interaction based on the probability itself while the parameters in the logistic model measure the odds ratio. This combination of model and parameter does not seem a natural choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.5">Does the link function matter?</head><p>First, I generate data from a simple one-dimensional logistic model.</p><p>&gt; n = 1 0 0 &gt; x = rnorm (n , 0 , 3 ) &gt; prob = 1 /( 1 + exp ( -1 + x )) &gt; y = rbinom (n , 1 , prob )</p><p>Then I fit the data with the linear probability model and binary models with four link functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&gt; lpmfit</head><p>= lm ( y ~x ) &gt; probitfit = glm ( y ~x , family = binomial ( link = " probit " )) Warning message : glm . fit : fitted probabilities numerically 0 or 1 occurred &gt; logitfit = glm ( y ~x , family = binomial ( link = " logit " )) &gt; cloglogfit = glm ( y ~x , family = binomial ( link = " cloglog " )) Warning message : glm . fit : fitted probabilities numerically 0 or 1 occurred &gt; cauchitfit = glm ( y ~x , family = binomial ( link = " cauchit " ))</p><p>The coefficients are quite different because the coefficients measure the association between x and y on difference scales. These parameters are not directly comparable.</p><formula xml:id="formula_589">&gt; betacoef = c ( lpmfit $ coef [ 2 ] , + probitfit $ coef [ 2 ] , + logitfit $ coef [ 2 ] , + cloglogfit $ coef [ 2 ] , + cauchitfit $ coef [ 2 ]</formula><p>) &gt; names ( betacoef ) = c ( " lpm " , " probit " , " logit " , " cloglog " , " cauchit " ) &gt; round <ref type="bibr">( betacoef , 2 )</ref> lpm probit logit cloglog cauchit -0 . 1 0 -0 . 8 3 -1 . 4 7 -1 . 0 7 -2 . 0 9</p><p>However, if we care only about the prediction, then these five models give very similar results.</p><p>&gt; table (y , lpmfit $ fitted . values &gt; 0 . 5 ) y FALSE TRUE 0 3 1 9 1 5 5 5 &gt; table (y , probitfit $ fitted . values &gt; 0 . 5 ) y FALSE TRUE 0 3 1 9 1 5 5 5 &gt; table (y , logitfit $ fitted . values &gt; 0 . 5 ) y FALSE TRUE 0 3 1 9 1 5 5 5 &gt; table (y , cloglogfit $ fitted . values &gt; 0 . 5 ) y FALSE TRUE 0 3 4 6 1 7 5 3 &gt; table (y , cauchitfit $ fitted . values &gt; 0 . 5 ) y FALSE TRUE 0 3 4 6 1 7 5 3</p><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq lpm probit logit cloglog cauchit 0.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0. An interesting fact is that the coefficients from the logit model approximately equal those from the probit model multiplied by 1.7, a constant that minimizes max y |g logit (by)g probit (y)|. We can easily compute this constant numerically: Based on the above calculation, the maximum difference is approximately 0.009. Therefore, the logit and probit link functions are extremely close up to the scaling factor 1.7. However, min b max y |g logit (by) -g * (y)| is much larger for the link functions of cauchit and cloglog. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6">Extensions of the logistic regression</head><formula xml:id="formula_590">where ℓ i (β) = y i (β 0 + β 1 x i1 + • • • + β p x ip ) -log(1 + e β0+β1xi1+•••+βpxip )</formula><p>is the log-likelihood function. When α = 1, it gives the ridge analog of the logistic regression; when α = 0, it gives the lasso analog; when α ∈ (0, 1), it gives the elastic net analog. The R package glmpath uses the coordinate descent algorithm based on a quadratic approximation of the log-likelihood function. We can select the tuning parameter λ based on cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6.2">Case-control study</head><p>A nice property of the logit model is that it works not only for the cohort study with data from conditional distribution y i | x i but also for the case-control study with data from the conditional distribution x i | y i . The former is a prospective study while the latter is a retrospective study. Below, I will explain the basic idea in <ref type="bibr" target="#b194">Prentice and Pyke (1979)</ref>.</p><p>Assume that (x i , y i , s i ) IID with</p><formula xml:id="formula_591">pr(y i = 1 | x i ) = e β0+x t i β 1 + e β0+x t i β (20.3) and pr(s i = 1 | x i , y i ) = pr(s i = 1 | y i ) = p 1 , if y i = 1, p 0 , if y i = 0. (20.4)</formula><p>But we only have data with s i = 1 with p 1 and p 0 often unknown. Fortunately, conditioning on s i = 1, we have the following result.</p><p>Theorem 20.1 Under (20.3) and (20.4), we have</p><formula xml:id="formula_592">pr(y i = 1 | x i , s i = 1) = e δ+β0+x t i β</formula><p>1 + e δ+β0+x t i β , where δ = log(p 1 /p 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 20.1: We have pr(y</head><formula xml:id="formula_593">i = 1 | x i , s i = 1) = pr(y i = 1 | x i )pr(s i = 1 | x i , y i = 1) pr(y i = 1 | x i )pr(s i = 1 | x i , y i = 1) + pr(y i = 0 | x i )pr(s i = 1 | x i , y i = 0)</formula><p>by Bayes' formula. Under the logit model, we have</p><formula xml:id="formula_594">pr(y i = 1 | x i , s i = 1) = e β 0 +x t i β 1+e β 0 +x t i β p 1 e β 0 +x t i β 1+e β 0 +x t i β p 1 + 1 1+e β 0 +x t i β p 0 = e β0+x t i β p 1 e β0+x t i β p 1 + p 0 = e β0+x t i β p 1 /p 0 e β0+x t i β p 1 /p 0 + 1 = e δ+β0+x t i β 1 + e δ+β0+x t i β .</formula><p>□ Theorem 20.1 ensures that conditioning on s i = 1, the model of y i given x i is still logit with the intercept changing from β 0 to β 0 + log(p 1 /p 0 ). Although we cannot consistently estimate the intercept without knowing (p 1 , p 0 ), we can still estimate all the slopes. <ref type="bibr" target="#b151">Kagan (2001)</ref> showed that the logistic link is the only one that enjoys this property. <ref type="bibr" target="#b206">Samarani et al. (2019)</ref> hypothesized that variation in the inherited activating Killer-cell Immunoglobulin-like Receptor genes in humans is associated with their innate susceptibility/resistance to developing Crohn's disease. They used a case-control study from three cities <ref type="bibr">(Manitoba, Montreal, and Ottawa)</ref> in Canada to investigate the potential association. </p><formula xml:id="formula_595">y * i = x t i β + ε i</formula><p>and -ε i has distribution function g(•) and is independent of x i . From this latent linear model, we can verify that pr(</p><formula xml:id="formula_596">y i = 1 | x i ) = pr(y * i ≥ 0 | x i ) = pr(x t i β + ε i ≥ 0 | x i ) = pr(-ε i ≤ x t i β | x i ) = g(x t i β).</formula><p>So the g(•) function can be interpreted as the distribution function of the error term in the latent linear model. This latent variable formulation provides another way to interpret the coefficients in the models for binary data. It is a powerful way to generate models for more complex data. We will see another example in the next chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.7.2">Inverse model</head><p>Assume that y i ∼ Bernoulli(q), (20.5) and <ref type="bibr">(20.6)</ref> where x i does not contain 1. This is called the linear discriminant model. We can verify that y i | x i follows a logit model as shown in the theorem below. </p><formula xml:id="formula_597">x i | y i = 1 ∼ N(µ 1 , Σ), x i | y i = 0 ∼ N(µ 0 , Σ),</formula><formula xml:id="formula_598">logit{pr(y i = 1 | x i )} = α + x t i β,</formula><p>where</p><formula xml:id="formula_599">α = log q 1 -q - 1 2 µ t 1 Σ -1 µ 1 -µ t 0 Σ -1 µ 0 , β = Σ -1 (µ 1 -µ 0 ) . (20.7)</formula><p>Proof of Theorem 20.2: Using Bayes' formula, we have</p><formula xml:id="formula_600">pr(y i = 1 | x i ) = pr(y i = 1, x i ) pr(x i ) = pr(y i = 1)pr(x i | y i = 1) pr(y i = 1)pr(x i | y i = 1) + pr(y i = 0)pr(x i | y i = 0) = e ∆ 1 + e ∆ , where ∆ = log pr(y i = 1)pr(x i | y i = 1) pr(y i = 0)pr(x i | y i = 0) = log q {(2π) p det(Σ)} -1/2 exp -(x i -µ 1 ) t Σ -1 (x i -µ 1 )/2 (1 -q) {(2π) p det(Σ)} -1/2 exp {-(x i -µ 0 ) t Σ -1 (x i -µ 0 )/2} = log q exp -(x i -µ 1 ) t Σ -1 (x i -µ 1 )/2 (1 -q) exp {-(x i -µ 0 ) t Σ -1 (x i -µ 0 )/2} = log q exp --2x t i Σ -1 µ 1 + µ t 1 Σ -1 µ 1 /2 (1 -q) exp {-(-2x t i Σ -1 µ 0 + µ t 0 Σ -1 µ 0 ) /2} = log q 1 -q - 1 2 µ t 1 Σ -1 µ 1 -µ t 0 Σ -1 µ 0 + x t i Σ -1 (µ 1 -µ 0 ) .</formula><p>So y i | x i follows a logistic model with α and β given in <ref type="bibr">(21.12)</ref>. □ We can easily obtain the moment estimators for the unknown parameters under (21.10) and <ref type="bibr">(21.11)</ref>. Let n 1 = n i=1 y i and n 0 = n -n 1 . The moment estimator for q is q = n 1 /n, the sample mean of the y i 's. The moment estimators for µ 1 and µ 0 are</p><formula xml:id="formula_601">μ1 = n -1 1 n i=1 y i x i , μ0 = n -1 0 n i=1 (1 -y i )x i ,</formula><p>the sample means of the x i 's for units with y i = 1 and y i = 0, respectively. The moment estimator for Σ is</p><formula xml:id="formula_602">Σ = n i=1 y i (x i -μ1 )(x i -μ1 ) t + n i=1 (1 -y i )(x i -μ0 )(x i -μ0 ) t /(n -2),</formula><p>the pooled covariance matrix, after centering the x i 's by the y-specific means. Based on Theorem 20.2, we can obtain estimates α and β by replacing the true parameters with their moment estimators. This gives us another way to fit the logistic model. <ref type="bibr" target="#b95">Efron (1975)</ref> compared the above moment estimator and the MLE under the logistic model. Since the linear discriminant model imposes stronger assumptions, the estimator based on Theorem 20.2 is more efficient. In contrast, the MLE of the logistic model is more robust because it does not impose the Normality assumption on x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.8">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.1">Invariance of logistic regression</head><p>This problem extends Problem 3.4.</p><p>Assume xi = x i Γ with an invertible Γ. Run logistic regression of y i 's on x i 's to obtain the coefficient β and fitted probabilities πi 's. Run another logistic regression of y i 's on xi 's to obtain the coefficient β and fitted probabilities πi 's.</p><p>Show that β = Γ β and πi = πi for all i's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.2">Two logistic regressions</head><p>This is an extension of Problem 17.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Model and Extensions</head><p>Given data (x i , z i , y i ) n i=1 where x i denotes the covariates, z i ∈ {1, 0} denotes the binary group indicator, and y i denotes the binary outcome. We can fit two separate logistic regressions:</p><p>logit{pr</p><formula xml:id="formula_603">(y i = 1 | z i = 1, x i )} = γ 1 + x t i β 1 and logit{pr(y i = 1 | z i = 0, x i )} = γ 0 + x t i β 0</formula><p>with the treated and control data, respectively. We can also fit a joint logistic regression using the pooled data:</p><formula xml:id="formula_604">logit{pr(y i = 1 | z i , x i )} = α 0 + α z z i + x t i α x + z i x t i α zx .</formula><p>Let the parameters with hats denote the MLEs, for example, γ1 is the MLE for γ 1 . Find (α 0 , αz , αx , αzx ) in terms of (γ 1 , β1 , γ0 , β0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.3">Likelihood for Probit model</head><p>Write down the likelihood function for the Probit model, and derive the steps for Newton's method and Fisher scoring for computing the MLE. How do we estimate the asymptotic covariance matrix of the MLE?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.4">Logit and general exponential family</head><p>Efron <ref type="bibr">(1975)</ref> pointed out an extension of Theorem 20.2. Show that under (21.10) and</p><formula xml:id="formula_605">f (x i | y i = y) = g(θ y , η)h(x i , η) exp(x t i θ y ), (y = 0, 1)</formula><p>with parameters (θ 1 , θ 0 , η), we have</p><formula xml:id="formula_606">logit{pr(y i = 1 | x i )} = α + x t i β.</formula><p>Find the formulas of α and β in terms of (θ 1 , θ 0 , η).</p><p>Hint: As a sanity check, you can compare this problem with Theorem 20.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.5">Empirical comparison of logistic regression and linear discriminant analysis</head><p>Compare the performance of logistic regression and linear discriminant analysis in terms of prediction accuracy. You should simulate at least three cases: (1) the model for linear discriminant analysis is correct;</p><p>(2) the model for linear discriminant analysis is incorrect but the model for logistic regression is correct;</p><p>(3) the model for logistic regression is incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6">Quadratic discriminant analysis</head><p>Assume that y i ∼ Bernoulli(q), and</p><formula xml:id="formula_607">x i | y i = 1 ∼ N(µ 1 , Σ 1 ), x i | y i = 0 ∼ N(µ 0 , Σ 0 ),</formula><p>where</p><formula xml:id="formula_608">x i ∈ R p does not contain 1. Prove that logit{pr(y i = 1 | x i )} = α + x t i β + x t i Λx i , where α = log q 1 -q - 1 2 log det(Σ 1 ) det(Σ 0 ) - 1 2 (µ t 1 Σ -1 1 µ 1 -µ t 0 Σ -1 0 µ 0 ), β = Σ -1 1 µ 1 -Σ -1 0 µ 0 , Λ = - 1 2 (Σ -1 1 -Σ -1 0 ).</formula><p>Remark: This problem extends the linear discriminant model in Section 20.7.2 to the quadratic discriminant model by allowing for heteroskedasticity in the conditional Normality of x given y. It implies the logistic model with the linear, quadratic, and interaction terms of the basic covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.7">Logit and other links</head><p>Compute the minimizer and minimum value of max y |g logit (by) -g * (y)| for * = cauchit and cloglog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.8">Data analysis</head><p>Reanalyze the data in Section 20.6.2, stratifying the analysis based on center. Do the results vary significantly across centers?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.9">R 2 in logistic regression</head><p>Recall that R 2 in the linear model measures the linear dependence of the outcome on the covariates. However, the definition of R 2 is not obvious in the logistic model. The glm function in R does not return any R 2 for the logistic regression.</p><p>Recall the following equivalent definitions of R 2 in the linear model</p><formula xml:id="formula_609">R 2 = n i=1 (ŷ i -ȳ) 2 n i=1 (y i -ȳ) 2 = 1 - n i=1 (y i -ŷi ) 2 n i=1 (y i -ȳ) 2 = ρ2 y ŷ = ( n i=1 (y i -ȳ)(ŷ i -ȳ)) 2 n i=1 (y i -ȳ) 2 n i=1 (ŷ i -ȳ) 2 .</formula><p>The fitted values are πi = π(x i , β) in the logistic model, which have mean ȳ with the intercept included in the model. Analogously, we can define R 2 in the logistic model as</p><formula xml:id="formula_610">R 2 model = ss m ss t , R 2 residual = 1 - ss r ss t , R 2 correlation = ρ2 y π = C 2 y π ss t ss m ,</formula><p>where</p><formula xml:id="formula_611">ss t = n i=1 (y i -ȳ) 2 , ss m = n i=1 (π i -ȳ) 2 , ss r = n i=1 (y i -πi ) 2 , C y π = n i=1 (y i -ȳ)(π i -ȳ).</formula><p>These three definitions are not equivalent in general. In particular, R 2 model differs from R 2 residual since ss t = ss m + ss r + 2C επ where</p><formula xml:id="formula_612">C επ = n i=1 (y i -πi )(π i -ȳ). 1. Prove that R 2 model ≥ 0, R 2 correlation ≥ 0 with equality holding if πi = ȳ for all i. Prove that R 2 model ≤ 1, R 2 residual ≤ 1, R 2</formula><p>correlation ≤ 1 with equality holding if y i = πi for all i. Note that R 2 residual may be negative. Give an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Define</head><formula xml:id="formula_613">π1 = n i=1 y i πi n i=1 y i , π0 = n i=1 (1 -y i )π i n i=1</formula><p>(1 -y i ) as the average of the fitted values for units with y i = 1 and y i = 0, respectively. Define</p><formula xml:id="formula_614">D = π1 -π0 . Prove that D = (R 2 model + R 2 residual )/2 = R 2 model R 2 correlation .</formula><p>Further prove that D ≥ 0 with equality holding if πi = ȳ for all i, and D ≤ 1 with equality holding if y i = πi for all i.</p><p>3. <ref type="bibr" target="#b178">McFadden (1974)</ref> defined the following R 2 :</p><formula xml:id="formula_615">R 2 mcfadden = 1 - log L( β) log L( β)</formula><p>recalling that β is the MLE assuming that all coefficients except the intercept are zero, and β is the MLE without any restrictions. This R 2 must lie within [0, 1]. Verify that under the Normal linear model, the above formula does not reduce to the usual R 2 .</p><p>4. <ref type="bibr" target="#b85">Cox and Snell (1989)</ref> defined the following R 2 :</p><formula xml:id="formula_616">R 2 CS = 1 - L( β) L( β) 2/n</formula><p>.</p><p>Verify that under the Normal linear model, the above formula reduces to the usual R 2 .</p><p>Remark: <ref type="bibr" target="#b222">Tjur (2009)</ref> gave an excellent discussion of R 2 model , R 2 residual , R 2 correlation and D. <ref type="bibr" target="#b182">Nagelkerke (1991)</ref> pointed out that the upper bound of this R 2 CS is 1 -(L( β)) 2/n &lt; 1 and proposed to modify it as</p><formula xml:id="formula_617">R 2 nagelkerke = 1 -L( β) L( β) 2/n 1 -L( β) 2/n</formula><p>to ensure that its upper bound is 1. However, this modification seems purely ad hoc. Although D is an appealing definition of R 2 for the logistic model, it does not generalize to other models. Overall, I feel that R 2 correlation is a better definition that easily generalizes to other models. <ref type="bibr" target="#b249">Zhang (2017)</ref> defined an R 2 based on the variance function of the outcome for generalized linear models including the binary logistic model. <ref type="bibr" target="#b145">Hu et al. (2006)</ref> studied the asymptotic properties of some of the R 2 s above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic Regressions for Categorical Outcomes</head><p>Categorical outcomes are common in empirical research. The first type of categorical outcome is nominal. For example, the outcome denotes the preference for fruits (apple, orange, and pear) or transportation services (Uber, Lyft, or BART). The second type of categorical outcome is ordinal. For example, the outcome denotes the course evaluation at Berkeley (1, 2, . . . , 7) or Amazon review (1 to 5 stars). This chapter discusses statistical modeling strategies for categorical outcomes, including two classes of models corresponding to the nominal and ordinal outcomes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.1">Multinomial distribution</head><p>A categorical random variable y taking values in {1, . . . , K} with probabilities pr(y</p><formula xml:id="formula_618">= k) = π k (k = 1, . . . , K) is often called a multinomial distribution, denoted by y ∼ Multinomial {1; (π 1 , . . . , π K )} ,<label>(21.1)</label></formula><p>where K k=1 π k = 1. We can calculate the mean and covariance matrix of y: Proposition 21.1 If y is the Multinomial random variable in <ref type="bibr">(21.1)</ref>, then (1(y = 1), . . . , 1(y = K -1)) has mean (π 1 , . . . , π K-1 ) and covariance matrix</p><formula xml:id="formula_619">     π 1 (1 -π 1 ) -π 1 π 2 • • • -π 1 π K-1 -π 1 π 2 π 2 (1 -π 2 ) • • • -π 2 π K-1 . . . . . . . . . . . . -π 1 π K-1 -π 2 π K-1 • • • π K-1 (1 -π K-1 )      . (21.2)</formula><p>As a byproduct, we know that the matrix in (21.2) is positive semi-definite.</p><p>Proof of Proposition 21.1: Without loss of generality, I will calculate the (1, 1)th and the (1, 2)th element of the matrix. First, 1(y = 1) is Bernoulli with probability π 1 , so the (1, 1)th element equals var(1(y = 1)) = π 1 (1 -π 1 ). Similarly, the (2, 2)th element equals var(1</p><formula xml:id="formula_620">(y = 2)) = π 2 (1 -π 2 ). Second, 1(y = 1)+1(y = 2) is Bernoulli with probability π 1 +π 2 , so var(1(y = 1)+1(y = 2)) = (π 1 + π 2 )(1 -π 1 -π 2 ). Therefore, the (1, 2)-th element equals cov(1(y = 1), 1(y = 2)) = {var(1(y = 1) + 1(y = 2)) -var(1(y = 1)) -var(1(y = 2))} /2 = -π 1 π 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>With independent samples of (x i , y i ) n i=1 , we want to model y i based on covariates x i<ref type="foot" target="#foot_21">foot_21</ref> :</p><formula xml:id="formula_621">y i | x i ∼ Multinomial [1; {π 1 (x i ), . . . , π K (x i )}] ,</formula><p>where K k=1 π k (x i ) = 1 for all x i . We can write the probability mass function of pr(y i | x i ) as yi=k) .</p><formula xml:id="formula_622">π yi (x i ) = K k=1 {π k (x i ) if y i = k} = K k=1 {π k (x i )} 1(</formula><p>Here π k (x i ) is a general function of x i . The remaining parts of this chapter will discuss the canonical choices of π k (x i ) for nominal and ordinal outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2">Multinomial logistic model for nominal outcomes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2.1">Modeling</head><p>Viewing category K as the reference level, we can model the ratio of the probabilities of categories k and K as log</p><formula xml:id="formula_623">π k (x i ) π K (x i ) = x t i β k (k = 1, . . . , K -1)</formula><p>which implies that</p><formula xml:id="formula_624">π k (x i ) = π K (x i )e x t i β k (k = 1, . . . , K -1).</formula><p>Due to the normalization, we have</p><formula xml:id="formula_625">K k=1 π k (x i ) = 1 =⇒ K k=1 π K (x i )e x t i β k = 1 =⇒ π K (x i ) K k=1 e x t i β k = 1 =⇒ π K (x i ) = 1 K k=1 e x t i β k =⇒ π k (x i ) = e x t i β k K l=1 e x t i β l (k = 1, . . . , K -1).</formula><p>A more compact form is</p><formula xml:id="formula_626">π k (x i ) = π k (x i , β) = e x t i β k K l=1 e x t i β l , (k = 1, . . . , K) (21.3)</formula><p>where β = (β 1 , . . . , β K-1 ) denotes the parameter with β K = 0 for the reference category.</p><p>From the ratio form of ( <ref type="formula" target="#formula_618">21</ref>.3), we can only identify β k -β K for all k = 1, . . . , K. So for convenience, we impose the restriction β K = 0. Model <ref type="bibr">(21.</ref>3) is called the multinomial logistic regression model.</p><p>Similar to the binary logistic regression model, we can interpret the coefficients as the conditional log odds ratio compared to the reference level:</p><formula xml:id="formula_627">β k,j = log π k (. . . , x ij + 1, . . .) π K (. . . , x ij + 1, . . .) -log π k (. . . , x ij , . . .) π K (. . . , x ij , . . .) = log π k (. . . , x ij + 1, . . .) π K (. . . , x ij + 1, . . .)</formula><p>π k (. . . , x ij , . . .) π K (. . . , x ij , . . .) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2.2">MLE</head><p>The likelihood function for the multinomial logistic model is</p><formula xml:id="formula_628">L(β) = n i=1 K k=1 {π k (x i )} 1(yi=k) = n i=1 K k=1 e x t i β k K l=1 e x t i β l 1(yi=k) = n i=1 K k=1 e 1(yi=k)x t i β k K k=1 e x t i β k ,</formula><p>and the log-likelihood function is</p><formula xml:id="formula_629">log L(β) = n i=1 K k=1 1(y i = k)x t i β k -log K k=1 e x t i β k .</formula><p>The score function is</p><formula xml:id="formula_630">∂ log L(β) ∂β =     ∂ log L(β) ∂β1</formula><p>. . .</p><formula xml:id="formula_631">∂ log L(β) ∂β K-1     ∈ R p(K-1)</formula><p>with</p><formula xml:id="formula_632">∂ log L(β) ∂β k = n i=1 x i 1(y i = k) - x i e x t i β k K l=1 e x t i β l = n i=1 x i 1(y i = k) - e x t i β k K l=1 e x t i β l = n i=1 x i {1(y i = k) -π k (x i , β)} ∈ R p , (k = 1, . . . , K -1).</formula><p>The Hessian matrix is</p><formula xml:id="formula_633">∂ 2 log L(β) ∂β∂β t =         ∂ 2 log L(β) ∂β1∂β t 1 ∂ 2 log L(β) ∂β1∂β t 2 • • • ∂ 2 log L(β) ∂β1∂β t K-1 ∂ 2 log L(β) ∂β2∂β t 1 ∂ 2 log L(β) ∂β2∂β t 2 • • • ∂ 2 log L(β) ∂β2∂β t K-1 . . . . . . . . . ∂ 2 log L(β) ∂β K-1 ∂β t 1 ∂ 2 log L(β) ∂β K-1 ∂β t 2 • • • ∂ 2 log L(β) ∂β K-1 ∂β t K-1         ∈ R p(K-1)×p(K-1) (21.4) with the (k, k)th block ∂ 2 log L(β) ∂β k ∂β t k = - n i=1 x i ∂ ∂β t k e x t i β k K l=1 e x t i β l = - n i=1 x i x t i e x t i β k K l=1 e x t i β l -e x t i β k e x t i β k ( K l=1 e x t i β l ) 2 = - n i=1 π k (x i , β) {1 -π k (x i , β)} x i x t i ∈ R p×p (k = 1, . . . , K -1)</formula><p>and the (k, l)th block</p><formula xml:id="formula_634">∂ 2 log L(β) ∂β k ∂β t l = - n i=1 x i ∂ ∂β t l e x t i β k K l=1 e x t i β l = - n i=1 x i x t i -e x t i β k e x t i β l ( K l=1 e x t i β l ) 2 = n i=1 π k (x i , β)π l (x i , β)x i x t i ∈ R p×p (k ̸ = l : k, l = 1, . . . , K -1).</formula><p>We can verify that the Hessian matrix is negative semi-definite based on Proposition 21.1, which is left as Problem 21.2.</p><p>In R, the function multinom in the nnet package uses Newton's method to fit the multinomial logistic model. We can make inference about the parameters based on the asymptotic Normality of the MLE. Based on a new observation with covariate x n+1 , we can make prediction based on the fitted probabilities π k (x n+1 , β), and furthermore classify it into K categories based on ŷn+1 = arg max 1≤k≤K π k (x n+1 , β).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.3">A latent variable representation for the multinomial logistic regression</head><p>We can view the multinomial logistic regression <ref type="bibr">(21.</ref>3) as an extension of the binary logistic regression model. The binary logistic regression has a latent variable representation as shown in Section 20.7.1. The multinomial logistic regression also has a latent variable representation below.</p><formula xml:id="formula_635">Assume        U i1 = x t i β 1 + ε i1 , . . . U iK = x t i β K + ε iK</formula><p>, where ε i1 , . . . , ε iK are IID standard Gumbel random variables 2 . Using the language of economics, (U i1 , . . . , U iK ) are the utilities associated with the choices (1, . . . , K). So unit i chooses k if k has the highest utility:</p><formula xml:id="formula_636">y i = k if U ik &gt; U il for all l ̸ = k.</formula><p>2 See Section B.1.3 for a review.</p><p>We can show that this latent variable model implies <ref type="bibr">(21.3)</ref>. This follows from the lemma below, which is due to <ref type="bibr" target="#b178">McFadden (1974)</ref> 3 . When K = 2, it also gives another latent variable representation for the binary logistic regression, which is different from the one in Section 20.7.1.</p><formula xml:id="formula_637">Lemma 21.1 Assume        U 1 = V 1 + ε 1 , . . . U K = V K + ε K ,</formula><p>where ε 1 , . . . , ε K are IID standard Gumbel. Define</p><formula xml:id="formula_638">y = arg max 1≤l≤K U l</formula><p>as the index corresponding to the maximum of the U k 's. We have</p><formula xml:id="formula_639">pr(y = k) = e V k K l=1 e V l .</formula><p>Proof of Lemma 21.1: Recall that the standard Gumbel random variable has CDF F (z) = exp(-e -z ) and density f (z) = exp(-e -z )e -z . The event "y = k" is equivalent to the event "U k &gt; U l for all l ̸ = k", so</p><formula xml:id="formula_640">pr(y = k) =pr(U k &gt; U l , l = 1, . . . , k -1, k + 1, . . . , K) =pr(V k + ε k &gt; V l + ε l , l = 1, . . . , k -1, k + 1, . . . , K) = ∞ -∞ pr(V k + z &gt; V l + ε l , l = 1, . . . , k -1, k + 1, . . . , K)f (z)dz</formula><p>where the last line follows from conditioning on ε k . By independence of the ε's, we have</p><formula xml:id="formula_641">pr(y = k) = ∞ -∞ l̸ =k pr(ε l &lt; V k -V l + z)f (z)dz = ∞ -∞ l̸ =k exp(-e -V k +V l -z ) exp(-e -z )e -z dz = ∞ -∞ exp   - l̸ =k e -V k +V l e -z   exp(-e -z )e -z dz.</formula><p>Changing of variables t = e -z with dz = -1/tdt, we obtain</p><formula xml:id="formula_642">pr(y = k) = ∞ 0 exp   -t l̸ =k e -V k +V l   exp(-t)dt = ∞ 0 exp(-tC k )dt where C k = 1 + l̸ =k e -V k +V l .</formula><p>3 Daniel McFadden shared the 2000 Nobel Memorial Prize in Economic Sciences with James Heckman.</p><p>The integral simplifies to 1/C k due to the density of the exponential distribution. Therefore,</p><formula xml:id="formula_643">pr(y = k) = 1 1 + l̸ =k e -V k +V l = e V k e V k + l̸ =k e V l = e V k K l=1 e V l</formula><p>. □ This lemma is remarkable. It extends to more complex utility functions. I will use it again in Section 21.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.4">Proportional odds model for ordinal outcomes</head><p>For ordinal outcomes, we can still use the multinomial logistic model, but by doing this, we discard the ordering information in the outcome. Consider a simple case with a scalar x i , the multinomial logistic model does not rule out the possibility that β k &lt; 0 and β k+1 &gt; 0, which implies that x i increases the probability of category k + 1 but decreases the probability of category k. However, the outcome must first reach level k and then increase to level k + 1. If this happens, it may be hard to interpret the model.</p><p>Motivated by the latent linear representation of the binary logistic model, we imagine that the ordinal outcome arises from discretizing a continuous latent variable:</p><formula xml:id="formula_644">y * i = x t i β + ε i , pr(ε i ≤ z | x i ) = g(z)<label>(21.5)</label></formula><p>and</p><formula xml:id="formula_645">y i = k, if α k-1 &lt; y * i ≤ α k , (k = 1, . . . , K)<label>(21.6)</label></formula><p>where</p><formula xml:id="formula_646">-∞ = α 0 &lt; α 1 &lt; • • • &lt; α K-1 &lt; α K = ∞.</formula><p>Figure <ref type="figure" target="#fig_110">21</ref>.1 illustrates the data generating process with K = 4. The unknown parameters are (β, α 1 , . . . , α K-1 ). The distribution of the latent error term g(•) is known, for example, it can be logistic or Normal. The former results in the proportional odds logistic model, and the latter results in the ordered Probit model. Based on the latent linear model, we can compute</p><formula xml:id="formula_647">pr(y i ≤ k | x i ) = pr(y * i ≤ α k | x i ) = pr(x t i β + ε i ≤ α k | x i ) = pr(ε i ≤ α k -x t i β | x i ) = g(α k -x t i β).</formula><p>I will focus on the proportional odds logistic model in the main text and defer the details for the ordered Probit model to Problem 21.5. With this model, we have</p><formula xml:id="formula_648">pr(y i ≤ k | x i ) = e α k -x t i β 1 + e α k -x t i β ordinal outcome y * = x T β + ε density of the latent variable 1 2 3 4 α 0 = -∞ α 1 α 2 α 3 α 4 = ∞ FIGURE 21.1: Latent variable representation of the ordinal outcome or logit{pr(y i ≤ k | x i )} = log pr(y i ≤ k | x i ) pr(y i &gt; k | x i ) = α k -x t i β.<label>(21.7)</label></formula><p>The model has the "proportional odds" property because</p><formula xml:id="formula_649">pr(y i ≤ k | . . . , x ij + 1, . . .) pr(y i &gt; k | . . . , x ij + 1, . . .) pr(y i ≤ k | . . . , x ij , . . .) pr(y i &gt; k | . . . , x ij , . . .) = e -βj</formula><p>which is a positive constant not depending on k. The sign of x t i β is negative due to the latent variable representation. Some textbooks and software packages use a positive sign, but the function polr in package MASS of R uses <ref type="bibr">(21.7)</ref>.</p><p>The proportional odds model implies a quite complicated form of the probability for each category:</p><formula xml:id="formula_650">pr(y i = k | x i ) = e α k -x t i β 1 + e α k -x t i β - e α k-1 -x t i β 1 + e α k-1 -x t i β , (k = 1, . . . , K).</formula><p>So the likelihood function is</p><formula xml:id="formula_651">L(β, α 1 , . . . , α K-1 ) = n i=1 K k=1 {pr(y i = k | x i )} 1(yi=k) = n i=1 K k=1 e α k -x t i β 1 + e α k -x t i β - e α k-1 -x t i β 1 + e α k-1 -x t i β 1(yi=k)</formula><p>.</p><p>The log-likelihood function is concave <ref type="bibr" target="#b192">(Pratt, 1981;</ref><ref type="bibr" target="#b68">Burridge, 1981)</ref>, and it is strictly concave in most cases. The function polr in the R package MASS computes the MLE of the proportional odds model using the BFGS algorithm. It uses the explicit formulas of the gradient of the log-likelihood function, and computes the Hessian matrix numerically. I relegate the formulas of the gradient as a homework problem. For more details of the Hessian matrix, see <ref type="bibr" target="#b44">Agresti (2010)</ref>, which is a textbook discussion on modeling ordinal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.5">A case study</head><p>I use a small observational dataset from the Karolinska Institute in Stockholm, Sweden to illustrate the application of logistic regressions. <ref type="bibr" target="#b203">Rubin (2008)</ref> used this dataset to investigate whether it is better for cardia cancer patients to be treated in a large or small-volume hospital, where volume is defined by the number of patients with cardia cancer treated in recent years. I use the following variables: highdiag indicating whether a patient was diagnosed at a high volume hospital, hightreat indicating whether a patient was treated at a high volume hospital, age representing the age, rural indicating whether the patient was from a rural area, and survival representing the years of survival after diagnosis with three categories ("1", "2-4", "5+"). The R code is in code20.5.R. karolinska = read . table ( " karolinska . txt " , header = TRUE ) karolinska = karolinska [ , c ( " highdiag " , " hightreat " , " age " , " rural " , " male " , " survival " )]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.5.1">Binary logistic for the treatment</head><p>We have two choices of the treatment: highdiag and hightreat. The logistic fit of highdiag on covariates is shown below.</p><p>&gt; diagglm = glm ( highdiag ~age + rural + male , + data = karolinska , + family = binomial ( link = " logit " )) &gt; summary ( diagglm ) Call : glm ( formula = highdiag ~age + rural + male , family = binomial ( link = " logit " ) , data = karolinska )</p><p>Deviance Residuals : Min 1 Q Median 3 Q Max -2 . 0 6 1 4 7 -0 . 9 8 6 4 5 -0 . 0 5 7 5 9 1 . 0 1 3 9 1 1 . 7 5 6 9 6</p><p>Coefficients : Estimate Std . Error z value Pr ( &gt;| z |) ( Intercept ) 3 . 4 6 6 0 4 1 . 1 4 5 4 5 3 . 0 2 6 0 . 0 0 2 4 7 9 ** age -0 . 0 3 1 2 4 0 . 0 1 4 8 1 -2 . 1 1 0 0 . 0 3 4 8 5 4 * rural -1 . 2 6 3 2 2 0 . 3 4 5 3 0 -3 . 6 5 8 0 . 0 0 0 2 5 4 *** male -0 . 9 7 5 2 4 0 . 4 1 3 0 3 -2 . 3 6 1 0 . 0 1 8 2 1 6 *</p><p>The logistic fit of hightreat is shown below.</p><p>&gt; treatglm = glm ( hightreat ~age + rural + male , + data = karolinska , + family = binomial ( link = " logit " )) &gt; summary ( treatglm ) Call : glm ( formula = hightreat ~age + rural + male , family = binomial ( link = " logit " ) , data = karolinska ) <ref type="table">2 9 1 2 -0 . 9 9 7 8  0 . 5 3 8 7  0 . 8 4 0 8  1 . 4 8 1 0</ref> Coefficients : Estimate Std . Error z value Pr ( &gt;| z |) ( Intercept ) 6 . 4 4 6 8 3 1 . 4 9 5 4 4 4 . 3 1 1 1 . 6 3e -0 5 *** age -0 . 0 6 2 9 7 0 . 0 1 8 9 0 -3 . 3 3 2 0 . 0 0 0 8 6 2 *** rural -1 . 2 8 7 7 7 0 . 3 9 5 7 2 -3 . 2 5 4 0 . 0 0 1 1 3 7 ** male -0 . 7 4 8 5 6 0 . 4 5 2 8 5 -1 . 6 5 3 0 . 0 9 8 3 2 9 .</p><formula xml:id="formula_652">Deviance Residuals : Min 1 Q Median 3 Q Max -2 .</formula><p>Both treatments are associated with the covariates. hightreat is more strongly associated with age. <ref type="bibr" target="#b203">Rubin (2008)</ref> argued that highdiag is more random than hightreat, and may have weaker association with other hidden covariates. For each model below, I fit the data twice corresponding to two choices of treatment. Overall, we should trust the results with highdiag more based on Rubin (2008)'s argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.5.2">Binary logistic for the outcome</head><p>I first fit binary logistic models for the dichotomized outcome indicating whether the patient survived longer than a year after diagnosis.</p><p>&gt; karolinska $ loneyear = ( karolinska $ survival != " 1 " ) &gt; loneyearglm = glm ( loneyear ~highdiag + age + rural + male , + data = karolinska , + family = binomial ( link = " logit " )) &gt; summary ( loneyearglm ) Call : glm ( formula = loneyear ~highdiag + age + rural + male , family = binomial ( link = " logit " ) , data = karolinska )</p><p>Deviance Residuals : Min 1 Q Median 3 Q Max -1 . 1 7 5 5 -0 . 9 9 3 6 -0 . 7 7 3 9 1 . 3 0 2 4 1 . 8 5 5 7</p><p>Coefficients : Estimate Std . Error z value Pr ( &gt;| z |) ( Intercept ) -1 . 2 2 9 1 9 1 . 1 5 5 4 5 -1 . 0 6 4 0 . 2 8 7 4 highdiag 0 . 1 3 6 8 4 0 . 3 6 5 8 6 0 . 3 7 4 0 . 7 0 8 4 age -0 . 0 0 3 8 9 0 . 0 1 4 1 1 -0 . 2 7 6 0 . 7 8 2 9 rural 0 . 3 3 3 6 0 0 . 3 5 7 9 8 0 . 9 3 2 0 . 3 5 1 4 male 0 . 8 6 7 0 6 0 . 4 4 0 3 4 1 . 9 6 9 0 . 0 4 8 9 * highdiag is not significant in the above regression.</p><p>&gt; loneyearglm = glm ( loneyear ~hightreat + age + rural + male , + data = karolinska , + family = binomial ( link = " logit " )) &gt; summary ( loneyearglm ) Call : glm ( formula = loneyear ~hightreat + age + rural + male , family = binomial ( link = " logit " ) , data = karolinska )</p><p>Deviance Residuals : Min 1 Q Median 3 Q Max -1 . 3 7 6 7 -0 . 9 6 8 3 -0 . 6 7 8 4 1 . 0 8 1 3 2 . 0 8 3 3</p><p>Coefficients : Estimate Std . Error z value Pr ( &gt;| z |) ( Intercept ) -3 . 3 5 3 9 7 7 1 . 3 1 7 9 4 2 -2 . 5 4 5 0 . 0 1 0 9 3 * hightreat 1 . 4 1 7 4 5 8 0 . 4 5 5 6 0 3 3 . 1 1 1 0 . 0 0 1 8 6 ** age 0 . 0 0 8 7 2 5 0 . 0 1 4 8 4 0 0 . 5 8 8 0 . 5 5 6 5 5 rural 0 . 6 3 3 2 7 8 0 . 3 6 8 5 2 5 1 . 7 1 8 0 . 0 8 5 7 2 . male 1 . 0 7 9 9 7 3 0 . 4 5 2 1 9 1 2 . 3 8 8 0 . 0 1 6 9 3 * hightreat is significant in the above regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.5.3">Multinomial logistic for the outcome</head><p>I then fit multinomial logistic models for the outcome with three categories.</p><p>&gt; library ( nnet ) &gt; yearmultinom = multinom ( survival ~highdiag + age + rural + male , + data = karolinska ) # weights : 1 8 ( 1 0 variable ) initial value 1 7 3 . 5 8 0 7 4 2 iter 1 0 value 1 3 4 . 3 3 1 9 9 2 final value 1 3 4 . 1 3 0 8 1 5 converged &gt; summary ( yearmultinom ) Call : multinom ( formula = survival ~highdiag + age + rural + male , data = karolinska )</p><p>Coefficients :</p><p>( Intercept ) highdiag age rural male 2 -4 -1 . 0 7 5 8 1 8 -0 . 0 6 9 7 3 1 8 7 -0 . 0 0 4 6 2 4 0 3 0 0 . 1 7 4 4 2 5 6 0 . 5 0 2 8 7 8 6 5 + -4 . 1 8 0 4 1 6 0 . 6 4 0 3 6 2 8 9 -0 . 0 0 1 8 4 6 4 5 3 0 . 7 3 6 5 1 1 1 2 . 1 6 2 8 7 1 7</p><p>Std . Errors : ( Intercept ) highdiag age rural male 2 -4 1 . 2 8 6 9 8 7 0 . 4 1 1 3 0 0 6 0 . 0 1 5 9 6 3 7 7 0 . 4 0 1 4 7 1 8 0 . 4 7 1 6 8 3 1 5 + 2 . 0 0 3 5 8 1 0 . 5 8 1 6 3 6 5 0 . 0 2 1 4 8 9 3 6 0 . 5 7 4 1 0 1 7 1 . 0 7 4 1 2 3 9</p><p>Residual Deviance : 2 6 8 . 2 6 1 6 AIC : 2 8 8 . 2 6 1 6 &gt; predict ( yearmultinom , type = " probs " )[ 1 : 5 , ] 1 2 -4 5 + 1 0 . 5 9 5 0 6 3 1 0 . 2 6 4 7 0 4 7 0 . 1 4 0 2 3 2 2 2 2 0 . 5 9 4 1 8 0 2 0 . 2 6 5 5 3 6 9 0 . 1 4 0 2 8 2 9 3 3 0 . 8 0 8 1 3 7 6 0 . 1 7 1 8 9 6 3 0 . 0 1 9 9 6 6 1 3 4 0 . 5 9 5 0 6 3 1 0 . 2 6 4 7 0 4 7 0 . 1 4 0 2 3 2 2 2 5 0 . 6 3 6 6 9 2 9 0 . 2 2 6 0 0 8 6 0 . 1 3 7 2 9 8 4 9 highdiag is not significant above. The predict function gives the fitted probabilities for all categories of the outcome.</p><p>&gt; yearmultinom = multinom ( survival ~hightreat + age + rural + male , + data = karolinska ) # weights : 1 8 ( 1 0 variable ) initial value 1 7 3 . 5 8 0 7 4 2 iter 1 0 value 1 2 9 . 5 4 8 6 4 2 final value 1 2 9 . 2 8 3 7 3 9 converged &gt; summary ( yearmultinom ) Call : multinom ( formula = survival ~hightreat + age + rural + male , data = karolinska )</p><p>Coefficients :</p><p>( Intercept ) hightreat age rural male polr ( formula = survival ~hightreat + age + rural + male , data = karolinska , Hess = TRUE )</p><p>Coefficients : Value Std . Error t value hightreat 1 . 3 9 9 5 3 8 0 . 4 4 5 1 8 3 . 1 4 3 8 age 0 . 0 0 8 0 3 2 0 . 0 1 4 3 8 0 . 5 5 8 4 rural 0 . <ref type="table">6 3 8 8 6 2  0 . 3 5 4 5 0 1 . 8 0 2 2  male  1 . 1 2 2 6 9 8  0 . 4 4 3 7 7 2 . 5 2 9 9</ref> Intercepts : Value Std . Error t value 1 | 2 -4 3 . 3 2 7 3 1 . 2 7 5 2 2 . 6 0 9 2 2 -4 | 5 + 4 . 9 2 5 8 1 . 3 1 0 6 3 . 7 5 8 3</p><p>Residual Deviance : 2 6 0 . 2 8 3 1 AIC : 2 7 2 . 2 8 3 1 hightreat is significant above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.6">Discrete choice models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.6.1">Model</head><p>The covariates in model ( <ref type="formula" target="#formula_618">21</ref>.3) depend only on individuals. <ref type="bibr" target="#b178">McFadden (1974)</ref> extends it to allow for choice-specific covariates z ik . His formulation is based on the latent utility representation:</p><formula xml:id="formula_653">       U i1 = z t i1 θ + ε i1 , . . . U iK = z t iK θ + ε iK , where ε i1 , . . . , ε iK are IID standard Gumbel. Unit i chooses k if k has the highest utility. Lemma 21.1 implies that π k (z i ) = π k (z i , θ) = e z t ik θ K l=1 e z t il θ , (k = 1, . . . , K).<label>(21.8)</label></formula><p>Model <ref type="bibr">(21.8)</ref> seems rather similar to model <ref type="bibr">(21.3)</ref>. However, there are many subtle differences. First, a component of z ik may vary only with choice k, for example, it can represent the price of choice k. Partition z ik into three types of covariates: x i that only vary cross individuals, c k that only vary across choices, and w ik that vary across both individuals and choices. Model <ref type="bibr">(21.8</ref>) reduces to</p><formula xml:id="formula_654">π k (z i ) = e x t i θx+c t k θc+w t ik θw K l=1 e x t i θx+c t l θc+w t il θw = e c t k θc+w t ik θw K l=1 e c t l θc+w t il θw</formula><p>, that is, the individual-specific covariates drop out. Therefore, z ik in model <ref type="bibr">(21.8)</ref> does not contain covariates that vary only with individuals. In particular, z ik in model <ref type="bibr">(21.8)</ref> does not contain the constant, but in contrast, the x i in model <ref type="bibr">(21.</ref>3) ususally contains the intercept by default.</p><p>Second, if we want to use individual-specific covariates in the model, they must have choice-specific coefficients. So a more general model unifying <ref type="bibr">(21.3</ref>) and (21.8) is</p><formula xml:id="formula_655">π k (x i , w ik , θ, β) = e w t ik θ+x t i β k K l=1 e w t il θ+x t i β l , (k = 1, . . . , K). (21.9)</formula><p>Equivalently, we can create pseudo covariates z ik as the original w ik together with interaction of x i and the dummy for choice k. For example, if K = 3 and x i contain the intercept and a scalar individual-specific covariate, then (z i1 , z i2 , z i3 ) are</p><formula xml:id="formula_656">  z i1 z i2 z i3   =   w i1 1 0 x i 0 w i2 0 1 0 x i w i3 0 0 0 0   ,</formula><p>where K = 3 is the reference level. So with augmented covariates, the discrete choice model <ref type="bibr">(21.8</ref>) is strictly more general than the multinomial logistic model <ref type="bibr">(21.3</ref>). In the special case with K = 2, model <ref type="bibr">(21.8</ref>) reduces to</p><formula xml:id="formula_657">pr(y i = 1 | x i ) = e x t i β 1 + e x t i β</formula><p>where</p><formula xml:id="formula_658">x i = z i1 -z i2 .</formula><p>Based on the model specification <ref type="bibr">(21.8)</ref>, the log likelihood function is</p><formula xml:id="formula_659">log L(θ) = n i=1 K k=1 1(y i = k) z t ik θ -log K l=1 e z t il θ .</formula><p>So the score function is</p><formula xml:id="formula_660">∂ ∂θ log L(θ) = n i=1 K k=1 1(y i = k) z ik - K l=1 e z t il θ z il K l=1 e z t il θ = n i=1 K k=1 1(y i = k){z ik -E(z ik ; θ)},</formula><p>where E(•; θ) is the average value of {z i1 , . . . , z iK } over the probability mass function</p><formula xml:id="formula_661">p k (θ) = e z t ik θ / K l=1 e z t il θ .</formula><p>The Hessian matrix is</p><formula xml:id="formula_662">∂ 2 ∂θ∂θ t log L(θ) = - n i=1 K k=1 1(y i = k) K l=1 e z t il θ z il z t il K l=1 e z t il θ - K l=1 e z t il θ z il K l=1 e z t il θ z t il ( K l=1 e z t il θ ) 2 = - n i=1 K k=1 1(y i = k)cov(z ik ; θ),</formula><p>where cov(•; θ) is the covariance matrix of {z i1 , . . . , z iK } over the probability mass function defined above. From these formulas, we can easily compute the MLE using Newton's method and obtain its asymptotic distribution based on the inverse of the Fisher information matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.6.2">Example</head><p>The R package mlogit provides a function mlogit to fit the general discrete logistic model <ref type="bibr" target="#b87">(Croissant, 2020)</ref>. Here I use an example from this package to illustrate the model fitting of mlogit. The R code is in code20.6.R.</p><p>&gt; library ( " nnet " ) &gt; library ( " mlogit " ) &gt; data ( " Fishing " ) &gt; head ( Fishing ) mode price . beach price . pier price . boat price . charter 1 charter 1 5 7 . 9 3 0 1 5 7 . 9 3 0 1 5 7 . 9 3 0 1 8 2 . 9 3 0 2 charter 1 5 . 1 1 4 1 5 . 1 1 4 1 0 . 5 3 4 3 4 . 5 3 4 3 boat 1 6 1 . 8 7 4 1 6 1 . 8 7 4 2 4 . 3 3 4 5 9 . 3 3 4 4 pier 1 5 . 1 3 4 1 5 . 1 3 4 5 5 . 9 3 0 8 4 . 9 3 0 5 boat 1 0 6 . 9 3 0 1 0 6 . 9 3 0 4 1 . 5 1 4 7 1 . 0 1 4 6 charter 1 9 2 . 4 7 4 1 9 2 . 4 7 4 2 8 . 9 3 4 6 3 . 9 3 4 catch . beach catch . pier catch . boat catch . charter income 1 0 . 0 6 7 8 0 . 0 5 0 3 0 . 2 6 0 1 0 . 5 3 9 1 7 0 8 3 . 3 3 2 2 0 . 1 0 4 9 0 . 0 4 5 1 0 . 1 5 7 4 0 . 4 6 7 1 1 2 5 0 . 0 0 0 3 0 . 5 3 3 3 0 . 4 5 2 2 0 . 2 4 1 3 1 . 0 2 6 6 3 7 5 0 . 0 0 0 4 0 . 0 6 7 8 0 . 0 7 8 9 0 . 1 6 4 3 0 . 5 3 9 1 2 0 8 3 . 3 3 3 5 0 . 0 6 7 8 0 . 0 5 0 3 0 . 1 0 8 2 0 . 3 2 4 0 4 5 8 3 . 3 3 2 6 0 . 5 3 3 3 0 . 4 5 2 2 0 . 1 6 6 5 0 . 3 9 7 5 4 5 8 3 . 3 3 2</p><p>The dataset Fishing is in the "wide" format, where mode denotes the choice of four modes of fishing (beach, pier, boat and charter), price and catch denote the price and catching rates which are choice-specific, income is individual-specific. We need to first transform the dataset into "long" format.</p><p>&gt; Fish = dfidx ( Fishing , + varying = 2 : 9 , + shape = " wide " , + choice = " mode " ) &gt; head ( Fish ) ~~~~~~f irst 1 0 observations out of 4 7 2 8 ~~~~~~m ode income price catch idx 1 FALSE 7 0 8 3 . 3 3 2 1 5 7 . 9 3 0 0 . 0 6 7 8 1 : each 2 FALSE 7 0 8 3 . 3 3 2 1 5 7 . 9 3 0 0 . 2 6 0 1 1 : boat 3 TRUE 7 0 8 3 . 3 3 2 1 8 2 . 9 3 0 0 . 5 3 9 1 1 : rter 4 FALSE 7 0 8 3 . 3 3 2 1 5 7 . 9 3 0 0 . 0 5 0 3 1 : pier 5 FALSE 1 2 5 0 . 0 0 0 1 5 . 1 1 4 0 . 1 0 4 9 2 : each 6 FALSE 1 2 5 0 . 0 0 0 1 0 . 5 3 4 0 . 1 5 7 4 2 : boat 7 TRUE 1 2 5 0 . 0 0 0 3 4 . 5 3 4 0 . 4 6 7 1 2 : rter 8 FALSE 1 2 5 0 . 0 0 0 1 5 . 1 1 4 0 . 0 4 5 1 2 : pier 9 FALSE 3 7 5 0 . 0 0 0 1 6 1 . 8 7 4 0 . 5 3 3 3 3 : each 1 0 TRUE 3 7 5 0 . 0 0 0 2 4 . 3 3 4 0 . 2 4 1 3 3 : boat Using only choice-specific covariates, we have the following fitted model: &gt; summary ( mlogit ( mode ~0 + price + catch , data = Fish ))</p><p>Call : mlogit ( formula = mode ~0 + price + catch , data = Fish , method = " nr " ) Frequencies of alternatives : choice beach boat charter pier 0 . 1 1 3 3 7 0 . 3 5 3 6 4 0 . 3 8 2 4 0 0 . 1 5 0 5 9 nr method 6 iterations , 0 h : 0 m : 0 s g '( -H )^-1 g ␣ = ␣ 0 . 0 0 0 1 7 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.7">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.1">Inverse model for the multinomial logit model</head><p>The following result extends Theorem 20.2.</p><p>Assume that y i ∼ Multinomial(1; q 1 , . . . , q K ), (21.10) and</p><formula xml:id="formula_663">x i | y i = k ∼ N(µ k , Σ),<label>(21.11)</label></formula><p>where x i does not contain 1. We can verify that y i | x i follows a multinomial logit model as shown in the theorem below.</p><p>Theorem 21.1 Under (21.10) and (21.11), we have</p><formula xml:id="formula_664">pr(y i = k | x i ) = e α k +x t i β k K l=1 e α l +x t i β l</formula><p>, where</p><formula xml:id="formula_665">α k = log q k - 1 2 µ t k Σ -1 µ k , β k = Σ -1 µ k .</formula><p>Prove Theorem 21.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2">Hessian matrix in the multinomial logit model</head><p>Prove that the Hessian matrix <ref type="bibr">(21.4</ref>) of the log-likelihood function of the multinomial logit model is negative semi-definite. Hint: Use Proposition 21.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.3">Iteratively reweighted least squares algorithm for the multinomial logit model</head><p>Similar to the binary logistic model, Newton's method for computing the MLE for the multinomial logit model can be written as iteratively reweighted least squares. Give the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.4">Score function of the proportional odds model</head><p>Derive the explicit formulas of the score function of the proportional odds model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.5">Ordered Probit regression</head><p>If we choose ε i | x i ∼ N(0, 1) in <ref type="bibr">(21.5)</ref>, then the corresponding model is called the ordered Probit regression. Write down the likelihood function and derive the score function for this model. Remark: You can use the function polr in R to fit this model with the specification method = "probit".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.6">Case-control study and multinomial logistic model</head><p>This problem extends Theorem 20.1. Assume that pr(</p><formula xml:id="formula_666">y i = k | x i ) = e α k +x t i β k K l=1 e α l +x t i β l and pr(s i = 1 | y i = k, x i ) = pr(s i = 1 | y i = k) = p k for k = 1, . . . , K. Show that pr(y i = k | x i , s i = 1) = e αk +x t i β k K l=1 e αl +x t i β l with αk = α k + log p k for k = 1, . . . , K.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22">Regression Models for Count Outcomes</head><p>A random variable for counts can take values in {0, 1, 2, . . .}. This type of variable is common in applied statistics. For example, it can represent how many times you visit the gym every week, how many lectures you have missed in the linear model course, how many traffic accidents happened in certain areas during certain periods, etc. This chapter focuses on statistical modeling of those outcomes given covariates. Hilbe ( <ref type="formula">2014</ref>) is a textbook focusing on count outcome regressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.1">Some random variables for counts</head><p>I first review four canonical choices of random variables for modeling count data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.1.1">Poisson</head><formula xml:id="formula_667">k ∼ Poisson(λ k ) for k = 1, . . . , K, then y 1 + • • • + y K ∼ Poisson(λ), and (y 1 , . . . , y K ) | y 1 + • • • + y K = n ∼ Multinomial (n, (λ 1 /λ, . . . , λ K /λ))</formula><p>,</p><formula xml:id="formula_668">where λ = λ 1 + • • • + λ K . Conversely, if S ∼ Poisson(λ) with λ = λ 1 + • • • + λ K , and (y 1 , . . . , y K ) | S = n ∼ Multinomial (n, (λ 1 /λ, . . . , λ K /λ)), then y 1 , . . . , y K are mutually independent with y k ∼ Poisson(λ k ) for k = 1, . . . , K.</formula><p>Where does the Poisson random variable come from? One way to generate Poisson is through independent Bernoulli random variables. I will review Le Cam (1960)'s theorem below without giving a proof. Theorem 22.1 Suppose X i 's are independent Bernoulli random variables with probabilities</p><formula xml:id="formula_669">p i 's (i = 1, . . . , n). Define λ n = n i=1 p i and S n = n i=1 X i . Then ∞ k=0 pr(S n = k) -e -λn λ k n k! ≤ 2 n i=1 p 2 i .</formula><p>As a special case, if p i = λ/n, then Theorem 22.1 implies</p><formula xml:id="formula_670">∞ k=0 pr(S n = k) -e -λ λ k k! ≤ 2 n i=1 (λ/n) 2 = λ 2 /n → 0.</formula><p>So the sum of IID Bernoulli random variables is approximately Poisson if the probability has order 1/n. This is called the law of rare events, or Poisson limit theorem, or Le Cam's theorem. By Theorem 22.1, we can use Poisson as a model for the sum of many rare events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.1.2">Negative-Binomial</head><p>The Poisson distribution restricts that the mean must be the same as the variance. It cannot capture the feature of overdispersed data with variance larger than the mean. The Negative-Binomial is an extension of the Poisson that allows for overdispersion. Here the definition of the Negative-Binomial below is different from its standard definition, but it is more natural as an extension of the Poisson 1 . Define y as the Negative-Binomial random variable, denoted by NB(µ, θ) with µ &gt; 0 and θ &gt; 0, if</p><formula xml:id="formula_671">y | λ ∼ Poisson(λ), λ ∼ Gamma(θ, θ/µ).<label>(22.1)</label></formula><p>So the Negative-Binomial is the Poisson with a random Gamma intensity, that is, the Negative-Binomial is a scale mixture of the Poisson. If θ → ∞, then λ is a point mass at µ and the Negative-Binomial reduces to Poisson(µ). We can verify that it has the following probability mass function.</p><p>Proposition 22.3 The Negative-Binomial random variable defined in ( <ref type="formula" target="#formula_671">22</ref>.1) has the probability mass function</p><formula xml:id="formula_672">pr(y = k) = Γ(k + θ) Γ(k + 1)Γ(θ) θ µ + θ θ µ µ + θ k , (k = 0, 1, 2, . . .). Proof of Proposition 22.3: We have pr(y = k) = ∞ 0 pr(y = k | λ)f (λ)dλ = ∞ 0 e -λ λ k k! (θ/µ) θ Γ(θ) λ θ-1 e -(θ/µ)λ dλ = (θ/µ) θ k!Γ(θ) ∞ 0 λ k+θ-1 e -(1+θ/µ)λ dλ.</formula><p>The function in the integral is the density function of Gamma(k + θ, 1 + θ/µ) without the normalizing constant (1 + θ/µ) k+θ Γ(k + θ) .</p><p>1 With IID Bernoulli(p) trials, the Negative-Binomial distribution, denoted by y ∼ NB ′ (r, p), is the number of success before the rth failure. Its probability mass function is</p><formula xml:id="formula_673">pr(y = k) = k + r -1 k (1 -p) r p k , (k = 0, 1, 2, . . .)</formula><p>If p = µ/(µ + θ) and r = θ then these two definitions coincide. This definition is more restrictive because r must be an integer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>So pr(y</head><formula xml:id="formula_674">= k) = (θ/µ) θ k!Γ(θ) (1 + θ/µ) k+θ Γ(k + θ) = Γ(k + θ) k!Γ(θ) θ µ + θ θ µ µ + θ k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>We can derive the mean and variance of the Negative-Binomial. <ref type="bibr">Proposition 22.4</ref> The Negative-Binomial random variable defined in <ref type="bibr">(22.1)</ref> has moments</p><formula xml:id="formula_675">E(y) = µ, var(y) = µ + µ 2 θ &gt; E(y).</formula><p>Proof of Proposition 22.4: Recall Proposition B.2 that a Gamma(α, β) random variable has mean α/β and variance α/β 2 . We have</p><formula xml:id="formula_676">E(y) = E {E(y | λ)} = E(λ) = θ θ/µ = µ,<label>and</label></formula><formula xml:id="formula_677">var(y) = E {var(y | λ)} + var {E(y | λ)} = E(λ) + var(λ) = θ θ/µ + θ (θ/µ) 2 = µ + µ 2 θ .</formula><p>□ So the dispersion parameter θ controls the variance of the Negative-Binomial. With the same mean, the Negative-Binomial has a larger variance than Poisson. Figure <ref type="figure" target="#fig_61">22</ref>.1 further compares the log probability mass functions of the Negative Binomial and Poisson. It shows that the Negative Binomial has a slightly higher probability at zero but much heavier tails than the Poisson.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.1.3">Zero-inflated count distributions</head><p>Many count distributions have larger masses at zero compared to Poisson and Negative-Binomial. Therefore, it is also important to have more general distributions capturing this feature of empirical data. We can simply add an additional zero component to the Poisson or the Negative-Binomial.</p><p>A zero-inflated Poisson random variable y is a mixture of two components: a point mass at zero and a Poisson(λ) random variable, with probabilities p and 1 -p, respectively. So y has the probability mass function</p><formula xml:id="formula_678">pr(y = k) = p + (1 -p)e -λ , if k = 0, (1 -p)e -λ λ k k! , if k = 1, 2, . . . .</formula><p>and moments below:</p><formula xml:id="formula_679">Proposition 22.5 E(y) = (1 -p)λ and var(y) = (1 -p)λ(1 + pλ).</formula><p>A zero-inflated Negative-Binomial random variable y is a mixture of two components:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.2">Regression models for counts</head><p>To model a count outcome y i given x i , we can still use OLS. However, a problem with OLS is that the predicted value can be negative. This can be easily fixed by running OLS of log(y i + 1) given x i . However, this still does not reflect the fact that y i is a count outcome. For example, these two OLS fits cannot easily make a prediction for the probabilities pr(y i ≥ 1 | x i ) or pr(y i &gt; 3 | x i ). A more direct approach is to model the conditional distribution of y i given x i using the distributions reviewed in Section 22.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.2.1">Poisson regression</head><p>Poisson regression assumes</p><formula xml:id="formula_680">y i | x i ∼ Poisson(λ i ), λ i = λ(x i , β) = e x t i β .</formula><p>So the mean and variance of</p><formula xml:id="formula_681">y i | x i are E(y i | x i ) = var(y i | x i ) = e x t i β .</formula><p>Because log E(y i | x i ) = x t i β, this model is sometimes called the log-linear model, with the coefficient β j interpreted as the conditional log mean ratio:</p><formula xml:id="formula_682">log E(y i | . . . , x ij + 1, . . .) E(y i | . . . , x ij , . . .) = β j .</formula><p>The likelihood function for independent Poisson random variables is</p><formula xml:id="formula_683">L(β) = n i=1 e -λi λ yi i y i ! ∝ n i=1 e -λi λ yi i ,</formula><p>and omitting the constants, we can write the log-likelihood function as</p><formula xml:id="formula_684">log L(β) = n i=1 (-λ i + y i log λ i ) = n i=1 -e x t i β + y i x t i β .</formula><p>The score function is</p><formula xml:id="formula_685">∂ log L(β) ∂β = n i=1 -x i e x t i β + x i y i = n i=1 x i y i -e x t i β = n i=1 x i {y i -λ(x i , β)} ,</formula><p>and the Hessian matrix is</p><formula xml:id="formula_686">∂ 2 log L(β) ∂β∂β t = - n i=1 x i ∂ ∂β t e x t i β = - n i=1 e x t i β x i x t i</formula><p>which is negative semi-definite. When the Hessian is negative definite, the MLE is unique. It must satisfy that</p><formula xml:id="formula_687">n i=1 x i y i -e x t i β = n i=1 x i y i -λ(x i , β) = 0.</formula><p>We can solve this nonlinear equation using Newton's method:</p><formula xml:id="formula_688">β new = β old - ∂ 2 log L(β old ) ∂β∂β t -1 ∂ log L(β old ) ∂β = β old -(X t W old X) -1 X t (Y -Λ old ),</formula><p>where</p><formula xml:id="formula_689">X =    x t 1 . . . x t n    , Y =    y 1 . . . y n    and Λ old =    exp(x t 1 β old ) . . . exp(x t n β old )    , W old = diag exp(x t i β old ) n i=1 .</formula><p>Similar to the derivation for the logit model, we can simplify Newton's method to</p><formula xml:id="formula_690">β new = (X t W old X) -1 X t W old Z old ,</formula><p>where</p><formula xml:id="formula_691">Z old = Xβ old + (W old ) -1 (Y -Λ old ).</formula><p>So we have an iterative reweighted least squares algorithm. In R, we can use the glm function with "family = poisson(link = "log")" to fit the Poisson regression, which uses Newton's method.</p><p>Statistical inference based on</p><formula xml:id="formula_692">β a ∼ N    β, - ∂ 2 log L( β) ∂β∂β t -1    = N β, (X t Ŵ X) -1 , where Ŵ = diag exp(x t i β) n i=1</formula><p>.</p><p>After obtaining the MLE, we can predict the mean E(y i | x i ) by λi = e x t i β . Because Poisson regression is a fully parametrized model, we can also predict any other probability quantities involving y i | x i . For example, we can predict pr(y i = 0 | x i ) by e -λi , and pr(y i ≥ 3 | x i ) by 1 -e -λi (1 + λi + λ2 i /2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.2.2">Negative-Binomial regression</head><p>Negative-Binomial regression assumes</p><formula xml:id="formula_693">y i | x i ∼ NB(µ i , θ), µ i = e x t i β ,</formula><p>so it has conditional mean and variance:</p><formula xml:id="formula_694">E(y i | x i ) = e x t i β , var(y i | x i ) = e x t i β (1 + e x t i β /θ).</formula><p>It is also a log-linear model.</p><p>The log-likelihood function for Negative-Binomial regression is log</p><formula xml:id="formula_695">L(β, θ) = n i=1 l i (β, θ) with l i (β, θ) = log Γ(y i + θ) -log Γ(y i + 1) -log Γ(θ) +θ log θ µ i + θ + y i log µ i µ i + θ ,</formula><p>where µ i = e x t i β has partial derivative ∂µ i /∂β = µ i x i . We can use Newton's algorithm or Fisher scoring algorithm to compute the MLE ( β, θ) which requires deriving the first and second derivatives of log L(β, θ) with respect to (β, θ). I will derive some important components and relegate other details to Problem 22.1. First,</p><formula xml:id="formula_696">∂ log L(β, θ) ∂β = n i=1 (1 + µ i /θ) -1 (y i -µ i )x i .</formula><p>The corresponding first-order condition can be viewed as the estimating equation of Poisson regression with weights (1 + µ i /θ) -1 . Second,</p><formula xml:id="formula_697">∂ 2 log L(β, θ) ∂β∂θ = n i=1 µ i (µ i + θ) 2 (y i -µ i )x i .</formula><p>We can verify</p><formula xml:id="formula_698">E ∂ 2 log L(β, θ) ∂β∂θ | X = 0</formula><p>since each term inside the summation has conditional expectation zero. This implies that the Fisher information matrix is diagonal, so β and θ are asymptotically independent. The glm.nb in the MASS package iterate between β and θ: given θ, update β based on Fisher scoring; given β, update θ based on Newton's algorithm. It reports standard errors based on the inverse of the Fisher information matrix. <ref type="foot" target="#foot_22">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.2.3">Zero-inflated regressions</head><p>The zero-inflated Poisson regression assumes that</p><formula xml:id="formula_699">y i | x i ∼ 0, with probability p i , Poisson(λ i ), with probability 1 -p i ,</formula><p>where</p><formula xml:id="formula_700">p i = e x t i γ 1 + e x t i γ , λ i = e x t i β .</formula><p>The zero-inflated Negative-Binomial regression assumes that</p><formula xml:id="formula_701">y i | x i ∼ 0, with probability p i , NB(µ i , θ), with probability 1 -p i ,</formula><p>where</p><formula xml:id="formula_702">p i = e x t i γ 1 + e x t i γ , µ i = e x t i β .</formula><p>To avoid over-parametrization, we can also restrict some coefficients to be zero. The zeroinfl function in the R package pscl can fit the zero-inflated Poisson and Negative-Binomial regressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3">A case study</head><p>I will use the dataset from <ref type="bibr" target="#b202">Royer et al. (2015)</ref> to illustrate the regressions for count outcomes. The R code is in code21.3.R. From the regression formula below, we are interested in the effect of two treatments incentive_commit and incentive on the number of visits to the gym, controlling for two pretreatment covariates target and member_gym_pre. &gt; library ( " foreign " ) &gt; library ( " MASS " ) &gt; gym 1 = read . dta ( " gym _ treatment _ exp _ weekly . dta " ) &gt; f . reg = weekly _ visit ~incentive _ commit + incentive + + target + member _ gym _ pre </p><formula xml:id="formula_703">&gt; AIClm = 1 : lweekids &gt; for ( i in 1 : lweekids ) + { + gymweek = gym 1 [ which ( gym 1 $ incentive _ week == weekids [ i ]) , ] + regweek = lm ( f . reg , data = gymweek ) + regweekcoef = summary ( regweek )$ coef + + c o e f i n c e n t i v e c o m m i t [ i ] = regweekcoef [ 2 , 1 ] + coefincentive [ i ] = regweekcoef [ 3 , 1 ] + s e i n c e n t i v e c o m m i t [ i ] = regweekcoef [ 2 , 2 ] + seincentive [ i ] = regweekcoef [ 3 , 2 ] + + AIClm [ i ] = AIC ( regweek ) + }</formula><p>By changing the line with lm by regweek = glm ( f . reg , family = poisson ( link = " log " ) , data = gymweek ) and regweek = glm . nb ( f . reg , data = gymweek ) we obtain the corresponding results from Poisson and Negative-Binomial regressions. Figure <ref type="figure" target="#fig_61">22</ref>.2 compares the regression coefficients with the associated confidence intervals over time. Three regressions give very similar patterns: incentive_commit has both short-term and longterm effects, but incentive only has short-term effects.</p><p>The left panel of Figure <ref type="figure" target="#fig_61">22</ref>.3 shows that variances are larger than the means for outcomes from all weeks, and the right panel of Figure <ref type="figure" target="#fig_61">22</ref>.3 shows the point estimates and confidence intervals of θ from Negative-Binomial regressions. Overall, overdispersion seems an important feature of the data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3.2">Zero-inflated regressions</head><formula xml:id="formula_704">[ i ] = regweekcoef $ count [ 2 , 1 ] + coefincentive [ i ] = regweekcoef $ count [ 3 , 1 ] + s e i n c e n t i v e c o m m i t [ i ] = regweekcoef $ count [ 2 , 2 ] + seincentive [ i ] = regweekcoef $ count [ 3 , 2 ] + + c o e f i n c e n t i v e c o m m i t 0 [ i ] = regweekcoef $ zero [ 2 , 1 ] + coefincentive 0 [ i ] = regweekcoef $ zero [ 3 , 1 ] + s e i n c e n t i v e c o m m i t 0 [ i ] = regweekcoef $ zero [ 2 , 2 ] + seincentive 0 [ i ] = regweekcoef $ zero [ 3 , 2 ] + + AIC 0 poisson [ i ] = AIC ( regweek ) + }</formula><p>Replacing the line with zeroinfl by regweek = zeroinfl ( f . reg , dist = " negbin " , data = gymweek ) we can fit the corresponding zero-inflated Negative-Binomial regressions. but have effects on the zero components. This suggests that the treatments affect the outcome mainly by changing the workers' behavior of whether to go to the gym. Another interesting result is the large θ's from the zero-inflated Negative-Binomial regression:</p><p>&gt; quantile ( gymtheta , probs = c ( 0 . 0 1 , 0 . 2 5 , 0 . 5 , 0 . 7 5 , 0 . 9 9 )) 1 % 2 5 % 5 0 % 7 5 % 9 9 % 1 2 . 3 1 3 . 1 1 3 . 7 1 4 . 4 1 5 . 7</p><p>Once the zero-inflated feature has been modeled, it is not crucial to account for the overdispersion. It is reasonable because the maximum outcome is five, ruling out heavytailedness. This is further corroborated by the following comparison of the AICs from five regression models. Figure <ref type="figure" target="#fig_61">22</ref>.6 shows that zero-inflated Poisson regressions have the smallest AICs, beating the zero-inflated Negative-Binomial regressions, which are more flexible but have more parameters to estimate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3">Overdispersion and zero-inflation</head><p>Show that for a zero-inflated Poisson, if p ≤ 1/2 then E(y) &lt; var(y) always holds. What is the condition for E(y) &lt; var(y) when p &gt; 1/2?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.4">Poisson latent variable and the binary regression model with the cloglog link</head><p>Assume that y * i | x i ∼ Poisson(e x t i β ), and define y i = 1(y * i &gt; 0) as the indicator that y * i is not zero. Show that y i | x i follows a cloglog model, that is,</p><formula xml:id="formula_705">pr(y i = 1 | x i ) = g(x t i β),</formula><p>where g(z) = 1 -exp(-e z ).</p><p>Remark: The cloglog model for binary outcome arises naturally from a latent Poisson model. It was only briefly mentioned in Chapter 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.5">Likelihood for the zero-inflated Poisson regression</head><p>Write down the likelihood function for the Zero-inflated Poisson model, and derive the steps for Newton's method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.6">Likelihood for the Zero-inflated Negative-Binomial regression</head><p>Write down the likelihood function for the Zero-inflated Negative-Binomial model, and derive the steps for Newton's method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized Linear Models: A Unification</head><p>This chapter unifies Chapters 20-22 under the formulation of the generalized linear model (GLM) by <ref type="bibr" target="#b183">Nelder and Wedderburn (1972)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.1">Generalized Linear Models</head><p>So far we have discussed the following models for independent observations (y i , x i ) n i=1 .</p><p>Example 23.1 The Normal linear model for continuous outcomes assumes</p><formula xml:id="formula_706">y i | x i ∼ N(µ i , σ 2 ), with µ i = x t i β.<label>(23.1)</label></formula><p>Example 23.2 The logistic model for binary outcomes assumes</p><formula xml:id="formula_707">y i | x i ∼ Bernoulli(µ i ), with µ i = e x t i β 1 + e x t i β .<label>(23.2)</label></formula><p>Example 23.3 The Poisson model for count outcomes assumes</p><formula xml:id="formula_708">y i | x i ∼ Poisson(µ i ), with µ i = e x t i β . (23.3) Example 23.4</formula><p>The Negative-Binomial model for overdispersed count outcomes assumes</p><formula xml:id="formula_709">y i | x i ∼ NB(µ i , δ), with µ i = e x t i β .<label>(23.4)</label></formula><p>We use δ for the dispersion parameter to avoid confusion because θ means something else below (Chapter 22 uses θ for the dispersion parameter).</p><p>In the above models, µ i denotes the conditional mean. This chapter will unify Examples 23.1-23.4 as GLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.1.1">Exponential family</head><p>Consider a general conditional probability density or mass function:</p><formula xml:id="formula_710">f (y i | x i ; θ i , ϕ) = exp y i θ i -b(θ i ) a(ϕ) + c(y i , ϕ) ,<label>(23.5)</label></formula><p>where (θ i , ϕ) are unknown parameters, and {a(•), b(•), c(•, •)} are known functions. The above conditional density <ref type="bibr">(23.5</ref>) is called the natural exponential family with dispersion, where θ i is the natural parameter depending on x i , and ϕ is the dispersion parameter. Sometimes, a(ϕ) = 1 and c(y i , ϕ) = c(y i ), simplifying the conditional density to a natural exponential family.  have a unified structure as <ref type="bibr">(23.5)</ref>, as detailed below.</p><p>The logistic and Poisson models are simpler without the dispersion parameter. The Normal linear model has a dispersion parameter for the variance. The Negative-Binomial model is more complex: without fixing δ it does not belong to the exponential family with dispersion.</p><p>The exponential family <ref type="bibr">(23.5</ref>) has nice properties derived from the classic Bartlett's identities <ref type="bibr" target="#b55">(Bartlett, 1953)</ref>. I first review Bartlett's identities: Lemma 23.1 Given a probability density or mass function f (y | θ) indexed by a scalar parameter θ, if we can change the order of expectation and differentiation, then</p><formula xml:id="formula_711">E ∂ log f (y | θ) ∂θ = 0 and E ∂ log f (y | θ) ∂θ 2 = E - ∂ 2 log f (y | θ) ∂θ 2 .</formula><p>This lemma is well-known in classic statistical theory for likelihood, and I give a simple proof below. </p><formula xml:id="formula_712">∂ ∂θ e ℓ(y|θ) ∂ ∂θ ℓ(y | θ)dy = 0 =⇒ e ℓ(y|θ) ∂ ∂θ ℓ(y | θ) 2 + e ℓ(y|θ) ∂ 2 ∂θ 2 ℓ(y | θ) dy = 0,</formula><p>which implies Bartlett's second identity. □ Lemma 23.1 implies the moments of the exponential family <ref type="bibr">(23.5)</ref>.</p><p>Theorem 23.1 The first two moments of <ref type="bibr">(23.5)</ref> are</p><formula xml:id="formula_713">E(y i | x i ; θ i , ϕ) ≡ µ i = b ′ (θ i ) and var(y i | x i ; θ i , ϕ) ≡ σ 2 i = b ′′ (θ i )a(ϕ).</formula><p>Proof of Theorem 23.1: The first two derivatives of the log conditional density are</p><formula xml:id="formula_714">∂ log f (y i | x i ; θ i , ϕ) ∂θ i = y i -b ′ (θ i ) a(ϕ) , ∂ 2 log f (y i | x i ; θ i , ϕ) ∂θ 2 i = - b ′′ (θ i ) a(ϕ) . Lemma 23.1 implies that E y i -b ′ (θ i ) a(ϕ) = 0, E y i -b ′ (θ i ) a(ϕ) 2 = b ′′ (θ i ) a(ϕ) ,</formula><p>which further imply the first two moments of y i given x i . □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.1.2">Generalized linear model</head><p>Section 23.1.1 is general, allowing the mean parameter µ i to depend on x i in an arbitrary way. This flexibility does not immediately generate a useful statistical procedure. To borrow information across observations, we assume that the relationship between y i and x i remain "stable" and can be captured by a fixed parameter β. A simple starting point is to use x t i β to approximate µ i , which, however, works naturally only for outcomes taking values in a wide range of (-∞, ∞). For general outcome variables, we can link its mean and the linear combination of covariates by µ i = µ(x t i β), where µ(•) is a known function and β is an unknown parameter. The inverse of µ(•) is called the link function. This is called a GLM, which has the following components:</p><p>(C1) the conditional distribution <ref type="bibr">(23.5</ref>) as an exponential family with dispersion;</p><p>(C2) the conditional mean µ i = b ′ (θ i ) and variance σ 2 i = b ′′ (θ i )a(ϕ) in Theorem 23.1;</p><p>(C3) the function linking the conditional mean and covariates µ i = µ(x t i β). Models (23.1)- <ref type="bibr">(23.4</ref>) are the classical examples. Figure <ref type="figure" target="#fig_110">23</ref>.1 illustrates the relationship among key quantities in a GLM. In particular,  </p><formula xml:id="formula_715">θ i = (b ′ ) -1 (µ i ) = (b ′ ) -1 {µ(x t i β)}<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.2">MLE for GLM</head><p>The contribution of unit i to the log-likelihood function is</p><formula xml:id="formula_716">ℓ i = log f (y i | x i ; β, ϕ) = y i θ i -b(θ i ) a(ϕ) + c(y i , ϕ).</formula><p>The contribution of unit i to the score function is</p><formula xml:id="formula_717">∂ℓ i ∂β = ∂ℓ i ∂θ i ∂θ i ∂µ i ∂µ i ∂β ,</formula><p>where</p><formula xml:id="formula_718">∂ℓ i ∂θ i = y i -b ′ (θ i ) a(ϕ) , ∂θ i ∂µ i = 1 b ′′ (θ i ) = a(ϕ) σ 2 i follow from Theorem 23.1. So ∂ℓ i ∂β = y i -b ′ (θ i ) σ 2 i ∂µ i ∂β = y i -µ i σ 2 i ∂µ i ∂β ,</formula><p>leading to the following score equation for the MLE:</p><formula xml:id="formula_719">n i=1 y i -µ i σ 2 i ∂µ i ∂β = 0,<label>(23.7)</label></formula><p>or, more explicitly,</p><formula xml:id="formula_720">n i=1 y i -µ(x t i β) σ 2 i µ ′ (x t i β)x i = 0</formula><p>The general relationship <ref type="bibr">(23.6</ref>) between θ i and β is quite complicated.</p><formula xml:id="formula_721">A natural choice of µ(•) is to cancel (b ′ ) -1 in (23.6) so that µ(•) = b ′ (•) =⇒ θ i = x t i β.</formula><p>This link function µ(•) is called the canonical link or the natural link, which leads to further simplifications:</p><formula xml:id="formula_722">µ i = b ′ (x t i β) =⇒ ∂µ i ∂β = b ′′ (x t i β)x i = b ′′ (θ i )x i = σ 2 i a(ϕ) x i ,<label>and</label></formula><formula xml:id="formula_723">∂ℓ i ∂β = y i -µ i σ 2 i σ 2 i a(ϕ) x i = a(ϕ) -1 x i (y i -µ i ) =⇒ a(ϕ) -1 n i=1 x i (y i -µ i ) = 0 =⇒ n i=1 x i (y i -µ i ) = 0. (23.8)</formula><p>We have shown that the MLEs of models (23.1)-( <ref type="formula" target="#formula_706">23</ref>.3) all satisfy <ref type="bibr">(23.8)</ref>. However, the MLE of <ref type="bibr">(23.4)</ref> does not because it does not use the natural link function resulting in µ(</p><formula xml:id="formula_724">•) ̸ = b ′ (•): µ( * ) = e * , b ′ ( * ) = δ e * 1 -e * .</formula><p>Using Bartlett's second identity in Lemma 23.1, we can write the expected Fisher information conditional on covariates as</p><formula xml:id="formula_725">n i=1 E ∂ℓ i ∂β ∂ℓ i ∂β t | x i = n i=1 E y i -µ i σ 2 i 2 ∂µ i ∂β ∂µ i ∂β t | x i = n i=1 1 σ 2 i ∂µ i ∂β ∂µ i ∂β t = n i=1 1 σ 2 i {µ ′ (x t i β)} 2 x i x t i = X t W X,</formula><p>where</p><formula xml:id="formula_726">W = diag 1 σ 2 i {µ ′ (x t i β)} 2 n i=1</formula><p>.</p><p>With the canonical link, it further simplifies to</p><formula xml:id="formula_727">n i=1 E ∂ℓ i ∂β ∂ℓ i ∂β t | x i = n i=1 E y i -µ i a(ϕ) 2 x i x t i | x i = {a(ϕ)} -2 n i=1 σ 2 i x i x t i .</formula><p>We can obtain the estimated covariance matrix by replacing the unknown parameters with their estimates. Now we review the estimated covariance matrices of the classical GLMs with canonical links. <ref type="bibr">Example 23.1 (continued)</ref> In the Normal linear model, V = σ2 (X t X) -1 with σ 2 estimated further by the residual sum of squares. <ref type="bibr">Example 23.2 (continued)</ref> In the binary logistic model, V = (X t Ŵ X) -1 with Ŵ = diag{π i (1 -πi )} n i=1 , where πi = e x t i β /(1 + e x t i β ).</p><p>Example <ref type="bibr">23.3 (continued)</ref> In the Poisson model, V = (X t Ŵ X) -1 with Ŵ = diag{ λi } n i=1 , where λi = e x t i β .</p><p>I relegate the derivation of the formula for the Negative-Binomial regression as Problem 23.2. It is a purely theoretical exercise since δ is usually unknown in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.3">Other GLMs</head><p>The glm function in R allows for the specification of the family parameters, with the corresponding canonical link functions shown below:</p><p>binomial ( link = " logit " ) gaussian ( link = " identity " ) Gamma ( link = " inverse " ) inverse . gaussian ( link = " 1 / mu ^2 " ) poisson ( link = " log " ) quasi ( link = " identity " , variance = " constant " ) quasibinomial ( link = " logit " ) quasipoisson ( link = " log " ) 3 correspond to the second, the first, and the fifth choices above. Below I will briefly discuss the third choice for the Gamma regression and omit the discussion of other choices. See the help file of the glm function and <ref type="bibr" target="#b176">McCullagh and Nelder (1989)</ref> for more details.</p><p>The Gamma(α, β) random variable is positive with mean α/β and variance α/β 2 . For convenience in modeling, we use a reparametrization Gamma ′ (µ, ν) where</p><formula xml:id="formula_728">µ ν = α/β α ⇐⇒ α β = ν ν/µ .</formula><p>So its mean equals µ and its variance equals µ 2 /ν which is quadratic in µ. A feature of the Gamma random variable is that its coefficient of variation equals 1/ √ ν which does not depend on the mean. So Gamma ′ (µ, ν) is a parametrization based on the mean and the coefficient of variation <ref type="bibr" target="#b176">(McCullagh and Nelder, 1989</ref>).<ref type="foot" target="#foot_23">foot_23</ref> Gamma ′ (µ, ν) has density</p><formula xml:id="formula_729">f (y) = β α Γ(α) y α-1 e -βy = (ν/µ) ν Γ(ν) y ν-1 e -(ν/µ)y ,</formula><p>and we can verify that it belongs to the exponential family with dispersion. Gamma regression assumes</p><formula xml:id="formula_730">y i | x i ∼ Gamma ′ (µ i , ν)</formula><p>where µ i = e x t i β . So it is also a log-linear model. This does not correspond to the canonical link. Instead, we should specify Gamma(link = "log") to fit the log-linear Gamma regression model.</p><p>The log-likelihood function is</p><formula xml:id="formula_731">log L(β, ν) = n i=1 - νy i e x t i β + (ν -1) log y i + ν log ν -νx t i β -log Γ(ν) . Then ∂ log L(β, ν) ∂β = n i=1 (νy i e -x t i β x i -νx i ) = ν n i=1 e -x t i β (y i -e x t i β )x i and ∂ 2 log L(β, ν) ∂β∂ν = n i=1 e -x t i β (y i -e x t i β )x i .</formula><p>So the MLE for β solves the following estimating equation</p><formula xml:id="formula_732">n i=1 e -x t i β (y i -e x t i β )x i = 0.</formula><p>Moreover, ∂ 2 log L(β, ν)/∂β∂ν has expectation zero so the Fisher information matrix is diagonal. In fact, it is identically zero when evaluated at β since it is identical to the estimating equation. I end this subsection with a comment on the estimating equation of β. It is similar to the Poisson score equation except for the additional weight e -x t i β . For positive outcomes, it is also conventional to fit OLS of log y i on x i , resulting in the following estimating equation <ref type="bibr" target="#b98">Firth (1988)</ref> compared Gamma and log-Normal regressions based on efficiency. However, these two models are not entirely comparable: Gamma regression assumes that the log of the conditional mean of y i given x i is linear in x i , whereas log-Normal regression assumes that the conditional mean of log y i given x i is linear in x i . By Jensen's inequality, log E(</p><formula xml:id="formula_733">n i=1 (log y i -x t i β)x i = 0.</formula><formula xml:id="formula_734">y i | x i ) ≥ E(log y i | x i ). See Problem 23.</formula><p>3 for more discussions of Gamma regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.4">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.1">MLE in GLMs with binary regressors</head><p>The MLEs for β in Models ( <ref type="formula" target="#formula_706">23</ref>.1)-( <ref type="formula" target="#formula_706">23</ref>.3) do not have explicit formulas in general. But in the special case with the covariate x i containing 1 and a binary covariate z i ∈ {0, 1}, their MLEs do have simple formulas. Find them in terms of sample means of the outcomes. Then find the variance estimators of β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.2">Negative-Binomial covariance matrices</head><p>Assume that δ is known. Derive the estimated asymptotic covariance matrices of the MLE in the Negative-Binomial regression with µ i = e x t i β .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23.3">Gamma regression</head><p>Verify that Gamma ′ (µ, ν) belongs to the natural exponential family with dispersion. Derive the first-and second-order derivatives of the log-likelihood function and Newton's algorithm for computing the MLE. Derive the estimated asymptotic covariance matrices of the MLE. Show that if</p><formula xml:id="formula_735">y i | x i ∼ Gamma ′ (µ i , ν) with µ i = e x t i β , then E(log y i | x i ) = ψ(ν) -log(ν) + x t i β and var(log y i | x i ) = ψ ′ (ν)</formula><p>where ψ(ν) = d log Γ(ν)/dν is the digamma function and ψ ′ (ν) is the trigamma function.</p><p>Remark: Use Proposition B.3 to calculate the moments. The above conditional mean function ensures that the OLS estimator of log y i on x i is consistent for all components of β except for the intercept.</p><p>From Generalized Linear Models to Restricted Mean Models: the Sandwich Covariance Matrix This chapter discusses the consequence of misspecified GLMs, extending the EHW covariance estimator to its analogs under the GLMs. It serves as a stepping stone to the next chapter on the generalized estimating equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.1">Restricted mean model</head><p>The logistic, Poisson, and Negative-Binomial models are extensions of the Normal linear model. All of them are fully parametric models. However, we have also discussed OLS as a restricted mean model E(y i | x i ) = x t i β without imposing any additional assumptions (e.g., the variance) on the conditional distribution. The restricted mean model is a semiparametric model. Then a natural question is: what are the analogs of the restricted mean model for the binary and count models?</p><p>Binary outcome is too special because the conditional mean determines the distribution. So if we assume that the conditional mean is µ i = e x t i β /(1 + e x t i β ), then conditional distribution must be Bernoulli(µ i ). Consequently, misspecification of the conditional mean function implies misspecification of the whole conditional distribution.</p><p>For other outcomes, the conditional mean cannot determine the conditional distribution. If we assume E(y i | x i ) = µ(x t i β), we can verify that</p><formula xml:id="formula_736">E n i=1 y i -µ(x t i β) σ2 (x i , β) ∂µ(x t i β) ∂β = E E n i=1 y i -µ(x t i β) σ2 (x i , β) ∂µ(x t i β) ∂β | x i = 0</formula><p>for any σ2 that can be a function of x i , the true parameter β, and maybe ϕ. So we can estimate β by solving the estimating equation:</p><formula xml:id="formula_737">n i=1 y i -µ(x t i β) σ2 (x i , β) ∂µ(x t i β) ∂β = 0. (24.1) If σ2 (x i , β) = σ 2 (x i ) = var(y i | x i</formula><p>), then the above estimating equation is the score equation derived from the GLM of an exponential family. If not, (24.1) is not a score function but it is still a valid estimating equation. In the latter case, σ2 (x i , β) is a "working" variance. This has important implications for the practical data analysis. First, we can interpret the MLE from a GLM more broadly: it is also valid under a restricted mean model even if the conditional distribution is misspecified. Second, we can construct more general estimators beyond the MLEs from GLMs. However, we must address the issue of variance estimation since the inference based on the Fisher information matrix no longer works in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.2">Sandwich covariance matrix</head><p>To simplify the notation, we assume (x i , y i ) n i=1 are IID draws although we usually view the covariates as fixed. This additional assumption is innocuous as the final inferential procedures are identical.</p><formula xml:id="formula_738">Theorem 24.1 Assume (x i , y i ) n i=1 are IID with E(y i | x i ) = µ(x t i β). We have √ n β -β → N(0, B -1 M B -1 ) with B = E 1 σ2 (x, β) ∂µ(x t β) ∂β ∂µ(x t β) ∂β t ,<label>(24.2)</label></formula><formula xml:id="formula_739">M = E σ 2 (x) {σ 2 (x, β)} 2 ∂µ(x t β) ∂β ∂µ(x t β) ∂β t . (24.3)</formula><p>Proof of Theorem 24.1: Applying Theorem D.1 to</p><formula xml:id="formula_740">w = (x, y), m(w, β) = y -µ(x t b) σ2 (x, β) ∂µ(x t β) ∂β ,</formula><p>we can derive the asymptotic distribution of the β. The bread matrix equals</p><formula xml:id="formula_741">B = -E ∂m(w, β) ∂β t = -E ∂ ∂β t y -µ(x t β) σ2 (x, β) ∂µ(x t β) ∂β = -E y -µ(x t β) σ2 (x, β) ∂ 2 µ(x t β) ∂β∂β t + ∂µ(x t β) ∂β ∂ ∂β t y -µ(x t β) σ2 (x, β) (24.4) = -E ∂µ(x t β) ∂β ∂ ∂β t y -µ(x t β) σ2 (x, β) = E ∂µ(x t β) ∂β ∂µ(x t β) ∂β t /σ 2 (x, β) + ∂µ(x t β) ∂β ∂ σ2 (x, β) ∂β t y -µ(x t β) {σ 2 (x, β)} 2 (24.5) = E ∂µ(x t β) ∂β ∂µ(x t β) ∂β t /σ 2 (x, β) ,</formula><p>where the first term of <ref type="bibr">(24.4</ref>) and the second term of (24.5) are both zero under the restricted mean model. The meat matrix equals</p><formula xml:id="formula_742">M = E {m(w, β)m(w, β) t } = E y -µ(x t β) σ2 (x) 2 ∂µ(x t β) ∂β ∂µ(x t β) ∂β t = E σ 2 (x) {σ 2 (x)} 2 ∂µ(x t β) ∂β ∂µ(x t β) ∂β t .</formula><p>□ We can estimate the asymptotic variance by replacing B and M by their sample analogs. With β and the residual εi = y i -µ(x t i β), we can conduct statistical inference based on the following Normal approximation:</p><formula xml:id="formula_743">β a ∼ N(β, V ), with V ≡ n -1 B-1 M B-1 , where B = n -1 n i=1 1 σ2 (x i , β) ∂µ(x t i β) ∂β ∂µ(x t i β) ∂β t , M = n -1 n i=1 ε2 i σ4 (x i , β) ∂µ(x t i β) ∂β ∂µ(x t i β) ∂β t .</formula><p>As a special case, when the GLM is correctly specified with σ 2 (x) = σ2 (x, β), then B = M and the asymptotic variance reduces to the inverse of the Fisher information matrix discussed in Section 23.2.</p><p>Example 24.1 (continued) In a working Normal linear model, σ2 (x i , β) = σ2 is constant and ∂µ(x t i β)/∂β = x i . So</p><formula xml:id="formula_744">B = n -1 n i=1 1 σ2 x i x t i , M = n -1 n i=1 ε2 i (σ 2 ) 2 x i x t i with residual εi = y i -x t i β, recovering the EHW variance estimator V = n i=1 x i x t i -1 n i=1 ε2 i x i x t i n i=1 x i x t i -1</formula><p>.</p><p>Example 24.2 (continued) In a working binary logistic model, σ2 (x i , β) = π(x i , β){1π(x i , β)} and ∂µ(x t i β)/∂β = π(x i , β){1 -π(x i , β)}x i , where π(x i , β) = µ(x t i β) = e x t i β /(1 + e x t i β ). So</p><formula xml:id="formula_745">B = n -1 n i=1 πi (1 -πi )x i x t i , M = n -1 n i=1 ε2 i x i x t i</formula><p>with fitted mean πi = e x t i β /(1 + e x t i β ) and residual εi = y i -πi , yielding a new covariance estimator</p><formula xml:id="formula_746">V = n i=1 πi (1 -πi )x i x t i -1 n i=1 ε2 i x i x t i n i=1 πi (1 -πi )x i x t i -1 . Example 24.3 (continued) In a working Poisson model, σ2 (x i , β) = λ(x i , β) and ∂µ(x t i β)/∂β = λ(x i , β)x i , where λ(x i , β) = µ(x t i β) = e x t i β . So B = n -1 n i=1 λi x i x t i , M = n -1 n i=1 ε2 i x i x t i</formula><p>with fitted mean λi = e x t i β and residual εi = y i -λi , yielding a new covariance estimator</p><formula xml:id="formula_747">V = n i=1 λi x i x t i -1 n i=1 ε2 i x i x t i n i=1 λi x i x t i -1</formula><p>.</p><p>Again, I relegate the derivation of the formulas for the Negative-Binomial regression as a homework problem. The R package sandwich implements the above covariance matrices <ref type="bibr" target="#b247">(Zeileis, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.3">Applications of the sandwich standard errors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.3.1">Linear regression</head><p>In R, several functions can compute the EHW standard error: the hccm function in the car package, and the vcovHC and sandwich functions in the sandwich package. The first two are special functions for OLS, and the third one works for general models. Below, we use these functions to compute various types of standard errors.</p><p>&gt; library ( " car " ) &gt; library ( " lmtest " ) &gt; library ( " sandwich " ) &gt; library ( " mlbench " ) &gt; &gt; # # linear regression &gt; data ( " BostonHousing " ) &gt; lm . boston = lm ( medv ~. , data = BostonHousing ) &gt; hccm 0 = hccm ( lm . boston , type = " hc 0 " ) &gt; sandwich 0 = sandwich ( lm . boston , adjust = FALSE ) &gt; vcovHC 0 = vcovHC ( lm . boston , type = " HC 0 " ) &gt; &gt; hccm 1 = hccm ( lm . boston , type = " hc 1 " ) &gt; sandwich 1 = sandwich ( lm . boston , adjust = TRUE ) &gt; vcovHC 1 = vcovHC ( lm . boston , type = " HC 1 " ) &gt; &gt; hccm 3 = hccm ( lm . boston , type = " hc 3 " ) &gt; vcovHC 3 = vcovHC ( lm . boston , type = " HC 3 " ) &gt; &gt; dat . reg = data . frame ( hccm 0 = diag ( hccm 0 )^( 0 . 5 ) , + sandwich 0 = diag ( sandwich 0 )^( 0 . 5 ) , + vcovHC 0 = diag ( vcovHC 0 )^( 0 . 5 ) , + + hccm 1 = diag ( hccm 1 )^( 0 . 5 ) , + sandwich 1 = diag ( sandwich 1 )^( 0 . 5 ) , + vcovHC 1 = diag ( vcovHC 1 )^( 0 . 5 ) , + + hccm 3 = diag ( hccm 3 )^( 0 . 5 ) , + vcovHC 3 = diag ( vcovHC 3 )^( 0 . 5 )) &gt; round ( dat . reg [ -1 , ] , 2 ) hccm 0 sandwich 0 vcovHC 0 hccm 1 sandwich 1 vcovHC 1 hccm 3 vcovHC 3 crim 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 0 . 0 3 zn 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 0 . 0 1 indus 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 0 . 0 5 chas 1 1 .</p><p>2 8 1 . 2 8 1 . 2 8 1 . 2 9 1 . 2 9 1 . 2 9 1 . 3 5 1 . 3 5 nox 3 . 7 3 3 . 7 3 3 . 7 3 3 . 7 9 3 . 7 9 3 . 7 9 3 . 9 2 3 . 9 2 rm 0 . 8 3 0 . 8 3 0 . 8 3 0 . 8 4 0 . 8 4 0 . 8 4 0 . 8 9 0 . 8 9 age 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 0 . 0 2 dis 0 . 2 1 0 . 2 1 0 . 2 1 0 . 2 1 0 . 2 1 0 . 2 1 0 . 2 2 0 . The sandwich function can compute HC0 and HC1, corresponding to adjusting for the degrees of freedom or not; hccm and vcovHC can compute other HC standard errors.</p><p>Estimate Std . Error z value Pr ( &gt;| z |) ( Intercept ) -6 . 6 7 6 4 1 2 . 4 6 0 3 5 -2 . 7 1 3 6 0 . 0 0 6 6 5 6 ** x 1 . 1 0 8 3 2 0 . 3 9 6 7 2 2 . 7 9 3 7 0 . 0 0 5 2 1 1 **</p><p>24.3.3 Poisson regression 24.3.3.1 A correctly specified Poisson regression I first generate data from a correctly specified Poisson regression. The two types of standard errors are very close. Because the true model is the Negative-Binomial regression, we can use the correct model to fit the data. Theoretically, the MLE is the most efficient estimator. However, in this particular dataset, the robust standard error from Poisson regression is no larger than the one from Negative-Binomial regression. Moreover, the robust standard errors from the Poisson and Negative-Binomial regressions are very close. reasonable if the parameter of interest is the risk ratio instead of the odds ratio. Importantly, since the Poisson model is a wrong model, we must use the sandwich covariance estimator. <ref type="bibr">24.3.5</ref> How robust are the robust standard errors? Section 24.1 discusses the restricted mean model as an extension of the GLM, allowing for misspecification of the GLM while still preserving the conditional mean. We can extend the discussion to other parametric models. <ref type="bibr" target="#b146">Huber (1967)</ref> started the literature on the statistical properties of the MLE in a misspecified model, and <ref type="bibr" target="#b238">White (1982)</ref> addressed detailed inferential problems. <ref type="bibr" target="#b67">Buja et al. (2019b)</ref> reviewed this topic recently.</p><p>The discussion in Section 24.1 is useful when the conditional mean is correctly specified. However, if we think the GLM is severely misspecified with a wrong conditional mean, then the robust sandwich standard errors are unlikely to be helpful because the MLE converges to a wrong parameter in the first place <ref type="bibr" target="#b109">(Freedman, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.4">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.1">MLE in GLMs with binary regressors</head><p>Continue with Problem 23.1. Find the variance estimators of β without assuming the models are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.2">Negative-Binomial covariance matrices</head><p>Continue with Problem 23.2. Derive the estimated asymptotic covariance matrices of the MLE without assuming the Negative-Binomial model is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.3">Robust standard errors in the Karolinska data</head><p>Report the robust standard errors in the case study of Section 21.5 in Chapter 21. For some models, the function coeftest(*, vcov = sandwich) does work. Alternatively, you can use the nonparametric bootstrap to obtain the robust standard errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.4">Robust standard errors in the gym data</head><p>Report the robust standard errors in the case study of Section 22.3 in Chapter 22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized Estimating Equation for Correlated Multivariate Data</head><p>Previous chapters dealt with cross-sectional data, that is, we observe n units at a particular time point, collecting various covariates and outcomes. In addition, we assume that these units are independent, and sometimes, we even assume they are IID draws. Many applications have correlated data. Two canonical examples are (E1) repeated measurements of the same set of units over time, which are often called longitudinal data in biostatistics <ref type="bibr" target="#b103">(Fitzmaurice et al., 2012)</ref> or panel data in econometrics <ref type="bibr" target="#b240">(Wooldridge, 2010)</ref>; and</p><p>(E2) clustered observations belonging to classrooms, villages, etc, which are common in cluster-randomized experiments in education <ref type="bibr" target="#b207">(Schochet, 2013)</ref> and public health <ref type="bibr">(Turner et al., 2017a,b)</ref>.</p><p>Many excellent textbooks cover this topic intensively. This chapter focuses on a simple yet powerful strategy, which is a natural extension of the GLM discussed in the last chapter.</p><p>It was initially proposed in <ref type="bibr" target="#b165">Liang and Zeger (1986)</ref>, the most cited paper published in Biometrika in the past one hundred years <ref type="bibr" target="#b221">(Titterington, 2013)</ref>. For simplicity, we will use the term "longitudinal data" for general correlated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.1">Examples of correlated data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.1.1">Longitudinal data</head><p>We have used the data from <ref type="bibr" target="#b202">Royer et al. (2015)</ref> in Chapter 22. Each worker's number of gym visits was repeatedly measured over more than 100 weeks. It is a standard longitudinal dataset. In Chapter 22, we conducted analysis for each week separately, and in this chapter, we will accommodate the longitudinal structure of the data.</p><p>25.1.2 Clustered data: a neuroscience experiment <ref type="bibr" target="#b180">Moen et al. (2016)</ref> examined the effects of Pten knockdown and fatty acid delivery on soma size of neurons in the brain of a mouse. The useful variables for our analysis are the id of mouse mouseid, the fatty acid level fa, the Pten knockdown indicator pten, the outcome somasize, the number of neurons numpten and numctrl under Pten knockdown or not.</p><p>&gt; Pten = read . csv ( " P t e n A n a l y s i s D a t a . csv " ) <ref type="bibr">[ , -( 7 : 9 )</ref>] &gt; head ( Pten ) mouseid fa pten somasize numctrl numpten 1 0 0 0 8 3 . 8 3 7 3 0 4 4 2 0 0 0 6 9 . 9 8 4 3 0 4 4 3 0 0 0 8 2 . 1 2 8 3 0 4 4 4 0 0 8 6 . 4 4 6 3 0 4 4 5 0 0 7 4 . 0 3 2 3 0 4 4 6 0 0 7 1 . 6 9 3 3 0 4 4</p><p>The three-way table below shows the treatment combinations for 14 mice, from which we can see that the Pten knockdown indicator varies within mice, but the fatty acid level varies only between mice.</p><p>&gt; table ( Pten $ mouseid , Pten $ fa , Pten $ pten ) , , = 0 0 1 0 3 0 0 1 5 8 0 2 1 8 0 3 2 0 4 5 6 0 5 0 3 9 6 0 3 3 7 0 5 8 8 0 6 0 9 0 0 1 0 0 0 1 1 0 0 1 2 0 0 1 3 0 0 , , = 1 0 1 0 4 4 0 1 6 8 0 2 3 3 0 3 1 1 0 4 7 6 0 5 0 5 5 6 0 5 5 7 0 7 5 8 0 9 2 9 0 0 1 0 0 0 1 1 0 0 1 2 0 0 1 3 0 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.1.3">Clustered data: a public health intervention</head><p>Poor sanitation leads to morbidity and mortality in developing countries. In 2012, <ref type="bibr" target="#b131">Guiteras et al. (2015)</ref> conducted a cluster-randomized experiment in rural Bangladesh to evaluate the effectiveness of different policies on the use of hygienic latrines. To illustrate our theory, we use a subset of their original data and exclude the households not eligible for subsidies or with missing outcomes, resulting in 10125 households in total. The median, mean, and maximum of village size are 83, 119, and 500, respectively. We choose the outcome y it as the binary indicator for whether the household (i, t) had access to a hygienic latrine or not, measured in June 2013, and covariate x it as the baseline access rate to hygienic latrines in the community that household (i, t) belonged to, measured in January 2012 before the experiment.</p><p>The useful variables below are z, x, y, and vid, which denote the binary treatment indicator, covariate x it , the outcome, and the village id vid, &gt; hygaccess = read . csv ( " hygaccess . csv " ) &gt; hygaccess = hygaccess [ , c ( " r 4 _ hyg _ access " , " treat _ cat _ 1 " , + " bl _ c _ hyg _ access " , " vid " , " eligible " )] &gt; hygaccess = hygaccess [ which ( hygaccess $ eligible == " Eligible " &amp; + hygaccess $ r 4 _ hyg _ access != " Missing " ) ,] &gt; hygaccess $ y = ifelse ( hygaccess $ r 4 _ hyg _ access == " Yes " , 1 , 0 ) &gt; hygaccess $ z = hygaccess $ treat _ cat _ 1 &gt; hygaccess $ x = hygaccess $ bl _ c _ hyg _ access</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.2">Marginal model and the generalized estimating equation</head><p>We will extend the restricted mean model to deal with longitudinal data, where we observe outcome y it and covariate x it for each unit i = 1, . . . , n at time t = 1, . . . , n i . The n i 's can vary across units. When n i = 1 for all units, we drop the time index and model the conditional mean as E(y i | x i ) = µ(x t i β), and use the following estimating equation to estimate the parameter β:</p><formula xml:id="formula_748">n i=1 y i -µ(x t i β) σ2 (x i , β) ∂µ(x t i β) ∂β = 0. (<label>25.1)</label></formula><p>In (25.1), σ2 (x i , β) is a working variance function usually motivated by a GLM, but it can be misspecified. With an n i × 1 vector outcome and an n i × p covariate matrix</p><formula xml:id="formula_749">Y i =    y i1 . . . y ini    , X i =    x t i1 . . . x t ini    , (i = 1, . . . , n) (25.2)</formula><p>we can extend the restricted mean model to <ref type="bibr">25.6)</ref> where <ref type="bibr">(25.</ref>3) and <ref type="bibr">(25.6) are definitions, and (25.4</ref>) and <ref type="bibr">(25.5</ref>) are the two key assumptions. Assumption <ref type="bibr">(25.4</ref>) requires that the conditional mean of y it depends only on x it but not on any other x is with s ̸ = t. Assumption <ref type="bibr">(25.5)</ref> requires that the relationship between x it and y it is stable across units and time points with the function µ(•) and the parameter β not varying with respect to i or t. The model assumptions in <ref type="bibr">(25.4</ref>) and <ref type="bibr">(25.5</ref>) are really strong, and I defer the critiques to the end of this chapter. Nevertheless, the marginal model attracts practitioners for (A1) its similarity to GLM and the restricted mean model, and</p><formula xml:id="formula_750">E(Y i | X i ) ≡    E(y i1 | X i ) . . . E(y ini | X i )    (25.3) =    E(y i1 | x i1 ) . . . E(y ini | x ini )    (25.4) =    µ(x t i1 β) . . . µ(x t ini β)    (25.5) ≡ µ(X i , β),<label>(</label></formula><p>(A2) its simplicity of requiring only specification of the marginal conditional means, not the whole joint distribution.</p><p>The advantage (A1) facilitates the interpretation of the coefficient, and the advantage (A2) is crucial because of the lack of familiar multivariate distributions in statistics except for the multivariate Normal. The generalized estimating equation (GEE) for β is the vector form of (25.1):</p><formula xml:id="formula_751">n i=1 ∂µ(X i , β) ∂β p×ni Ṽ -1 (X i , β) ni×ni {Y i -µ(X i , β)} ni×1 = 0 p×1 ,<label>(25.7)</label></formula><p>where <ref type="bibr">(25.7</ref>) has a similar form as (25.1) with three terms organized to match the dimension so that matrix multiplications are well-defined:</p><p>(GEE1) the last term</p><formula xml:id="formula_752">Y i -µ(X i , β) =    y i1 -µ(x t i1 β) . . . y ini -µ(x t ini β)   </formula><p>represents the residual vector, (GEE2) the second term is the inverse of Ṽ (X i , β), a working covariance matrix of the conditional distribution of Y i given X i which may be misspecified:</p><formula xml:id="formula_753">Ṽ (X i , β) ̸ = V (X i ) ≡ cov(Y i | X i ).</formula><p>It is relatively easy to specify the working variance σ2 (x it , β) for each marginal component, for example, based on the marginal GLM. So the key is to specify the</p><formula xml:id="formula_754">n i × n i dimensional correlation matrix R i to obtain Ṽ (X i , β) = diag{σ(x it , β)} ni i=1 R i diag{σ(x it , β)} ni i=1 .</formula><p>We assume that the R i 's are given now, and will discuss how to choose them in a later section.</p><p>(GEE3) the first term is the partial derivative of an n i × 1 vector µ(X i , β) = (µ(x t i1 β), . . . , µ(x t ini β)) t with respect to a p × 1 vector β = (β 1 , . . . , β p ) t :</p><formula xml:id="formula_755">∂µ(X i , β) ∂β = ∂µ(x t i1 β) ∂β , . . . , ∂µ(x t ini β) ∂β =      ∂µ(x t i1 β) ∂β1 • • • ∂µ(x t in i β) ∂β1 . . . . . . ∂µ(x t i1 β) ∂βp • • • ∂µ(x t in i β) ∂βp     </formula><p>, which is a p × n i matrix, denoted by D i (β).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.3">Statistical inference with GEE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.3.1">Computation using the Gauss-Newton method</head><p>We can use Newton's method to solve the GEE <ref type="bibr">(25.7)</ref>. However, calculating the derivative of the left-hand side of (25.7) involves calculating the second order derivative of µ(X i , β) with respect to β. A simpler alternative without calculating the second-order derivative is the Gauss-Newton method based on the following approximation:</p><formula xml:id="formula_756">0 = n i=1 ∂µ(X i , β) ∂β Ṽ -1 (X i ) {Y i -µ(X i , β)} ∼ = n i=1 D i (β old ) Ṽ -1 (X i , β old ) Y i -µ(X i , β old ) -D t i (β old )(β -β old ) = n i=1 D i (β old ) Ṽ -1 (X i , β old ) Y i -µ(X i , β old ) - n i=1 D i (β old ) Ṽ -1 (X i , β old )D t i (β old )(β -β old ).</formula><p>So given β old , we update it as</p><formula xml:id="formula_757">β new = β old + n i=1 D i (β old ) Ṽ -1 (X i , β old )D t i (β old ) -1 × n i=1 D i (β old ) Ṽ -1 (X i , β old ) Y i -µ(X i , β old ) . (25.8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.3.2">Asymptotic inference</head><p>The asymptotic distribution of β follows from Theorem D.2. Similar to the proof of Theorem 24.1, we can verify that √ n( β -β) → N(0, B -1 M B -1 ) in distribution where</p><formula xml:id="formula_758">B = E n -1 n i=1 D i (β) Ṽ -1 (X i , β)D t i (β) , M = E n -1 n i=1 D i (β) Ṽ -1 (X i , β)V (X i ) Ṽ -1 (X i , β)D t i (β) .</formula><p>After obtaining β and the residual vector εi = Y i -µ(X i , β) for unit i (i = 1, . . . , n), we can conduct asymptotic inference based on the Normal approximation</p><formula xml:id="formula_759">β a ∼ N(β, n -1 B-1 M B-1 ),</formula><p>where</p><formula xml:id="formula_760">B = n -1 n i=1 D i ( β) Ṽ -1 (X i , β)D t i ( β), M = n -1 n i=1 D i ( β) Ṽ -1 (X i , β)ε i εt i Ṽ -1 (X i , β)D t i ( β).</formula><p>This covariance estimator proposed by <ref type="bibr" target="#b165">Liang and Zeger (1986)</ref>, is robust to the misspecification of the marginal variances and the correlation structure as long as the conditional mean of Y i given X i is correctly specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.3.3">Implementation: choice of the working covariance matrix</head><p>We have not discussed the choice of the working correlation matrix R i . Different choices do not affect the consistency but affect the efficiency of β. A simple starting point is the independent working correlation matrix R i = I ni . Under this correlation matrix, the GEE reduces to</p><formula xml:id="formula_761">n i=1 ∂µ(x t i1 β) ∂β , . . . , ∂µ(x t ini β) ∂β    σ-2 (x i1 , β) . . . σ-2 (x ini , β)    ×    y i1 -µ(x t i1 β) . . . y ini -µ(x t ini β)    = 0,</formula><p>or, more compactly,</p><formula xml:id="formula_762">n i=1 ni t=1 y it -µ(x t it β) σ2 (x it , β) ∂µ(x t it β) ∂β = 0. (25.9)</formula><p>This is simply the estimating equation of a restricted mean model treating all data points (i, t) as independent observations. This implies that the point estimate assuming the independent working correlation matrix is still consistent, although we must change the standard error as in Section 25.3.2. With this simple starting point, we have a consistent yet inefficient estimate of β, and then we can compute the residuals. The correlation among the residuals contains information about the true covariance matrix. With small and equal n i 's, we can estimate the conditional covariance without imposing any structure based on the residuals. Using the estimated covariance matrix, we can update the GEE estimate to improve efficiency. This leads to a two-step procedure.</p><p>An important intermediate case is motivated by the exchangeability of the data points within the same unit i, so the working covariance matrix is Ṽ (X</p><formula xml:id="formula_763">i , β) = diag{σ(x it )} ni i=1 R i (ρ)diag{σ(x it )} ni i=1 , where R i (ρ) =      1 ρ • • • ρ ρ 1 • • • ρ . . . . . . . . . ρ ρ • • • 1     </formula><p>.</p><p>We can estimate ρ based on the residuals from the first step.</p><p>The above three choices of the working covariance matrix are called "independent", "unstructured", and "exchangeable" in the "corstr" parameter of the function gee in the gee package in R. This function also contains other choices proposed by <ref type="bibr" target="#b165">Liang and Zeger (1986)</ref>.</p><p>A carefully chosen working covariance matrix can lead to efficiency gain compared to the simple independent covariance matrix. An efficient estimator requires a correctly specified working covariance matrix. This is often an infeasible goal, and what is more, the conditional covariance cov(Y i | X i ) is a nuisance parameter if the conditional mean is the main parameter of interest. In practice, the independent working covariance suffices in many applications despite its potential efficiency loss. This is similar to the use of OLS in the presence of heteroskedasticity in linear models. <ref type="bibr">Section 25.4</ref> focuses on the independent working covariance, which is common in econometrics. <ref type="bibr">Section 25.6</ref> gives further justifications for this simple strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.4">A special case: cluster-robust standard error</head><p>Importantly, <ref type="bibr" target="#b165">Liang and Zeger (1986)</ref>'s standard error treats each cluster i as an independent contributor to the uncertainty. In econometrics, this is called the cluster-robust standard error. Alternatively, we can use the bootstrap by resampling the clusters to approximate the asymptotic covariance matrix. I will discuss linear and logistic regressions in this section, and leave the technical details of Poisson regression as a homework problem.</p><p>Stack the Y i 's and X i 's in <ref type="bibr">(25.2)</ref> together to obtain</p><formula xml:id="formula_764">Y =    Y 1 . . . Y n    , X =    X 1 . . . X n    ,</formula><p>which are the N dimensional outcome vector and N × p covariate matrix, where N = n i=1 n i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.4.1">OLS</head><p>An important special case is the marginal linear model with an independent working covariance matrix and homoskedasticity, resulting in the following estimating equation:</p><formula xml:id="formula_765">n i=1 ni t=1</formula><p>x it (y it -x t it β) = 0.</p><p>So the point estimator is just the pooled OLS using all data points:</p><formula xml:id="formula_766">β = n i=1 ni t=1 x it x t it -1 n i=1 ni t=1 x it y it = n i=1 X t i X i -1 n i=1 X t i Y i = (X t X) -1 X t Y.</formula><p>The three forms of β above are identical: the first one is based on N observations, the second one is based on n independent units, and the last one is based on the matrix form with the pooled data. Although the point estimate is identical to the case with independent data points, we must adjust for the standard error according to Section 25.3.2. From</p><formula xml:id="formula_767">D i (β) = (x i1 , . . . , x ini ) = X t i , we can verify that ĉ ov( β) = n i=1 X t i X i -1 n i=1 X t i εi εt i X i n i=1 X t i X i -1</formula><p>, where εi = Y i -X i β = (ε i1 , . . . , εini ) t is the residual vector of unit i. This is called the (Liang-Zeger) cluster-robust covariance matrix in econometrics. The square roots of the diagonal terms are called the cluster-robust standard errors. The cluster-robust covariance matrix is often much larger than the (Eicker-Huber-White) heteroskedasticity-robust covariance matrix assuming independence of observations (i, t):</p><formula xml:id="formula_768">ĉ ov ehw ( β) = n i=1 ni t=1 x it x t it -1 n i=1 ni t=1 ε2 it x it x t it n i=1 ni t=1 x it x t it -1</formula><p>.</p><p>Note that</p><formula xml:id="formula_769">X t X = n i=1 X t i X i = n i=1 ni t=1</formula><p>x it x t it , so the bread matrices in ĉ ov( β) and ĉ ov ehw ( β) are identical. The only difference is due to the meat matrices:</p><formula xml:id="formula_770">n i=1 X t i εi εt i X i = n i=1 ni t=1 εit x it ni t=1 εit x it t ̸ = n i=1 ni t=1 ε2 it x it x t it in general.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.4.2">Logistic regression</head><p>For binary outcomes, we can use the marginal logistic model with an independent working covariance matrix, resulting in the following estimating equation:</p><formula xml:id="formula_771">n i=1 ni t=1</formula><p>x it {y it -π(x it , β)} = 0</p><p>where π(x it , β) = e x t it β /(1 + e x t it β ). So the point estimator is the pooled logistic regression using all data points, but we must adjust for the standard error according to Section 25.3.2. From</p><formula xml:id="formula_772">D i (β) = (π(x i1 , β){1 -π(x i1 , β)}x i1 , . . . , π(x ini , β){1 -π(x ini , β)}x ini ) = X t i Ṽ (X i , β), with Ṽ (X i , β) = diag{π(x it , β){1 -π(x it , β)}} ni t=1 , we can verify that B = n -1 n i=1 X t i Vi X i , M = n -1 n i=1 X t i εi εt i X i ,</formula><p>where εi = (ε i1 , . . . , εini ) t with residual εit = y it -e xit β /(1 + e xit β ), and Vi = diag{π(x it , β){1 -π(x it , β)}} ni t=1 . So the cluster-robust covariance estimator for logistic regression is</p><formula xml:id="formula_773">ĉ ov( β) = n i=1 X t i Vi X i -1 n i=1 X t i εi εt i X i n i=1 X t i Vi X i -1</formula><p>.</p><p>I leave the cluster-robust covariance estimator for Poisson regression to Problem 25.5.</p><p>zLPP + Subsidy + Supply 0 . 7 3 8 9 0 . 0 5 5 7 8 1 3 . 2 4 6 0 . 1 3 6 1 5 . 4 zSupply Only 0 . 3 6 1 4 0 . 0 7 5 1 4 4 . 8 1 0 0 . 2 4 2 6 1 . 4 x 2 . 0 4 8 8 0 . 0 8 2 0 9 2 4 . 9 5 7 0 . 2 1 5 8 9 . 4 &gt; &gt; hygaccess . gee = gee ( y ~z + x , id = vid , + family = binomial ( link = logit ) , + corstr = " exchangeable " , + data = hygaccess ) &gt; summary ( hygaccess . gee )$ coef Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -1 . 7 9 7 6 0 . 1 3 2 4 -1 3 . 5 7 5 0 . 1 5 4 1 -1 1 . 6 zLPP Only 0 . 3 0 3 8 0 . 1 7 8 1 1 . 7 0 5 0 . 1 9 4 6 1 . 5 zLPP + Subsidy 0 . 7 2 2 7 0 . 1 3 1 6 5 . 4 9 1 0 . 1 2 7 1 5 . 6 zLPP + Subsidy + Supply 0 . 8 5 4 7 0 . 1 3 2 7 6 . 4 4 1 0 . 1 2 4 7 6 . 8 zSupply Only 0 . 3 2 3 6 0 . 1 9 1 1 1 . 6 9 3 0 . 2 3 9 8 1 . 3 x 1 . 9 4 9 7 0 . 1 1 2 8 1 7 . 2 8 6 0 . 1 9 4 7 1 0 . 0</p><p>Covariate adjustment improves efficiency and makes the choice of the working covariance matrix less important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.5.3">Longitudinal data</head><p>The regression formula f.reg will remain the same although other parameters may vary.</p><p>&gt; library ( " gee " ) &gt; library ( " foreign " ) &gt; gym 1 = read . dta ( " gym _ treatment _ exp _ weekly . dta " ) &gt; f . reg = weekly _ visit ~incentive _ commit + incentive + target + member _ gym _ pre Using all data, we find a significant effect of incentive_commit but an insignificant effect of incentive.</p><p>normal . gee = gee ( f . reg , id = id , + family = gaussian , + corstr = " independence " , + data = gym 1 ) &gt; normal . gee = summary ( normal . gee )$ coef &gt; normal . gee Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -0 . 6 9 0 0 5 0 . 0 1 1 1 3 6 -6 1 . 9 6 8 0 . 0 8 6 7 2 -7 . 9 5 7 2 incentive _ commit 0 . 1 5 6 6 6 0 . 0 0 8 3 5 8 1 8 . 7 4 5 0 . 0 6 3 7 6 2 . 4 5 6 9 incentive 0 . 0 1 0 2 2 0 . 0 0 8 2 7 5 1 . 2 3 5 0 . 0 5 9 1 0 0 . 1 7 2 9 target 0 . 6 2 6 6 6 0 . 0 0 7 4 6 5 8 3 . 9 4 9 0 . 0 6 7 7 3 9 . 2 5 2 7 member _ gym _ pre 1 . 1 4 9 1 9 0 . 0 0 7 0 7 7 1 6 2 . 3 7 5 0 . 0 6 2 5 2 1 8 . 3 8 0 1</p><p>However, this pooled analysis can be misleading because we have seen from the analysis before that the treatments have no effects in the pre-experimental periods and smaller effects in the long term. A pooled analysis can dilute the short-term effects, missing the treatment effect heterogeneity across time. This can be fixed by the following subgroup analysis based on time.</p><p>&gt; normal . gee 1 = gee ( f . reg , id = id , + subset = ( incentive _ week &lt; 0 ) , + family = gaussian , + corstr = " independence " , + data = gym 1 ) &gt; normal . gee 1 = summary ( normal . gee 1 )$ coef &gt; normal . gee 1 Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -0 . 8 7 9 3 7 4 0 . 0 4 2 3 0 -2 0 . 7 8 6 8 0 . 0 8 7 3 9 -1 0 . 0 6 2 incentive _ commit -0 . 0 0 4 2 4 1 0 . 0 3 1 7 5 -0 . 1 3 3 6 0 . 0 6 2 4 3 -0 . 0 6 7 incentive -0 . 0 7 3 8 8 4 0 . 0 3 1 4 4 -2 . 3 5 0 2 0 . 0 6 2 2 3 -1 . 1 8 7 target 0 . 7 4 2 6 7 5 0 . 0 2 8 3 6 2 6 . 1 8 8 7 0 . 0 6 7 0 1 1 1 . 0 8 3 member _ gym _ pre 1 . 6 0 1 5 6 9 0 . 0 2 6 8 9 5 9 . 5 6 6 4 0 . 0 6 6 0 0 2 4 . 7 6 3 &gt; &gt; &gt; normal . gee 2 = gee ( f . reg , id = id , + subset = ( incentive _ week &gt; 0 &amp; incentive _ week &lt; 1 ) , + family = gaussian , + corstr = " independence " , + data = gym 1 ) &gt; normal . gee 2 = summary ( normal . gee 2 )$ coef &gt; normal . gee 2 Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -0 . 7 9 2 5 0 . 0 3 2 7 5 -2 4 . 1 9 4 0 . 0 8 9 8 2 -8 . 8 incentive _ commit 0 . 3 6 6 2 0 . 0 2 4 5 8 1 4 . 8 9 8 0 . 0 6 8 9 5 5 . 3 incentive 0 . 1 7 4 4 0 . 0 2 4 3 4 7 . 1 6 6 0 . 0 6 4 5 7 2 . 7 target 0 . 6 7 3 5 0 . 0 2 1 9 6 3 0 . 6 7 4 0 . 0 7 1 5 9 9 . 4 member _ gym _ pre 1 . 4 1 3 8 0 . 0 2 0 8 2 6 7 . 9 1 4 0 . 0 6 7 2 7 2 1 . 0 &gt; &gt; normal . gee 3 = gee ( f . reg , id = id , + subset = ( incentive _ week &gt;= 1 5 ) , + family = gaussian , + corstr = " independence " , + data = gym 1 ) &gt; normal . gee 3 = summary ( normal . gee 3 )$ coef &gt; normal . gee 3 Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -0 . 6 6 1 5 0 0 0 . 0 1 2 2 2 2 -5 4 . 1 3 0 . 0 9 0 2 8 -7 . 3 3 incentive _ commit 0 . 1 3 4 7 8 9 0 . 0 0 9 1 7 3 1 4 . 6 9 0 . 0 6 6 7 6 2 . 0 9 incentive -0 . 0 0 9 7 1 6 0 . 0 0 9 0 8 2 -1 . 0 7 0 . 0 6 1 4 2 -0 . 1 2 target 0 . 6 1 1 6 3 5 0 . 0 0 8 1 9 3 7 4 . 6 6 0 . 0 7 0 4 2 8 . 6 0 member _ gym _ pre 1 . 0 7 7 8 7 4 0 . 0 0 7 7 6 8 1 3 8 . 7 7 0 . 0 6 4 9 4 1 6 . 5 7</p><p>Changing the family parameter to poisson(link = log), we can fit a marginal log-linear model with independent Poisson covariance. Figure <ref type="figure" target="#fig_73">25</ref>.1 shows the point estimates and confidence intervals based on the regressions above. The confidence intervals based on the cluster-robust standard errors are much wider than those based on the EHW standard errors. Without dealing with clustering, the confidence intervals are too narrow and give wrong inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.6">Critiques on the key assumptions</head><p>Consider the simple case with n i = 2 for all i below. <ref type="bibr">25.6.1 Assumption (25.4)</ref> Assumption <ref type="bibr">(25.4)</ref> </p><formula xml:id="formula_774">requires E(y it | X i ) = E(y it | x it ),</formula><p>which holds automatically if x it = x i is time-invariant. With time-varying covariates, it effectively rules out the dynamics between x and y. Assumption <ref type="bibr">(25.4</ref>) holds in the following data-generating process:</p><formula xml:id="formula_775">x i1 / /</formula><p>x i2 y i1 y i2 q q q q q q q q q q q q q q q q pooled before short long  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal</head><formula xml:id="formula_776">x i1 / / " " x i2 y i1 y i2 or x i1 / / x i2 y i1 &lt; &lt; y i2 or x i1 / / " " x i2 y i1 &lt; &lt; y i2</formula><p>With more complex data generating processes, Assumption <ref type="bibr">(25.4)</ref> does not hold in general: <ref type="bibr" target="#b165">Liang and Zeger (1986)</ref> assumed fixed covariates, ruling out the dynamics of x. <ref type="bibr" target="#b189">Pepe and Anderson (1994)</ref> pointed out the importance of Assumption <ref type="bibr">(25.4)</ref> in GEE with random time-varying covariates. <ref type="bibr" target="#b189">Pepe and Anderson (1994)</ref> also showed that with an independent working covariance matrix, we can drop Assumption <ref type="bibr">(25.4)</ref> as long as the marginal conditional mean is correctly specified. That is, if</p><formula xml:id="formula_777">x i1 / / " " x i2 y i1 &lt; &lt; / / y i2</formula><formula xml:id="formula_778">E(y it | x it ) = µ(x t it β), then E n i=1 ni t=1 y it -µ(x t it β) σ2 (x it , β) ∂µ(x t it β) ∂β = n i=1 ni t=1 E E{y it -µ(x t it β) | x it } σ2 (x it , β) ∂µ(x t it β) ∂β = 0.</formula><p>This gives another justification for the use of the independent working covariance matrix even though it can result in efficiency loss when Assumption <ref type="bibr">(25.4</ref>) holds. <ref type="bibr">25.6.2 Assumption (25.5)</ref> Assumption <ref type="bibr">(25.5</ref>) requires a "stable" relationship between x and y across clusters and time points:</p><p>E(y it | x it ) = µ(x t it β) where µ and β do not depend on i or t. For clustered data, we can justify this assumption by the exchangeability of the units within clusters. However, it is much harder to interpret or justify it for longitudinal data with complex outcome dynamics.</p><p>We consider linear structural equations with a scalar time-invariant covariate. Without direct dependence of y i2 on y i1 , the data generating process</p><formula xml:id="formula_779">y i1 = α 1 + βx i + ε i1 , y i2 = α 2 + βx i + ε i2 , corresponding to the graph x i } } ! ! y i1 y i2 has conditional expectations E(y it | x i ) = α t + βx i if E(ε it | x i ) = 0. (25.10)</formula><p>However, with direct dependence of y i2 on y i1 , the data generating process <ref type="formula" target="#formula_748">25</ref>.10) holds. The stability assumption requires</p><formula xml:id="formula_780">y i1 = α 1 + βx i + ε i1 , y i2 = α 2 + γy i1 + δx i + ε i2 , corresponding to the graph x i } } ! ! y i1 / / y i2 has conditional expectations E(y i1 | x i ) = α 1 + βx i but E(y i2 | x i ) = α 2 + γ(α 1 + βx i ) + δx i = (α 2 + γα 1 ) + (δ + βγ)x i if (</formula><formula xml:id="formula_781">α 1 = α 2 + γα 1 , β = βγ + δ,</formula><p>which are strange restrictions on the model parameters.</p><p>With time-varying covariates, this issue becomes even more subtle because Assumption <ref type="bibr">(25.4</ref>) is unlikely to hold in the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.6.3">Explanation and prediction</head><p>Liang and Zeger (1986)'s marginal model is more useful if the goal is to explain the relationship between x and y, in particular, a component of x it represents the time-invariant treatment and y it represents the time-varying outcomes. If the goal is prediction, then the marginal model can be problematic. For instance, if we observe the covariate value for a 25.4 Cluster-robust standard error in ANOVA This problem extends Problems 5.5, 6.3 and 19.6.</p><p>Inherit the setting from Problem 19.6. If the units are clustered by a factor c i ∈ {1, . . . , M } for i = 1, . . . , n, we can obtain the cluster-robust covariances Vlz and V ′ lz from the two WLS fits. Show that Vlz = V ′ lz .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.5">Cluster-robust standard error for Poisson regression</head><p>Similar to Sections 25.4.1 and 25.4.2, derive the cluster-robust covariance matrix for Poisson regression:</p><formula xml:id="formula_782">ĉ ov( β) = n i=1 X t i Vi X i -1 n i=1 X t i εi εt i X i n i=1 X t i Vi X i -1</formula><p>, where εi = Y i -µ(X i , β) and Vi = diag{e x t it β } ni t=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.6">Data analysis</head><p>Re-analyze the data from <ref type="bibr" target="#b202">Royer et al. (2015)</ref> using the exchangeable working covariance matrix. Compare the corresponding results with Figure <ref type="figure" target="#fig_73">25</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part VIII</head><p>Beyond Modeling the Conditional Mean</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantile Regression</head><p>26.1 From the mean to the quantile For a random variable y, we can define its mean as</p><formula xml:id="formula_783">E(y) = arg min µ∈R E (y -µ) 2 .</formula><p>With IID data (y i ) n i=1 , we can compute the sample mean</p><formula xml:id="formula_784">ȳ = n -1 n i=1 y i = arg min µ∈R n -1 n i=1 (y i -µ) 2 ,</formula><p>which satisfies the CLT:</p><formula xml:id="formula_785">√ n(ȳ -E(y)) → N(0, var(y))</formula><p>in distribution if the variance var(y) is finite.  However, the mean can miss important information about y. How about other features of the outcome y? Quantiles can characterize the distribution of y. For a random variable y, we can define its distribution function as F (c) = pr(y ≤ c) and its τ th quantile as</p><formula xml:id="formula_786">F -1 (τ ) = inf {q : F (q) ≥ τ } . ρτ(u) u τ = 1 3 ρτ(u) u τ = 1 2 ρτ(u) u τ = 2 3 FIGURE 26.2: Check function</formula><p>This defines a quantile function F -1 : [0, 1] → R. If the distribution function is strictly monotone, then the quantile function reduces to the inverse of the distribution function, and the τ -th quantile solves τ = pr(y ≤ q) as an equation of q. See Figure <ref type="figure" target="#fig_110">26</ref>.1. For simplicity, this chapter focuses on the case with a monotone distribution function. The definition above formulates the mean as the minimizer of an objective function. Similarly, we can define quantiles in an equivalent way below.</p><p>Proposition 26.1 With a monotone distribution function and positive density at the τ th quantile, we have</p><formula xml:id="formula_787">F -1 (τ ) = arg min q∈R E {ρ τ (y -q)} ,</formula><p>where</p><formula xml:id="formula_788">ρ τ (u) = u {τ -1(u &lt; 0)} = uτ, if u ≥ 0, -u(1 -τ ), if u &lt; 0,</formula><p>is the check function (the name comes from its shape; see Figure <ref type="figure" target="#fig_110">26.</ref>2). In particular, the median of y is median(y) = F -1 (0.5) = arg min q∈R E {|y -q|} .</p><p>Proof of Proposition 26.1: To simplify the proof, we further assume that y has density function f (•). We will use Leibniz's integral rule:</p><formula xml:id="formula_789">d dx b(x) a(x) f (x, t)dt = f (x, b(x))b ′ (x) -f (x, a(x))a ′ (x) + b(x) a(x)</formula><p>∂f (x, t) ∂x dt.</p><p>We can write</p><formula xml:id="formula_790">E {ρ τ (y -q)} = q -∞ (τ -1)(c -q)f (c)dc + ∞ q τ (c -q)f (c)dc.</formula><p>To minimize it over q, we can solve the first-order condition</p><formula xml:id="formula_791">∂E {ρ τ (y -q)} ∂q = (1 -τ ) q -∞ f (c)dc -τ ∞ q f (c)dc = 0.</formula><p>So (1 -τ )pr(y ≤ q) -τ {1 -pr(y ≤ q)} = 0 which implies that τ = pr(y ≤ q), so the τ th quantile satisfies the first-order condition. The second-order condition ensures it is the minimizer:</p><formula xml:id="formula_792">∂ 2 E {ρ τ (y -q)} ∂q 2 q=F -1 (τ )</formula><p>= f F -1 (τ ) &gt; 0 by Leibniz's integral rule again. □ The empirical distribution function is F (c) = n -1 n i=1 1(y i ≤ c), which is a step function, increasing but not strictly monotone. With Proposition 26.1, we can easily define the sample quantile as</p><formula xml:id="formula_793">F -1 (τ ) = arg min q∈R n -1 n i=1 ρ τ (y i -q),</formula><p>which may not be unique even though the population quantile is. We can view F -1 (τ ) as a set containing all minimizers, and with large samples the values in the set do not differ much. Similar to the sample mean, the sample quantile also satisfies a CLT.</p><p>Theorem 26.1 Assume (y i ) n i=1 iid ∼ y with distribution function F (•) that is strictly increasing and density function f (•) that is positive at the τ th quantile. The sample quantile is consistent for the true quantile and is asymptotically Normal:</p><formula xml:id="formula_794">√ n F -1 (τ ) -F -1 (τ ) → N 0, τ (1 -τ ) [f {F -1 (τ )}] 2 in distribution.</formula><p>In particular, the sample median satisfies</p><formula xml:id="formula_795">√ n F -1 (0.5) -median(y) → N 0, 1 4 [f {median(y)}] 2 in distribution.</formula><p>Proof of Theorem 26.1: Based on the first order condition in Proposition 26.1, the population quantile solves E{m τ (y -q)} = 0, and the sample quantile solves</p><formula xml:id="formula_796">n -1 n i=1 m τ (y i -q) = 0,</formula><p>where the check function has a partial derivative with respect to u except for the point 0:</p><formula xml:id="formula_797">m τ (u) = (τ -1)1(u &lt; 0) + τ 1(u &gt; 0) = τ -1(u &lt; 0).</formula><p>By Theorem D.1, we only need to find the bread and meat matrices, which are scalars now:</p><formula xml:id="formula_798">B = ∂E{m τ (y -q)} ∂q q=F -1 (τ ) = ∂E{τ -1(y ≤ q)} ∂q q=F -1 (τ ) = - ∂F (q) ∂q q=F -1 (τ ) = -f {F -1 (τ )},</formula><p>and</p><formula xml:id="formula_799">M = E {m τ (y -q)} 2 q=F -1 (τ ) = E {τ -1(y ≤ q)} 2 q=F -1 (τ ) = E τ 2 + 1(y ≤ q) -2 • 1(y ≤ q)τ q=F -1 (τ ) = τ 2 + τ -2τ 2 = τ (1 -τ ).</formula><p>Therefore, √ n{ F -1 (τ ) -F -1 (τ )} converges to Normal with mean zero and variance</p><formula xml:id="formula_800">M/B 2 = τ (1 -τ )/[f {F -1 (τ )}] 2 .</formula><p>□ To conduct statistical inference for the quantile F -1 (τ ), we need to estimate the density of y at the τ th quantile to obtain the estimated standard error of F -1 (τ ). Alternatively, we can use the bootstrap to obtain the estimated standard error. We will discuss the inference of quantiles in R in Section 26.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.2">From the conditional mean to conditional quantile</head><p>With an explanatory variable x for outcome y, we can define the conditional mean as</p><formula xml:id="formula_801">E(y | x) = arg min m(•) E {y -m(x)} 2 .</formula><p>We can use a linear function x t β to approximate the conditional mean with the population OLS coefficient</p><formula xml:id="formula_802">β = arg min b∈R p E (y -x t b) 2 = {E(xx t )} -1 E(xy),</formula><p>and the sample OLS coefficient</p><formula xml:id="formula_803">β = n -1 n i=1 x i x t i -1 n -1 n i=1</formula><p>x i y i .</p><p>We have discussed the statistical properties of β in previous chapters. Motivated by Proposition 26.1, we can define the conditional quantile function as</p><formula xml:id="formula_804">F -1 (τ | x) = arg min q(•) E [ρ τ {y -q(x)}] .</formula><p>We can use a linear function x t β(τ ) to approximate the conditional quantile function with</p><formula xml:id="formula_805">β(τ ) = arg min b∈R p E {ρ τ (y -x t b)}</formula><p>called the τ th population regression quantile, and</p><formula xml:id="formula_806">β(τ ) = arg min b∈R p n -1 n i=1 ρ τ (y i -x t i b) (26.1)</formula><p>called the τ th sample regression quantile. As a special case, when τ = 0.5, we have the regression median:</p><formula xml:id="formula_807">β(0.5) = arg min b∈R p n -1 n i=1 |y i -x t i b|,</formula><p>which is also called the least absolute deviations (LAD). <ref type="bibr" target="#b157">Koenker and Bassett Jr (1978)</ref> started the literature under a correctly specified conditional quantile model:</p><formula xml:id="formula_808">F -1 (τ | x) = x t β(τ ).</formula><p>The interpretation of the j-th coordinate of the coefficient, β j (τ ), is the partial influence of x ij on the τ th conditional quantile of y i given x i . <ref type="bibr" target="#b49">Angrist et al. (2006)</ref> discussed quantile regression under misspecification, viewing it as the best linear approximation to the true conditional quantile function. This chapter will focus on the statistical properties of the sample regression quantiles following <ref type="bibr" target="#b49">Angrist et al. (2006)</ref>'s discussion of statistical inference allowing for the misspecification of the quantile regression model. Before that, we first comment on the population regression quantiles based on some generative models. Below assume that the v i 's are IID independent of the covariates x i 's, with mean zero and distribution g(c) = pr(v i ≤ c).</p><p>Example 26.1 Under the linear model y i = x t i β + σv i , we can verify that</p><formula xml:id="formula_809">E(y i | x i ) = x t i β and F -1 (τ | x i ) = x t i β + σg -1 (τ ).</formula><p>Therefore, with the first regressor being 1, we have β 1 (τ ) = β 1 + σg -1 (τ ), β j (τ ) = β j , (j = 2, . . . , p).</p><p>In this case, both the true conditional mean and quantile functions are linear, and the population regression quantiles are constant across τ except for the intercept.</p><p>Example 26.2 Under a heteroskedastic linear model y i = x t i β + (x t i γ)v i with x t i γ &gt; 0 for all x i 's, we can verify that</p><formula xml:id="formula_810">E(y i | x i ) = x t i β and F -1 (τ | x i ) = x t i β + x t i γg -1 (τ ). Therefore, β(τ ) = β + γg -1 (τ ).</formula><p>In this case, both the true conditional quantile functions are linear, and all coordinates of the population regression quantiles vary with τ .</p><p>Example 26.3 Under the transformed linear model log y i = x t i β + σv i , we can verify that</p><formula xml:id="formula_811">E(y i | x i ) = exp(x t i β)M v (σ),</formula><p>where M v (t) = E(e tv ) is the moment generating function of v, and</p><formula xml:id="formula_812">F -1 (τ | x i ) = exp x t i β + σg -1 (τ ) .</formula><p>In this case, both the true conditional mean and quantile functions are log-linear in covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.3">Sample regression quantiles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.3.1">Computation</head><p>The regression quantiles (26.1) do not have explicit formulas in general, and we need to solve the optimization problem numerically. Motivated by the piece-wise linear feature of the check function, we decompose y i -x t i β into the difference between its positive part and negative part:</p><formula xml:id="formula_813">y i -x t i β = u i -v i , where u i = max(y i -x t i β, 0), v i = -min(y i -x t i β, 0). So the objective function simplifies to the summation of ρ τ (y i -x t i β) = τ u i + (1 -τ )v i ,</formula><p>which is simply a linear function of the u i 's and v i 's. Of course, these u i 's and v i 's are not arbitrary because they must satisfy the constraints by the data. Using the notation</p><formula xml:id="formula_814">Y =    y 1 . . . y n    , X =    x t 1 . . . x t n    , u =    u 1 . . . u n    , v =    v 1 . . . v n    ,</formula><p>finding the τ th regression quantile is equivalent to a linear programming problem with linear objective function and linear constraints:</p><formula xml:id="formula_815">min b,u,v τ 1 t n u + (1 -τ )1 t n v, s.t. Y = Xb + u -v,</formula><p>and u i ≥ 0, v i ≥ 0 (i = 1, . . . , n).</p><p>The function rq in the R package quantreg computes the regression quantiles with various choices of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.3.2">Asymptotic inference</head><p>Similar to the sample quantiles, the regression quantiles are also consistent for the population regression quantiles and asymptotically Normal. So we can conduct asymptotic inference based on the results in the following theorem <ref type="bibr" target="#b49">(Angrist et al., 2006)</ref>.</p><p>Theorem 26.2 Assume (y i , x i ) n i=1 iid ∼ (y, x). Under some regularity conditions, we have</p><formula xml:id="formula_816">√ n β(τ ) -β(τ ) → N(0, B -1 M B -1 )</formula><p>in distribution, where</p><formula xml:id="formula_817">B = E f y|x {x t β(τ )} xx t , M = E {τ -1 (y -x t β(τ ) ≤ 0)} 2 xx t ,</formula><p>with f y|x (•) denoting the conditional density of y given x.</p><p>Proof of Theorem 26.2: The population regression quantile solves</p><formula xml:id="formula_818">E {m τ (y -x t b)x} = 0,</formula><p>and the sample regression quantile solves</p><formula xml:id="formula_819">n -1 n i=1 m τ (y i -x t i b)x i = 0.</formula><p>By Theorem D.1, we only need to calculate the explicit forms of B and M . Let F y|x (•) and f y|x (•) be the conditional distribution and density functions. We have</p><formula xml:id="formula_820">E {m τ (y -x t b)x} = E [{τ -1(y -x t b ≤ 0)} x] = E τ -F y|x (x t b) x , so ∂E {m τ (y -x t b)x} ∂b t = -E f y|x (x t b)xx t .</formula><p>This implies the formula of B. The formula of M follows from</p><formula xml:id="formula_821">M = E m 2 τ (y -x t β(τ ))xx t = E {τ -1(y -x t β(τ ) ≤ 0)} 2 xx t .</formula><p>□ Based on Theorem 26.2, we can estimate the asymptotic covariance matrix of β(τ ) by n</p><formula xml:id="formula_822">-1 B-1 M B-1 , where M = n -1 n i=1 τ -1 y i -x t i β(τ ) ≤ 0 2 x i x t i and B = (2nh) -1 n i=1 1 |y i -x t i β(τ )| ≤ h x i x t i</formula><p>for a carefully chosen h. Powell (1991)'s theory suggests to use h satisfying condition h = O(n -1/3 ), but the theory is not so helpful since it only suggests the order of h. The quantreg package in R chooses a specific h that satisfies this condition. In finite samples, the bootstrap often gives a better estimation of the asymptotic covariance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.4">Numerical examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.4.1">Sample quantiles</head><p>We can use the quantile function to obtain the sample quantiles. However, it does not report standard errors. Instead, we can use the rq function to compute sample quantiles by regressing the outcome on constant 1. These two functions may return different sample quantiles when they are not unique. The difference is often small with large sample sizes. I use the following simulation to compare various methods for standard error estimation. The first data-generating process has a standard Normal outcome.</p><p>Exponential(1) Normal(0,1) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.05 0.10 0.15 0.20 quantiles standard errors method boot iid ker true FIGURE 26.3: Standard errors for sample quantiles library ( quantreg ) mc = 2 0 0 0 n = 2 0 0 taus = ( 1 : 9 )/ 1 0 get . se = function ( x ){ x $ coef [ 1 ,2 ]} q . normal = replicate ( mc ,{ y = rnorm ( n ) qy = rq ( y ~1 , tau = taus ) se . iid = summary ( qy , se = " iid " ) se . ker = summary ( qy , se = " ker " ) se . boot = summary ( qy , se = " boot " ) qy = qy $ coef se . iid = sapply ( se . iid , get . se ) se . ker = sapply ( se . ker , get . se ) se . boot = sapply ( se . boot , get . se ) c ( qy , se . iid , se . ker , se . boot ) })</p><p>In the above, se = "iid", se = "ker", and se = "boot" correspond to the standard errors by <ref type="bibr" target="#b157">Koenker and Bassett Jr (1978)</ref>, <ref type="bibr" target="#b191">Powell (1991)</ref>, and the bootstrap. I also run the same simulation but replace the Normal outcome with Exponential: y = rexp(n). Figure <ref type="figure" target="#fig_110">26</ref>.3 compares the estimated standard errors with the true asymptotic standard error in Theorem 26.1. Bootstrap works the best, and the one involving kernel estimation of the density seems biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.4.2">OLS versus LAD</head><p>I will use simulation to compare OLS and LAD. In rq, the default value is tau=0.5, fitting the LAD. The first data-generating process is a Normal linear model:</p><formula xml:id="formula_823">x = rnorm ( n ) simu . normal = replicate ( mc , { y = 1 + x + rnorm ( n ) c ( lm ( y ~x )$ coef [ 2 ] , rq ( y ~x )$ coef [ 2 ]) })</formula><p>The second data generating process replaces the error term to a Laplace distribution<ref type="foot" target="#foot_24">foot_24</ref> : simu . laplace = replicate ( mc , { y = 1 + x + rexp ( n ) -rexp ( n ) c ( lm ( y ~x )$ coef <ref type="bibr">[ 2 ]</ref> , rq ( y ~x )$ coef <ref type="bibr">[ 2 ]</ref>) })</p><p>OLS is the MLE under a Normal linear model, and LAD is the MLE under a linear model with independent Laplace errors.</p><p>The third data-generating process replaces the error term with standard Exponential:</p><p>simu . exp = replicate ( mc , { y = 1 + x + rexp ( n ) c ( lm ( y ~x )$ coef <ref type="bibr">[ 2 ]</ref> , rq ( y ~x )$ coef <ref type="bibr">[ 2 ]</ref>) })</p><p>The fourth data generating process has y i = 1 + e i x i with e i IID Exponential, so</p><formula xml:id="formula_824">E(y i | x i ) = 1 + x i , var(y i | x i ) = x 2 i ,</formula><p>which is a heteroskedastic linear model, and</p><formula xml:id="formula_825">F -1 (0.5 | x i ) = 1 + median(e i )x i = 1 + (log 2)x i ,</formula><p>which is a linear quantile model. The coefficients are different in the conditional mean and quantile functions.</p><p>x = abs ( x ) simu . x = replicate ( mc , { y = 1 + rexp ( n )* x c ( lm ( y ~x )$ coef <ref type="bibr">[ 2 ]</ref> , rq ( y ~x )$ coef <ref type="bibr">[ 2 ]</ref>) })</p><p>Figure <ref type="figure" target="#fig_110">26</ref>.4 compares OLS and LAD under the above four data-generating processes. With Normal errors, OLS is more efficient; with Laplace errors, LAD is more efficient. This confirms the theory of MLE. With Exponential errors, LAD is also more efficient than OLS. Under the fourth data-generating process, LAD is more efficient than OLS. In general, however, OLS and LAD target the conditional mean and conditional median, respectively. Since the parameters differ in general, the comparison of the standard errors is not very meaningful. Both OLS and LAD give useful information about the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.5">Application</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.5.1">Parents' and children's heights</head><p>I revisit Galton's data introduced in Chapter 2. The following code gives the coefficients for quantiles 0.1 to 0.9. 26.5.2 U.S. wage structure <ref type="bibr" target="#b49">Angrist et al. (2006)</ref> used quantile regression to study the U.S. wage structure. They used census data in <ref type="bibr">1980, 1990, and 2000</ref> to fit quantile regressions on log weekly wage on education and other variables. The following code gives the coefficients for quantile regressions with τ equaling 0.1 to 0.9. I repeated the regressions with data from three years. Due to the large sample size, I use the m-of-n bootstrap with m = 500. &gt; census 9 0 = read . dta ( " census 9 0 . dta " ) &gt; census 0 0 = read . dta ( " census 0 0 . dta " ) &gt; f . reg = logwk ~educ + exper + exper 2 + black &gt; &gt; m . boot = 5 0 0 &gt; rq 8 0 = rq ( f . reg , data = census 8 0 , tau = taus ) &gt; rqlist 8 0 = summary ( rq 8 0 , se = " boot " , + bsmethod = " xy " , mofn = m . boot ) &gt; rq 9 0 = rq ( f . reg , data = census 9 0 , tau = taus ) &gt; rqlist 9 0 = summary ( rq 9 0 , se = " boot " , + bsmethod = " xy " , mofn = m . boot ) &gt; rq 0 0 = rq ( f . reg , data = census 0 0 , tau = taus ) &gt; rqlist 0 0 = summary ( rq 0 0 , se = " boot " , + bsmethod = " xy " , mofn = m . boot )</p><p>Figure <ref type="figure" target="#fig_110">26</ref>.6 shows the coefficient of educ across years and across quantiles. In 1980, the coefficients are nearly constant across quantiles, showing no evidence of heterogeneity in the return of education. Compared with 1980, the return of education in 1990 increases across all quantiles, but it increases more at the upper quantiles. Compared with 1990, the return of education in 2000 decreases at the lower quantiles and increases at the upper quantiles, showing more dramatic heterogeneity across quantiles.</p><p>The original data used by <ref type="bibr" target="#b49">Angrist et al. (2006)</ref> contain weights due to sampling. Ideally, we should use the weights in the quantile regression. Like lm, the rq function also allows for specifying weights.</p><p>The R code in this section is in code24.5.R.</p><p>0.08 0.10 0.12 0.14 0.16 0.25 0.50 0.75 τ coefficient of educ year 00 90 80 quantile regressions FIGURE 26.6: Angrist et al. (2006)'s data 26.6 Extensions With clustered data, we must use the cluster-robust standard error which can be approximated by the clustered bootstrap with the rq function. I use Hagemann (2017)'s example below where the students are clustered in classrooms. See code24.6.R. &gt; star = read . csv ( " star . csv " ) &gt; star . rq = rq ( pscore ~small + regaide + black + + girl + poor + tblack + texp + + tmasters + factor ( fe ) , + data = star ) &gt; res = summary ( star . rq , se = " boot " )$ coef [ 2 : 9 , ] &gt; res . clus = summary ( star . rq , se = " boot " , + cluster = star $ classid )$ coef [ 2 : 9 , ] &gt; round ( res , 3 ) Value Std . Error t value Pr ( &gt;| t |) small</p><p>6 . 5 0 0 1 . 1 2 2 5 . 7 9 5 0 . 0 0 0 regaide 0 . 2 9 4 1 . 0 7 1 0 . 2 7 4 0 . 7 8 4 black -1 0 . 3 3 4 1 . 6 5 7 -6 . 2 3 7 0 . 0 0 0 girl 5 . 0 7 3 0 . 8 7 8 5 . 7 7 7 0 . 0 0 0 poor -1 4 . 3 4 4 1 . 0 2 4 -1 4 . 0 1 1 0 . 0 0 0 tblack -0 . 1 9 7 1 . 7 5 1 -0 . 1 1 3 0 . 9 1 0 texp 0 . 4 1 3 0 . 0 9 8 4 . 2 3 1 0 . 0 0 0 tmasters -0 . 5 3 0 1 . 0 6 8 -0 . 4 9 7 0 . 6 1 9 &gt; round ( res . clus , 3 ) Value Std . Error t value Pr ( &gt;| t |) small 6 . 5 0 0 1 . 6 6 2 3 . 9 1 2 0 . 0 0 0 regaide 0 . 2 9 4 1 . 6 2 7 0 . 1 8 1 0 . 8 5 7 black -1 0 . 3 3 4 1 . 8 4 9 -5 . 5 8 8 0 . 0 0 0 girl 5 . 0 7 3 0 . 8 1 9 6 . 1 9 5 0 . 0 0 0 poor -1 4 . 3 4 4 1 . 1 5 2 -1 2 . 4 5 5 0 . 0 0 0 tblack -0 . 1 9 7 3 . 1 1 3 -0 . 0 6 3 0 . 9 4 9 texp 0 . 4 1 3 0 . 1 6 8 2 . 4 6 5 0 . 0 1 4 tmasters -0 . 5 3 0 1 . 4 4 4 -0 . 3 6 7 0 . 7 1 3</p><p>With high dimensional covariates, we can use regularized quantile regression. For instance, the rq function can implement the lasso version with method = "lasso" and a prespecified lambda. It does not implement the ridge version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.7">Homework problems</head><p>26.1 Quantile regression with a binary regressor For i = 1, . . . , n, the first 1/3 observations have x i = 1 and the last 2/3 observations have x i = 0; y i | x i = 1 follows an Exponential(1), and y i | x i = 0 follows an Exponential <ref type="bibr">(2)</ref>. Find</p><formula xml:id="formula_826">(α, β) = arg min (a,b) n i=1 ρ 1/2 (y i -a -bx i ).</formula><p>and the joint asymptotic distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.2">Conditional quantile function in bivariate Normal</head><p>Show that if (x, y) follows a bivariate Normal, the conditional quantile function of y given x is linear in x with the same slope across all τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.3">Quantile range and variance</head><p>A symmetric random variable y satisfies y ∼ -y. Define the 1 -α quantile range of a symmetric random variable y as the interval of its α/2 and 1 -α/2 quantiles. Given two symmetric random variables y 1 and y 2 , show that if the 1 -α quantile range of y 1 is wider than that of y 2 for all α, then var(y 1 ) ≥ var(y 2 ). Does the converse of the statement hold? If so, give a proof; if not, give a counterexample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.4">Interquartile range and estimation</head><p>The interquartile range of a random variable y equals the difference between its 75% and 25% quantiles. Based on IID data (y i ) n i=1 , write a function to estimate the interquartile range and the corresponding standard error using the bootstrap. Use simulated data to evaluate the finite sample properties of the point estimate (e.g., bias and variance) and the 95% confidence interval (e.g. coverage rate and length).</p><p>Find the asymptotic distribution of the estimator for the interquartile range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.5">Joint asymptotic distribution of the sample median and the mean</head><p>Assume that y 1 , . . . , y n ∼ y are IID. Find the joint asymptotic distribution of the sample mean ȳ and median m. Hint: The mean µ and median m satisfy the estimating equation with w(y, µ, m) = y -µ 0.5 -1(y -m ≤ 0) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26.6">Weighted quantile regression and application</head><p>Many real data contain weights due to sampling. For example, in <ref type="bibr" target="#b49">Angrist et al. (2006)</ref>'s data, perwt is the sampling weight. Define the weighted quantile regression problem theoretically and re-analyze <ref type="bibr" target="#b49">Angrist et al. (2006)</ref>'s data with weights. Note that similar to lm and glm, the quantile regression function rq also has a parameter weights.</p><p>Modeling Time-to-Event Outcomes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.1">Examples</head><p>Time-to-event data are common in biomedical and social sciences. Statistical analysis of time-to-event data is called survival analysis in biostatistics and duration analysis in econometrics. The former name comes from biomedical applications where the outcome denotes the survival time or the time to the recurrence of the disease of interest. The latter name comes from the economic applications where the outcome denotes the weeks unemployed or days until the next arrest after being released from incarceration. See <ref type="bibr" target="#b152">Kalbfleisch and Prentice (2011)</ref> for biomedical applications and <ref type="bibr" target="#b135">Heckman and Singer (1984)</ref> for economic applications. <ref type="bibr" target="#b110">Freedman (2008)</ref> gave a concise and critical introduction to survival analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.1.1">Survival analysis</head><p>The Combined Pharmacotherapies and Behavioral Interventions study evaluated the efficacy of medication, behavioral therapy, and their combination for the treatment of alcohol dependence <ref type="bibr" target="#b52">(Anton et al., 2006)</ref>. Between January 2001 and January 2004, n = 1224 recently alcohol-abstinent volunteers were randomized to receive medical management with 16 weeks of naltrexone (100mg daily) or placebo, with or without a combined behavioral intervention. It was a 2 × 2 factorial experiment. The outcome of interest is the time to the first day of heavy drinking and other endpoints. I adopt the data from <ref type="bibr" target="#b166">Lin et al. (2016)</ref>.</p><p>&gt; COMBINE = read . table ( " combine _ data . txt " , header = TRUE )[ , -1 ] &gt; head ( COMBINE ) AGE GENDER T 0 _ PDA NALTREXONE THERAPY site relapse futime 1 3 1 male 3 . 3 3 3 3 3 3 1 0 site _ 0 0 1 1 2 2 4 1 female 1 6 . 6 6 6 6 6 7 1 1 site _ 0 1 8 3 4 4 male 7 3 . 3 3 3 3 3 3 0 1 site _ 0 1 2 0 4 6 5 male 1 0 . 0 0 0 0 0 0 1 0 site _ 0 0 1 1 2 5 3 9 male 0 . 0 0 0 0 0 0 0 1 site _ 0 1 4 6 5 6 male 1 3 . 3 3 3 3 3 3 0 0 site _ 0 1 1 NALTREXONE and THERAPY are two treatment indicators. futime is the follow-up time, which is censored if relapse equals 0. For those censored observations, futime equals 112, so it is administrative censoring. Figure <ref type="figure" target="#fig_110">27</ref>.1 shows the histograms of futime in four treatment groups. A large number of patients have censored outcomes. Other variables are covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.1.2">Duration analysis</head><p>Carpenter (2002) asked the question: Why does the U.S. Food and Drug Administration approve some drugs more quickly than others? With data about 450 drugs reviewed from 1977 to 2000, he studied the dependence of review times on various covariates, including political influence, wealth of the richest organization representing the disease, media cover- &gt; fda &lt;-read . dta ( " fda . dta " ) &gt; names ( fda )</p><p>[ 1 ] " acttime " " censor " " hcomm " " hfloor " " scomm " [ 6 ] " sfloor " " prespart " " demhsmaj " " demsnmaj " " orderent " [ 1 1 ] " stafcder " " prevgenx " " lethal " " deathrt 1 " " hosp 0 1 " [ 1 6 ] " hospdisc " " hhosleng " " acutediz " " orphdum " " mandiz 0 1 " [ 2 1 ] " femdiz 0 1 " " peddiz 0 1 " " natreg " " natregsq " " wpnoavg 3 " [ 2 6 ] " vandavg 3 " " condavg 3 " " _ st " " _ d " " _ t " [ 3 1 ] " _ t 0 " " caseid "</p><p>An obvious feature of time-to-event data is that the outcome is non-negative. This can be easily dealt with by the log transformation. However, the outcomes may be censored, resulting in inadequate tail information. With right censoring, modeling the mean involves extrapolation in the right tail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.2">Time-to-event data</head><p>Let T ≥ 0 denote the outcome of interest. We can characterize a non-negative continuous T using its density f (t), distribution function F (t), survival function S(t) = 1 -F (t) = pr(T &gt; t), and hazard function</p><formula xml:id="formula_827">λ(t) = lim ∆t↓0 pr(t ≤ T &lt; t + ∆t | T ≥ t)/∆t.</formula><p>Within a small time interval [t, t + ∆t], we have approximation pr(t ≤ T &lt; t + ∆t | T ≥ t) ∼ = λ(t)∆t, so the hazard function denotes the death rate within a small interval conditioning on surviving up to time t. Both the survival and hazard functions are commonly used to describe a positive random variable. First, the survival function has a simple relationship with the expectation.</p><p>Proposition 27.1 For a non-negative random variable T ,</p><formula xml:id="formula_828">E(T ) = ∞ 0 S(t)dt.</formula><p>Proposition 27.1 holds for both continuous and discrete non-negative random variables. It states that the expectation of a nonnegative random variable equals the area under the survival function. It does not require the existence of the density function of T . Proof of Proposition 27.1: Fubini's theorem allows us to swap the expectation and integral below:</p><formula xml:id="formula_829">E(T ) = E T 0 dt = E ∞ 0 1(T &gt; t)dt = ∞ 0 E{1(T &gt; t)}dt = ∞ 0 pr(T &gt; t)dt = ∞ 0 S(t)dt.</formula><p>□ Second, the survival and hazard functions can determine each other in the following way.</p><p>Proposition 27.2 For a non-negative continuous random variable T ,</p><formula xml:id="formula_830">λ(t) = f (t) S(t) = - d dt log S(t), S(t) = exp - t 0 λ(s)ds .</formula><p>Proof of Proposition 27.2: By definition,</p><formula xml:id="formula_831">λ(t) = lim ∆t↓0 pr(t ≤ T &lt; t + ∆t) ∆t 1 pr(T ≥ t) = lim ∆t↓0 F (t + ∆t) -F (t) ∆t 1 S(t) = f (t) S<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t) .</head><p>We can further write the above equation as Example 27.2 (Gamma) The Gamma(α, β) random variable T has density f (t) = β α t α-1 e -βt /Γ(α). When α = 1, it reduces to Exponential(β) with a constant hazard function. In general, the survival function and hazard function do not have simple forms, but we can use dgamma and pgamma to compute them numerically. The left panel of Figure <ref type="figure" target="#fig_110">27</ref>.2 plots the hazard functions of Gamma(α, β). When α &lt; 1, the hazard function is decreasing; when α &gt; 1, the hazard function is increasing. q q q q q q FIGURE 27.3: Discrete survival function with masses (0.1, 0.05, 0.15, 0.2, 0.3, 0.2) at <ref type="bibr">(1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6)</ref> where Z ∼ Exponential(1). We can verify that T has the density function</p><formula xml:id="formula_832">λ(t) = f (t) S(t) = - dS(t)/dt S(t) = - d dt log S(t),</formula><formula xml:id="formula_833">f (t) = a b t b a-1 exp - t b a , survival<label>function</label></formula><formula xml:id="formula_834">S(t) = exp - t b a ,</formula><p>and hazard function</p><formula xml:id="formula_835">λ(t) = a b t b a-1</formula><p>.</p><p>So when a = 1, Weibull reduces to Exponential with constant hazard function. When a &gt; 1, the hazard function increases; when a &lt; 1, the hazard function decreases.</p><p>We can characterize a positive discrete random variable T ∈ {t 1 , t 2 , . . .} by its probability mass function f</p><formula xml:id="formula_836">(t k ) = pr(T = t k ), distribution function F (t) = k:t k ≤t f (t k ), survival function S(t) = k:t k &gt;t f (t k ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and discrete hazard function</head><formula xml:id="formula_837">λ k = pr(T = t k | T ≥ t k ) = f (t k ) S(t k -) ,</formula><p>where S(t k -) denotes the left limit of the function S(t) at t k . Figure <ref type="figure" target="#fig_110">27</ref>.3 shows an example of a survival function for a discrete random variable, which shows that S(t) is a step function and right-continuous with left limits. The discrete hazard and survival functions have the following connection which will be useful for the next section. (1 -λ k ).</p><p>Note that S(t) is a step function decreasing at each t k because λ k is probability and thus bounded between zero and one. Proof of Proposition 27.3: By definition,</p><formula xml:id="formula_838">1 -λ k = 1 -pr(T = t k | T ≥ t k ) = pr(T &gt; t k | T ≥ t k )</formula><p>is the probability of surviving longer than t k conditional on surviving at least as long as t k . We can verify Proposition 27.3 within each interval of the t k 's. For example, if t &lt; t 1 , then</p><formula xml:id="formula_839">S(t) = pr(T &gt; t) = 1; if t 1 ≤ t &lt; t 2 , then S(t) = pr(T &gt; t 1 ) = pr(T &gt; t 1 , T ≥ t 1 ) = pr(T &gt; t 1 | T ≥ t 1 )pr(T ≥ t 1 ) = 1 -λ 1 ; if t 2 ≤ t &lt; t 3 , then S(t) = pr(T &gt; t 2 ) = pr(T &gt; t 2 , T ≥ t 2 ) = pr(T &gt; t 2 | T ≥ t 2 )pr(T ≥ t 2 ) = (1 -λ 2 )(1 -λ 1 ).</formula><p>We can also verify other values of S(t) by induction. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.3">Kaplan-Meier survival curve</head><p>Without censoring, estimating the CDF or the survival function is rather straightforward. With IID data (T 1 , . . . , T n ), we can estimate the CDF by F (t) = n -1 n i=1 1(T i ≤ t) and the survival function by Ŝ(t) = 1 -F (t).</p><p>Figure <ref type="figure" target="#fig_110">27</ref>.4 shows the common data structure with censoring in survival analysis:</p><p>(S1) t 1 , . . . , t K are the death times, and d 1 , . . . , d K are the corresponding number of deaths;</p><p>(S2) r 1 , . . . , r K are the number of patients at risk, that is, r 1 patients are not dead or censored right before time t 1 , and so on;</p><p>(S3) c 1 , . . . , c K are the number of censored patients within interval [t 1 , t 2 ), . . . , [d K , ∞). <ref type="bibr" target="#b153">Kaplan and Meier (1958)</ref> proposed the following simple estimator for the survival function.</p><p>Definition 27.1 (Kaplan-Meier curve) First estimate the discrete hazard function at the failure times {t 1 , . . . , t K } as λk = d k /r k (k = 1, . . . , K) and then estimate the survival function as</p><formula xml:id="formula_840">Ŝ(t) = k:t k ≤t (1 -λk ).</formula><p>The Ŝ(t) in Definition 27.1 is also called the product-limit estimator of the survival function due to its mathematical form.</p><p>At each failure time t k , we view d k as the result of r k Bernoulli trials with probability λ k . So λk = d k /r k has variance λ k (1 -λ k )/r k which can be estimated by v ar( λk ) = λk (1 -λk )/r k .</p><p>We can estimate the variance of the survival function using the delta method. We can approximate the variance of log Ŝ</p><formula xml:id="formula_841">(t) = k:t k ≤t log(1 -λk ) ∼ = k:t k ≤t log(1 -λ k ) - k:t k ≤t (1 -λ k ) -1 ( λk -λ k ) by v ar log Ŝ(t) = k:t k ≤t (1 -λ k ) -2 v ar( λk ) = k:t k ≤t (1 -λk ) -2 λk (1 -λk )/r k = k:t k ≤t d k r k (r k -d k ) ,</formula><p>which is called Greenwood's formula <ref type="bibr" target="#b130">(Greenwood, 1926)</ref>. A hidden assumption above is the independence of the λk 's. This assumption cannot be justified due to the dependence of the events. However, a deeper theory of counting processes shows that Greenwood's formula is valid even without the independence <ref type="bibr" target="#b104">(Fleming and Harrington, 2011)</ref>.</p><p>Based on Greenwood's formula, we can construct a confidence interval for log S(t):</p><formula xml:id="formula_842">log Ŝ(t) ± z α v ar log Ŝ(t) ,</formula><p>which implies a confidence interval for S(t). However, this interval can be outside of range [0, 1] because the log transformation log S(t) is in the range of (-∞, 0) but the Normal approximation is in the range (-∞, ∞). A better transformation is log-log:</p><formula xml:id="formula_843">v(t) = log {-log S(t)} , v(t) = log -log Ŝ(t) .</formula><p>Using Taylor expansion, we can approximate the variance of</p><formula xml:id="formula_844">v(t) ∼ = log {-log S(t)} - 1 log S(t) log Ŝ(t) -log S(t) by v ar log Ŝ(t) log Ŝ(t) 2 .</formula><p>Based on this formula and Greenwood's formula above, we can construct a confidence interval for v(t):</p><p>log -log Ŝ(t) ± z α v ar log Ŝ(t) / log Ŝ(t), which implies another confidence interval for S(t). In the R package survival, the function survfit can fit the Kaplan-Meier curve, where the specifications conf.type = "log" and conf.type = "log-log" return confidence intervals based on the log and log-log transformations, respectively. Figure <ref type="figure" target="#fig_110">27</ref>.5 plots four curves based on the combination of NALTREXONE and THERAPY using the data of <ref type="bibr" target="#b166">Lin et al. (2016)</ref>. I do not show the confidence intervals due to the large overlap. The above discussion on the Kaplan-Meier curve is rather heuristic. More fundamentally, what is the underlying censoring mechanism that ensures the possibility that the distribution of the survival time can be recovered by the observed data? It turns out that we have implicitly assumed that the survival time and the censoring time are independent. Homework problem 27.1 gives a theoretical statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.4">Cox model for time-to-event outcome</head><p>Another important problem is to model the dependence of the survival time T on covariates x. The major challenge is that the survival time is often censored. Let C i be the censoring time of unit i, and we can only observe the minimum value of the survival time and the censoring time. So the observed data are (x i , y i , δ i ) n i=1 , where</p><formula xml:id="formula_845">y i = min(T i , C i ), δ i = 1(T i ≤ C i )</formula><p>are the event time and the censoring indicator, respectively. A key assumption is that the censoring mechanism is noninformative:</p><formula xml:id="formula_846">Assumption 27.1 (noninformative censoring) T i C i | x i .</formula><p>We can start with parametric models.</p><p>Example 27.5 Assume T</p><formula xml:id="formula_847">i | x i ∼ Log-Normal(x t i β, σ 2 ). Equivalently, log T i = x t i β + ε i ,</formula><p>where the ε i 's are IID N(0, σ 2 ) independent of the x i 's. This is a Normal linear model on log T i .</p><p>Example 27.6 Assume that T</p><formula xml:id="formula_848">i | x i ∼ Weibull(a, b = e x t i β ).</formula><p>Based on the definition of the Weibull distribution in Example 27.1, we have</p><formula xml:id="formula_849">log T i = x t i β + ε i</formula><p>where the ε i 's are IID a -1 log Exponential(1), independent of the x i 's.</p><p>The R package survival contains the function survreg to fit parametric survival models including the choices of dist = "lognormal", dist = "weibull", etc. However, these parametric models are not commonly used in practice. The parametric forms can be too strong, and due to right censoring, the inference can be driven by extrapolation to the right tail. <ref type="bibr" target="#b82">Cox (1972)</ref> proposed to model the conditional hazard function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.4.1">Cox model and its interpretation</head><formula xml:id="formula_850">λ(t | x) = lim ∆t↓0 pr(t ≤ T &lt; t + ∆t | T ≥ t, x)/∆t = f (t | x) S(t | x) .</formula><p>His celebrated proportional hazards model has the following form.</p><p>Assumption 27.2 (Cox proportional hazards model) Assume the conditional hazard ratio function has the form</p><formula xml:id="formula_851">λ(t | x) = λ 0 (t) exp(x t β),<label>(27.2)</label></formula><p>where β is an unknown parameter and λ 0 (•) is an unknown function.</p><p>Exponential(1) Gamma(2,2) lnorm(0,1) 0 2 4 6 0 2 4 6 0 2 4 6 0.00 0.25 0.50 0.75 1.00 t survival functions power 0 0.5 2 Under proportional hazards assumption FIGURE 27.6: Proportional hazards assumption with different baseline survival functions, where the power equals γ = exp(β).</p><p>where the product is over K time points with failures, x k is the covariate value of the failure at time t k , and R(t k ) contains the indices of the units at risk at time t k , i.e., the units not censored or failed right before the time t k . <ref type="bibr" target="#b110">Freedman (2008)</ref> gives a heuristic explanation of the partial likelihood based on the following results which extends Proposition B.7 on the Exponential distribution.</p><p>Theorem 27.1 If T 1 , . . . , T n are independent with hazard function λ i (t) (i = 1, . . . , n), then their minimum value T = min 1≤i≤n T i has hazard function</p><formula xml:id="formula_852">n i=1 λ i (t). Moreover, if λ i (t) = c i λ(t), then pr(T i = T ) = c i n i ′ =1 c i ′ . Proof of Theorem 27.1: The survival function of T is pr(T &gt; t) = pr(T 1 &gt; t, . . . , T n &gt; t) = n i=1 S i (t),</formula><p>so Proposition 27.2 implies that its hazard function is</p><formula xml:id="formula_853">- d dt log pr(T &gt; t) = n i=1 - d dt log S i (t) = n i=1 λ i (t).</formula><p>So the first conclusion follows.</p><p>As a byproduct of the above proof, the density of T is</p><formula xml:id="formula_854">n i=1 λ i (t) n i=1 S i (t) based in Proposition 27.2. It must have integral one; with λ i (t) = c i λ(t), this implies n i=1 c i ∞ 0 λ(t) n i=1 S i (t)dt = 1. (27.3) Therefore, we have pr(T i = T ) = pr{T i ≤ T i ′ for all i ′ ̸ = i} = ∞ 0 i ′ ̸ =i S i ′ (t)f i (t)dt = ∞ 0 n i ′ =1 S i ′ (t)λ i (t)dt = c i ∞ 0 λ(t) n i ′ =1 S i ′ (t)dt = c i n i=1 c i ′ ,</formula><p>where the last equality holds due to (27.3). □ Theorem 27.1 explains each of the K components in the partial likelihood. At time t k , the units in R(t k ) are all at risk, and unit k fails, assuming no ties. The probability that unit k has the smallest failure time among units in R</p><formula xml:id="formula_855">(t k ) is exp(x t k β) l∈R(t k ) exp(x t l β)</formula><p>from Theorem 27.1. The product in the partial likelihood is based on the independence of the events at the K failure times, which is more difficult to justify. A rigorous justification relies on the deeper theory of counting processes <ref type="bibr" target="#b104">(Fleming and Harrington, 2011)</ref> or semiparametric statistics <ref type="bibr" target="#b226">(Tsiatis, 2007)</ref>. The log-likelihood function is</p><formula xml:id="formula_856">log L(β) = K k=1    x t k β -log l∈R(t k ) exp(x t l β)    ,</formula><p>and the score function is</p><formula xml:id="formula_857">∂ log L(β) ∂β = K k=1 x k - l∈R(t k ) exp(x t l β)x l l∈R(t k ) exp(x t l β) . Define π β (l | R k ) = exp(x t l β)/ l∈R(t k ) exp(x t l β), (l ∈ R(t k ))</formula><p>which sum to one, so they induce a probability measure leading to expectation</p><formula xml:id="formula_858">E β (• | R k ) and covariance cov β (• | R k ).</formula><p>With this notation, the score function simplifies to</p><formula xml:id="formula_859">∂ log L(β) ∂β = K k=1 {x k -E β (x | R k )} , where E β (x | R k ) = l∈R(t k ) π l (β | R k )</formula><p>x l ; the Hessian matrix simplifies to</p><formula xml:id="formula_860">∂ 2 log L(β) ∂β∂β t = - K k=1 cov β (x | R k ) ⪯ 0, where cov β (x | R k ) = l∈R(t k ) exp(x t l β)x l x t l l∈R(t k ) exp(x t l β) -l∈R(t k ) exp(x t l β)x l l∈R(t k ) exp(x t l β)x t l    l∈R(t k ) exp(x t l β)    2 = l∈R(t k ) π β (l | R k )x l x t l - l∈R(t k ) π β (l | R k )x l l∈R(t k ) π β (l | R k )x t l .</formula><p>The coxph function in the R package survival uses Newton's method to compute the maximizer β of the partial likelihood function, and uses the inverse of the observed Fisher information to approximate its asymptotic variance. <ref type="bibr" target="#b167">Lin and Wei (1989)</ref> proposed a sandwich covariance estimator to allow for the misspecification of the Cox model. The coxph function with robust = TRUE reports the corresponding robust standard errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.4.3">Examples</head><p>Using <ref type="bibr" target="#b166">Lin et al. (2016)</ref>'s data, we have the following results. -0 . 2 4 9 7 1 9 0 . 7 7 9 0 2 0 0 . 0 9 7 6 9 0 -2 . 5 5 6 0 . 0 1 0 5 8 * THERAPY -0 . 1 6 7 0 5 0 0 . 8 4 6 1 5 8 0 . 0 9 6 1 0 2 -1 . 7 3 8 0 . 0 8 2 1 7 . AGE -0 . 0 1 5 5 4 0 0 . 9 8 4 5 8 0 0 . 0 0 3 5 5 9 -4 . 3 6 6 1 . 2 7e -0 5 *** GENDERmale -0 . 1 4 0 6 2 1 0 . 8 6 8 8 1 8 0 . 0 7 5 3 6 8 -1 . 8 6 6 0 . 0 6 2 0 7 . T 0 _ PDA 0 . 0 0 2 5 5 0 1 . 0 0 2 5 5 3 0 . 0 0 1 3 6 8 1 . 8 6 3 0 . 0 6 2 4 2 . sitesite _ 1 -0 . 0 9 1 8 5 3 0 . 9 1 2 2 3 9 0 . 1 6 7 2 6 1 -0 . 5 4 9 0 . 5 8 2 9 0 sitesite _ 1 0 -0 . 2 2 7 1 8 5 0 . 7 9 6 7 7 4 0 . 1 7 5 4 2 7 -1 . 2 9 5 0 . 1 9 5 3 1 sitesite _ 2 0 . 1 2 1 2 3 6 1 . 1 2 8 8 9 2 0 . 1 6 0 0 5 2 0 . 7 5 7 0 . 4 4 8 7 6 sitesite _ 3 -0 . 0 8 4 4 8 3 0 . 9 1 8 9 8 7 0 . 1 6 1 1 2 1 -0 . 5 2 4 0 . 6 0 0 0 4 sitesite _ 4 -0 . 4 7 1 6 1 2 0 . 6 2 3 9 9 6 0 . 1 7 5 2 0 3 -2 . 6 9 2 0 . 0 0 7 1 1 ** sitesite _ 5 -0 . 1 2 8 2 8 6 0 . 8 7 9 6 0 2 0 . 1 6 1 7 8 2 -0 . 7 9 3 0 . 4 2 7 8 0 sitesite _ 6 -0 . 2 4 0 5 6 3 0 . 7 8 6 1 8 5 0 . 1 6 1 9 5 8 -1 . 4 8 5 0 . 1 3 7 4 5 sitesite _ 7 0 . 3 7 2 0 0 4 1 . 4 5 0 6 3 9 0 . 1 5 7 6 1 6 2 . 3 6 0 0 . 0 1 8 2 7 * sitesite _ 8 0 . 0 6 7 7 0 0 1 . 0 7 0 0 4 5 0 . 1 6 0 8 7 6 0 . 4 2 1 0 . 6 7 3 8 8 sitesite _ 9 0 . 2 6 7 3 7 3 1 . 3 0 6 5 2 8 0 . 1 5 4 9 1 1 1 . 7 2 6 0 . 0 8 4 3 5 . NALTREXONE : THERAPY 0 . 3 3 7 5 3 9 1 . 4 0 1 4 9 5 0 . 1 3 7 4 4 1 2 . 4 5 6 0 . 0 1 4 0 5 * NALTREXONE has a significant negative log hazard ratio, but THERAPY has a nonsignificant negative log hazard ratio. More interestingly, their interaction NALTREXONE:THERAPY has a significant positive log hazard ratio. This suggests that combining NALTREXONE and THERAPY is worse than using NALTREXONE alone to delay the first time of heavy drinking and other endpoints. This is also coherent with the survival curves in Figure <ref type="figure" target="#fig_110">27</ref>.5, in which the best Kaplan-Meier curve corresponds to NALTREXONE=1, THERAPY=0.</p><p>Using Keele (2010)'s data, we have the following results:</p><p>&gt; cox . fit &lt;-coxph ( Surv ( acttime , censor ) + hcomm + hfloor + scomm + sfloor + + prespart + demhsmaj + demsnmaj + + prevgenx + lethal + + deathrt 1 + acutediz + hosp 0 1 + + hospdisc + hhosleng + + mandiz 0 1 + femdiz 0 1 + peddiz 0 1 + orphdum + + natreg + I ( natreg ^2 ) + vandavg 3 + wpnoavg 3 + + condavg 3 + orderent + stafcder , + data = fda ) &gt; summary ( cox . fit ) Call : coxph ( formula = Surv ( acttime , censor ) ~hcomm + hfloor + scomm + sfloor + prespart + demhsmaj + demsnmaj + prevgenx + lethal + deathrt 1 + acutediz + hosp 0 1 + hospdisc + hhosleng + mandiz 0 1 + femdiz 0 1 + peddiz 0 1 + orphdum + natreg + I ( natreg ^2 ) + vandavg 3 + wpnoavg 3 + condavg 3 + orderent + stafcder , data = fda ) n = 4 0 8 , number of events = 2 6 2 coef exp ( coef ) se ( coef ) z Pr ( &gt;| z |) hcomm</p><p>3 . 6 4 2e -0 1 1 . 4 3 9 e + 0 0 2 . 9 5 1 e + 0 0 0 . 1 2 3 0 . 9 0 1 7 7 5 hfloor 7 . 9 4 4 e + 0 0 2 . 8 1 9 e + 0 3 8 . 1 7 3 e + 0 0 0 . 9 7 2 0 . 3 3 1 0 7 1 scomm 4 . 7 1 6e -0 1 1 . 6 0 3 e + 0 0 1 . 8 9 8 e + 0 0 0 . 2 4 8 0 . 8 0 3 7 7 1 sfloor 2 . 6 0 4 e + 0 0 1 . 3 5 2 e + 0 1 2 . 3 7 0 e + 0 0 1 . 0 9 9 0 . 2 7 1 8 7 7 prespart 8 . 0 3 8e -0 1 2 . 2 3 4 e + 0 0 3 . 0 4 2e -0 1 2 . 6 4 3 0 . 0 0 8 2 2 6 ** demhsmaj 1 . 3 6 3 e + 0 0 3 . 9 0 9 e + 0 0 1 . 9 1 7 e + 0 0 0 . 7 1 1 0 . 4 7 6 8 9 0 demsnmaj 1 . 2 1 7 e + 0 0 3 . 3 7 7 e + 0 0 5 . 6 0 6e -0 1 2 . 1 7 1 0 . 0 2 9 9 4 0 * prevgenx -9 . 9 1 5e -0 4 9 . 9 9 0e -0 1 7 . 7 7 9e -0 4 -1 . 2 7 5 0 . 2 0 2 4 5 9 lethal 7 . 8 7 2e -0 2 1 . 0 8 2 e + 0 0 2 . 3 7 8e -0 1 0 . 3 3 1 0 . 7 4 0 6 0 5 deathrt 1 6 . 5 3 7e -0 1 1 . 9 2 3 e + 0 0 2 . 4 3 5e -0 1 2 . 6 8 5 0 . 0 0 7 2 5 3 ** acutediz 1 . 9 9 4e -0 1 1 . 2 2 1 e + 0 0 2 . 2 6 2e -0 1 0 . 8 8 2 0 . 3 7 7 8 9 6 hosp 0 1 4 . 2 8 0e -0 2 1 . 0 4 4 e + 0 0 2 . 4 9 5e -0 1 0 . 1 7 2 0 . 8 6 3 7 6 8 hospdisc -1 . 2 3 8e -0 6 1 . 0 0 0 e + 0 0 5 . 2 7 8e -0 7 -2 . 3 4 5 0 . 0 1 9 0 0 2 * hhosleng -1 . 2 7 3e -0 2 9 . 8 7 4e -0 1 1 . 9 8 8e -0 2 -0 . 6 4 0 0 . 5 2 1 8 9 1 mandiz 0 1 -1 . 1 7 7e -0 1 8 . 8 8 9e -0 1 3 . 8 0 0e -0 1 -0 . 3 1 0 0 . 7 5 6 7 1 1 femdiz 0 1 9 . 0 3 2e -0 1 2 . 4 6 8 e + 0 0 3 . 4 9 7e -0 1 2 . 5 8 3 0 . 0 0 9 7 9 9 ** peddiz 0 1 -3 . 4 0 1e -0 2 9 . 6 6 6e -0 1 5 . 1 1 2e -0 1 -0 . 0 6 7 0 . 9 4 6 9 6 8 orphdum 5 . 5 4 0e -0 1 1 . 7 4 0 e + 0 0 2 . 1 0 9e -0 1 2 . 6 2 6 0 . 0 0 8 6 3 0 ** natreg -2 . 2 2 1e -0 2 9 . 7 8 0e -0 1 8 . 2 8 2e -0 3 -2 . 6 8 2 0 . 0 0 7 3 1 8 ** I ( natreg ^2 ) 1 . 0 2 9e -0 4 1 . 0 0 0 e + 0 0 4 . 5 6 7e -0 5 2 . 2 5 3 0 . 0 2 4 2 7 6 * vandavg 3 -2 . 0 1 4e -0 2 9 . 8 0 1e -0 1 1 . 5 3 6e -0 2 -1 . 3 1 1 0 . 1 8 9 8 0 2 wpnoavg 3 5 . 2 2 0e -0 3 1 . 0 0 5 e + 0 0 1 . 4 2 6e -0 3 3 . 6 6 0 0 . 0 0 0 2 5 2 *** condavg 3 9 . 6 2 8e -0 3 1 . 0 1 0 e + 0 0 2 . 2 7 1e -0 2 0 . 4 2 4 0 . 6 7 1 6 3 7 orderent -1 . 8 1 0e -0 2 9 . 8 2 1e -0 1 8 . 1 4 7e -0 3 -2 . 2 2 2 0 . 0 2 6 2 9 6 * stafcder 8 . 0 1 3e -0 4 1 . 0 0 1 e + 0 0 7 . 9 8 6e -0 4 1 . 0 0 3 0 . 3 1 5 7 1 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.4.4">Log-rank test as a score test from Cox model</head><p>A standard problem in clinical trials is to compare the survival times under treatment and control. Assume no ties in the failure times, and let x denote the binary indicator for treatment. Under the proportional hazards assumption, the control group has hazard λ 0 (t), and the treatment group has hazard λ 1 (t) = λ 0 (t)e β . We are interested in testing the null hypothesis</p><formula xml:id="formula_861">β = 0 ⇐⇒ λ 1 (t) = λ 0 (t) ⇐⇒ S 1 (t) = S 0 (t).</formula><p>Under the null hypothesis, the score function reduces to</p><formula xml:id="formula_862">∂ log L(β) ∂β β=0 = K k=1 {x k -E β=0 (x | R k )} = K k=1 x k - r k1 r k , because E β=0 (x | R k ) = l∈R(t k ) x l l∈R(t k ) 1 = r k1 r k</formula><p>equaling the ratio of the number of treated units at risk r k1 over the number of units at risk r k , at time t k . The Fisher information at the null is</p><formula xml:id="formula_863">- ∂ 2 log L(0) ∂β∂β t = K k=1 cov β=0 (x | R k ) = K k=1 r k1 r k 1 - r k1 r k .</formula><p>The score test for classical parametric models relies on</p><formula xml:id="formula_864">∂ log L(0) ∂β a ∼ N 0, - ∂ 2 log L(0) ∂β∂β t ,</formula><p>which follows from Bartlett's identity and the CLT. Applying this fact to Cox's model, we have</p><formula xml:id="formula_865">LR = K k=1 x k -r k1 r k K k=1 r k1 r k 1 -r k1 r k a ∼ N(0, 1).</formula><p>So we reject the null at level α if |LR| is larger than the upper 1 -α/2 quantile of standard Normal. This is almost identical to the log-rank test without ties. Allowing for ties, <ref type="bibr" target="#b174">Mantel (1966)</ref> proposed a more general form of the log-rank test<ref type="foot" target="#foot_26">foot_26</ref> . The survdiff function in the survival package implements various tests including the logrank test as a special case. Below, I use the gehan dataset in the MASS package to illustrate the log rank test. The data were from a matched-pair experiment of 42 leukaemia patients <ref type="bibr" target="#b122">(Gehan, 1965)</ref>. Treated units received the drug 6-mercaptopurine, and the rest are controls. For illustration purposes, I ignore the pair indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&gt; library ( MASS ) &gt; head ( gehan ) pair time cens treat</head><formula xml:id="formula_866">1 1 1 1 control 2 1 1 0 1 6 -MP 3 2 2 2 1 control 4 2 7 1 6 -MP 5 3 3 1 control 6 3 3 2 0 6 -MP &gt; survdiff ( Surv ( time , cens ) ~treat , + data = gehan ) Call : survdiff ( formula = Surv ( time , cens ) ~treat , data = gehan ) N Observed Expected (O -E )^2 / E (O -E )^2 / V treat = 6 -MP 2 1 9 1 9 . 3 5 . 4 6 1 6 . 8 treat = control 2 1</formula><p>2 1 1 0 . 7 9 . 7 7 1 6 . 8</p><p>Chisq = 1 6 . 8 on 1 degrees of freedom , p = 4e -0 5</p><p>The treatment was quite effective, yielding an extremely small p-value even with moderate sample size. It is also clear from the Kaplan-Meier curves in Figure <ref type="figure" target="#fig_110">27</ref>.7 and the results from fitting the Cox proportional hazards model. 4 . 8 1 6 9 0 . 4 1 2 4 3 . 8 1 2 0 . 0 0 0 1 3 8 *** exp ( coef ) exp ( -coef ) lower . 9 5 upper . 9 5 treatcontrol 4 . 8 1 7 0 . 2 0 7 6 2 . 1 4 7 1 0 . 8 1 Concordance = 0 . 6 9 ( se = 0 . 0 4 1 ) Likelihood ratio test = 1 6 . 3 5 on 1 df , p = 5e -0 5 Wald test = 1 4 . 5 3 on 1 df , p = 1e -0 4 Score ( logrank ) test = 1 7 . 2 5 on 1 df , p = 3e -0 5</p><p>The log-rank test is a standard tool in survival analysis. However, what it delivers is just a special case of the Cox proportional hazards model. The p-value from the log-rank test is close to the p-value from the score test of the Cox proportional hazards model with only a binary treatment indicator. The latter can also adjust for other pretreatment covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.5">Extensions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.5.1">Stratified Cox model</head><p>Many randomized trials are stratified. The Combined Pharmacotherapies and Behavioral Interventions study reviewed at the beginning of this chapter is an example with site indicating the strata. The previous analysis includes the dummy variables of site in the Cox model. An alternative more flexible model is to allow for different baseline hazard functions across strata. Technically, assume</p><formula xml:id="formula_867">λ s (t | x) = λ s (t) exp(β t x)</formula><p>for strata s = 1, . . . , S, where β is an unknown parameter and {λ 1 (•), . . . , λ S (•)} are unknown functions. Therefore, within each stratum s, the proportional hazards assumption holds; across strata, the proportional hazard assumption may not hold. Within stratum s, we can obtain the partial likelihood L s (β); by independence of the data across strata, we can obtain the joint partial likelihood Based on the standard procedure, we can obtain the MLE and conduct inference based on the large-sample theory. The coxph function can naturally allow for stratification with the + strata() in the regression formula. -0 . 2 5 2 2 3 9 0 . 7 7 7 0 5 9 0 . 0 9 7 7 8 8 0 . 0 9 6 4 3 7 -2 . 6 1 6 0 . 0 0 8 9 1 THERAPY -0 . 1 7 3 4 5 6 0 . 8 4 0 7 5 4 0 . 0 9 6 2 5 8 0 . 0 9 5 9 5 8 -1 . 8 0 8 0 . 0 7 0 6 6 AGE -0 . 0 1 5 1 0 4 0 . 9 8 5 0 1 0 0 . 0 0 3 5 5 4 0 . 0 0 3 5 1 2 -4 . 3 0 1 1 . 7e -0 5 GENDERmale -0 . 1 3 9 8 3 7 0 . 8 6 9 5 0 0 0 . 0 7 5 3 8 8 0 . 0 7 6 5 8 0 -1 . 8 2 6 0 . 0 6 7 8 5 T 0 _ PDA 0 . 0 0 2 7 4 7 1 . 0 0 2 7 5 1 0 . 0 0 1 3 6 9 0 . 0 0 1 3 5 0 2 . 0 3 5 0 . 0 4 1 8 2 NALTREXONE : THERAPY 0 . 3 3 5 6 7 1 1 . 3 9 8 8 7 9 0 . 1 3 7 6 7 6 0 . 1 3 6 8 9 0 2 . 4 5 2 0 . 0 1 4 2 0 NALTREXONE ** THERAPY . AGE *** GENDERmale . T 0 _ PDA * NALTREXONE : THERAPY * exp ( coef ) exp ( -coef ) lower . 9 5 upper . 9 5 NALTREXONE 0 . 7 7 7 1 1 . 2 8 6 9 0 . 6 4 3 2 0 . 9 3 8 7 THERAPY 0 . 8 4 0 8 1 . 1 8 9 4 0 . 6 9 6 6 1 . 0 1 4 7 AGE 0 . 9 8 5 0 1 . 0 1 5 2 0 . 9 7 8 3 0 . 9 9 1 8 GENDERmale 0 . 8 6 9 5 1 . 1 5 0 1 0 . 7 4 8 3 1 . 0 1 0 3 T 0 _ PDA 1 . 0 0 2 8 0 . 9 9 7 3 1 . 0 0 0 1 1 . 0 0 5 4 NALTREXONE : THERAPY 1 . 3 9 8 9 0 . 7 1 4 9 1 . 0 6 9 7 1 . 8 2 9 4 treat *** adult agedx exp ( coef ) exp ( -coef ) lower . 9 5 upper . 9 5 treat 0 . 4 5 7 7 2 . 1 8 4 7 0 . 3 4 2 3 0 . 6 1 2 2 adult 0 . 8 7 2 0 1 . 1 4 6 8 0 . 4 8 8 9 1 . 5 5 5 3 agedx 1 . 0 0 7 9 0 . 9 9 2 2 0 . 9 8 7 8 1 . 0 2 8 4 Concordance = 0 . 5 9 6 ( se = 0 . 0 2 3 ) Likelihood ratio test = 2 3 . 1 3 on 3 df , p = 4e -0 5 Wald test = 2 8 . 5 5 on 3 df , p = 3e -0 6 Score ( logrank ) test = 2 3 . 0 1 on 3 df , p = 4e -0 5 , Robust = 2 6 . 5 5 p = 7e -0 6 ( Note : the likelihood ratio and score tests assume independence of observations within a cluster , the Wald and robust score tests do not ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.5.3">Penalized Cox model</head><p>The glmnet package implements the penalized version of the Cox model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.6">Critiques on survival analysis</head><p>The Kaplan-Meier curve and the Cox proportional hazards model are standard tools for analyzing medical data with censored survival times. They are among the most commonly used methods in medical journals. <ref type="bibr" target="#b153">Kaplan and Meier (1958)</ref> and <ref type="bibr" target="#b82">Cox (1972)</ref> are two of the most cited papers in statistics. <ref type="bibr" target="#b110">Freedman (2008)</ref> criticized these two standard tools. Both rely on the critical assumption of noninformative censoring that censoring and survival time are independent or conditionally independent given covariates. When censoring is due to administrative constraints, this may be a plausible assumption. The data from <ref type="bibr" target="#b166">Lin et al. (2016)</ref> is a convincing example of noninformative censoring. However, many other studies have more complex censoring mechanisms, for example, one may drop out of the study, and another may be killed by an irrelevant cause. The Cox model relies on an additional assumption of proportional hazards. This particular functional form facilitates the interpretation of the coefficients as log conditional hazard ratios if the model is correctly specified. However, its interpretation becomes obscure when the model is mis-specified. Two survival curves based on <ref type="bibr" target="#b166">Lin et al. (2016)</ref> 's data cross each other, which makes the proportional hazards assumption dubious. <ref type="bibr" target="#b136">Hernán (2010)</ref> offered a more fundamental critique on hazard-based survival analysis. For example, in a randomized treatment-control experiment, the hazard ratio at time t is the ratio of the instantaneous probability of death conditioning on the event that the patients have survived up to time t: lim ∆t↓0 pr(t ≤ T &lt; t + ∆t | x = 1, T ≥ t)/∆t lim ∆t↓0 pr(t ≤ T &lt; t + ∆t | x = 0, T ≥ t)/∆t This ratio is difficult to interpret because patients who have survived up to time t can be quite different in treatment and control groups, especially when the treatment is effective. Even though patients are randomly assigned at the baseline, the survivors up to time t are not. <ref type="bibr" target="#b136">Hernán (2010)</ref> suggested focusing on the comparison of the survival functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Linearly independent vectors and rank</head><p>We call a set of vectors A 1 , . . . , A m ∈ R n linearly independent if</p><formula xml:id="formula_868">x 1 A 1 + • • • + x m A m = 0 must imply x 1 = • • • = x m =</formula><p>0. We call A j1 , . . . , A j k maximally linearly independent if adding another vector makes them linearly dependent. Define k as the rank of of {A 1 , . . . , A m } and also define k as the rank of the matrix A = (A 1 , . . . , A m ).</p><p>A set of vectors may have different subsets of vectors that are maximally linearly independent. But the rank k is unique. We can also define the rank of a matrix in terms of its row vectors. A remarkable theorem in linear algebra is that it does not matter whether we define the rank of a matrix in terms of its column vectors or row vectors.</p><p>From the matrix product formulas (A.1) and (A.2), we have the following result.</p><p>Proposition A.1 rank(AB) ≤ min{rank(A), rank(B)}.</p><p>The rank decomposition of a matrix decomposes A into the product of two matrices of full ranks. □ Proposition A.1 ensures that rank(B) ≥ k and rank(C) ≥ k so they must both have rank k. The decomposition in Proposition A.2 is not unique since the choice of the maximally linearly independent column vectors of A is not unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some special matrices</head><p>An n×n matrix A is symmetric if A t = A. An n×n diagonal matrix A has zero off-diagonal elements, denoted by A = diag{a 11 , . . . , a nn }. Diagonal matrices are symmetric.</p><p>An n × n matrix is orthogonal if A t A = AA t = I n . The column vectors of an orthogonal matrix are orthonormal; so are its row vectors. If A is orthogonal, then ∥Ax∥ = ∥x∥ for any vector x ∈ R n . That is, multiplying a vector by an orthogonal matrix does not change the length of the vector. Geometrically, an orthogonal matrix corresponds to rotation.</p><p>An n × n matrix A is upper triangular if a ij = 0 for i &gt; j and lower triangular if a ij = 0 for i &lt; j. An n × n matrix A can be factorized as</p><formula xml:id="formula_869">A = LU</formula><p>where L is a lower triangular matrix and U is an upper triangular matrix. This is called the LU decomposition of a matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Determinant</head><p>The original definition of the determinant of a square matrix A = (a ij ) has a very complex form, which will not be used in this book.</p><p>The determinant of a 2 × 2 matrix has a simple form: if both A and B are invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eigenvalues and eigenvectors</head><p>For an n × n matrix A, if there exists a pair of n-dimensional vector x and a scalar λ such that Ax = λx, then we call λ an eigenvalue and x the associated eigenvector of A. From the definition, eigenvalue and eigenvector always come in pairs. The following eigen-decomposition theorem is important for real symmetric matrices.</p><p>Theorem A.1 If A is an n × n symmetric matrix, then there exists an orthogonal matrix P such that P t AP = diag{λ 1 , . . . , λ n },</p><p>where the λ's are the n eigenvalues of A, and the column vectors of P = (γ 1 , • • • , γ n ) are the corresponding eigenvectors.</p><p>If we write the eigendecomposition as AP = P diag{λ 1 , . . . , λ n } or, equivalently,</p><formula xml:id="formula_870">A(γ 1 , • • • , γ n ) = (λ 1 γ 1 , • • • , λ n γ n ),</formula><p>then (λ i , γ i ) must be a pair of eigenvalue and eigenvector. Moreover, the eigendecomposition in Theorem A.1 is unique up to the permutation of the columns of P and the corresponding λ i 's.</p><p>Corollary A.1 If P t AP = diag{λ 1 , . . . , λ n }, then</p><formula xml:id="formula_871">A = P diag{λ 1 , . . . , λ n }P t , A k = A • A • • • A = P diag{λ k 1 , . . . , λ k n }P t ;</formula><p>if the eigenvalues of A are nonzero, then</p><p>A -1 = P diag{1/λ 1 , . . . , 1/λ n }P t .</p><p>The eigen-decomposition is also useful for defining the square root of an n×n symmetric matrix. In particular, if the eigenvalues of A are nonnegative, then we can define A 1/2 = P diag{ λ 1 , . . . , λ n }P t By definition, A 1/2 is a symmetric matrix satisfying A 1/2 A 1/2 = A. There are other definitions of the square root of a symmetric matrix, but we adopt this form in this book.</p><p>From Theorem A.1, we can write A as</p><formula xml:id="formula_872">A = P diag{λ 1 , . . . , λ n }P t = (γ 1 , • • • , γ n )diag{λ 1 , . . . , λ n }    γ t 1 . . . γ t n    = n i=1 λ i γ i γ t i .</formula><p>For an n × n symmetric matrix A, its rank equals the number of non-zero eigenvalues and its determinant equals the product of all eigenvalues. The matrix A is of full rank if all its eigenvalues are non-zero, which implies that its rank equals n and its determinant is non-zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quadratic form</head><p>For an n × n symmetric matrix A = (a ij ) and an n-dimensional vector x, we can define the quadratic form as</p><formula xml:id="formula_873">x t Ax = ⟨x, Ax⟩ = n i=1 n j=1 a ij x i x j .</formula><p>We always consider a symmetric matrix in the quadratic form without loss of generality. Otherwise, we can symmetrize A as Ã = (A + A t )/2 without changing the value of the quadratic form because</p><p>x t Ax = x t Ãx.</p><p>We call A positive semi-definite, denoted by A ⪰ 0, if x t Ax ≥ 0 for all x; we call A positive definite, denoted by A ≻ 0, if x t Ax &gt; 0 for all nonzero x.</p><p>We can also define the partial order between matrices. We call A ⪰ B if and only if A -B ⪰ 0, and we call A ≻ B if and only if A -B ≻ 0. This is important in statistics because we often compare the efficiency of estimators based on their variances or covariance matrices. Given two unbiased estimators θ1 and θ2 for a scalar parameter θ, we say that θ1 is more efficient than θ2 if var( θ2 ) ≥ var( θ1 ). In the vector case, we say that θ1 is more efficient than θ2 if cov( θ2 ) ⪰ cov( θ1 ), which is equivalent to var(ℓ t θ2 ) ≥ var(ℓ t θ1 ) for any linear combination of the estimators.</p><p>The eigenvalues of a symmetric matrix determine whether it is positive semi-definite or positive definite.</p><p>Theorem A.2 For a symmetric matrix A, it is positive semi-definite if and only if all its eigenvalues are nonnegative, and it is positive definite if and only if all its eigenvalues are positive.</p><p>An important result is the relationship between the eigenvalues and the extreme values of the quadratic form. Assume that the eigenvalues are rearranged in decreasing order such that λ 1 ≥ • • • ≥ λ n . For a unit vector x, we have that has length ∥α∥ 2 = ∥x∥ 2 = 1. Then the maximum value of x t Ax is λ 1 which is achieved at α 1 = 1 and α 2 = • • • = α n = 0 (for example, if x = γ 1 , then α 1 = 1 and α 2 = • • • = α n = 0). For a unit vector x that is orthogonal to γ 1 , we have that</p><formula xml:id="formula_874">x t Ax = n i=2 λ i α 2 i</formula><p>where α = P t x has unit length with α 1 = 0. The maximum value of x t Ax is λ 2 which is achieved at α 2 = 1 and α 1 = α 3 = • • • = α n = 0, for example, x = γ 2 . By induction, we have the following theorem.</p><p>Theorem A.3 Suppose that an n × n symmetric matrix has eigen-decomposition n i=1 λ i γ i γ t i where λ 1 ≥ • • • ≥ λ n .</p><p>1. The optimization problem max x</p><p>x t Ax such that ∥x∥ = 1 has maximum λ 1 which can be achieved by γ 1 .</p><p>2. The optimization problem max x</p><p>x t Ax such that ∥x∥ = 1, x ⊥ γ 1 has maximum λ 2 which can be achieved by γ 2 .</p><p>3. The optimization problem max x</p><p>x t Ax such that ∥x∥ = 1, x ⊥ γ 1 , . . . , x ⊥ γ k has maximum λ k+1 which can be achieved by γ k+1 (k = 1, . . . , n -1).</p><p>Theorem A.3 implies the following theorem on the Rayleigh quotient r(x) = x t Ax/x t x (x ∈ R n ). with the maximizer and minimizer being the eigenvectors corresponding to the maximum and minimum eigenvalues, respectively.</p><p>An immediate consequence of Theorem A.4 is that the diagonal elements of A are bounded by the smallest and largest eigenvalues of A. This follows by taking x = (0, . . . , 1, . . . , 0) t where only the ith element equals 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trace</head><p>The trace of an n × n matrix A = (a ij ) is the sum of all its diagonal elements, denoted by trace(A) = n i=1 a ii .</p><p>The trace operator has two important properties that can sometimes help to simplify calculations.</p><p>Proposition A.5 trace(AB) = trace(BA) as long as AB and BA are both square matrices.</p><p>We can verify Proposition A.5 by definition. It states that AB and BA have the same trace although AB differs from BA in general. In fact, it is particularly useful if the dimension of BA is much lower than the dimension of AB. For example, if both A = (a 1 , . . . , a n ) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Projection matrix</head><p>An n × n matrix H is a projection matrix if it is symmetric and H 2 = H. The eigenvalues of H must be either 1 or 0. To see this, we assume that Hx = λx for some nonzero vector x, and use two ways to calculate H 2 x: H 2 x = Hx = λx, H 2 x = H(Hx) = H(λx) = λHx = λ 2 x.</p><p>So (λ -λ 2 )x = 0 which implies that λ -λ 2 = 0, i.e., λ = 0 or 1. So the trace of H equals its rank: trace(H) = rank(H).</p><p>Why is this a reasonable definition of a "projection matrix"? Or, why must a projection matrix satisfy H t = H and H 2 = H? First, it is reasonable to require that Hx 1 = x 1 for any x 1 ∈ C(H), the column space of H. Since x 1 = Hα for some α, we indeed have Hx 1 = H(Hα) = H 2 α = Hα = x 1 because of the property H 2 = H. Second, it is reasonable to require that x 1 ⊥ x 2 for any vector x 1 = Hα ∈ C(H) and x 2 such that Hx 2 = 0. So we need α t H t x 2 = 0 which is true if H = H t . Therefore, the two conditions are natural for the definition of a projection matrix.</p><p>More interestingly, a project matrix has a more explicit form as stated below.</p><p>Theorem A.5 If an n × p matrix X has p linearly independent columns, then H = X(X t X) -1 X t is a projection matrix. Conversely, if an n × n matrix H is a projection matrix with rank p, then H = X(X t X) -1 X t for some n × p matrix X with linearly independent columns.</p><p>It is relatively easy to verify the first part of Theorem A.5; see Chapter 3. The second part of Theorem A.5 follows from the eigen-decomposition of H, with the first p eigen-vectors being the column vectors of X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cholesky decomposition</head><p>An n × n positive semi-definite matrix A can be decomposed as A = LL t where L is an n × n lower triangular matrix with non-negative diagonal elements. If A is positive definite, the decomposition is unique. In general, it is not. Take an arbitrary orthogonal matrix Q, we have A = LQQ t L t = CC t where C = LQ. So we can decompose a positive semi-definite matrix A as A = CC t , but this decomposition is not unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Singular value decomposition (SVD)</head><p>Any n × m matrix A can be decomposed as</p><formula xml:id="formula_875">A = U DV t</formula><p>where U is n × n orthogonal matrix, V is m × m orthogonal matrix, and D is n × m matrix with all zeros for the non-diagonal elements. For a tall matrix with n ≥ m, the diagonal matrix D has many zeros, so we can also write . . .</p><formula xml:id="formula_876">V t r    = r k=1 d k U k V t k .</formula><p>The SVD implies that</p><formula xml:id="formula_877">AA t = U DD t U t , A t A = V D t DV t ,</formula><p>which are the eigen decompositions of AA t and A t A. This ensures that AA t and A t A have the same non-zero eigenvalues. An application of the SVD is to define the pseudoinverse of any matrix. Define D + as the pseudoinverse of D with the non-zero elements inverted but the zero elements intact at zero. Define</p><formula xml:id="formula_878">A + = V D + U t = r k=1 d -1 k V k U t k</formula><p>as the pseudoinverse of A. The definition holds even if A is not a square matrix. We can verify that AA + A = A, A + AA + = A + .</p><p>If A is a square nondegenerate matrix, then A + = A -1 equals the standard definition of the inverse. In the special case with a symmetric A, its SVD is identical to its eigen decomposition. If A is not invertible, its pseudoinverse equals A + = P diag(λ -1 1 , . . . , λ -1 k , 0, . . . , 0)P t if rank(A) = k &lt; n and λ 1 , λ 1 , . . . , λ k are the nonzero eigen-values.</p><p>Another application of the SVD is the polar decomposition for any square matrix A. Since A = U DV t = U DU t U V t with orthogonal U and V , we have A = (AA t ) 1/2 Γ, where (AA t ) 1/2 = U DU t and Γ = U V t is an orthogonal matrix. The above formulas become more powerful in conjunction with the chain rule. For example, for any differentiable function h(z) mapping from R to R with derivative h ′ (z), we have ∂h(a t x) ∂x = h ′ (a t x)a, ∂h(x t Ax) ∂x = 2h ′ (x t Ax)Ax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Vector calculus</head><p>For any differentiable function h(z) mapping from R q to R with gradient ∂h(z)/∂z, we have ∂h(B t x) ∂x = ∂h(B t 1 x, . . . , B t q x) ∂x = q j=1 ∂h(B t 1 x, . . . , B t q x) ∂z j B j = B ∂h(B t x) ∂z .</p><p>Moreover, we can also define the Hessian matrix of a function f (x) mapping from R p to R:</p><formula xml:id="formula_879">∂ 2 f (x) ∂x∂x t = ∂ 2 f (x) ∂x i ∂x j 1≤i,j≤p = ∂ ∂x t ∂f (x) ∂x .</formula><p>A.3 Homework problems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Triangle inequality of the inner product</head><p>With three unit vectors u, v, w ∈ R n , prove that 1 -⟨u, w⟩ ≤ 1 -⟨u, v⟩ + 1 -⟨v, w⟩.</p><p>Remark: The result is a direct consequence of the standard triangle inequality but it has an interesting implication. If ⟨u, v⟩ ≥ 1 -ϵ and ⟨v, w⟩ ≥ 1 -ϵ, then ⟨u, w⟩ ≥ 1 -4ϵ. This implied inequality is mostly interesting when ϵ is small. It states that when u and v are highly corrected and v and w are highly correlated, then u and w must also be highly correlated. Note that we can find counterexamples for the following relationship: ⟨u, v⟩ &gt; 0, ⟨v, w⟩ &gt; 0 but ⟨u, w⟩ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Van der Corput inequality</head><p>Assume that v, u 1 , . . . , u m ∈ R n have unit length. Prove that Remark: This result is not too difficult to prove but it says something fundamentally interesting. If v is correlated with many vectors u 1 , . . . , u m , then at least some vectors in u 1 , . . . , u m must be also correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Inverse of a block matrix</head><p>Prove that</p><formula xml:id="formula_880">A B C D -1 = A -1 + A -1 B(D -CA -1 B) -1 CA -1 -A -1 B(D -CA -1 B) -1 -(D -CA -1 B) -1 CA -1 (D -CA -1 B) -1 = (A -BD -1 C) -1 -(A -BD -1 C) -1 BD -1 -D -1 C(A -BD -1 C) -1 D -1 + D -1 C(A -BD -1 C) -1 BD -1 ,</formula><p>provided that all the inverses of the matrices exist. The two forms of the inverse imply the Woodbury formula:</p><formula xml:id="formula_881">(A -BD -1 C) -1 = A -1 + A -1 B(D -CA -1 B) -1 CA -1 ,</formula><p>which further implies the Sherman-Morrison formula:</p><formula xml:id="formula_882">(A + uv t ) -1 = A -1 -(1 + v t A -1 u) -1 A -1 uv t A -1 ,</formula><p>where A is an invertible square matrix, and u and v are two column vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Matrix determinant lemma</head><p>Prove that given the identity matrix I n and two n-vectors u and v, we have det(I n + uv t ) = 1 + v t u.</p><p>Further show that if I n is replaced by an n × n invertible matrix A, we have det(A + uv t ) = (1 + v t A -1 u) • det(A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Decomposition of a positive semi-definite matrix</head><p>Show that if A is positive semi-definite, then there exists a matrix C such that A = CC t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Trace of the product of two matrices</head><p>Prove that A and B are two n × n positive semi-definite matrices, then trace(AB) ≥ 0.</p><p>Hint: Use the eigen-decomposition of A = n i=1 λ i γ i γ t i . Remark: In fact, a stronger result holds. If two n × n symmetric matrices A and B have eigenvalues</p><formula xml:id="formula_883">λ 1 ≥ • • • ≥ λ n , µ 1 ≥ • • • ≥ µ n respectively, then n i=1 λ i µ n+1-i ≤ trace(AB) ≤ n i=1 λ i µ i .</formula><p>The result is due to Von Neumann <ref type="bibr">(1937)</ref> and <ref type="bibr" target="#b204">Ruhe (1970)</ref>. See also <ref type="bibr">Chen and Li (2019, Lemma 4.12)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7">Vector calculus</head><p>What is the formula for ∂x t Ax/∂x if A is not symmetric in Proposition A.7?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Variables</head><p>Let "IID" denote "independent and identically distributed", "</p><p>iid ∼" denote a sequence of random variables that are IID with some common distribution, and " " denote independence between random variables. Define Euler's Gamma function as</p><formula xml:id="formula_884">Γ(z) = ∞ 0 x z-1 e -x dx, (z &gt; 0),</formula><p>which is a natural extension of the factorial since Γ(n) = (n -1)!. Further define the digamma function as ψ(z) = d log Γ(z)/dz and the trigamma function as ψ ′ (z). In R, we can use gamma ( z ) lgamma ( z ) digamma ( z ) trigamma ( z )</p><p>to compute Γ(z), log Γ(z), ψ(z), and ψ ′ (z).</p><p>B.1 Some important univariate random variables B.1.1 Normal, χ 2 , t and F</p><p>The standard Normal random variable Z ∼ N(0, 1) has density f (z) = (2π) -1/2 exp -z 2 /2 .</p><p>A Normal random variable X has mean µ and variance σ 2 , denoted by N(µ, σ 2 ), if X = µ + σZ. We can show that X has density f (x) = (2π) 1/2 exp -(x -µ) 2 /(2σ 2 ) .</p><p>A chi-squared random variable with degrees of freedom n, denoted by Q n ∼ χ 2 n , can be represented as</p><formula xml:id="formula_885">Q n = n i=1 Z 2 i ,</formula><p>where Z i iid ∼ N(0, 1). We can show that its density is f n (q) = q n/2 exp(-q/2) 2 n/2 Γ(n/2) , (q &gt; 0). (B.1)</p><p>We can verify that the above density (B.1) is well-defined even if we change the integer n to be an arbitrary positive real number ν, and call the corresponding random variable Q ν a chi-squared random variable with degrees of freedom ν, denoted by Q ν ∼ χ 2 ν .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Model and Extensions</head><p>A t random variable with degrees of freedom ν can be represented as</p><formula xml:id="formula_886">t ν = Z Q ν /ν</formula><p>where Z ∼ N(0, 1), Q ν ∼ χ 2 ν , and Z Q ν . An F random variable with degrees of freedom (r, s) can be represented as</p><formula xml:id="formula_887">F = Q r /r Q s /s</formula><p>where Q r ∼ χ 2 r , Q s ∼ χ 2 s , and Q r Q s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Beta-Gamma duality</head><p>The Gamma(α, β) random variable with parameters α, β &gt; 0 has density</p><formula xml:id="formula_888">f (x) = β α Γ(α)</formula><p>x α-1 e -βx , (x &gt; 0). (B.</p><p>2)</p><p>The Beta(α, β) random variable with parameters α, β &gt; 0 has density</p><formula xml:id="formula_889">f (x) = Γ(α + β) Γ(α)Γ(β)</formula><p>x α-1 (1 -x) β-1 , (0 &lt; x &lt; 1).</p><p>These two random variables are closely related as shown in the following theorem. Another simple but useful fact is that χ 2 is a special Gamma random variable. Comparing the densities in (B.1) and (B.2), we obtain the following result.</p><p>Proposition B.1 χ 2 n ∼ Gamma(n/2, 1/2). We can also calculate the moments of the Gamma and Beta distributions. .</p><p>Second, we have pr(X i = X) = pr(X i &lt; X j for all j ̸ = i) Therefore, the density of y at c ≥ 0 is λ 2 e λc , and the density of y at c ≤ 0 is λ 2 e -λc , which can be unified as λ 2 e λ|c| . □ If X 0 is the standard exponential random variable, then we define the Gumbel(µ, β) random variable as Y = µ -β log X 0 .</p><p>The standard Gumbel distribution has µ = 0 and β = 1, with CDF F (y) = exp(-e -y ), y ∈ R and density f (y) = exp(-e -y )e -y , y ∈ R.</p><p>By definition and Proposition B.7, we can verify that the maximum of IID Gumbels is also Gumbel. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Multivariate distributions</head><p>A random vector (X 1 , . . . , X n ) t is a vector consisting of n random variables. If all components are continuous, we can define its joint density f X1•••Xn (x 1 , . . . , x n ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Independence</head><p>Random variables (X 1 , . . . , X n ) are mutually independent if</p><formula xml:id="formula_890">f X1•••Xn (x 1 , . . . , x n ) = f X1 (x 1 ) • • • f Xn (x n ).</formula><p>Note that in this definition, each of (X 1 , . . . , X n ) can be vectors. We have the following rules under independence. For a scalar random variable, cov(Y ) = var(Y ).</p><p>Proposition B.12 For A ∈ R r×n , Y ∈ R n and C ∈ R r , we have cov(AY + C) = Acov(Y )A t .</p><p>Using Proposition B.12, we can verify that for any n-dimensional random vector, cov(Y ) ⪰ 0 because for all x ∈ R n , we have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Multivariate Normal and its properties</head><p>I use a generative definition of the multivariate Normal random vector. First, Z is a standard Normal random vector if Z = (Z 1 , . . . , Z n ) t has components Z i iid ∼ N(0, 1). Given a mean vector µ and a positive semi-definite covariance matrix Σ, define a Normal random vector Y ∼ N(µ, Σ) with mean µ and covariance Σ if Y can be represented as</p><formula xml:id="formula_891">Y = µ + AZ, (B.3)</formula><p>where A satisfies Σ = AA t . We can verify that cov(Y ) = Σ, so indeed Σ is its covariance matrix. If Σ ≻ 0, then we can verify that Y has density f Y (y) = (2π) -n/2 {det(Σ)} -1/2 exp -(y -µ) t Σ -1 (y -µ)/2 . (B.4)</p><p>We can easily verify the following result by calculating the density.</p><p>Proposition B.14 If Z ∼ N(0, I n ) and Γ is an orthogonal matrix, then ΓZ ∼ N(0, I n ).</p><p>I do not define multivariate Normal based on the density (B.4) because it is only well defined with a positive definite Σ. I do not define multivariate Normal based on the characteristic function because it is more advanced than the level of this book. Definition (B.3) does not require Σ to be positive definite and is more elementary. However, it has a subtle issue of uniqueness. Although the decomposition Σ = AA t is not unique, the resulting distribution Y = µ + AZ is. We can verify this using the Polar decomposition. Because A = Σ 1/2 Γ where Γ is an orthogonal matrix, we have Y = µ + Σ 1/2 ΓZ = µ + Σ 1/2 Z where Z = ΓZ is a standard Normal random vector by Proposition B.14. Importantly, although the definition (B.3) can be general, we usually use the following representation</p><formula xml:id="formula_892">Y = µ + Σ 1/2 Z. Theorem B.5 Assume that Y 1 Y 2 ∼ N µ 1 µ 2 , Σ 11 Σ 12 Σ 21 Σ 22 .</formula><p>Then Y 1 Y 2 if and only if Σ 12 = 0. □ An obvious corollary of Proposition B.15 is that if X 1 ∼ N(µ 1 , σ 2 1 ) and X 2 ∼ N(µ 2 , σ 2 2 ) are independent, then X 1 +X 2 ∼ N(µ 1 +µ 2 , σ 2 1 +σ 2 2 ). So the summation of two independent Normals is also Normal. Remarkably, the reverse of the result is also true.</p><p>Theorem B.6 (Levy-Cramer) If X 1 X 2 and X 1 + X 2 is Normal, then both X 1 and X 2 must be Normal.</p><p>The statement of Theorem B.6 is extremely simple. But its proof is non-trivial and beyond the scope of this book. See <ref type="bibr" target="#b56">Benhamou et al. (2018)</ref> for a proof.</p><formula xml:id="formula_893">Theorem B.7 Assume Y 1 Y 2 ∼ N µ 1 µ 2 , Σ 11 Σ 12 Σ 21 Σ 22 .</formula><p>1. The marginal distributions are Normal:</p><formula xml:id="formula_894">Y 1 ∼ N (µ 1 , Σ 11 ) , Y 2 ∼ N (µ 2 , Σ 22 ) .</formula><p>2. If Σ 22 ≻ 0, then the conditional distribution is Normal:</p><formula xml:id="formula_895">Y 1 | Y 2 = y 2 ∼ N µ 1 + Σ 12 Σ -1 22 (y 2 -µ 2 ), Σ 11 -Σ 12 Σ -1 22 Σ 21 ;</formula><p>Y 2 is independent of the residual</p><formula xml:id="formula_896">Y 1 -Σ 12 Σ -1 22 (Y 2 -µ 2 ) ∼ N µ 1 , Σ 11 -Σ 12 Σ -1 22 Σ 21 .</formula><p>I review some other results of the multivariate Normal below.</p><p>Proposition B.16 Assume Y ∼ N(µ, σ 2 I n ). If AB t = 0, then AY BY .</p><p>Proposition B.17 Assume</p><formula xml:id="formula_897">Y 1 Y 2 ∼ N µ 1 µ 2 , σ 2 1 ρσ 1 σ 2 ρσ 1 σ 2 σ 2 2 ,</formula><p>where ρ is the correlation coefficient defined as</p><formula xml:id="formula_898">ρ = cov(Y 1 , Y 2 )</formula><p>var(Y 1 )var(Y 2 ) .</p><p>Then the conditional distribution is</p><formula xml:id="formula_899">Y 1 | Y 2 = y 2 ∼ N µ 1 + ρ σ 1 σ 2 (y 2 -µ 2 ), σ 2 1 (1 -ρ 2 ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Quadratic forms of random vectors</head><p>Given a random vector Y and a symmetric matrix A, we can define the quadratic form Y t AY , which is a random variable playing an important role in statistics. The first theorem is about its mean. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>The variance of the quadratic form is much more complicated for a general random vector. For the multivariate Normal random vector, we have the following formula. 2. If Y ∼ N(0, I n ) and H is a projection matrix of rank K, then</p><formula xml:id="formula_900">Y t HY ∼ χ 2 K .</formula><p>3. If Y ∼ N(0, H) where H is a projection matrix of rank K, then</p><formula xml:id="formula_901">Y t Y ∼ χ 2 K .</formula><p>Proof of Theorem B.10:</p><p>1. I only prove the general result with rank(Σ) = k ≤ n. By definition, Y = µ + Σ 1/2 Z where Z is a standard Normal random vector, then</p><formula xml:id="formula_902">(Y -µ) t Σ + (Y -µ) = Z t Σ 1/2 Σ + Σ 1/2 Z = k i=1 Z 2 i ∼ χ 2 k .</formula><p>2. Using the eigendecomposition of the projection matrix H = P diag {1, . . . , 1, 0, . . . , 0} P t with K 1's in the diagonal matrix, we have Y t HY = Y t P diag {1, . . . , 1, 0, . . . , 0} P t Y = Z t diag {1, . . . , 1, 0, . . . , 0} Z, where Z = (Z 1 , . . . , Z n ) t = P t Y ∼ N(0, P t P ) = N(0, I n ) is a standard Normal random vector. So</p><formula xml:id="formula_903">Y t HY = K i=1 Z 2 i ∼ χ 2 K .</formula><p>3. Writing Y = H 1/2 Z where Z is a standard Normal random vector, we have</p><formula xml:id="formula_904">Y t Y = Z t H 1/2 H 1/2 Z = Z t HZ ∼ χ 2 K</formula><p>using the second result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>B.5 Homework problems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Beta-Gamma duality</head><p>Prove Theorem B.1. Hint: Calculate the joint density of (X + Y, X/(X + Y )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gamma and Beta moments</head><p>Prove Propositions B.2-B.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Maximums of Gumbels</head><p>Prove Proposition B.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Independence and uncorrelatedness in the multivariate Normal</head><p>Prove Theorem B.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Transformation of bivariate Normal</head><p>Prove that if (Y 1 , Y 2 ) t follows a bivariate Normal distribution</p><formula xml:id="formula_905">Y 1 Y 2 ∼ N 0 0 , 1 ρ ρ 1 , then Y 1 + Y 2 Y 1 -Y 2 .</formula><p>Remark: This result holds for arbitrary ρ. Condition (LF2) often holds by proper standardization, and the key is to verify Condition (LF1). Condition (LF1) is general but it looks cumbersome. In many cases, we impose a stronger moment condition that is easier to verify: (LF1') kn i=1 E∥Z ni ∥ 2+δ → 0 for some δ &gt; 0. We can show that (LF1') implies that (LF1):</p><formula xml:id="formula_906">kn i=1 E ∥Z ni ∥ 2 1 {∥Z ni ∥ &gt; c} = kn i=1 E ∥Z ni ∥ 2+δ ∥Z ni ∥ -δ 1 ∥Z ni ∥ δ &gt; c δ ≤ kn i=1 E∥Z ni ∥ 2+δ c -δ → 0.</formula><p>Condition (LF1') is called the Lyapunov condition. A beautiful application of the Lindeberg-Feller CLT is the proof of Huber (1973)'s theorem on OLS mentioned in Chapter 11. I first review the theorem and then give a proof.</p><p>Theorem C.1 Assume Y = Xβ + ε where the covariates are fixed and error terms ε = (ε 1 , . . . , ε n ) t are IID non-Normal with mean zero and finite variance σ 2 . Recall the OLS estimator β = (X t X) -1 X t Y . Any linear combination of β is asymptotically Normal if and only if max</p><formula xml:id="formula_907">1≤i≤n h ii → 0,</formula><p>where h ii is the ith diagonal element of the hat matrix H = X(X t X) -1 X t .</p><p>In the main text, h ii is called the leverage score of unit i. The maximum leverage score κ = max 1≤i≤n h ii plays an important role in analyzing the properties of OLS. Theorem C.1 assumes that the errors are not Normal because the asymptotic Normality under Normal errors is a trivial result (See Chapter 5). It is slightly different from the asymptotic analysis in Chapter 6. Theorem C.1 only concerns any linear combination of the OLS estimator alone, but the results in Chapter 6 allow for the joint inference of several linear combinations of the OLS estimator. An implicit assumption of Chapter 6 is that the dimension p of the covariate matrix is fixed, but Theorem C.1 allows for a diverging p. The leverage score condition implicitly restricts the dimension and moments of the covariates. Another interesting feature of Theorem C.1 is that the statement is coordinate-free, that is, it holds up to a non-singular transformation of the covariates (See also Problems 3.4 and 3.5). The proof of sufficiency follows <ref type="bibr" target="#b147">Huber (1973)</ref> closely, and the proof of necessity was suggested by Professor Peter Bickel. Proof of Theorem C.1: I first simplify the notation without essential loss of generality. By the invariance of the OLS estimator and the hat matrix in Problems 3.4 and 3.5, we can also assume X t X = I p . So β -β = (X t X) -1 X t ε = X t ε and the hat matrix H = X(X t X) -1 X t = XX t has diagonal elements h ii = x t i x i = ∥x i ∥ 2 and non-diagonal elements h ij = x t i x j . We can also assume σ 2 = 1.</p><p>Consider a fixed vector a ∈ R p and assume ∥a∥ 2 = 1. We have a t β -a t β = a t X t ε ≡ s t ε, where s = Xa ⇐⇒ s i = x t i a (i = 1, . . . , n) satisfies ∥s∥ 2 = a t X t Xa = ∥a∥ 2 = 1 and s 2 i = (x t i a) 2 ≤ ∥x i ∥ 2 ∥a∥ 2 = ∥x i ∥ 2 = h ii by the Cauchy-Schwarz inequality.</p><p>I first prove the sufficiency. The key term a t β -a t β is a linear combination of the IID errors, and it has mean 0 and variance var(s t ε) = ∥s∥ 2 = 1. We only need to verify Condition (LF1) to establish the CLT. It holds because for any fixed c &gt; 0, we have </p><formula xml:id="formula_908">n i=1 E s 2 i ε 2 i 1 {|s i ε i | &gt; c} ≤ n i=1 s 2 i max 1≤i≤n E ε 2 i 1 {|s i ε i | &gt; c} (C.4) = max 1≤i≤n E ε 2 i 1 {|s i ε i | &gt; c} (C.5) ≤ E ε 2 i 1 κ 1/2 |ε i | &gt; c (C.</formula><formula xml:id="formula_909">* = x t i * β = x t i * X t ε = n j=1 x t i * x j ε j = h i * i * ε i * + j̸ =i * h i * j ε j .</formula><p>If ŷi * is asymptotically Normal, then both h i * i * ε i * and j̸ =i * h i * j ε j must have Normal limiting distributions by Theorem B.6. Therefore, h i * i * must converge to zero because ε i * has a non-Normal distribution. So max 1≤i≤n h ii must converge to zero. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Tools for proving convergence in probability and distribution</head><p>The first tool is the continuous mapping theorem:</p><p>Proposition C.9 Let f : R K → R L be continuous except on a set O with pr(Z ∈ O) = 0. Then Z n → Z implies f (Z n ) → f (Z) in probability (and in distribution).</p><p>The second tool is Slutsky's Theorem:</p><p>Proposition C.10 Let Z n and W n be random vectors. If Z n → Z in distribution, and W n → c in probability (or in distribution) for a constant c, then</p><formula xml:id="formula_910">1. Z n + W n → Z + c in distribution; 2. W n Z n → cZ in distribution; 3. W -1 n Z n → c -1 Z in distribution if c ̸ = 0.</formula><p>The third tool is the delta method. I will present a special case below for asymptotically Normal random vectors. Heuristically, it states that if T n is asymptotically Normal, then any function of T n is also asymptotically Normal. This is true because any function is a locally linear function by the first-order Taylor expansion.</p><p>Proposition C.11 Let f (z) be a function from R p to R q , and ∂f (z)/∂z ∈ R p×q be the partial derivative matrix. If √ n(Z n -θ) → N(µ, Σ) in distribution, then where m(•, •) is a vector function with the same dimension as b, and W = {w i } n i=1 are the observed data. Let β denote the solution which is an estimator of β. Under mild regularity conditions, β is consistent and asymptotically Normal<ref type="foot" target="#foot_27">foot_27</ref> . This is the classical theory of M-estimation. I will review it below. See <ref type="bibr" target="#b211">Stefanski and Boos (2002)</ref> for a reader-friendly introduction that contains many interesting and important examples. The proofs below are not rigorous. See <ref type="bibr" target="#b184">Newey and McFadden (1994)</ref> for the rigorous ones. Proof of Theorem D.1: I give a "physics" proof. When I use approximations, I mean the error terms are of higher orders under some regularity conditions. The consistency follows from swapping the order of "solving equation" and "taking the limit based on the law of large numbers": For non-IID data, the above covariance estimator can be conservative unless E{m(w i , β)} = 0 for all i = 1, . . . , n.</p><formula xml:id="formula_911">√ n{f (Z n ) -f (θ)} → N ∂f (θ) ∂z t µ, ∂f<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 M-estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Maximum likelihood estimator</head><p>As an important application of Theorem D.1, we can derive the asymptotic properties of the maximum likelihood estimator (MLE) θ under IID sampling from a parametric model  <ref type="bibr" target="#b101">Fisher (1925b)</ref> pioneered the asymptotic theory of the MLE under correctly specified models.</p><p>If the model is incorrect, I(θ) can be different from J(θ) but the sandwich covariance B -1 M B -1 still holds. So a covariance matrix estimator for the MLE under misspecification is I n ( θ) -1 J n ( θ)I n ( θ) -1 .</p><p>Huber (1967) studied the asymptotic theory of the MLE under correctly specified models. He focused on the case with IID observations and pioneered the sandwich covariance formula. Perhaps a more important question is what is the parameter if the model is misspecified.</p><p>The population analog of the MLE is the minimizer of</p><formula xml:id="formula_912">-E{log f (y | θ)},</formula><p>where the expectation is over true but unknown distribution y ∼ g(y). We can rewrite the population objective function as -g(y) log f (y | θ)dy = g(y) log g(y) f (y | θ) dy -g(y) log g(y)dy.</p><p>The first term is called the Kullback-Leibler divergence or relative entropy of g(y) and f (y | θ), whereas the second term is called the entropy of g(y). The first term depends on θ whereas the second term does not. Therefore, the targeted parameter of the MLE is the minimizer of the Kullback-Leibler divergence. By Gibbs' inequality, the Kullback-Leibler divergence is non-negative in general and is 0 if g(y) = f (y | θ). Therefore, if the model is correct, then the true θ indeed minimizes the Kullback-Leibler divergence with minimum value 0</p><p>Example D.4 Assume that y 1 , . . . , y n iid ∼ N(µ, 1). The log-likelihood contributed by unit i is log f</p><formula xml:id="formula_913">(y i | µ) = - 1 2 log(2π) - 1 2 (y i -µ) 2 , so ∂ log f (y i | µ) ∂µ = y i -µ, ∂ 2 log f (y i | µ) ∂µ 2 = -1.</formula><p>The MLE is μ = ȳ. If the model is correctly specified, we can use</p><formula xml:id="formula_914">I n (μ) -1 = n -1 or J n (μ) -1 = 1/ n i=1 (y i -μ) 2</formula><p>to estimate the variance of μ. If the model is misspecified, we can use</p><formula xml:id="formula_915">I n (μ) -1 J n (μ)I n (μ) -1 = n i=1 (y i -μ) 2 /n 2</formula><p>to estimate the variance of μ. The sandwich variance estimator seems the best overall. The Normal model can be totally wrong but it is still meaningful to estimate the mean parameter µ = E(y). The MLE is just the sample moment estimator which has variance var(y)/n. Since the sample variance s 2 = n i=1 (y i -μ) 2 /(n -1) is unbiased for var(y), a natural unbiased estimator for var(μ) is s 2 /n, which is close to the sandwich variance estimator.</p><p>The above discussion extends to the case with independent but non-IID data. The covariance estimators still apply by replacing each f by f i within the summation. Note that the sandwich covariance estimator is conservative in general. <ref type="bibr" target="#b238">White (1982)</ref> pioneered the asymptotic analysis of the MLE with misspecified models in econometrics but made a mistake for the M term. <ref type="bibr" target="#b75">Chow (1984)</ref>  </p><formula xml:id="formula_916">l i = - 1 2 log(2πσ 2 ) - 1 2σ 2 (y i -x t i β) 2 , (i = 1, . . . , n)</formula><p>with the simplification l i = log f (y i | x i , β, σ 2 ). So the first-order derivatives are</p><formula xml:id="formula_917">∂l i ∂β = 1 σ 2 x i (y i -x t i β), ∂l i ∂σ 2 = - 1 2σ 2 + 1 2(σ 2 ) 2 (y i -x t i β) 2 ;</formula><p>the second derivative is</p><formula xml:id="formula_918">∂ 2 l i ∂β 2 = - 1 σ 2 x i x t i , ∂ 2 l i ∂(σ 2 ) 2 = 1 2(σ 2 ) 2 - 1 (σ 2 ) 3 (y i -x t i β) 2 and ∂ 2 l i ∂β∂σ 2 = - 1 (σ 2 ) 2 x i (y i -x t i β).</formula><p>The MLE of β is the OLS estimator β and the MLE of σ 2 is σ2 = n i=1 ε2 i /n, where εi = y i -x t i β is the residual. We have</p><formula xml:id="formula_919">I n ( β, σ2 ) = diag 1 σ2 n i=1 x i x t i , n 2(σ 2 ) 2 ,</formula><p>and</p><formula xml:id="formula_920">J n ( β, σ2 ) = 1 (σ 2 ) 2 n i=1 ε2 i x i x t i * * * ,</formula><p>where the * terms do not matter for the later calculations. If the Normal linear model is correctly specified, we can use the (1, 1)th block of I n ( β, σ2 ) -1 as the covariance estimator for β, which equals σ2 n i=1</p><p>x i x t i -1</p><p>.</p><p>If the Normal linear model is misspecified, we can use the (1, 1)th block of I n ( β, σ2 ) -1 J n ( β, σ2 )I n ( β, σ2 ) -1 as the covariance estimator for β, which equals Show that E( Ṽ ) = var(x) when x 1 , . . . , x n are independent with the same mean µ, and E( Ṽ ) ≥ var(x) when x i ∼ [µ i , σ 2 i ] are independent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Version 1: a basic linear model course assuming minimal technical preparations If you want to teach a basic linear model course without assuming strong technical preparations from the students, you can start with the appendices by reviewing basic linear algebra, probability theory, and statistical inference. Then you can cover Chapters 2-17. If time permits, you can consider covering Chapter 20 due to the importance of the logistic model for binary data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Version 2 :</head><label>2</label><figDesc>an advanced linear model course assuming strong technical preparations If you want to teach an advanced linear model course assuming strong technical preparations from the students, you can start with the main text directly. When I did this, I asked xix xx</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>variances, and ρ = σxy /(σ x σxy ) is the sample Pearson correlation coefficient with the sample covariance σxy = (n -1) -1 n i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 2</head><label>2</label><figDesc>FIGURE 2.1: Galton's dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>&gt;</head><figDesc>library ( " HistData " ) &gt; xx = Galton Families $ m id pa re n tH ei gh t &gt; yy = Galton Families $ childHeight &gt; &gt; center _ x = mean ( xx ) &gt; center _ y = mean ( yy ) &gt; sd _ x = sd ( xx ) &gt; sd _ y = sd ( yy ) &gt; rho _ xy = cor ( xx , yy ) &gt; &gt; beta _ fit = rho _ xy * sd _ y / sd _ x &gt; alpha _ fit = center _ y -beta _ fit * center _</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>Assumption 4.1 (Gauss-Markov model) We have Y = Xβ + ε</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Theorem 4. 3</head><label>3</label><figDesc>Defineσ2 = rss/(n -p) = n i=1 ε2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>The above covariance decomposition simplifies tocov( β) = cov( β) + cov( ββ), which implies cov( β) -cov( β) = cov( ββ) ⪰ 0.□ In the process of the proof, we have shown two stronger results cov( ββ, β) = 0 and cov( ββ) = cov( β) -cov( β).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>Assumption 5.1 (Normal linear model) We haveY ∼ N(Xβ, σ 2 I n ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>FIGURE 5.1: Prediction in Galton's regression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2 .</head><label>2</label><figDesc>Proof of Corollary 7.1: We have ε = (I -H)Y and ê = (I -H2 ) Ỹ = (I -H2 )(I -H 1 )Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>FIGURE 8</head><label>8</label><figDesc>FIGURE 8.1: Correlations among three variables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FIGURE 8 . 2 :</head><label>82</label><figDesc>FIGURE 8.2: An Example of Simpson's Paradox. The two solid regression lines are fitted separately using the data from two groups, and the dash regression line is fitted using the pooled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>FIGURE</head><figDesc>FIGURE 9.3: A path model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>R 2 and the sample Pearson correlation coefficient Prove Theorem 10.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><figDesc>FIGURE 11.1: Outlier detections</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>FIGURE 11 . 2 :</head><label>112</label><figDesc>FIGURE 11.2: Outlier detections in the LaLonde data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><figDesc>var(y) is the population Pearson correlation coefficient. So Corollary 12.1 gives the population version of the Galtonian formula. I leave the proofs of Theorem 12.2 and Corollary 12.1 as Problems 12.2 and 12.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><figDesc>FIGURE 12.1: Best linear approximations correspond to three different distributions of x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><figDesc>standard error : 1 . 2 3 7 on 9 degrees of freedom Multiple R -squared : 0 . 6 6 6 2 , Adjusted R -squared : 0 . 6 2 9 2 F -statistic : 1 7 . 9 7 on 1 and 9 DF , p -value : 0 . 0 0 2 1 7 9 &gt; ols 3 = lm ( y 3 ~x 3 , data = anscombe ) &gt; summary ( ols 3 ) Call : lm ( formula = y 3 ~x 3 , data = anscombe ) : 1 7 . 9 7 on 1 and 9 DF , p -value : 0 . 0 0 2 1 7 6 &gt; ols 4 = lm ( y 4 ~x 4 , data = anscombe ) &gt; summary ( ols 4 ) Call : lm ( formula = y 4 ~x 4 , data = anscombe )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><figDesc>FIGURE 12.3: Anscombe's Quartet: scatter plots</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><figDesc>FIGURE 12.4: Residual plots</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><figDesc>library ( " mlbench " ) data ( BostonHousing ) attach ( BostonHousing ) n = dim ( BostonHousing )[ 1 ] p = dim ( BostonHousing )[ 2 ]  -1 ymin = min ( medv ) ymax = max ( medv ) grid . y = seq ( ymin -3 0 , ymax + 3 0 , 0 . 1 ) BostonHousing = BostonHousing [ order ( medv ) , ] detach ( BostonHousing ) ols . fit . full = lm ( medv ~. , data = BostonHousing , x = TRUE , y = TRUE , qr = TRUE ) beta = ols . fit . full $ coef e . sigma = summary ( ols . fit . full )$ sigma X = ols . fit . full $ x Y = ols . fit . full $ y X . QR = ols . fit . full $ qr X . Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><figDesc>FIGURE 12.5: Leave-one-out prediction intervals based on the Boston housing data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>13. 4</head><label>4</label><figDesc>Using press to construct an unbiased estimator for σ 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><figDesc>FIGURE 13.1: Freedman's simulation. The first row shows the histograms of the R 2 s, and the second row shows the histograms of the p-values in testing that all coefficients are 0. The first column corresponds to the full model without testing, and the second column corresponds to the selected model with testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><figDesc>FIGURE 13.3: Training and testing errors: nonlinear mean function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><figDesc>βridge (t) = arg min b0,b1,...,bp rss(b 0 , b 1 , . . . , b p )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><figDesc>} based on the standardized variables, we can obtain the predicted values based on the original variables as ŷi (λ) -ȳ = βridge 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>FIGURE 14 . 2 :</head><label>142</label><figDesc>FIGURE 14.2: Bias-variance trade-off in ridge regression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><figDesc>then the mode of the posterior distribution of β | Y equals βridge (σ 2 /τ 2 ): βridge (σ 2 /τ 2 ) = arg max β f (β | Y ) where f (β | Y ) is the posterior density of β given Y . 14.2 Derivative of the MSE Show that ∂mse(λ) ∂λ λ=0 &lt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>FIGURE 15 . 2 :</head><label>152</label><figDesc>FIGURE 15.2: Lasso with a non-sparse solution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><figDesc>The solution inLemma 15.1 is a function of b 0 and λ, and we will use the notationS(b 0 , λ) = sign(b 0 ) (|b 0 | -λ) +from now on, where S denotes the soft-thresholding operator. For a given λ &gt; 0, it is a function of b 0 illustrated by Figure15.4. The proof of Lemma 15.1 is to solve the optimization problem. It is tricky since we cannot naively solve the first-order condition due to the nonsmoothness of |b| at 0. Nevertheless, it is only a one-dimensional optimization problem, and I relegate the proof as Problem 15.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><figDesc>training and testing data &gt; set . seed ( 2 3 0 ) &gt; nsample = dim ( BostonHousing )[ 1 ] &gt; trainindex = sample ( 1 : nsample , floor ( nsample * 0 . 9 )) &gt; &gt; xmatrix = model . matrix ( medv ~. , data = BostonHousing )[ , -1 ] &gt; yvector = BostonHousing $ medv &gt; dat = data . frame ( yvector , xmatrix ) &gt; &gt; # # linear regression &gt; bostonlm = lm ( yvector ~. , data = dat [ trainindex , ]) &gt; predicterror = dat $ yvector [ -trainindex ] -+ predict ( bostonlm , dat [ -trainindex , ]) &gt; mse . ols = sum ( predicterror ^2 )/ length ( predicterror ) &gt; &gt; # # ridge regression &gt; lambdas = seq ( 0 , 5 , 0 . 0 1 ) &gt; lm 0 = lm . ridge ( yvector ~. , data = dat [ trainindex , ] , + lambda = lambdas ) &gt; coefridge = coef ( lm 0 )[ which . min ( lm 0 $ GCV ) , ] &gt; p r e d i c t e r r o r r i d g e = dat $ yvector [ -trainindex ] -+ cbind ( 1 , xmatrix [ -trainindex , ])%*% coefridge &gt; mse . ridge = sum ( p r e d i c t e r r o r r i d g e ^2 )/ length ( p r e d i c t e r r o r r i d g e ) &gt; &gt; # # lasso &gt; cvboston = cv . glmnet ( x = xmatrix [ trainindex , ] , y = yvector [ trainindex ]) &gt; coeflasso = coef ( cvboston , s = " lambda . min " ) &gt; p r e d i c t e r r o r l a s s o = dat $ yvector [ -trainindex ] -+ cbind ( 1 , xmatrix [ -trainindex , ])%*% coeflasso &gt; mse . lasso = sum ( p r e d i c t e r r o r l a s s o ^2 )/ length ( p r e d i c t e r r o r l a s s o ) &gt; &gt; c ( mse . ols , mse . ridge , mse . lasso )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><figDesc>&gt; # # adding more noisy covariates &gt; n . noise = 2 0 0 &gt; xnoise = matrix ( rnorm ( nsample * n . noise ) , nsample , n . noise ) &gt; xmatrix = cbind ( xmatrix , xnoise ) &gt; dat = data . frame ( yvector , xmatrix ) &gt; &gt; # # linear regression &gt; bostonlm = lm ( yvector ~. , data = dat [ trainindex , ]) &gt; predicterror = dat $ yvector [ -trainindex ] -+ predict ( bostonlm , dat [ -trainindex , ]) &gt; mse . ols = sum ( predicterror ^2 )/ length ( predicterror ) &gt; &gt; # # ridge regression &gt; lambdas = seq ( 1 0 0 , 1 5 0 , 0 . 0 1 ) &gt; lm 0 = lm . ridge ( yvector ~. , data = dat [ trainindex , ] , + lambda = lambdas ) &gt; coefridge = coef ( lm 0 )[ which . min ( lm 0 $ GCV ) , ] &gt; p r e d i c t e r r o r r i d g e = dat $ yvector [ -trainindex ] -+ cbind ( 1 , xmatrix [ -trainindex , ])%*% coefridge &gt; mse . ridge = sum ( p r e d i c t e r r o r r i d g e ^2 )/ length ( p r e d i c t e r r o r r i d g e ) &gt; &gt; q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><figDesc>FIGURE 15.5: Comparing ridge and lasso</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>FIGURE 15 .</head><label>15</label><figDesc>FIGURE 15.6: Shrinkage estimators    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15.6  shows the constraints corresponding to different values of q.Zou and<ref type="bibr" target="#b252">Hastie (2005)</ref> proposed the elastic net which combines the penalties of the lasso and ridge:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>FIGURE 15 . 7 :</head><label>157</label><figDesc>FIGURE 15.7: Comparing the ridge, lasso, and elastic net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>FIGURE 16 . 2 :</head><label>162</label><figDesc>FIGURE 16.1: Box-Cox transformation in the jobs data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><figDesc>FIGURE 16.3: Nonparametric regression using the basis expansion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><figDesc>FIGURE 16.4: Generalized additive model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>&gt;</head><figDesc>ols . fit = lm ( read ~math * socst , data = hsbdemo ) &gt; summary ( ols . fit ) Call : lm ( formula = read ~math * socst , data = hsbdemo ) Residuals :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>&gt;</head><figDesc>hsbdemo $ math . c = hsbdemo $ math -mean ( hsbdemo $ math ) &gt; hsbdemo $ socst . c = hsbdemo $ socst -mean ( hsbdemo $ socst ) &gt; ols . fit = lm ( read ~math . c * socst .c , data = hsbdemo ) &gt; summary ( ols . fit ) Call : lm ( formula = read ~math . c * socst .c , data = hsbdemo )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><figDesc>FIGURE 19.1: Fulton data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>2 :</head><label>2</label><figDesc>FIGURE 19.2: Local linear regression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>FIGURE 19</head><label>19</label><figDesc>FIGURE 19.3: Survey sampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>2 ,</head><label>2</label><figDesc>(x i -xw )(y i -ȳw ) n i=1 w i (x i -xw ) αw = ȳw -βw xw where xw = n i=1 w i x i / n i=1 w i and ȳw = n i=1 w i y i / n i=1 w i are the weighted means of the covariate and outcome.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>FIGURE 20</head><label>20</label><figDesc>FIGURE 20.1: Distributions and densities corresponding to the link functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>&gt;</head><figDesc>flu = read . table ( " fludata . txt " , header = TRUE ) &gt; flu = within ( flu , rm ( receive )) &gt; assign . logit = glm ( outcome ~. , + family = binomial ( link = logit ) , + data = flu ) &gt; summary ( assign . logit ) Call : glm ( formula = outcome ~. , family = binomial ( link = logit ) , data = flu )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>FIGURE 20 . 2 :</head><label>202</label><figDesc>FIGURE 20.2:  Comparing the fitted probabilities from different link functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><figDesc>20.6.1 Penalized logistic regressionSimilar to the high dimensional linear model, we can also extend the logit model to a penalized version. Since the objective function for the original logit model is the log-likelihood, we can minimize the following penalized log-likelihood function:1 -α)|β j |},</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>&gt;</head><figDesc>dat = read . csv ( " samarani . csv " ) &gt; pool . glm = glm ( case _ comb ~ds 1 + ds 2 + ds 3 + ds 4 _ a + + ds 4 _ b + ds 5 + ds 1 _ 3 + center , + family = binomial ( link = logit ) , + data = dat ) &gt; summary ( pool . glm ) Call : glm ( formula = case _ comb ~ds 1 + ds 2 + ds 3 + ds 4 _ a + ds 4 _ b + ds 5 + ds 1 _ 3 + center , family = binomial ( link = logit ) , data = dat )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Theorem 20 . 2</head><label>202</label><figDesc>Under(21.10) and (21.11), we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>A</head><figDesc>random variable y is Poisson(λ) if its probability mass function is pr(y = k) = e -λ λ k k! , (k = 0, 1, 2, . . .) which sums to 1 by the Taylor expansion formula e λ = ∞ k=0 λ k k! . The Poisson(λ) random variable has the following properties: Proposition 22.1 If y ∼ Poisson(λ), then E(y) = var(y) = λ. Proposition 22.2 If y 1 , . . . , y K are mutually independent with y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head>FIGURE 22 ,</head><label>22</label><figDesc>FIGURE 22.1: Comparing the log probabilities of Poisson and Negative-Binomial with the same mean</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head>FIGURE 22 . 2 :</head><label>222</label><figDesc>FIGURE 22.2: Linear, Poisson, </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head>Figure 22 .</head><label>22</label><figDesc>Figure 22.4 plots the histograms of the outcomes from four weeks before and four weeks after the experiment. Eight histograms all show severe zero inflation because most workers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><figDesc>FIGURE 22.4: Zero-inflation of the data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>Figure 22 FIGURE 22</head><label>2222</label><figDesc> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><head>FIGURE 22 . 6 :</head><label>226</label><figDesc>FIGURE 22.6: Comparing AICs from five regression models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><figDesc>Proof of Lemma 23.1: Define ℓ(y | θ) = log f (y | θ) as the log likelihood function, so e ℓ(y|θ) is the density satisfying e ℓ(y|θ) dy = f (y | θ)dy = 1 by the definition of a probability density function (we can replace the integral by summation for a probability mass function). Differentiate the above identity to obtain | θ)dy = 0, which implies Bartlett's first identity. Differentiate it twice to obtain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_68"><figDesc>23.6)    depends on x i and β, with (b ′ ) -1 indicating the inverse function of b ′ (•).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_69"><head>FIGURE 23 . 1 :</head><label>231</label><figDesc>FIGURE 23.1: Quantities in a GLM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_71"><head>2 A</head><label>2</label><figDesc>rpois (n , lambda . x ) &gt; pois . pois = glm ( y ~x , family = poisson ( link = log )) &gt; summary ( pois . pois ) Coefficients : Estimate Std . Error z value Pr ( &gt;| z |) ( Intercept ) -0 . 0 Negative-Binomial regression model I then generate data from a Negative-Binomial regression model. The conditional mean function is still E(y i | x i ) = e x t i β , so we can still use Poisson regression as a working model. The robust standard error doubles the classical standard error. &gt; library ( MASS ) &gt; theta = 0 . 2 &gt; y = rnegbin (n , mu = lambda .x , theta = theta ) &gt; nb . pois = glm ( y ~x , family = poisson ( link = log )) &gt; summary ( nb . pois ) Coefficients : Estimate Std . Error z value Pr ( &gt;| z |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_72"><figDesc>on EHW and LZ standard errors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_73"><head>FIGURE 25 .</head><label>25</label><figDesc>FIGURE 25.1: GEE analysis of the gym data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_75"><head>FIGURE</head><figDesc>FIGURE 26.1: CDF and quantile</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_76"><figDesc>Figure26.5 shows the quantile regression lines, which are almost parallel with different intercepts. In Galton's data, x and y are very close to a bivariate Normal distribution. Theoretically, we can verify that with bivariate Normal (x, y), the conditional quantile function F -1 (τ | x) is linear in x with the same slope. See Problem 26.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_77"><figDesc>FIGURE 26.5: Galton's data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_78"><figDesc>FIGURE 27.1: Histograms of the time to event in the data from<ref type="bibr" target="#b166">Lin et al. (2016)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_79"><head>FIGURE 27 . 2 :</head><label>272</label><figDesc>FIGURE 27.2: Left: Gamma(α, β = 2) hazard functions; Right: Log-Normal(µ, σ 2 ) hazard functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_80"><figDesc>Example 27.3 (Log-Normal) The Log-Normal random variable T ∼Log-Normal(µ, σ 2 ) equals exponential of N(µ, σ 2 ). The right panel of Figure 27.2 plots the hazard functions with four different parameter combinations. Example 27.4 (Weibull) The Weibull distribution has many different parametrizations. Here I follow the R function dweibull, which has a shape parameter a &gt; 0 and scale parameter b &gt; 0. The Weibull(a, b) random variable T can be generated by T = bZ 1/a (27.1) which is equivalent to log T = log b + a -1 log Z,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_81"><figDesc>FIGURE 27.4: Data structure for the Kaplan-Meier curve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_82"><head>&gt;</head><figDesc>FIGURE 27.5: Lin et al. (2016)'s data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_83"><figDesc>&gt; cox . fit &lt;-coxph ( Surv ( futime , relapse ) ~NALTREXONE * THERAPY + + AGE + GENDER + T 0 _ PDA + site , + data = COMBINE ) &gt; summary ( cox . fit ) Call : coxph ( formula = Surv ( futime , relapse ) ~NALTREXONE * THERAPY + AGE + GENDER + T 0 _ PDA + site , data = COMBINE )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_84"><figDesc>FIGURE 27.7: Kaplan-Meier curves with 95% confidence intervals based on Gehan (1965)'s data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_86"><head>&gt;</head><figDesc>cox . fit &lt;-coxph ( Surv ( futime , relapse ) ~NALTREXONE * THERAPY + + AGE + GENDER + T 0 _ PDA + strata ( site ) = Surv ( futime , relapse ) ~NALTREXONE * THERAPY + AGE + GENDER + T 0 _ PDA + strata ( site ) , data = COMBINE , robust = TRUE ) n = 1 2 2 6 , number of events = 8 5 6 coef exp ( coef ) se ( coef ) robust se z Pr ( &gt;| z |) NALTREXONE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_87"><head>Proposition A. 2</head><label>2</label><figDesc>If an n × m matrix has rank k, then A = BC for some n × k matrix B and k × m matrix C.Proof of Proposition A.2: Let A j1 , . . . , A j k be the maximally linearly independent column vectors of A. Stack them into an n × k matrix B = (A j1 , . . . , A j k ). They can linearly represent all column vectors of A:A = (c 11 A j1 + • • • + c k1 A j k , . . . , c 1m A j1 + • • • + c km A j k ) = (BC 1 , . . . , BC m ) = BC,where C = (C 1 , . . . , C m ) is an k × m matrix with column vectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_88"><head>2</head><label>2</label><figDesc>the determinant are more useful. I will review two.Proposition A.3 For two square matrices A and B, we havedet(AB) = det(A)det(B) = det(BA).Proposition A.4 For two square matrices A ∈ R m×m and B ∈ R n×n , we have Let I n be the n × n identity matrix. An n × n matrix A is invertible/nonsingular if there exists an n × n matrix B such that AB = BA = I n . We call B the inverse of A, denoted by A -1 . If A is an orthogonal matrix, then A t = A -1 .A square matrix is invertible if and only if det(A) ̸ = 0. The inverse of a 2 ×-1 = B -1 A -1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_90"><head>Theorem A. 4 (</head><label>4</label><figDesc>Rayleigh quotient and eigenvalues) The maximum and minimum eigenvalues of an n × n symmetric matrix A equalsλ max (A) = max x̸ =0 r(x), λ min (A) = min x̸ =0 r(x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_91"><figDesc>t and B = (b 1 , . . . , b n ) are vectors, then trace(AB) = trace(BA) = ⟨B t , A⟩ = n i=1 a i b i . Proposition A.6 The trace of an n × n symmetric matrix A equals the sum of its eigenvalues: trace(A) = n i=1 λ i . Proof of Proposition A.6: It follows from the eigen-decomposition and Proposition A.5. Let Λ = diag{λ 1 , . . . , λ n }, and we have trace(A) = trace(P ΛP t ) = trace(ΛP t P ) = trace(Λ) = n i=1 λ i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_92"><head>A</head><figDesc>= U DV twhere U is n × m matrix with orthonormal columns (U t U = I m ), V is m × m orthogonal matrix, and D is m × m diagonal matrix. Similarly, for a wide matrix with n ≤ m, we can writeA = U DV t where U is n × n orthogonal matrix, V is m × n matrix with orthonormal columns (V t V = I n ),and D is n × n diagonal matrix. If D has only r ≤ min(m, n) nonzero elements, then we can further simplify the decomposition as A = U DV twhere U is n × r matrix with orthonormal columns (U t U = I r ), V is m × r matrix with orthonormal columns (V t V = I r ), and D is r × r diagonal matrix. With more explicit forms ofU = (U 1 , . . . , U r ), D = diag(d 1 , . . . , d r ), V = (V 1 , . . . , V r ),we can write A as A = (U 1 , . . . , U r )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_93"><figDesc>If f (x) is a function from R p to R, then we use the notation ∂f (x) ∂x ≡</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_95"><head>Theorem B. 1</head><label>1</label><figDesc>If X ∼ Gamma(α, θ), Y ∼ Gamma(β, θ) and X Y , then 1. X + Y ∼ Gamma(α + β, θ), 2. X/(X + Y ) ∼ Beta(α, β), 3. X + Y X/(X + Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_96"><head>Proposition B. 2</head><label>2</label><figDesc>If X ∼ Gamma(α, β), then If X ∼ Gamma(α, β), then E(log X) = ψ(α) -log β, var(log X) = ψ ′ (α). Proposition B.4 If X ∼ Beta(α, β), then E(X) = α α + β , var(X) = αβ (α + β)(α + β + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_97"><head>□ 0 e</head><label>0</label><figDesc>The difference between two IID exponential random variables follows the Laplace distribution Proposition B.8 If y 1 and y 2 are two IID Exponential(λ), then y = y 1 -y 2 has densityλ 2 exp(-λ|c|), -∞ &lt; c &lt; ∞which is the density of a Laplace distribution with mean 0 and variance 2/λ 2 .Proof of Proposition B.8: Both y 1 and y 2 have density f (c) = λe -λc and CDF F (c) = 1 -e -λc . The CDF of y = y 1 -y 2 at c ≥ 0 ispr(y 1 -y 2 ≤ c) = ∞ 0 pr(y 2 ≥ z -c)λe -λz dz = ∞ -λ(z-c) λe -λz dz = λe λc ∞ 0 e -2λz dz = λe λc /(2λ) = e λc /2.By symmetry, y 1 -y 2 ∼ y 2 -y 1 , so the CDF at c ≤ 0 is pr(y 1 -y 2 ≤ c) = 1 -pr(y 1 -y 2 ≤ -c)= 1 -e -λc /2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_98"><head>Proposition B. 9</head><label>9</label><figDesc>If Y 1 , . . . , Y n are IID Gumbel(µ, β), thenmax 1≤i≤n Y i ∼ Gumbel(µ + β log n, β). If Y 1 , . . . , Y n are independent Gumbel(µ i , 1), then max 1≤i≤n Y i ∼ Gumbel log n i=1e µi , 1 .I leave the proof as Problem 2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_99"><head>2 .</head><label>2</label><figDesc>For a random vector X Y with X and Y possibly being vectors, if it has joint density f XY (x, y), then we can obtain the marginal distribution of X f X (x) = f XY (x, y)dy and define the conditional densityf Y |X (y | x) = f XY (x, y) f X (x) if f X (x) ̸ = 0.Based on the conditional density, we can define the conditional expectation of any function of Y asE {g(Y ) | X = x} = g(y)f Y |X (y | x)dyand the conditional variance asvar {g(Y ) | X = x} = E {g(Y )} 2 | X = x -[E {g(Y ) | X = x}]In the above definitions, the conditional mean and variance are both deterministic functions of x. We can replace x by the random variable X to define E {g(Y ) | X} and var {g(Y ) | X}, which are functions of the random variable X and are thus random variables. Below are two important laws of conditional expectation and variance. Theorem B.2 (Law of total expectation) We have E(Y ) = E {E(Y | X)} . Theorem B.3 (Law of total variance or analysis of variance) We have var(Y ) = E {var(Y | X)} + var {E(Y | X)} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_100"><head>Proposition B. 10</head><label>10</label><figDesc>If X Y , then h(X) g(Y ) for any functions h(•) and g(•).Proposition B.11 If X Y , then f XY (x, y) = f X (x)f Y (y), f Y |X (y | x) = f Y (y), E {g(Y ) | X} = E {g(Y )} , E {g(Y )h(X)} = E {g(Y )} E {h(X)} .Expectations of random vectors or random matricesFor a random matrix W = (W ij ), we define E(W ) = (E(W ij )). For constant matrices A and C, we can verify thatE(AW + C) = AE(W ) + C, E(AW C) = AE(W )C.Covariance between two random vectorsIf W ∈ R r and Y ∈ R s , then their covariance cov(W, Y ) = E {W -E(W )} {Y -E(Y )} t is an r × s matrix. As a special case, cov(Y ) = cov(Y, Y ) = E {Y -E(Y )} {Y -E(Y )} t = E(Y Y t ) -E(Y )E(Y ) t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_101"><head>x</head><figDesc>t cov(Y )x = cov(x t Y ) = var(x t Y ) ≥ 0.Proposition B.13 For two random vectors W and Y , we havecov(AW + C, BY + D) = Acov(W, Y )B t and cov(AW + BY ) = Acov(W )A t + Bcov(Y )B t + Acov(W, Y )B t + Bcov(Y, W )A t .Similar to Theorem B.3, we have the following decomposition of the covariance. Theorem B.4 (Law of total covariance) We have cov (Y, W ) = E {cov (Y, W | X)} + cov {E(Y | X), E(W | X)} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_102"><head>I</head><figDesc>leave the proof of Theorem B.5 as Problem 2.4.Proposition B.15 If Y ∼ N(µ,Σ), then BY + C ∼ N(Bµ + C, BΣB t ), that is, any linear transformation of a Normal random vector is also a Normal random vector.Proof of Proposition B.15: By definition, Y = µ + Σ 1/2 Z where Z is the standard Normal random vector, we haveBY + c = B(µ + Σ 1/2 Z) + C = Bµ + C + BΣ 1/2 Z ∼ N(Bµ + C, BΣ 1/2 Σ 1/2t B t )∼ N(Bµ + C, BΣB t ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_103"><head>Theorem B. 8</head><label>8</label><figDesc>If Y has mean µ and covariance Σ, thenE(Y t AY ) = trace(AΣ) + µ t Aµ.Proof of Theorem B.8: The proof relies on the following two basic facts.• E(Y Y t ) = cov(Y ) + E(Y )E(Y t ) = Σ + µµ t . • For an n × n symmetric random matrix W = (w ij ), we have E {trace(W )} = trace {E(W )} because E ( n i=1 w ii ) = n i=1 E(w ii ). The conclusion follows from E(Y t AY ) = E {trace(Y t AY )} = E {trace(AY Y t )} = trace {E(AY Y t )} = trace AE(Y Y T ) = trace {A(Σ + µµ t )} = trace AΣ + Aµµ T= trace(AΣ) + trace(µ t Aµ) = trace(AΣ) + µ t Aµ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_104"><head>Theorem B. 9</head><label>9</label><figDesc>If Y ∼ N(µ, Σ), then var(Y t AY ) = 2trace(AΣAΣ) + 4µ t AΣAµ.I relegate the proof as Problem 2.10. From its definition, χ 2 n is the summation of the squares of n IID standard Normal random variables. It is closely related to quadratic forms of multivariate Normals.Theorem B.10 1. If Y ∼ N(µ, Σ) is an n-dimensional random vector with Σ ≻ 0, then (Y -µ) t Σ -1 (Y -µ) ∼ χ 2 n . If rank(Σ) = k ≤ n, then (Y -µ) t Σ + (Y -µ) ∼ χ 2 k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_105"><figDesc>ni ) → Σ. Then kn i=1 {Z ni -E(Z ni )} → N(0, Σ) in distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_106"><figDesc>follows from the property of max, (C.5) follows from the fact ∥s∥ 2 = 1, (C.6) follows from the fact that |s i | ≤ |h ii | ≤ κ 1/2 , and (C.7) follows from κ → 0 and the dominant convergence theorem in Proposition C.5.I then prove the necessity. Pick one i * from arg max 1≤i≤n h ii . Consider a special linear combination of the OLS estimator: ŷi * = x t i * β, which is the fitted value of the i * th observation and has the form ŷi</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_107"><figDesc>θ) ∂z t Σ ∂f (θ) ∂z in distribution.Proof of Proposition C.11: I will give an informal proof. Using Taylor expansion, we have√ n{f (Z n ) -f (θ)} ∼ = ∂f (θ) ∂z t √ n(Z n -θ), which is a linear transformation of √ n(Z n -θ). Because √ n(Z n -θ) → N(µ, Σ) in distribution, we have √ n{f (Z n ) -f (θ)} → ∂f (θ) ∂z t N(µ, Σ) = N ∂f (θ) ∂z t µ, ∂f (θ) ∂z t Σ ∂f (θ) ∂z in distribution. □ Proposition C.11 above is more useful when ∂f (θ)/∂z ̸ = 0. Otherwise, we need to invoke higher-order Taylor expansion to obtain a more accurate asymptotic approximation. D M-Estimation and MLE A wide range of statistics estimation problems can be formulated as an estimating equation: m(W, b) = n -1 n i=1 m(w i , b) = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_108"><head>I</head><figDesc>start with the simple case with IID data. Theorem D.1 Assume that W = {w i } n i=1 are IID with the same distribution as w. The true parameter β ∈ R p is the unique solution of E {m(w, b)} = 0, and the estimator β ∈ R p is the solution of m(W, b) = 0.Under some regularity conditions,√ n β -β → N(0, B -1 M B -t )in distribution, where B = -∂E {m(w, β)} ∂b t , M = E{m(w, β)m(w, β) t }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_109"><figDesc>The asymptotic Normality follows from three steps. First, from the Taylor expansion0 = m(W, β) ∼ = m(W, β) + ∂ m(W, i , β) → N(0, M ) in distribution.Finally, Slutsky's theorem implies the result. □ The above result also holds with independent but non-IID data. Theorem D.2 Assume that {w i } n i=1 are independent observations. The true parameter β ∈ R p is the unique solution to E { m(W, b)} = 0, and the estimator β ∈ R p is the solution to m(W, b) = 0, Under some regularity conditions, √ n β -β → N(0, B -1 M B -t )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_110"><head>∂ 2 ∂</head><label>2</label><figDesc>'s first identity. Under regularity conditions, √ n( θ -θ) converges in distribution to Normal with mean zero and covariance B -1 M B -1 , whereB = -∂ ∂θ t E ∂ log f (y | θ) ∂θ = E -∂ 2 log f (y | θ) ∂θ∂θ tis called the Fisher information matrix, denoted by I(θ), andM = E ∂ log f (y | θ) ∂θ ∂ log f (y | θ) ∂θ tis sometimes also called the Fisher information matrix, denoted by J(θ).If the model is correct, Bartlett's second identity ensures thatI(θ) = J(θ), (D.2)and therefore √ n( θ -θ) converges in distribution to Normal with mean zero and covariance I(θ) -1 = J(θ) -1 . So a covariance matrix estimator for the MLE is I n ( θ) -1 or J n ( θ) -1 , whereI n ( θ) = -n i=1 log f (y i | θ) ∂θ∂θ t and J n ( θ) = n i=1 log f (y i | θ) ∂θ ∂ log f (y i | θ) ∂θ t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_111"><figDesc>corrected his error, and Abadie et al. (2014) developed a more general theory. A leading application is the MLE under a misspecified Normal linear model. The EHW robust covariance arises naturally in this case. Example D.5 The Normal linear model has individual log-likelihood:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="37,120.53,85.11,355.10,273.71" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Who was Francis Galton? He was Charles Darwin's nephew and was famous for his pioneer work in statistics and for devising a method for classifying fingerprints that proved useful in forensic science. He also invented the term eugenics, a field that causes a lot of controversies nowadays.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>This book uses different notions of "independence" which can be confusing sometimes. In linear algebra, a set of vectors is linearly independent if any nonzero linear combination of them is not zero; see Chapter A. In probability theory, two random variables are independent if their joint density factorizes into the product of the marginal distributions; see Chapter B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>In this book, I do not spell it as homoscedasticity since "k" better indicates the meaning of variance.<ref type="bibr" target="#b177">McCulloch (1985)</ref> gave a convincing argument. See also<ref type="bibr" target="#b188">Paloyo (2014)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3"><p>Because X has linearly independent columns, X t X is a non-degenerate and thus positive definite matrix. Since u t C(X t X) -1 C t u ≥ 0, to show that C(X t X) -1 C t is non-degenerate, we only need to show thatu t C(X t X) -1 C t u = 0 =⇒ u = 0. From u t C(X t X) -1 C t u = 0, we know C t u = u 1 c 1 + • • • u l c l = 0.Since the rows of C are linearly independent, we must have u = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>The "0 +" in the above code forces the OLS to exclude the constant term.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_5"><p>The usual form of Simpson's paradox is in terms of a</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_6"><p>× 2 × 2 table with all binary variables. Here we focus on the continuous version.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_7"><p>This also follows from Theorem A.4 since the eigenvalues of H are 0 and 1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_8"><p>We have already proved a more general result on the covariance matrix of Ŷ in Theorem 4.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_9"><p>Exchangeability is a technical term in probability and statistics. Random elements z 1 , . . . , zn are exchangeable if (z π(1) , . . . , z π(n) ) have the same distribution as (z 1 , . . . , zn), where π(1), . . . , π(n) is a permutation of the integers 1, . . . , n. In other words, a set of random elements are exchangeable if their joint distribution does not change under re-ordering. IID random elements are exchangeable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_10"><p>Splitting a dataset into the training and testing datasets is a standard tool to assess the out-of-sample performance of proposed methods. It is important in statistics and machine learning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_11"><p>A celebrated theorem due to Weierstrass states that on a bounded interval, any continuous function can be approximated arbitrarily well by a polynomial function. Here is the mathematical statement of Weierstrass's theorem. Suppose f is a continuous function defined on the interval[a, b]. For every ε &gt; 0, there exists a polynomial p such that for all x ∈ [a, b], we have |f (x) -p(x)| &lt; ε.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_12"><p>The approximation is due to the Taylor expansion log(1+ x) = x -x 2 /2 + x 3 /3 -• • • ≈ x.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_13"><p>Note that this function uses a definition of bic that differs from the above definition by a constant, but this does not change the model selection result.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_14"><p>This is also called the Tikhonov regularization<ref type="bibr" target="#b220">(Tikhonov, 1943)</ref>. See<ref type="bibr" target="#b61">Bickel and Li (2006)</ref> for a review of the idea of regularization in statistics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_15"><p>I choose this standardization because it is the default choice in the function lm.ridge in the R package MASS. In practical data analysis, the covariates may have concrete meanings. In those cases, you may not want to scale the covariates in the way as Condition 14.1. However, the discussion below does not rely on the choice of scaling although it requires centering the covariates and outcome.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_16"><p>&gt; library ( " mlbench " )</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_17"><p>The matrix X t Σ -1 X is positive definite and thus invertible, because (1) for any α ∈ R p , Σ -1 ⪰ 0 =⇒ α t X t ΣXα ≥ 0, and (2) α t X t ΣXα = 0 ⇐⇒ Xα = 0 ⇐⇒ α = 0 since X has linearly independent columns.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_18"><p><ref type="bibr" target="#b58">Berkson (1944)</ref> was an early use of the logit model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_19"><p><ref type="bibr" target="#b63">Bliss (1934)</ref> was an early use of the probit model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_20"><p>The notation can be confusing because β denotes both the true parameter and the dummy variable for the likelihood function.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_21"><p>An alternative strategy is to model 1(y i = k) | x i for each k. The advantage of this strategy is that it reduces to binary logistic models. The disadvantage of this strategy is that it does not model the whole distribution of y i and can lose efficiency in estimation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_22"><p>The command rnbreg in Stata uses the Berndt-Hall-Hall-Hausman (BHHH) algorithm by default, which may give slightly different numbers compared with R. The BHHH algorithm is similar to Newton's algorithm but avoids calculating the Hessian matrix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_23"><p>The coefficient of variation of a random variable A equals var(A)/E(A).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_24"><p>Note that the difference between two independent Exponentials has the same distribution as Laplace. See Proposition B.8.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_25"><p>&gt; library ( foreign ) &gt; census 8 0 = read . dta ( " census 8 0 . dta " )</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_26"><p><ref type="bibr" target="#b190">Peto and Peto (1972)</ref> popularized the name log-rank test.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_27"><p>There are counterexamples in which β is inconsistent; see<ref type="bibr" target="#b112">Freedman and Diaconis (1982)</ref>. The examples in this book are all regular.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.4">Gauss-Markov theorem for restricted OLS</head><p>The Gauss-Markov theorem for βr holds, as an extension of Theorem 4.4 for β.</p><p>Theorem 18.3 Under the Gauss-Markov model with the restrictions (18.1), βr is the best linear unbiased estimator in the sense that cov( βr ) -cov( βr ) ⪰ 0 for any linear estimator βr = c+ Ãr Y , with c ∈ R p and Ãr ∈ R p×n , that satisfies E( βr ) = β for all β under constraint (18.1).</p><p>Prove Theorem 18.3. Remark: As a corollary of Theorem 18.3, we have (X t X) -1 ⪰ M r (X t X) -1 M t r because the restricted OLS estimator is BLUE whereas the unrestricted OLS is not, under the Gauss-Markov theorem with the restriction (18.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.5">Short regression as restricted OLS</head><p>The short regression is a special case of the formula of βr . Show that βr = (X t 1 X 1 ) -1 X t 1 Y 0 l with C = (0 l×k , I l×l ), r = 0 l .</p><p>In this special case, p = k + l.</p><p>From the short regression, we can obtain the EHW estimated covariance matrix Vehw,1 . We can also obtain the EHW estimated covariance matrix Vehw,r from the restricted OLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Show that</head><p>Vehw,r = Vehw,1 0 0 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.6">Reducing restricted OLS to OLS</head><p>Consider the restricted OLS fit Y = X βr + εr subject to C βr = 0, <ref type="bibr">(18.3)</ref> where X ∈ R n×p and C ∈ R l×p . Let C ⊥ ∈ R (p-l)×p be an orthogonal complement of C in the sense that (C t ⊥ , C t ) is nonsingular with C ⊥ C t = 0. Define</p><p>Consider the corresponding unrestricted OLS fit Y = X ⊥ β⊥ + ε⊥ , (18.4) First, prove that the coefficient and residual vectors must satisfy β⊥ = C ⊥ βr , ε⊥ = εr .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second, prove</head><p>Vehw,⊥ = C ⊥ Vehw,r C t ⊥ , where Vehw,⊥ is the EHW robust covariance matrix from (18.4) and Vehw,r is the EHW robust covariance matrix from (18.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">-4</head><p>-3 . 3 1 2 4 3 3 1 . 3 2 6 3 5 4 0 . 0 0 8 5 2 7 5 6 1 0 . 5 1 8 6 6 5 4 0 . 7 5 1 4 4 5 1 5 + -5 . 9 3 5 1 7 2 1 . 6 2 7 7 1 1 0 . 0 0 8 9 7 8 1 0 3 0 . 9 0 6 3 8 3 1 2 . 2 7 8 0 8 7 7</p><p>Std . Errors : ( Intercept ) hightreat age rural male 2 -4 1 . <ref type="table">4 6 3 2 5 8 0 . 5 1 4 1 1 2 7 0 . 0 1 6 6 0 6 4 8 0 . 4 0 8 5 9 7 6 0 . 4 8 0 6 9 5 3  5 +  2 . 1 9 0 3 0 5 0 . 7 3 2 0 7 8 8 0 . 0 2 2 4 4 8 6 7 0 . 5 6 4 5 5 9 5 1 . 0 7 3 9 6 6 9</ref> Residual Deviance : 2 5 8 . 5 6 7 5 AIC : 2 7 8 . 5 6 7 5 hightreat is significant above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.5.4">Proportional odds logistic for the outcome</head><p>The multinomial logisitic model does not reflect the ordering information of the outcome. For instance, in the regression with highdiag, the coefficient for level "2-4" is -0.06973187 &lt; 0, but the coefficient for level "5+" is 0.64036289 &gt; 0, which means that highdiag decreases the probability of "2-4" but increases the probability of "5". However, this is illogical since a patient must first live longer than two years and then live longer than five years. Nevertheless, it is not a severe problem in this case study because those coefficients are not significant.</p><p>&gt; library ( MASS ) &gt; yearpo = polr ( survival ~highdiag + age + rural + male , + Hess = TRUE , + data = karolinska ) &gt; summary ( yearpo ) Call : polr ( formula = survival ~highdiag + age + rural + male , data = karolinska , Hess = TRUE )</p><p>Coefficients : Value Std . Error t value highdiag 0 . 2 1 6 7 5 5 0 . 3 5 8 9 2 0 . 6 0 3 9 age -0 . 0 0 2 8 8 1 0 . 0 1 3 7 8 -0 . 2 0 9 1 rural 0 . 3 7 1 8 9 8 0 . 3 5 3 1 3 1 . 0 5 3 2 male 0 . 9 4 3 9 5 5 0 . 4 3 5 8 8 2 . 1 6 5 6</p><p>Intercepts : Value Std . Error t value 1 | 2 -4 1 . 4 0 7 9 1 . 1 3 0 9 1 . 2 4 5 0 2 -4 | 5 + 2 . 9 2 8 4 1 . 1 5 1 4 2 . 5 4 3 4</p><p>Residual Deviance : 2 7 1 . 0 7 7 8 AIC : 2 8 3 . 0 7 7 8 &gt; predict ( yearpo , type = " probs " )[ 1 : 5 , ] 1 2 -4 5 + 1 0 . 5 8 6 2 4 6 5 0 . 2 8 0 0 8 9 2 0 . 1 3 3 6 6 4 2 7 2 0 . 5 8 5 5 4 7 5 0 . 2 8 0 4 5 4 2 0 . 1 3 3 9 9 8 2 3 3 0 . 8 0 8 7 3 4 1 0 . 1 4 2 1 0 6 5 0 . 0 4 9 1 5 9 4 8 4 0 . 5 8 6 2 4 6 5 0 . 2 8 0 0 8 9 2 0 . 1 3 3 6 6 4 2 7 5 0 . 6 2 0 5 9 8 3 0 . 2 6 1 5 1 1 2 0 . 1 1 7 8 9 0 5 0 highdiag is not significant above. The predict function gives the fitted probabilities of three categories. price ␣ -0 . 0 2 0 4 7 6 5 ␣ ␣ 0 . 0 0 1 2 2 3 1 ␣ -1 6 . 7 4 2 ␣ &lt;␣ 2 . 2e -1 6 ␣ *** catch ␣ ␣ 0 . 9 5 3 0 9 8 2 ␣ ␣ 0 . 0 8 9 4 1 3 4 ␣ ␣ 1 0 . 6 5 9 ␣ &lt;␣ 2 . 2e -1 6 ␣ *** Log -Likelihood : ␣ -1 3 1 2</p><p>If we do not enforce 0 + price, we allow for intercepts that vary across choices: &gt; summary ( mlogit ( mode ~price + catch , data = Fish ))</p><p>Call : mlogit ( formula = mode ~price + catch , data = Fish , method = " nr " )</p><p>Frequencies of alternatives : choice beach boat charter pier 0 . 1 1 3 3 7 0 . 3 5 3 6 4 0 . 3 8 2 4 0 0 . 1 5 0 5 9 nr method 7 iterations , 0 h : 0 m : 0 s g '( -H )^-1 g ␣ = ␣ 6 . 2 2E -0 6 successive ␣ function ␣ values ␣ within ␣ tolerance ␣ limits Call : mlogit ( formula = mode ~0 | income , data = Fish , method = " nr " ) Frequencies of alternatives : choice beach boat charter pier 0 . 1 1 3 3 7 0 . 3 5 3 6 4 0 . 3 8 2 4 0 0 . 1 5 0 5 9 nr method 4 iterations , 0 h :</p><p>3 8 9 2e -0 1 ␣ ␣ 1 . 9 6 7 3e -0 1 ␣ ␣ 3 . 7 5 6 0 ␣ 0 . 0 0 0 1 7 2 7 ␣ *** ( Intercept ): charter ␣ ␣ 1 . 3 4 1 3 e + 0 0 ␣ ␣ 1 . 9 4 5 2e -0 1 ␣ ␣ 6 . 8 9 5 5 ␣ 5 . 3 6 7e -1 2 ␣ *** ( Intercept ): pier ␣ ␣ ␣ ␣ ␣ 8 . 1 4 1 5e -0 1 ␣ ␣ 2 . 2 8 6 3e -0 1 ␣ ␣ 3 . 5 6 1 0 ␣ 0 . 0 0 0 3 6 9 5 ␣ *** income : boat ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ 9 . 1 9 0 6e -0 5 ␣ ␣ 4 . 0 6 6 4e -0 5 ␣ ␣ 2 . 2 6 0 2 ␣ 0 . 0 2 3 8 1 1 6 ␣ * income : charter ␣ ␣ ␣ ␣ ␣ ␣ -3 . 1 6 4 0e -0 5 ␣ ␣ 4 . 1 8 4 6e -0 5 ␣ -0 . 7 5 6 1 ␣ 0 . 4 4 9 5 9 0 8 income : pier ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ -1 . 4 3 4 0e -0 4 ␣ ␣ 5 . 3 2 8 8e -0 5 ␣ -2 . 6 9 1 1 ␣ 0 . 0 0 7 1 2 2 3 ␣ ** Log -Likelihood : ␣ -1 4 7 7 . 2 McFadden ␣ R ^2 : ␣ ␣ 0 . 0 1 3 7 3 6 Likelihood ␣ ratio ␣ test ␣ : ␣ chisq ␣ = ␣ 4 1 . 1 4 5 ␣ ( p . value ␣ = ␣ 6 . 0 9 3 1e -0 9 )</p><p>It is equivalent to fitting the multinomial logistic model: summary ( multinom ( mode ~income , data = Fishing ))</p><p>The most general model includes all covariates.</p><p>&gt; summary ( mlogit ( mode ~price + catch | income , data = Fish ))</p><p>Call : mlogit ( formula = mode ~price + catch | income , data = Fish , method = " nr " )</p><p>Frequencies of alternatives : choice beach boat charter pier 0 . 1 1 3 3 7 0 . 3 5 3 6 4 0 . 3 8 2 4 0 0 . 1 5 0 5 9 nr method 7 iterations , 0 h : 0 m : 0 s g '( -H )^-1 g ␣ = ␣ 1 . 3 7E -0 5 successive ␣ function ␣ values ␣ within ␣ tolerance ␣ limits</p><p>3 6 6 7 ␣ 0 . 0 1 7 9 4 8 5 ( Intercept ): charter ␣ ␣ 1 . 6 9 4 4 e + 0 0 ␣ ␣ 2 . 2 4 0 5e -0 1 ␣ ␣ ␣ 7 . 5 6 2 4 ␣ 3 . 9 5 2e -1 4 ( Intercept ): pier ␣ ␣ ␣ ␣ ␣ 7 . 7 7 9 6e -0 1 ␣ ␣ 2 . 2 0 4 9e -0 1 ␣ ␣ ␣ 3 . 5 2 8 3 ␣ 0 . 0 0 0 4 1 8 3 price ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ -2 . 5 1 1 7e -0 2 ␣ ␣ 1 . 7 3 1 7e -0 3 ␣ -1 4 . 5 0 4 2 ␣ &lt;␣ 2 . 2e -1 6 catch ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ 3 . 5 7 7 8e -0 1 ␣ ␣ 1 . 0 9 7 7e -0 1 ␣ ␣ ␣ 3 . 2 5 9 3 ␣ 0 . 0 0 1 1 1 7 0 income : boat ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ 8 . 9 4 4 0e -0 5 ␣ ␣ 5 . 0 0 6 7e -0 5 ␣ ␣ ␣ 1 . 7 8 6 4 ␣ 0 . 0 7 4 0 3 4 5 income : charter ␣ ␣ ␣ ␣ ␣ ␣ -3 . 3 2 9 2e -0 5 ␣ ␣ 5 . 0 3 4 1e -0 5 ␣ ␣ -0 . 6 6 1 3 ␣ 0 . 5 0 8 4 0 3 1 income : pier ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ ␣ -1 . 2 7 5 8e -0 4 ␣ ␣ 5 . 0 6 4 0e -0 5 ␣ ␣ -2 . 5 1 9 3 ␣ 0 . 0 1 1 7 5 8 2</p><p>McFadden ␣ R ^2 : ␣ ␣ 0 . 1 8 8 6 8 Likelihood ␣ ratio ␣ test ␣ : ␣ chisq ␣ = ␣ 5 6 5 . 1 7 ␣ ( p . value ␣ = ␣ &lt;␣ 2 . 2 2e -1 6 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.6.3">More comments</head><p>The assumption of Gumbel error terms is very strong. However, relaxing this assumption leads to much more complicated forms of the conditional probabilities of the outcome. The model <ref type="bibr">(21.8)</ref> </p><p>so the choice between k and l does not depend on the existence of other choices. This is called the independence of irrelevant alternatives (IIA) assumption. This is often a plausible assumption. However, it may be violated. For example, with the apple and orange, someone chooses the apple; but with the apple, orange, and banana, s/he may choose the orange. The model <ref type="bibr">(21.8)</ref> is the basic form of the discrete choice model. <ref type="bibr" target="#b224">Train (2009)</ref> is a monograph on this topic which provides many extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Model and Extensions</head><p>Example 23.1 (continued) Model ( <ref type="formula">23</ref>.1) has conditional probability density function</p><p>Example 23.2 (continued) Model ( <ref type="formula">23</ref>.2) has conditional probability mass function</p><p>and a(ϕ) = 1.</p><p>Example 23.3 (continued) Model ( <ref type="formula">23</ref>.3) has conditional probability mass function</p><p>and a(ϕ) = 1.</p><p>Example 23.4 (continued) Model <ref type="bibr">(23.4)</ref>, for a fixed δ, has conditional probability mass function</p><p>and a(ϕ) = 1.   <ref type="table">1</ref><ref type="table">2</ref><ref type="table">3</ref><ref type="table">4</ref><ref type="table">5</ref>. 9 9 1e -0 8 *** assign -0 . 1 9 7 8 3 0 . 1 3 7 1 7 8 5 -1 . 4 3 9 9 0 . 1 4 9 8 8 5 age -0 . 0 0 7 5 9 0 . 0 0 5 7 0 5 3 -1 . 3 9 9 7 0 . 1 6 1 5 9 0 copd 0 . 3 3 7 7 1 0 . 1 5 5 6 7 8 1 2 . 1 6 5 0 0 . 0 3 0 3 9 1 * dm 0 . 4 5 4 1 6 0 . 1 3 9 4 7 0 9 3 . 2 5 7 6 0 . 0 0 1 1 2 4 ** heartd 0 . 6 7 6 9 5 0 . 1 5 2 1 1 0 5 4 . 4 4 5 4 8 . 7 7 4e -0 6 *** race -0 . 2 4 2 8 8 0 . 1 4 3 0 9 5 7 -1 . 6 9 7 8 0 . 0 8 9 5 4 4 . renal 1 . 5 1 9 4 9 0 . 3 6 5 9 2 3 8 4 . 1 5 2 5 3 . 2 8 8e -0 5 *** sex -0 . 2 1 2 5 4 0 . 1 4 8 9 4 3 5 -1 . 4 2 4 0 0 . 1 5 4 4 4 7 liverd 0 . 0 9 8 7 2 1 . 1 4 1 1 1 3 3 0 . 0 8 6 7 0 . 9 3 0 8 9 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.3.2.2">A misspecified logistic regression</head><p>Freedman ( <ref type="formula">2006</ref>) discussed the following misspecified logistic regression. The discrepancy between the two types of standard errors is a warning of the misspecification of the conditional mean function because it determines the whole conditional distribution. In this case, it is not meaningful to interpret the coefficients. Overall, for count outcome regression, it seems that Poisson regression suffices as long as we use the robust standard error. The Negative-Binomial is unlikely to offer more if only the conditional mean is of interest. <ref type="bibr" target="#b251">Zou (2004)</ref> proposed to use Poisson regression to analyze binary outcomes. This can be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24.3.4">Poisson regression for binary outcomes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.5">Application</head><p>We will use the gee package for all the analyses below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.5.1">Clustered data: a neuroscience experiment</head><p>The original study was interested in the potential interaction between two treatments, so I always include the interaction term in the regression model.</p><p>From the simple specification below, pten has a significant effect, but fa and the interactions are not significant.</p><p>Pten . gee = gee ( somasize ~factor ( fa )* pten , + id = mouseid , + family = gaussian , + corstr = " independence " , + data = Pten ) &gt; summary ( Pten . gee )$ coef Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) 9 3 . 1 0 6 1 . 5 9 4 5 8 . 4 2 1 6 3 . 0 5 9 3 0 . 4 3 factor ( fa ) 1 3 . 7 5 6 2 . 1 7 5 1 . 7 2 6 8 3 . 1 7 4 1 . 1 8 factor ( fa ) 2 6 . 9 0 7 2 . 5 5 1 2 . 7 0 7 8 5 . 4 0 7 1 . 2 7 pten 1 1 . 0 3 9 2 . 0 8 2 5 . 3 0 1 6 2 . 2 0 0 5 . 0 1 factor ( fa ) 1 : pten 8 . 7 2 7 2 . 8 3 4 3 . 0 7 9 5 5 . 0 2 3 1 . 7 3 factor ( fa ) 2 : pten -2 . 9 0 4 3 . 2 7 0 -0 . 8 8 8 1 3 . 5 5 4 -0 . 8 1 &gt; &gt; &gt; Pten . gee = gee ( somasize ~factor ( fa )* pten , + id = mouseid , + family = gaussian , + corstr = " exchangeable " , + data = Pten ) &gt; summary ( Pten . gee )$ coef Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) 9 0 . 9 0 0 3 . 5 3 2 2 5 . 7 3 7 6 2 . 7 0 1 3 3 . 6 5 factor ( fa ) 1 4 . 9 2 1 5 . 1 1 5 0 . 9 6 2 1 2 . 9 1 4 1 . 6 8 factor ( fa ) 2 6 . 4 0 8 5 . 0 6 6 1 . 2 6 4 9 5 . 9 0 4 1 . 0 8 pten 1 1 . 5 0 1 1 . 9 7 9 5 . 8 1 2 0 2 . 1 9 0 5 . 2 5 factor ( fa ) 1 : pten 8 . 8 0 7 2 . 6 8 8 3 . 2 7 6 6 5 . 0 5 0 1 . 7 4 factor ( fa ) 2 : pten -1 . 5 2 5 3 . 1 1 3 -0 . 4 8 9 8 2 . 7 0 3 -0 . 5 6</p><p>Including two covariates, we have the following results. The covariates are predictive of the outcome, changing the significance level of the main effect of fa. The interaction terms between pten and fa are not significant either. 3 . 1 6 6 -0 . 9 3 5 9 3 . 3 1 0 5 -0 . 8 &gt; &gt; &gt; Pten . gee = gee ( somasize ~factor ( fa )* pten + numctrl + numpten , + id = mouseid , + family = gaussian , + corstr = " exchangeable " , + data = Pten ) &gt; summary ( Pten . gee )$ coef Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) 8 5 . 3 3 1 6 5 . 2 8 7 2 1 6 . 1 3 9 3 5 . 4 0 9 5 1 5 . 7 7 4 5 factor ( fa ) 1 5 . 4 9 5 2 4 . 2 7 6 1 1 . 2 8 5 1 4 . 0 2 0 7 1 . 3 6 6 7 factor ( fa ) 2 1 2 . 2 1 7 4 4 . 1 6 6 9 2 . 9 3 2 0 4 . 2 3 6 3 2 . 8 8 4 0 pten 1 1 . 8 0 4 4 1 . 9 7 1 8 5 . 9 8 6 5 2 . 1 9 4 6 5 . 3 7 8 9 numctrl 0 . 9 3 2 6 0 . 2 8 6 7 3 . 2 5 2 7 0 . 3 4 7 9 2 . 6 8 1 0 numpten -0 . 5 6 7 8 0 . 2 5 0 4 -2 . 2 6 7 4 0 . 2 7 7 2 -2 . 0 4 8 2 factor ( fa ) 1 : pten 8 . 5 1 3 7 2 . 6 7 7 7 3 . 1 7 9 5 5 . 0 6 1 2 1 . 6 8 2 1 factor ( fa ) 2 : pten -1 . 7 7 5 5 3 . 0 9 9 5 -0 . 5 7 2 8 2 . 7 5 4 7 -0 . 6 4 4 5</p><p>From the regressions above, we observe that (1) two choices of the covariance matrix do not lead to fundamental differences; and (2) without using the cluster-robust standard error, the results can be misleading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.5.2">Clustered data: a public health intervention</head><p>We first fit simple GEE without using the covariate. -0 . 7 7 9 9 0 . 1 3 1 4 -5 . 9 3 7 1 0 . 1 5 2 2 -5 . 1 2 3 5 zLPP Only 0 . 1 6 3 8 0 . 2 0 4 2 0 . 8 0 2 1 0 . 2 2 9 0 0 . 7 1 5 2 zLPP + Subsidy 0 . 7 7 8 9 0 . 1 5 0 0 5 . 1 9 2 6 0 . 1 7 9 0 4 . 3 5 2 4 zLPP + Subsidy + Supply 0 . 7 3 4 8 0 . 1 5 0 6 4 . 8 7 9 8 0 . 1 7 6 0 4 . 1 7 5 3 zSupply Only 0 . 2 6 9 0 0 . 2 2 0 7 1 . 2 1 8 7 0 . 3 0 1 1 0 . 8 9 3 1</p><p>Without adjusting for the covariates, treatment levels "zLPP+Subsidy" and "zLPP+Subsidy+Supply" are significant. The exchangeable working covariance matrix does seem to improve the estimated precision.</p><p>We then fit GEE with a covariate.</p><p>&gt; hygaccess . gee = gee ( y ~z + x , id = vid , + family = binomial ( link = logit ) , + corstr = " independence " , + data = hygaccess ) &gt; summary ( hygaccess . gee )$ coef Estimate Naive S . E . Naive z Robust S . E . Robust z ( Intercept ) -1 . 7 5 2 6 0 . 0 6 1 7 4 -2 8 . 3 8 6 0 . 1 3 9 8 -1 2 . 5 3 8 zLPP Only 0 . 2 2 7 7 0 . 0 6 8 3 3 3 . 3 3 2 0 . 1 3 9 3 1 . 6 3 5 zLPP + Subsidy 0 . 6 8 5 0 0 . 0 5 6 4 5 1 2 . 1 3 3 0 . 1 1 9 1 5 . 7 4 9 future observation x is , the marginal model gives predicted outcome µ(x t is β) with the associated standard error computed based on the delta method. We can see two obvious problems with this prediction. First, it does not depend on s. Consequently, predicting s = 10 is the same as predicting s = 100. However, the intuition is overwhelming that predicting the long-run outcome is much more difficult than predicting the short-run outcome, so we hope the standard error should be much larger for predicting the outcome at s = 100. Second, the prediction does not depend on the lag outcomes because the marginal model ignores the dynamics of the outcome. With longitudinal observations, building a model with the lag outcomes may increase the prediction ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.7">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.1">Sandwich asymptotic covariance matrix for GEE</head><p>Verify the formulas of B and M in Section 25.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.2">Cluster-robust standard error in OLS with a cluster-specific binary regressor</head><p>Consider a special case with x it = (1, x i ) t and x i ∈ {0, 1} for i = 1, . . . , n, and view "1" as treatment and "0" as control. Show that the coefficient of x i in the pooled OLS fit of y it on x it equals τ = ȳ1 -ȳ0 where</p><p>denoting the total number of observations under treatment and control, respectively. Further show that the cluster-robust standard error of τ equals the square root of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25.3">Cluster-robust standard error in GLM with a cluster-specific binary regressor</head><p>Inherit the setting from Problem 25.2. With a binary outcome y it , show that the coefficient of x i in the pooled logit regression of y it on x it equals τ = logitȳ 1 -logitȳ 0 . Further show that the cluster-robust standard error of τ equals the square root of</p><p>.</p><p>With a count outcome y it , show that the coefficient of x i in the pooled Poisson regression of y it on x it equals τ = log ȳ1 -log ȳ0 . Further show that the cluster-robust standard error of τ equals the square root of</p><p>Unlike other regression models, x does not contain the intercept in (27.2). If the first component of x is 1, then we can write</p><p>and redefine λ 0 (t)e β1 as another unknown function. With an intercept, we cannot identify λ 0 (t) and β 1 separately. So we drop the intercept to ensure identifiability.</p><p>From the log-linear form of the conditional hazard function, we have</p><p>so each coordinate of β measures the log conditional hazard ratio holding other covariates constant. Because of this, (27.2) is called the proportional hazards model. A positive β j suggests a "positive" effect on the hazard function and thus a "negative" effect on the survival time itself. Consider a special case with a binary x i , then the proportional hazards assumption implies that λ(t | 1) = γλ(t | 0) with γ = exp(β), and therefore the survival functions satisfy</p><p>which is a power transformation. Qualitatively, we have the following two cases:</p><p>(PH1) β &lt; 0: so the hazard ratio γ = exp(β) &lt; 1 and S(t | 1) ≥ S(t | 0) for all t, which implies a longer survival time under treatment;</p><p>(PH2) β &gt; 0: so the hazard ratio γ = exp(β) &gt; 1 and S(t | 1) ≤ S(t | 0) for all t, which implies a shorter survival time under treatment.</p><p>Figure <ref type="figure">27</ref>.6 shows some survival functions satisfying the proportional hazards assumption, none of which cross each other within the interval t ∈ (0, ∞). When the two survival functions cross, the proportional hazards assumption does not hold. Theoretically, we can allow the covariates to be time-dependent, that is, x i (t) can depend on t and thus is a stochastic process. However, the interpretation of the coefficient becomes challenging <ref type="bibr" target="#b99">(Fisher and Lin, 1999)</ref>. This chapter focuses on the simple case with timeinvariant covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.4.2">Partial likelihood</head><p>The likelihood function is rather complicated, which depends on an unknown function λ 0 (t). Assuming no ties, <ref type="bibr" target="#b82">Cox (1972)</ref> proposed to use the partial likelihood function to estimate β:</p><p>, Concordance = 0 . 5 6 1 ( se = 0 . 0 1 1 ) Likelihood ratio test = 3 5 . 2 4 on 6 df , p = 4e -0 6 Wald test = 3 3 . 8 5 on 6 df , p = 7e -0 6 Score ( logrank ) test = 3 4 . 9 4 on 6 df , p = 4e -0 6 , Robust = 3 4 . 1 5 p = 6e -0 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.5.2">Clustered Cox model</head><p>With clustered data, we must adjust for the standard errors. The coxph function reports the cluster-robust standard errors with the specification of cluster. A canonical example of clustered data is from the matched-pair design if we view the pairs as clusters. The example below uses the data from <ref type="bibr" target="#b148">Huster et al. (1989)</ref> in which two eyes of a patient were either assigned to treatment or control.</p><p>&gt; library ( " timereg " ) &gt; data ( diabetes ) &gt; pair . cox = coxph ( Surv ( time , status ) ~treat + adult + agedx , + robust = TRUE , + data = diabetes ) &gt; summary ( pair . cox ) Call : coxph ( formula = Surv ( time , status ) ~treat + adult + agedx , data = diabetes , robust = TRUE ) n = 3 9 4 , number of events = 1 5 5 coef exp ( coef ) se ( coef ) robust se z Pr ( &gt;| z |) treat -0 . 7 8 1 4 8 3 0 . 4 5 7 7 2 7 0 . 1 6 8 9 7 7 0 . 1 7 0 1 1 2 -4 . 5 9 4 4 . 3 5e -0 6 adult -0 . 1 3 6 9 6 7 0 . 8 7 1 9 9 9 0 . 2 8 9 3 4 4 0 . 2 7 0 9 0 9 -0 . 5 0 6 0 . 6 1 3 agedx 0 . 0 0 7 8 3 6 1 . 0 0 7 8 6 6 0 . 0 0 9 6 8 1 0 . 0 0 9 3 6 0 0 . 8 3 7 0 . 4 0 3 treat *** adult agedx exp ( coef ) exp ( -coef ) lower . 9 5 upper . 9 5 treat 0 . 4 5 7 7 2 . 1 8 4 7 0 . 3 2 7 9 0 . 6 3 8 9 adult 0 . 8 7 2 0 1 . 1 4 6 8 0 . 5 1 2 8 1 . 4 8 2 9 agedx 1 . 0 0 7 9 0 . 9 9 2 2 0 . 9 8 9 5 1 . 0 2 6 5 Concordance = 0 . 5 9 6 ( se = 0 . 0 2 4 ) Likelihood ratio test = 2 3 . 1 3 on 3 df , p = 4e -0 5 Wald test = 2 1 . 5 4 on 3 df , p = 8e -0 5 Score ( logrank ) test = 2 3 . 0 1 on 3 df , p = 4e -0 5 , Robust = 2 2 . 0 9 p = 6e -0 5 ( Note : the likelihood ratio and score tests assume independence of observations within a cluster , the Wald and robust score tests do not ). &gt; pair . cox = coxph ( Surv ( time , status ) ~treat + adult + agedx , + robust = TRUE , cluster = id , + data = diabetes ) &gt; summary ( pair . cox ) Call : coxph ( formula = Surv ( time , status ) ~treat + adult + agedx , data = diabetes , robust = TRUE , cluster = id ) n = 3 9 4 , number of events = 1 5 5 coef exp ( coef ) se ( coef ) robust se z Pr ( &gt;| z |) treat -0 . 7 8 1 4 8 3 0 . 4 5 7 7 2 7 0 . 1 6 8 9 7 7 0 . 1 4 8 3 3 0 -5 . 2 6 9 1 . 3 7e -0 7 adult -0 . 1 3 6 9 6 7 0 . 8 7 1 9 9 9 0 . 2 8 9 3 4 4 0 . 2 9 5 2 3 9 -0 . 4 6 4 0 . 6 4 3 agedx 0 . 0 0 7 8 3 6 1 . 0 0 7 8 6 6 0 . 0 0 9 6 8 1 0 . 0 1 0 2 7 2 0 . 7 6 3 0 . 4 4 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.7">Homework problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.1">Identifiability of the survival time under independent censoring</head><p>Assume the survival time T and censoring time C are continuous and independent random variables. But we can only observe y = min(T, C) and δ = 1(T ≤ C). Show that the hazard function of T can be identified by the following formula:</p><p>λ T (t) = pr(y = t, δ = 1) pr(y ≥ t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.2">Log-Normal regression model</head><p>Does the log-Normal regression model in Example 27.5 satisfy the proportional hazards assumption? Based on (y</p><p>, what is the likelihood function under Assumption 27.1? Compare it with the partial likelihood function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.3">Weibull random variable</head><p>Using (27.1) to show the formulas of density, survival, and hazard functions. Calculate its mean and variance.</p><p>Hint: Use the Gamma function to express the moments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.4">Weibull regression model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.5">Invariance of the proportional hazards model</head><p>Assume that T | x follows a proportional hazards model. Show that any non-negative and strictly increasing transformation g(T ) | x also follows a proportional hazards model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part IX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Algebra</head><p>All vectors are column vectors in this book. This is coherent with R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Basics of vectors and matrices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Euclidean space</head><p>The n-dimensional Euclidean space R n is a set of all n-dimensional vectors equipped with an inner product:</p><p>x i y i where x = (x 1 , . . . , x n ) t and y = (y 1 , . . . , y n ) t are two n-dimensional vectors. The length of a vector x is defined as</p><p>The Cauchy-Schwarz inequality states that |⟨x, y⟩| ≤ ∥x∥ • ∥y∥, or, more transparently,</p><p>The equality holds if and only if y i = a + bx i for some a and b, for all i = 1, . . . , n. We can use the Cauchy-Schwarz inequality to prove the triangle inequality ∥x + y∥ ≤ ∥x∥ + ∥y∥.</p><p>We say that x and y are orthogonal, denoted by x ⊥ y, if ⟨x, y⟩ = 0. We call a set of vectors v 1 , . . . , v m ∈ R n orthonormal if they all have unit length and are mutually orthogonal.</p><p>Geometrically, we can define the cosine of the angle between two vectors x, y ∈ R n as</p><p>For unit vectors, it reduces to the inner product. When both x and y are orthogonal to 1 n , that is, x = n -1 n i=1 x i = 0 and ȳ = n -1 n i=1 y i = 0, the formula of the cosine of the angle is identical to the sample Pearson correlation coefficient</p><p>.</p><p>Sometimes, we simply say that the cosine of the angle of two vectors measures their correlation even when they are not orthogonal to 1 n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Model and Extensions</head><p>Column space of a matrix Given an n × m matrix A, we can view it in terms of all elements</p><p>or row vectors</p><p>where</p><p>where A j ∈ R n j = 1, . . . , m. In statistics, the rows are corresponding to the units, so the ith row vector is the vector observations for unit i. Moreover, viewing A in terms of its column vectors can give more insights. Define the column space of A as</p><p>which is the set of all linear combinations of the column vectors A 1 , . . . , A m . The column space is important because we can write Aα, with α = (α 1 , . . . , α m ) t , as</p><p>We define the row space of A as the column space of A t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Matrix product</head><p>Given an n × m matrix A = (a ij ) and an m × r matrix B = (b ij ), we can define their product as C = AB where the n × r matrix C = (c ij ) has the (i, j)th element</p><p>In terms of the row vectors or column vectors of A and B, we have</p><p>the inner product of the ith row vector of A and the jth column vector of B. Moreover, the matrix product satisfies</p><p>so the column vectors of AB belongs to the column space of A; it also satisfies</p><p>so the row vectors of AB belong to the column space of B t , or equivalently, the row space of B.</p><p>for the component-wise partial derivative, which must have the same dimension as x. It is often called the gradient of f. For example, for a linear function f (x) = x t a = a t x with a, x ∈ R p , we have</p><p>. . .</p><p>. . .</p><p>for a quadratic function f (x) = x t Ax with a symmetric A ∈ R p×p and x ∈ R p , we have</p><p>. . .</p><p>These are two important rules of vector calculus used in this book, summarized below.</p><p>Proposition A.7 We have</p><p>We can also extend the definition to vector functions. If f (x) = (f 1 (x), . . . , f q (x)) t is a function from R p to R q , then we use the notation</p><p>which is a p × q matrix with rows corresponding to the elements of x and the columns corresponding to the elements of f (x). We can easily extend the first result of Proposition A.7.</p><p>Proposition A.8 For B ∈ R p×q and x ∈ R p , we have</p><p>Proof of Proposition A.8: Partition B = (B 1 , . . . , B q ) in terms of its column vectors.</p><p>The jth element of B t x is B t j x so the j-th column of ∂B t x/∂x is B j based on Proposition A.7. This verifies that ∂B t x/∂x equals B. □ Some authors define ∂f (x)/∂x as the transpose of (A.8). I adopt this form for its natural connection with (A.7) when q = 1. Sometimes, it is indeed more convenient to work with the transpose of ∂f (x)/∂x. Then I will use the notation</p><p>which puts the transpose notation on x.</p><p>I leave the proofs of the above propositions as Problem 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3 Exponential, Laplace, and Gumbel distributions</head><p>An Exponential(λ) random variable X ≥ 0 has density f (x) = λe -λx , mean 1/λ, median log 2/λ and variance 1/λ 2 . The standard Exponential random variable X 0 has λ = 1, and X 0 /λ generates Exponential(λ).</p><p>An important feature of Exponential(λ) is the memoryless property.</p><p>If X represents the survival time, then the probability of surviving another x time is always the same no matter how long the existing survival time is. Proof of Proposition B.6: Because pr(X &gt; x) = e -λx , we have</p><p>e -λc = e -λx = pr(X ≥ x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>The minimum of independent exponential random variables also follows an exponential distribution.</p><p>Proposition B.7 Assume that X i ∼ Exponential(λ i ) are independent (i = 1, . . . , n). Then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Normal conditional distributions</head><p>Suppose that (X 1 , X 2 ) has the joint distribution</p><p>where C 0 is the normalizing constant depending on (A, B, C 1 , C 2 ). To ensure that this is a well-defined density, we need A ≥ 0, and if A = 0 then |B| &lt; 1. Prove that the conditional distributions are</p><p>.</p><p>Remark: For a bivariate Normal distribution, the two conditional distributions are both Normal. The converse of the statement is not true. That is, even if the two conditional distributions are both Normal, the joint distribution may not be bivariate Normal. <ref type="bibr" target="#b123">Gelman and Meng (1991)</ref> reported this interesting result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Inverse of covariance matrix and conditional independence in multivariate Normal</head><p>Assume X = (X 1 , . . . , X p ) t ∼ N(µ, Σ). Denote the inverse of its covariance matrix by Σ -1 = (σ jk ) 1≤j,k≤p . Show that for any pair of j ̸ = k, we have</p><p>where X \(j,k) contains all the variables except X j and X k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Independence of linear and quadratic functions of the multivariate Normal</head><p>Assume Y ∼ N(µ, σ 2 I n ). For an n dimensional vector a and two n × n symmetric matrices A and B, show that</p><p>Hint: To simplify the proof, you can the pseudoinverse of A which satisfies AA + A = A. In fact, a strong result holds. <ref type="bibr" target="#b186">Ogasawara and Takahashi (1951)</ref> proved the following theorem; see also <ref type="bibr">Styan (1970, Theorem 5)</ref>.</p><p>Theorem B.11 Assume Y ∼ N(µ, Σ). Define quadratic forms Y t AY and Y t BY for two symmetric matrices A and B. The Y t AY and Y t BY are independent if and only if ΣAΣBΣ = 0, ΣAΣBµ = ΣBΣAµ = 0, µ t AΣBµ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Independence of the sample mean and variance of IID Normals</head><p>If X 1 , . . . , X n iid ∼ N(µ, σ 2 ), then X S 2 , where X = n -1 n i=1 X i and S 2 = (n -1) -1 n i=1 (X i -X) 2 . Remark: A remarkable result due to <ref type="bibr" target="#b121">Geary (1936)</ref> ensures the reverse of the above result. That is, if X 1 , . . . , X n are IID and X S 2 , then X 1 , . . . , X n must be Normals. See <ref type="bibr" target="#b171">Lukacs (1942)</ref> and <ref type="bibr" target="#b56">Benhamou et al. (2018)</ref> for proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Limiting Theorems and Basic Asymptotics</head><p>This chapter reviews the basics of limiting theorems and asymptotic analyses that are useful for this book. See <ref type="bibr" target="#b184">Newey and McFadden (1994)</ref> and <ref type="bibr" target="#b230">Van der Vaart (2000)</ref> for in-depth discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Convergence in probability</head><p>This definition incorporates the classic definition of convergence of non-random vectors:</p><p>Proposition C.1 If non-random vectors Z n → Z, the convergence also holds in probability.</p><p>Convergence in probability for random vectors is equivalent to element-wise convergence because of the following result:</p><p>The above proposition does not require any conditions on the joint distribution of (Z n , W n ).</p><p>For an IID sequence of random vectors, we have the following weak law of large numbers: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Convergence in distribution</head><p>Definition C.3 Random vectors Z n ∈ R K converge to Z in distribution, if for all every continuous point z of the function t → pr(Z ≤ t), we have pr(Z n ≤ z) → pr(Z ≤ z), n → ∞.</p><p>When the limit is a constant, we have an equivalence of convergences in probability and distribution:</p><p>Proposition C.6 If c is a non-random vector, then Z n → c in probability is equivalent to Z n → c in distribution.</p><p>For IID sequences of random vectors, we have the Lindeberg-Lévy central limit theorem (CLT):</p><p>Proposition C.7 (Lindeberg-Lévy CLT) If random vectors Z 1 , . . . , Z n are IID with mean µ and covariance Σ, then n 1/2 ( Zn -µ) = n -1/2 n i=1 (Z i -µ) → N(0, Σ) in distribution.</p><p>The more general Lindeberg-Feller CLT holds for independent sequences of random vectors:</p><p>Proposition C.8 For each n, let Z n1 , . . . , Z n,kn be independent random vectors with finite variances such that (LF1) in distribution, the standard CLT for the sample mean. Moreover, the sandwich covariance estimator is</p><p>which equals the sample variance of x, multiplied by (n -1)/n 2 ≈ 1/n. This is a standard result.</p><p>If we only assume that x 1 , . . . , x n are independent with the same mean µ but possibly different variances σ 2 i (i = 1, . . . , n), the sample mean x is still a reasonable estimator for µ which solves the same estimating equation above. Moreover, the sandwich covariance estimator V is still a consistent estimator for the true variance of x. This is less standard.</p><p>If we assume that x i ∼ [µ i , σ 2 i ] are independent, we can still use x to estimate µ = n -1 n i=1 µ i . The estimating equation remains the same as above. The sandwich covariance estimator V becomes conservative since E(x i -µ) ̸ = 0 in general. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">outcomes . . . . . . . . . . . . . . . . . . . . . . . . 20.1.1 Linear probability model . . . . . . . . . . . . . . . . . . . . . . . . . 20.1.2 General link functions . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 Maximum likelihood estimator of the logistic model . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">3 Statistics with the logit model . . . . . . . . . . . . . . . . . . . . . . . . . 20.3.1 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20.3.2 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">4 More on interpretations of the coefficients . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">5 Does the link function matter? . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">6 Extensions of the logistic regression . . . . . . . . . . . . . . . . . . . . . . 20.6.1 Penalized logistic regression . . . . . . . . . . . . . . . . . . . . . . . 20.6.2 Case-control study . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">7 Other model formulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20.7.1 Latent linear model . . . . . . . . . . . . . . . . . . . . . . . . . . . 20.7.2 Inverse model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">8 Homework problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Logistic Regressions for Categorical Outcomes 21.1 Multinomial distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.2 Multinomial logistic model for nominal outcomes . . . . . . . . . . . . . . . 21.2.1 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.2.2 MLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A latent variable representation for the multinomial logistic regression</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Proportional odds model for ordinal outcomes</title>
		<author>
			<orgName type="collaboration">. . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">5 A case study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.5.1 Binary logistic for the treatment . . . . . . . . . . . . . . . . . . . . 21.5.2 Binary logistic for the outcome . . . . . . . . . . . . . . . . . . . . . 21.5.3 Multinomial logistic for the outcome . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>21.5.4 Proportional odds logistic for the outcome . . . . . . . . . . . . . .</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">6 Discrete choice models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.6.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.6.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.6.3 More comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">22 Regression Models for Count Outcomes 22.1 Some random variables for counts</title>
		<author>
			<orgName type="collaboration">7 Homework problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ; . . . . . . . . . . . . . . . . . . . . . . . 22.1.1 Poisson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.1.2 Negative-Binomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.1.3 Zero-inflated count distributions . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 Regression models for counts . . . . . . . . . . . . . . . . . . . . . . . . . . 22.2.1 Poisson regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.2.2 Negative-Binomial regression . . . . . . . . . . . . . . . . . . . . . . 22.2.3 Zero-inflated regressions . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">3 A case study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.3.1 Linear, Poisson, and Negative-Binomial regressions . . . . . . . . . . 22.3.2 Zero-inflated regressions . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">4 Homework problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x Contents</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">1 Generalized Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 23.1.1 Exponential family . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23.1.2 Generalized linear model . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>Generalized Linear Models: A Unification 23</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 MLE for GLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Other GLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">4 Homework problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">1 Restricted mean model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24.2 Sandwich covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 24.3 Applications of the sandwich standard errors . . . . . . . . . . . . . . . . . 24.3.1 Linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24.3.2 Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24.3.2.1 An application . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>From Generalized Linear Models to Restricted Mean Models: the Sandwich Covariance Matrix 24</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 A misspecified logistic regression . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">3.3 Poisson regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24.3.3.1 A correctly specified Poisson regression . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 A Negative-Binomial regression model . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Poisson regression for binary outcomes</title>
		<author>
			<orgName type="collaboration">. . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How robust are the robust standard errors?</title>
		<author>
			<orgName type="collaboration">. . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">4 Homework problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Generalized Estimating Equation for Correlated Multivariate Data 25.1 Examples of correlated data . . . . . . . . . . . . . . . . . . . . . . . . . . 25.1.1 Longitudinal data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25.1.2 Clustered data: a neuroscience experiment . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>1.3 Clustered data: a public health intervention . . . . . . . . . . . . .</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 Marginal model and the generalized estimating equation . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">3 Statistical inference with GEE . . . . . . . . . . . . . . . . . . . . . . . . . 25.3.1 Computation using the Gauss-Newton method . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 Asymptotic inference . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>3.3 Implementation: choice of the working covariance matrix . . . . . .</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">4 A special case: cluster-robust standard error . . . . . . . . . . . . . . . . . 25.4.1 OLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25.4.2 Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">5 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25.5.1 Clustered data: a neuroscience experiment . . . . . . . . . . . . . . . 25.5.2 Clustered data: a public health intervention . . . . . . . . . . . . . . 25.5.3 Longitudinal data . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">6 Critiques on the key assumptions . . . . . . . . . . . . . . . . . . . . . . . 25.6.1 Assumption (25.4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25.6.2 Assumption (25.5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25.6.3 Explanation and prediction . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">7 Homework problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">run OLS of X on (1, W ) and obtain residual vector εx and residual sum of squares rss x</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m">we update u based on the ridge regression of Y on X v with tuning parameter λ/2, where X v = Xdiag</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m">= 506 observations. Add p = n columns of covariates of random noise, and compare OLS, ridge, and lasso, as in Section 15.4. Add p = 2n columns of covariates of random noise, and compare OLS, ridge, and lasso</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two discussants, Professors Peter Bühlmann and Chris Holmes, make some excellent comments</title>
		<author>
			<persName><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">gives a review of the lasso, as well as its history and recent developments</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>16.3 Homework problems 16.1 Piecewise linear regression Generate data in the same way as the example in Figure 16.3, and fit a continuous piecewise linear function with cutoff points 0, 0.2, 0.4, 0.6, 0.8, 1</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What are the associated asymptotic standard errors? 22.8 Data analysis Zeileis et al. (2008) gives a tutorial on count outcome regressions using the dataset from Deb and Trivedi</title>
	</analytic>
	<monogr>
		<title level="m">Prediction in the Zero-inflated Negative-Binomial regression After obtaining the MLE ( β, γ) and its asymptotic covariance matrix V , predict the conditional mean E(y i | x i ), the conditional probability pr(y i = 0 | x i ), and the conditional probability pr(y i ≥ 5 | x i )</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Replicate and extend their analysis based on the discussion in this chapter</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">was an application of Negative-Binomial regression, and Albergaria and Fávero (2017) replicated their study and argued that the zero-inflated Negative-Binomial regression was more appropriate</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Fisman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Replicate and extend their analysis based on the discussion in this chapter</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">) = 2trace(A 1 ΣA 2 Σ) + 4µ t A 1 ΣA 2 µ. Hint: Write Y = µ + Σ 1/2 Z and reduce the problem to calculating the moments of standard Normals</title>
		<imprint>
			<pubPlace>Y t A 1 Y, Y t A 2 Y</pubPlace>
		</imprint>
	</monogr>
	<note>10 Variance of the quadratic form of the multivariate Normal Prove Theorem B.9. Use it to further prove that if Y ∼ N(µ, Σ), then cov</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inference for misspecified models with fixed regressors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="1601" to="1614" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Analysis of Ordinal Categorical Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agresti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Agresti</surname></persName>
		</author>
		<title level="m">Foundations of Linear and Generalized Linear Models</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interaction terms in logit and probit models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Norton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics Letters</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="123" to="129" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On least squares and linear combination of observations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Aitkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of Edinburgh</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="42" to="48" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Narrow replication of fisman and miguel&apos;s (2007a)&apos;corruption, norms, and legal enforcement: Evidence from diplomatic parking tickets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Albergaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Fávero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Econometrics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="919" to="922" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Quantile regression under misspecification, with an application to the US wage structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Angrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fernández-Val</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="539" to="563" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Mostly Harmless Econometrics: An Empiricist&apos;s Companion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Angrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Pischke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graphs in statistical analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Anscombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="17" to="21" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Combined pharmacotherapies and behavioral interventions for alcohol dependence: the COM-BINE study: a randomized controlled trial</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Anton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ciraulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Couper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Gastfriend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hosking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Locastro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Association</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="page" from="2003" to="2017" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On nonsingularity of block two-by-two matrices</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Z</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and Its Applications</title>
		<imprint>
			<biblScope unit="volume">439</biblScope>
			<biblScope unit="page" from="2388" to="2404" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">1173</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Approximate confidence intervals</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="12" to="19" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Three remarkable properties of the normal distribution for sample variance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Benhamou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Theoretical Mathematics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1792" to="9709" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Numerical solution of saddle point problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="137" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Application of the logistic function to bio-assay</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="357" to="365" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A theorem of Jacobi and its generalization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="779" to="783" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Interpretation of interaction: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berrington De González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="371" to="385" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Regularization in statistics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="271" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Wage discrimination: reduced form and structural estimates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Blinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Human Resources</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="436" to="455" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The method of probits</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Bliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="38" to="39" />
			<date type="published" when="1934">1934</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An analysis of transformations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="211" to="243" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">An automatic finite-sample robustness metric: When can dropping a little data make a big difference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14999</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Models as approximations i: Consequences illustrated with linear regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Traskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="523" to="544" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Models as approximations ii: A model-free theory of parametric regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Kuchibhotla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="545" to="565" />
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A note on maximum likelihood estimation for regression models using grouped data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="41" to="45" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Inference on causal effects in a generalized regression kink design</title>
		<author>
			<persName><forename type="first">D</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="2453" to="2483" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Groups, the media, agency waiting costs, and FDA drug approval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="490" to="505" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Transformation and Weighting in Regression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hadi</surname></persName>
		</author>
		<title level="m">Sensitivity Analysis in Linear Regression</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Model-free nonconvex matrix completion: Local minima analysis and applications in memory-efficient kernel PCA</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Tests of equality between sets of coefficients in two linear regressions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="591" to="605" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Maximum-likelihood estimation of misspecified models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Modelling</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="134" to="138" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Plane Answers to Complex Questions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Christensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Making sense of sensitivity: Extending omitted variable bias</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="39" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The omission or addition of an independent variate in multiple linear regression</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Cochran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
	<note>Supplement to the</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Detection of influential observation in linear regression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="15" to="18" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Regression analysis when there is prior information about supplementary variables</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="176" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Tests of separate families of hypotheses</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Neyman</surname></persName>
		</editor>
		<meeting>the Fourth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1961">1961</date>
			<biblScope unit="page" from="105" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Regression models and life-tables</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="187" to="202" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Interaction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">On a generalization of a result of W. G. Cochran</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="755" to="759" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Analysis of Binary Data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Snell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>CRC press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Asymptotic inference under heteroskedasticity of unknown form</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cribari-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="215" to="233" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Estimation of random utility models in R: The mlogit package</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Croissant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Extensions of the markoff theorem on least squares</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Res Mem</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="19" to="38" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Demand for medical care by the elderly: a finite mixture approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Econometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="336" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A simulation study of alternatives to ordinary least squares</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schatzoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wermuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="77" to="91" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Improving weighted least squares inference</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Diciccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrics and Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="96" to="119" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The Frisch-Waugh-Lovell theorem for standard errors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Probability Letters</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page">108945</biblScope>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Two seemingly paradoxical results in linear models: the variance inflation factor and the analysis of covariance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">A First Course in Causal Inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The efficiency of logistic regression compared to normal discriminant analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="892" to="898" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Limit theorems for regressions with unequal and dependent errors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Cam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Neyman</surname></persName>
		</editor>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="59" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gijbels</surname></persName>
		</author>
		<title level="m">Local Polynomial Modelling and its Applications</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Multiplicative errors: log-normal or gamma</title>
		<author>
			<persName><forename type="first">D</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="266" to="268" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Time-dependent covariates in the Cox proportionalhazards regression model</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Public Health</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<title level="m">Statistical Methods for Research Workers. Edinburgh: Oliver and Boyd</title>
		<imprint>
			<date type="published" when="1925">1925</date>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Theory of statistical estimation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1925">1925b</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="700" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Corruption, norms, and legal enforcement: Evidence from diplomatic parking tickets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Miguel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Political Economy</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="1020" to="1048" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Fitzmaurice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ware</surname></persName>
		</author>
		<title level="m">Applied Longitudinal Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Harrington</surname></persName>
		</author>
		<title level="m">Counting Processes and Survival Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Impact of a confounding variable on a regression coefficient</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods and Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="147" to="194" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A statistical view of some chemometrics regression tools</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="109" to="135" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Bootstrapping regression models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1218" to="1228" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A note on screening regression equations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="152" to="155" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">On the so-called &quot;Huber sandwich estimator&quot; and &quot;robust standard errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="299" to="302" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Survival analysis: A primer</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="110" to="119" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
		<title level="m">Statistical Models: Theory and Practice</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">On inconsistent M-estimators</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="454" to="461" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Ecological regression and voting rights</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sacks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Everett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="673" to="711" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Pathwise coordinate optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Höfling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="302" to="332" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">glmnet: Lasso and elastic-net regularized generalized linear models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R package version</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Statistical confluence analysis by means of complete regression systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Frisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1934">1934</date>
			<biblScope unit="volume">5</biblScope>
			<pubPlace>Norway</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University Institute of Economics.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Partial time regressions as compared with individual trends</title>
		<author>
			<persName><forename type="first">R</forename><surname>Frisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Waugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="387" to="401" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Regression analysis for sample survey</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Fuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhya</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="117" to="132" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Regression towards mediocrity in hereditary stature</title>
		<author>
			<persName><forename type="first">F</forename><surname>Galton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Anthropological Institute of Great Britain and Ireland</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="246" to="263" />
			<date type="published" when="1886">1886</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">The distribution of &quot;Student&apos;s&quot; ratio for non-normal samples. Supplement to the</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Geary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="178" to="184" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A generalized Wilcoxon test for comparing arbitrarily singly-censored samples</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Gehan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="203" to="224" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A note on bivariate distributions that are conditionally normal</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="125" to="126" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Splitting a predictor at the upper quarter or third and the lower quarter or third</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Models, assumptions and model checking in ecological regressions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ansolabehere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Minnite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Generalized cross-validation as a method for choosing a good ridge parameter</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="215" to="223" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Ecological regressions and behavior of individuals</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="663" to="664" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Some alternatives to ecological correlation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Sociology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="610" to="625" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">The restricted least squares estimator: A pedagogical note</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Seaks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="563" to="567" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">The natural duration of cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Greenwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Report on the Natural Duration of Cancer</title>
		<imprint>
			<biblScope unit="issue">33</biblScope>
			<date type="published" when="1926">1926</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Encouraging sanitation investment in the developing world: a cluster-randomized trial</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guiteras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levinsohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mobarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="page" from="903" to="906" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Cluster-robust bootstrap inference in quantile regression models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hagemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="446" to="456" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Hedonic housing prices and the demand for clean air</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harrison</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubinfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Environmental Economics and Management</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Ridge regularization: An essential concept in data science</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="426" to="433" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Econometric duration analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="63" to="132" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">The hazards of hazard ratios</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="13" to="15" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hilbe</surname></persName>
		</author>
		<title level="m">Modeling Count Data</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Jackknifing in unbalanced situations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Hinkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="285" to="292" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Assessing the effect of an influenza vaccine in an encouragement design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Ridge regression: some simulations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kannard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="105" to="123" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Ridge regression: A historical context</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Hoerl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="420" to="425" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Lasso, fractional norm and structured sparse estimation using a Hadamard product parametrization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Hoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="186" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">A generalization of sampling without replacement from a finite universe</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Pseudo-R 2 in logistic regression model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="847" to="860" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">The behavior of maximum likelihood estimates under nonstandard conditions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">M L</forename><surname>Cam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Neyman</surname></persName>
		</editor>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="221" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Robust regression: asymptotics, conjectures and Monte Carlo</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="799" to="821" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Modelling paired survival data with covariates</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Huster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brookmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Self</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">The Blinder-Oaxaca decomposition for linear regression models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Stata Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="453" to="479" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Tests of certain linear hypotheses and their application to some educational problems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Research Memoirs</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="93" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">A note on the logistic link function</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="599" to="601" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kalbfleisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Prentice</surname></persName>
		</author>
		<title level="m">The Statistical Analysis of Failure Time Data</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Nonparametric estimation from incomplete observations</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="457" to="481" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Proportionally difficult: testing for nonproportional hazards in Cox models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Keele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="189" to="205" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior from Aggregate Data</title>
		<author>
			<persName><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">How robust standard errors expose methodological problems they do not fix, and what to do about it</title>
		<author>
			<persName><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Regression quantiles</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bassett</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="33" to="50" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Parameterschätzung bei a-priori-Information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koopmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Vandenhoeck &amp; Ruprecht</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Evaluating the econometric evaluations of training programs with experimental data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="604" to="620" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">A simulation study of ridge and other regression estimators</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Lawless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="307" to="323" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">An approximation theorem for the Poisson Binomial distribution</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1181" to="1197" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">A general concept of unbiasedness</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="587" to="592" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Distributionfree predictive inference for regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>G'sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="1094" to="1111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Survey weighted hat matrix and leverages</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Valliant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Survey Methodology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Longitudinal data analysis using generalized linear models</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="13" to="22" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Simultaneous inference on treatment effects in survival studies with factorial designs</title>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Bunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Couper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1078" to="1085" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">The robust inference for the cox proportional hazards model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1074" to="1078" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Correcting for heteroscedasticity with heteroscedasticity consistent standard errors in the linear regression model: Small sample considerations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ervin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">47405</biblScope>
			<pubPlace>Bloomington, IN</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Indiana University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Using heteroscedasticity consistent standard errors in the linear regression model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ervin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="217" to="224" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Seasonal adjustment of economic time series and multiple regression analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="993" to="1010" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">A characterization of the normal distribution</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lukacs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1942">1942</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="91" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Mackinnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="305" to="325" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Improving survey-weighted least squares regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Magee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Evaluation of survival data and two new rank order statistics arising in its consideration</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Chemother. Rep</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="163" to="170" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Marshall</surname></persName>
		</author>
		<title level="m">Principles of Economics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Macmillan and Company</publisher>
			<date type="published" when="1890">1890</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Chapman and Hall/CRC</publisher>
			<pubPlace>Boca Raton</pubPlace>
		</imprint>
	</monogr>
	<note>Generalized Linear Models second edition</note>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">On heteros*edasticity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page">483</biblScope>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Conditional logit analysis of qualitative choice behavior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfadden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Econometrics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Zarembka</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">An unbalanced jackknife</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="880" to="891" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Analyzing clustered data: why and how to account for multiple observations nested within a study participant?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Fricano-Kugler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Luikart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Malley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">146721</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Monahan</surname></persName>
		</author>
		<title level="m">A Primer on Linear Models</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">A note on a general definition of the coefficient of determination</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nagelkerke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="691" to="692" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Generalized linear models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W M</forename><surname>Wedderburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (General)</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="370" to="384" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Large sample estimation and hypothesis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfadden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Econometrics IV</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Engle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Mcfadden</surname></persName>
		</editor>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="2112" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Male-female wage differentials in urban labor markets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Oaxaca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Economic Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="693" to="709" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Independence of quadratic quantities in a normal system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Science of the Hiroshima University, Series A</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">On the lasso and its dual</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Presnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Turlach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="319" to="337" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">When did we begin to spell &apos;heteros*edasticity&apos; correctly?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Paloyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philippine Review of Economics</title>
		<imprint>
			<biblScope unit="volume">LI</biblScope>
			<biblScope unit="page" from="162" to="178" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">A cautionary note on inference for marginal regression models with longitudinal data and general correlated response data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="939" to="951" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Asymptotically efficient rank invariant test procedures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Peto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (General)</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Estimation of monotonic regression models under quantile restrictions. Nonparametric and semiparametric methods in Econometrics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="357" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Concavity of the log likelihood</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="103" to="106" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Logistic regression diagnostics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pregibon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="705" to="724" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Logistic disease incidence models and case-control studies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Prentice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pyke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="403" to="411" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Approximate tests of correlation in time-series</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Quenouille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="68" to="84" />
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Notes on bias in estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Quenouille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="353" to="360" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<title level="m">Linear Statistical Inference and its Applications</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Jackknifing maximum likelihood estimates</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Reeds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="727" to="739" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Ecological correlations and the behavior of individuals</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Sociology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="351" to="357" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">On the relationship between the Johnson-Neyman region of significance and statistical tests of parallel within-group regressions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rogosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="73" to="84" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Resurrecting weighted least squares</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Incentives, commitments, and habit formation in exercise: evidence from a field experiment with workers at a fortune-500 company</title>
		<author>
			<persName><forename type="first">H</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sydnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Journal: Applied Economics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="51" to="84" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">For objective causal inference, design trumps analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="808" to="840" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Perturbation bounds for means of eigenvalues and invariant subspaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="343" to="354" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">An effective bandwidth selector for local least squares regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Sheather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1257" to="1270" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Activating killer-cell immunoglobulin-like receptor genes confer risk for crohn&apos;s disease in children and adults of the western european descent: Findings based on case-control studies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Samarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Mack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iannello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Debbeche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jantchou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deslandres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Amre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">217767</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Estimators for clustered education RCTs using the Neyman model for causal inference</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Z</forename><surname>Schochet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="219" to="238" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">An asymptotic theory for linear model selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica sinica</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="221" to="242" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<title level="m" type="main">Algebraic and statistical properties of the ordinary least squares interpolator</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sekhon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15769</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">But economics is not an experimental science</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Sims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="59" to="68" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">The calculus of m-estimation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Stefanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Boos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Gauss and the invention of least squares</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Stigler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="465" to="474" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Notes on the distribution of quadratic forms in singular normal variables</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P H</forename><surname>Styan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="567" to="572" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Partial identification in econometrics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Economics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">A note on the prediction sum of squares statistic for restricted least squares</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tarpey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="116" to="118" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<monogr>
		<title level="m" type="main">Principles of Econometrics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Theil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso: a retrospective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="282" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">The lasso problem and uniqueness</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1456" to="1490" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">On the stability of inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Tikhonov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="195" to="198" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="17" to="73" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Coefficients of determination in logistic regression models-a new proposal: The coefficient of discrimination</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tjur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="366" to="372" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Sample selection models in R: Package sampleSelection</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toomet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Henningsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Train</surname></persName>
		</author>
		<title level="m">Discrete Choice Methods with Simulation</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Convergence of a block coordinate descent method for nondifferentiable minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="475" to="494" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Tsiatis</surname></persName>
		</author>
		<title level="m">Semiparametric Theory and Missing Data</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Bias and confidence in not quite large samples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">614</biblScope>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<monogr>
		<title level="m" type="main">Review of recent methodological developments in group-randomized trials: part 1-design</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>American Journal of Public Health</publisher>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="907" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<monogr>
		<title level="m" type="main">Review of recent methodological developments in group-randomized trials: part 2-analysis</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>American Journal of Public Health</publisher>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1078" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der Vaart</surname></persName>
		</author>
		<title level="m">Asymptotic Statistics</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<monogr>
		<title level="m" type="main">Explanation in Causal Inference: Methods for Mediation and Interaction</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Vanderweele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Some matrix-inequalities and metrization of matric-space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tomsk Univ. Rev</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="286" to="300" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<monogr>
		<title level="m" type="main">Algorithmic Learning in a Random World</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">The jackknife and heteroskedasticity: Consistent variance estimation for regression models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Weisberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Applied Linear Regression</note>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity</title>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="817" to="838" />
			<date type="published" when="1980">1980a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Using least squares to approximate unknown regression functions</title>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Economic Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="149" to="170" />
			<date type="published" when="1980">1980b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of misspecified models</title>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<monogr>
		<title level="m" type="main">Generalized Additive Models: an Introduction with R</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<monogr>
		<title level="m" type="main">Econometric Analysis of Cross Section and Panel Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wooldridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Introductory econometrics: a modern approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wooldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cengage Learning</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="673" to="690" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Correlation and causation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Agricultural Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="557" to="585" />
			<date type="published" when="1921">1921</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">The method of path coefficients</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="161" to="215" />
			<date type="published" when="1934">1934</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Jackknife, bootstrap and other resampling methods in regression analysis</title>
		<author>
			<persName><forename type="first">C.-F</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1261" to="1295" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Veridical data science</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumbier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="3920" to="3929" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">On the theory of correlation for any number of variables, treated by a new system of notation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">U</forename><surname>Yule</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London. Series A</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="182" to="193" />
			<date type="published" when="1907">1907</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Object-oriented computation of sandwich estimators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Regression models for count data in R</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kleiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jackman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">A coefficient of determination for generalized linear models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="310" to="316" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Regression-based causal inference with factorial experiments: estimands, model specifications, and design-based properties</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="799" to="815" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">A modified Poisson regression approach to prospective studies with binary data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Epidemiology</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="702" to="706" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
